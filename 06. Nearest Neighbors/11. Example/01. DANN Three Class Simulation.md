## Aplicando o DANN ao Problema de Simula√ß√£o de Tr√™s Classes: Uma An√°lise de Desempenho

```mermaid
graph LR
    A["Simulated 3-Class Problem"] --> B["k-NN Classification"];
    A --> C["DANN Classification"];
    B --> D["Fixed Distance Metric"];
    C --> E["Adaptive Distance Metric"];
    E --> F["Local Covariance Matrices"];
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#eee,stroke:#ccc,stroke-width:1px
    style C fill:#eee,stroke:#ccc,stroke-width:1px
    style D fill:#ddd,stroke:#aaa,stroke-width:1px
    style E fill:#ddd,stroke:#aaa,stroke-width:1px
    style F fill:#ddd,stroke:#aaa,stroke-width:1px
```

### Introdu√ß√£o

Este cap√≠tulo explora a aplica√ß√£o do algoritmo **Discriminant Adaptive Nearest Neighbors (DANN)** a um **problema simulado de classifica√ß√£o de tr√™s classes**, com o objetivo de analisar como o DANN se comporta em um cen√°rio de classifica√ß√£o com m√∫ltiplas classes e como ele se compara a outras abordagens de k-NN [^13.4]. O problema simulado que utilizaremos inclui distribui√ß√µes de dados que apresentam complexidade, o que permite avaliar a capacidade do DANN de se adaptar a diferentes densidades e fronteiras de decis√£o n√£o lineares. Analisaremos como o DANN utiliza as matrizes de covari√¢ncia entre classes para adaptar a m√©trica de dist√¢ncia localmente, o que resulta em um modelo com maior capacidade de discrimina√ß√£o entre as classes. Avaliaremos tamb√©m a qualidade das fronteiras de decis√£o obtidas com o DANN em compara√ß√£o com modelos k-NN com m√©tricas de dist√¢ncia mais simples.

### O Problema de Simula√ß√£o de Tr√™s Classes: Um Desafio de Classifica√ß√£o

O **problema de simula√ß√£o de tr√™s classes** utilizado neste cap√≠tulo consiste em um cen√°rio de classifica√ß√£o onde os dados s√£o gerados a partir de tr√™s distribui√ß√µes diferentes, representando tr√™s classes distintas [^13.3]. A escolha de um problema de tr√™s classes permite avaliar a capacidade de modelos de classifica√ß√£o de lidar com desafios que s√£o mais comuns em aplica√ß√µes reais do que problemas de duas classes.

A distribui√ß√£o das tr√™s classes √© gerada de forma a que as classes se sobreponham em algumas regi√µes do espa√ßo de *features*, o que torna o problema n√£o linearmente separ√°vel e mais complexo. Algumas das caracter√≠sticas espec√≠ficas desse problema s√£o:

1.  **Sobreposi√ß√£o entre Classes:** As tr√™s classes apresentam sobreposi√ß√£o em algumas regi√µes do espa√ßo de *features*, o que torna a separa√ß√£o das classes mais dif√≠cil.
2.  **Fronteiras de Decis√£o N√£o Lineares:** As fronteiras entre as classes n√£o seguem um padr√£o linear, mas apresentam formas complexas e irregulares.
3.  **Varia√ß√£o na Densidade:** A densidade dos dados varia em diferentes regi√µes do espa√ßo de *features*, o que pode tornar o uso de m√©tricas de dist√¢ncia fixas menos eficaz.

Essas caracter√≠sticas tornam esse problema de simula√ß√£o um bom teste para avaliar a capacidade dos algoritmos de classifica√ß√£o de lidar com complexidade, n√£o linearidade e varia√ß√µes na distribui√ß√£o dos dados. A aplica√ß√£o do DANN nesse problema permite analisar como esse m√©todo se adapta a esses desafios.

**Lemma 142:** O problema simulado de tr√™s classes apresenta distribui√ß√µes de dados complexas, com sobreposi√ß√£o entre classes e fronteiras de decis√£o n√£o lineares, o que o torna um cen√°rio √∫til para comparar diferentes algoritmos de classifica√ß√£o.
*Prova*: Ao simular diferentes classes que se sobrep√µem no espa√ßo de features, √© poss√≠vel testar como modelos de classifica√ß√£o respondem a esse desafio de separa√ß√£o de classes complexas. $\blacksquare$

**Corol√°rio 142:** A avalia√ß√£o do desempenho do DANN no problema de simula√ß√£o de tr√™s classes permite verificar sua capacidade de lidar com desafios que surgem em aplica√ß√µes de classifica√ß√£o reais.

> ‚ö†Ô∏è **Nota Importante**:  O problema simulado de tr√™s classes apresenta caracter√≠sticas como a sobreposi√ß√£o de classes e fronteiras de decis√£o n√£o lineares, que o tornam um desafio para modelos de classifica√ß√£o.

> ‚ùó **Ponto de Aten√ß√£o**: A complexidade do problema simulado permite avaliar a capacidade do DANN de se adaptar a diferentes tipos de dados e distribui√ß√µes de classes.

### Aplica√ß√£o do DANN: Adapta√ß√£o da M√©trica e da Vizinhan√ßa Localmente

A aplica√ß√£o do algoritmo **DANN (Discriminant Adaptive Nearest Neighbors)** ao problema simulado de tr√™s classes envolve a adapta√ß√£o da m√©trica de dist√¢ncia e da vizinhan√ßa de forma local, com base nas caracter√≠sticas da distribui√ß√£o dos dados em cada regi√£o [^13.4]. Para cada ponto de consulta, o DANN executa os seguintes passos:

1.  **Sele√ß√£o da Vizinhan√ßa Inicial:** Utilizando uma m√©trica de dist√¢ncia inicial (normalmente a dist√¢ncia Euclidiana), o DANN seleciona os $k$ vizinhos mais pr√≥ximos do ponto de consulta no conjunto de treinamento.
2.  **C√°lculo das Matrizes de Covari√¢ncia:** Com base nos $k$ vizinhos selecionados, s√£o calculadas as matrizes de covari√¢ncia intra-classe (W) e entre classes (B). Essas matrizes s√£o estimadas localmente e utilizam apenas as informa√ß√µes dos vizinhos para adaptar a regi√£o de proximidade.
3.  **Ajuste da M√©trica de Dist√¢ncia:** A matriz de covari√¢ncia entre classes $B$ √© transformada utilizando a matriz de covari√¢ncia intra-classe, como mostrado anteriormente. A m√©trica de dist√¢ncia √© ajustada localmente, de modo que a dist√¢ncia entre o ponto de consulta e seus vizinhos seja calculada com uma m√©trica adaptada √†s caracter√≠sticas da distribui√ß√£o local de dados.
4.  **Classifica√ß√£o:** A classe do ponto de consulta √© determinada utilizando a vota√ß√£o majorit√°ria entre os r√≥tulos das classes dos $k$ vizinhos, utilizando a nova m√©trica de dist√¢ncia.

A adapta√ß√£o da m√©trica e da vizinhan√ßa localmente permite que o DANN lide com as diferentes regi√µes do espa√ßo de *features* de forma mais eficaz, adaptando sua decis√£o √†s particularidades de cada regi√£o, e de forma a que sua escolha de vizinhos seja mais discriminat√≥ria em rela√ß√£o √†s classes.

```mermaid
graph TB
  subgraph "DANN Algorithm Steps"
    direction TB
    A["1. Initial Neighborhood Selection"] --> B["2. Covariance Matrix Calculation"]
    B --> C["3. Distance Metric Adjustment"]
    C --> D["4. Classification"]
  end
    style A fill:#eee,stroke:#ccc,stroke-width:1px
    style B fill:#eee,stroke:#ccc,stroke-width:1px
    style C fill:#eee,stroke:#ccc,stroke-width:1px
    style D fill:#eee,stroke:#ccc,stroke-width:1px
```

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que temos um ponto de consulta $x_q = [2, 3]$ e que, ap√≥s a sele√ß√£o inicial de vizinhos (com $k=5$ usando dist√¢ncia Euclidiana), os seguintes 5 vizinhos foram selecionados:
>
> - $x_1 = [1, 2]$ (Classe A)
> - $x_2 = [1.5, 2.5]$ (Classe A)
> - $x_3 = [2.5, 3.5]$ (Classe B)
> - $x_4 = [3, 2]$ (Classe B)
> - $x_5 = [3.5, 3]$ (Classe C)
>
> **Passo 1:**  Vizinhos iniciais selecionados (listados acima).
>
> **Passo 2:** C√°lculo das matrizes de covari√¢ncia:
>   - **Matriz de Covari√¢ncia Intra-classe (W):**
>     Para simplificar, vamos assumir que a matriz de covari√¢ncia intra-classe $W$ calculada a partir desses vizinhos (usando as classes A, B e C separadamente) seja:
>
>     $W = \begin{bmatrix} 0.5 & 0.1 \\ 0.1 & 0.6 \end{bmatrix}$
>
>   - **Matriz de Covari√¢ncia Entre Classes (B):**
>     A matriz de covari√¢ncia entre classes $B$, que representa a dispers√£o entre as m√©dias das classes, pode ser calculada como:
>
>     $B = \begin{bmatrix} 1.2 & 0.3 \\ 0.3 & 1.1 \end{bmatrix}$
>
> **Passo 3:** Ajuste da M√©trica de Dist√¢ncia:
>
>   A m√©trica de dist√¢ncia √© ajustada usando uma transforma√ß√£o que envolve $W$ e $B$. Em termos pr√°ticos, isso significa que a dist√¢ncia entre $x_q$ e seus vizinhos n√£o ser√° mais a dist√¢ncia euclidiana direta, mas sim uma dist√¢ncia ponderada pelas matrizes de covari√¢ncia. A transforma√ß√£o √© feita de modo que a dist√¢ncia seja menor na dire√ß√£o que melhor separa as classes, e maior nas dire√ß√µes onde as classes se sobrep√µem.
>
>   Essa transforma√ß√£o pode ser representada por uma matriz $M$ tal que a nova dist√¢ncia $d'(x_q, x_i)$ √© calculada como: $d'(x_q, x_i) = \sqrt{(x_q - x_i)^T M (x_q - x_i)}$.  A matriz $M$ √© derivada de $W$ e $B$ (e pode envolver regulariza√ß√£o para evitar problemas de singularidade).
>
>   Por exemplo, vamos supor que ap√≥s a transforma√ß√£o, a dist√¢ncia entre $x_q$ e $x_1$ passa de $d(x_q, x_1) = \sqrt{(2-1)^2 + (3-2)^2} = \sqrt{2} \approx 1.41$ para $d'(x_q, x_1) = 0.9$ usando a m√©trica ajustada.
>
>   Analogamente, a dist√¢ncia entre $x_q$ e $x_3$ pode passar de $d(x_q, x_3) = \sqrt{(2-2.5)^2 + (3-3.5)^2} = \sqrt{0.5} \approx 0.71$ para $d'(x_q, x_3) = 1.1$ com a m√©trica adaptada.
>
> **Passo 4:** Classifica√ß√£o:
>
>   Ap√≥s recalcular as dist√¢ncias com a m√©trica ajustada, os vizinhos mais pr√≥ximos podem mudar. Suponha que, com a m√©trica ajustada, os 5 vizinhos mais pr√≥ximos sejam agora:
>
>   - $x_1$ (Classe A)
>   - $x_2$ (Classe A)
>   - $x_4$ (Classe B)
>   - $x_6 = [2.8, 2.8]$ (Classe B)
>   - $x_7 = [1.8, 3.2]$ (Classe A)
>
>   A classe mais frequente √© a classe A (3 ocorr√™ncias), ent√£o o ponto de consulta $x_q$ seria classificado como Classe A.
>
> Este exemplo ilustra como o DANN ajusta a m√©trica de dist√¢ncia localmente, levando em considera√ß√£o as caracter√≠sticas da vizinhan√ßa para determinar a classe de um novo ponto.

```mermaid
graph LR
    subgraph "Distance Metric Adjustment"
        direction TB
        A["Initial Euclidean Distance d(x_q, x_i)"] --> B["Local Covariance Matrices W and B"]
        B --> C["Transformation Matrix M (derived from W and B)"]
        C --> D["Adjusted Distance d'(x_q, x_i) = sqrt((x_q - x_i)^T M (x_q - x_i))"]
    end
  style A fill:#eee,stroke:#ccc,stroke-width:1px
  style B fill:#eee,stroke:#ccc,stroke-width:1px
  style C fill:#eee,stroke:#ccc,stroke-width:1px
  style D fill:#eee,stroke:#ccc,stroke-width:1px
```

**Lemma 143:** O DANN adapta a m√©trica de dist√¢ncia e a forma da vizinhan√ßa localmente, utilizando informa√ß√µes das matrizes de covari√¢ncia entre classes e intra-classe na vizinhan√ßa de cada ponto, o que torna o m√©todo capaz de se adaptar a distribui√ß√µes de dados complexas.
*Prova*: A f√≥rmula de adapta√ß√£o da dist√¢ncia do DANN utiliza informa√ß√µes da matriz de covari√¢ncia entre classes que permite estender a regi√£o de decis√£o na dire√ß√£o que melhor separa as classes. $\blacksquare$

**Corol√°rio 143:** A adapta√ß√£o local da m√©trica de dist√¢ncia e da vizinhan√ßa torna o DANN uma abordagem mais robusta e flex√≠vel para problemas de classifica√ß√£o com distribui√ß√µes de dados n√£o homog√™neas.

> ‚ö†Ô∏è **Nota Importante**: O DANN adapta a m√©trica de dist√¢ncia e a vizinhan√ßa localmente, utilizando informa√ß√µes das matrizes de covari√¢ncia entre classes, permitindo que o m√©todo seja mais sens√≠vel √† estrutura local dos dados.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha do n√∫mero de vizinhos $k$ e do par√¢metro de regulariza√ß√£o $\epsilon$ s√£o importantes no DANN e influenciam o seu desempenho no problema de classifica√ß√£o.

### An√°lise das Fronteiras de Decis√£o: Compara√ß√£o com k-NN Padr√£o

A an√°lise das **fronteiras de decis√£o** obtidas com o DANN em compara√ß√£o com o k-NN padr√£o permite avaliar como a adapta√ß√£o local da m√©trica de dist√¢ncia afeta o processo de classifica√ß√£o.

1.  **k-NN Padr√£o:** O k-NN padr√£o utiliza uma m√©trica de dist√¢ncia fixa (normalmente a dist√¢ncia Euclidiana) e seleciona os $k$ vizinhos mais pr√≥ximos com base nessa m√©trica. Isso resulta em fronteiras de decis√£o que s√£o influenciadas pela forma da distribui√ß√£o global dos dados, e n√£o por caracter√≠sticas locais. As fronteiras obtidas podem apresentar um comportamento inadequado nas regi√µes onde a distribui√ß√£o das classes n√£o √© uniforme.
2.  **DANN:** O DANN, ao adaptar a m√©trica de dist√¢ncia localmente, resulta em fronteiras de decis√£o que se ajustam melhor √† estrutura dos dados. A regi√£o de vizinhan√ßa se estende nas dire√ß√µes onde as classes s√£o melhor separadas, o que permite que o modelo capture de forma mais precisa as complexidades das distribui√ß√µes locais. Essa abordagem produz uma fronteira mais adapt√°vel, com um formato mais irregular e sens√≠vel √†s informa√ß√µes de classe de cada regi√£o.

A an√°lise das fronteiras de decis√£o demonstra como o DANN consegue obter uma melhor separa√ß√£o entre as classes em regi√µes onde elas se sobrep√µem ou onde a densidade dos dados varia.

> üí° **Exemplo Visual:**
>
> Para ilustrar a diferen√ßa nas fronteiras de decis√£o, imagine um cen√°rio bidimensional onde:
>
> *   **Classe A** est√° concentrada em torno de (1,1) com uma pequena dispers√£o.
> *   **Classe B** est√° concentrada em torno de (3,3), tamb√©m com uma pequena dispers√£o.
> *   **Classe C** forma um arco que se estende de (1,3) a (3,1).
>
> ```mermaid
>   graph LR
>       A((Classe A)) --> B(Regi√£o de Decis√£o k-NN)
>       B --> C((Classe B))
>       C --> D(Regi√£o de Decis√£o DANN)
>       D --> E((Classe C))
>       style A fill:#ccf,stroke:#333,stroke-width:2px
>       style C fill:#fcc,stroke:#333,stroke-width:2px
>       style E fill:#cfc,stroke:#333,stroke-width:2px
>       style B fill:#eee,stroke:#ccc,stroke-width:1px,stroke-dasharray:5,5
>       style D fill:#eee,stroke:#ccc,stroke-width:1px
>
> ```
>
> *   **k-NN Padr√£o:** As fronteiras de decis√£o do k-NN seriam mais retil√≠neas, tentando separar as classes com linhas relativamente simples, o que resultaria em erros de classifica√ß√£o onde o arco da Classe C se sobrep√µe √†s regi√µes de A e B.
> *   **DANN:** O DANN produziria fronteiras mais flex√≠veis, curvando-se para dentro do arco da Classe C, adaptando a forma da fronteira para melhor seguir a distribui√ß√£o dos dados. As regi√µes de decis√£o seriam mais adaptadas √† forma das classes, resultando em uma separa√ß√£o mais precisa.

```mermaid
graph LR
    subgraph "Decision Boundary Comparison"
    direction TB
        A["k-NN (Fixed Metric)"] --> B["Global Data Distribution Influence"]
        B --> C["Linear Decision Boundaries"]
        C --> D["Potential for Misclassification"]
        E["DANN (Adaptive Metric)"] --> F["Local Data Structure Influence"]
        F --> G["Flexible, Non-Linear Boundaries"]
        G --> H["Improved Separation of Classes"]
    end
  style A fill:#eee,stroke:#ccc,stroke-width:1px
  style B fill:#eee,stroke:#ccc,stroke-width:1px
  style C fill:#eee,stroke:#ccc,stroke-width:1px
    style D fill:#eee,stroke:#ccc,stroke-width:1px
      style E fill:#eee,stroke:#ccc,stroke-width:1px
  style F fill:#eee,stroke:#ccc,stroke-width:1px
  style G fill:#eee,stroke:#ccc,stroke-width:1px
    style H fill:#eee,stroke:#ccc,stroke-width:1px
```

**Lemma 144:** As fronteiras de decis√£o obtidas com o DANN s√£o mais adapt√°veis √† estrutura local dos dados em compara√ß√£o com o k-NN padr√£o, o que permite obter modelos de classifica√ß√£o mais precisos em cen√°rios complexos.
*Prova*: A adapta√ß√£o da m√©trica e vizinhan√ßa faz com que as decis√µes de classifica√ß√£o sejam baseadas em informa√ß√µes locais e na proje√ß√£o da vizinhan√ßa no espa√ßo onde as classes s√£o mais separ√°veis. $\blacksquare$

**Corol√°rio 144:** A adapta√ß√£o das vizinhan√ßas do k-NN permite que o algoritmo apresente resultados superiores aos do k-NN com m√©trica fixa em cen√°rios com distribui√ß√µes complexas de dados.

> ‚ö†Ô∏è **Nota Importante**:  As fronteiras de decis√£o obtidas com o DANN s√£o mais adapt√°veis e se ajustam melhor √† estrutura local dos dados, em compara√ß√£o com as fronteiras obtidas com o k-NN padr√£o, que pode resultar em uma representa√ß√£o mais precisa da separa√ß√£o de classes.

> ‚ùó **Ponto de Aten√ß√£o**:  A adapta√ß√£o das fronteiras de decis√£o no DANN torna o modelo mais robusto a varia√ß√µes na densidade dos dados e √† complexidade das distribui√ß√µes das classes.

### Resultados e Conclus√µes: DANN em Problemas de Tr√™s Classes

Os resultados obtidos com a aplica√ß√£o do DANN no problema simulado de tr√™s classes demonstram que a adapta√ß√£o local da m√©trica de dist√¢ncia permite melhorar significativamente o desempenho do k-NN em problemas com dados complexos e com diferentes graus de sobreposi√ß√£o entre as classes.

Em compara√ß√£o com o k-NN padr√£o com a dist√¢ncia Euclidiana, o DANN apresentou:

1.  **Menor Taxa de Erro:** A taxa de erro m√©dia do DANN foi menor do que a do k-NN padr√£o, o que indica que o DANN √© capaz de classificar novos pontos com maior precis√£o.
2.  **Fronteiras de Decis√£o Mais Adaptadas:** As fronteiras de decis√£o obtidas com o DANN se ajustam melhor √† estrutura local dos dados, resultando em uma separa√ß√£o mais precisa das classes em regi√µes de alta densidade e de sobreposi√ß√£o entre classes.
3.  **Menor Sensibilidade ao Ru√≠do:** A capacidade do DANN de selecionar vizinhos mais relevantes com base na matriz de covari√¢ncia inter-classe e de estender as vizinhan√ßas nas dire√ß√µes mais importantes para a classifica√ß√£o, torna-o menos sens√≠vel ao ru√≠do e a *outliers* presentes nos dados.

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar a melhoria no desempenho, considere os seguintes resultados hipot√©ticos em um conjunto de teste com 1000 pontos:
>
> | M√©todo  | Taxa de Erro |
> | :------ | :----------- |
> | k-NN    | 15%          |
> | DANN    | 8%           |
>
> Isso significa que o k-NN classificou incorretamente 150 pontos, enquanto o DANN classificou incorretamente apenas 80 pontos.
>
> Al√©m disso, podemos analisar a precis√£o por classe:
>
> | M√©todo | Precis√£o Classe A | Precis√£o Classe B | Precis√£o Classe C |
> | :----- | :---------------- | :---------------- | :---------------- |
> | k-NN   | 82%               | 85%               | 80%               |
> | DANN   | 92%               | 90%               | 89%               |
>
> Observa-se que o DANN apresenta melhor precis√£o em todas as classes, mostrando que sua adapta√ß√£o local melhora a classifica√ß√£o em todas as regi√µes do espa√ßo de caracter√≠sticas.

```mermaid
graph LR
    subgraph "Performance Comparison"
        direction TB
        A["k-NN"] --> B["Higher Error Rate"]
        A --> C["Lower Class Precision"]
        D["DANN"] --> E["Lower Error Rate"]
        D --> F["Higher Class Precision"]
        B & E --> G["Error Rate Improvement"]
        C & F --> H["Precision Improvement"]
    end
  style A fill:#eee,stroke:#ccc,stroke-width:1px
  style B fill:#eee,stroke:#ccc,stroke-width:1px
  style C fill:#eee,stroke:#ccc,stroke-width:1px
  style D fill:#eee,stroke:#ccc,stroke-width:1px
    style E fill:#eee,stroke:#ccc,stroke-width:1px
    style F fill:#eee,stroke:#ccc,stroke-width:1px
    style G fill:#eee,stroke:#ccc,stroke-width:1px
        style H fill:#eee,stroke:#ccc,stroke-width:1px
```

Esses resultados demonstram como a adapta√ß√£o local da m√©trica de dist√¢ncia √© crucial para obter um bom desempenho em problemas de classifica√ß√£o com distribui√ß√µes complexas de dados. O DANN representa uma evolu√ß√£o do algoritmo k-NN, permitindo que este se adapte localmente √† estrutura dos dados, em vez de utilizar uma m√©trica global.

**Lemma 145:** Os resultados experimentais mostram que o DANN apresenta desempenho superior ao k-NN com m√©trica de dist√¢ncia fixa no problema de classifica√ß√£o simulado de tr√™s classes.
*Prova*: A aplica√ß√£o do DANN permite resultados melhores, devido a sua adapta√ß√£o local e por dar maior import√¢ncia a informa√ß√µes locais relevantes para a classifica√ß√£o. $\blacksquare$

**Corol√°rio 145:** A utiliza√ß√£o da matriz de covari√¢ncia entre classes para adaptar a m√©trica de dist√¢ncia permite ao DANN apresentar um bom desempenho em dados com distribui√ß√µes complexas.

> ‚ö†Ô∏è **Nota Importante**: O DANN apresenta melhor desempenho em compara√ß√£o com o k-NN padr√£o devido √† sua capacidade de adaptar a m√©trica de dist√¢ncia localmente, levando em conta a estrutura e as rela√ß√µes entre classes dos dados na regi√£o de consulta.

> ‚ùó **Ponto de Aten√ß√£o**: O c√°lculo das matrizes de covari√¢ncia localmente pode aumentar a complexidade computacional do DANN, sendo importante escolher corretamente os hiperpar√¢metros para garantir um bom desempenho.

### Conclus√£o

A aplica√ß√£o do algoritmo DANN ao problema simulado de tr√™s classes ilustra a import√¢ncia de adaptar o processo de classifica√ß√£o √† estrutura local dos dados. O DANN supera o k-NN padr√£o, utilizando informa√ß√µes da matriz de covari√¢ncia entre classes para ajustar a m√©trica de dist√¢ncia localmente, resultando em modelos mais precisos e robustos. A capacidade do DANN de lidar com distribui√ß√µes de dados complexas, sobreposi√ß√µes de classes e diferentes densidades de dados o torna uma ferramenta valiosa para problemas de classifica√ß√£o em aplica√ß√µes reais, onde a estrutura dos dados n√£o √© homog√™nea e as classes apresentam variabilidade.

### Footnotes

[^13.4]: "Friedman (1994a) proposed a method in which rectangular neighborhoods are found adaptively by successively carving away edges of a box containing the training data. Here we describe the discriminant adaptive nearest-neighbor (DANN) rule of Hastie and Tibshirani (1996a)...At each query point a neighborhood of say 50 points is formed, and the class distribution among the points is used to decide how to deform the neighborhood--that is, to adapt the metric. The adapted metric is then used in a nearest-neighbor rule at the query point." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3]: "These classifiers are memory-based, and require no model to be fit. Given a query point xo, we find the k training points x(r), r = 1,..., k closest in distance to xo, and then classify using majority vote among the k neighbors." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
