## M√©todos Iterativos para *Clustering*: Encontrando Agrupamentos e Centros em Dados N√£o Rotulados

```mermaid
graph LR
    subgraph "Iterative Clustering Process"
        direction TB
        A["Unlabeled Data"] --> B["Initialization: Random Cluster Centers"]
        B --> C{"Assignment Step: Assign data points to nearest center"}
        C --> D{"Update Step: Recalculate cluster centers using assigned data"}
        D --> E{"Convergence Check: Centers stabilized?"}
        E -- "No" --> C
        E -- "Yes" --> F["Final Clusters and Centers"]
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora m√©todos iterativos para **clustering**, com foco especial na identifica√ß√£o de **agrupamentos (clusters)** e seus respectivos **centros** em dados n√£o rotulados [^13.2.1]. O *clustering* √© uma t√©cnica fundamental em an√°lise de dados e aprendizado de m√°quina que busca descobrir estruturas nos dados sem a necessidade de informa√ß√µes sobre os r√≥tulos de classe. Discutiremos como m√©todos iterativos, como o K-Means, operam para encontrar grupos de dados similares e determinar seus centros, e como essa abordagem √© √∫til na explora√ß√£o de dados e na prepara√ß√£o de modelos para classifica√ß√£o supervisionada. Analisaremos os passos fundamentais do algoritmo K-Means, seus pontos fortes e limita√ß√µes, e como ele se diferencia de outros algoritmos de *clustering*.

### A Natureza Iterativa do *Clustering*

O *clustering* iterativo √© um processo que busca identificar agrupamentos de dados e determinar seus centros por meio de um procedimento que se repete at√© que uma condi√ß√£o de converg√™ncia seja atingida. A ideia central √© que, em um conjunto de dados n√£o rotulado, existem regi√µes onde os dados s√£o mais similares entre si do que com o resto dos dados, e o objetivo √© descobrir essas regi√µes e represent√°-las por seus centros.

M√©todos iterativos, como o K-Means, alternam entre duas etapas principais:

1.  **Atribui√ß√£o (Assignment):** Nesta etapa, cada ponto de dados √© atribu√≠do ao *cluster* cujo centro √© o mais pr√≥ximo, de acordo com alguma m√©trica de dist√¢ncia (normalmente a dist√¢ncia Euclidiana).
2.  **Atualiza√ß√£o (Update):** Nesta etapa, os centros dos *clusters* s√£o recalculados com base na m√©dia dos pontos a eles atribu√≠dos na etapa anterior.

Essas duas etapas s√£o repetidas iterativamente at√© que os centros dos *clusters* se estabilizem, o que indica que o algoritmo convergiu para uma solu√ß√£o. A converg√™ncia significa que os pontos n√£o mudam mais de *cluster* e os centros dos *clusters* n√£o se movem significativamente entre itera√ß√µes.

**Lemma 27:** O processo iterativo de *clustering* busca convergir para uma solu√ß√£o que minimize a vari√¢ncia intra-cluster, ou seja, que os pontos dentro de um mesmo *cluster* sejam o mais similar poss√≠vel.
*Prova*: A fase de atribui√ß√£o realoca cada ponto para o cluster cujo centro √© o mais pr√≥ximo, e a fase de atualiza√ß√£o move os centros para o ponto central dos clusters atuais, levando a uma redu√ß√£o gradual da dist√¢ncia de cada ponto ao seu centro de cluster, garantindo a converg√™ncia para um ponto de √≥timo local. $\blacksquare$

**Corol√°rio 27:** A inicializa√ß√£o dos centros dos *clusters* tem um impacto significativo na solu√ß√£o final, e diferentes inicializa√ß√µes podem levar a resultados diferentes, necessitando da execu√ß√£o do algoritmo v√°rias vezes com inicializa√ß√µes aleat√≥rias para escolha do melhor resultado.

> üí° **Exemplo Num√©rico:**
> Imagine que temos 5 pontos de dados em um espa√ßo 2D: A(1, 2), B(1.5, 1.8), C(5, 8), D(8, 8), E(1, 0.6). Inicializamos 2 centros de clusters aleatoriamente: C1(1,1) e C2(7,7).
>
> **Itera√ß√£o 1:**
>
> *   **Atribui√ß√£o:**
>     *   Dist√¢ncia(A, C1) = $\sqrt{(1-1)^2 + (2-1)^2}$ = 1
>     *   Dist√¢ncia(A, C2) = $\sqrt{(1-7)^2 + (2-7)^2}$ = 7.81
>     *   A √© atribu√≠do ao cluster 1
>     *   Dist√¢ncia(B, C1) = $\sqrt{(1.5-1)^2 + (1.8-1)^2}$ = 0.94
>     *   Dist√¢ncia(B, C2) = $\sqrt{(1.5-7)^2 + (1.8-7)^2}$ = 7.56
>     *   B √© atribu√≠do ao cluster 1
>     *   Dist√¢ncia(C, C1) = $\sqrt{(5-1)^2 + (8-1)^2}$ = 8.06
>     *   Dist√¢ncia(C, C2) = $\sqrt{(5-7)^2 + (8-7)^2}$ = 2.23
>     *   C √© atribu√≠do ao cluster 2
>     *   Dist√¢ncia(D, C1) = $\sqrt{(8-1)^2 + (8-1)^2}$ = 9.90
>     *   Dist√¢ncia(D, C2) = $\sqrt{(8-7)^2 + (8-7)^2}$ = 1.41
>     *  D √© atribu√≠do ao cluster 2
>     *   Dist√¢ncia(E, C1) = $\sqrt{(1-1)^2 + (0.6-1)^2}$ = 0.4
>     *   Dist√¢ncia(E, C2) = $\sqrt{(1-7)^2 + (0.6-7)^2}$ = 8.96
>     *   E √© atribu√≠do ao cluster 1
>
> *   **Atualiza√ß√£o:**
>     *   Novo centro C1 = M√©dia(A, B, E) = ((1+1.5+1)/3, (2+1.8+0.6)/3) = (1.17, 1.47)
>     *   Novo centro C2 = M√©dia(C, D) = ((5+8)/2, (8+8)/2) = (6.5, 8)
>
>   Este processo se repetiria at√© que os centros dos clusters n√£o se movessem significativamente, indicando converg√™ncia.

> ‚ö†Ô∏è **Nota Importante**: O *clustering* iterativo √© uma abordagem fundamental para identificar estruturas em dados n√£o rotulados, permitindo descobrir agrupamentos de pontos similares e representar esses grupos por seus centros.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha da m√©trica de dist√¢ncia e do n√∫mero de *clusters* s√£o hiperpar√¢metros cruciais que afetam o resultado do *clustering*, sendo necess√°rio o uso de t√©cnicas de sele√ß√£o de modelo para otimizar esses par√¢metros.

### K-Means: Um Algoritmo Iterativo para *Clustering*

O algoritmo **K-Means** √© um dos algoritmos de *clustering* iterativos mais populares e amplamente utilizados [^13.2.1]. O objetivo do K-Means √© particionar um conjunto de dados n√£o rotulado em $R$ *clusters*, onde $R$ √© um hiperpar√¢metro a ser definido. O algoritmo busca determinar os centros dos *clusters* e atribuir cada ponto de dados ao *cluster* mais pr√≥ximo, de forma a minimizar a vari√¢ncia intra-cluster, ou seja, a soma das dist√¢ncias quadr√°ticas entre cada ponto e o centro de seu *cluster*.

```mermaid
graph LR
    subgraph "K-Means Algorithm Steps"
        direction TB
        A["Initialization: Randomly select R cluster centers"] --> B["Assignment Step: Assign each data point to nearest cluster"]
        B --> C["Update Step: Recalculate cluster centers as means of assigned data points"]
        C --> D{"Convergence Check: Centers stabilized or max iterations reached?"}
        D -- "No" --> B
        D -- "Yes" --> E["Final Clusters and Centers"]
    end
```

O algoritmo K-Means opera em tr√™s passos principais:

1.  **Inicializa√ß√£o:** O algoritmo come√ßa com a escolha aleat√≥ria de $R$ pontos no espa√ßo de *features* como centros dos *clusters*.
2.  **Atribui√ß√£o:** Cada ponto de dados √© atribu√≠do ao *cluster* cujo centro √© o mais pr√≥ximo (menor dist√¢ncia Euclidiana).
3.  **Atualiza√ß√£o:** Os centros dos *clusters* s√£o recalculados como a m√©dia dos pontos a eles atribu√≠dos na etapa anterior.

As etapas 2 e 3 s√£o repetidas iterativamente at√© que os centros dos *clusters* n√£o se movam mais significativamente ou at√© que um n√∫mero m√°ximo de itera√ß√µes seja atingido.

**Lemma 28:** O algoritmo K-Means converge para um m√≠nimo local da fun√ß√£o de custo, que √© a soma das dist√¢ncias quadr√°ticas dos pontos ao centroide de seu *cluster*.
*Prova*: A cada itera√ß√£o, o algoritmo realoca cada ponto ao centroide mais pr√≥ximo e depois calcula os novos centroides, diminuindo a vari√¢ncia intra-cluster at√© convergir em um m√≠nimo local. $\blacksquare$

**Corol√°rio 28:** A inicializa√ß√£o dos centros dos *clusters* no K-Means afeta a solu√ß√£o final, e m√∫ltiplas inicializa√ß√µes com escolha do melhor resultado, com base na vari√¢ncia intra-cluster, s√£o necess√°rias para encontrar solu√ß√µes mais adequadas.

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o mesmo conjunto de dados anterior: A(1, 2), B(1.5, 1.8), C(5, 8), D(8, 8), E(1, 0.6). Desta vez, vamos usar K-means com R=2. Inicializamos os centroides aleatoriamente como C1(1,1) e C2(7,7).
>
> **Itera√ß√£o 1:** (como no exemplo anterior)
> *   **Atribui√ß√£o:** A, B, E -> Cluster 1; C, D -> Cluster 2
> *   **Atualiza√ß√£o:** C1 = (1.17, 1.47), C2 = (6.5, 8)
>
> **Itera√ß√£o 2:**
> *   **Atribui√ß√£o:**
>     *   Dist√¢ncia(A, C1) = $\sqrt{(1-1.17)^2 + (2-1.47)^2}$ = 0.56
>     *   Dist√¢ncia(A, C2) = $\sqrt{(1-6.5)^2 + (2-8)^2}$ = 7.93
>     *   A √© atribu√≠do ao cluster 1
>     *   Dist√¢ncia(B, C1) = $\sqrt{(1.5-1.17)^2 + (1.8-1.47)^2}$ = 0.46
>     *   Dist√¢ncia(B, C2) = $\sqrt{(1.5-6.5)^2 + (1.8-8)^2}$ = 8.03
>     *   B √© atribu√≠do ao cluster 1
>     *   Dist√¢ncia(C, C1) = $\sqrt{(5-1.17)^2 + (8-1.47)^2}$ = 7.63
>     *   Dist√¢ncia(C, C2) = $\sqrt{(5-6.5)^2 + (8-8)^2}$ = 1.5
>     *   C √© atribu√≠do ao cluster 2
>     *    Dist√¢ncia(D, C1) = $\sqrt{(8-1.17)^2 + (8-1.47)^2}$ = 9.22
>     *   Dist√¢ncia(D, C2) = $\sqrt{(8-6.5)^2 + (8-8)^2}$ = 1.5
>     *    D √© atribu√≠do ao cluster 2
>     *   Dist√¢ncia(E, C1) = $\sqrt{(1-1.17)^2 + (0.6-1.47)^2}$ = 0.89
>     *   Dist√¢ncia(E, C2) = $\sqrt{(1-6.5)^2 + (0.6-8)^2}$ = 9.29
>     *   E √© atribu√≠do ao cluster 1
> *   **Atualiza√ß√£o:**
>     *   Novo centro C1 = M√©dia(A, B, E) = ((1+1.5+1)/3, (2+1.8+0.6)/3) = (1.17, 1.47)
>     *   Novo centro C2 = M√©dia(C, D) = ((5+8)/2, (8+8)/2) = (6.5, 8)
>
> Neste caso, os centros n√£o mudaram entre itera√ß√µes, ent√£o o K-means convergiu. Os clusters s√£o {A, B, E} e {C, D} com centros em (1.17, 1.47) e (6.5, 8) respectivamente.

> ‚ö†Ô∏è **Nota Importante**: O algoritmo K-Means √© um m√©todo iterativo simples e eficiente para *clustering* de dados n√£o rotulados, mas sua converg√™ncia para um m√≠nimo local e sua sensibilidade √† inicializa√ß√£o devem ser consideradas.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha do n√∫mero de *clusters* (R) √© um hiperpar√¢metro cr√≠tico do K-Means e deve ser escolhido utilizando m√©tricas de qualidade do *clustering* e t√©cnicas de valida√ß√£o cruzada.

###  K-Means para Classifica√ß√£o Supervisionada

Embora o K-Means seja originalmente um algoritmo de *clustering* n√£o supervisionado, ele pode ser adaptado para problemas de classifica√ß√£o supervisionada [^13.2.1]. Para isso, o algoritmo √© aplicado separadamente a cada classe no conjunto de treinamento. Assim, para cada classe, o K-Means encontra os centros de *clusters* que representam a distribui√ß√£o dos dados dessa classe.

```mermaid
graph LR
    subgraph "K-Means for Supervised Classification"
        direction TB
        A["Labeled Training Data"] --> B["Apply K-Means to each class"]
        B --> C["Find cluster centers for each class"]
        C --> D["Classification: Assign new point to class with nearest cluster center"]
    end
```

Ap√≥s a obten√ß√£o dos centros de *clusters* por classe, a classifica√ß√£o de um novo ponto de consulta √© feita atribuindo-o √† classe cujo centro de *cluster* √© o mais pr√≥ximo, usando a dist√¢ncia Euclidiana. Essa adapta√ß√£o do K-Means para classifica√ß√£o supervisionada permite usar a estrutura dos agrupamentos no espa√ßo de *features* para tomar decis√µes de classifica√ß√£o.

A aplica√ß√£o do K-Means a dados rotulados resulta em uma forma mais eficiente para classificar novos pontos, pois a dist√¢ncia √© calculada apenas em rela√ß√£o aos centros, que s√£o muito menos numerosos que os dados originais.

**Lemma 29:** A adapta√ß√£o do K-Means para classifica√ß√£o supervisionada permite representar a distribui√ß√£o dos dados de cada classe por um conjunto de prot√≥tipos (centros dos *clusters*), o que simplifica o processo de classifica√ß√£o em rela√ß√£o ao uso do conjunto de dados completo.
*Prova*: Ao particionar os dados de cada classe em *clusters*, o K-means cria uma forma compacta de representa√ß√£o da distribui√ß√£o. $\blacksquare$

**Corol√°rio 29:** Em problemas de classifica√ß√£o supervisionada, o n√∫mero de *clusters* por classe no K-Means pode ser ajustado por valida√ß√£o cruzada, buscando o melhor equil√≠brio entre a capacidade de representar a distribui√ß√£o da classe e evitar *overfitting*.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos dados de treinamento com duas classes: Classe A e Classe B. Os dados da Classe A s√£o: A1(1, 2), A2(1.5, 1.8), A3(1, 0.6) e os dados da Classe B s√£o: B1(5, 8), B2(8, 8).
>
> 1. **K-Means por Classe:**
>   * **Classe A:** Aplicamos K-Means com R=1 (ou seja, encontramos o centroide dos dados da classe A). O centroide ser√° a m√©dia: C_A = ((1+1.5+1)/3, (2+1.8+0.6)/3) = (1.17, 1.47).
>   * **Classe B:** Aplicamos K-Means com R=1 (encontramos o centroide dos dados da classe B). O centroide ser√° a m√©dia: C_B = ((5+8)/2, (8+8)/2) = (6.5, 8).
>
> 2. **Classifica√ß√£o:**
>   * Para classificar um novo ponto, digamos X(2, 2), calculamos a dist√¢ncia de X aos centros de cada classe:
>     * Dist√¢ncia(X, C_A) = $\sqrt{(2-1.17)^2 + (2-1.47)^2}$ = 1.0
>     * Dist√¢ncia(X, C_B) = $\sqrt{(2-6.5)^2 + (2-8)^2}$ = 7.5
>   * Como a dist√¢ncia para C_A √© menor, X seria classificado como Classe A.
>
> Este exemplo ilustra como o K-Means pode ser usado para criar prot√≥tipos (centros de clusters) para cada classe, e como a classifica√ß√£o √© feita com base na proximidade a esses prot√≥tipos.

> ‚ö†Ô∏è **Nota Importante**: A adapta√ß√£o do K-Means para classifica√ß√£o supervisionada envolve a cria√ß√£o de prot√≥tipos, que s√£o os centros dos *clusters* de cada classe, permitindo uma abordagem mais eficiente para classifica√ß√£o com base na proximidade a esses prot√≥tipos.

> ‚ùó **Ponto de Aten√ß√£o**: O K-Means, quando usado para classifica√ß√£o supervisionada, √© uma forma de aplicar a ideia de representa√ß√£o por prot√≥tipos, simplificando a forma como a classifica√ß√£o √© realizada.

### Limita√ß√µes do K-Means

Apesar de sua popularidade e efici√™ncia, o K-Means apresenta algumas limita√ß√µes que devem ser consideradas:

1.  **Sensibilidade √† Inicializa√ß√£o:** O K-Means converge para um m√≠nimo local da fun√ß√£o de custo, e a solu√ß√£o final pode depender da inicializa√ß√£o aleat√≥ria dos centros dos *clusters*. M√∫ltiplas inicializa√ß√µes com escolha do melhor resultado podem ajudar a mitigar este problema, mas n√£o o resolve completamente.
2.  **Dificuldade com *Clusters* N√£o Convexos:** O K-Means assume que os *clusters* s√£o aproximadamente convexos, o que pode levar a resultados inadequados quando os *clusters* t√™m formas mais complexas.
3.  **Sensibilidade a *Outliers*:** A m√©dia, utilizada para calcular os centros dos *clusters*, √© sens√≠vel a *outliers*, o que pode levar a resultados enviesados.
4.  **Necessidade de Definir o N√∫mero de *Clusters*:** A escolha do n√∫mero de *clusters* √© um hiperpar√¢metro que precisa ser determinado antes de executar o algoritmo, e a escolha incorreta pode prejudicar o desempenho do *clustering*.
5. **Assume igualdade de tamanho e densidade:** O K-means tenta encontrar *clusters* de tamanho e densidade compar√°vel, e pode ter dificuldade em representar regi√µes de dados com grande diferen√ßa nesses par√¢metros.

```mermaid
graph LR
    subgraph "Limitations of K-Means"
    direction TB
        A["Sensitivity to initial cluster centers"]
        B["Difficulty with non-convex clusters"]
        C["Sensitivity to outliers"]
        D["Need to predefine number of clusters (R)"]
        E["Assumes equal size and density of clusters"]
    A --> F["Suboptimal or biased clustering results"]
    B --> F
    C --> F
    D --> F
    E --> F
    end
```

**Lemma 30:** As limita√ß√µes do K-Means decorrem de suas suposi√ß√µes sobre a forma e distribui√ß√£o dos *clusters*, e da natureza iterativa do algoritmo que converge para um m√≠nimo local.
*Prova*: O uso da dist√¢ncia Euclidiana e da m√©dia na defini√ß√£o da vari√¢ncia intra-cluster faz com que o K-means seja sens√≠vel √† escala dos dados e aos *outliers*. $\blacksquare$

**Corol√°rio 30:** Em problemas de *clustering* com distribui√ß√µes complexas, √© importante usar t√©cnicas adicionais, como padroniza√ß√£o dos dados e an√°lise de crit√©rios de qualidade do *clustering*, para mitigar as limita√ß√µes do K-Means.

> üí° **Exemplo Num√©rico (Outliers):**
>
> Considere os pontos A(1,1), B(2,1), C(1,2), e um outlier D(10,10).  Vamos usar K-Means com R=2 e inicializar os centroides como C1(0,0) e C2(10,10).
>
> **Itera√ß√£o 1:**
> *   **Atribui√ß√£o:** A, B, C -> Cluster 1; D -> Cluster 2
> *   **Atualiza√ß√£o:** C1 = (1.33, 1.33), C2 = (10, 10)
>
> **Itera√ß√£o 2:**
> *   **Atribui√ß√£o:** A, B, C -> Cluster 1; D -> Cluster 2
> *   **Atualiza√ß√£o:** C1 = (1.33, 1.33), C2 = (10, 10)
>
> Os centros convergem sem que o outlier tenha sido separado, e ele influenciou o centro do Cluster 2. Se n√£o tiv√©ssemos o outlier, os centroides seriam mais pr√≥ximos de A, B, C. Isso mostra a sensibilidade do K-Means a outliers.

> üí° **Exemplo Num√©rico (Clusters N√£o Convexos):**
>
> Imagine um conjunto de dados em forma de "lua crescente". O K-Means, devido √† sua busca por clusters convexos, teria dificuldade em separar esses dois grupos, que s√£o naturalmente n√£o convexos. Ele tentaria criar clusters mais circulares, o que n√£o se encaixa na forma dos dados.

> ‚ö†Ô∏è **Nota Importante**: Embora o K-Means seja um algoritmo eficaz para *clustering* de dados n√£o rotulados, ele apresenta limita√ß√µes que devem ser consideradas ao escolher essa t√©cnica.

> ‚ùó **Ponto de Aten√ß√£o**:  M√©todos de *clustering* mais sofisticados podem ser mais adequados para lidar com distribui√ß√µes de dados complexas, *outliers* ou quando a escolha do n√∫mero de *clusters* n√£o √© clara.

### Conclus√£o

Os m√©todos iterativos para *clustering*, como o K-Means, s√£o ferramentas poderosas para explorar a estrutura de dados n√£o rotulados, identificar agrupamentos e determinar seus centros. Embora o K-Means seja um algoritmo popular e eficiente, ele apresenta limita√ß√µes que devem ser consideradas, como a sensibilidade √† inicializa√ß√£o, dificuldade com *clusters* n√£o convexos e necessidade de definir o n√∫mero de *clusters* antes da execu√ß√£o do algoritmo. A compreens√£o dessas limita√ß√µes e o uso de t√©cnicas de pr√©-processamento, avalia√ß√£o e valida√ß√£o s√£o essenciais para a aplica√ß√£o eficaz do K-Means e de outros algoritmos iterativos de *clustering* em problemas de an√°lise de dados.

### Footnotes

[^13.2.1]: "K-means clustering is a method for finding clusters and cluster centers in a set of unlabeled data. One chooses the desired number of cluster centers, say R, and the K-means procedure iteratively moves the centers to minimize the total within cluster variance. Given an initial set of centers, the K-means algorithm alternates the two steps: for each center we identify the subset of training points (its cluster) that is closer to it than any other center; the means of each feature for the data points in each cluster are computed, and this mean vector becomes the new center for that cluster." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
