## Algoritmo Iterativo K-Means: Atribui√ß√£o e Atualiza√ß√£o de Centros de *Clusters*

```mermaid
graph TB
    subgraph "K-Means Algorithm"
        direction TB
        A["Initialize Cluster Centers"]
        B["Assignment Step: Assign points to nearest cluster"]
        C["Update Step: Recalculate cluster centers"]
        D["Check Convergence: Centers stable?"]
        E["End"]
        A --> B
        B --> C
        C --> D
        D -- "No" --> B
        D -- "Yes" --> E
    end
```

### Introdu√ß√£o

Este cap√≠tulo detalha o **algoritmo iterativo K-Means**, focando na descri√ß√£o precisa dos passos de **atribui√ß√£o de pontos a centros de *clusters*** e **rec√°lculo da m√©dia desses pontos como novos centros** [^13.2.1]. O K-Means √© um dos algoritmos de *clustering* mais populares, e sua compreens√£o requer um olhar cuidadoso sobre como os dados s√£o particionados em grupos (clusters) e como os centros desses grupos s√£o determinados iterativamente. Analisaremos cada etapa do algoritmo em detalhes, suas implica√ß√µes na qualidade do *clustering* e como o algoritmo busca convergir para uma solu√ß√£o est√°vel, com os prot√≥tipos representando os dados de forma concisa.

### A Ess√™ncia do Algoritmo K-Means: Atribui√ß√£o e Atualiza√ß√£o

O algoritmo K-Means √© um m√©todo iterativo que busca particionar um conjunto de dados n√£o rotulado em $R$ *clusters*, onde $R$ √© um hiperpar√¢metro previamente definido [^13.2.1]. A ess√™ncia do algoritmo est√° na altern√¢ncia entre duas etapas principais:

1.  **Atribui√ß√£o de pontos a centros de *clusters* (Assignment):** Nesta etapa, cada ponto de dados √© atribu√≠do ao *cluster* cujo centro √© o mais pr√≥ximo. A proximidade √© tipicamente medida pela dist√¢ncia Euclidiana entre o ponto e o centro do *cluster*. O resultado dessa etapa √© um agrupamento dos dados em $R$ *clusters*.
2.  **Rec√°lculo da m√©dia dos pontos como novos centros (Update):** Nesta etapa, os centros dos *clusters* s√£o recalculados com base na m√©dia dos pontos a eles atribu√≠dos na etapa anterior. O novo centro de cada *cluster* √© a m√©dia de todas as coordenadas dos pontos pertencentes √†quele *cluster*.

Essas duas etapas s√£o repetidas iterativamente at√© que os centros dos *clusters* se estabilizem, ou seja, n√£o se movam significativamente entre itera√ß√µes. A condi√ß√£o de converg√™ncia tamb√©m pode ser um n√∫mero m√°ximo de itera√ß√µes. O objetivo do K-Means √© minimizar a soma das dist√¢ncias quadr√°ticas entre cada ponto de dados e o centro de seu respectivo *cluster*, que √© uma medida da vari√¢ncia intra-cluster.

**Lemma 31:** O algoritmo K-Means busca minimizar a vari√¢ncia intra-cluster por meio da altern√¢ncia entre as etapas de atribui√ß√£o e atualiza√ß√£o, e sua converg√™ncia para um m√≠nimo local √© garantida pelo decr√©scimo da fun√ß√£o objetivo a cada itera√ß√£o.
*Prova*: A fase de atribui√ß√£o realoca cada ponto ao centroide mais pr√≥ximo, o que diminui a soma das dist√¢ncias quadr√°ticas, e a fase de atualiza√ß√£o move os centroides para o ponto m√©dio dos clusters atuais, tamb√©m diminuindo essa dist√¢ncia. $\blacksquare$

**Corol√°rio 31:** A sensibilidade √† inicializa√ß√£o do K-means √© consequ√™ncia do algoritmo convergir para m√≠nimos locais, onde a solu√ß√£o obtida depende da posi√ß√£o inicial aleat√≥ria dos centros.

> üí° **Exemplo Num√©rico:**
>
> Imagine que temos 3 pontos em um espa√ßo 2D: $x_1 = (1, 2)$, $x_2 = (1.5, 1.8)$, e $x_3 = (5, 8)$. Queremos dividi-los em 2 clusters ($R=2$). Inicializamos os centros aleatoriamente como $c_1 = (1, 1)$ e $c_2 = (5, 5)$.
>
> **Itera√ß√£o 1:**
>
> *   **Atribui√ß√£o:**
>     *   Calculamos a dist√¢ncia Euclidiana de cada ponto aos centros:
>         *   $d(x_1, c_1) = \sqrt{(1-1)^2 + (2-1)^2} = 1$
>         *   $d(x_1, c_2) = \sqrt{(1-5)^2 + (2-5)^2} = 5$
>         *   $d(x_2, c_1) = \sqrt{(1.5-1)^2 + (1.8-1)^2} \approx 1.03$
>         *   $d(x_2, c_2) = \sqrt{(1.5-5)^2 + (1.8-5)^2} \approx 4.66$
>         *   $d(x_3, c_1) = \sqrt{(5-1)^2 + (8-1)^2} \approx 8.06$
>         *   $d(x_3, c_2) = \sqrt{(5-5)^2 + (8-5)^2} = 3$
>     *   Atribu√≠mos os pontos aos clusters: $x_1$ e $x_2$ ao cluster 1, e $x_3$ ao cluster 2.  $S_1 = \{x_1, x_2\}$, $S_2 = \{x_3\}$.
> *   **Atualiza√ß√£o:**
>     *   Recalculamos os centros dos clusters:
>         *   $c_1' = \frac{(1, 2) + (1.5, 1.8)}{2} = (1.25, 1.9)$
>         *   $c_2' = \frac{(5, 8)}{1} = (5, 8)$
>
> Os novos centros s√£o $c_1' = (1.25, 1.9)$ e $c_2' = (5, 8)$. O algoritmo continuaria iterando at√© que os centros n√£o mudassem significativamente.

> ‚ö†Ô∏è **Nota Importante**: O algoritmo K-Means alterna entre as etapas de atribui√ß√£o e atualiza√ß√£o, buscando convergir para uma solu√ß√£o que minimize a vari√¢ncia intra-cluster.

> ‚ùó **Ponto de Aten√ß√£o**:  A escolha da dist√¢ncia Euclidiana como m√©trica de proximidade implica que o K-Means pode ser sens√≠vel √† escala das *features*, sendo necess√°rio o pr√©-processamento dos dados com padroniza√ß√£o antes de aplicar o algoritmo.

### Detalhes da Etapa de Atribui√ß√£o

A etapa de **atribui√ß√£o** no algoritmo K-Means envolve a aloca√ß√£o de cada ponto de dados ao *cluster* cujo centro √© o mais pr√≥ximo [^13.2.1]. Para isso, cada ponto de dados √© comparado com os centros de todos os *clusters* utilizando a dist√¢ncia Euclidiana como medida de proximidade.

Formalmente, seja $X = \{x_1, x_2, \ldots, x_N\}$ o conjunto de $N$ pontos de dados, e seja $C = \{c_1, c_2, \ldots, c_R\}$ o conjunto de $R$ centros de *clusters*. A atribui√ß√£o de um ponto $x_i$ ao *cluster* $j$ √© feita utilizando a seguinte regra:

$$j^* = \arg\min_j d(x_i, c_j)$$

Onde $d(x_i, c_j)$ √© a dist√¢ncia Euclidiana entre o ponto $x_i$ e o centro do *cluster* $c_j$, e $j^*$ √© o √≠ndice do *cluster* mais pr√≥ximo de $x_i$. Ap√≥s a atribui√ß√£o, cada ponto $x_i$ pertence a um dos $R$ *clusters*.
```mermaid
graph TB
    subgraph "Assignment Step"
        direction TB
        A["Input: Data point x_i, cluster centers C = {c_1, ..., c_R}"]
        B["Compute Euclidean distances: d(x_i, c_j) for all j"]
        C["Find closest cluster index: j* = argmin_j d(x_i, c_j)"]
        D["Output: Assign x_i to cluster j*"]
        A --> B
        B --> C
        C --> D
    end
```

**Lemma 32:** A etapa de atribui√ß√£o no K-Means garante que cada ponto seja alocado ao *cluster* que minimiza a dist√¢ncia entre o ponto e o centro desse *cluster*.
*Prova*: A regra de atribui√ß√£o √© baseada na dist√¢ncia Euclidiana, e o ponto √© atribu√≠do ao centroide mais pr√≥ximo, o que garante que a dist√¢ncia do ponto ao seu centro de cluster seja minimizada. $\blacksquare$

**Corol√°rio 32:** A complexidade computacional da etapa de atribui√ß√£o √© linear em rela√ß√£o ao n√∫mero de pontos de dados e ao n√∫mero de *clusters*, o que a torna computacionalmente eficiente para grandes conjuntos de dados.

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior, na primeira itera√ß√£o, o ponto $x_1 = (1, 2)$ √© comparado com $c_1 = (1, 1)$ e $c_2 = (5, 5)$.
>
> *   $d(x_1, c_1) = \sqrt{(1-1)^2 + (2-1)^2} = 1$
> *   $d(x_1, c_2) = \sqrt{(1-5)^2 + (2-5)^2} = 5$
>
> Como $d(x_1, c_1) < d(x_1, c_2)$, o ponto $x_1$ √© atribu√≠do ao cluster 1. Este processo √© repetido para todos os pontos e todos os centros.

> ‚ö†Ô∏è **Nota Importante**: A etapa de atribui√ß√£o √© o cora√ß√£o do algoritmo K-Means, garantindo que cada ponto seja alocado ao *cluster* mais pr√≥ximo.

> ‚ùó **Ponto de Aten√ß√£o**:  O c√°lculo da dist√¢ncia Euclidiana em cada itera√ß√£o √© a opera√ß√£o mais custosa do algoritmo, sendo fundamental otimizar essa etapa para obter bom desempenho em grandes conjuntos de dados.

### Detalhes da Etapa de Atualiza√ß√£o

A etapa de **atualiza√ß√£o** do algoritmo K-Means envolve o rec√°lculo dos centros dos *clusters* com base nos pontos a eles atribu√≠dos na etapa anterior [^13.2.1]. O novo centro de cada *cluster* √© a m√©dia de todas as coordenadas dos pontos pertencentes √†quele *cluster*.

Formalmente, seja $S_j$ o conjunto de pontos de dados atribu√≠dos ao *cluster* $j$. O novo centro do *cluster* $c_j'$ √© dado por:

$$c_j' = \frac{1}{|S_j|} \sum_{x_i \in S_j} x_i$$

Onde $|S_j|$ √© o n√∫mero de pontos no *cluster* $j$, e a soma √© feita sobre todos os pontos $x_i$ que pertencem ao *cluster* $j$. O novo centro $c_j'$ √© um vetor que representa o ponto m√©dio do *cluster* $j$.

```mermaid
graph TB
    subgraph "Update Step"
      direction TB
      A["Input: Cluster assignments S_j for each cluster j"]
      B["Calculate new cluster center c_j':  c_j' = (1/|S_j|) * sum(x_i in S_j) x_i"]
      C["Output: Updated cluster centers C' = {c_1', ..., c_R'}" ]
      A --> B
      B --> C
    end
```

**Lemma 33:** A etapa de atualiza√ß√£o garante que os centros dos *clusters* sejam reposicionados na m√©dia dos pontos a eles atribu√≠dos, o que diminui a dist√¢ncia entre cada ponto e o centro de seu *cluster*.
*Prova*: A m√©dia √© o valor que minimiza a soma das dist√¢ncias quadr√°ticas dos pontos, o que significa que mover o centro para a m√©dia dos seus pontos diminui essa dist√¢ncia. $\blacksquare$

**Corol√°rio 33:** O processo iterativo de atualiza√ß√£o dos centros dos *clusters* leva √† converg√™ncia para um m√≠nimo local da fun√ß√£o de custo, onde os centros n√£o se movem mais significativamente e as atribui√ß√µes dos pontos aos clusters se estabilizam.

> üí° **Exemplo Num√©rico:**
>
> Ap√≥s a atribui√ß√£o na primeira itera√ß√£o, t√≠nhamos $S_1 = \{x_1, x_2\}$ e $S_2 = \{x_3\}$.
>
> *   O novo centro do cluster 1, $c_1'$, √© calculado como a m√©dia de $x_1$ e $x_2$:
>     $c_1' = \frac{(1, 2) + (1.5, 1.8)}{2} = (1.25, 1.9)$
> *   O novo centro do cluster 2, $c_2'$, √© calculado como a m√©dia de $x_3$:
>     $c_2' = \frac{(5, 8)}{1} = (5, 8)$
>
> Estes novos centros ser√£o usados na pr√≥xima itera√ß√£o para a etapa de atribui√ß√£o.

> ‚ö†Ô∏è **Nota Importante**: A etapa de atualiza√ß√£o √© respons√°vel por ajustar os centros dos *clusters* para refletir a distribui√ß√£o dos dados e minimizar a vari√¢ncia intra-cluster.

> ‚ùó **Ponto de Aten√ß√£o**:  A m√©dia, utilizada no c√°lculo dos novos centros, √© sens√≠vel a *outliers*, o que pode afetar negativamente o desempenho do K-Means quando os dados apresentam ru√≠do.

### Condi√ß√£o de Converg√™ncia e Inicializa√ß√£o

O algoritmo K-Means itera as etapas de atribui√ß√£o e atualiza√ß√£o at√© que uma **condi√ß√£o de converg√™ncia** seja satisfeita [^13.2.1]. A condi√ß√£o de converg√™ncia mais comum √© que os centros dos *clusters* n√£o se movam mais significativamente entre itera√ß√µes, o que indica que o algoritmo encontrou um m√≠nimo local da fun√ß√£o de custo. Outras condi√ß√µes de converg√™ncia podem ser um n√∫mero m√°ximo de itera√ß√µes ou um limiar para a varia√ß√£o dos centros entre itera√ß√µes.

A **inicializa√ß√£o** do K-Means, ou seja, a escolha dos centros dos *clusters* na primeira itera√ß√£o, √© um aspecto crucial que afeta a solu√ß√£o final do algoritmo [^13.2.1]. A inicializa√ß√£o mais comum √© a escolha aleat√≥ria de $R$ pontos de dados como centros iniciais, mas essa abordagem pode levar a resultados sub√≥timos. Estrat√©gias de inicializa√ß√£o mais sofisticadas podem ser usadas para melhorar a converg√™ncia e a qualidade do *clustering*.

```mermaid
graph TB
    subgraph "Convergence and Initialization"
        direction TB
        A["Initialize: Choose R initial cluster centers (c_1, ..., c_R)"]
        B["Iterate: Assignment and Update steps"]
        C["Check Convergence: Change in centers < threshold OR Max iterations reached?"]
        D["Output: Final cluster centers and assignments"]
        A --> B
        B --> C
        C -- "Yes" --> D
        C -- "No" --> B
    end
```

**Lemma 34:** A inicializa√ß√£o dos centros dos *clusters* no K-Means afeta a solu√ß√£o final, devido √† converg√™ncia do algoritmo para m√≠nimos locais, e uma inicializa√ß√£o ruim pode levar a solu√ß√µes sub√≥timas.
*Prova*: Como o algoritmo K-means busca o m√≠nimo local da fun√ß√£o de custo, uma escolha ruim dos centros iniciais pode levar o algoritmo a um m√≠nimo local de pior desempenho. $\blacksquare$

**Corol√°rio 34:** A repeti√ß√£o do K-means v√°rias vezes, com inicializa√ß√µes aleat√≥rias e sele√ß√£o do melhor resultado (menor vari√¢ncia intra-cluster) √© uma forma de reduzir a influ√™ncia da inicializa√ß√£o na qualidade do *clustering*.

> üí° **Exemplo Num√©rico:**
>
> Suponha que, em vez de inicializar os centros em $c_1 = (1, 1)$ e $c_2 = (5, 5)$, tiv√©ssemos inicializado em $c_1 = (1.2, 2.1)$ e $c_2 = (1.4, 1.9)$.
>
> *   Com essa inicializa√ß√£o, na primeira itera√ß√£o, $x_1$, $x_2$ e $x_3$ poderiam ser atribu√≠dos de forma diferente, levando a uma converg√™ncia para um m√≠nimo local diferente.
>
> Para mitigar isso, podemos executar o K-Means v√°rias vezes com diferentes inicializa√ß√µes aleat√≥rias e escolher o resultado com a menor soma das dist√¢ncias intra-cluster.

> ‚ö†Ô∏è **Nota Importante**: A escolha da condi√ß√£o de converg√™ncia e da estrat√©gia de inicializa√ß√£o afeta o tempo de execu√ß√£o do K-Means e a qualidade do *clustering*.

> ‚ùó **Ponto de Aten√ß√£o**:  A converg√™ncia do K-Means para um m√≠nimo local n√£o garante que a solu√ß√£o seja a melhor poss√≠vel, e √© necess√°rio avaliar a qualidade do *clustering* por meio de m√©tricas e valida√ß√£o cruzada.

### Conclus√£o

O algoritmo K-Means √© um m√©todo iterativo fundamental para *clustering* de dados n√£o rotulados, que se baseia na altern√¢ncia entre a atribui√ß√£o de pontos aos *clusters* mais pr√≥ximos e o rec√°lculo dos centros dos *clusters* pela m√©dia dos pontos a eles atribu√≠dos. A compreens√£o detalhada de cada etapa do algoritmo, bem como a consci√™ncia de suas limita√ß√µes e da import√¢ncia da inicializa√ß√£o e da escolha do n√∫mero de *clusters*, s√£o essenciais para aplicar e interpretar os resultados do K-Means em problemas de an√°lise de dados.

### Footnotes

[^13.2.1]: "K-means clustering is a method for finding clusters and cluster centers in a set of unlabeled data. One chooses the desired number of cluster centers, say R, and the K-means procedure iteratively moves the centers to minimize the total within cluster variance. Given an initial set of centers, the K-means algorithm alternates the two steps: for each center we identify the subset of training points (its cluster) that is closer to it than any other center; the means of each feature for the data points in each cluster are computed, and this mean vector becomes the new center for that cluster." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
