## Classifica√ß√£o com K-Means: *Clustering* por Classe e Atribui√ß√£o de R√≥tulos a Centr√≥ides

```mermaid
graph LR
    subgraph "K-Means for Supervised Classification"
        direction TB
        A["Training Data"]
        subgraph "Per-Class Clustering"
            direction LR
            B["Class 1 Data"] --> C["K-Means 1"]
            D["Class 2 Data"] --> E["K-Means 2"]
            F["Class N Data"] --> G["K-Means N"]
            C --> H["Centroids 1"]
            E --> I["Centroids 2"]
            G --> J["Centroids N"]
        end
        H & I & J --> K["Label Assignment"]
        K --> L["Labeled Centroids (Prototypes)"]
        L --> M["New Data Point"]
        M --> N["Distance Calculation"]
         N --> O["Classification: Nearest Prototype"]
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora como o algoritmo **K-Means** pode ser adaptado para tarefas de **classifica√ß√£o supervisionada**, detalhando os passos que envolvem o *clustering* independente por classe, a atribui√ß√£o de r√≥tulos de classe aos **centr√≥ides** resultantes e a classifica√ß√£o de novos dados com base na proximidade ao prot√≥tipo mais pr√≥ximo [^13.2.1]. Embora o K-Means seja originalmente um m√©todo de *clustering* n√£o supervisionado, sua aplica√ß√£o em classifica√ß√£o supervisionada permite utilizar sua capacidade de identificar agrupamentos nos dados para realizar a atribui√ß√£o de classes a novos pontos. Analisaremos cada passo desse processo, suas vantagens e limita√ß√µes, e como ele se diferencia de abordagens mais tradicionais de classifica√ß√£o.

### Adapta√ß√£o do K-Means para Classifica√ß√£o Supervisionada

A adapta√ß√£o do algoritmo **K-Means** para classifica√ß√£o supervisionada envolve uma abordagem que utiliza o K-Means em cada classe separadamente, seguida pela atribui√ß√£o de r√≥tulos aos centr√≥ides e posterior classifica√ß√£o de novos pontos [^13.2.1]. Em vez de aplicar o K-Means a todo o conjunto de dados, o algoritmo √© aplicado independentemente a cada classe do conjunto de treinamento, buscando identificar *clusters* dentro de cada classe. O procedimento geral pode ser descrito nos seguintes passos:

1.  **Agrupamento (Clustering) por Classe:** Para cada classe no conjunto de treinamento, o algoritmo K-Means √© executado para identificar os *clusters* de dados e seus respectivos centros (centr√≥ides). O n√∫mero de *clusters* para cada classe pode ser um hiperpar√¢metro a ser definido, e a escolha desse n√∫mero pode ser feita utilizando t√©cnicas de valida√ß√£o cruzada.
2.  **Atribui√ß√£o de R√≥tulos aos Centr√≥ides:** Ap√≥s o *clustering* por classe, cada centr√≥ide obtido pelo K-Means recebe o r√≥tulo da classe a qual ele pertence. Esses centr√≥ides rotulados passam a ser considerados prot√≥tipos que representam a distribui√ß√£o dos dados em cada classe.
3.  **Classifica√ß√£o de Novos Dados:** Para classificar um novo ponto de consulta, a dist√¢ncia Euclidiana entre o ponto e todos os prot√≥tipos √© calculada. O ponto √© ent√£o atribu√≠do √† classe do prot√≥tipo mais pr√≥ximo. Essa etapa √© an√°loga √† etapa de classifica√ß√£o em m√©todos tradicionais de prot√≥tipos, mas os prot√≥tipos s√£o criados utilizando o K-means separadamente em cada classe.

**Lemma 35:** A adapta√ß√£o do K-Means para classifica√ß√£o supervisionada busca representar a distribui√ß√£o de cada classe por um conjunto de prot√≥tipos (centr√≥ides), o que possibilita a classifica√ß√£o de novos pontos com base na proximidade a esses prot√≥tipos.
*Prova*: Ao aplicar K-Means separadamente em cada classe, a distribui√ß√£o de cada classe √© resumida em um conjunto de prot√≥tipos que capturam a variabilidade dos dados naquela classe. $\blacksquare$

**Corol√°rio 35:** O uso do K-Means para criar prot√≥tipos por classe reduz a complexidade computacional e de armazenamento em rela√ß√£o ao uso direto de todo o conjunto de treinamento para classifica√ß√£o, como no caso do 1-NN.

> üí° **Exemplo Num√©rico:**
>
> Imagine que temos um conjunto de dados de treinamento com duas classes (A e B), cada uma com 100 pontos de duas dimens√µes.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.cluster import KMeans
> from sklearn.preprocessing import StandardScaler
>
> # Gerando dados sint√©ticos para duas classes
> np.random.seed(42)
> class_A = np.random.randn(100, 2) + [2, 2]
> class_B = np.random.randn(100, 2) + [-2, -2]
> data = np.concatenate((class_A, class_B), axis=0)
> labels = np.array([0] * 100 + [1] * 100) # 0 para classe A, 1 para classe B
>
> # Normalizando os dados
> scaler = StandardScaler()
> scaled_data = scaler.fit_transform(data)
>
> # Separando dados por classe
> class_A_data = scaled_data[labels == 0]
> class_B_data = scaled_data[labels == 1]
>
> # Aplicando K-Means em cada classe (com 2 clusters por classe)
> kmeans_A = KMeans(n_clusters=2, random_state=42, n_init=10)
> kmeans_B = KMeans(n_clusters=2, random_state=42, n_init=10)
>
> kmeans_A.fit(class_A_data)
> kmeans_B.fit(class_B_data)
>
> centroids_A = kmeans_A.cluster_centers_
> centroids_B = kmeans_B.cluster_centers_
>
> # Atribuindo r√≥tulos aos centr√≥ides (0 para classe A, 1 para classe B)
> labeled_centroids = np.concatenate((centroids_A, centroids_B), axis=0)
> centroid_labels = np.array([0] * 2 + [1] * 2)
>
> # Visualiza√ß√£o dos dados e centr√≥ides
> plt.figure(figsize=(8, 6))
> plt.scatter(class_A_data[:, 0], class_A_data[:, 1], label='Classe A', marker='o')
> plt.scatter(class_B_data[:, 0], class_B_data[:, 1], label='Classe B', marker='x')
> plt.scatter(centroids_A[:, 0], centroids_A[:, 1], color='red', marker='*', s=200, label='Centr√≥ides A')
> plt.scatter(centroids_B[:, 0], centroids_B[:, 1], color='blue', marker='*', s=200, label='Centr√≥ides B')
> plt.title('Dados e Centr√≥ides ap√≥s K-Means por Classe')
> plt.xlabel('Feature 1 (Normalizada)')
> plt.ylabel('Feature 2 (Normalizada)')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> # Exibindo os centr√≥ides
> print("Centr√≥ides da Classe A:\n", centroids_A)
> print("Centr√≥ides da Classe B:\n", centroids_B)
> ```
>
> Neste exemplo, aplicamos o K-Means separadamente em cada classe, encontrando 2 centr√≥ides por classe. Os centr√≥ides resultantes (em vermelho e azul) s√£o usados como prot√≥tipos para classificar novos pontos.

> ‚ö†Ô∏è **Nota Importante**:  A adapta√ß√£o do K-Means para classifica√ß√£o supervisionada permite usar a capacidade do algoritmo de identificar *clusters* em dados n√£o rotulados para criar prot√≥tipos que representam as distribui√ß√µes de cada classe.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha do n√∫mero de *clusters* em cada classe √© um hiperpar√¢metro importante que influencia o desempenho do modelo de classifica√ß√£o e que deve ser otimizado usando valida√ß√£o cruzada.

### Detalhes do *Clustering* por Classe

O passo de **clustering por classe** √© fundamental na adapta√ß√£o do K-Means para classifica√ß√£o supervisionada [^13.2.1]. Neste passo, o algoritmo K-Means √© aplicado independentemente a cada classe do conjunto de treinamento, ou seja, o conjunto de treinamento √© dividido em subconjuntos, um para cada classe, e o K-Means √© aplicado a cada subconjunto individualmente.

```mermaid
graph LR
    subgraph "Clustering per Class"
    direction TB
        A["Training Data"]
        subgraph "Split by Class"
            direction LR
            B["Class 1 Data"]
            C["Class 2 Data"]
            D["Class N Data"]
        end
        A --> B
        A --> C
        A --> D
        B --> E["K-Means (Class 1)"]
        C --> F["K-Means (Class 2)"]
        D --> G["K-Means (Class N)"]
        E --> H["Centroids (Class 1)"]
        F --> I["Centroids (Class 2)"]
        G --> J["Centroids (Class N)"]
    end
```

O objetivo de aplicar o K-Means em cada classe separadamente √© identificar os agrupamentos (clusters) de dados que s√£o caracter√≠sticos da distribui√ß√£o de cada classe. O n√∫mero de *clusters* por classe ($R_k$, onde $k$ √© o √≠ndice da classe) pode ser um hiperpar√¢metro a ser definido, e a escolha desse hiperpar√¢metro √© importante para a capacidade do modelo de capturar a variabilidade dos dados dentro de cada classe.

Ap√≥s a aplica√ß√£o do K-Means a cada classe, obtemos um conjunto de $R_k$ centr√≥ides para a classe $k$. Esses centr√≥ides s√£o os prot√≥tipos que ser√£o utilizados para classificar novos dados na pr√≥xima etapa.

**Lemma 36:** A aplica√ß√£o do K-Means por classe permite identificar os padr√µes espec√≠ficos de agrupamento de cada classe, e os prot√≥tipos resultantes s√£o representa√ß√µes dos centros das regi√µes de alta densidade de dados de cada classe.
*Prova*: Ao usar dados espec√≠ficos de cada classe, o K-means se adapta melhor √†s caracter√≠sticas pr√≥prias de cada classe. $\blacksquare$

**Corol√°rio 36:** A escolha do n√∫mero de *clusters* em cada classe ($R_k$) afeta a capacidade do modelo de representar a variabilidade dos dados de cada classe, e a valida√ß√£o cruzada √© necess√°ria para determinar valores √≥timos de $R_k$.

> üí° **Exemplo Num√©rico:**
>
> Continuando com o exemplo anterior, vamos ilustrar como a escolha do n√∫mero de clusters ($R_k$) afeta os centr√≥ides.
>
> ```python
> # Aplicando K-Means com 3 clusters na classe A
> kmeans_A_3 = KMeans(n_clusters=3, random_state=42, n_init=10)
> kmeans_A_3.fit(class_A_data)
> centroids_A_3 = kmeans_A_3.cluster_centers_
>
> # Visualiza√ß√£o dos centr√≥ides com 3 clusters na classe A
> plt.figure(figsize=(8, 6))
> plt.scatter(class_A_data[:, 0], class_A_data[:, 1], label='Classe A', marker='o')
> plt.scatter(class_B_data[:, 0], class_B_data[:, 1], label='Classe B', marker='x')
> plt.scatter(centroids_A_3[:, 0], centroids_A_3[:, 1], color='green', marker='*', s=200, label='Centr√≥ides A (3 clusters)')
> plt.scatter(centroids_B[:, 0], centroids_B[:, 1], color='blue', marker='*', s=200, label='Centr√≥ides B (2 clusters)')
> plt.title('Centr√≥ides com 3 clusters na Classe A e 2 na Classe B')
> plt.xlabel('Feature 1 (Normalizada)')
> plt.ylabel('Feature 2 (Normalizada)')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print("Centr√≥ides da Classe A (3 clusters):\n", centroids_A_3)
> ```
>
> Ao aumentar o n√∫mero de *clusters* para 3 na classe A, observamos que os centr√≥ides se ajustam melhor √† distribui√ß√£o dos dados dentro dessa classe, capturando mais detalhes. Isso demonstra como a escolha de $R_k$ influencia a representa√ß√£o da classe.

> ‚ö†Ô∏è **Nota Importante**:  O passo de *clustering* por classe √© crucial para a adapta√ß√£o do K-Means para classifica√ß√£o supervisionada, e a aplica√ß√£o separada do algoritmo permite que ele capture as particularidades da distribui√ß√£o dos dados de cada classe.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha de aplicar o mesmo n√∫mero de *clusters* para todas as classes ou usar um n√∫mero diferente de *clusters* para cada classe √© uma decis√£o de projeto que deve ser feita com base no problema espec√≠fico.

### Detalhes da Atribui√ß√£o de R√≥tulos e Classifica√ß√£o de Novos Dados

Ap√≥s a aplica√ß√£o do K-Means em cada classe e a obten√ß√£o dos centr√≥ides de cada *cluster*, cada centr√≥ide √© associado a um **r√≥tulo de classe**, que √© o mesmo r√≥tulo da classe para a qual o algoritmo foi aplicado [^13.2.1]. Esses centr√≥ides rotulados passam a ser os prot√≥tipos que ser√£o utilizados na etapa de classifica√ß√£o.

```mermaid
graph LR
    subgraph "Label Assignment and New Data Classification"
        direction TB
        A["Centroids from Clustering"]
        A --> B["Label Assignment"]
        B --> C["Labeled Prototypes"]
        C --> D["New Data Point x_0"]
         D --> E["Distance Calculation d(x_0, c_j)"]
        E --> F["Classification: argmin d(x_0, c_j)"]
    end
```

A etapa de **classifica√ß√£o de novos dados** envolve a atribui√ß√£o de um novo ponto de consulta √† classe do prot√≥tipo mais pr√≥ximo. A proximidade √© tipicamente medida pela dist√¢ncia Euclidiana entre o novo ponto e os prot√≥tipos de todas as classes.

Formalmente, seja $x_0$ o novo ponto de consulta, e seja $C = \{c_1, c_2, \ldots, c_P\}$ o conjunto de todos os prot√≥tipos (centr√≥ides de todos os *clusters* de todas as classes), cada um com um r√≥tulo de classe $l_j$. A classe atribu√≠da a $x_0$ √© dada por:

$$\hat{l}(x_0) = l_{j^*}, \quad \text{onde} \quad j^* = \arg\min_j d(x_0, c_j)$$

Onde $d(x_0, c_j)$ √© a dist√¢ncia Euclidiana entre o ponto de consulta $x_0$ e o prot√≥tipo $c_j$, e $l_{j^*}$ √© o r√≥tulo de classe associado ao prot√≥tipo mais pr√≥ximo $c_{j^*}$.

**Lemma 37:** A atribui√ß√£o de um novo ponto de consulta √† classe do prot√≥tipo mais pr√≥ximo no K-Means adaptado para classifica√ß√£o √© equivalente √† atribui√ß√£o √† regi√£o do espa√ßo de *features* com densidade de dados da classe mais pr√≥xima.
*Prova*: A regi√£o de dados da classe √© representada pelo conjunto de centros de cluster, e a atribui√ß√£o √© feita com base na proximidade aos centros de cluster. $\blacksquare$

**Corol√°rio 37:** A etapa de classifica√ß√£o do K-Means adaptado para classifica√ß√£o utiliza os prot√≥tipos (centr√≥ides) como um resumo das distribui√ß√µes das classes, permitindo classificar novos pontos com base na proximidade a esses resumos.

> üí° **Exemplo Num√©rico:**
>
> Vamos demonstrar a classifica√ß√£o de um novo ponto de consulta usando os centr√≥ides calculados anteriormente.
>
> ```python
> from sklearn.metrics.pairwise import euclidean_distances
>
> # Definindo um novo ponto de consulta
> new_point = np.array([[0, 0]]) # Ponto no centro do espa√ßo
> new_point_scaled = scaler.transform(new_point)
>
> # Calculando dist√¢ncias entre o novo ponto e todos os centr√≥ides
> all_centroids = np.concatenate((centroids_A, centroids_B), axis=0)
> distances = euclidean_distances(new_point_scaled, all_centroids)
>
> # Encontrando o √≠ndice do centr√≥ide mais pr√≥ximo
> closest_centroid_index = np.argmin(distances)
>
> # Obtendo o r√≥tulo do centr√≥ide mais pr√≥ximo
> predicted_label = centroid_labels[closest_centroid_index]
>
> print(f"Novo ponto de consulta: {new_point}")
> print(f"Centr√≥ides:\n{all_centroids}")
> print(f"Dist√¢ncias do novo ponto aos centr√≥ides: {distances}")
> print(f"√çndice do centr√≥ide mais pr√≥ximo: {closest_centroid_index}")
> print(f"Classe predita para o novo ponto: {predicted_label}")
>
> # Visualiza√ß√£o do novo ponto e do centr√≥ide mais pr√≥ximo
> plt.figure(figsize=(8, 6))
> plt.scatter(class_A_data[:, 0], class_A_data[:, 1], label='Classe A', marker='o')
> plt.scatter(class_B_data[:, 0], class_B_data[:, 1], label='Classe B', marker='x')
> plt.scatter(all_centroids[:, 0], all_centroids[:, 1], color='red', marker='*', s=200, label='Centr√≥ides')
> plt.scatter(new_point_scaled[:, 0], new_point_scaled[:, 1], color='purple', marker='D', s=200, label='Novo Ponto')
> plt.title('Classifica√ß√£o de um Novo Ponto')
> plt.xlabel('Feature 1 (Normalizada)')
> plt.ylabel('Feature 2 (Normalizada)')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Neste exemplo, o novo ponto (em roxo) √© classificado como pertencente √† classe cujo centr√≥ide mais pr√≥ximo (em vermelho) √© da classe A (label 0). A dist√¢ncia Euclidiana √© usada para determinar essa proximidade.

> ‚ö†Ô∏è **Nota Importante**: A classifica√ß√£o de novos dados utilizando os prot√≥tipos do K-Means √© um processo direto de atribui√ß√£o √† classe do prot√≥tipo mais pr√≥ximo, que depende da qualidade dos prot√≥tipos encontrados no passo anterior.

> ‚ùó **Ponto de Aten√ß√£o**: A dist√¢ncia Euclidiana √© utilizada para classificar novos dados, o que implica que as *features* devem ser padronizadas para evitar que *features* com escalas maiores dominem o c√°lculo da dist√¢ncia.

### Vantagens e Limita√ß√µes do K-Means para Classifica√ß√£o

A adapta√ß√£o do K-Means para classifica√ß√£o supervisionada oferece algumas vantagens em rela√ß√£o a outros m√©todos:

1.  **Simplicidade e Efici√™ncia:** O algoritmo K-Means √© simples de implementar e computacionalmente eficiente, o que o torna adequado para grandes conjuntos de dados.
2.  **Representa√ß√£o por Prot√≥tipos:** O K-Means permite representar a distribui√ß√£o dos dados de cada classe por um conjunto de prot√≥tipos (centr√≥ides), o que reduz a complexidade de armazenamento e computacional durante a classifica√ß√£o.
3.  **Capacidade de Capturar Estruturas Locais:** O K-Means busca identificar agrupamentos de dados dentro de cada classe, o que permite capturar estruturas locais na distribui√ß√£o dos dados que podem ser ignoradas por m√©todos lineares.

```mermaid
graph LR
    subgraph "K-Means Advantages"
        direction TB
        A["Simplicity and Efficiency"]
        B["Prototype Representation"]
         C["Local Structure Capture"]
    end
```

No entanto, a adapta√ß√£o do K-Means para classifica√ß√£o tamb√©m apresenta algumas limita√ß√µes:

1.  **Sensibilidade √† Inicializa√ß√£o:** O K-Means √© sens√≠vel √† inicializa√ß√£o aleat√≥ria dos centr√≥ides, o que pode levar a resultados sub√≥timos.
2.  **Dificuldade com *Clusters* N√£o Convexos:** O K-Means assume que os *clusters* s√£o aproximadamente convexos, o que pode ser problem√°tico em dados onde as classes formam regi√µes mais complexas.
3.  **Escolha do N√∫mero de *Clusters*:** A escolha do n√∫mero de *clusters* para cada classe √© um hiperpar√¢metro que precisa ser ajustado, o que pode ser um desafio.
4. **Suposi√ß√£o de igualdade de tamanho e densidade dos clusters:** O K-means tenta encontrar *clusters* de tamanho e densidade compar√°vel, o que pode ser uma limita√ß√£o na modelagem de dados reais.

```mermaid
graph LR
    subgraph "K-Means Limitations"
        direction TB
        A["Sensitivity to Initialization"]
        B["Non-Convex Cluster Difficulty"]
         C["Number of Clusters Choice"]
         D["Cluster Size and Density Assumption"]
    end
```

**Lemma 38:** A adapta√ß√£o do K-means para classifica√ß√£o √© um m√©todo eficiente para representa√ß√£o de distribui√ß√µes e classifica√ß√£o por proximidade, mas com limita√ß√µes devido √†s hip√≥teses simplificadoras do K-means original.
*Prova*: A deriva√ß√£o das vantagens e limita√ß√µes do K-means quando usado para classifica√ß√£o supervisionada surge das limita√ß√µes do algoritmo K-means e de seu uso como um criador de prot√≥tipos. $\blacksquare$

**Corol√°rio 38:** M√©todos mais sofisticados para ajuste dos prot√≥tipos, como o LVQ (Learning Vector Quantization), podem superar algumas das limita√ß√µes do K-Means adaptado para classifica√ß√£o.

> ‚ö†Ô∏è **Nota Importante**: A adapta√ß√£o do K-Means para classifica√ß√£o supervisionada oferece um m√©todo simples e eficiente para representa√ß√£o dos dados, mas √© importante considerar suas limita√ß√µes.

> ‚ùó **Ponto de Aten√ß√£o**: A valida√ß√£o cruzada √© fundamental para otimizar o n√∫mero de *clusters* por classe e avaliar o desempenho do modelo de classifica√ß√£o.

### Conclus√£o

A utiliza√ß√£o do K-Means para classifica√ß√£o supervisionada, com o *clustering* independente por classe, a atribui√ß√£o de r√≥tulos aos centr√≥ides e a classifica√ß√£o de novos dados com base na proximidade aos prot√≥tipos, oferece uma abordagem interessante e eficiente para problemas de classifica√ß√£o. A capacidade do K-Means de identificar agrupamentos de dados em cada classe separadamente permite criar prot√≥tipos que representam a variabilidade dos dados de cada classe. Apesar de suas limita√ß√µes, o K-Means adaptado para classifica√ß√£o √© uma ferramenta √∫til quando se deseja explorar os dados por meio da representa√ß√£o por prot√≥tipos, com um custo computacional relativamente baixo e capacidade de adapta√ß√£o a distribui√ß√µes de dados complexas.

### Footnotes

[^13.2.1]: "K-means clustering is a method for finding clusters and cluster centers in a set of unlabeled data... To use K-means clustering for classification of labeled data, the steps are: apply K-means clustering to the training data in each class separately, using R prototypes per class; assign a class label to each of the K √ó R prototypes; classify a new feature x to the class of the closest prototype." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
