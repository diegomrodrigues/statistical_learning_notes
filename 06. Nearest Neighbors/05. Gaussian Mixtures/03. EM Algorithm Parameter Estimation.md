## Algoritmo EM: Estimativa de Par√¢metros em Modelos de Misturas Gaussianas

```mermaid
graph LR
    subgraph "EM Algorithm for GMM"
    direction TB
        A["Initialize Parameters: Œº, Œ£, œÄ"]
        B["E-step: Compute Responsibilities P(z_i = j | x_i)"]
        C["M-step: Update Parameters Œº, Œ£, œÄ"]
        D["Check Convergence"]
        A --> B
        B --> C
        C --> D
        D -- "Not Converged" --> B
        D -- "Converged" --> E["Output Parameters"]
    end
```

### Introdu√ß√£o

Este cap√≠tulo detalha o **algoritmo Expectation-Maximization (EM)**, explorando seu papel na **estimativa de par√¢metros** em modelos de **misturas gaussianas (GMMs)** [^13.2.3]. O algoritmo EM √© uma t√©cnica iterativa para encontrar estimativas de m√°xima verossimilhan√ßa para modelos probabil√≠sticos, especialmente √∫til quando os dados s√£o incompletos ou quando existem vari√°veis latentes que n√£o s√£o diretamente observadas. Analisaremos como o algoritmo EM opera, alternando entre as etapas de Expectation (E-step) e Maximization (M-step), e como essa altern√¢ncia garante a converg√™ncia do algoritmo para uma solu√ß√£o que maximiza a verossimilhan√ßa dos dados. Discutiremos tamb√©m a import√¢ncia do algoritmo EM no contexto das GMMs e como ele difere de outros m√©todos de otimiza√ß√£o.

### O Algoritmo EM: Uma Abordagem para Estimativa de M√°xima Verossimilhan√ßa

O algoritmo **Expectation-Maximization (EM)** √© um m√©todo iterativo para encontrar estimativas de m√°xima verossimilhan√ßa para modelos probabil√≠sticos, especialmente em casos onde os dados s√£o incompletos ou onde existem **vari√°veis latentes** que n√£o s√£o diretamente observadas [^13.2.3]. Em problemas de aprendizado de m√°quina, vari√°veis latentes s√£o aquelas que n√£o s√£o diretamente medidas, mas que influenciam as observa√ß√µes que podem ser coletadas, como a perten√ßa de um ponto a um cluster em modelos de mistura.

```mermaid
graph LR
    subgraph "EM Algorithm Steps"
        direction TB
        A["Expectation (E-step)"] --> B["Compute Expected Latent Variables"]
        C["Maximization (M-step)"] --> D["Update Model Parameters"]
    end
    B --> E["Iterate E and M until convergence"]
    D --> E
```

O algoritmo EM alterna entre duas etapas principais:

1.  **Expectation (E-step):** Nesta etapa, calcula-se a esperan√ßa das vari√°veis latentes, dados os par√¢metros atuais do modelo e os dados observados. Essa etapa envolve a estima√ß√£o de um conjunto de probabilidades, com base na distribui√ß√£o do modelo.
2.  **Maximization (M-step):** Nesta etapa, os par√¢metros do modelo s√£o atualizados para maximizar a verossimilhan√ßa dos dados, dado os valores esperados das vari√°veis latentes calculados na etapa E.

O algoritmo EM repete iterativamente as etapas E e M at√© que uma condi√ß√£o de converg√™ncia seja atingida. A converg√™ncia significa que a verossimilhan√ßa dos dados n√£o melhora mais significativamente entre itera√ß√µes ou at√© que um n√∫mero m√°ximo de itera√ß√µes seja atingido.

**Lemma 67:** O algoritmo EM busca encontrar estimativas de m√°xima verossimilhan√ßa para os par√¢metros de um modelo, utilizando um processo iterativo que alterna entre a estimativa das vari√°veis latentes e a maximiza√ß√£o da fun√ß√£o de verossimilhan√ßa.
*Prova*: A etapa E calcula as probabilidades de cada ponto para as componentes gaussianas (vari√°veis latentes) utilizando a fun√ß√£o de verossimilhan√ßa, e a etapa M utiliza essa probabilidade para recalcular os par√¢metros das gaussianas, maximizando a verossimilhan√ßa com os novos par√¢metros. $\blacksquare$

**Corol√°rio 67:** O algoritmo EM converge para um m√°ximo local da fun√ß√£o de verossimilhan√ßa, e a inicializa√ß√£o dos par√¢metros afeta a solu√ß√£o final, necessitando de estrat√©gias de m√∫ltiplas inicializa√ß√µes.

> ‚ö†Ô∏è **Nota Importante**: O algoritmo EM √© uma t√©cnica iterativa para encontrar estimativas de m√°xima verossimilhan√ßa em modelos probabil√≠sticos, especialmente √∫til quando existem vari√°veis latentes.

> ‚ùó **Ponto de Aten√ß√£o**: A inicializa√ß√£o dos par√¢metros e a escolha do n√∫mero de componentes gaussianas s√£o aspectos cruciais para a converg√™ncia e o desempenho do algoritmo EM.

### Aplica√ß√£o do Algoritmo EM em Misturas Gaussianas (GMMs)

Em modelos de **misturas gaussianas (GMMs)**, o algoritmo EM √© utilizado para estimar os par√¢metros das componentes gaussianas, que s√£o os prot√≥tipos do modelo [^13.2.3]. Os par√¢metros que precisam ser estimados em uma GMM s√£o as m√©dias (centr√≥ides), as matrizes de covari√¢ncia e os pesos das componentes gaussianas.

```mermaid
graph LR
    subgraph "GMM Parameters Estimation"
        direction TB
        A["GMM Parameters"]
        B["Means (Œº): Centroids"]
        C["Covariance Matrices (Œ£)"]
        D["Mixing Weights (œÄ)"]
        A --> B
        A --> C
        A --> D
    end
```

1.  **Inicializa√ß√£o:** O algoritmo EM come√ßa com a inicializa√ß√£o aleat√≥ria dos par√¢metros das gaussianas, onde um centr√≥ide e uma matriz de covari√¢ncia s√£o gerados para cada componente da mistura.
2.  **Expectation (E-step):** Nesta etapa, √© calculada a probabilidade de cada ponto de dados pertencer a cada componente gaussiana, dadas as estimativas atuais dos par√¢metros. As probabilidades s√£o calculadas utilizando a densidade gaussiana e a mistura de probabilidades *a priori*. Formalmente, a probabilidade *a posteriori* de um ponto $x_i$ pertencer √† componente $j$ √© dada por:

$$P(z_i = j | x_i) = \frac{\pi_j \mathcal{N}(x_i; \mu_j, \Sigma_j)}{\sum_{k=1}^K \pi_k \mathcal{N}(x_i; \mu_k, \Sigma_k)}$$

Onde $z_i$ √© a componente gaussiana a qual o ponto $x_i$ pertence, $\pi_j$ √© a probabilidade *a priori* da componente $j$, $\mathcal{N}(x_i; \mu_j, \Sigma_j)$ √© a densidade gaussiana do ponto $x_i$ em rela√ß√£o √† componente $j$ com m√©dia $\mu_j$ e covari√¢ncia $\Sigma_j$.
3.  **Maximization (M-step):** Nesta etapa, os par√¢metros das gaussianas (m√©dia, matriz de covari√¢ncia e pesos) s√£o atualizados para maximizar a verossimilhan√ßa dos dados, usando as probabilidades calculadas na etapa E. As m√©dias e as covari√¢ncias s√£o recalculadas como m√©dias ponderadas por estas probabilidades.

O algoritmo EM itera entre as etapas E e M at√© que a verossimilhan√ßa dos dados convirja.

**Lemma 68:** A aplica√ß√£o do algoritmo EM em GMMs busca estimar os par√¢metros das gaussianas (centr√≥ides, matrizes de covari√¢ncia e probabilidades *a priori*), de modo a maximizar a verossimilhan√ßa dos dados sob o modelo de mistura gaussiana.
*Prova*: As etapas E e M, iteradas at√© a converg√™ncia, garantem o ajuste dos par√¢metros do modelo GMM com o objetivo de maximizar a verossimilhan√ßa dos dados. $\blacksquare$

**Corol√°rio 68:** O resultado do algoritmo EM aplicado √†s GMMs √© um conjunto de gaussianas que modelam a distribui√ß√£o dos dados, onde cada gaussiana define um prot√≥tipo usado no processo de classifica√ß√£o.

> ‚ö†Ô∏è **Nota Importante**:  O algoritmo EM √© utilizado para ajustar os par√¢metros das gaussianas em GMMs, e permite modelar a distribui√ß√£o dos dados de forma flex√≠vel e precisa.

> ‚ùó **Ponto de Aten√ß√£o**:  A inicializa√ß√£o dos par√¢metros e a escolha do n√∫mero de componentes gaussianas s√£o aspectos cr√≠ticos que influenciam o desempenho do algoritmo EM e que devem ser definidos com cuidado e valida√ß√£o.

### Etapas E e M em Detalhe

A etapa de **Expectation (E-step)** no algoritmo EM para GMMs consiste em calcular a probabilidade de cada ponto de dados pertencer a cada componente gaussiana, dados os par√¢metros atuais do modelo [^13.2.3]. Formalmente, a probabilidade do ponto $x_i$ pertencer √† componente $j$ √© dada por:

$$P(z_i = j | x_i, \Theta_t) = \frac{\pi_j^{(t)} \mathcal{N}(x_i; \mu_j^{(t)}, \Sigma_j^{(t)})}{\sum_{k=1}^K \pi_k^{(t)} \mathcal{N}(x_i; \mu_k^{(t)}, \Sigma_k^{(t)})}$$

Onde $t$ indica a itera√ß√£o atual do algoritmo, $z_i$ √© a vari√°vel latente que representa o cluster da gaussiana a qual $x_i$ pertence, $\Theta_t$ representa os par√¢metros do modelo na itera√ß√£o $t$, $\pi_j^{(t)}$ √© a probabilidade *a priori* da componente $j$, $\mu_j^{(t)}$ √© a m√©dia da componente $j$, $\Sigma_j^{(t)}$ √© a matriz de covari√¢ncia da componente $j$ e $\mathcal{N}$ √© a distribui√ß√£o normal.

```mermaid
graph LR
    subgraph "E-step: Calculating Probabilities"
        direction TB
         A["P(z_i = j | x_i, Œò_t)"]
         B["Numerator: œÄ_j^(t) * N(x_i; Œº_j^(t), Œ£_j^(t))"]
         C["Denominator: ‚àë(k=1 to K) œÄ_k^(t) * N(x_i; Œº_k^(t), Œ£_k^(t))"]
         A --> B
         A --> C
    end
```

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que temos um conjunto de dados bidimensional e estamos usando um modelo GMM com duas componentes gaussianas (K=2). Em uma dada itera√ß√£o *t*, temos os seguintes par√¢metros estimados:
>
> - Componente 1: $\pi_1^{(t)} = 0.4$, $\mu_1^{(t)} = [1, 1]$, $\Sigma_1^{(t)} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$
> - Componente 2: $\pi_2^{(t)} = 0.6$, $\mu_2^{(t)} = [5, 5]$, $\Sigma_2^{(t)} = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}$
>
> Vamos considerar um ponto de dado $x_i = [2, 2]$.
>
> **Passo 1: Calcular a densidade gaussiana para cada componente:**
>
> A densidade gaussiana √© dada por:
>
> $\mathcal{N}(x; \mu, \Sigma) = \frac{1}{\sqrt{(2\pi)^k |\Sigma|}} \exp\left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)\right)$
>
> Onde k √© a dimens√£o do dado (k=2 neste caso).
>
> Para a componente 1:
>
> $\mathcal{N}(x_i; \mu_1^{(t)}, \Sigma_1^{(t)}) = \frac{1}{2\pi} \exp\left(-\frac{1}{2} \begin{bmatrix} 2-1 \\ 2-1 \end{bmatrix}^T \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}^{-1} \begin{bmatrix} 2-1 \\ 2-1 \end{bmatrix}\right) = \frac{1}{2\pi} \exp\left(-\frac{1}{2} [1, 1] \begin{bmatrix} 1 \\ 1 \end{bmatrix}\right) = \frac{1}{2\pi} \exp(-1) \approx 0.0585$
>
> Para a componente 2:
>
> $\mathcal{N}(x_i; \mu_2^{(t)}, \Sigma_2^{(t)}) = \frac{1}{2\pi\sqrt{4}} \exp\left(-\frac{1}{2} \begin{bmatrix} 2-5 \\ 2-5 \end{bmatrix}^T \begin{bmatrix} 0.5 & 0 \\ 0 & 0.5 \end{bmatrix} \begin{bmatrix} 2-5 \\ 2-5 \end{bmatrix}\right) = \frac{1}{4\pi} \exp\left(-\frac{1}{2} [-3, -3] \begin{bmatrix} -1.5 \\ -1.5 \end{bmatrix}\right) = \frac{1}{4\pi} \exp(-4.5) \approx 0.0013$
>
> **Passo 2: Calcular a probabilidade a posteriori:**
>
> $P(z_i = 1 | x_i, \Theta_t) = \frac{0.4 \times 0.0585}{0.4 \times 0.0585 + 0.6 \times 0.0013} = \frac{0.0234}{0.0234 + 0.00078} \approx 0.967$
>
> $P(z_i = 2 | x_i, \Theta_t) = \frac{0.6 \times 0.0013}{0.4 \times 0.0585 + 0.6 \times 0.0013} = \frac{0.00078}{0.0234 + 0.00078} \approx 0.033$
>
> Assim, neste exemplo, o ponto $x_i = [2, 2]$ tem uma probabilidade de 96.7% de pertencer √† componente 1 e 3.3% de pertencer √† componente 2.

Na etapa de **Maximization (M-step)**, os par√¢metros das componentes gaussianas s√£o atualizados para maximizar a verossimilhan√ßa dos dados, usando os valores esperados das vari√°veis latentes calculadas na etapa E. Os novos valores para os par√¢metros s√£o dados por:

$$\pi_j^{(t+1)} = \frac{1}{N} \sum_{i=1}^N P(z_i = j | x_i, \Theta_t)$$
$$\mu_j^{(t+1)} = \frac{\sum_{i=1}^N P(z_i = j | x_i, \Theta_t) x_i}{\sum_{i=1}^N P(z_i = j | x_i, \Theta_t)}$$
$$\Sigma_j^{(t+1)} = \frac{\sum_{i=1}^N P(z_i = j | x_i, \Theta_t)(x_i - \mu_j^{(t+1)})(x_i - \mu_j^{(t+1)})^T}{\sum_{i=1}^N P(z_i = j | x_i, \Theta_t)}$$

Onde $N$ √© o n√∫mero de dados de treino, $\pi_j^{(t+1)}$, $\mu_j^{(t+1)}$ e $\Sigma_j^{(t+1)}$ s√£o os novos par√¢metros da componente $j$ na itera√ß√£o $t+1$.

```mermaid
graph LR
    subgraph "M-step: Parameter Updates"
        direction TB
        A["Mixing Weights: œÄ_j^(t+1)"]
        B["Means: Œº_j^(t+1)"]
        C["Covariance Matrices: Œ£_j^(t+1)"]
        A --> D["Weighted Sums Based on P(z_i = j | x_i, Œò_t)"]
        B --> D
        C --> D
        D --> E["Updated Parameters"]
    end
```

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, suponha que tenhamos tr√™s pontos de dados:
>
> $x_1 = [2, 2]$, $x_2 = [1, 0]$, $x_3 = [6, 6]$
>
> E as probabilidades calculadas na Etapa E para cada ponto e cada componente s√£o:
>
> | Ponto | $P(z_i = 1 | x_i, \Theta_t)$ | $P(z_i = 2 | x_i, \Theta_t)$ |
> |-------|----------------------------|----------------------------|
> | $x_1$ | 0.967                      | 0.033                      |
> | $x_2$ | 0.99                       | 0.01                       |
> | $x_3$ | 0.02                       | 0.98                       |
>
> **Passo 1: Atualizar os pesos $\pi_j$:**
>
> $\pi_1^{(t+1)} = \frac{1}{3} (0.967 + 0.99 + 0.02) = \frac{1.977}{3} \approx 0.659$
>
> $\pi_2^{(t+1)} = \frac{1}{3} (0.033 + 0.01 + 0.98) = \frac{1.023}{3} \approx 0.341$
>
> **Passo 2: Atualizar as m√©dias $\mu_j$:**
>
> $\mu_1^{(t+1)} = \frac{0.967 * [2, 2] + 0.99 * [1, 0] + 0.02 * [6, 6]}{0.967 + 0.99 + 0.02} = \frac{[1.934, 1.934] + [0.99, 0] + [0.12, 0.12]}{1.977} = \frac{[3.044, 2.054]}{1.977} \approx [1.54, 1.04]$
>
> $\mu_2^{(t+1)} = \frac{0.033 * [2, 2] + 0.01 * [1, 0] + 0.98 * [6, 6]}{0.033 + 0.01 + 0.98} = \frac{[0.066, 0.066] + [0.01, 0] + [5.88, 5.88]}{1.023} = \frac{[5.956, 5.946]}{1.023} \approx [5.82, 5.81]$
>
> **Passo 3: Atualizar as matrizes de covari√¢ncia $\Sigma_j$:**
>
> Este passo envolve c√°lculos mais complexos, mas o princ√≠pio √© similar:
>
> $\Sigma_j^{(t+1)} = \frac{\sum_{i=1}^N P(z_i = j | x_i, \Theta_t)(x_i - \mu_j^{(t+1)})(x_i - \mu_j^{(t+1)})^T}{\sum_{i=1}^N P(z_i = j | x_i, \Theta_t)}$
>
> Onde $(x_i - \mu_j^{(t+1)})(x_i - \mu_j^{(t+1)})^T$ √© o produto externo do vetor $(x_i - \mu_j^{(t+1)})$ consigo mesmo.
>
> Por exemplo, para a primeira componente e o ponto $x_1$:
>
> $(x_1 - \mu_1^{(t+1)})(x_1 - \mu_1^{(t+1)})^T = \begin{bmatrix} 2 - 1.54 \\ 2 - 1.04 \end{bmatrix} \begin{bmatrix} 2 - 1.54 & 2 - 1.04 \end{bmatrix} = \begin{bmatrix} 0.46 \\ 0.96 \end{bmatrix} \begin{bmatrix} 0.46 & 0.96 \end{bmatrix} = \begin{bmatrix} 0.2116 & 0.4416 \\ 0.4416 & 0.9216 \end{bmatrix}$
>
> Este processo √© repetido para todos os pontos e componentes, somando e normalizando pelo peso total para obter $\Sigma_j^{(t+1)}$.
>
> Estes novos par√¢metros s√£o ent√£o usados na pr√≥xima itera√ß√£o da Etapa E.

**Lemma 69:** As etapas E e M do algoritmo EM aplicado a GMMs garantem que a verossimilhan√ßa dos dados, dado o modelo, n√£o decres√ßa em cada itera√ß√£o.
*Prova*: A etapa E estima as probabilidades de pertin√™ncia de cada ponto a cada componente gaussiana, e a etapa M utiliza essas probabilidades para atualizar os par√¢metros das gaussianas, garantindo a maximiza√ß√£o da verossimilhan√ßa. $\blacksquare$

**Corol√°rio 69:** As etapas E e M buscam um m√≠nimo local da fun√ß√£o de custo, por isso √© necess√°rio realizar m√∫ltiplas inicializa√ß√µes para buscar um resultado mais pr√≥ximo do √≥timo global.

> ‚ö†Ô∏è **Nota Importante**: As etapas E e M do algoritmo EM s√£o calculadas iterativamente, buscando a converg√™ncia dos par√¢metros do modelo GMM para valores que maximizem a verossimilhan√ßa dos dados.

> ‚ùó **Ponto de Aten√ß√£o**: O algoritmo EM converge para um m√°ximo local, e a escolha inicial dos par√¢metros pode afetar a solu√ß√£o final.

### Conclus√£o

O algoritmo EM √© uma ferramenta poderosa para estimar os par√¢metros de modelos de misturas gaussianas, permitindo modelar distribui√ß√µes complexas e multimodais. A altern√¢ncia entre as etapas E e M garante a converg√™ncia para um m√°ximo local da verossimilhan√ßa dos dados, e os par√¢metros estimados (centr√≥ides e matrizes de covari√¢ncia) s√£o utilizados como prot√≥tipos para a classifica√ß√£o de novos pontos. A compreens√£o do funcionamento interno do algoritmo EM √© essencial para utilizar e ajustar GMMs de forma eficaz em problemas de aprendizado de m√°quina.

### Footnotes

[^13.2.3]: "The Gaussian mixture model can also be thought of as a prototype method, similar in spirit to K-means and LVQ. We discuss Gaussian mixtures in some detail in Sections 6.8, 8.5 and 12.7. Each cluster is described in terms of a Gaussian density, which has a centroid (as in K-means), and a covariance matrix...The two steps of the alternating EM algorithm are very similar to the two steps in K-means: In the E-step, each observation is assigned a responsibility or weight for each cluster, based on the likelihood of each of the corresponding Gaussians. In the M-step, each observation contributes to the weighted means (and covariances) for every cluster." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
