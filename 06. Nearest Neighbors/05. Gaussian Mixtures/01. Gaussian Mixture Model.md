## Modelagem por Misturas Gaussianas: Representando Classes com Centr√≥ides e Matrizes de Covari√¢ncia

```mermaid
graph LR
    subgraph "Gaussian Mixture Model (GMM) Representation"
        direction TB
        A["Class Representation"] --> B["Mixture of Gaussians"]
        B --> C["Gaussian Component 1: (Œº‚ÇÅ, Œ£‚ÇÅ)"]
        B --> D["Gaussian Component 2: (Œº‚ÇÇ, Œ£‚ÇÇ)"]
        B --> E["Gaussian Component N: (Œº‚Çô, Œ£‚Çô)"]
        C --> F["Centroid (Mean) Œº‚ÇÅ"]
        C --> G["Covariance Matrix Œ£‚ÇÅ"]
        D --> H["Centroid (Mean) Œº‚ÇÇ"]
        D --> I["Covariance Matrix Œ£‚ÇÇ"]
        E --> J["Centroid (Mean) Œº‚Çô"]
        E --> K["Covariance Matrix Œ£‚Çô"]
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora o uso de **misturas gaussianas (GMMs)** como uma abordagem para modelar cada classe separadamente, representando-as por meio de uma distribui√ß√£o gaussiana caracterizada por um **centr√≥ide** (m√©dia) e uma **matriz de covari√¢ncia** [^13.2.3]. As GMMs s√£o um m√©todo de prot√≥tipos que, diferentemente do K-Means e do LVQ (Learning Vector Quantization), utilizam distribui√ß√µes probabil√≠sticas para modelar a estrutura dos dados e representar cada classe por um conjunto de gaussianas, e como isso leva ao c√°lculo de probabilidades a posteriori para a classifica√ß√£o de novos pontos. Analisaremos como esses prot√≥tipos s√£o ajustados pelo algoritmo EM (Expectation-Maximization), e como a abordagem difere da utiliza√ß√£o de dist√¢ncias diretas a prot√≥tipos como no K-Means e LVQ.

### Modelagem de Classes com Distribui√ß√µes Gaussianas

Uma abordagem alternativa para modelar as distribui√ß√µes das classes em problemas de classifica√ß√£o √© utilizar **distribui√ß√µes gaussianas**, ou seja, modelar cada classe com uma ou mais distribui√ß√µes normais (gaussianas) [^13.2.3]. Cada distribui√ß√£o gaussiana √© caracterizada por um **centr√≥ide (m√©dia)**, que representa o centro da distribui√ß√£o, e uma **matriz de covari√¢ncia**, que descreve a forma e a orienta√ß√£o da distribui√ß√£o.

Ao utilizar v√°rias gaussianas para modelar uma classe, temos uma **mistura gaussiana (GMM)**, que permite representar distribui√ß√µes mais complexas, que n√£o podem ser adequadamente representadas por uma √∫nica gaussiana. Em ess√™ncia, a abordagem das GMMs busca aproximar a distribui√ß√£o de cada classe utilizando uma combina√ß√£o ponderada de gaussianas.

No contexto de m√©todos de prot√≥tipos, os par√¢metros das gaussianas (m√©dia e matriz de covari√¢ncia) atuam como prot√≥tipos, que s√£o utilizados para classificar novos pontos de consulta. Ao inv√©s de utilizar dist√¢ncias diretas entre pontos e prot√≥tipos, como no K-Means e LVQ, a classifica√ß√£o com GMMs utiliza probabilidades *a posteriori* que medem a probabilidade de o ponto de consulta pertencer a cada componente gaussiana.

**Lemma 59:** A utiliza√ß√£o de distribui√ß√µes gaussianas para modelar as classes permite representar distribui√ß√µes complexas de dados por meio da combina√ß√£o de m√∫ltiplas gaussianas, cada uma com seu centr√≥ide e matriz de covari√¢ncia.
*Prova*: A escolha de uma distribui√ß√£o gaussiana e a combina√ß√£o de diferentes distribui√ß√µes permite aproximar diversas formas de distribui√ß√µes de dados. $\blacksquare$

**Corol√°rio 59:** O n√∫mero de componentes gaussianas utilizado em uma GMM √© um hiperpar√¢metro que afeta a complexidade do modelo, e sua escolha deve balancear a capacidade do modelo de aproximar a distribui√ß√£o dos dados e evitar o *overfitting*.

> ‚ö†Ô∏è **Nota Importante**: A abordagem das GMMs permite modelar cada classe utilizando uma distribui√ß√£o gaussiana, ou combina√ß√£o de gaussianas, e esses par√¢metros s√£o usados como prot√≥tipos para classifica√ß√£o.

> ‚ùó **Ponto de Aten√ß√£o**:  A modelagem com gaussianas implica uma escolha de par√¢metros que buscam ajustar os dados, e n√£o uma divis√£o em regi√µes de alta densidade.

### Par√¢metros de uma Gaussiana: Centr√≥ide e Matriz de Covari√¢ncia

Uma distribui√ß√£o gaussiana, tamb√©m conhecida como distribui√ß√£o normal, √© caracterizada por dois par√¢metros principais: o **centr√≥ide (m√©dia)** e a **matriz de covari√¢ncia**.

1.  **Centr√≥ide (M√©dia):** O centr√≥ide, representado por $\mu$, √© um vetor que indica o centro da distribui√ß√£o. Em um espa√ßo de *features* de $p$ dimens√µes, o centr√≥ide √© um vetor de $p$ componentes. Formalmente, o centr√≥ide √© a m√©dia de todos os pontos que pertencem a essa gaussiana.
2.  **Matriz de Covari√¢ncia:** A matriz de covari√¢ncia, representada por $\Sigma$, descreve a forma e a orienta√ß√£o da distribui√ß√£o. Ela indica a variabilidade dos dados em cada dimens√£o e as correla√ß√µes entre diferentes dimens√µes. A matriz de covari√¢ncia √© uma matriz quadrada de dimens√£o $p \times p$, onde $p$ √© o n√∫mero de *features*. Os elementos da diagonal da matriz representam as vari√¢ncias de cada *feature*, enquanto os elementos fora da diagonal representam as covari√¢ncias entre as *features*. Uma matriz de covari√¢ncia diagonal significa que as *features* n√£o s√£o correlacionadas.

```mermaid
graph LR
    subgraph "Gaussian Parameters"
        direction TB
        A["Gaussian Distribution"] --> B["Centroid (Mean): Œº"]
        A --> C["Covariance Matrix: Œ£"]
        B --> D["Center of Distribution"]
        C --> E["Shape and Orientation"]
        C --> F["Variances along diagonal"]
        C --> G["Covariances off diagonal"]
    end
```

Esses par√¢metros (centr√≥ide e matriz de covari√¢ncia) atuam como prot√≥tipos que representam uma regi√£o do espa√ßo de *features* com alta densidade de dados. Ao utilizar misturas gaussianas, cada componente gaussiana possui seu pr√≥prio centr√≥ide e matriz de covari√¢ncia, permitindo modelar distribui√ß√µes mais complexas e multimodais.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos dados bidimensionais ($p=2$) representando duas caracter√≠sticas de flores (comprimento e largura da p√©tala). Para uma classe espec√≠fica de flores, podemos ter um centr√≥ide (m√©dia) e uma matriz de covari√¢ncia estimados a partir dos dados:
>
> Centr√≥ide (m√©dia): $\mu = \begin{bmatrix} 5.0 \\ 3.5 \end{bmatrix}$
>
> Matriz de Covari√¢ncia: $\Sigma = \begin{bmatrix} 0.6 & 0.2 \\ 0.2 & 0.4 \end{bmatrix}$
>
> Aqui, o centr√≥ide indica que o comprimento m√©dio da p√©tala √© 5.0 e a largura m√©dia √© 3.5. A matriz de covari√¢ncia nos diz que h√° uma vari√¢ncia de 0.6 no comprimento da p√©tala, 0.4 na largura da p√©tala e uma covari√¢ncia positiva de 0.2 entre as duas, indicando que, em geral, flores com p√©talas mais longas tamb√©m tendem a ter p√©talas mais largas. Se a covari√¢ncia fosse zero, significaria que o comprimento e a largura da p√©tala n√£o est√£o linearmente correlacionados.
>
> Se tiv√©ssemos uma matriz de covari√¢ncia diagonal, como $\Sigma = \begin{bmatrix} 0.6 & 0 \\ 0 & 0.4 \end{bmatrix}$, isso indicaria que as caracter√≠sticas de comprimento e largura s√£o independentes. Uma matriz de covari√¢ncia esf√©rica, como $\Sigma = \begin{bmatrix} 0.5 & 0 \\ 0 & 0.5 \end{bmatrix}$, indicaria que a variabilidade em todas as dire√ß√µes √© a mesma.

**Lemma 60:** O centr√≥ide e a matriz de covari√¢ncia de uma distribui√ß√£o gaussiana capturam a localiza√ß√£o, forma, orienta√ß√£o e variabilidade dos dados no espa√ßo de *features*.
*Prova*: A m√©dia define a posi√ß√£o central, a vari√¢ncia define a dispers√£o e a covari√¢ncia a rela√ß√£o linear entre as vari√°veis. $\blacksquare$

**Corol√°rio 60:** Em GMMs, os prot√≥tipos de cada componente gaussiana s√£o definidos por seus par√¢metros (m√©dia e matriz de covari√¢ncia).

> ‚ö†Ô∏è **Nota Importante**:  O centr√≥ide e a matriz de covari√¢ncia s√£o os par√¢metros que definem a forma de uma distribui√ß√£o gaussiana, e s√£o utilizados como prot√≥tipos no contexto da modelagem por misturas gaussianas.

> ‚ùó **Ponto de Aten√ß√£o**: A matriz de covari√¢ncia √© crucial para modelar as rela√ß√µes entre as *features*, e sua escolha (diagonal, cheia ou esf√©rica) afeta a flexibilidade do modelo e o n√∫mero de par√¢metros a serem estimados.

### Ajuste de GMMs com o Algoritmo Expectation-Maximization (EM)

O ajuste dos par√¢metros de uma **mistura gaussiana (GMM)**, que s√£o os centr√≥ides e as matrizes de covari√¢ncia de cada componente gaussiana, √© realizado pelo **algoritmo Expectation-Maximization (EM)** [^13.2.3]. O algoritmo EM √© um m√©todo iterativo que busca maximizar a verossimilhan√ßa dos dados sob o modelo de mistura gaussiana.

```mermaid
graph LR
    subgraph "EM Algorithm for GMM"
        direction TB
        A["Initialize GMM Parameters (Œº, Œ£, œÄ)"] --> B["E-step: Compute Probabilities"]
        B --> C["Compute P(z·µ¢=j | x·µ¢)"]
        C --> D["M-step: Update Parameters"]
        D --> E["Update Œº‚±º, Œ£‚±º, œÄ‚±º using P(z·µ¢=j | x·µ¢)"]
        E --> F["Check Convergence"]
        F -- "Not Converged" --> B
        F -- "Converged" --> G["Final GMM Parameters"]
    end
```

O algoritmo EM alterna entre duas etapas:

1.  **Expectation (E-step):** Nesta etapa, calcula-se a probabilidade de cada ponto de dados pertencer a cada componente gaussiana, dadas as estimativas atuais dos par√¢metros das gaussianas. Essas probabilidades s√£o calculadas utilizando a densidade gaussiana e a mistura de probabilidades *a priori*. Formalmente, calculam-se as probabilidades *a posteriori* de pertin√™ncia de um dado $x_i$ a cada uma das componentes gaussianas.
2.  **Maximization (M-step):** Nesta etapa, os par√¢metros de cada componente gaussiana (m√©dia e matriz de covari√¢ncia) s√£o atualizados para maximizar a verossimilhan√ßa dos dados dados as probabilidades calculadas no passo E. Os novos par√¢metros s√£o calculados com base em uma vers√£o ponderada dos dados de acordo com as probabilidades obtidas no passo E.

As etapas E e M s√£o repetidas iterativamente at√© que a verossimilhan√ßa dos dados convirja ou at√© que um n√∫mero m√°ximo de itera√ß√µes seja atingido. Ao final da itera√ß√£o do EM, temos os par√¢metros de cada componente gaussiana, que representam os prot√≥tipos para o modelo de mistura.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simplificado com dois pontos de dados $x_1 = [1, 2]$ e $x_2 = [5, 6]$ e duas componentes gaussianas. Inicialmente, podemos supor que os par√¢metros das gaussianas s√£o:
>
> Componente 1: $\mu_1 = [0, 0]$, $\Sigma_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$, $\pi_1 = 0.5$
>
> Componente 2: $\mu_2 = [4, 4]$, $\Sigma_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$, $\pi_2 = 0.5$
>
> **E-step (Primeira itera√ß√£o):**
> Calculamos a probabilidade de cada ponto pertencer a cada componente usando a densidade gaussiana e as probabilidades a priori. Vamos supor que a densidade gaussiana $\mathcal{N}(x; \mu, \Sigma)$ para um ponto $x$ √© calculada usando a f√≥rmula:
>
> $\mathcal{N}(x; \mu, \Sigma) = \frac{1}{\sqrt{(2\pi)^p |\Sigma|}} \exp\left(-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu)\right)$
>
> onde $p$ √© a dimens√£o dos dados (2 neste caso) e $|\Sigma|$ √© o determinante da matriz de covari√¢ncia.
>
> Vamos calcular as probabilidades n√£o normalizadas para $x_1$:
>
> $\mathcal{N}(x_1; \mu_1, \Sigma_1) \approx 0.013$
> $\mathcal{N}(x_1; \mu_2, \Sigma_2) \approx 0.00000016$
>
> E para $x_2$:
>
> $\mathcal{N}(x_2; \mu_1, \Sigma_1) \approx 0.00000016$
> $\mathcal{N}(x_2; \mu_2, \Sigma_2) \approx 0.013$
>
> As probabilidades a posteriori (normalizadas) s√£o:
>
> $P(z_1 = 1 | x_1) = \frac{0.5 * 0.013}{0.5 * 0.013 + 0.5 * 0.00000016} \approx 1.0$
> $P(z_1 = 2 | x_1) = \frac{0.5 * 0.00000016}{0.5 * 0.013 + 0.5 * 0.00000016} \approx 0.0$
>
> $P(z_2 = 1 | x_2) = \frac{0.5 * 0.00000016}{0.5 * 0.00000016 + 0.5 * 0.013} \approx 0.0$
> $P(z_2 = 2 | x_2) = \frac{0.5 * 0.013}{0.5 * 0.00000016 + 0.5 * 0.013} \approx 1.0$
>
>
> **M-step (Primeira itera√ß√£o):**
>
> Os novos par√¢metros s√£o calculados usando as probabilidades a posteriori como pesos. Por exemplo, as novas m√©dias seriam:
>
> $\mu_1^{new} = \frac{1.0 * [1, 2] + 0.0 * [5, 6]}{1.0 + 0.0} = [1, 2]$
> $\mu_2^{new} = \frac{0.0 * [1, 2] + 1.0 * [5, 6]}{0.0 + 1.0} = [5, 6]$
>
> As matrizes de covari√¢ncia e as probabilidades *a priori* tamb√©m s√£o atualizadas de forma semelhante.
>
> As etapas E e M s√£o repetidas at√© a converg√™ncia dos par√¢metros. Nesse caso, ap√≥s algumas itera√ß√µes, os par√¢metros convergiriam para valores que ajustam bem as gaussianas aos dados.

**Lemma 61:** O algoritmo EM busca maximizar a verossimilhan√ßa dos dados dado o modelo de mistura gaussiana, ajustando os centr√≥ides e matrizes de covari√¢ncia iterativamente at√© a converg√™ncia.
*Prova*: As etapas E e M garantem a converg√™ncia para um m√°ximo local da fun√ß√£o de verossimilhan√ßa. $\blacksquare$

**Corol√°rio 61:** O algoritmo EM √© uma ferramenta √∫til para estimar os par√¢metros de modelos probabil√≠sticos em que as vari√°veis latentes (as componentes da gaussiana para cada ponto) n√£o s√£o observadas, sendo √∫til em diversas aplica√ß√µes de aprendizado de m√°quina.

> ‚ö†Ô∏è **Nota Importante**:  O algoritmo EM √© um m√©todo iterativo para ajustar as componentes gaussianas do GMM, e permite estimar a m√©dia e a matriz de covari√¢ncia de cada componente com base nos dados de treinamento.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha da inicializa√ß√£o dos par√¢metros do modelo, assim como o n√∫mero de componentes gaussianas, podem afetar a converg√™ncia do algoritmo EM, e devem ser tratadas com m√©todos espec√≠ficos de inicializa√ß√£o e sele√ß√£o de modelo.

### Classifica√ß√£o com GMMs: Probabilidades *a Posteriori*

Em modelos de **misturas gaussianas (GMMs)**, a classifica√ß√£o de novos pontos √© realizada com base nas **probabilidades *a posteriori***, que medem a probabilidade de cada ponto de dados pertencer a cada componente gaussiana [^13.2.3].

```mermaid
graph LR
    subgraph "GMM Classification"
        direction TB
        A["Input Data Point: x·µ¢"] --> B["Compute P(z·µ¢=j | x·µ¢) for each Gaussian component j"]
        B --> C["P(z·µ¢=j | x·µ¢) =  (œÄ‚±º * N(x·µ¢; Œº‚±º, Œ£‚±º)) / Œ£‚Çñ (œÄ‚Çñ * N(x·µ¢; Œº‚Çñ, Œ£‚Çñ))"]
        C --> D["Select Component with Maximum P(z·µ¢=j | x·µ¢)"]
        D --> E["Assign x·µ¢ to the selected component's class"]
    end
```

Em vez de classificar um ponto atribuindo-o ao prot√≥tipo mais pr√≥ximo, como no K-Means e LVQ, o ponto √© atribu√≠do √† classe da componente gaussiana que produz a maior probabilidade *a posteriori*. Formalmente, a probabilidade *a posteriori* de um ponto $x_i$ pertencer √† componente $j$ de uma mistura gaussiana com $K$ componentes √© dada por:

$$P(z_i = j | x_i) = \frac{\pi_j \mathcal{N}(x_i; \mu_j, \Sigma_j)}{\sum_{k=1}^K \pi_k \mathcal{N}(x_i; \mu_k, \Sigma_k)}$$

onde $z_i$ √© a componente gaussiana a qual o ponto $x_i$ pertence, $\pi_j$ √© a probabilidade *a priori* da componente $j$, $\mathcal{N}(x_i; \mu_j, \Sigma_j)$ √© a densidade gaussiana do ponto $x_i$ em rela√ß√£o √† componente $j$ com m√©dia $\mu_j$ e covari√¢ncia $\Sigma_j$.

Ap√≥s calcular as probabilidades *a posteriori* de pertencer a cada componente gaussiana, o ponto √© atribu√≠do √† classe da componente que tem a maior probabilidade *a posteriori*. Esta abordagem probabil√≠stica para classifica√ß√£o √© mais robusta do que a atribui√ß√£o baseada na dist√¢ncia ao prot√≥tipo mais pr√≥ximo.

> üí° **Exemplo Num√©rico:**
>
> Suponha que, ap√≥s treinar um GMM com duas componentes, temos os seguintes par√¢metros:
>
> Componente 1: $\mu_1 = [1, 1]$, $\Sigma_1 = \begin{bmatrix} 0.5 & 0 \\ 0 & 0.5 \end{bmatrix}$, $\pi_1 = 0.6$
>
> Componente 2: $\mu_2 = [5, 5]$, $\Sigma_2 = \begin{bmatrix} 0.8 & 0 \\ 0 & 0.8 \end{bmatrix}$, $\pi_2 = 0.4$
>
> Agora, queremos classificar um novo ponto $x = [2, 2]$. Calculamos a densidade gaussiana para cada componente:
>
> $\mathcal{N}(x; \mu_1, \Sigma_1) \approx 0.25$
> $\mathcal{N}(x; \mu_2, \Sigma_2) \approx 0.0005$
>
> As probabilidades *a posteriori* s√£o:
>
> $P(z = 1 | x) = \frac{0.6 * 0.25}{0.6 * 0.25 + 0.4 * 0.0005} \approx 0.997$
> $P(z = 2 | x) = \frac{0.4 * 0.0005}{0.6 * 0.25 + 0.4 * 0.0005} \approx 0.003$
>
> O ponto $x = [2, 2]$ seria classificado como pertencente √† Componente 1, pois tem uma probabilidade *a posteriori* muito maior de pertencer a essa componente (0.997) do que √† Componente 2 (0.003).

**Lemma 62:** A classifica√ß√£o de novos pontos usando GMMs √© realizada com base nas probabilidades *a posteriori* de pertencer a cada componente gaussiana, o que garante que a classifica√ß√£o seja feita com base na distribui√ß√£o de probabilidade estimada pelos par√¢metros das gaussianas.
*Prova*: A probabilidade *a posteriori* de um ponto pertencer a uma componente gaussiana √© proporcional √† verossimilhan√ßa daquele ponto dada a distribui√ß√£o gaussiana, o que reflete a ader√™ncia do ponto √†quela distribui√ß√£o. $\blacksquare$

**Corol√°rio 62:** A abordagem probabil√≠stica das GMMs permite que a classifica√ß√£o leve em considera√ß√£o a incerteza na aloca√ß√£o de um ponto a cada componente, sendo mais robusta do que a classifica√ß√£o por dist√¢ncia.

> ‚ö†Ô∏è **Nota Importante**:  As GMMs utilizam uma abordagem probabil√≠stica para a classifica√ß√£o, atribuindo novos pontos √† classe com maior probabilidade *a posteriori*, em vez de atribuir √† classe do prot√≥tipo mais pr√≥ximo.

> ‚ùó **Ponto de Aten√ß√£o**:  A abordagem probabil√≠stica das GMMs pode levar a fronteiras de decis√£o mais suaves e melhor capacidade de generaliza√ß√£o, quando comparadas a modelos que utilizam dist√¢ncias diretas.

### Conclus√£o

A modelagem de classes com distribui√ß√µes gaussianas, atrav√©s de GMMs, oferece uma abordagem flex√≠vel para a classifica√ß√£o, em que os prot√≥tipos s√£o representados pelas m√©dias e matrizes de covari√¢ncia das gaussianas. O algoritmo EM √© utilizado para ajustar os par√¢metros dos prot√≥tipos, e a classifica√ß√£o √© feita com base nas probabilidades *a posteriori* de cada ponto pertencer a cada gaussiana, resultando em uma representa√ß√£o mais precisa das distribui√ß√µes e um procedimento de classifica√ß√£o probabil√≠stico e mais robusto. Essa abordagem contrasta com modelos de prot√≥tipos que utilizam dist√¢ncias diretas e oferece uma alternativa com bom desempenho quando as distribui√ß√µes dos dados s√£o complexas ou multimodais.

### Footnotes

[^13.2.3]: "The Gaussian mixture model can also be thought of as a prototype method, similar in spirit to K-means and LVQ. We discuss Gaussian mixtures in some detail in Sections 6.8, 8.5 and 12.7. Each cluster is described in terms of a Gaussian density, which has a centroid (as in K-means), and a covariance matrix. The comparison becomes crisper if we restrict the component Gaussians to have a scalar covariance matrix (Exercise 13.1)...Similarly, when Gaussian mixture models are used to represent the feature density in each class, it produces smooth posterior probabilities." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
