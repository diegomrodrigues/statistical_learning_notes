## M√©todos *Model-Free*: Efic√°cia e Aplica√ß√µes como Caixas-Pretas

```mermaid
graph LR
    subgraph "Model-Free Methods"
    A["Data"] --> B["k-NN"]
    A --> C["Prototype Methods"]
    C --> D["K-Means"]
    C --> E["LVQ"]
    C --> F["Gaussian Mixtures"]
    B --> G["Classification"]
    D --> G
    E --> G
    F --> G
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Este cap√≠tulo aprofunda a discuss√£o sobre m√©todos de classifica√ß√£o e reconhecimento de padr√µes que s√£o, em ess√™ncia, *model-free*, com um foco espec√≠fico em sua efic√°cia em cen√°rios do mundo real [^13.1]. Ao contr√°rio de modelos lineares que buscam rela√ß√µes interpret√°veis, as t√©cnicas *model-free* s√£o frequentemente utilizadas como "caixas-pretas" (*black boxes*), priorizando a precis√£o preditiva em detrimento da interpreta√ß√£o da rela√ß√£o entre as *features* e os resultados [^13.1]. Esta se√ß√£o analisa porque estas t√©cnicas, apesar de sua simplicidade conceitual, frequentemente se destacam em problemas complexos, incluindo aqueles com alta dimensionalidade e n√£o linearidade. Exploraremos tanto as abordagens de prot√≥tipos (K-means, LVQ, Misturas Gaussianas) quanto as de k-vizinhos mais pr√≥ximos (k-NN) para entender suas capacidades e limita√ß√µes [^13.1].

### M√©todos de Prot√≥tipos: K-Means, LVQ e Misturas Gaussianas

Os m√©todos de prot√≥tipos, que incluem **K-Means**, **Learning Vector Quantization (LVQ)** e **Misturas Gaussianas**, s√£o t√©cnicas poderosas para classifica√ß√£o e reconhecimento de padr√µes devido √† sua capacidade de representar a distribui√ß√£o dos dados por um conjunto de pontos representativos, os prot√≥tipos.

**K-Means Clustering:** O K-Means √© um algoritmo de *clustering* que visa particionar os dados em $R$ grupos (clusters), onde $R$ √© um hiperpar√¢metro a ser definido. O algoritmo busca encontrar os centros de cada cluster, de forma a minimizar a vari√¢ncia intra-cluster [^13.2]. A cada itera√ß√£o, o K-means alterna entre a atribui√ß√£o de pontos ao cluster mais pr√≥ximo e a atualiza√ß√£o do centroide de cada cluster [^13.2]. Embora o K-means seja tradicionalmente usado para *clustering* n√£o supervisionado, ele pode ser aplicado √† classifica√ß√£o de dados rotulados ao aplicar o algoritmo separadamente a cada classe, e usando os centros encontrados como prot√≥tipos [^13.2].

```mermaid
graph LR
    subgraph "K-Means Algorithm"
        direction TB
        A["Initialize R centroids"] --> B["Assign data points to nearest centroid"]
        B --> C["Update centroids based on cluster means"]
        C --> D["Check for convergence"]
        D -->|No| B
        D -->|Yes| E["Result: Cluster centroids"]
    end
```

> üí° **Exemplo Num√©rico:**
> Suponha que temos um conjunto de dados bidimensional com 6 pontos, representados pelas coordenadas: (1, 1), (1, 2), (2, 1), (5, 5), (6, 6), e (5, 6). Queremos usar o K-Means para criar 2 clusters (R=2).
>
> **Passo 1: Inicializa√ß√£o:** Escolhemos aleatoriamente dois pontos como centroides iniciais, por exemplo, (1, 1) e (6, 6).
>
> **Passo 2: Atribui√ß√£o:** Calculamos a dist√¢ncia Euclidiana de cada ponto aos centroides e atribu√≠mos cada ponto ao cluster do centroide mais pr√≥ximo.
>
>   - Ponto (1,1): Dist√¢ncia para (1,1) = 0, Dist√¢ncia para (6,6) = 7.07. Atribu√≠do ao Cluster 1.
>   - Ponto (1,2): Dist√¢ncia para (1,1) = 1, Dist√¢ncia para (6,6) = 6.71. Atribu√≠do ao Cluster 1.
>   - Ponto (2,1): Dist√¢ncia para (1,1) = 1, Dist√¢ncia para (6,6) = 6.40. Atribu√≠do ao Cluster 1.
>   - Ponto (5,5): Dist√¢ncia para (1,1) = 5.66, Dist√¢ncia para (6,6) = 1.41. Atribu√≠do ao Cluster 2.
>   - Ponto (6,6): Dist√¢ncia para (1,1) = 7.07, Dist√¢ncia para (6,6) = 0. Atribu√≠do ao Cluster 2.
>   - Ponto (5,6): Dist√¢ncia para (1,1) = 6.40, Dist√¢ncia para (6,6) = 1. Atribu√≠do ao Cluster 2.
>
> **Passo 3: Atualiza√ß√£o:** Calculamos os novos centroides como a m√©dia dos pontos em cada cluster.
>
>   - Novo centroide do Cluster 1: ((1+1+2)/3, (1+2+1)/3) = (1.33, 1.33)
>   - Novo centroide do Cluster 2: ((5+6+5)/3, (5+6+6)/3) = (5.33, 5.67)
>
> **Passo 4: Repeti√ß√£o:** Repetimos os passos 2 e 3 at√© que os centroides n√£o mudem significativamente ou um n√∫mero m√°ximo de itera√ß√µes seja atingido. Ap√≥s algumas itera√ß√µes, os centroides convergem para aproximadamente (1.33, 1.33) e (5.33, 5.67), com os pontos (1,1), (1,2), e (2,1) no primeiro cluster e (5,5), (6,6), e (5,6) no segundo.

**Lemma 5:** O algoritmo K-means converge para um m√≠nimo local da fun√ß√£o de custo, que √© definida como a soma das dist√¢ncias quadr√°ticas de cada ponto ao centroide de seu cluster.
*Prova*: A cada itera√ß√£o, o K-means garante que a fun√ß√£o de custo diminua: na fase de atribui√ß√£o, cada ponto √© movido para o centroide mais pr√≥ximo, e na fase de atualiza√ß√£o, os centroides s√£o movidos para a m√©dia dos pontos a eles atribu√≠dos, o que tamb√©m diminui a vari√¢ncia intra-cluster. Como a fun√ß√£o de custo √© limitada e decrescente a cada itera√ß√£o, o algoritmo converge para um m√≠nimo local [^13.2]. $\blacksquare$

```mermaid
graph LR
    subgraph "K-Means Convergence"
    A["Cost function: Sum of squared distances"] --> B["Assignment Step: points move to nearest centroid"]
    B --> C["Update Step: centroids move to mean of assigned points"]
    C --> D["Monotonic decrease in cost function"]
    D --> E["Convergence to local minimum"]
    end
```

**Learning Vector Quantization (LVQ):** O LVQ, inspirado no K-Means, √© um m√©todo de aprendizado supervisionado que ajusta prot√≥tipos de acordo com a classe de cada ponto de treino. O LVQ inicializa um conjunto de prot√≥tipos (geralmente usando uma solu√ß√£o K-means) para cada classe e, a cada itera√ß√£o, move os prot√≥tipos da classe correta na dire√ß√£o de cada ponto de treino, e move os prot√≥tipos das classes erradas na dire√ß√£o oposta [^13.2]. O LVQ tende a afastar os prot√≥tipos das fronteiras de decis√£o, o que melhora o desempenho da classifica√ß√£o [^13.2].

```mermaid
graph LR
    subgraph "LVQ Algorithm"
        direction TB
        A["Initialize prototypes (K-Means)"] --> B["Find nearest prototype to training point"]
        B --> C["Move correct prototype towards training point"]
        C --> D["Move incorrect prototypes away from training point"]
    end
```

> üí° **Exemplo Num√©rico:**
> Imagine que temos duas classes (A e B) e dois prot√≥tipos por classe, inicializados em (1,1) e (2,2) para a classe A e (5,5) e (6,6) para a classe B. Temos um ponto de treino (1.5, 1.5) da classe A. A taxa de aprendizagem inicial √© $\epsilon = 0.1$.
>
> **Passo 1: Encontrar o prot√≥tipo mais pr√≥ximo:** O prot√≥tipo mais pr√≥ximo a (1.5, 1.5) √© (1,1) da classe A.
>
> **Passo 2: Atualizar o prot√≥tipo correto:** O prot√≥tipo (1,1) √© movido na dire√ß√£o do ponto de treino (1.5, 1.5):
>
>   - Novo prot√≥tipo: (1,1) + $\epsilon$ * ((1.5, 1.5) - (1,1)) = (1,1) + 0.1 * (0.5, 0.5) = (1.05, 1.05).
>
> **Passo 3: Atualizar prot√≥tipos incorretos:** Os prot√≥tipos da classe B (5,5) e (6,6) s√£o movidos na dire√ß√£o oposta ao ponto de treino:
>
>  - Novo prot√≥tipo (5,5): (5,5) - $\epsilon$ * ((1.5, 1.5) - (5,5)) = (5,5) - 0.1 * (-3.5, -3.5) = (5.35, 5.35)
>  - Novo prot√≥tipo (6,6): (6,6) - $\epsilon$ * ((1.5, 1.5) - (6,6)) = (6,6) - 0.1 * (-4.5, -4.5) = (6.45, 6.45)
>
> A cada itera√ß√£o, a taxa de aprendizagem $\epsilon$ decresce, e os prot√≥tipos convergem para posi√ß√µes que melhor separam as classes.

**Corol√°rio 5:** A taxa de aprendizagem (learning rate) do LVQ, $\epsilon$, deve ser decrescente com o tempo para garantir a converg√™ncia do algoritmo. O valor inicial e a velocidade de decaimento da taxa de aprendizagem s√£o importantes para o desempenho do modelo [^13.2].

**Misturas Gaussianas:** As Misturas Gaussianas (Gaussian Mixture Models - GMMs) modelam a distribui√ß√£o dos dados como uma combina√ß√£o linear de densidades Gaussianas. Cada componente Gaussiana representa um cluster, e cada cluster √© caracterizado por um centroide e uma matriz de covari√¢ncia [^13.2.3]. A classifica√ß√£o de um novo ponto √© feita atribuindo-o √† componente Gaussiana que produz a maior probabilidade *a posteriori* [^13.2.3]. O ajuste dos par√¢metros de uma GMM √© feito usando o algoritmo Expectation-Maximization (EM), que alterna entre a atribui√ß√£o de pesos a cada ponto para cada cluster (E-step) e a atualiza√ß√£o dos par√¢metros dos clusters (M-step) [^13.2.3].

```mermaid
graph LR
    subgraph "Gaussian Mixture Model"
    A["Data Distribution"] --> B["Mixture of Gaussian components"]
    B --> C["Each component: mean and covariance matrix"]
    C --> D["Classification: highest posterior probability"]
    end
```

```mermaid
graph LR
    subgraph "Expectation-Maximization (EM) Algorithm"
        direction TB
        A["Initialize Gaussian parameters"] --> B["E-step: Assign weights to data points for each component"]
        B --> C["M-step: Update Gaussian parameters using weighted data points"]
        C --> D["Check for convergence"]
         D -->|No| B
        D -->|Yes| E["Result: optimized GMM parameters"]
    end
```

> üí° **Exemplo Num√©rico:**
> Suponha que temos um conjunto de dados com duas classes, e modelamos cada classe com uma Gaussiana. Inicializamos duas Gaussianas com m√©dias $\mu_1 = [1, 1]$ e $\mu_2 = [5, 5]$ e matrizes de covari√¢ncia $\Sigma_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ e $\Sigma_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.
>
> **Passo 1: Expectation (E-step):** Para cada ponto de dado $x$, calculamos a probabilidade de que ele perten√ßa a cada Gaussiana (usando as densidades gaussianas). Por exemplo, para um ponto $x = [2,2]$, calculamos:
>
>   - $p(x|\mu_1, \Sigma_1) = \frac{1}{2\pi \sqrt{|\Sigma_1|}} \exp(-\frac{1}{2}(x-\mu_1)^T \Sigma_1^{-1}(x-\mu_1)) \approx 0.15$
>   - $p(x|\mu_2, \Sigma_2) = \frac{1}{2\pi \sqrt{|\Sigma_2|}} \exp(-\frac{1}{2}(x-\mu_2)^T \Sigma_2^{-1}(x-\mu_2)) \approx 0.00002$
>
>   Normalizamos as probabilidades para obter as probabilidades a posteriori, por exemplo, $P(cluster=1|x) = \frac{0.15}{0.15+0.00002} \approx 1$.
>
> **Passo 2: Maximization (M-step):** Atualizamos as m√©dias e covari√¢ncias de cada Gaussiana, usando os pesos calculados no passo E.
>
>   - Nova m√©dia $\mu_1$: A m√©dia dos pontos ponderada pela sua probabilidade de pertencer ao cluster 1.
>   - Nova covari√¢ncia $\Sigma_1$: A covari√¢ncia dos pontos ponderada pela sua probabilidade de pertencer ao cluster 1.
>
> Repetimos os passos E e M at√© que os par√¢metros das Gaussianas convirjam.

> ‚ö†Ô∏è **Nota Importante**: A escolha do n√∫mero de prot√≥tipos (K no K-Means, R no LVQ, n√∫mero de componentes em GMMs) influencia significativamente a capacidade do modelo de aproximar a distribui√ß√£o dos dados. A valida√ß√£o cruzada √© uma t√©cnica importante para escolher o valor adequado [^13.3.1].

> ‚ùó **Ponto de Aten√ß√£o**: As Misturas Gaussianas podem fornecer fronteiras de decis√£o mais suaves do que o K-means, mas a escolha do n√∫mero de componentes (prot√≥tipos) tamb√©m √© cr√≠tica para o bom desempenho do modelo [^13.2.3].

> ‚úîÔ∏è **Destaque**: O LVQ tende a posicionar os prot√≥tipos de forma mais estrat√©gica em rela√ß√£o √†s fronteiras de decis√£o, o que frequentemente resulta em maior acur√°cia do que o K-means [^13.2.2].

### k-Nearest Neighbors (k-NN): Efic√°cia e Limita√ß√µes

O m√©todo de **k-Nearest Neighbors (k-NN)** √© um classificador *memory-based*, isto √©, n√£o requer um modelo a ser ajustado [^13.3]. Em vez disso, para classificar um novo ponto, o algoritmo busca os $k$ pontos de treino mais pr√≥ximos a ele (usando alguma m√©trica de dist√¢ncia, geralmente a dist√¢ncia Euclidiana) e classifica o novo ponto de acordo com a classe majorit√°ria entre esses vizinhos [^13.3].

O k-NN √© eficaz porque se baseia na premissa de que pontos pr√≥ximos no espa√ßo de *features* tendem a pertencer √† mesma classe. A escolha do valor de $k$ influencia o comportamento do classificador: valores pequenos de $k$ tornam o modelo mais sens√≠vel ao ru√≠do, enquanto valores grandes tendem a suavizar as fronteiras de decis√£o [^13.3].

```mermaid
graph LR
    subgraph "k-Nearest Neighbors (k-NN)"
    A["New Data Point"] --> B["Find k nearest neighbors in training data"]
    B --> C["Classify based on majority class among neighbors"]
    end
```

> üí° **Exemplo Num√©rico:**
> Considere um conjunto de dados com 10 pontos, onde 5 pertencem √† classe A e 5 √† classe B. As coordenadas dos pontos s√£o:
>
> Classe A: (1,1), (1,2), (2,1), (2,2), (1.5, 1.5)
> Classe B: (5,5), (5,6), (6,5), (6,6), (5.5, 5.5)
>
> Queremos classificar um novo ponto (3,3) usando k-NN com k=3.
>
> **Passo 1: Calcular as dist√¢ncias:** Calculamos a dist√¢ncia Euclidiana entre o ponto (3,3) e todos os pontos de treino.
>
>   - Dist√¢ncias para os pontos da Classe A: 2.83, 2.24, 2.24, 1.41, 2.12
>   - Dist√¢ncias para os pontos da Classe B: 2.83, 3.16, 3.16, 4.24, 3.54
>
> **Passo 2: Encontrar os k vizinhos mais pr√≥ximos:** Os 3 vizinhos mais pr√≥ximos s√£o (2,2), (1.5, 1.5), e (2,1), todos da classe A.
>
> **Passo 3: Classificar:** Como os 3 vizinhos mais pr√≥ximos s√£o da classe A, classificamos o ponto (3,3) como da classe A.
>
> Se us√°ssemos k=5, os 5 vizinhos mais pr√≥ximos seriam (2,2), (1.5, 1.5), (2,1), (1,2) e (1,1), todos da classe A, e o resultado seria o mesmo.

**Lemma 6:** O erro do classificador de 1-vizinho mais pr√≥ximo √© assintoticamente limitado a duas vezes o erro de Bayes, isto √©, o erro m√≠nimo poss√≠vel.
*Prova*: A prova deste resultado, inicialmente apresentado por Cover e Hart (1967), assume que o ponto de consulta coincide com um ponto de treino (condi√ß√£o assint√≥tica). Nesse caso, o erro do 1-NN √© a probabilidade de que o vizinho mais pr√≥ximo tenha uma classe diferente, que √©, em m√©dia, duas vezes a probabilidade de erro de Bayes [^13.3]. $\blacksquare$

```mermaid
graph LR
    subgraph "1-NN Error Limit"
    A["1-NN error probability"] --> B["Assumes query point coincides with training point"]
    B --> C["Error is probability of different class in nearest neighbor"]
    C --> D["Asymptotically, 1-NN error <= 2 * Bayes error"]
    end
```

Embora o k-NN seja eficaz em muitos problemas, ele tem algumas limita√ß√µes. Primeiro, a computa√ß√£o das dist√¢ncias entre um novo ponto e todos os pontos de treino pode ser custosa em conjuntos de dados grandes e de alta dimensionalidade. Segundo, o k-NN √© suscet√≠vel √† maldi√ß√£o da dimensionalidade, onde a densidade dos dados diminui com o aumento da dimens√£o, fazendo com que os vizinhos mais pr√≥ximos sejam menos representativos da distribui√ß√£o local [^13.4].

**Corol√°rio 6:** M√©todos de redu√ß√£o de dimensionalidade podem ser combinados com o k-NN para mitigar os efeitos da maldi√ß√£o da dimensionalidade. A sele√ß√£o de *features* ou a proje√ß√£o dos dados em um subespa√ßo de baixa dimens√£o podem melhorar o desempenho do k-NN em problemas de alta dimensionalidade.

> ‚ö†Ô∏è **Ponto Crucial**:  O valor de k (n√∫mero de vizinhos) tem um impacto significativo no desempenho do k-NN. A escolha de k deve ser feita via valida√ß√£o cruzada, de forma a minimizar o erro de classifica√ß√£o [^13.3].

> ‚ùó **Ponto de Aten√ß√£o**:  Em problemas com alta dimensionalidade, a dist√¢ncia Euclidiana pode se tornar menos informativa devido √† maldi√ß√£o da dimensionalidade. M√©tricas de dist√¢ncia adaptativas ou redu√ß√£o de dimensionalidade podem ser necess√°rias [^13.4].

> ‚úîÔ∏è **Destaque**:  Em problemas onde as fronteiras de decis√£o s√£o complexas e n√£o lineares, o k-NN pode superar modelos lineares tradicionais, pois ele n√£o assume uma forma espec√≠fica para a fronteira de decis√£o [^13.3].

### Casos de Sucesso: Aplica√ß√µes em Dados Reais

As t√©cnicas *model-free* t√™m sido aplicadas com sucesso em uma ampla variedade de problemas do mundo real. Um exemplo not√°vel √© a classifica√ß√£o de d√≠gitos manuscritos, onde o k-NN, junto com o uso de dist√¢ncias invariantes e tangentes, t√™m demonstrado alto desempenho, conforme reportado em experimentos do projeto STATLOG [^13.3.3], [^13.3.2]. Em outro estudo de caso, o k-NN alcan√ßou resultados superiores na classifica√ß√£o de cenas de imagens de sat√©lite, sendo capaz de identificar diferentes tipos de uso do solo a partir de imagens multiespectrais [^13.3.2].

Esses exemplos ilustram a capacidade das t√©cnicas *model-free* de lidar com dados complexos, onde a modelagem expl√≠cita pode ser dif√≠cil. Como "caixas pretas", esses m√©todos se adaptam √† estrutura dos dados sem impor suposi√ß√µes fortes, o que os torna particularmente √∫teis em situa√ß√µes com grande incerteza e complexidade.

### Pergunta Te√≥rica Avan√ßada: Demonstra√ß√£o formal da rela√ß√£o entre o erro do k-NN e o erro de Bayes
**Resposta:**
A rela√ß√£o entre o erro do k-NN e o erro de Bayes √© um resultado fundamental em aprendizado estat√≠stico. Considere um problema de classifica√ß√£o com $K$ classes, onde $p_k(x)$ √© a probabilidade condicional de que um ponto $x$ perten√ßa √† classe $k$. O erro de Bayes √© o erro m√≠nimo poss√≠vel, dado por $E_{Bayes} = 1 - \max_k p_k(x)$.

Para o k-NN, seja $C_k(x)$ o conjunto dos $k$ vizinhos mais pr√≥ximos de $x$ e seja $n_k$ o n√∫mero de vizinhos em $C_k(x)$ que pertencem √† classe $k$. O erro do k-NN √© dado por $E_{kNN}(x) = 1 - \frac{\max_k n_k}{k}$.

Se $k=1$, ent√£o $E_{1NN}(x)$ √© o erro do classificador 1-NN, que √© dado por $E_{1NN}(x) = \sum_{k=1}^K p_k(x)(1-p_k(x))$.
Para um problema de duas classes (K=2), seja $p(x) = p_1(x)$. O erro do 1-NN √© $E_{1NN}(x) = p(x)(1-p(x)) + (1-p(x))p(x) = 2p(x)(1-p(x))$. Enquanto o erro de Bayes √© $E_{Bayes} = 1 - \max\{p(x), 1-p(x)\}$.

Quando $p(x) = 0.5$, ent√£o $E_{Bayes} = 0.5$ e $E_{1NN}(x) = 0.5$. Quando $p(x) = 0$ ou $p(x)=1$, ent√£o $E_{Bayes} = 0$ e $E_{1NN}(x)=0$. Pode ser demonstrado que $E_{1NN}(x) \leq 2E_{Bayes}(x)$.
Quando k>1, o erro do k-NN tamb√©m converge para um erro limitado pelo dobro do erro de Bayes, com uma taxa de converg√™ncia que depende de $k$ e da estrutura dos dados [^13.3]. O resultado assint√≥tico assume que a distribui√ß√£o dos dados em torno de $x$ √© suave, e $k$ tende ao infinito enquanto $k/n \to 0$.

```mermaid
graph LR
    subgraph "k-NN and Bayes Error Relationship"
        direction TB
        A["Bayes Error: E_{Bayes} = 1 - max_k p_k(x)"]
        B["k-NN Error: E_{kNN}(x) = 1 - max_k (n_k/k)"]
        C["1-NN Error: E_{1NN}(x) = Œ£_k p_k(x)(1 - p_k(x))"]
        D["For two classes: E_{1NN}(x) = 2p(x)(1 - p(x))"]
        E["Asymptotically, E_{kNN}(x) <= 2 * E_{Bayes} (x)"]
    A --> B
    B --> C
    C --> D
    D --> E
    end
```

### Conclus√£o

Os m√©todos *model-free* oferecem uma abordagem alternativa para classifica√ß√£o e reconhecimento de padr√µes, especialmente eficazes como *black boxes* em problemas do mundo real. A flexibilidade dos m√©todos de prot√≥tipos e a simplicidade do k-NN permitem que eles se adaptem a complexidades que os modelos lineares tradicionais podem ter dificuldades em capturar. Embora a interpretabilidade seja sacrificada, o desempenho preditivo dessas t√©cnicas frequentemente justifica seu uso, especialmente quando se prioriza a precis√£o em detrimento da compreens√£o detalhada da rela√ß√£o entre *features* e resultados. A escolha do m√©todo mais adequado depende das caracter√≠sticas do problema, incluindo o tamanho do conjunto de dados, a dimensionalidade, a complexidade das fronteiras de decis√£o e a necessidade de interpretabilidade.

### Footnotes

[^13.1]: "In this chapter we discuss some simple and essentially model-free methods for classification and pattern recognition. Because they are highly unstructured, they typically are not useful for understanding the nature of the relationship between the features and class outcome. However, as black box prediction engines, they can be very effective, and are often among the best performers in real data problems." *(Trecho de  "13. Prototype Methods and Nearest-Neighbors")*

[^13.2]: "Throughout this chapter, our training data consists of the N pairs (x1,91),...,(xn, 9N) where gi is a class label taking values in {1, 2, . . ., K}. Prototype methods represent the training data by a set of points in feature space. These prototypes are typically not examples from the training sample, except in the case of 1-nearest-neighbor classification discussed later. Each prototype has an associated class label, and classification of a query point x is made to the class of the closest prototype. "Closest" is usually defined by Euclidean distance in the feature space, after each feature has been standardized to have overall mean 0 and variance 1 in the training sample." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.1]: "K-means clustering is a method for finding clusters and cluster centers in a set of unlabeled data. One chooses the desired number of cluster centers, say R, and the K-means procedure iteratively moves the centers to minimize the total within cluster variance. Given an initial set of centers, the K-means algorithm alternates the two steps: for each center we identify the subset of training points (its cluster) that is closer to it than any other center; the means of each feature for the data points in each cluster are computed, and this mean vector becomes the new center for that cluster." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.2]: "In this technique due to Kohonen (1989), prototypes are placed strategically with respect to the decision boundaries in an ad-hoc way. LVQ is an online algorithm-observations are processed one at a time. The idea is that the training points attract prototypes of the correct class, and repel other prototypes. When the iterations settle down, prototypes should be close to the training points in their class. The learning rate e is decreased to zero with each iteration, following the guidelines for stochastic approximation learning rates." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.3]: "The Gaussian mixture model can also be thought of as a prototype method, similar in spirit to K-means and LVQ. We discuss Gaussian mixtures in some detail in Sections 6.8, 8.5 and 12.7. Each cluster is described in terms of a Gaussian density, which has a centroid (as in K-means), and a covariance matrix. The comparison becomes crisper if we restrict the component Gaussians to have a scalar covariance matrix" * (Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3]: "These classifiers are memory-based, and require no model to be fit. Given a query point xo, we find the k training points x(r), r = 1,..., k closest in distance to xo, and then classify using majority vote among the k neighbors." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3.1]: "We tested the nearest-neighbors, K-means and LVQ classifiers on two simulated problems. There are 10 independent features Xj, each uniformly distributed on [0, 1]. The two-class 0-1 target variable is defined as follows: Y = I{X1 > 1/2}; problem 1: "easy", Y = I{sign [Œ†(Xj ‚àí 1/2) > 0]}; problem 2: "difficult." Figure 13.5 shows the mean and standard error of the misclassification error for nearest-neighbors, K-means and LVQ over ten realizations, as the tuning parameters are varied. We see that K-means and LVQ give nearly identical results. For the best choices of their tuning parameters, K-means and LVQ outperform nearest-neighbors for the first problem, and they perform similarly for the second problem. Notice that the best value of each tuning parameter is clearly situation dependent." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3.2]: "The STATLOG project (Michie et al., 1994) used part of a LANDSAT image as a benchmark for classification (82 √ó 100 pixels). Figure 13.6 shows four heat-map images...Five-nearest-neighbors produced the predicted map shown in the bottom right panel...For each pixel we extracted an 8-neighbor feature map-the pixel itself and its 8 immediate neighbors..." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3.3]: "In some problems, the training features are invariant under certain natural transformations. The nearest-neighbor classifier can exploit such invariances by incorporating them into the metric used to measure the distances between objects. Here we give an example where this idea was used with great success...The problem is handwritten digit recognition" * (Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.4]: "When nearest-neighbor classification is carried out in a high-dimensional feature space, the nearest neighbors of a point can be very far away, causing bias and degrading the performance of the rule...In general, this calls for adapting the metric used in nearest-neighbor classification..." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3]: "These classifiers are memory-based, and require no model to be fit. Given a query point xo, we find the k training points x(r), r = 1,..., k closest in distance to xo, and then classify using majority vote among the k neighbors." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
