## Vizinhos Mais Pr√≥ximos: Relev√¢ncia em Regress√£o e Classifica√ß√£o, e Limita√ß√µes da Dimensionalidade

```mermaid
graph LR
    subgraph "k-NN Application Contexts"
        direction LR
        A["Input Data: (x_i, y_i)"] --> B["Classification Task"]
        A --> C["Regression Task"]
        B --> D["k-NN: Majority Vote for Class"]
        C --> E["k-NN: Average of Neighbor Values"]
        D --> F["Class Label Prediction"]
        E --> G["Continuous Value Prediction"]
    end
    F --> H["Performance affected by Curse of Dimensionality"]
    G --> H
```

### Introdu√ß√£o

Este cap√≠tulo aprofunda a an√°lise do m√©todo dos **vizinhos mais pr√≥ximos (Nearest Neighbors - NN)**, explorando sua relev√¢ncia tanto em problemas de regress√£o quanto de classifica√ß√£o [^13.1]. Embora o foco principal deste cap√≠tulo tenha sido na classifica√ß√£o, a t√©cnica dos vizinhos mais pr√≥ximos tamb√©m pode ser utilizada para regress√£o, onde o objetivo √© prever um valor cont√≠nuo em vez de uma classe discreta. Abordaremos as diferen√ßas na aplica√ß√£o do m√©todo em cada contexto, bem como as limita√ß√µes que surgem em problemas de alta dimensionalidade, particularmente na regress√£o, devido √† maldi√ß√£o da dimensionalidade [^13.1]. A an√°lise incluir√° tamb√©m as adapta√ß√µes necess√°rias para mitigar os efeitos da alta dimensionalidade, permitindo que o m√©todo continue a ser uma ferramenta eficaz mesmo em espa√ßos de *features* complexos.

### Vizinhos Mais Pr√≥ximos em Regress√£o

Em problemas de regress√£o, o objetivo √© estimar uma vari√°vel dependente cont√≠nua $Y$ com base em um conjunto de vari√°veis independentes (features) $X$. O m√©todo dos **k-vizinhos mais pr√≥ximos (k-NN)** para regress√£o adapta a l√≥gica de proximidade utilizada na classifica√ß√£o para prever valores cont√≠nuos [^13.1]. A diferen√ßa principal reside na forma como as informa√ß√µes dos vizinhos s√£o agregadas. Enquanto na classifica√ß√£o utiliza-se a vota√ß√£o majorit√°ria para determinar a classe, na regress√£o, a m√©dia dos valores de $Y$ dos k vizinhos mais pr√≥ximos √© utilizada como previs√£o para o ponto de consulta $x_0$.

Formalmente, dado um conjunto de dados de treino $\{(x_i, y_i)\}_{i=1}^N$, onde $x_i$ √© um vetor de *features* e $y_i$ √© o valor da vari√°vel dependente associada, a previs√£o para um novo ponto $x_0$ usando o k-NN √© dada por:

$$\hat{y}(x_0) = \frac{1}{k} \sum_{x_i \in \mathcal{N}_k(x_0)} y_i$$

Onde $\mathcal{N}_k(x_0)$ denota o conjunto dos $k$ vizinhos mais pr√≥ximos de $x_0$ no espa√ßo de *features*. A escolha do valor de $k$ em regress√£o tem um efeito semelhante ao observado na classifica√ß√£o: valores pequenos de $k$ levam a previs√µes mais sens√≠veis ao ru√≠do local, enquanto valores grandes tendem a suavizar as previs√µes e reduzir a vari√¢ncia, mas podem aumentar o vi√©s [^13.1].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados de treinamento com os seguintes pontos $(x_i, y_i)$:
>
>  - (1, 2)
>  - (2, 3)
>  - (3, 4)
>  - (4, 6)
>  - (5, 5)
>
> Queremos prever o valor de $y$ para um novo ponto $x_0 = 3.5$ usando k-NN com $k=3$.
>
> 1. **Calculando as dist√¢ncias:**
>    - Dist√¢ncia entre $x_0 = 3.5$ e $x_1 = 1$: $|3.5 - 1| = 2.5$
>    - Dist√¢ncia entre $x_0 = 3.5$ e $x_2 = 2$: $|3.5 - 2| = 1.5$
>    - Dist√¢ncia entre $x_0 = 3.5$ e $x_3 = 3$: $|3.5 - 3| = 0.5$
>    - Dist√¢ncia entre $x_0 = 3.5$ e $x_4 = 4$: $|3.5 - 4| = 0.5$
>    - Dist√¢ncia entre $x_0 = 3.5$ e $x_5 = 5$: $|3.5 - 5| = 1.5$
>
> 2. **Encontrando os 3 vizinhos mais pr√≥ximos:**
>    - Os 3 vizinhos mais pr√≥ximos s√£o os pontos com $x = 3, x = 4$ e $x = 2$ (note que x=3 e x=4 tem a mesma dist√¢ncia, ent√£o escolhemos qualquer um dos dois). Os valores de $y$ correspondentes s√£o 4, 6 e 3.
>
> 3. **Calculando a m√©dia dos valores de $y$ dos vizinhos:**
>    - $\hat{y}(3.5) = \frac{4 + 6 + 3}{3} = \frac{13}{3} \approx 4.33$
>
> Portanto, a previs√£o para $x_0 = 3.5$ usando k-NN com $k=3$ √© aproximadamente 4.33.
>
> Se tiv√©ssemos escolhido $k=1$, a previs√£o seria o valor de $y$ do vizinho mais pr√≥ximo, que √© 4 (correspondente a x=3 ou x=4, qualquer um dos dois). Se tiv√©ssemos escolhido $k=5$, a previs√£o seria a m√©dia de todos os valores de $y$, que seria $\frac{2+3+4+6+5}{5}=4$.

**Lemma 7:** O estimador de k-NN para regress√£o √© um estimador n√£o-param√©trico, isto √©, n√£o faz suposi√ß√µes sobre a forma da fun√ß√£o de regress√£o. O estimador converge assintoticamente para a verdadeira fun√ß√£o de regress√£o quando $N \to \infty$ e $k/N \to 0$.
*Prova*: A prova deste resultado depende da suavidade da fun√ß√£o de regress√£o. O estimador k-NN aproxima a fun√ß√£o de regress√£o localmente pela m√©dia dos valores de $Y$ dos k vizinhos, e sob certas condi√ß√µes de suavidade e converg√™ncia de $k$ em rela√ß√£o ao tamanho do conjunto de dados ($N$), o estimador converge para a verdadeira fun√ß√£o de regress√£o. $\blacksquare$
```mermaid
graph TB
    subgraph "k-NN Regression Convergence"
        A["k-NN Estimator: '≈∑(x‚ÇÄ) = (1/k) ‚àë y·µ¢'"]
        B["Convergence Condition 1: 'f' is smooth"]
        C["Convergence Condition 2: 'k ‚Üí ‚àû'"]
        D["Convergence Condition 3: 'k/N ‚Üí 0'"]
        A --> B
        A --> C
        A --> D
        B & C & D --> E["'≈∑(x‚ÇÄ)' converges to 'f(x‚ÇÄ)'"]
    end
```

**Corol√°rio 7:** O k-NN em regress√£o, assim como na classifica√ß√£o, pode sofrer do problema da dimensionalidade. A densidade de dados diminui √† medida que a dimens√£o do espa√ßo de *features* aumenta, tornando as dist√¢ncias menos informativas e afetando a precis√£o das previs√µes.

> ‚ö†Ô∏è **Nota Importante**:  A escolha do valor de $k$ em regress√£o √© crucial para o desempenho do modelo. A valida√ß√£o cruzada pode ser usada para escolher um valor de $k$ que equilibre o vi√©s e a vari√¢ncia do estimador [^13.1].

> ‚ùó **Ponto de Aten√ß√£o**:  A m√©dia simples dos vizinhos pode ser sens√≠vel a *outliers*, que podem afetar negativamente a previs√£o. M√©todos de agrega√ß√£o mais robustos, como a mediana, podem ser utilizados em vez da m√©dia [^13.3].

### Maldi√ß√£o da Dimensionalidade: Limita√ß√µes em Regress√£o

A **maldi√ß√£o da dimensionalidade** √© um fen√¥meno que afeta o desempenho do m√©todo dos vizinhos mais pr√≥ximos, tanto em regress√£o quanto em classifica√ß√£o, especialmente quando o n√∫mero de *features* √© alto [^13.1]. O problema surge devido √† natureza esparsa dos dados em espa√ßos de alta dimens√£o. √Ä medida que o n√∫mero de *features* aumenta, a densidade dos dados diminui, e a dist√¢ncia entre os pontos se torna mais uniforme e menos informativa [^13.4].

Em outras palavras, em um espa√ßo de alta dimens√£o, os vizinhos mais pr√≥ximos de um ponto de consulta podem n√£o ser verdadeiramente "pr√≥ximos", pois a dist√¢ncia para todos os pontos tende a convergir para o mesmo valor, especialmente quando os dados est√£o esparsos. Isso prejudica a capacidade do k-NN de capturar a estrutura local dos dados e realizar previs√µes ou classifica√ß√µes precisas.

Na regress√£o, a maldi√ß√£o da dimensionalidade se manifesta de forma mais pronunciada, pois a previs√£o √© feita pela m√©dia dos valores da vari√°vel dependente dos vizinhos. Se os vizinhos mais pr√≥ximos n√£o s√£o representativos da vizinhan√ßa local devido √† alta dimensionalidade, a m√©dia pode levar a previs√µes enviesadas e com alta vari√¢ncia [^13.1].

**Lemma 8:** A dist√¢ncia Euclidiana perde informatividade em espa√ßos de alta dimens√£o devido √† concentra√ß√£o das dist√¢ncias. Em outras palavras, √† medida que a dimens√£o aumenta, a dist√¢ncia entre quaisquer dois pontos aleat√≥rios tende a convergir para o mesmo valor, o que dificulta a identifica√ß√£o de vizinhos "pr√≥ximos" [^13.4].
*Prova*: Seja $x$ e $y$ dois pontos aleat√≥rios em um espa√ßo de $p$ dimens√µes, onde cada coordenada √© amostrada de forma independente de uma distribui√ß√£o uniforme em $[0,1]$. A dist√¢ncia Euclidiana √© dada por $d(x,y) = \sqrt{\sum_{i=1}^p (x_i - y_i)^2}$. A esperan√ßa da dist√¢ncia ao quadrado √© dada por $E[d^2(x,y)] = \sum_{i=1}^p E[(x_i-y_i)^2] = \frac{p}{6}$. A vari√¢ncia da dist√¢ncia ao quadrado √© dada por $Var[d^2(x,y)] = \frac{4p}{45}$. √Ä medida que $p$ aumenta, o valor da dist√¢ncia tamb√©m cresce, mas a vari√¢ncia da dist√¢ncia tamb√©m aumenta, mostrando a concentra√ß√£o das dist√¢ncias. $\blacksquare$
```mermaid
graph TB
    subgraph "Curse of Dimensionality in Distance"
        A["Euclidean Distance: 'd(x,y) = ‚àö(‚àë(x·µ¢ - y·µ¢)¬≤) '"]
        B["Expected Squared Distance: 'E[d¬≤(x,y)] = p/6'"]
        C["Variance of Squared Distance: 'Var[d¬≤(x,y)] = 4p/45'"]
        A --> B
        A --> C
        B & C --> D["As 'p' increases, distances become less informative"]
    end
```

> üí° **Exemplo Num√©rico:**
>
> Imagine um espa√ßo de 1 dimens√£o (uma linha) onde os pontos s√£o distribu√≠dos uniformemente entre 0 e 1. A dist√¢ncia m√©dia entre dois pontos aleat√≥rios ser√° aproximadamente 1/3. Agora, imagine um espa√ßo de 2 dimens√µes (um plano) onde os pontos s√£o distribu√≠dos uniformemente em um quadrado de lado 1. A dist√¢ncia m√©dia entre dois pontos aleat√≥rios ser√° aproximadamente 0.52.
>
> Agora, vamos para um espa√ßo de 10 dimens√µes, com pontos distribu√≠dos uniformemente em um hipercubo de lado 1. A dist√¢ncia m√©dia entre dois pontos aleat√≥rios ser√° aproximadamente 1.29. Se aumentarmos para 100 dimens√µes, a dist√¢ncia m√©dia ser√° aproximadamente 5.16.
>
> O que vemos √© que a dist√¢ncia m√©dia aumenta com a dimens√£o. Al√©m disso, a *vari√¢ncia* das dist√¢ncias tamb√©m aumenta, o que significa que as dist√¢ncias se tornam menos discriminativas. Em altas dimens√µes, a dist√¢ncia entre um ponto e seus vizinhos mais pr√≥ximos se torna muito similar √† dist√¢ncia entre esse ponto e outros pontos mais distantes, tornando a no√ß√£o de "vizinho" menos significativa e prejudicando o desempenho do k-NN.

**Corol√°rio 8:** M√©todos de redu√ß√£o de dimensionalidade, como an√°lise de componentes principais (PCA) ou sele√ß√£o de *features*, podem ser usados para mitigar os efeitos da maldi√ß√£o da dimensionalidade, reduzindo o n√∫mero de *features* e preservando a estrutura dos dados.

> ‚ö†Ô∏è **Nota Importante**: A maldi√ß√£o da dimensionalidade √© mais problem√°tica em regress√£o do que em classifica√ß√£o devido √† forma como as informa√ß√µes dos vizinhos s√£o agregadas (m√©dia versus vota√ß√£o), e os m√©todos de redu√ß√£o de dimensionalidade s√£o fundamentais para o bom desempenho do k-NN em problemas de alta dimens√£o [^13.1].

> ‚ùó **Ponto de Aten√ß√£o**:  A escolha de uma m√©trica de dist√¢ncia apropriada, que leve em considera√ß√£o a estrutura dos dados, pode ajudar a melhorar o desempenho do k-NN em espa√ßos de alta dimensionalidade [^13.3].

### Adapta√ß√µes para Alta Dimensionalidade

Para contornar a maldi√ß√£o da dimensionalidade, v√°rias t√©cnicas podem ser aplicadas em conjunto com o m√©todo dos vizinhos mais pr√≥ximos, tanto em regress√£o quanto em classifica√ß√£o. Algumas das abordagens mais comuns incluem:

1.  **Redu√ß√£o de Dimensionalidade:** A aplica√ß√£o de t√©cnicas de redu√ß√£o de dimensionalidade, como PCA, sele√ß√£o de *features* usando m√©todos de filtro ou *wrappers*, visa reduzir o n√∫mero de *features* para apenas aquelas que cont√™m informa√ß√µes relevantes para a previs√£o ou classifica√ß√£o [^13.4].

    > üí° **Exemplo Num√©rico:**
    >
    > Suponha que temos um conjunto de dados com 100 *features*, mas ap√≥s aplicar PCA, descobrimos que 20 componentes principais explicam 95% da vari√¢ncia dos dados. Podemos ent√£o usar apenas esses 20 componentes para aplicar o k-NN, reduzindo drasticamente a dimensionalidade e mitigando a maldi√ß√£o da dimensionalidade.
    >
    >```python
    >import numpy as np
    >from sklearn.decomposition import PCA
    >
    ># Simula√ß√£o de dados com 100 features
    >X = np.random.rand(1000, 100)
    >
    ># Aplicando PCA para reduzir a dimensionalidade para 20 componentes
    >pca = PCA(n_components=20)
    >X_reduced = pca.fit_transform(X)
    >
    >print(f"Dimens√£o original: {X.shape[1]}")
    >print(f"Dimens√£o ap√≥s PCA: {X_reduced.shape[1]}")
    >```
```mermaid
graph LR
    subgraph "Dimensionality Reduction Techniques"
        direction TB
        A["High-Dimensional Data: 'X'"]
        B["Feature Selection Methods"]
        C["PCA (Principal Component Analysis)"]
        A --> B
        A --> C
        B --> D["Reduced Dimensional Data: 'X_reduced'"]
        C --> D
    end
    D --> E["k-NN Application"]

```

2. **M√©tricas de Dist√¢ncia Adaptativas:** Em vez de usar a dist√¢ncia Euclidiana padr√£o, podem-se utilizar m√©tricas de dist√¢ncia adaptativas, que levam em considera√ß√£o a estrutura local dos dados. Essas m√©tricas podem ser aprendidas a partir dos pr√≥prios dados e dar mais peso √†s *features* que s√£o relevantes para a previs√£o ou classifica√ß√£o [^13.4].

    > üí° **Exemplo Num√©rico:**
    >
    > Em um problema de regress√£o onde algumas *features* s√£o mais relevantes do que outras, podemos usar uma dist√¢ncia ponderada. Por exemplo, se $x_1$ √© uma *feature* muito importante e $x_2$ √© menos importante, podemos definir uma dist√¢ncia ponderada como:
    >
    > $d_w(x, x') = \sqrt{w_1(x_1 - x_1')^2 + w_2(x_2 - x_2')^2}$, onde $w_1 > w_2$
    >
    > Os pesos $w_1$ e $w_2$ podem ser aprendidos usando m√©todos de otimiza√ß√£o ou heur√≠sticas.
```mermaid
graph LR
    subgraph "Adaptive Distance Metrics"
        direction TB
        A["Standard Euclidean Distance"]
        B["Weighted Distance: 'dw(x, x') = ‚àö(‚àë w·µ¢(x·µ¢ - x·µ¢')¬≤)'"]
        C["Feature Relevance Weights"]
        B --> C
        A --> D["Apply to k-NN"]
        B --> D
    end
```
3. **Proje√ß√£o em Subespa√ßos:** Projetar os dados em subespa√ßos de baixa dimensionalidade, onde os dados s√£o mais concentrados e as dist√¢ncias mais informativas, tamb√©m √© uma estrat√©gia eficaz para reduzir o impacto da maldi√ß√£o da dimensionalidade [^13.4].

    > üí° **Exemplo Num√©rico:**
    >
    > Imagine que os dados est√£o distribu√≠dos em uma estrutura linear dentro de um espa√ßo de alta dimens√£o. Em vez de usar todas as dimens√µes, podemos projetar os dados nessa estrutura linear (subespa√ßo) de baixa dimens√£o, onde a dist√¢ncia entre os pontos √© mais significativa. Isso pode ser feito usando t√©cnicas como PCA ou an√°lise de subespa√ßos.
```mermaid
graph LR
    subgraph "Subspace Projection"
        direction TB
        A["High-Dimensional Data"]
        B["Identification of Subspace"]
         A --> B
        B --> C["Projection onto Subspace"]
        C --> D["Data in Lower Dimensional Space"]
    end
    D --> E["Apply to k-NN"]
```

4.  **Dist√¢ncias Invariantes:** Em problemas onde os dados s√£o invariantes sob certas transforma√ß√µes (por exemplo, rota√ß√µes ou transla√ß√µes), m√©tricas de dist√¢ncia invariantes podem ser usadas para tornar o k-NN mais robusto a essas transforma√ß√µes [^13.3.3].

    > üí° **Exemplo Num√©rico:**
    >
    > Em reconhecimento de imagem, as imagens podem ser rotacionadas ou translacionadas. Nesse caso, usar a dist√¢ncia euclidiana diretamente nas imagens pode n√£o ser ideal. M√©tricas de dist√¢ncia invariantes a rota√ß√µes ou transla√ß√µes s√£o mais apropriadas. Por exemplo, a dist√¢ncia entre duas imagens pode ser calculada ap√≥s alinhar as imagens usando t√©cnicas de alinhamento de imagem.
```mermaid
graph LR
    subgraph "Invariant Distance Metrics"
        direction TB
        A["Data with Transformations"]
        B["Transformation Invariant Metric"]
        A --> B
        B --> C["Apply Invariant Metric"]
        C --> D["k-NN with Invariance"]
    end
```

### Pergunta Te√≥rica Avan√ßada: Deriva√ß√£o da condi√ß√£o para o k-NN em regress√£o convergir para a fun√ß√£o de regress√£o
**Resposta:**
A converg√™ncia do k-NN para regress√£o, para a fun√ß√£o de regress√£o verdadeira, exige que a fun√ß√£o de regress√£o seja suave e que $k$ tenda ao infinito, enquanto $k/N$ tende a zero. Para entender esta condi√ß√£o, assumimos um modelo de regress√£o $y = f(x) + \epsilon$, onde $\epsilon$ √© um erro aleat√≥rio com m√©dia zero. O estimador k-NN para regress√£o √© dado por:

$$\hat{f}(x_0) = \frac{1}{k} \sum_{i=1}^k y_{(i)}$$

onde $y_{(i)}$ √© o valor da vari√°vel dependente do i-√©simo vizinho mais pr√≥ximo de $x_0$. Para mostrar a converg√™ncia do estimador k-NN para a verdadeira fun√ß√£o de regress√£o $f(x_0)$, √© necess√°rio analisar seu vi√©s e vari√¢ncia.

O vi√©s do estimador √© dado por:
$$Bias(\hat{f}(x_0)) = E[\hat{f}(x_0)] - f(x_0) = E[\frac{1}{k} \sum_{i=1}^k f(x_{(i)})] - f(x_0)$$
Onde $x_{(i)}$ s√£o as *features* dos k vizinhos. Se $f$ √© uma fun√ß√£o suave e os vizinhos est√£o pr√≥ximos de $x_0$, ent√£o $f(x_{(i)})$ √© pr√≥ximo de $f(x_0)$, e o vi√©s tende a zero.

A vari√¢ncia do estimador √© dada por:
$$Var(\hat{f}(x_0)) = Var(\frac{1}{k} \sum_{i=1}^k y_{(i)}) = \frac{1}{k^2} \sum_{i=1}^k Var(y_{(i)})$$
Assumindo que os erros $\epsilon$ s√£o independentes e com vari√¢ncia $\sigma^2$, e que os $y_{(i)}$ s√£o independentes, temos:
$$Var(\hat{f}(x_0)) = \frac{1}{k} \sigma^2$$
Como a vari√¢ncia decresce com o aumento de $k$, √© necess√°rio que $k \to \infty$. Para o vi√©s tender a zero, √© necess√°rio que os vizinhos mais pr√≥ximos estejam perto de $x_0$, o que implica que o raio da vizinhan√ßa deve diminuir conforme o n√∫mero de pontos aumenta, portanto $k/N \to 0$.

Em resumo, o estimador k-NN converge para a verdadeira fun√ß√£o de regress√£o $f(x_0)$ se:
 1.  $f$ √© suave
 2.  $k \to \infty$
 3.  $k/N \to 0$

### Conclus√£o

O m√©todo dos vizinhos mais pr√≥ximos √© uma ferramenta vers√°til, aplic√°vel tanto em problemas de classifica√ß√£o quanto de regress√£o. No entanto, a maldi√ß√£o da dimensionalidade imp√µe s√©rias limita√ß√µes, especialmente em regress√£o, tornando a escolha de uma m√©trica de dist√¢ncia apropriada e a aplica√ß√£o de m√©todos de redu√ß√£o de dimensionalidade fundamentais para obter um bom desempenho. A compreens√£o das limita√ß√µes e das adapta√ß√µes necess√°rias permite que o k-NN continue a ser um m√©todo √∫til em diversas situa√ß√µes, mesmo em problemas complexos com alta dimensionalidade.

### Footnotes

[^13.1]: "In this chapter we discuss some simple and essentially model-free methods for classification and pattern recognition...The nearest-neighbor technique can also be used in regression; this was touched on in Chapter 2 and works reasonably well for low-dimensional problems. However, with high-dimensional features, the bias-variance tradeoff does not work as favorably for nearest-neighbor regression as it does for classification." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3]: "These classifiers are memory-based, and require no model to be fit. Given a query point xo, we find the k training points x(r), r = 1,..., k closest in distance to xo, and then classify using majority vote among the k neighbors." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3.3]: "In some problems, the training features are invariant under certain natural transformations. The nearest-neighbor classifier can exploit such invariances by incorporating them into the metric used to measure the distances between objects." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.4]: "When nearest-neighbor classification is carried out in a high-dimensional feature space, the nearest neighbors of a point can be very far away, causing bias and degrading the performance of the rule...In general, this calls for adapting the metric used in nearest-neighbor classification..." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
