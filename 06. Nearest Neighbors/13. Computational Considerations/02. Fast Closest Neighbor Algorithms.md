## Algoritmos RÃ¡pidos para Busca de Vizinhos Mais PrÃ³ximos: Otimizando a EficiÃªncia do k-NN

<imagem: Diagrama comparando o funcionamento de algoritmos de busca eficientes, como Ã¡rvores KD e ball trees, com a busca exaustiva, mostrando como esses algoritmos exploram a estrutura dos dados para reduzir o nÃºmero de cÃ¡lculos de distÃ¢ncia necessÃ¡rios para encontrar os vizinhos mais prÃ³ximos, e como isso melhora a eficiÃªncia computacional do k-NN.>

### IntroduÃ§Ã£o

Este capÃ­tulo explora **algoritmos rÃ¡pidos** para a busca de **vizinhos mais prÃ³ximos**, com foco em como esses algoritmos otimizam a eficiÃªncia computacional do mÃ©todo de **k-vizinhos mais prÃ³ximos (k-NN)** [^13.5]. Em problemas de aprendizado de mÃ¡quina com grandes conjuntos de dados e alta dimensionalidade, a busca exaustiva, que calcula a distÃ¢ncia entre o ponto de consulta e todos os pontos de treinamento, pode ser proibitivamente cara. Analisaremos como algoritmos como as Ã¡rvores KD e as *ball trees* exploram a estrutura dos dados para reduzir o nÃºmero de cÃ¡lculos de distÃ¢ncia necessÃ¡rios para identificar os $k$ vizinhos mais prÃ³ximos, e como a utilizaÃ§Ã£o desses algoritmos Ã© crucial para tornar o k-NN escalÃ¡vel e eficiente em diversas aplicaÃ§Ãµes prÃ¡ticas.

```mermaid
graph LR
    subgraph "k-NN Efficiency Optimization"
        direction TB
        A["k-NN Algorithm"]
        B["Exhaustive Search"]
        C["Fast Algorithms (KD-Trees, Ball Trees)"]
        D["Large Datasets & High Dimensionality"]
        E["Reduced Distance Calculations"]
        F["Scalable and Efficient k-NN"]
        A --> B
        A --> C
        B --> D
        C --> E
        E --> F
     end
```

### Busca Exaustiva vs. Busca Eficiente: Reduzindo a Complexidade Computacional

A abordagem mais direta para encontrar os $k$ vizinhos mais prÃ³ximos de um ponto de consulta Ã© realizar uma **busca exaustiva**, onde a distÃ¢ncia entre o ponto de consulta e todos os pontos de treinamento Ã© calculada e os $k$ pontos mais prÃ³ximos sÃ£o selecionados [^13.3]. Essa abordagem tem uma complexidade computacional de **O(Np)**, onde $N$ Ã© o nÃºmero de amostras de treinamento e $p$ Ã© o nÃºmero de *features*. Conforme visto em seÃ§Ãµes anteriores, essa complexidade pode se tornar proibitiva para grandes conjuntos de dados.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Imagine um conjunto de dados com 10.000 amostras ($N = 10.000$) e 10 *features* ($p = 10$). Em uma busca exaustiva, para encontrar os vizinhos mais prÃ³ximos de um Ãºnico ponto de consulta, vocÃª teria que calcular a distÃ¢ncia entre esse ponto e todas as 10.000 amostras. Isso resulta em 10.000 cÃ¡lculos de distÃ¢ncia, cada um envolvendo operaÃ§Ãµes sobre as 10 *features*. Se cada cÃ¡lculo de distÃ¢ncia levasse, por exemplo, 1 microsegundo, a busca exaustiva levaria 10 milissegundos. Agora, imagine repetir isso para muitos pontos de consulta, ou para conjuntos de dados muito maiores. Ã‰ fÃ¡cil perceber que o custo computacional pode se tornar um gargalo.

Para reduzir o custo computacional da busca dos vizinhos, foram desenvolvidos **algoritmos de busca eficiente** que exploram a estrutura dos dados para realizar a busca sem a necessidade de calcular a distÃ¢ncia para todos os pontos do conjunto de treinamento. Esses algoritmos utilizam estruturas de dados e tÃ©cnicas de busca que permitem ignorar pontos que nÃ£o sÃ£o candidatos a vizinhos mais prÃ³ximos, e identificar rapidamente os $k$ pontos mais prÃ³ximos de um dado ponto.

Alguns dos algoritmos de busca eficientes mais utilizados incluem:

1.  **Ãrvores KD (KD-Trees):** As Ã¡rvores KD sÃ£o estruturas de dados que particionam o espaÃ§o de *features* de forma recursiva, criando uma Ã¡rvore binÃ¡ria que facilita a busca por vizinhos prÃ³ximos.
2.  **Ball Trees:** As *ball trees* sÃ£o estruturas de dados que particionam o espaÃ§o de *features* em hiperesferas (balls), o que torna a busca por vizinhos mais rÃ¡pida em espaÃ§os de alta dimensÃ£o.

```mermaid
graph LR
    subgraph "Computational Complexity"
        direction LR
        A["Exhaustive Search"] --> B["Distance Calculation: O(Np)"]
        B --> C["High Computational Cost"]
        C --> D["Fast Algorithms (KD-Trees, Ball Trees)"]
        D --> E["Reduced Distance Calculations"]
        E --> F["Lower Computational Cost"]

    end
```

**Lemma 164:** Algoritmos de busca eficiente, como Ã¡rvores KD e *ball trees*, permitem reduzir o custo computacional da busca dos vizinhos mais prÃ³ximos em comparaÃ§Ã£o com a busca exaustiva, por meio da utilizaÃ§Ã£o de estruturas de dados que exploram a estrutura dos dados para minimizar o nÃºmero de cÃ¡lculos de distÃ¢ncia.
*Prova*: As estruturas de Ã¡rvores KD e *ball trees* dividem o espaÃ§o de features de forma recursiva e permitem ignorar as regiÃµes do espaÃ§o onde os pontos nÃ£o sÃ£o vizinhos, evitando o cÃ¡lculo desnecessÃ¡rio de distÃ¢ncias. $\blacksquare$

**CorolÃ¡rio 164:** O uso de algoritmos de busca eficiente Ã© essencial para tornar o k-NN escalÃ¡vel para grandes conjuntos de dados e problemas de alta dimensÃ£o.

> âš ï¸ **Nota Importante**: Algoritmos de busca eficiente, como Ã¡rvores KD e *ball trees*, reduzem o custo computacional da busca por vizinhos mais prÃ³ximos, o que torna o k-NN mais escalÃ¡vel.

> â— **Ponto de AtenÃ§Ã£o**:  A escolha do algoritmo de busca eficiente mais apropriado depende das caracterÃ­sticas do conjunto de dados, e algumas abordagens sÃ£o mais adequadas do que outras para determinados cenÃ¡rios.

### Ãrvores KD: Particionamento Recursivo do EspaÃ§o de *Features*

As **Ã¡rvores KD (KD-trees)** sÃ£o estruturas de dados que particionam o espaÃ§o de *features* de forma recursiva, criando uma Ã¡rvore binÃ¡ria que facilita a busca por vizinhos mais prÃ³ximos [^13.5]. A construÃ§Ã£o de uma Ã¡rvore KD envolve os seguintes passos:

1.  **DivisÃ£o do EspaÃ§o:** O espaÃ§o de *features* Ã© dividido em duas regiÃµes utilizando um hiperplano ortogonal a uma das dimensÃµes. A dimensÃ£o utilizada para a divisÃ£o Ã© escolhida de forma a maximizar a variÃ¢ncia das *features* na regiÃ£o.
2.  **Particionamento Recursivo:** Cada regiÃ£o obtida Ã© dividida recursivamente, criando uma Ã¡rvore binÃ¡ria. A cada nÃ­vel da Ã¡rvore, uma dimensÃ£o diferente Ã© utilizada para a divisÃ£o.
3.  **Folhas da Ãrvore:** As folhas da Ã¡rvore representam regiÃµes do espaÃ§o de *features* que contÃ©m poucos pontos, onde a busca por vizinhos mais prÃ³ximos pode ser realizada diretamente.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Considere um conjunto de dados bidimensional com os seguintes pontos: `[(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)]`. Uma Ã¡rvore KD poderia ser construÃ­da da seguinte forma:
>
> 1.  **NÃ­vel 1:** A primeira divisÃ£o pode ser feita na dimensÃ£o x (primeira coordenada), usando a mediana dos valores x, que Ã© 6. Isso divide os pontos em dois grupos: `[(2,3), (5,4), (4,7)]` (esquerda) e `[(9,6), (8,1), (7,2)]` (direita).
> 2.  **NÃ­vel 2:** Para o grupo da esquerda, a divisÃ£o Ã© feita na dimensÃ£o y (segunda coordenada), usando a mediana dos valores y, que Ã© 4. Isso cria dois novos grupos: `[(2,3)]` e `[(5,4), (4,7)]`. Para o grupo da direita, a divisÃ£o tambÃ©m Ã© feita na dimensÃ£o y, com mediana 2, criando: `[(8,1), (7,2)]` e `[(9,6)]`.
> 3.  **NÃ­vel 3:** As folhas da Ã¡rvore sÃ£o os pontos individuais ou grupos pequenos de pontos.
>
> Este particionamento recursivo cria uma estrutura de Ã¡rvore que permite uma busca mais rÃ¡pida por vizinhos prÃ³ximos.
>
> ```mermaid
> graph LR
>     A((6, mediana x)) --> B((4, mediana y));
>     A --> C((2, mediana y));
>     B --> D((2,3));
>     B --> E((5,4), (4,7));
>     C --> F((8,1), (7,2));
>     C --> G((9,6));
> ```
>
> A busca por vizinhos agora pode navegar por essa Ã¡rvore, em vez de calcular a distÃ¢ncia para todos os pontos.

```mermaid
graph LR
    subgraph "KD-Tree Construction"
        direction TB
        A["Initial Feature Space"] --> B["Split by Dimension with Max Variance"]
        B --> C["Recursive Partitioning"]
        C --> D["Binary Tree Structure"]
        D --> E["Leaf Nodes with Few Data Points"]
     end
```

Para buscar os $k$ vizinhos mais prÃ³ximos de um ponto de consulta utilizando uma Ã¡rvore KD, o algoritmo segue os seguintes passos:

1.  **Busca na Ãrvore:** O algoritmo navega pela Ã¡rvore, seguindo as regiÃµes mais prÃ³ximas ao ponto de consulta, atÃ© chegar a uma regiÃ£o folha.
2.  **Busca Local:** Na regiÃ£o folha, as distÃ¢ncias entre o ponto de consulta e todos os pontos sÃ£o calculadas.
3.  **SeleÃ§Ã£o dos k Vizinhos:** Os $k$ pontos mais prÃ³ximos sÃ£o selecionados, e a busca retorna para os nÃ­veis superiores da Ã¡rvore, para verificar se em outros ramos mais prÃ³ximos existem pontos que podem se tornar vizinhos.
4.  **Refinamento:** Os vizinhos sÃ£o comparados com os resultados obtidos nos outros ramos, a fim de garantir que os $k$ vizinhos retornados sÃ£o, de fato, os mais prÃ³ximos.

A estrutura da Ã¡rvore KD permite que a busca por vizinhos seja realizada em tempo muito inferior ao da busca exaustiva.

**Lemma 165:** As Ã¡rvores KD particionam recursivamente o espaÃ§o de *features*, criando uma estrutura hierÃ¡rquica que facilita a busca por vizinhos mais prÃ³ximos, reduzindo o nÃºmero de cÃ¡lculos de distÃ¢ncia necessÃ¡rios.
*Prova*: Ao organizar os dados em uma estrutura de Ã¡rvore, o algoritmo de busca pode focar em apenas um ramo da Ã¡rvore, evitando o cÃ¡lculo de distÃ¢ncias em regiÃµes que nÃ£o sÃ£o relevantes para o ponto de consulta. $\blacksquare$

**CorolÃ¡rio 165:** O uso de Ã¡rvores KD permite uma busca de vizinhos mais rÃ¡pida do que a busca exaustiva, o que melhora a eficiÃªncia computacional do k-NN.

> âš ï¸ **Nota Importante**: As Ã¡rvores KD sÃ£o estruturas de dados que particionam recursivamente o espaÃ§o de *features*, facilitando a busca por vizinhos mais prÃ³ximos.

> â— **Ponto de AtenÃ§Ã£o**: A construÃ§Ã£o de Ã¡rvores KD Ã© uma etapa que requer computaÃ§Ã£o, mas o ganho na velocidade da busca nos vizinhos compensa esse custo inicial.

### *Ball Trees*: Particionamento por Hiperesferas em EspaÃ§os de Alta DimensÃ£o

As **ball trees** sÃ£o estruturas de dados que particionam o espaÃ§o de *features* por meio de **hiperesferas (balls)**, o que as torna mais adequadas do que Ã¡rvores KD para a busca de vizinhos em espaÃ§os de alta dimensÃ£o [^13.5]. A construÃ§Ã£o de uma *ball tree* envolve os seguintes passos:

1.  **DefiniÃ§Ã£o da Hiperesfera Raiz:** A raiz da *ball tree* representa uma hiperesfera que envolve todos os pontos do conjunto de dados. O centro e o raio da hiperesfera sÃ£o definidos com base nos dados.
2.  **Particionamento Recursivo:** A hiperesfera raiz Ã© dividida em duas hiperesferas menores que representam subconjuntos dos dados, com os respectivos centros e raios.
3.  **RepetiÃ§Ã£o:** Esse particionamento recursivo continua atÃ© que as hiperesferas nas folhas da Ã¡rvore contenham poucos pontos de dados.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Considere o mesmo conjunto de dados bidimensional do exemplo da Ã¡rvore KD: `[(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)]`. Uma *ball tree* poderia ser construÃ­da da seguinte forma:
>
> 1.  **NÃ­vel 1:** A hiperesfera raiz engloba todos os pontos. O centro pode ser a mÃ©dia dos pontos, e o raio a distÃ¢ncia mÃ¡xima de um ponto ao centro.
> 2.  **NÃ­vel 2:** A hiperesfera raiz Ã© dividida em duas hiperesferas menores. Por exemplo, uma hiperesfera poderia conter `[(2,3), (5,4), (4,7)]`, e a outra `[(9,6), (8,1), (7,2)]`. Os centros e raios de cada hiperesfera sÃ£o calculados de acordo com os pontos que elas contÃ©m.
> 3.  **NÃ­vel 3:** O particionamento continua recursivamente atÃ© que as folhas contenham poucos pontos.
>
> ```mermaid
> graph LR
>     A(Hiperesfera Raiz) --> B(Hiperesfera 1);
>     A --> C(Hiperesfera 2);
>     B --> D(Hiperesfera 1.1);
>     B --> E(Hiperesfera 1.2);
>     C --> F(Hiperesfera 2.1);
>     C --> G(Hiperesfera 2.2);
> ```
>
> A busca por vizinhos agora pode navegar por essa Ã¡rvore, verificando as hiperesferas mais prÃ³ximas do ponto de consulta.

```mermaid
graph LR
    subgraph "Ball Tree Construction"
        direction TB
        A["Initial Hypersphere Enclosing All Data"] --> B["Recursive Partitioning into Smaller Hyperspheres"]
        B --> C["Hyperspheres Representing Subsets of Data"]
        C --> D["Leaf Hyperspheres with Few Data Points"]
    end
```

Para buscar os $k$ vizinhos mais prÃ³ximos de um ponto de consulta utilizando uma *ball tree*, o algoritmo realiza os seguintes passos:

1.  **Busca na Ãrvore:** O algoritmo navega na Ã¡rvore, visitando as hiperesferas mais prÃ³ximas do ponto de consulta.
2.  **CÃ¡lculo da DistÃ¢ncia:** A distÃ¢ncia entre o ponto de consulta e as hiperesferas Ã© calculada, e algumas hiperesferas sÃ£o descartadas na busca, sem precisar computar a distÃ¢ncia entre o ponto de consulta e todos os pontos nelas contidos.
3.  **SeleÃ§Ã£o dos k Vizinhos:**  Os $k$ pontos contidos nas hiperesferas mais prÃ³ximas sÃ£o selecionados.

A vantagem da *ball tree* em relaÃ§Ã£o Ã  Ã¡rvore KD Ã© que a busca por vizinhos pode ser feita de forma mais eficiente em espaÃ§os de alta dimensÃ£o, pois as hiperesferas se adaptam melhor Ã  distribuiÃ§Ã£o dos dados nesses espaÃ§os, enquanto a Ã¡rvore KD tem um modelo mais rÃ­gido de partiÃ§Ã£o.

**Lemma 166:** As *ball trees* particionam recursivamente o espaÃ§o de *features* por meio de hiperesferas, o que permite uma busca por vizinhos mais eficiente que a busca exaustiva em espaÃ§os de alta dimensÃ£o.
*Prova*: As hiperesferas permitem representar regiÃµes com fronteiras mais apropriadas que a divisÃ£o em hiperplanos feita pelas Ã¡rvores KD, sendo tambÃ©m mais eficientes na implementaÃ§Ã£o. $\blacksquare$

**CorolÃ¡rio 166:** O uso de *ball trees* reduz o nÃºmero de cÃ¡lculos de distÃ¢ncia necessÃ¡rios para a busca por vizinhos no k-NN, o que torna o mÃ©todo mais escalÃ¡vel para dados de alta dimensÃ£o.

> âš ï¸ **Nota Importante**: *Ball trees* sÃ£o estruturas de dados que particionam o espaÃ§o de *features* utilizando hiperesferas, o que permite uma busca por vizinhos mais eficiente em espaÃ§os de alta dimensÃ£o.

> â— **Ponto de AtenÃ§Ã£o**:  A construÃ§Ã£o de *ball trees* tambÃ©m requer um custo computacional, mas o ganho em desempenho na busca de vizinhos geralmente compensa esse custo.

### Abordagens de AproximaÃ§Ã£o: *Tradeoffs* entre EficiÃªncia e PrecisÃ£o

AlÃ©m de Ã¡rvores KD e *ball trees*, existem outras abordagens que buscam aproximar a busca por vizinhos mais prÃ³ximos de forma a aumentar a eficiÃªncia computacional e de armazenamento [^13.5]. Essas abordagens envolvem *tradeoffs* entre a eficiÃªncia e a precisÃ£o, e alguns dos mÃ©todos incluem:

1.  **Locality Sensitive Hashing (LSH):** O LSH Ã© uma tÃ©cnica que utiliza funÃ§Ãµes de hash para mapear pontos de dados para *buckets* de forma que pontos similares tendam a cair no mesmo *bucket* ou em *buckets* vizinhos. A busca por vizinhos Ã© realizada apenas nos *buckets* relevantes, o que acelera a busca.
2.  **AproximaÃ§Ã£o da DistÃ¢ncia:** A aproximaÃ§Ã£o de distÃ¢ncias utiliza tÃ©cnicas matemÃ¡ticas para aproximar o cÃ¡lculo da distÃ¢ncia Euclidiana, reduzindo o nÃºmero de operaÃ§Ãµes necessÃ¡rias. O uso de distÃ¢ncias aproximadas resulta em modelos mais rÃ¡pidos mas com menor precisÃ£o na busca por vizinhos.
3. **ReduÃ§Ã£o de Dados:** Modelos de protÃ³tipos ou outros mÃ©todos de condensaÃ§Ã£o de dados podem ser utilizados para reduzir a quantidade de dados de treinamento, o que diminui o nÃºmero de distÃ¢ncias a serem calculadas no momento da classificaÃ§Ã£o.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Imagine que vocÃª estÃ¡ usando LSH para buscar vizinhos mais prÃ³ximos em um conjunto de dados de imagens. As funÃ§Ãµes de hash podem ser projetadas de forma que imagens similares (por exemplo, variaÃ§Ãµes da mesma cena) tenham hashes similares. Quando vocÃª busca os vizinhos de uma nova imagem, o LSH rapidamente direciona a busca para o *bucket* onde as imagens similares estÃ£o, evitando a necessidade de comparar com todas as outras imagens. Isso resulta em um grande ganho de eficiÃªncia, embora alguns vizinhos mais prÃ³ximos possam ser perdidos por estarem em outros *buckets*.

```mermaid
graph LR
    subgraph "Approximate Search Tradeoffs"
        direction LR
        A["Locality Sensitive Hashing (LSH)"] --> B["Hash Similar Data Points into Buckets"]
        A --> C["Reduced Search Space"]
        B & C --> D["Increased Efficiency, Potential Precision Loss"]
        E["Approximation of Distance"] --> F["Faster Distance Calculations"]
        E --> G["Reduced Computational Cost, Precision Tradeoff"]
        H["Data Reduction"] --> I["Reduced Training Data"]
        I --> J["Faster Nearest Neighbor Search, Potential Precision Loss"]
    end
```

A escolha da tÃ©cnica de busca mais apropriada depende do tipo de problema e dos requisitos de desempenho. Para dados com alta dimensionalidade e grande volume, a combinaÃ§Ã£o de tÃ©cnicas de reduÃ§Ã£o de dimensionalidade com busca aproximada por vizinhos Ã© frequente, a fim de obter um equilÃ­brio entre a velocidade e a qualidade da classificaÃ§Ã£o.

**Lemma 167:** Algoritmos de busca aproximada, como LSH, ou mÃ©todos que comprimem os dados de treino, permitem diminuir ainda mais o custo computacional, com o *tradeoff* de uma reduÃ§Ã£o na precisÃ£o da busca por vizinhos mais prÃ³ximos.
*Prova*: As abordagens de aproximaÃ§Ã£o buscam diminuir o nÃºmero de operaÃ§Ãµes para cÃ¡lculo de distÃ¢ncia ou o nÃºmero de pontos utilizados para a decisÃ£o de classificaÃ§Ã£o, reduzindo a qualidade da aproximaÃ§Ã£o, mas com aumento de velocidade na classificaÃ§Ã£o. $\blacksquare$

**CorolÃ¡rio 167:** O *tradeoff* entre eficiÃªncia e precisÃ£o na busca de vizinhos deve ser levado em conta para a escolha das tÃ©cnicas de otimizaÃ§Ã£o mais apropriadas para cada tipo de problema.

> âš ï¸ **Nota Importante**:  A escolha de tÃ©cnicas de busca aproximada para vizinhos no k-NN envolve um *tradeoff* entre eficiÃªncia computacional e precisÃ£o, e a melhor abordagem depende das necessidades do problema especÃ­fico.

> â— **Ponto de AtenÃ§Ã£o**:  A utilizaÃ§Ã£o de tÃ©cnicas de aproximaÃ§Ã£o na busca por vizinhos pode levar Ã  introduÃ§Ã£o de novos parÃ¢metros de ajuste, e a escolha desses parÃ¢metros Ã© relevante para garantir um bom desempenho do modelo.

### ConclusÃ£o

A busca eficiente por vizinhos mais prÃ³ximos Ã© essencial para tornar o k-NN escalÃ¡vel para grandes conjuntos de dados e problemas de alta dimensionalidade. Algoritmos como Ã¡rvores KD e *ball trees* exploram a estrutura dos dados para reduzir o nÃºmero de cÃ¡lculos de distÃ¢ncia, enquanto tÃ©cnicas de aproximaÃ§Ã£o oferecem um *tradeoff* entre eficiÃªncia e precisÃ£o. A escolha do algoritmo de busca e das tÃ©cnicas de otimizaÃ§Ã£o mais apropriados depende das caracterÃ­sticas do problema especÃ­fico, e o uso adequado dessas tÃ©cnicas permite que o k-NN seja uma ferramenta eficaz para problemas complexos de classificaÃ§Ã£o e reconhecimento de padrÃµes.

```mermaid
graph LR
    subgraph "k-NN Optimization Summary"
        direction TB
        A["k-NN Algorithm"]
        B["Efficient Nearest Neighbor Search"]
        C["KD-Trees, Ball Trees (Structure Exploitation)"]
        D["Approximate Techniques (LSH, Distance Approx.)"]
        E["Tradeoff: Efficiency vs Precision"]
        F["Scalability for Large & High-Dimensional Data"]
        A --> B
        B --> C
        B --> D
        D --> E
        C & E --> F
    end
```

### Footnotes

[^13.5]: "One drawback of nearest-neighbor rules in general is the computational load, both in finding the neighbors and storing the entire training set...There are fast algorithms for finding nearest-neighbors (Friedman et al., 1975; Friedman et al., 1977) which can reduce this load somewhat. Hastie and Simard (1998) reduce the computations for tangent distance by developing analogs of K-means clustering in the context of this invariant metric." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3]: "These classifiers are memory-based, and require no model to be fit. Given a query point xo, we find the k training points x(r), r = 1,..., k closest in distance to xo, and then classify using majority vote among the k neighbors." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
