## Classifica√ß√£o Baseada em Prot√≥tipos: Atribuindo Pontos de Consulta √† Classe do Prot√≥tipo Mais Pr√≥ximo

```mermaid
graph LR
    subgraph "Prototype-Based Classification"
        direction TB
        A["Input Data: Query Point x0"]
        B["Prototype Selection: {m_j}"]
        C["Distance Calculation: d(x0, m_j)"]
        D["Class Assignment: min_j d(x0, m_j)"]
        A --> B
        B --> C
        C --> D
        D --> E["Output: Class Label c_j*"]
    end
```

### Introdu√ß√£o

Este cap√≠tulo detalha o processo de **classifica√ß√£o baseada em prot√≥tipos**, explorando como um ponto de consulta √© atribu√≠do √† classe do prot√≥tipo mais pr√≥ximo no espa√ßo de *features* [^13.2]. A abordagem central dos m√©todos baseados em prot√≥tipos √© a representa√ß√£o do conjunto de dados por meio de um conjunto selecionado de pontos, ou prot√≥tipos, cada um com um r√≥tulo de classe associado. Discutiremos como a proximidade entre um ponto de consulta e os prot√≥tipos √© avaliada, geralmente utilizando a dist√¢ncia Euclidiana, e como essa avalia√ß√£o determina a classe √† qual o ponto de consulta ser√° atribu√≠do. Abordaremos tamb√©m o papel da m√©trica de dist√¢ncia na efic√°cia do m√©todo e como diferentes m√©tricas podem ser utilizadas em situa√ß√µes espec√≠ficas. Al√©m disso, analisaremos a diferen√ßa entre esse processo de classifica√ß√£o e o m√©todo de k-vizinhos mais pr√≥ximos (k-NN), onde a classifica√ß√£o √© baseada na vota√ß√£o majorit√°ria dos vizinhos mais pr√≥ximos.

### Classifica√ß√£o por Proximidade a Prot√≥tipos

A ess√™ncia da classifica√ß√£o baseada em prot√≥tipos reside na ideia de que pontos no espa√ßo de *features* pertencem √† mesma classe se estiverem pr√≥ximos de um mesmo prot√≥tipo [^13.2]. O processo de classifica√ß√£o consiste em tr√™s passos principais:

1.  **Sele√ß√£o dos Prot√≥tipos:** O primeiro passo √© a determina√ß√£o de um conjunto de prot√≥tipos que representam as classes. Esses prot√≥tipos s√£o pontos no espa√ßo de *features* que, idealmente, capturam as principais caracter√≠sticas da distribui√ß√£o de cada classe. M√©todos como K-Means, LVQ (Learning Vector Quantization) e GMMs (Gaussian Mixture Models) s√£o usados para gerar esses prot√≥tipos.
2.  **C√°lculo da Proximidade:** Dado um ponto de consulta (novo ponto a ser classificado), √© necess√°rio calcular sua proximidade a cada prot√≥tipo. A m√©trica de dist√¢ncia mais comum √© a dist√¢ncia Euclidiana, que quantifica a dist√¢ncia "em linha reta" entre dois pontos no espa√ßo de *features*. Outras m√©tricas, como dist√¢ncia de Mahalanobis, podem ser usadas em situa√ß√µes espec√≠ficas.
3.  **Atribui√ß√£o da Classe:** O ponto de consulta √© atribu√≠do √† classe do prot√≥tipo mais pr√≥ximo. Isso significa que, ap√≥s calcular a dist√¢ncia do ponto de consulta a cada prot√≥tipo, o ponto √© classificado na classe do prot√≥tipo que possui a menor dist√¢ncia.

Formalmente, seja $x_0$ um ponto de consulta a ser classificado, e seja $\{m_j\}_{j=1}^P$ o conjunto de prot√≥tipos, cada um com um r√≥tulo de classe $c_j$. A classe atribu√≠da a $x_0$ √© dada por:

$$\hat{c}(x_0) = c_{j^*}, \quad \text{onde} \quad j^* = \arg\min_j d(x_0, m_j)$$

onde $d(x_0, m_j)$ √© a dist√¢ncia entre o ponto de consulta $x_0$ e o prot√≥tipo $m_j$, e $j^*$ √© o √≠ndice do prot√≥tipo mais pr√≥ximo.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um cen√°rio de classifica√ß√£o bin√°ria com duas classes: A e B. Suponha que tenhamos dois prot√≥tipos:
>
> - Prot√≥tipo 1 ($m_1$): Coordenadas (2, 3), Classe A
> - Prot√≥tipo 2 ($m_2$): Coordenadas (5, 8), Classe B
>
> E um ponto de consulta $x_0$ com coordenadas (3, 5). Utilizaremos a dist√¢ncia Euclidiana para calcular a proximidade.
>
> **C√°lculo da Dist√¢ncia Euclidiana:**
>
> $d(x_0, m_1) = \sqrt{(3-2)^2 + (5-3)^2} = \sqrt{1^2 + 2^2} = \sqrt{5} \approx 2.24$
>
> $d(x_0, m_2) = \sqrt{(3-5)^2 + (5-8)^2} = \sqrt{(-2)^2 + (-3)^2} = \sqrt{13} \approx 3.61$
>
> Como $d(x_0, m_1) < d(x_0, m_2)$, o ponto de consulta $x_0$ ser√° classificado como pertencente √† Classe A.
>
> ```mermaid
> graph LR
>     A(Prot√≥tipo 1 - Classe A (2,3)) -->|Dist√¢ncia = 2.24| C(Ponto de Consulta (3,5))
>     B(Prot√≥tipo 2 - Classe B (5,8)) -->|Dist√¢ncia = 3.61| C
>     style C fill:#f9f,stroke:#333,stroke-width:2px
> ```

**Lemma 15:** A escolha de uma m√©trica de dist√¢ncia apropriada √© fundamental para o desempenho de modelos baseados em prot√≥tipos, pois a m√©trica influencia diretamente a no√ß√£o de "proximidade" e, portanto, a atribui√ß√£o de classe.
*Prova*: M√©tricas de dist√¢ncia que n√£o refletem a estrutura dos dados podem levar a classifica√ß√µes incorretas. A dist√¢ncia Euclidiana √© sens√≠vel a diferen√ßas nas escalas das *features*, enquanto a dist√¢ncia de Mahalanobis leva em considera√ß√£o a covari√¢ncia das *features*, sendo mais apropriada em alguns casos. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Imagine que temos duas *features*: "altura em cent√≠metros" e "peso em gramas". A dist√¢ncia Euclidiana padr√£o daria muito mais peso √† diferen√ßa na altura (que √© da ordem de centenas) do que no peso (que pode ser da ordem de milhares, mas varia menos). Se n√£o normalizarmos, um ponto com pouca diferen√ßa na altura e muita diferen√ßa no peso pode ser considerado mais distante do que um ponto com o inverso. Por isso, a normaliza√ß√£o √© importante.

```mermaid
graph LR
    subgraph "Distance Metric Influence"
        direction TB
        A["Data with Unequal Scales"]
        B["Euclidean Distance"]
        C["Normalization Required"]
        D["Mahalanobis Distance"]
        A --> B
        A --> D
        B --> C
        C --> E["Corrected Proximity Calculation"]
        D --> E
    end
```

**Corol√°rio 15:** A normaliza√ß√£o dos dados √© um passo importante para garantir que a dist√¢ncia Euclidiana seja uma m√©trica apropriada para a compara√ß√£o de prot√≥tipos em um espa√ßo de *features*, onde *features* com escalas diferentes poderiam dominar o c√°lculo da dist√¢ncia.

> ‚ö†Ô∏è **Nota Importante**: O processo de classifica√ß√£o com prot√≥tipos √© direto: o ponto de consulta √© atribu√≠do √† classe do prot√≥tipo mais pr√≥ximo. A complexidade reside na cria√ß√£o e ajuste dos prot√≥tipos.

> ‚ùó **Ponto de Aten√ß√£o**: A dist√¢ncia Euclidiana √© uma escolha comum para a medi√ß√£o da proximidade, mas outras m√©tricas podem ser mais apropriadas dependendo da natureza dos dados.

### Prot√≥tipos e M√©todos Espec√≠ficos: K-Means, LVQ e GMMs

**K-Means:** Quando o K-Means √© utilizado como m√©todo de classifica√ß√£o, os prot√≥tipos s√£o os centros dos *clusters* encontrados para cada classe [^13.2.1]. Para classificar um novo ponto, calcula-se a dist√¢ncia Euclidiana do ponto a todos os centros de cluster, e o ponto √© atribu√≠do √† classe do centroide mais pr√≥ximo. Os prot√≥tipos do K-Means representam os centros das regi√µes de dados de cada classe, e a classifica√ß√£o se baseia na ideia de que pontos dentro da mesma regi√£o devem compartilhar a mesma classe.

```mermaid
graph LR
    subgraph "K-Means Prototypes"
        direction TB
        A["Input Data: Training Samples"]
        B["K-Means Algorithm"]
        C["Cluster Centers as Prototypes"]
        D["Classification: Euclidean Distance to Prototypes"]
        A --> B
        B --> C
        C --> D
        D --> E["Class Assignment"]
    end
```

**Lemma 16:** Os prot√≥tipos do K-Means representam a m√©dia dos pontos em cada cluster, e, portanto, a classifica√ß√£o por dist√¢ncia ao prot√≥tipo mais pr√≥ximo equivale √† atribui√ß√£o √† classe com a m√©dia mais pr√≥xima no espa√ßo de *features*.
*Prova*: Os centros dos *clusters* encontrados pelo K-Means s√£o a m√©dia dos pontos a eles atribu√≠dos, e a classifica√ß√£o por proximidade se baseia nessa m√©dia como representa√ß√£o da regi√£o de cada classe. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos dados de duas classes em 2D, e aplicamos K-Means com K=2 para cada classe. A classe A tem pontos (1,1), (2,1), (1,2), (2,2) e a classe B tem pontos (4,4), (5,4), (4,5), (5,5).
>
> Os prot√≥tipos resultantes seriam:
>
> - Prot√≥tipo Classe A: (1.5, 1.5)
> - Prot√≥tipo Classe B: (4.5, 4.5)
>
> Um novo ponto (3,3) seria classificado.
>
> $d((3,3), (1.5, 1.5)) = \sqrt{(3-1.5)^2 + (3-1.5)^2} = \sqrt{2.25 + 2.25} \approx 2.12$
>
> $d((3,3), (4.5, 4.5)) = \sqrt{(3-4.5)^2 + (3-4.5)^2} = \sqrt{2.25 + 2.25} \approx 2.12$
>
> Nesse caso, o ponto (3,3) estaria equidistante dos dois prot√≥tipos. Em um caso real, o ponto seria atribu√≠do √† classe do prot√≥tipo mais pr√≥ximo (ou a um dos prot√≥tipos arbitrariamente, em caso de empate).

**Learning Vector Quantization (LVQ):** No LVQ, os prot√≥tipos s√£o ajustados para se posicionarem estrategicamente em rela√ß√£o √†s fronteiras de decis√£o das classes [^13.2.2]. A classifica√ß√£o √© feita da mesma forma: o ponto de consulta √© atribu√≠do √† classe do prot√≥tipo mais pr√≥ximo. A diferen√ßa √© que os prot√≥tipos LVQ n√£o s√£o apenas representantes da m√©dia dos dados, mas tamb√©m t√™m como objetivo discriminar entre as classes. O LVQ utiliza os dados de treino para ajustar os prot√≥tipos, atraindo-os para pontos da mesma classe e afastando-os de pontos de classes diferentes, o que resulta numa representa√ß√£o mais discriminativa do que a obtida com K-Means.

```mermaid
graph LR
    subgraph "LVQ Prototypes"
        direction TB
        A["Training Data"]
        B["Initialize Prototypes"]
        C["Adjust Prototypes: Attract/Repel"]
        D["Final Prototypes Near Decision Boundaries"]
        E["Classification: Euclidean Distance"]
        A --> B
        B --> C
        C --> D
        D --> E
        E --> F["Class Assignment"]
    end
```

**Corol√°rio 16:** Os prot√≥tipos LVQ s√£o mais eficazes em classificar pontos pr√≥ximos √†s fronteiras de decis√£o, pois o algoritmo de aprendizagem busca posicion√°-los em locais que maximizem a capacidade de discrimina√ß√£o.

> üí° **Exemplo Num√©rico:**
>
> Imagine que, ap√≥s inicializar os prot√≥tipos como no exemplo do K-Means, o LVQ ajusta os prot√≥tipos com base em dados de treino. O prot√≥tipo da Classe A pode ser movido para (2, 2) e o da Classe B para (4, 4), resultando em uma fronteira de decis√£o mais precisa em rela√ß√£o aos dados de treino. A classifica√ß√£o de um novo ponto (3, 3) seria feita usando esses novos prot√≥tipos, resultando em uma classifica√ß√£o diferente do exemplo anterior.

**Misturas Gaussianas (GMMs):** Nas GMMs, os prot√≥tipos s√£o os par√¢metros das componentes gaussianas (m√©dia e covari√¢ncia), e a classifica√ß√£o n√£o √© feita diretamente pela dist√¢ncia a esses prot√≥tipos [^13.2.3]. Em vez disso, a probabilidade de o ponto de consulta pertencer a cada componente gaussiana √© calculada, e o ponto √© atribu√≠do √† classe da componente com a maior probabilidade *a posteriori*. Embora n√£o seja uma classifica√ß√£o direta por dist√¢ncia ao prot√≥tipo mais pr√≥ximo, a atribui√ß√£o √© feita com base na proximidade a uma regi√£o do espa√ßo de *features* representada por cada componente gaussiana.

```mermaid
graph LR
    subgraph "GMM Prototypes"
        direction TB
        A["Input Data"]
        B["GMM Fit: Parameters (mean, covariance)"]
        C["Calculate Posterior Probability: P(class|x)"]
        D["Class Assignment: Max Probability"]
        A --> B
        B --> C
        C --> D
    end
```

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma GMM com duas componentes gaussianas, uma para cada classe. A componente da Classe A tem m√©dia (2, 2) e covari√¢ncia [[1, 0], [0, 1]], e a componente da Classe B tem m√©dia (5, 5) e covari√¢ncia [[1, 0], [0, 1]].
>
> Para classificar um ponto (3, 3), calculamos a probabilidade do ponto pertencer a cada componente gaussiana. A probabilidade de pertencer √† componente da Classe A ser√° maior do que a probabilidade de pertencer √† componente da Classe B, ent√£o o ponto ser√° classificado como Classe A. Este c√°lculo envolve a fun√ß√£o de densidade gaussiana, que utiliza a m√©dia e a covari√¢ncia de cada componente.

> ‚ö†Ô∏è **Nota Importante**: Enquanto o K-Means e o LVQ utilizam dist√¢ncias diretas aos prot√≥tipos para classifica√ß√£o, as GMMs utilizam probabilidades *a posteriori* baseadas na densidade gaussiana para atribui√ß√£o de classes.

> ‚ùó **Ponto de Aten√ß√£o**: Em todos os m√©todos de prot√≥tipos, a escolha da m√©trica de dist√¢ncia (ou a distribui√ß√£o gaussiana no caso de GMMs) tem um impacto crucial no resultado da classifica√ß√£o, e a valida√ß√£o cruzada √© essencial para otimizar esses par√¢metros.

### Compara√ß√£o com k-Vizinhos Mais Pr√≥ximos (k-NN)

O m√©todo de **k-Vizinhos Mais Pr√≥ximos (k-NN)** contrasta com os m√©todos de prot√≥tipos no processo de classifica√ß√£o [^13.3]. Enquanto os m√©todos de prot√≥tipos atribuem um ponto de consulta √† classe do prot√≥tipo mais pr√≥ximo, o k-NN atribui o ponto √† classe majorit√°ria entre seus $k$ vizinhos mais pr√≥ximos no conjunto de treinamento.

```mermaid
graph LR
    subgraph "k-NN Classification"
        direction TB
        A["Input Data: Query Point x0"]
        B["Find k-Nearest Neighbors in Training Data"]
        C["Majority Vote among Neighbors' Classes"]
        A --> B
        B --> C
        C --> D["Output: Predicted Class"]
    end
```

A principal diferen√ßa √© que o k-NN n√£o usa prot√≥tipos; em vez disso, usa todos os dados de treinamento como refer√™ncia. Para classificar um novo ponto, o k-NN calcula a dist√¢ncia desse ponto a todos os pontos do conjunto de treinamento e atribui ao novo ponto a classe mais frequente entre os k vizinhos mais pr√≥ximos.

A classifica√ß√£o por prot√≥tipos, em contraste com a abordagem k-NN, busca uma representa√ß√£o simplificada do espa√ßo de *features*, usando um n√∫mero menor de prot√≥tipos que capturam a ess√™ncia das distribui√ß√µes das classes. O k-NN, por outro lado, depende diretamente dos pontos do conjunto de treinamento, o que pode ser computacionalmente mais custoso, mas tamb√©m pode capturar varia√ß√µes mais finas na distribui√ß√£o dos dados, embora com risco de *overfitting*.

**Lemma 17:** O m√©todo k-NN usa um processo de vota√ß√£o para atribui√ß√£o de classe, ao passo que m√©todos de prot√≥tipo usam diretamente a proximidade a um prot√≥tipo, mas ambos os m√©todos dependem da no√ß√£o de proximidade no espa√ßo de *features*.
*Prova*: No k-NN, a classe √© atribu√≠da pela maioria dos r√≥tulos entre os k vizinhos mais pr√≥ximos, enquanto m√©todos de prot√≥tipos atribuem a classe do prot√≥tipo mais pr√≥ximo. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um conjunto de treinamento com os seguintes pontos:
>
> - Classe A: (1,1), (2,1), (1,2), (2,2)
> - Classe B: (4,4), (5,4), (4,5), (5,5)
>
> Para classificar um novo ponto (3,3) usando k-NN com k=3:
>
> 1. Calculamos a dist√¢ncia do ponto (3,3) para todos os pontos do conjunto de treino.
> 2. Os 3 vizinhos mais pr√≥ximos s√£o (2,2), (4,4) e (2,1).
> 3. Como 2 dos 3 vizinhos mais pr√≥ximos s√£o da Classe A, o ponto (3,3) √© classificado como Classe A. Se us√°ssemos k=5, os vizinhos mais pr√≥ximos seriam (2,2), (2,1), (1,2), (4,4) e (4,5), e o ponto seria classificado como Classe B.

**Corol√°rio 17:** A escolha entre k-NN e m√©todos de prot√≥tipos depende da complexidade do problema, do tamanho do conjunto de dados, e da necessidade de uma representa√ß√£o compacta dos dados.

> ‚ö†Ô∏è **Nota Importante**: A classifica√ß√£o baseada em prot√≥tipos √© mais eficiente computacionalmente do que o k-NN, pois a dist√¢ncia √© calculada apenas em rela√ß√£o a um conjunto menor de prot√≥tipos.

> ‚ùó **Ponto de Aten√ß√£o**:  O k-NN pode ser mais adequado quando a distribui√ß√£o dos dados √© muito irregular, pois ele n√£o tenta aproximar a distribui√ß√£o por prot√≥tipos.

> ‚úîÔ∏è **Destaque**:  Em problemas com fronteiras de decis√£o complexas, prot√≥tipos cuidadosamente ajustados podem levar a uma capacidade de generaliza√ß√£o semelhante ao k-NN com um custo computacional menor.

### Conclus√£o

A classifica√ß√£o baseada em prot√≥tipos oferece uma abordagem eficaz e vers√°til para atribuir pontos de consulta a classes espec√≠ficas, com base na proximidade desses pontos a um conjunto de prot√≥tipos cuidadosamente selecionados. Essa abordagem √© fundamental nos m√©todos *model-free* e se diferencia do k-NN, que usa todos os dados de treinamento para classifica√ß√£o. A escolha do m√©todo de prot√≥tipos espec√≠fico (K-Means, LVQ ou GMMs) e da m√©trica de dist√¢ncia adequada (ou distribui√ß√£o gaussiana) depende da complexidade dos dados e do problema em quest√£o, mas todos esses m√©todos compartilham a ideia central de classificar por proximidade a prot√≥tipos que representam as distribui√ß√µes das classes.

### Footnotes

[^13.2]: "Throughout this chapter, our training data consists of the N pairs (x1,91),...,(xn, 9N) where gi is a class label taking values in {1, 2, . . ., K}. Prototype methods represent the training data by a set of points in feature space. These prototypes are typically not examples from the training sample, except in the case of 1-nearest-neighbor classification discussed later. Each prototype has an associated class label, and classification of a query point x is made to the class of the closest prototype." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.1]: "K-means clustering is a method for finding clusters and cluster centers in a set of unlabeled data... To use K-means clustering for classification of labeled data, the steps are: apply K-means clustering to the training data in each class separately, using R prototypes per class; assign a class label to each of the K √ó R prototypes; classify a new feature x to the class of the closest prototype." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.2]: "In this technique due to Kohonen (1989), prototypes are placed strategically with respect to the decision boundaries in an ad-hoc way...The idea is that the training points attract prototypes of the correct class, and repel other prototypes...When the iterations settle down, prototypes should be close to the training points in their class." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.3]: "The Gaussian mixture model can also be thought of as a prototype method, similar in spirit to K-means and LVQ...Each cluster is described in terms of a Gaussian density, which has a centroid (as in K-means), and a covariance matrix...when Gaussian mixture models are used to represent the feature density in each class, it produces smooth posterior probabilities." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3]: "These classifiers are memory-based, and require no model to be fit. Given a query point xo, we find the k training points x(r), r = 1,..., k closest in distance to xo, and then classify using majority vote among the k neighbors." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
