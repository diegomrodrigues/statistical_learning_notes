## Prot√≥tipos em Classifica√ß√£o: Pontos no Espa√ßo de *Features* com R√≥tulos de Classe

```mermaid
graph LR
    subgraph "Classifica√ß√£o com Prot√≥tipos"
        direction TB
        A["Conjunto de Treinamento"]
        B["Extra√ß√£o de Features"]
        C["Prot√≥tipos ('aprendidos')"]
        D["Classifica√ß√£o de Novos Pontos"]
        A --> B
        B --> C
        C --> D
        style C fill:#ccf,stroke:#333,stroke-width:2px
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora a fundo a natureza dos **prot√≥tipos** em m√©todos de classifica√ß√£o, enfatizando que, diferentemente do m√©todo de 1-vizinho mais pr√≥ximo (1-NN), os prot√≥tipos n√£o s√£o, em geral, amostras originais do conjunto de treinamento, mas sim pontos no espa√ßo de *features* com r√≥tulos de classe associados [^13.2]. Abordaremos a diferen√ßa fundamental entre prot√≥tipos e amostras de treinamento, e como essa distin√ß√£o afeta a forma como a classifica√ß√£o √© realizada. Analisaremos detalhadamente o papel dos prot√≥tipos em m√©todos como K-Means, LVQ (Learning Vector Quantization) e Misturas Gaussianas (GMMs), comparando-os com a abordagem do 1-NN, que usa as pr√≥prias amostras de treinamento como refer√™ncia para classifica√ß√£o. O objetivo √© esclarecer o conceito de prot√≥tipo e seu impacto na flexibilidade, efici√™ncia computacional e capacidade de generaliza√ß√£o dos m√©todos *model-free*.

### Prot√≥tipos vs. Amostras de Treinamento: Uma Distin√ß√£o Fundamental

A distin√ß√£o entre prot√≥tipos e amostras de treinamento √© crucial para compreender a filosofia dos m√©todos baseados em prot√≥tipos. Em geral, um **prot√≥tipo** √© um ponto no espa√ßo de *features* que representa uma regi√£o ou distribui√ß√£o de uma classe espec√≠fica [^13.2]. Os prot√≥tipos s√£o constru√≠dos (ou aprendidos) a partir dos dados de treinamento, mas n√£o s√£o necessariamente amostras originais do conjunto de treinamento. Cada prot√≥tipo possui um r√≥tulo de classe associado, e a classifica√ß√£o de um novo ponto √© realizada com base em sua proximidade a esses prot√≥tipos [^13.2].

Por outro lado, as **amostras de treinamento** s√£o os pontos de dados reais observados que comp√µem o conjunto de dados original. Essas amostras s√£o usadas para criar ou ajustar os prot√≥tipos, ou, como no caso do k-NN, s√£o usadas diretamente para classificar novos pontos.

```mermaid
graph LR
    subgraph "Prot√≥tipos vs. Amostras"
        direction LR
        A["Amostras de Treinamento"] --> B["Prot√≥tipos (K-Means, LVQ, GMMs)"]
        A --> C["1-NN"]
         B --> D["Representa√ß√£o Simplificada"]
        C --> E["Amostras Originais"]
        style B fill:#ccf,stroke:#333,stroke-width:2px
    end
```

A principal diferen√ßa entre prot√≥tipos e amostras de treinamento √© que os prot√≥tipos s√£o uma representa√ß√£o simplificada dos dados, muitas vezes com um n√∫mero menor de pontos do que as amostras originais. Essa simplifica√ß√£o tem um impacto significativo na efici√™ncia computacional e na capacidade de generaliza√ß√£o do modelo. Al√©m disso, o posicionamento estrat√©gico dos prot√≥tipos no espa√ßo de *features* permite que m√©todos baseados em prot√≥tipos capturem as principais caracter√≠sticas da distribui√ß√£o das classes, mesmo que os dados originais sejam complexos ou ruidosos.

**Lemma 12:** A representa√ß√£o de dados usando prot√≥tipos cria um conjunto menor de pontos que resume a distribui√ß√£o das classes, reduzindo a complexidade computacional e de armazenamento em compara√ß√£o com o uso do conjunto de treinamento original.
*Prova*: Ao usar prot√≥tipos, somente as coordenadas dos prot√≥tipos e seus r√≥tulos de classe precisam ser armazenadas, em vez de todo o conjunto de dados. Os c√°lculos de dist√¢ncia tamb√©m s√£o feitos apenas em rela√ß√£o aos prot√≥tipos, e n√£o em rela√ß√£o a todas as amostras do conjunto de treinamento. $\blacksquare$

**Corol√°rio 12:** A cria√ß√£o e o uso de prot√≥tipos introduzem um n√≠vel de abstra√ß√£o e simplifica√ß√£o na representa√ß√£o dos dados, permitindo que modelos *model-free* lidem com conjuntos de dados grandes ou complexos de forma mais eficiente.

> ‚ö†Ô∏è **Nota Importante**: O m√©todo de 1-NN √© uma exce√ß√£o, pois usa as pr√≥prias amostras de treinamento como prot√≥tipos, o que implica que a complexidade computacional e a capacidade de generaliza√ß√£o dependem do tamanho e da qualidade do conjunto de dados de treinamento [^13.2].

> ‚ùó **Ponto de Aten√ß√£o**: A escolha de onde posicionar os prot√≥tipos tem um impacto crucial no desempenho dos m√©todos de classifica√ß√£o, e diferentes t√©cnicas (K-Means, LVQ, GMMs) oferecem estrat√©gias diferentes para esse posicionamento.

### Prot√≥tipos em M√©todos Espec√≠ficos: K-Means, LVQ e GMMs

**K-Means:** No algoritmo **K-Means**, os prot√≥tipos s√£o os centroides dos *clusters* formados pela parti√ß√£o dos dados [^13.2.1]. O algoritmo busca encontrar os centros dos *clusters* de forma a minimizar a vari√¢ncia dentro de cada *cluster*. Embora o K-Means seja um m√©todo de *clustering* n√£o supervisionado, ele pode ser adaptado para classifica√ß√£o aplicando-o separadamente a cada classe e utilizando os centros dos *clusters* resultantes como prot√≥tipos [^13.2.1]. Nesse caso, os prot√≥tipos K-means s√£o os centroides dos dados, e n√£o amostras originais de treinamento.

```mermaid
graph LR
    subgraph "K-Means como M√©todo de Prot√≥tipos"
        direction TB
        A["Conjunto de Dados de Treinamento (por classe)"]
        B["Execu√ß√£o do K-Means"]
        C["Centroides dos Clusters"]
        D["Prot√≥tipos K-Means"]
        A --> B
        B --> C
        C --> D
        style C fill:#ccf,stroke:#333,stroke-width:2px
    end
```

> üí° **Exemplo Num√©rico:**
>
> Imagine um conjunto de dados bidimensional com duas classes, cada uma com 10 pontos. Ap√≥s aplicar o K-Means com dois clusters para cada classe, obtemos dois centroides para cada classe. Digamos que os centroides da Classe 1 sejam (1, 1) e (2, 2), e os da Classe 2 sejam (5, 5) e (6, 6). Estes quatro pontos (1, 1), (2, 2), (5, 5) e (6, 6) s√£o os prot√≥tipos. Eles representam as regi√µes de cada classe, mas n√£o s√£o amostras originais.
>
> ```python
> import numpy as np
> from sklearn.cluster import KMeans
> import matplotlib.pyplot as plt
>
> # Dados de exemplo (2 classes)
> X_class1 = np.array([[1, 1], [1.5, 1.5], [2, 1], [1, 2], [2, 2], [1.3, 1.8], [1.6, 1.2], [1.8, 1.9], [2.2, 1.3], [1.7, 1.7]])
> X_class2 = np.array([[5, 5], [5.5, 5.5], [6, 5], [5, 6], [6, 6], [5.3, 5.8], [5.6, 5.2], [5.8, 5.9], [6.2, 5.3], [5.7, 5.7]])
>
> # K-Means para cada classe
> kmeans_class1 = KMeans(n_clusters=2, random_state=0, n_init = 'auto').fit(X_class1)
> kmeans_class2 = KMeans(n_clusters=2, random_state=0, n_init = 'auto').fit(X_class2)
>
> # Prot√≥tipos (centroides)
> prototypes_class1 = kmeans_class1.cluster_centers_
> prototypes_class2 = kmeans_class2.cluster_centers_
>
> # Plot
> plt.scatter(X_class1[:, 0], X_class1[:, 1], c='blue', label='Classe 1')
> plt.scatter(X_class2[:, 0], X_class2[:, 1], c='red', label='Classe 2')
> plt.scatter(prototypes_class1[:, 0], prototypes_class1[:, 1], marker='x', s=200, c='black', label='Prot√≥tipos Classe 1')
> plt.scatter(prototypes_class2[:, 0], prototypes_class2[:, 1], marker='x', s=200, c='black', label='Prot√≥tipos Classe 2')
> plt.legend()
> plt.title('Prot√≥tipos K-Means')
> plt.show()
>
> print("Prot√≥tipos da Classe 1:\n", prototypes_class1)
> print("Prot√≥tipos da Classe 2:\n", prototypes_class2)
> ```
>
> Neste exemplo, os prot√≥tipos (marcados com 'x') s√£o os centroides dos clusters encontrados pelo K-Means para cada classe. Eles n√£o correspondem a nenhum ponto de dado original.

**Lemma 13:** Os prot√≥tipos do K-Means s√£o os centros de massa das distribui√ß√µes de dados, o que significa que eles representam a m√©dia dos pontos em cada cluster e n√£o exemplos espec√≠ficos dos dados de treinamento.
*Prova*: A cada itera√ß√£o, os prot√≥tipos do K-Means s√£o movidos para a m√©dia dos pontos a eles atribu√≠dos, convergindo para o centro de cada cluster. $\blacksquare$

**Learning Vector Quantization (LVQ):** No **LVQ**, os prot√≥tipos s√£o ajustados de forma supervisionada com base na classe de cada ponto de treino [^13.2.2]. O LVQ inicializa os prot√≥tipos, muitas vezes usando os resultados do K-Means, e em cada itera√ß√£o, move os prot√≥tipos da classe correta em dire√ß√£o ao ponto de treino e se afasta dos prot√≥tipos de classes incorretas. Os prot√≥tipos LVQ tendem a ser posicionados em regi√µes estrat√©gicas do espa√ßo de *features*, perto das fronteiras de decis√£o. Tamb√©m neste caso, os prot√≥tipos s√£o representa√ß√µes da distribui√ß√£o dos dados, e n√£o amostras originais do conjunto de treinamento.

```mermaid
graph LR
    subgraph "Aprendizado de Prot√≥tipos no LVQ"
        direction TB
        A["Prot√≥tipos Iniciais"]
        B["Ponto de Treinamento"]
        C["Atualiza√ß√£o Supervisionada do Prot√≥tipo"]
         D["Prot√≥tipos Ajustados"]
        A --> B
        B --> C
        C --> D
         style D fill:#ccf,stroke:#333,stroke-width:2px
    end
```

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um prot√≥tipo da Classe 1 em (1,1) e um ponto de treino da Classe 1 em (1.5, 1.5). Em uma itera√ß√£o do LVQ, o prot√≥tipo √© movido em dire√ß√£o ao ponto de treino. Se a taxa de aprendizagem for 0.1, o novo prot√≥tipo ser√° (1 + 0.1*(1.5-1), 1 + 0.1*(1.5-1)) = (1.05, 1.05). Se um prot√≥tipo da Classe 2 estivesse em (5,5) e um ponto de treino da Classe 1 estivesse em (1.5, 1.5), o prot√≥tipo da Classe 2 se afastaria do ponto de treino: (5 - 0.1*(1.5-5), 5 - 0.1*(1.5-5)) = (5.35, 5.35).
>
> ```python
> import numpy as np
>
> def lvq_update(prototype, train_point, learning_rate, correct_class=True):
>     if correct_class:
>         return prototype + learning_rate * (train_point - prototype)
>     else:
>         return prototype - learning_rate * (train_point - prototype)
>
> # Prot√≥tipos iniciais
> prototype_class1 = np.array([1.0, 1.0])
> prototype_class2 = np.array([5.0, 5.0])
>
> # Ponto de treino
> train_point_class1 = np.array([1.5, 1.5])
>
> # Taxa de aprendizagem
> learning_rate = 0.1
>
> # Atualiza√ß√£o do prot√≥tipo da classe correta (Classe 1)
> updated_prototype_class1 = lvq_update(prototype_class1, train_point_class1, learning_rate)
>
> # Atualiza√ß√£o do prot√≥tipo da classe incorreta (Classe 2)
> updated_prototype_class2 = lvq_update(prototype_class2, train_point_class1, learning_rate, correct_class=False)
>
> print(f"Prot√≥tipo da Classe 1 atualizado: {updated_prototype_class1}")
> print(f"Prot√≥tipo da Classe 2 atualizado: {updated_prototype_class2}")
> ```
>
> Aqui, demonstramos uma √∫nica itera√ß√£o do LVQ, mostrando como os prot√≥tipos se movem com base na classe do ponto de treino.

**Corol√°rio 13:** O LVQ busca posicionar os prot√≥tipos perto das fronteiras de decis√£o, otimizando assim a capacidade discriminat√≥ria do modelo, diferentemente do K-means, que foca na representatividade das regi√µes de dados.

**Misturas Gaussianas (GMMs):** No caso das **GMMs**, os prot√≥tipos s√£o representados pelas m√©dias e covari√¢ncias das componentes gaussianas que modelam a distribui√ß√£o dos dados [^13.2.3]. Cada componente gaussiana representa um "cluster" ou regi√£o no espa√ßo de *features*. Os par√¢metros das GMMs s√£o ajustados pelo algoritmo EM, e a classifica√ß√£o de novos pontos √© feita com base na probabilidade de pertencer a cada componente gaussiana. Os prot√≥tipos, nesse caso, s√£o as m√©dias (centr√≥ides) dos componentes gaussianos, que representam regi√µes de densidade dos dados e n√£o amostras originais.

```mermaid
graph LR
    subgraph "GMMs como M√©todo de Prot√≥tipos"
        direction TB
        A["Componentes Gaussianas (M√©dia e Covari√¢ncia)"]
        B["Ajuste via Algoritmo EM"]
        C["Prot√≥tipos (M√©dias das Gaussianas)"]
        D["Classifica√ß√£o Probabil√≠stica"]
        A --> B
        B --> C
        C --> D
       style C fill:#ccf,stroke:#333,stroke-width:2px
    end
```

> üí° **Exemplo Num√©rico:**
>
> Suponha que uma classe seja modelada por duas gaussianas. A primeira tem m√©dia (2,2) e covari√¢ncia [[1,0],[0,1]], e a segunda tem m√©dia (4,4) e covari√¢ncia [[0.5,0],[0,0.5]]. Os prot√≥tipos seriam as m√©dias (2,2) e (4,4), que s√£o os centros de cada componente gaussiana. A classifica√ß√£o de um novo ponto envolveria calcular a probabilidade de pertencer a cada gaussiana e atribuir a classe com a maior probabilidade.
>
> ```python
> import numpy as np
> from scipy.stats import multivariate_normal
>
> # Par√¢metros das Gaussianas (prot√≥tipos)
> mean1 = np.array([2, 2])
> cov1 = np.array([[1, 0], [0, 1]])
> mean2 = np.array([4, 4])
> cov2 = np.array([[0.5, 0], [0, 0.5]])
>
> # Ponto a ser classificado
> x = np.array([3, 3])
>
> # C√°lculo da probabilidade de cada gaussiana
> prob1 = multivariate_normal.pdf(x, mean=mean1, cov=cov1)
> prob2 = multivariate_normal.pdf(x, mean=mean2, cov=cov2)
>
> # Classifica√ß√£o
> if prob1 > prob2:
>     predicted_class = 1
> else:
>     predicted_class = 2
>
> print(f"Probabilidade da Gaussiana 1: {prob1}")
> print(f"Probabilidade da Gaussiana 2: {prob2}")
> print(f"Classe Predita: {predicted_class}")
> ```
>
> Este exemplo mostra como as m√©dias das gaussianas s√£o usadas como prot√≥tipos e como a probabilidade de um ponto pertencer a cada componente √© utilizada para classifica√ß√£o.

> ‚ö†Ô∏è **Nota Importante**: Embora o K-Means e o LVQ produzam prot√≥tipos como pontos no espa√ßo de *features*, as GMMs usam par√¢metros das gaussianas para representar prot√≥tipos.

> ‚ùó **Ponto de Aten√ß√£o**:  Em todos esses m√©todos, a localiza√ß√£o e o n√∫mero de prot√≥tipos s√£o par√¢metros cruciais para o desempenho do modelo, e precisam ser ajustados com base nas caracter√≠sticas dos dados.

> ‚úîÔ∏è **Destaque**: Os prot√≥tipos, tanto no K-Means, LVQ quanto GMMs, s√£o constru√≠dos a partir dos dados de treino, mas n√£o s√£o amostras diretas, o que permite que esses m√©todos sejam mais flex√≠veis e adaptativos do que m√©todos que dependem diretamente das amostras de treino, como o 1-NN.

### 1-NN: Uma Exce√ß√£o na Representa√ß√£o de Dados

O m√©todo do **1-Vizinho Mais Pr√≥ximo (1-NN)** √© uma exce√ß√£o na representa√ß√£o de dados via prot√≥tipos [^13.3]. No 1-NN, cada amostra do conjunto de treinamento atua como um prot√≥tipo. Isso significa que o modelo n√£o simplifica a representa√ß√£o dos dados; em vez disso, usa todos os pontos originais como refer√™ncia para classifica√ß√£o. Quando um novo ponto √© apresentado para classifica√ß√£o, o 1-NN busca o ponto de treinamento mais pr√≥ximo e atribui ao novo ponto o r√≥tulo de classe desse vizinho.

Essa abordagem "pregui√ßosa" (lazy learning) tem a vantagem de n√£o exigir a cria√ß√£o de prot√≥tipos artificiais, mas tem o inconveniente de ser computacionalmente custosa e sens√≠vel ao ru√≠do no conjunto de treinamento. A complexidade do modelo 1-NN est√° relacionada diretamente com o tamanho do conjunto de treinamento e a dimens√£o do espa√ßo de *features* [^13.3].

```mermaid
graph LR
    subgraph "1-NN como M√©todo de Classifica√ß√£o"
        direction TB
        A["Amostras de Treinamento"]
        B["Ponto a Ser Classificado"]
        C["C√°lculo da Dist√¢ncia para Todas as Amostras"]
        D["Vizinho Mais Pr√≥ximo"]
        E["Classifica√ß√£o"]
         A --> C
        B --> C
        C --> D
        D --> E
    end
```

> üí° **Exemplo Num√©rico:**
>
> Imagine um conjunto de treinamento com os seguintes pontos e classes: (1, 1, Classe 1), (2, 2, Classe 1), (5, 5, Classe 2), (6, 6, Classe 2). Se o novo ponto a ser classificado for (3, 3), o 1-NN calcular√° a dist√¢ncia para cada ponto de treino e identificar√° (2, 2, Classe 1) como o mais pr√≥ximo. Portanto, o novo ponto ser√° classificado como Classe 1.
>
> ```python
> import numpy as np
> from sklearn.neighbors import KNeighborsClassifier
>
> # Dados de treinamento
> X_train = np.array([[1, 1], [2, 2], [5, 5], [6, 6]])
> y_train = np.array([0, 0, 1, 1])  # 0 para Classe 1, 1 para Classe 2
>
> # Novo ponto a ser classificado
> x_new = np.array([[3, 3]])
>
> # Classificador 1-NN
> knn = KNeighborsClassifier(n_neighbors=1)
> knn.fit(X_train, y_train)
>
> # Classifica√ß√£o
> predicted_class = knn.predict(x_new)[0]
>
> print(f"Classe Predita: {predicted_class}")
> ```
>
> Este exemplo ilustra como o 1-NN usa os pontos de treino diretamente como prot√≥tipos, calculando a dist√¢ncia entre o ponto novo e todos os pontos do conjunto de treino.

**Lemma 14:** A representa√ß√£o de dados do 1-NN √© equivalente ao uso de todo o conjunto de treinamento como prot√≥tipos, implicando que o modelo n√£o realiza uma simplifica√ß√£o dos dados, e tem sua capacidade de generaliza√ß√£o e desempenho diretamente ligados √†s caracter√≠sticas do conjunto de treinamento.
*Prova*: Como a classifica√ß√£o de um novo ponto √© feita usando diretamente o r√≥tulo do ponto de treino mais pr√≥ximo, a representa√ß√£o dos dados √©, de fato, o conjunto de treino completo. $\blacksquare$

**Corol√°rio 14:** O m√©todo 1-NN tem uma complexidade de armazenamento e computacional que cresce linearmente com o tamanho do conjunto de treinamento e √© suscet√≠vel ao problema da dimensionalidade devido ao uso direto das amostras de treino, necessitando de m√©todos de pr√©-processamento dos dados, sele√ß√£o de *features* ou redu√ß√£o de dimensionalidade.

> ‚ö†Ô∏è **Nota Importante**: O 1-NN utiliza as pr√≥prias amostras de treinamento como prot√≥tipos, o que o diferencia dos m√©todos que aprendem prot√≥tipos artificiais (K-Means, LVQ e GMMs).

> ‚ùó **Ponto de Aten√ß√£o**: O 1-NN √© computacionalmente custoso para grandes conjuntos de dados, pois a dist√¢ncia para todas as amostras de treinamento deve ser calculada para cada nova classifica√ß√£o.

> ‚úîÔ∏è **Destaque**: A diferen√ßa fundamental entre o 1-NN e os m√©todos de prot√≥tipos est√° na forma como eles representam o conjunto de dados: o 1-NN usa as pr√≥prias amostras, enquanto os outros m√©todos usam prot√≥tipos que s√£o uma representa√ß√£o simplificada dos dados.

### Conclus√£o

A compreens√£o da natureza dos prot√≥tipos √© fundamental para utilizar adequadamente os m√©todos *model-free* de classifica√ß√£o. Os prot√≥tipos, seja como centros de *clusters* em K-Means, como pontos ajustados nas fronteiras de decis√£o em LVQ, ou como par√¢metros de gaussianas em GMMs, oferecem uma forma flex√≠vel e eficiente de representar os dados de treinamento. A distin√ß√£o entre prot√≥tipos e amostras de treinamento ressalta a capacidade desses m√©todos de abstrair e generalizar padr√µes a partir dos dados, ao contr√°rio do 1-NN, que usa os pr√≥prios dados de treinamento como refer√™ncia. A escolha do m√©todo mais adequado depende da complexidade dos dados, dos requisitos computacionais e da necessidade de uma representa√ß√£o compacta e generaliz√°vel.

```mermaid
graph LR
    subgraph "Compara√ß√£o dos M√©todos de Classifica√ß√£o"
        direction LR
         A["K-Means"] --> B["Prot√≥tipos: Centr√≥ides"]
         C["LVQ"] --> D["Prot√≥tipos: Ajustados Supervisionados"]
         E["GMMs"] --> F["Prot√≥tipos: Par√¢metros Gaussianos"]
         G["1-NN"] --> H["Prot√≥tipos: Amostras de Treinamento"]
        style B,D,F fill:#ccf,stroke:#333,stroke-width:2px
    end
```

### Footnotes

[^13.2]: "Throughout this chapter, our training data consists of the N pairs ($x_1,g_1$),...,($x_n, g_N$) where $g_i$ is a class label taking values in {1, 2, . . ., K}. Prototype methods represent the training data by a set of points in feature space. These prototypes are typically not examples from the training sample, except in the case of 1-nearest-neighbor classification discussed later. Each prototype has an associated class label, and classification of a query point x is made to the class of the closest prototype." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.1]: "K-means clustering is a method for finding clusters and cluster centers in a set of unlabeled data. One chooses the desired number of cluster centers, say R, and the K-means procedure iteratively moves the centers to minimize the total within cluster variance." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.2]: "In this technique due to Kohonen (1989), prototypes are placed strategically with respect to the decision boundaries in an ad-hoc way. LVQ is an online algorithm-observations are processed one at a time. The idea is that the training points attract prototypes of the correct class, and repel other prototypes." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.3]: "The Gaussian mixture model can also be thought of as a prototype method, similar in spirit to K-means and LVQ. We discuss Gaussian mixtures in some detail in Sections 6.8, 8.5 and 12.7. Each cluster is described in terms of a Gaussian density, which has a centroid (as in K-means), and a covariance matrix." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3]: "These classifiers are memory-based, and require no model to be fit. Given a query point $x_o$, we find the k training points $x^{(r)}$, r = 1,..., k closest in distance to $x_o$, and then classify using majority vote among the k neighbors." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
