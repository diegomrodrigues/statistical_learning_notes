## Fronteiras Irregulares: A Capacidade dos M√©todos de Prot√≥tipos para Modelar Regi√µes de Decis√£o Complexas

```mermaid
graph LR
    subgraph "Decision Boundary Modeling"
    direction TB
        A["Input Data (Features)"] --> B["Linear Methods (LDA, Logistic Regression)"]
        A --> C["Prototype-Based Methods (K-Means, LVQ, GMMs)"]
        B --> D["Linear Decision Boundaries"]
        C --> E["Irregular Decision Boundaries"]
        D --> F["Limited for Complex Data"]
        E --> G["Flexible for Non-Linear Data"]
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora a capacidade dos m√©todos baseados em prot√≥tipos de lidar com **fronteiras de decis√£o irregulares**, um desafio comum em problemas de classifica√ß√£o e reconhecimento de padr√µes [^13.2]. Ao contr√°rio dos m√©todos lineares, que assumem que as classes podem ser separadas por hiperplanos, os m√©todos baseados em prot√≥tipos s√£o capazes de modelar regi√µes de decis√£o mais complexas e n√£o lineares, por meio do posicionamento estrat√©gico dos prot√≥tipos no espa√ßo de *features*. Analisaremos como t√©cnicas como K-Means, LVQ (Learning Vector Quantization) e Misturas Gaussianas (GMMs) podem representar fronteiras irregulares com maior precis√£o do que modelos lineares ou outros m√©todos que dependem de suposi√ß√µes fortes sobre a forma das fronteiras de decis√£o. Abordaremos tamb√©m as limita√ß√µes desses m√©todos e quando outras abordagens podem ser mais apropriadas.

### Fronteiras de Decis√£o: Lineares vs. Irregulares

Em problemas de classifica√ß√£o, o objetivo √© determinar uma **fronteira de decis√£o** que divide o espa√ßo de *features* em regi√µes correspondentes a diferentes classes. Modelos lineares, como a regress√£o log√≠stica ou o LDA (Linear Discriminant Analysis), assumem que as classes podem ser separadas por hiperplanos [^4.1], [^4.3]. Essas abordagens s√£o eficazes quando as classes s√£o linearmente separ√°veis ou aproximadamente separ√°veis, mas podem apresentar dificuldades para modelar fronteiras de decis√£o mais complexas e n√£o lineares.

```mermaid
graph LR
    subgraph "Linear vs Irregular Decision Boundaries"
    direction LR
        A["Linear Decision Boundaries"] --> B["Hyperplane Separations"]
        B --> C["Suitable for Linearly Separable Data"]
        C --> D["Limited Representation Power"]
    
        E["Irregular Decision Boundaries"] --> F["Non-Linear Separations"]
        F --> G["Suitable for Complex Data"]
        G --> H["Requires Flexible Models"]
        end
```

**Fronteiras de decis√£o irregulares** s√£o caracterizadas por regi√µes de decis√£o com formas n√£o lineares e, muitas vezes, complexas [^13.2]. Essas fronteiras podem surgir devido a intera√ß√µes n√£o lineares entre as *features*, distribui√ß√µes complexas das classes, ou ru√≠do presente nos dados. M√©todos lineares podem sofrer de *bias* quando tentam modelar essas fronteiras, pois sua capacidade de representa√ß√£o √© limitada a transforma√ß√µes lineares.

Os m√©todos baseados em prot√≥tipos, por outro lado, s√£o capazes de modelar fronteiras de decis√£o irregulares, posicionando os prot√≥tipos de forma estrat√©gica no espa√ßo de *features* para capturar a forma dessas regi√µes. Essa capacidade de modelar fronteiras complexas √© uma vantagem significativa em rela√ß√£o a modelos lineares, tornando os m√©todos de prot√≥tipos mais flex√≠veis e adapt√°veis a diversos problemas.

**Lemma 21:** A capacidade de modelar fronteiras de decis√£o irregulares permite que m√©todos baseados em prot√≥tipos capturem padr√µes complexos nos dados que n√£o podem ser representados por modelos lineares.
*Prova*: Ao posicionar prot√≥tipos de forma estrat√©gica, m√©todos como LVQ e GMM conseguem capturar regi√µes de decis√£o n√£o lineares, o que modelos baseados em hiperplanos n√£o conseguem. $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 21: Prototype Methods Capture Irregular Boundaries"
    direction TB
        A["Prototype Methods"] --> B["Strategic Prototype Placement"]
        B --> C["Non-Linear Decision Regions"]
        C --> D["Capture Complex Patterns"]
        E["Linear Models"] --> F["Hyperplane Based"]
        F --> G["Limited to Linear Separations"]
        D --> H["Superiority over Linear Models"]
    end
```

**Corol√°rio 21:** A escolha entre modelos lineares e m√©todos baseados em prot√≥tipos depende da complexidade das fronteiras de decis√£o no problema em quest√£o, sendo os m√©todos baseados em prot√≥tipos mais apropriados para dados n√£o linearmente separ√°veis.

> ‚ö†Ô∏è **Nota Importante**:  Modelos lineares assumem fronteiras de decis√£o lineares, o que limita sua capacidade de representar regi√µes de decis√£o complexas e irregulares.

> ‚ùó **Ponto de Aten√ß√£o**: A capacidade de modelar fronteiras irregulares √© uma vantagem fundamental de m√©todos baseados em prot√≥tipos, tornando-os mais adequados para problemas com alta complexidade nas distribui√ß√µes de classes.

### M√©todos de Prot√≥tipos e sua Capacidade de Modelagem de Fronteiras Irregulares

**K-Means:** O **K-Means**, embora seja um m√©todo de *clustering*, pode ser usado para modelar fronteiras de decis√£o irregulares aplicando o algoritmo separadamente a cada classe e utilizando os centros dos clusters como prot√≥tipos [^13.2.1]. O K-Means n√£o ajusta os prot√≥tipos para discriminar as classes, e as fronteiras de decis√£o resultantes s√£o geralmente lineares ou piecewise lineares (fronteiras formadas por segmentos de linha reta), que podem aproximar regi√µes irregulares, mas pode n√£o ser a escolha ideal.

> üí° **Exemplo Num√©rico:**
>
> Imagine que temos duas classes de dados em 2D, onde a classe A forma um c√≠rculo e a classe B envolve esse c√≠rculo. Se aplicarmos o K-Means separadamente a cada classe com k=2, poder√≠amos ter 2 prot√≥tipos para A e 2 para B. As fronteiras de decis√£o seriam retas, dividindo o espa√ßo em 4 regi√µes. Essas retas n√£o capturam a forma circular real das classes, demonstrando que o K-Means n√£o modela bem a fronteira irregular nesse caso.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.cluster import KMeans
> from sklearn.preprocessing import StandardScaler
>
> # Gerando dados sint√©ticos para duas classes
> np.random.seed(0)
> radius = 5
> n_samples_circle = 200
> n_samples_ring = 300
>
> # Classe A (c√≠rculo)
> circle_angles = np.random.uniform(0, 2*np.pi, n_samples_circle)
> circle_radius = np.random.normal(radius, 0.5, n_samples_circle)
> circle_x = circle_radius * np.cos(circle_angles)
> circle_y = circle_radius * np.sin(circle_angles)
> class_A = np.column_stack((circle_x, circle_y))
>
> # Classe B (anel)
> ring_angles = np.random.uniform(0, 2*np.pi, n_samples_ring)
> ring_radius = np.random.normal(radius*1.8, 0.5, n_samples_ring)
> ring_x = ring_radius * np.cos(ring_angles)
> ring_y = ring_radius * np.sin(ring_angles)
> class_B = np.column_stack((ring_x, ring_y))
>
> # Combinando os dados
> X = np.vstack((class_A, class_B))
> y = np.array([0] * n_samples_circle + [1] * n_samples_ring)
>
> # Normalizando os dados
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)
>
> # K-Means para cada classe separadamente
> kmeans_A = KMeans(n_clusters=2, random_state=0, n_init=10)
> kmeans_B = KMeans(n_clusters=2, random_state=0, n_init=10)
>
> kmeans_A.fit(X_scaled[y == 0])
> kmeans_B.fit(X_scaled[y == 1])
>
> centroids_A = kmeans_A.cluster_centers_
> centroids_B = kmeans_B.cluster_centers_
>
> centroids = np.vstack((centroids_A, centroids_B))
>
> # Plotando os dados e os prot√≥tipos
> plt.figure(figsize=(8, 6))
> plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], label='Classe A', color='skyblue', alpha=0.6)
> plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], label='Classe B', color='coral', alpha=0.6)
> plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200, c=['blue', 'blue', 'red', 'red'], label='Prot√≥tipos K-Means')
>
> # Criando uma grade para visualizar as fronteiras de decis√£o
> x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
> y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
>
> # Classificando os pontos da grade usando os prot√≥tipos
> grid_points = np.c_[xx.ravel(), yy.ravel()]
> distances = np.sqrt(((grid_points[:, :, np.newaxis] - centroids.T) ** 2).sum(axis=1))
> closest_prototype = np.argmin(distances, axis=1)
> Z = (closest_prototype >= 2).reshape(xx.shape)
>
> # Plotando as fronteiras de decis√£o
> plt.contourf(xx, yy, Z, alpha=0.2, cmap=plt.cm.RdBu)
>
> plt.title('K-Means para Fronteiras Irregulares')
> plt.xlabel('Feature 1 (Normalizado)')
> plt.ylabel('Feature 2 (Normalizado)')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Este exemplo mostra que, mesmo com m√∫ltiplos prot√≥tipos por classe, as fronteiras de decis√£o do K-Means s√£o lineares e n√£o capturam a complexidade das formas das classes.

```mermaid
graph LR
    subgraph "K-Means for Classification"
        direction TB
        A["Apply K-Means per class"] --> B["Cluster Centroids as Prototypes"]
        B --> C["Distance-based Classification"]
        C --> D["Piecewise Linear Decision Boundaries"]
        D --> E["Limitations for Irregular Regions"]
    end
```

**Lemma 22:** As fronteiras de decis√£o do K-Means quando usado para classifica√ß√£o podem ser aproximadas por pol√≠gonos convexos, e n√£o necessariamente refletem a complexidade real das fronteiras de decis√£o.
*Prova*: Cada regi√£o de decis√£o do K-means √© definida pela proximidade ao centroide, e a fronteira entre regi√µes √© o lugar geom√©trico onde as dist√¢ncias aos centros mais pr√≥ximos se igualam, o que leva a fronteiras piecewise lineares. $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 22: K-Means Decision Boundaries"
        direction TB
        A["K-Means Decision Regions"] --> B["Proximity to Centroids"]
        B --> C["Boundaries: Equal Distances"]
        C --> D["Piecewise Linear Approximations"]
        D --> E["Convex Polygons"]
        E --> F["May not match true complex boundaries"]
    end
```

**Learning Vector Quantization (LVQ):** O **LVQ** √© mais eficaz do que o K-Means para modelar fronteiras de decis√£o irregulares, pois ele ajusta os prot√≥tipos com base nas classes dos dados, buscando posicion√°-los estrategicamente em rela√ß√£o √†s fronteiras de decis√£o [^13.2.2]. Ao mover os prot√≥tipos na dire√ß√£o de pontos da mesma classe e afast√°-los de pontos de classes diferentes, o LVQ √© capaz de criar regi√µes de decis√£o mais complexas e n√£o lineares. O LVQ gera fronteiras de decis√£o mais precisas e pr√≥ximas das reais, em rela√ß√£o ao K-Means.

> üí° **Exemplo Num√©rico:**
>
> Usando os mesmos dados do exemplo anterior, o LVQ ajustaria os prot√≥tipos (inicialmente posicionados aleatoriamente) iterativamente. Prot√≥tipos da classe A seriam atra√≠dos para o c√≠rculo e repelidos do anel, e vice-versa. Ap√≥s algumas itera√ß√µes, os prot√≥tipos se posicionariam de forma a melhor delinear a fronteira circular entre as classes, resultando em uma fronteira de decis√£o mais precisa do que o K-Means.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn_lvq import Gmlvq
> from sklearn.preprocessing import StandardScaler
>
> # Gerando dados sint√©ticos para duas classes (mesmo do exemplo K-Means)
> np.random.seed(0)
> radius = 5
> n_samples_circle = 200
> n_samples_ring = 300
>
> circle_angles = np.random.uniform(0, 2*np.pi, n_samples_circle)
> circle_radius = np.random.normal(radius, 0.5, n_samples_circle)
> circle_x = circle_radius * np.cos(circle_angles)
> circle_y = circle_radius * np.sin(circle_angles)
> class_A = np.column_stack((circle_x, circle_y))
>
> ring_angles = np.random.uniform(0, 2*np.pi, n_samples_ring)
> ring_radius = np.random.normal(radius*1.8, 0.5, n_samples_ring)
> ring_x = ring_radius * np.cos(ring_angles)
> ring_y = ring_radius * np.sin(ring_angles)
> class_B = np.column_stack((ring_x, ring_y))
>
> X = np.vstack((class_A, class_B))
> y = np.array([0] * n_samples_circle + [1] * n_samples_ring)
>
> # Normalizando os dados
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)
>
> # Treinando o LVQ
> lvq = Gmlvq(prototypes_per_class=2, random_state=0)
> lvq.fit(X_scaled, y)
>
> prototypes = lvq.prototypes_
>
> # Plotando os dados e os prot√≥tipos
> plt.figure(figsize=(8, 6))
> plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], label='Classe A', color='skyblue', alpha=0.6)
> plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], label='Classe B', color='coral', alpha=0.6)
> plt.scatter(prototypes[:, 0], prototypes[:, 1], marker='X', s=200, c=['blue', 'blue', 'red', 'red'], label='Prot√≥tipos LVQ')
>
> # Criando uma grade para visualizar as fronteiras de decis√£o
> x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
> y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
>
> # Classificando os pontos da grade usando o LVQ
> grid_points = np.c_[xx.ravel(), yy.ravel()]
> Z = lvq.predict(grid_points).reshape(xx.shape)
>
> # Plotando as fronteiras de decis√£o
> plt.contourf(xx, yy, Z, alpha=0.2, cmap=plt.cm.RdBu)
>
> plt.title('LVQ para Fronteiras Irregulares')
> plt.xlabel('Feature 1 (Normalizado)')
> plt.ylabel('Feature 2 (Normalizado)')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> ```
>
> Este exemplo ilustra como o LVQ consegue posicionar os prot√≥tipos de forma mais adequada para modelar a fronteira de decis√£o complexa, aproximando-se melhor da forma circular.

```mermaid
graph LR
    subgraph "Learning Vector Quantization (LVQ)"
    direction TB
        A["Initialize Prototypes"] --> B["Iterative Adjustment"]
        B --> C["Attract to Same Class Points"]
        B --> D["Repel from Different Class Points"]
        C & D --> E["Strategic Prototype Positioning"]
        E --> F["Complex, Non-Linear Decision Regions"]
        F --> G["More Accurate Boundaries than K-Means"]
    end
```

**Corol√°rio 22:** A capacidade do LVQ de ajustar iterativamente os prot√≥tipos com base nos r√≥tulos de classe permite que ele capture formas complexas nas fronteiras de decis√£o, melhorando a capacidade discriminat√≥ria do modelo.

```mermaid
graph LR
    subgraph "Corollary 22: LVQ Adaptability"
    direction TB
        A["Iterative Prototype Adjustment"] --> B["Class Label Feedback"]
        B --> C["Capture Complex Boundary Shapes"]
        C --> D["Improved Discriminatory Power"]
    end
```

**Misturas Gaussianas (GMMs):** As **GMMs** tamb√©m s√£o capazes de modelar fronteiras de decis√£o irregulares, usando componentes gaussianas para representar as distribui√ß√µes das classes [^13.2.3]. A combina√ß√£o de m√∫ltiplas gaussianas com diferentes m√©dias e covari√¢ncias permite que as GMMs aproximem formas complexas no espa√ßo de *features*. A classifica√ß√£o de um novo ponto √© feita com base na probabilidade de pertencer a cada componente gaussiana, o que pode gerar fronteiras de decis√£o suaves e n√£o lineares.

> üí° **Exemplo Num√©rico:**
>
> Novamente utilizando os mesmos dados, as GMMs modelariam as classes A e B usando uma mistura de gaussianas. A classe A poderia ser representada por uma gaussiana centralizada no c√≠rculo, e a classe B por uma mistura de gaussianas formando um anel ao redor. A probabilidade de um novo ponto pertencer a cada classe seria calculada com base nessas gaussianas, gerando uma fronteira de decis√£o suave e n√£o linear que acompanha a forma das classes.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.mixture import GaussianMixture
> from sklearn.preprocessing import StandardScaler
>
> # Gerando dados sint√©ticos para duas classes (mesmo do exemplo K-Means e LVQ)
> np.random.seed(0)
> radius = 5
> n_samples_circle = 200
> n_samples_ring = 300
>
> circle_angles = np.random.uniform(0, 2*np.pi, n_samples_circle)
> circle_radius = np.random.normal(radius, 0.5, n_samples_circle)
> circle_x = circle_radius * np.cos(circle_angles)
> circle_y = circle_radius * np.sin(circle_angles)
> class_A = np.column_stack((circle_x, circle_y))
>
> ring_angles = np.random.uniform(0, 2*np.pi, n_samples_ring)
> ring_radius = np.random.normal(radius*1.8, 0.5, n_samples_ring)
> ring_x = ring_radius * np.cos(ring_angles)
> ring_y = ring_radius * np.sin(ring_angles)
> class_B = np.column_stack((ring_x, ring_y))
>
> X = np.vstack((class_A, class_B))
> y = np.array([0] * n_samples_circle + [1] * n_samples_ring)
>
> # Normalizando os dados
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)
>
> # Treinando a GMM
> gmm = GaussianMixture(n_components=2, random_state=0, covariance_type='full')
> gmm.fit(X_scaled)
>
> # Plotando os dados
> plt.figure(figsize=(8, 6))
> plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], label='Classe A', color='skyblue', alpha=0.6)
> plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], label='Classe B', color='coral', alpha=0.6)
>
> # Criando uma grade para visualizar as fronteiras de decis√£o
> x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
> y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
>
> # Classificando os pontos da grade usando a GMM
> grid_points = np.c_[xx.ravel(), yy.ravel()]
> Z = gmm.predict(grid_points).reshape(xx.shape)
>
> # Plotando as fronteiras de decis√£o
> plt.contourf(xx, yy, Z, alpha=0.2, cmap=plt.cm.RdBu)
>
> plt.title('GMM para Fronteiras Irregulares')
> plt.xlabel('Feature 1 (Normalizado)')
> plt.ylabel('Feature 2 (Normalizado)')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Este exemplo mostra como as GMMs conseguem modelar fronteiras de decis√£o suaves e n√£o lineares, adaptando-se bem √† forma complexa das classes.

```mermaid
graph LR
    subgraph "Gaussian Mixture Models (GMMs)"
        direction TB
        A["Gaussian Components"] --> B["Class Distribution Modeling"]
        B --> C["Multiple Gaussian Mixtures"]
        C --> D["Varying Means and Covariances"]
        D --> E["Approximating Complex Shapes"]
        E --> F["Soft, Non-Linear Decision Boundaries"]
    end
```

> ‚ö†Ô∏è **Nota Importante**: O LVQ e as GMMs s√£o mais eficazes do que o K-Means para modelar fronteiras de decis√£o irregulares, pois eles incorporam informa√ß√µes sobre os r√≥tulos de classe no processo de aprendizado dos prot√≥tipos.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha do n√∫mero de prot√≥tipos e o m√©todo de otimiza√ß√£o (LVQ ou EM) influenciam a capacidade dos m√©todos de prot√≥tipos de representar as fronteiras de decis√£o com precis√£o.

### k-Vizinhos Mais Pr√≥ximos (k-NN): Uma Abordagem Alternativa para Fronteiras Irregulares

O m√©todo de **k-Vizinhos Mais Pr√≥ximos (k-NN)** oferece uma abordagem alternativa para modelar fronteiras de decis√£o irregulares, sem depender de um conjunto de prot√≥tipos [^13.3]. O k-NN usa os pr√≥prios dados de treinamento para classificar um novo ponto, atribuindo a ele a classe majorit√°ria entre seus $k$ vizinhos mais pr√≥ximos. A flexibilidade do k-NN em modelar fronteiras de decis√£o irregulares est√° relacionada com o fato de que ele n√£o assume nenhuma forma espec√≠fica para a fronteira, adaptando-se √† distribui√ß√£o dos dados.

> üí° **Exemplo Num√©rico:**
>
> Usando os mesmos dados das classes A e B, o k-NN classificaria um novo ponto com base nos r√≥tulos dos seus k vizinhos mais pr√≥ximos no conjunto de treinamento. Para k=1, a fronteira de decis√£o seria altamente irregular, seguindo a forma dos pontos de treinamento. Para valores maiores de k, a fronteira se tornaria mais suave.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.neighbors import KNeighborsClassifier
> from sklearn.preprocessing import StandardScaler
>
> # Gerando dados sint√©ticos para duas classes (mesmo do exemplo K-Means, LVQ e GMM)
> np.random.seed(0)
> radius = 5
> n_samples_circle = 200
> n_samples_ring = 300
>
> circle_angles = np.random.uniform(0, 2*np.pi, n_samples_circle)
> circle_radius = np.random.normal(radius, 0.5, n_samples_circle)
> circle_x = circle_radius * np.cos(circle_angles)
> circle_y = circle_radius * np.sin(circle_angles)
> class_A = np.column_stack((circle_x, circle_y))
>
> ring_angles = np.random.uniform(0, 2*np.pi, n_samples_ring)
> ring_radius = np.random.normal(radius*1.8, 0.5, n_samples_ring)
> ring_x = ring_radius * np.cos(ring_angles)
> ring_y = ring_radius * np.sin(ring_angles)
> class_B = np.column_stack((ring_x, ring_y))
>
> X = np.vstack((class_A, class_B))
> y = np.array([0] * n_samples_circle + [1] * n_samples_ring)
>
> # Normalizando os dados
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)
>
> # Treinando o k-NN
> knn = KNeighborsClassifier(n_neighbors=5) # Usando k=5
> knn.fit(X_scaled, y)
>
> # Plotando os dados
> plt.figure(figsize=(8, 6))
> plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], label='Classe A', color='skyblue', alpha=0.6)
> plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], label='Classe B', color='coral', alpha=0.6)
>
> # Criando uma grade para visualizar as fronteiras de decis√£o
> x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
> y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
>
> # Classificando os pontos da grade usando o k-NN
> grid_points = np.c_[xx.ravel(), yy.ravel()]
> Z = knn.predict(grid_points).reshape(xx.shape)
>
> # Plotando as fronteiras de decis√£o
> plt.contourf(xx, yy, Z, alpha=0.2, cmap=plt.cm.RdBu)
>
> plt.title('k-NN para Fronteiras Irregulares')
> plt.xlabel('Feature 1 (Normalizado)')
> plt.ylabel('Feature 2 (Normalizado)')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Este exemplo demonstra como o k-NN se adapta bem a fronteiras irregulares, com a forma da fronteira de decis√£o dependendo da distribui√ß√£o dos dados de treinamento e do valor de k.

```mermaid
graph LR
    subgraph "k-Nearest Neighbors (k-NN)"
        direction TB
        A["No Explicit Prototypes"] --> B["Classification: k Nearest Neighbors"]
        B --> C["Majority Class among Neighbors"]
        C --> D["Adaptive Decision Boundaries"]
        D --> E["No Assumptions on Boundary Shape"]
        E --> F["Flexibility in Handling Irregularities"]
    end
```

O k-NN pode se sair muito bem em problemas com fronteiras de decis√£o irregulares, mas tamb√©m apresenta algumas limita√ß√µes. A complexidade computacional do k-NN √© maior do que os m√©todos de prot√≥tipos, especialmente quando o conjunto de treinamento √© grande, e a escolha do valor de $k$ e da m√©trica de dist√¢ncia podem afetar significativamente o desempenho do modelo. A maldi√ß√£o da dimensionalidade tamb√©m pode ser um problema para k-NN em problemas com alta dimensionalidade.

```mermaid
graph LR
    subgraph "k-NN Limitations"
    direction LR
        A["k-NN"] --> B["High Computational Cost"]
        B --> C["Large Training Sets"]
        A --> D["Parameter Sensitivity"]
        D --> E["Choice of 'k' and Distance Metrics"]
        A --> F["Curse of Dimensionality"]
    end
```

**Lemma 23:** A complexidade do k-NN em modelar fronteiras de decis√£o irregulares depende do n√∫mero de pontos no conjunto de treinamento e da complexidade da distribui√ß√£o de dados, tendo dificuldade em generalizar para regi√µes com poucos pontos de treino.
*Prova*: O k-NN n√£o generaliza por si s√≥, a forma da fronteira de decis√£o √© definida pelos pontos pr√≥ximos, e a capacidade de adapta√ß√£o √† irregularidade dos dados √© diretamente proporcional √† densidade do conjunto de dados de treino. $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 23: k-NN and Data Density"
        direction TB
        A["k-NN Model Complexity"] --> B["Number of Training Points"]
        B --> C["Complexity of Data Distribution"]
        C --> D["Difficulty in Generalizing Low Density Regions"]
        D --> E["Decision Boundary defined by local point density"]
    end
```

**Corol√°rio 23:** A escolha entre k-NN e m√©todos de prot√≥tipos para modelar fronteiras irregulares deve levar em considera√ß√£o o tamanho do conjunto de dados, a necessidade de uma representa√ß√£o compacta, a complexidade das fronteiras de decis√£o e a necessidade de evitar o *overfitting*.

> ‚ö†Ô∏è **Nota Importante**: O k-NN √© uma abordagem n√£o param√©trica que se adapta bem a fronteiras irregulares, mas pode ser computacionalmente mais custoso do que m√©todos de prot√≥tipos para grandes conjuntos de dados.

> ‚ùó **Ponto de Aten√ß√£o**:  A escolha do valor de $k$ e a m√©trica de dist√¢ncia apropriada podem melhorar a capacidade do k-NN de modelar fronteiras irregulares.

### Conclus√£o

Os m√©todos baseados em prot√≥tipos oferecem uma forma eficaz e flex√≠vel de modelar fronteiras de decis√£o irregulares, usando o posicionamento estrat√©gico dos prot√≥tipos no espa√ßo de *features*. T√©cnicas como o LVQ e as GMMs, em particular, s√£o mais eficazes do que o K-Means para modelar fronteiras complexas, pois incorporam informa√ß√µes sobre os r√≥tulos de classe. O k-NN oferece uma abordagem alternativa que se adapta bem a fronteiras irregulares, mas pode ser computacionalmente mais custoso do que m√©todos de prot√≥tipos. A escolha do m√©todo mais apropriado depende da complexidade do problema, da quantidade de dados e dos recursos computacionais dispon√≠veis. A compreens√£o das vantagens e limita√ß√µes de cada m√©todo √© crucial para obter um desempenho eficaz em diferentes aplica√ß√µes.

### Footnotes

[^13.2]: "Throughout this chapter, our training data consists of the N pairs (x1,91),...,(xn, 9N) where gi is a class label taking values in {1, 2, . . ., K}. Prototype methods represent the training data by a set of points in feature space. These prototypes are typically not examples from the training sample, except in the case of 1-nearest-neighbor classification discussed later. Each prototype has an associated class label, and classification of a query point x is made to the class of the closest prototype. "Closest" is usually defined by Euclidean distance in the feature space, after each feature has been standardized to have overall mean 0 and variance 1 in the training sample." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.1]: "K-means clustering is a method for finding clusters and cluster centers in a set of unlabeled data... To use K-means clustering for classification of labeled data, the steps are: apply K-means clustering to the training data in each class separately, using R prototypes per class; assign a class label to each of the K √ó R prototypes; classify a new feature x to the class of the closest prototype." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.2]: "In this technique due to Kohonen (1989), prototypes are placed strategically with respect to the decision boundaries in an ad-hoc way...The idea is that the training points attract prototypes of the correct class, and repel other prototypes...When the iterations settle down, prototypes should be close to the training points in their class." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.3]: "The Gaussian mixture model can also be thought of as a prototype method, similar in spirit to K-means and LVQ...Each cluster is described in terms of a Gaussian density, which has a centroid (as in K-means), and a covariance matrix...when Gaussian mixture models are used to represent the feature density in each class, it produces smooth posterior probabilities." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3]: "These classifiers are memory-based, and require no model to be fit. Given a query point xo, we find the k training points x(r), r = 1,..., k closest in distance to xo, and then classify using majority vote among the k neighbors." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^4.1]: "This chapter focuses on linear methods for classification, which are linear in the parameters and lead to linear decision boundaries." *(Trecho de "4. Linear Methods for Classification")*

[^4.3]: "Linear Discriminant Analysis (LDA) is a classical method for classification. It assumes that the classes are normally distributed and that their covariance matrices are the same." *(Trecho de "4. Linear Methods for Classification")*
