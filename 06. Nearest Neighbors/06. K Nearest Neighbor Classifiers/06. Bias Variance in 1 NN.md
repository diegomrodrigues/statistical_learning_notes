## Vi√©s e Vari√¢ncia no k-NN: O Caso do 1-NN com Baixo Vi√©s e Alta Vari√¢ncia

```mermaid
graph LR
    subgraph "Bias-Variance Tradeoff"
        direction LR
        A["Model Complexity"] --> B["Low Bias"]
        A --> C["High Variance"]
         B --> D("Overfitting")
        C --> D
        A --> E["High Bias"]
        A --> F["Low Variance"]
        E --> G("Underfitting")
        F --> G
         D --> H["Optimal Point"]
        G --> H
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora os conceitos de **vi√©s** e **vari√¢ncia** no contexto do m√©todo de **1-vizinho mais pr√≥ximo (1-NN)**, destacando como este algoritmo apresenta uma caracter√≠stica particular: **baixo vi√©s e alta vari√¢ncia** [^13.3]. Analisaremos como a natureza do 1-NN, que se baseia na informa√ß√£o de um √∫nico vizinho mais pr√≥ximo, impacta seu vi√©s (erro sistem√°tico) e vari√¢ncia (sensibilidade √†s varia√ß√µes no conjunto de treinamento). Compreenderemos como essa rela√ß√£o entre vi√©s e vari√¢ncia √© fundamental para o desempenho do 1-NN e como essa rela√ß√£o se manifesta tamb√©m em outros modelos de aprendizado de m√°quina.

### Vi√©s e Vari√¢ncia: Descrevendo o Erro de um Modelo

Os conceitos de **vi√©s** e **vari√¢ncia** s√£o ferramentas importantes para analisar o erro de modelos de aprendizado de m√°quina, e representam diferentes tipos de erro que um modelo pode apresentar.

1.  **Vi√©s (Bias):** O vi√©s representa o erro sistem√°tico de um modelo. Um modelo com alto vi√©s tende a simplificar demais a rela√ß√£o entre as *features* e o resultado, levando a erros sistem√°ticos e a *underfitting*. Um modelo com baixo vi√©s se adapta bem aos dados de treinamento e √© capaz de modelar rela√ß√µes complexas, mas pode ser mais suscet√≠vel a *overfitting*.

2.  **Vari√¢ncia (Variance):** A vari√¢ncia representa a sensibilidade de um modelo a flutua√ß√µes aleat√≥rias no conjunto de treinamento. Um modelo com alta vari√¢ncia √© sens√≠vel a pequenas mudan√ßas nos dados de treinamento, o que pode levar a uma alta variabilidade nas predi√ß√µes, e a *overfitting*. Um modelo com baixa vari√¢ncia √© mais est√°vel e suas predi√ß√µes n√£o variam muito de acordo com o conjunto de treinamento.

O ideal √© que um modelo apresente baixo vi√©s e baixa vari√¢ncia, mas na pr√°tica, existe um *tradeoff* entre esses dois tipos de erro. A escolha do modelo mais adequado deve levar em considera√ß√£o esse *tradeoff*, e o objetivo √© encontrar um modelo que equilibre o vi√©s e a vari√¢ncia para minimizar o erro total.

```mermaid
graph TD
    subgraph "Error Decomposition"
        direction TB
        A["Total Error"] --> B["Bias¬≤: (E[fÃÇ(x)] - f(x))¬≤"]
        A --> C["Variance: E[(fÃÇ(x) - E[fÃÇ(x)])¬≤]"]
        A --> D["Irreducible Error: var(Œµ)"]
    end
```

**Lemma 93:** O vi√©s e a vari√¢ncia s√£o componentes do erro total de um modelo de aprendizado de m√°quina, e representam diferentes tipos de erro: um erro sistem√°tico (vi√©s) e a sensibilidade a flutua√ß√µes nos dados de treino (vari√¢ncia).
*Prova*: O erro total de um modelo pode ser decomposto em tr√™s componentes: o vi√©s (bias), a vari√¢ncia e o erro irredut√≠vel. $\blacksquare$

**Corol√°rio 93:** O *tradeoff* entre vi√©s e vari√¢ncia √© fundamental para a escolha do modelo mais adequado para um determinado problema, e o objetivo √© minimizar o erro total por meio do ajuste do modelo.

> ‚ö†Ô∏è **Nota Importante**: O vi√©s representa o erro sistem√°tico do modelo, enquanto a vari√¢ncia representa a sensibilidade do modelo a varia√ß√µes no conjunto de treinamento.

> ‚ùó **Ponto de Aten√ß√£o**:  O objetivo em aprendizado de m√°quina √© encontrar um modelo que minimize tanto o vi√©s quanto a vari√¢ncia, o que geralmente envolve um compromisso entre esses dois tipos de erro.

### O 1-NN: Baixo Vi√©s e Alta Vari√¢ncia

O m√©todo de **1-vizinho mais pr√≥ximo (1-NN)** √© um exemplo de um classificador que apresenta **baixo vi√©s e alta vari√¢ncia** [^13.3]. A raz√£o para essa caracter√≠stica est√° na forma como o 1-NN realiza a classifica√ß√£o: ele atribui um novo ponto √† classe do √∫nico ponto de treinamento mais pr√≥ximo, sem considerar outros vizinhos ou nenhuma estrutura global dos dados.

1.  **Baixo Vi√©s:** O baixo vi√©s do 1-NN se deve √† sua capacidade de se adaptar a qualquer forma da fronteira de decis√£o entre as classes. Ao classificar um ponto de consulta com base em um √∫nico vizinho mais pr√≥ximo, o 1-NN se ajusta perfeitamente aos dados de treinamento e captura at√© as menores varia√ß√µes nas regi√µes de classes, e n√£o introduz um vi√©s do modelo. Isso faz com que a fronteira de decis√£o do 1-NN seja muito irregular, mas com baixo vi√©s.
2.  **Alta Vari√¢ncia:** A alta vari√¢ncia do 1-NN se deve √† sua sensibilidade a pequenas mudan√ßas no conjunto de treinamento. Se o ponto de treinamento mais pr√≥ximo de um ponto de consulta mudar devido a uma pequena altera√ß√£o nos dados de treinamento, a classifica√ß√£o pode ser diferente. Essa sensibilidade √†s flutua√ß√µes nos dados de treinamento torna o 1-NN um modelo com alta vari√¢ncia.

A combina√ß√£o de baixo vi√©s e alta vari√¢ncia faz com que o 1-NN seja um modelo que se adapta bem aos dados de treinamento, mas com um alto risco de *overfitting*. O modelo pode apresentar excelentes resultados no conjunto de treinamento, mas seu desempenho em dados n√£o vistos pode ser ruim, pois √© sens√≠vel ao ru√≠do e √†s caracter√≠sticas de cada amostra em particular.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um cen√°rio de classifica√ß√£o bin√°ria com duas classes (A e B). Temos um conjunto de treinamento pequeno com 5 pontos:
>
> - Classe A: (1, 1), (2, 2)
> - Classe B: (4, 4), (5, 5), (6, 6)
>
> Agora, vamos analisar como o 1-NN se comporta com diferentes conjuntos de treinamento ligeiramente alterados.
>
> **Conjunto de Treinamento 1 (Original):**
>
> ```
> Classe A: (1, 1), (2, 2)
> Classe B: (4, 4), (5, 5), (6, 6)
> ```
>
> Um novo ponto (3, 3) seria classificado como Classe A, pois o ponto (2, 2) √© o mais pr√≥ximo.
>
> **Conjunto de Treinamento 2 (com pequena mudan√ßa):**
>
> Vamos adicionar um novo ponto da classe A e remover um ponto da classe B.
>
> ```
> Classe A: (1, 1), (2, 2), (3.5, 3.5)
> Classe B: (4, 4), (5, 5)
> ```
>
> O mesmo ponto (3, 3) agora seria classificado como Classe B, pois (4, 4) √© o ponto mais pr√≥ximo.
>
> Essa mudan√ßa na classifica√ß√£o de um mesmo ponto, devido a uma pequena altera√ß√£o no conjunto de treinamento, ilustra a **alta vari√¢ncia** do 1-NN. O modelo se adaptou muito ao novo ponto (3.5, 3.5), alterando a decis√£o para pontos pr√≥ximos. O modelo tem **baixo vi√©s** porque ele consegue classificar corretamente os pontos do conjunto de treinamento, mas a alta vari√¢ncia o torna inst√°vel.

```mermaid
graph LR
    subgraph "1-NN Classification Process"
        direction TB
        A["Input Query Point (x_q)"] --> B["Find Nearest Neighbor (x_n) in Training Set"]
        B --> C["Assign Class of x_n to x_q"]
         C --> D["Output Class Label"]
    end
```

**Lemma 94:** O 1-NN apresenta baixo vi√©s devido √† sua capacidade de se adaptar √† forma dos dados, mas apresenta alta vari√¢ncia devido √† sua sensibilidade a flutua√ß√µes no conjunto de treinamento, que pode gerar instabilidade em sua capacidade de generaliza√ß√£o.
*Prova*: O baixo vi√©s surge da escolha do vizinho mais pr√≥ximo como representativo da regi√£o local, o que garante um ajuste ao conjunto de treino, mas a alta vari√¢ncia √© consequ√™ncia da instabilidade na escolha desse vizinho. $\blacksquare$

**Corol√°rio 94:** A combina√ß√£o de baixo vi√©s e alta vari√¢ncia faz com que o 1-NN seja um modelo com boa capacidade de ajuste a dados complexos, mas tamb√©m com um risco de *overfitting*.

> ‚ö†Ô∏è **Nota Importante**:  O 1-NN apresenta baixo vi√©s, pois se ajusta bem aos dados de treinamento, mas apresenta alta vari√¢ncia devido √† sua sensibilidade a flutua√ß√µes nos dados.

> ‚ùó **Ponto de Aten√ß√£o**:  O 1-NN pode apresentar bom desempenho no conjunto de treinamento, mas seu desempenho em dados n√£o vistos √© afetado por sua alta vari√¢ncia.

### Rela√ß√£o entre Vi√©s e Vari√¢ncia no k-NN: O Papel do Par√¢metro *k*

No contexto do k-NN, o par√¢metro $k$ (n√∫mero de vizinhos) desempenha um papel fundamental na rela√ß√£o entre vi√©s e vari√¢ncia [^13.3].

1.  **Valores Pequenos de k:** Valores pequenos de $k$ (ex: $k=1$) tornam o modelo mais sens√≠vel a detalhes do conjunto de treinamento, como ru√≠do e *outliers*, o que resulta em modelos de baixo vi√©s e alta vari√¢ncia. O modelo se ajusta de forma muito precisa aos dados de treinamento, mas tem dificuldade para generalizar.
2.  **Valores Grandes de k:** Valores grandes de $k$ fazem com que o modelo considere um n√∫mero maior de vizinhos para tomar a decis√£o, o que leva a modelos mais est√°veis e menos sens√≠veis a flutua√ß√µes nos dados de treinamento, reduzindo sua vari√¢ncia. No entanto, essa escolha tamb√©m aumenta o vi√©s do modelo, pois regi√µes mais amplas no espa√ßo de *features* s√£o consideradas no momento da classifica√ß√£o, o que faz com que o modelo n√£o capture a estrutura local dos dados.

A escolha do valor ideal de $k$ no k-NN envolve um *tradeoff* entre vi√©s e vari√¢ncia: valores pequenos levam a modelos com baixo vi√©s e alta vari√¢ncia, enquanto valores grandes levam a modelos com alto vi√©s e baixa vari√¢ncia. O objetivo √© escolher um valor de $k$ que equilibre o vi√©s e a vari√¢ncia, de forma a minimizar o erro total de generaliza√ß√£o do modelo. M√©todos de valida√ß√£o cruzada podem ser usados para escolher o valor √≥timo do par√¢metro $k$.

```mermaid
graph LR
    subgraph "k-NN Parameter Influence"
        direction LR
        A["Small k"] --> B["Low Bias"]
        A --> C["High Variance"]
        B --> D["Overfitting"]
         C --> D
        A --> E["Large k"]
         E --> F["High Bias"]
        E --> G["Low Variance"]
          F --> H["Underfitting"]
        G --> H
    end
```

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o mesmo conjunto de dados do exemplo anterior, mas agora com k=3:
>
> **Conjunto de Treinamento:**
>
> ```
> Classe A: (1, 1), (2, 2), (3.5, 3.5)
> Classe B: (4, 4), (5, 5), (6, 6)
> ```
>
> Para classificar o ponto (3, 3) com k=3, calculamos os 3 vizinhos mais pr√≥ximos:
>
> 1.  (2, 2) - Classe A
> 2.  (3.5, 3.5) - Classe A
> 3.  (4, 4) - Classe B
>
> A classe mais frequente entre os 3 vizinhos √© A (2 votos contra 1 de B), portanto, o ponto (3, 3) √© classificado como Classe A.
>
> Agora, vamos comparar com o caso do 1-NN:
>
> | k | Ponto (3,3) | Classifica√ß√£o |
> |---|---|---|
> | 1 | (3,3) | B |
> | 3 | (3,3) | A |
>
> Com k=3, o modelo se torna menos sens√≠vel a pequenas varia√ß√µes no conjunto de treinamento, reduzindo a vari√¢ncia. A decis√£o agora √© influenciada por mais pontos, o que leva a uma decis√£o mais est√°vel. No entanto, a fronteira de decis√£o agora √© menos flex√≠vel, o que pode aumentar o vi√©s do modelo.

**Lemma 95:** No k-NN, a escolha do par√¢metro $k$ tem um impacto direto no equil√≠brio entre vi√©s e vari√¢ncia, com valores baixos de $k$ levando a modelos de baixa vi√©s e alta vari√¢ncia, e valores altos de $k$ levando a modelos de alto vi√©s e baixa vari√¢ncia.
*Prova*: Valores baixos de k levam o modelo a considerar apenas vizinhos muito pr√≥ximos, o que faz com que o modelo siga o conjunto de dados de treino com precis√£o, mas com alta variabilidade caso esse vizinhos pr√≥ximos sejam afetados por ru√≠dos. Valores altos fazem o modelo ignorar as particularidades do conjunto de treino e basear sua decis√£o em √°reas mais amplas, levando a maior vi√©s. $\blacksquare$

**Corol√°rio 95:** A escolha do valor de $k$ √© um aspecto fundamental do k-NN, e a valida√ß√£o cruzada √© um m√©todo para escolher o valor de $k$ que otimiza o balan√ßo entre vi√©s e vari√¢ncia.

> ‚ö†Ô∏è **Nota Importante**: O par√¢metro $k$ no k-NN controla o equil√≠brio entre vi√©s e vari√¢ncia, e a escolha adequada de $k$ √© essencial para obter um bom desempenho do modelo.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha ideal de k varia de acordo com as caracter√≠sticas do problema e do conjunto de dados, e deve ser obtida por meio de valida√ß√£o cruzada.

### Estrat√©gias para Reduzir a Vari√¢ncia do 1-NN

Devido √† sua alta vari√¢ncia, o 1-NN pode n√£o ser a melhor escolha em muitas aplica√ß√µes pr√°ticas. No entanto, existem algumas estrat√©gias que podem ser usadas para reduzir a vari√¢ncia do 1-NN e melhorar seu desempenho:

1.  **Utiliza√ß√£o do k-NN:** Aumentar o valor de $k$ no k-NN √© a forma mais simples de reduzir a vari√¢ncia do modelo, utilizando informa√ß√£o de mais de um vizinho para classificar um ponto.
2.  **Sele√ß√£o de Caracter√≠sticas:** Remover *features* irrelevantes ou ruidosas do conjunto de dados pode ajudar a reduzir o impacto de flutua√ß√µes aleat√≥rias na dist√¢ncia e a melhorar a robustez do modelo.
3.  **Agrega√ß√£o de Modelos:** Utilizar o 1-NN como parte de um modelo maior, onde as previs√µes de v√°rios 1-NN s√£o agregadas para produzir um resultado mais est√°vel. Por exemplo, o uso de *bagging* ou *boosting* sobre modelos 1-NN.
4.  **Redu√ß√£o de Dimensionalidade:** Projetar os dados em um espa√ßo de baixa dimensionalidade pode reduzir o impacto da maldi√ß√£o da dimensionalidade, que aumenta a vari√¢ncia dos modelos k-NN em altas dimens√µes.

```mermaid
graph LR
    subgraph "Variance Reduction Strategies"
    direction TB
        A["1-NN Model (High Variance)"] --> B["Increase k in k-NN"]
        A --> C["Feature Selection"]
        A --> D["Model Aggregation (Bagging/Boosting)"]
         A --> E["Dimensionality Reduction (PCA)"]
        B & C & D & E --> F["Reduced Variance"]
    end
```

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados com 10 *features*, mas apenas 2 *features* s√£o relevantes para a classifica√ß√£o. O 1-NN usando todas as 10 *features* pode ser muito sens√≠vel a ru√≠dos nas *features* irrelevantes, levando a uma alta vari√¢ncia.
>
> **Conjunto de Dados Original:**
>
> 10 *features* (2 relevantes + 8 irrelevantes/ruidosas)
>
> Se aplicarmos uma t√©cnica de sele√ß√£o de *features* e reduzirmos o n√∫mero de *features* para 2, o 1-NN se torna menos sens√≠vel a ru√≠dos e, portanto, tem sua vari√¢ncia reduzida.
>
> **Conjunto de Dados Ap√≥s Sele√ß√£o de *Features*:**
>
> 2 *features* (apenas as relevantes)
>
> Outra estrat√©gia seria utilizar *Principal Component Analysis (PCA)* para projetar os dados em um espa√ßo de baixa dimensionalidade. Vamos supor que as duas primeiras componentes principais capturam a maior parte da vari√¢ncia dos dados:
>
> **Conjunto de Dados Ap√≥s PCA:**
>
> 2 componentes principais
>
> Em ambos os casos, o 1-NN ter√° uma vari√¢ncia menor, pois o modelo se torna mais robusto a ru√≠dos e flutua√ß√µes nas *features* originais.

**Lemma 96:** A utiliza√ß√£o de estrat√©gias para reduzir a vari√¢ncia do 1-NN melhora a capacidade de generaliza√ß√£o do modelo para dados n√£o vistos e aumenta a sua robustez a ru√≠dos no conjunto de dados de treinamento.
*Prova*: Utilizar as informa√ß√µes de mais de um vizinho ou de subespa√ßos com menor dimensionalidade estabiliza a decis√£o, tornando o modelo menos dependente de detalhes espec√≠ficos do ponto mais pr√≥ximo. $\blacksquare$

**Corol√°rio 96:** A escolha da abordagem adequada para reduzir a vari√¢ncia do 1-NN depende das caracter√≠sticas do conjunto de dados e dos objetivos do problema de classifica√ß√£o.

> ‚ö†Ô∏è **Nota Importante**:  Existem diversas estrat√©gias que podem ser usadas para reduzir a vari√¢ncia do 1-NN, como o uso do k-NN, sele√ß√£o de *features*, agrega√ß√£o de modelos e redu√ß√£o de dimensionalidade.

> ‚ùó **Ponto de Aten√ß√£o**:  A escolha da estrat√©gia mais adequada para lidar com a alta vari√¢ncia do 1-NN deve ser feita considerando o equil√≠brio entre vi√©s e vari√¢ncia e a complexidade computacional do modelo.

### Conclus√£o

O 1-NN √© um classificador que apresenta baixo vi√©s e alta vari√¢ncia, o que o torna uma ferramenta adequada para capturar padr√µes complexos nos dados, mas tamb√©m sens√≠vel a flutua√ß√µes nos dados de treinamento. A compreens√£o dessa rela√ß√£o entre vi√©s e vari√¢ncia √© fundamental para a aplica√ß√£o eficaz do 1-NN e para a escolha de outras abordagens que busquem um equil√≠brio entre esses dois tipos de erro. A escolha do n√∫mero de vizinhos $k$ no k-NN √© uma das formas de ajustar o balan√ßo entre vi√©s e vari√¢ncia.

### Footnotes

[^13.3]: "These classifiers are memory-based, and require no model to be fit. Given a query point xo, we find the k training points x(r), r = 1,..., k closest in distance to xo, and then classify using majority vote among the k neighbors...Because it uses only the training point closest to the query point, the bias of the 1-nearest-neighbor estimate is often low, but the variance is high." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
