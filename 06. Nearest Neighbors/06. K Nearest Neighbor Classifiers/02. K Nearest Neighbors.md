## Classifica√ß√£o por Vota√ß√£o Majorit√°ria: Atribuindo um Ponto √† Classe Mais Frequente entre Seus k-Vizinhos

```mermaid
graph LR
    subgraph "k-NN Classification Process"
        direction TB
        A["Query Point: x_0"]
        B["Training Data: (x_i, y_i)"]
        C["Distance Calculation"]
        D["Select k-Nearest Neighbors"]
        E["Majority Voting"]
        F["Class Assignment"]
        A --> C
        B --> C
        C --> D
        D --> E
        E --> F
    end
```

### Introdu√ß√£o

Este cap√≠tulo detalha o processo de **classifica√ß√£o por vota√ß√£o majorit√°ria** no contexto do m√©todo de **k-vizinhos mais pr√≥ximos (k-NN)**, explorando como um ponto de consulta √© atribu√≠do √† classe mais frequente entre seus $k$ vizinhos mais pr√≥ximos no conjunto de treinamento [^13.3]. A ideia central do k-NN √© que pontos pr√≥ximos no espa√ßo de *features* tendem a pertencer √† mesma classe, e a classifica√ß√£o por vota√ß√£o majorit√°ria √© uma forma de agregar a informa√ß√£o dos vizinhos mais pr√≥ximos para determinar a classe de um novo ponto. Analisaremos como esse mecanismo de vota√ß√£o opera, como a escolha do valor de $k$ influencia o processo de classifica√ß√£o, e como a vota√ß√£o majorit√°ria se diferencia de outras abordagens de classifica√ß√£o.

### Classifica√ß√£o por Vota√ß√£o Majorit√°ria: Agregando Informa√ß√£o dos Vizinhos

A **classifica√ß√£o por vota√ß√£o majorit√°ria** √© um m√©todo de classifica√ß√£o utilizado no algoritmo k-NN que atribui um ponto de consulta √† classe mais frequente entre seus $k$ vizinhos mais pr√≥ximos no conjunto de treinamento [^13.3]. Em vez de usar um √∫nico prot√≥tipo como refer√™ncia, como em outros m√©todos de classifica√ß√£o, o k-NN utiliza as informa√ß√µes dos $k$ vizinhos mais pr√≥ximos para tomar uma decis√£o sobre a classe do novo ponto.

O processo de classifica√ß√£o por vota√ß√£o majorit√°ria envolve os seguintes passos:

1.  **Sele√ß√£o dos k Vizinhos:** Para um novo ponto de consulta $x_0$, os $k$ pontos de treinamento mais pr√≥ximos de $x_0$ s√£o identificados, usando alguma m√©trica de dist√¢ncia (normalmente a dist√¢ncia Euclidiana).
2.  **Vota√ß√£o:** Cada um dos $k$ vizinhos "vota" em sua respectiva classe, isto √©, cada vizinho contribui com seu r√≥tulo de classe.
3.  **Atribui√ß√£o:** O ponto de consulta $x_0$ √© atribu√≠do √† classe que recebeu o maior n√∫mero de votos entre os $k$ vizinhos mais pr√≥ximos.

Em caso de empate (duas ou mais classes recebem o mesmo n√∫mero de votos), a decis√£o pode ser tomada de forma aleat√≥ria entre as classes empatadas ou com outros m√©todos de desempate.

**Lemma 78:** A classifica√ß√£o por vota√ß√£o majorit√°ria no k-NN atribui um novo ponto √† classe que √© mais frequente entre seus $k$ vizinhos mais pr√≥ximos, ou seja, na regi√£o local ao ponto, utilizando a distribui√ß√£o das classes nessa regi√£o para tomar a decis√£o.
*Prova*: Por defini√ß√£o do algoritmo do k-NN, a classe mais frequente entre os vizinhos √© a classe atribu√≠da ao novo ponto. $\blacksquare$

**Corol√°rio 78:** A escolha do n√∫mero de vizinhos $k$ tem um impacto significativo na decis√£o final, e o valor de $k$ influencia a complexidade da fronteira de decis√£o, com valores menores levando a fronteiras mais irregulares, e valores maiores a fronteiras mais suaves.

> üí° **Exemplo Num√©rico:**
>
> Imagine que temos um conjunto de treinamento com pontos em duas classes (A e B) no espa√ßo bidimensional. Consideremos um ponto de consulta $x_0 = (2.5, 2.5)$ e os seguintes pontos de treinamento com suas respectivas classes:
>
> *   $x_1 = (1, 1)$, Classe A
> *   $x_2 = (1, 2)$, Classe A
> *   $x_3 = (2, 2)$, Classe A
> *   $x_4 = (3, 3)$, Classe B
> *   $x_5 = (4, 4)$, Classe B
> *   $x_6 = (3, 1)$, Classe B
>
> Se usarmos $k=3$, os 3 vizinhos mais pr√≥ximos de $x_0$ s√£o $x_2$, $x_3$ e $x_4$. Dois deles s√£o da classe A e um da classe B. Portanto, o ponto $x_0$ seria classificado como A. Se usarmos $k=5$, os 5 vizinhos mais pr√≥ximos seriam $x_1$, $x_2$, $x_3$, $x_4$ e $x_6$. Nesse caso, ter√≠amos tr√™s votos para a classe A e dois votos para a classe B, e o ponto $x_0$ seria classificado como A. A mudan√ßa no valor de $k$ n√£o alterou a classifica√ß√£o nesse caso, mas em outros casos, isso poderia acontecer.

> ‚ö†Ô∏è **Nota Importante**: A classifica√ß√£o por vota√ß√£o majorit√°ria no k-NN agrega as informa√ß√µes sobre os r√≥tulos de classe dos $k$ vizinhos mais pr√≥ximos para determinar a classe de um novo ponto.

> ‚ùó **Ponto de Aten√ß√£o**:  O valor de $k$ (n√∫mero de vizinhos) √© um hiperpar√¢metro crucial do k-NN, que afeta a complexidade do modelo e deve ser ajustado utilizando t√©cnicas de valida√ß√£o cruzada.

### Passos da Classifica√ß√£o no k-NN: C√°lculo das Dist√¢ncias e Vota√ß√£o

O processo de classifica√ß√£o no k-NN envolve dois passos principais: o **c√°lculo das dist√¢ncias** e a **vota√ß√£o majorit√°ria**.

```mermaid
graph LR
    subgraph "k-NN Classification Steps"
        direction TB
        A["Input Query Point: x_0"]
        B["Calculate Distances to Training Points"]
        C["Select k-Nearest Neighbors"]
        D["Majority Voting on Neighbors"]
        E["Output Class Label"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```

1.  **C√°lculo das Dist√¢ncias:** Para um novo ponto de consulta $x_0$, √© necess√°rio calcular a dist√¢ncia entre $x_0$ e todos os pontos de treinamento no conjunto de dados original. A m√©trica de dist√¢ncia mais utilizada √© a dist√¢ncia Euclidiana, mas outras m√©tricas como a dist√¢ncia de Manhattan ou a dist√¢ncia de Mahalanobis podem ser utilizadas, dependendo da natureza dos dados. A dist√¢ncia Euclidiana entre dois pontos $x = (x_1, x_2, \ldots, x_p)$ e $y = (y_1, y_2, \ldots, y_p)$ √© definida como:

$$d(x, y) = \sqrt{\sum_{i=1}^p (x_i - y_i)^2}$$

Onde $p$ √© o n√∫mero de *features* dos dados. Ap√≥s calcular a dist√¢ncia de $x_0$ a todos os pontos de treinamento, os $k$ pontos mais pr√≥ximos de $x_0$ s√£o selecionados.

2.  **Vota√ß√£o Majorit√°ria:** Ap√≥s a sele√ß√£o dos $k$ vizinhos mais pr√≥ximos, cada vizinho "vota" em sua respectiva classe, e a classe atribu√≠da ao ponto de consulta $x_0$ √© a que receber o maior n√∫mero de votos. A fun√ß√£o da vota√ß√£o √© agregar as informa√ß√µes locais sobre a distribui√ß√£o de classes na vizinhan√ßa do ponto de consulta.

**Lemma 79:** O c√°lculo das dist√¢ncias permite identificar os k-vizinhos mais pr√≥ximos de um dado ponto, enquanto a vota√ß√£o majorit√°ria atribui o ponto √† classe mais frequente naquela vizinhan√ßa, agregando as informa√ß√µes de classes pr√≥ximas.
*Prova*: O processo de vota√ß√£o agrega informa√ß√µes de classes vizinhas e fornece uma forma robusta de classifica√ß√£o, no sentido de que a decis√£o √© baseada na maioria e n√£o em um √∫nico vizinho. $\blacksquare$

**Corol√°rio 79:** A escolha da m√©trica de dist√¢ncia afeta o desempenho do k-NN, e o uso de m√©tricas de dist√¢ncia adaptativas pode melhorar o resultado.

> üí° **Exemplo Num√©rico:**
>
> Vamos usar os pontos do exemplo anterior e calcular as dist√¢ncias. Considere $x_0 = (2.5, 2.5)$ e os pontos $x_1 = (1, 1)$, $x_2 = (1, 2)$, $x_3 = (2, 2)$, $x_4 = (3, 3)$, $x_5 = (4, 4)$ e $x_6 = (3, 1)$. As dist√¢ncias euclidianas s√£o:
>
> *   $d(x_0, x_1) = \sqrt{(2.5-1)^2 + (2.5-1)^2} = \sqrt{1.5^2 + 1.5^2} = \sqrt{4.5} \approx 2.12$
> *   $d(x_0, x_2) = \sqrt{(2.5-1)^2 + (2.5-2)^2} = \sqrt{1.5^2 + 0.5^2} = \sqrt{2.5} \approx 1.58$
> *   $d(x_0, x_3) = \sqrt{(2.5-2)^2 + (2.5-2)^2} = \sqrt{0.5^2 + 0.5^2} = \sqrt{0.5} \approx 0.71$
> *   $d(x_0, x_4) = \sqrt{(2.5-3)^2 + (2.5-3)^2} = \sqrt{(-0.5)^2 + (-0.5)^2} = \sqrt{0.5} \approx 0.71$
> *   $d(x_0, x_5) = \sqrt{(2.5-4)^2 + (2.5-4)^2} = \sqrt{(-1.5)^2 + (-1.5)^2} = \sqrt{4.5} \approx 2.12$
> *   $d(x_0, x_6) = \sqrt{(2.5-3)^2 + (2.5-1)^2} = \sqrt{(-0.5)^2 + (1.5)^2} = \sqrt{2.5} \approx 1.58$
>
> Se escolhermos $k=3$, os 3 vizinhos mais pr√≥ximos seriam $x_3$, $x_4$, e $x_2$. A dist√¢ncia √© usada para selecionar os vizinhos, e a vota√ß√£o √© usada para atribuir a classe.

> ‚ö†Ô∏è **Nota Importante**: O processo de classifica√ß√£o no k-NN envolve o c√°lculo da dist√¢ncia entre um ponto de consulta e todos os pontos de treinamento e a atribui√ß√£o do ponto √† classe mais frequente entre seus vizinhos.

> ‚ùó **Ponto de Aten√ß√£o**: O c√°lculo da dist√¢ncia para todos os pontos de treino √© a etapa mais custosa do k-NN e o uso de algoritmos de busca eficiente pode reduzir essa complexidade.

### Influ√™ncia do Par√¢metro *k* na Classifica√ß√£o

O valor do par√¢metro $k$ (n√∫mero de vizinhos) tem uma influ√™ncia crucial na classifica√ß√£o com o k-NN, e a escolha adequada do valor de $k$ √© um desafio importante [^13.3].

```mermaid
graph LR
    subgraph "Effect of k on k-NN"
        direction TB
        A["Small k"]
        B["Large k"]
        C["Sensitive to Noise"]
        D["Complex Decision Boundary"]
        E["Robust to Noise"]
        F["Smoother Decision Boundary"]
        G["Potential Overfitting"]
         H["Potential Underfitting"]
        A --> C
        A --> D
         A --> G
        B --> E
        B --> F
         B --> H
    end
```

1.  **Pequenos Valores de k:** Valores pequenos de $k$ tornam o modelo mais sens√≠vel ao ru√≠do nos dados de treinamento. Nesse caso, o modelo √© muito dependente das informa√ß√µes locais, e pode ter dificuldade em generalizar para novos pontos fora da regi√£o de treino, levando a *overfitting*.
2.  **Grandes Valores de k:** Valores grandes de $k$ tornam o modelo menos sens√≠vel ao ru√≠do, pois a decis√£o √© baseada em um n√∫mero maior de vizinhos. No entanto, o modelo torna-se mais enviesado, pois passa a incluir pontos mais distantes e que podem n√£o ser representativos da regi√£o de decis√£o local, levando a *underfitting*.

A escolha do valor ideal de $k$ deve ser feita por valida√ß√£o cruzada, buscando um valor que equilibre o vi√©s e a vari√¢ncia do modelo, e minimize o erro de classifica√ß√£o para dados n√£o utilizados no ajuste. O valor de $k$ deve ser escolhido de acordo com as caracter√≠sticas de distribui√ß√£o e complexidade das classes no conjunto de dados.

**Lemma 80:** A escolha do n√∫mero de vizinhos $k$ no k-NN influencia o balan√ßo entre vi√©s e vari√¢ncia do modelo, com valores pequenos de $k$ tornando o modelo mais sens√≠vel a ru√≠dos e valores grandes suavizando as fronteiras de decis√£o.
*Prova*: Valores baixos de $k$ consideram apenas informa√ß√µes de pontos muito pr√≥ximos, o que torna a decis√£o sens√≠vel a varia√ß√µes locais, e valores altos incluem informa√ß√µes de regi√µes mais afastadas que podem introduzir vi√©s. $\blacksquare$

**Corol√°rio 80:** A escolha do valor ideal de $k$ √© um problema de otimiza√ß√£o que deve ser resolvido com valida√ß√£o cruzada ou outras t√©cnicas de sele√ß√£o de modelo.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados com duas classes, A e B, e estamos usando k-NN para classificar um ponto novo.
>
> *   **Caso 1: k = 1**
>     Se $k=1$, o ponto novo ser√° classificado com a classe do seu vizinho mais pr√≥ximo. Se esse vizinho for um outlier ou um ponto ruidoso, a classifica√ß√£o pode estar errada. Isso leva a uma fronteira de decis√£o muito irregular, seguindo cada ponto de treino.
>
> *   **Caso 2: k = 5**
>     Se $k=5$, a classifica√ß√£o ser√° baseada na maioria das classes entre os 5 vizinhos mais pr√≥ximos. Isso torna a classifica√ß√£o mais robusta a ru√≠dos, mas pode suavizar demais a fronteira de decis√£o, misturando regi√µes de classes diferentes.
>
> *   **Caso 3: k = n√∫mero total de pontos de treino**
>     Se $k$ for o n√∫mero total de pontos de treino, o k-NN sempre classificar√° o ponto novo na classe mais frequente no conjunto de treino, independentemente da sua posi√ß√£o. Isso √© um caso extremo de underfitting, pois o modelo ignora completamente a estrutura local dos dados.
>
> Usando valida√ß√£o cruzada, podemos testar diferentes valores de $k$ e escolher aquele que nos d√° a melhor acur√°cia em dados n√£o utilizados no treino.

> ‚ö†Ô∏è **Nota Importante**: A escolha do valor de $k$ √© um hiperpar√¢metro crucial do k-NN que deve ser otimizado, buscando equilibrar a sensibilidade ao ru√≠do e a capacidade de generaliza√ß√£o.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha adequada de $k$ √© um aspecto cr√≠tico para o desempenho do k-NN, e deve ser determinada com base nas caracter√≠sticas do conjunto de dados.

### Conclus√£o

A classifica√ß√£o por vota√ß√£o majorit√°ria no k-NN √© um m√©todo simples e eficaz que utiliza as informa√ß√µes sobre as classes dos $k$ vizinhos mais pr√≥ximos para atribuir um r√≥tulo a um novo ponto de consulta. A escolha adequada do valor de $k$ √© essencial para garantir um bom desempenho do modelo, equilibrando a sensibilidade ao ru√≠do com a capacidade de generaliza√ß√£o. A simplicidade e adaptabilidade do k-NN o tornam uma ferramenta √∫til em diversas aplica√ß√µes de classifica√ß√£o e reconhecimento de padr√µes.

### Footnotes

[^13.3]: "These classifiers are memory-based, and require no model to be fit. Given a query point xo, we find the k training points x(r), r = 1,..., k closest in distance to xo, and then classify using majority vote among the k neighbors." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
