## Desempenho dos M√©todos *Model-Free*: Sucesso em Problemas Complexos com Fronteiras Irregulares e Diversidade de Prot√≥tipos

```mermaid
graph LR
    subgraph "Model-Free Methods vs. Linear Models"
        direction LR
        A["Complex Problems with Irregular Boundaries and Diverse Prototypes"]
        B["Model-Free Methods (k-NN, LVQ, GMMs)"]
        C["Linear Models (Logistic Regression, LDA)"]
        A --> B
        A --> C
        B -- "Effective" --> D["Accurate Classification"]
        C -- "Limited" --> E["Poor Performance"]
        D -- "Adaptable to Complex Data Distributions"
        E -- "Struggles with Non-linear Boundaries"
     end
```

### Introdu√ß√£o

Este cap√≠tulo explora o **desempenho** dos m√©todos *model-free* em problemas complexos de classifica√ß√£o e reconhecimento de padr√µes, com √™nfase em sua capacidade de lidar com **fronteiras de decis√£o irregulares** e a presen√ßa de muitas amostras representativas, ou seja, **prot√≥tipos**, por classe [^13.1]. M√©todos como o k-vizinhos mais pr√≥ximos (k-NN), o Learning Vector Quantization (LVQ) e as Misturas Gaussianas (GMMs) se destacam por sua flexibilidade e capacidade de se adaptar a distribui√ß√µes complexas de dados, o que os torna particularmente √∫teis em cen√°rios onde os modelos lineares tradicionais apresentam limita√ß√µes. Analisaremos como a combina√ß√£o de t√©cnicas de prot√≥tipos e m√©todos baseados em mem√≥ria resulta em modelos capazes de lidar com desafios complexos que surgem em dados do mundo real.

### A Efic√°cia dos M√©todos *Model-Free* em Cen√°rios Complexos

Os m√©todos *model-free*, incluindo o k-NN e m√©todos baseados em prot√≥tipos como LVQ e GMMs, t√™m demonstrado grande efic√°cia em uma variedade de problemas complexos de classifica√ß√£o, especialmente em cen√°rios onde as distribui√ß√µes das classes apresentam caracter√≠sticas que dificultam a aplica√ß√£o de modelos lineares [^13.1].

Esses cen√°rios complexos se caracterizam por:

1.  **Fronteiras de Decis√£o Irregulares:** As fronteiras entre as classes n√£o seguem um padr√£o linear, mas apresentam formas complexas e n√£o lineares. Modelos lineares, como a regress√£o log√≠stica ou o LDA (Linear Discriminant Analysis), podem ter dificuldades para capturar essas fronteiras irregulares, o que limita seu desempenho.
2.  **Diversidade de Prot√≥tipos por Classe:** Cada classe pode apresentar uma grande variabilidade interna, e ser representada por diversos "prot√≥tipos", que n√£o necessariamente est√£o todos pr√≥ximos uns dos outros no espa√ßo de *features*. Modelos que assumem uma forma espec√≠fica de distribui√ß√£o de classes (ex. gaussianas unimodais) podem n√£o representar de forma adequada essa variabilidade.
3. **Dados de Alta Dimensionalidade:** M√©todos lineares em espa√ßos de alta dimensionalidade podem ter desempenho comprometido devido √† dispers√£o dos dados e ao problema da maldi√ß√£o da dimensionalidade.

```mermaid
graph TB
    subgraph "Challenges in Complex Classification"
        direction TB
        A["Complex Scenarios"]
        B["Irregular Decision Boundaries"]
        C["High Diversity of Prototypes"]
        D["High Dimensional Data"]
        A --> B
        A --> C
        A --> D
    end
```

A capacidade dos m√©todos *model-free* de lidar com esses desafios se deve, principalmente, √† sua flexibilidade e √† sua capacidade de se adaptar a estruturas complexas nos dados. A aus√™ncia de um modelo param√©trico e a representa√ß√£o dos dados por prot√≥tipos ou vizinhos pr√≥ximos permitem que esses m√©todos capturem as nuances da distribui√ß√£o de dados de forma mais eficaz do que modelos param√©tricos.

**Lemma 89:** M√©todos *model-free* apresentam bom desempenho em problemas com fronteiras de decis√£o irregulares e variabilidade de prot√≥tipos nas classes, devido √† sua flexibilidade e capacidade de adapta√ß√£o √† complexidade da distribui√ß√£o de dados.
*Prova*: M√©todos baseados em prot√≥tipos utilizam prot√≥tipos estrategicamente posicionados que capturam a forma de cada classe, enquanto que m√©todos como o k-NN tomam decis√µes baseados em informa√ß√µes locais e n√£o em modelos globais. $\blacksquare$

**Corol√°rio 89:** Modelos lineares, que assumem fronteiras de decis√£o lineares, podem apresentar dificuldades em problemas onde essa suposi√ß√£o n√£o √© v√°lida.

> ‚ö†Ô∏è **Nota Importante**: M√©todos *model-free* s√£o particularmente eficazes em cen√°rios complexos com fronteiras de decis√£o irregulares e grande diversidade de prot√≥tipos nas classes.

> ‚ùó **Ponto de Aten√ß√£o**: A capacidade de se adaptar √† complexidade dos dados √© uma vantagem dos m√©todos *model-free* em rela√ß√£o a modelos lineares ou outros m√©todos com suposi√ß√µes sobre a forma da distribui√ß√£o dos dados.

### k-Vizinhos Mais Pr√≥ximos (k-NN): Sucesso em Problemas com Fronteiras Irregulares

O m√©todo de **k-vizinhos mais pr√≥ximos (k-NN)** √© um exemplo de sucesso de m√©todo *model-free* em problemas de classifica√ß√£o com **fronteiras de decis√£o irregulares** [^13.3]. A capacidade do k-NN de modelar fronteiras de decis√£o complexas se deve ao fato de que a decis√£o de classifica√ß√£o √© baseada na informa√ß√£o local dos $k$ vizinhos mais pr√≥ximos, sem assumir nenhuma forma espec√≠fica para a fronteira de decis√£o.

Em problemas onde a fronteira de decis√£o n√£o √© linear, ou apresenta regi√µes com curvaturas complexas, o k-NN pode obter resultados superiores a modelos lineares, pois n√£o imp√µe nenhuma restri√ß√£o √† forma das fronteiras de decis√£o. A escolha do valor de $k$ influencia a suavidade da fronteira de decis√£o, com valores menores de $k$ resultando em fronteiras mais irregulares, e valores maiores levando a fronteiras mais suaves. T√©cnicas de adapta√ß√£o local do valor de $k$ tamb√©m aumentam a capacidade do modelo de lidar com distribui√ß√µes de dados heterog√™neas.

```mermaid
graph TB
  subgraph "k-NN Decision Boundary"
      direction TB
      A["k-NN Classification"]
      B["Find k Nearest Neighbors"]
      C["Majority Vote"]
      D["Irregular Decision Boundary"]
      A --> B
      B --> C
      C --> D
      style D fill:#f9f,stroke:#333,stroke-width:2px
    end
```

O k-NN tem apresentado resultados not√°veis em problemas com dados de alta dimensionalidade, como reconhecimento de imagens e an√°lise de texto, onde a estrutura dos dados √© complexa e irregular, e modelos lineares frequentemente t√™m desempenho limitado.

> üí° **Exemplo Num√©rico:**
> Considere um problema de classifica√ß√£o bin√°ria onde temos duas classes, A e B, distribu√≠das de forma n√£o linear em um espa√ßo 2D. As amostras da classe A formam um c√≠rculo no centro do espa√ßo, enquanto as amostras da classe B est√£o distribu√≠das ao redor desse c√≠rculo.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.neighbors import KNeighborsClassifier
>
> # Gera dados n√£o lineares
> np.random.seed(42)
> radius = np.random.uniform(2, 4, 100)
> angle = np.random.uniform(0, 2 * np.pi, 100)
> X_A = np.array([radius * np.cos(angle), radius * np.sin(angle)]).T
> y_A = np.zeros(100)
>
> radius = np.random.uniform(0, 1.5, 100)
> angle = np.random.uniform(0, 2 * np.pi, 100)
> X_B = np.array([radius * np.cos(angle), radius * np.sin(angle)]).T
> y_B = np.ones(100)
>
> X = np.concatenate((X_A, X_B))
> y = np.concatenate((y_A, y_B))
>
> # Treina o modelo k-NN com k=3
> knn = KNeighborsClassifier(n_neighbors=3)
> knn.fit(X, y)
>
> # Cria uma malha para visualiza√ß√£o das fronteiras de decis√£o
> x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
> y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
>                      np.arange(y_min, y_max, 0.02))
>
> Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])
> Z = Z.reshape(xx.shape)
>
> # Plotagem da fronteira de decis√£o
> plt.contourf(xx, yy, Z, alpha=0.4)
> plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
> plt.title('Fronteira de Decis√£o k-NN (k=3)')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
> plt.show()
> ```
> Neste exemplo, o k-NN com `k=3` consegue modelar a fronteira de decis√£o circular entre as duas classes, algo que um modelo linear teria dificuldade em fazer. A visualiza√ß√£o mostra como o modelo se adapta √† forma complexa da distribui√ß√£o dos dados.

**Lemma 90:** A abordagem local do k-NN, com base na informa√ß√£o dos $k$ vizinhos mais pr√≥ximos, permite que o modelo se adapte a fronteiras de decis√£o irregulares, o que n√£o √© poss√≠vel em modelos lineares.
*Prova*: A decis√£o do k-NN √© tomada com base na vizinhan√ßa do ponto de consulta, o que permite que o modelo represente regi√µes de decis√£o de qualquer forma, e n√£o apenas lineares. $\blacksquare$

**Corol√°rio 90:** O k-NN √© uma escolha adequada para problemas com fronteiras de decis√£o complexas e n√£o lineares, onde o objetivo √© classificar o ponto baseado nas informa√ß√µes de sua vizinhan√ßa.

> ‚ö†Ô∏è **Nota Importante**: O k-NN √© uma abordagem eficaz para lidar com fronteiras de decis√£o irregulares devido √† sua capacidade de modelar as distribui√ß√µes dos dados localmente.

> ‚ùó **Ponto de Aten√ß√£o**:  Embora o k-NN seja flex√≠vel, a escolha do valor de k e da m√©trica de dist√¢ncia √© crucial para um bom desempenho do modelo, e a aus√™ncia de ajuste do modelo o torna sens√≠vel ao ru√≠do e ao tamanho dos dados.

### M√©todos de Prot√≥tipos: Modelagem de Classes com Diversidade de Prot√≥tipos

M√©todos de prot√≥tipos, como o **LVQ (Learning Vector Quantization)** e as **GMMs (Misturas Gaussianas)**, tamb√©m se destacam por seu bom desempenho em problemas onde as classes apresentam uma **diversidade de prot√≥tipos** [^13.2.2], [^13.2.3]. Nesses problemas, a distribui√ß√£o de cada classe √© complexa e pode ser representada de forma mais adequada por diversos prot√≥tipos, e n√£o por um √∫nico centr√≥ide, como em modelos lineares.

```mermaid
graph TB
    subgraph "Prototype-Based Methods"
        direction TB
        A["Prototype-Based Methods"]
        B["LVQ: Learning Vector Quantization"]
        C["GMMs: Gaussian Mixture Models"]
        A --> B
        A --> C
    end
```

O LVQ utiliza um mecanismo de atra√ß√£o e repuls√£o que permite posicionar os prot√≥tipos de forma estrat√©gica em rela√ß√£o √†s fronteiras de decis√£o, o que faz com que o modelo capture a diversidade interna de cada classe. Ao mover os prot√≥tipos de acordo com o r√≥tulo das amostras de treino, o LVQ consegue modelar classes com diversas regi√µes de alta densidade, criando prot√≥tipos com capacidade de discrimina√ß√£o entre classes.

As GMMs, por sua vez, utilizam uma combina√ß√£o de gaussianas para modelar a distribui√ß√£o de cada classe, o que lhes permite representar formas complexas e multimodais. Cada gaussiana atua como um prot√≥tipo, e a combina√ß√£o de diversas gaussianas permite que o modelo capture a variabilidade interna de cada classe, incluindo a presen√ßa de diversos "prot√≥tipos" que representam diferentes regi√µes da distribui√ß√£o.

```mermaid
graph TB
  subgraph "LVQ Prototype Placement"
      direction LR
      A["Initial Prototypes"]
      B["Iterative Attraction/Repulsion"]
      C["Strategic Prototype Placement"]
       A --> B
       B --> C
  end
```

> üí° **Exemplo Num√©rico:**
> Para ilustrar o LVQ, considere um problema com duas classes, onde cada classe √© composta por dois agrupamentos distintos (prot√≥tipos).
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from neupy import algorithms
>
> # Gera dados com dois prot√≥tipos por classe
> np.random.seed(42)
> class_1_prot_1 = np.random.multivariate_normal([1, 1], [[0.1, 0], [0, 0.1]], 50)
> class_1_prot_2 = np.random.multivariate_normal([3, 3], [[0.1, 0], [0, 0.1]], 50)
> class_2_prot_1 = np.random.multivariate_normal([1, 3], [[0.1, 0], [0, 0.1]], 50)
> class_2_prot_2 = np.random.multivariate_normal([3, 1], [[0.1, 0], [0, 0.1]], 50)
>
> X_class_1 = np.concatenate((class_1_prot_1, class_1_prot_2))
> y_class_1 = np.zeros(100)
>
> X_class_2 = np.concatenate((class_2_prot_1, class_2_prot_2))
> y_class_2 = np.ones(100)
>
> X = np.concatenate((X_class_1, X_class_2))
> y = np.concatenate((y_class_1, y_class_2))
>
> # Treina o LVQ com 4 prot√≥tipos
> lvqnet = algorithms.LVQ(n_inputs=2, n_classes=2, n_prototypes=4)
> lvqnet.train(X, y, epochs=100)
>
> # Plota os dados e os prot√≥tipos
> plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', label='Dados')
> prototypes = lvqnet.prototypes
> plt.scatter(prototypes[:,0], prototypes[:,1], c=['r', 'r', 'b', 'b'], marker='*', s=200, label='Prot√≥tipos')
> plt.title('LVQ com Prot√≥tipos')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
> plt.legend()
> plt.show()
> ```
> Neste exemplo, o LVQ ajusta quatro prot√≥tipos, dois para cada classe. Os prot√≥tipos (estrelas) se posicionam de forma a representar cada um dos agrupamentos de cada classe. Um modelo linear, por outro lado, teria dificuldade em modelar esta distribui√ß√£o, pois tentaria separar as classes com uma √∫nica reta, ignorando a complexidade da distribui√ß√£o.

> üí° **Exemplo Num√©rico:**
> Para ilustrar as GMMs, vamos gerar dados com duas classes, onde cada classe tem duas gaussianas, representando dois prot√≥tipos distintos.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.mixture import GaussianMixture
>
> # Gera dados com dois prot√≥tipos por classe
> np.random.seed(42)
> class_1_prot_1 = np.random.multivariate_normal([1, 1], [[0.3, 0], [0, 0.3]], 100)
> class_1_prot_2 = np.random.multivariate_normal([3, 3], [[0.3, 0], [0, 0.3]], 100)
> class_2_prot_1 = np.random.multivariate_normal([1, 3], [[0.3, 0], [0, 0.3]], 100)
> class_2_prot_2 = np.random.multivariate_normal([3, 1], [[0.3, 0], [0, 0.3]], 100)
>
> X_class_1 = np.concatenate((class_1_prot_1, class_1_prot_2))
> y_class_1 = np.zeros(200)
>
> X_class_2 = np.concatenate((class_2_prot_1, class_2_prot_2))
> y_class_2 = np.ones(200)
>
> X = np.concatenate((X_class_1, X_class_2))
> y = np.concatenate((y_class_1, y_class_2))
>
> # Treina o GMM com 2 componentes por classe
> gmm = GaussianMixture(n_components=4, random_state=42)
> gmm.fit(X)
>
> # Plota os dados e as gaussianas
> plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', label='Dados')
>
> # Cria uma malha para visualiza√ß√£o das gaussianas
> x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
> y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
>                      np.arange(y_min, y_max, 0.02))
> Z = -gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])
> Z = Z.reshape(xx.shape)
> plt.contourf(xx, yy, Z, cmap='RdBu', alpha=0.3)
>
> plt.title('GMM com Prot√≥tipos Gaussianos')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
> plt.legend()
> plt.show()
> ```
> Neste exemplo, o GMM com 4 componentes consegue modelar a distribui√ß√£o de cada classe com duas gaussianas (prot√≥tipos). Cada gaussiana representa um agrupamento dentro da classe, mostrando como o GMM captura a diversidade de prot√≥tipos. A sobreposi√ß√£o das gaussianas reflete a complexidade da distribui√ß√£o.

```mermaid
graph TB
    subgraph "GMM Components as Prototypes"
        direction TB
        A["GMM"]
        B["Gaussian Component 1 (Prototype 1)"]
        C["Gaussian Component 2 (Prototype 2)"]
        D["Gaussian Component N (Prototype N)"]
        A --> B
        A --> C
        A --> D
    end
```

**Lemma 91:** O LVQ e as GMMs s√£o m√©todos que permitem modelar classes com diversidade de prot√≥tipos, o que possibilita representar a complexidade das distribui√ß√µes de dados de cada classe.
*Prova*: O LVQ usa o mecanismo de atra√ß√£o e repuls√£o para posicionar prot√≥tipos discriminativos e o GMM combina v√°rias gaussianas, onde cada gaussiana atua como um prot√≥tipo da distribui√ß√£o. $\blacksquare$

**Corol√°rio 91:** A capacidade de modelar classes com diversidade de prot√≥tipos torna LVQ e GMMs mais adequados que os modelos lineares em aplica√ß√µes onde as classes n√£o apresentam uma estrutura unimodal e de distribui√ß√£o simples.

> ‚ö†Ô∏è **Nota Importante**:  LVQ e GMMs s√£o capazes de representar classes com variabilidade interna, usando m√∫ltiplos prot√≥tipos para capturar a diversidade da distribui√ß√£o de dados de cada classe.

> ‚ùó **Ponto de Aten√ß√£o**: M√©todos de prot√≥tipos, como LVQ e GMMs, s√£o mais adequados que modelos lineares quando a distribui√ß√£o dos dados √© complexa e quando n√£o existe um √∫nico prot√≥tipo representativo para cada classe.

### Exemplos de Sucesso: Reconhecimento de Imagens e Outras Aplica√ß√µes

Os m√©todos *model-free* t√™m demonstrado sucesso em uma variedade de problemas complexos, incluindo:

1.  **Reconhecimento de Imagens:** O k-NN e o LVQ t√™m sido utilizados com sucesso em problemas de reconhecimento de d√≠gitos manuscritos, objetos em imagens e outros problemas de vis√£o computacional, onde as fronteiras de decis√£o s√£o complexas e os dados apresentam alta dimensionalidade [^13.3.3], [^13.3.2].
2.  **Processamento de Linguagem Natural:** M√©todos baseados em mem√≥ria e prot√≥tipos t√™m sido utilizados em problemas de classifica√ß√£o de texto, an√°lise de sentimentos e reconhecimento de entidades nomeadas, onde a complexidade da linguagem e a variabilidade dos dados tornam a modelagem com m√©todos param√©tricos um desafio.
3.  **Bioinform√°tica:** M√©todos de prot√≥tipos t√™m sido usados em an√°lise de dados biol√≥gicos, como a classifica√ß√£o de sequ√™ncias de DNA e RNA, onde os dados apresentam grande variabilidade e as fronteiras de decis√£o s√£o irregulares.
4.  **Sistemas de Recomenda√ß√£o:** M√©todos como k-NN t√™m sido utilizados em sistemas de recomenda√ß√£o para identificar itens que sejam similares aos interesses do usu√°rio, tendo em conta que padr√µes complexos de relacionamento entre usu√°rios e itens frequentemente tornam modelos lineares menos adequados.

```mermaid
graph TB
  subgraph "Applications of Model-Free Methods"
    direction TB
    A["Model-Free Methods"]
    B["Image Recognition"]
    C["Natural Language Processing"]
    D["Bioinformatics"]
    E["Recommendation Systems"]
    A --> B
    A --> C
    A --> D
    A --> E
  end
```

Esses exemplos ilustram a versatilidade e a capacidade de adapta√ß√£o dos m√©todos *model-free* a diferentes tipos de problemas, o que os torna valiosos para aplica√ß√µes do mundo real.

**Lemma 92:** M√©todos *model-free* t√™m apresentado resultados eficazes em diversas aplica√ß√µes reais, incluindo reconhecimento de imagens, processamento de linguagem natural, bioinform√°tica e sistemas de recomenda√ß√£o, o que demonstra sua capacidade de modelar problemas complexos e com caracter√≠sticas n√£o lineares.
*Prova*: Diversos trabalhos comprovam a capacidade de m√©todos *model-free* em modelar problemas complexos do mundo real. $\blacksquare$

**Corol√°rio 92:** A escolha entre m√©todos *model-free* e outros modelos de aprendizado de m√°quina deve ser feita considerando a complexidade dos dados, a necessidade de adapta√ß√£o e a interpretabilidade dos resultados.

> ‚ö†Ô∏è **Nota Importante**:  M√©todos *model-free* t√™m demonstrado sucesso em uma ampla variedade de aplica√ß√µes, o que evidencia sua capacidade de lidar com dados complexos e diversos.

> ‚ùó **Ponto de Aten√ß√£o**:  A escolha do m√©todo mais apropriado depende da natureza do problema, da quantidade de dados dispon√≠veis e dos requisitos computacionais e de interpretabilidade.

### Conclus√£o

Os m√©todos *model-free* oferecem uma abordagem eficaz para lidar com a complexidade de dados do mundo real, caracterizados por fronteiras de decis√£o irregulares e a presen√ßa de muitos prot√≥tipos por classe. A flexibilidade do k-NN e a capacidade de modelar classes por meio de prot√≥tipos nos algoritmos LVQ e GMMs permitem que esses modelos obtenham resultados not√°veis em problemas que desafiam os modelos lineares tradicionais. A escolha do m√©todo mais adequado depende das caracter√≠sticas do problema, da disponibilidade de dados e dos requisitos de interpretabilidade e performance.

### Footnotes

[^13.1]: "In this chapter we discuss some simple and essentially model-free methods for classification and pattern recognition. Because they are highly unstructured, they typically are not useful for understanding the nature of the relationship between the features and class outcome. However, as black box prediction engines, they can be very effective, and are often among the best performers in real data problems." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.3]: "These classifiers are memory-based, and require no model to be fit. Given a query point xo, we find the k training points x(r), r = 1,..., k closest in distance to xo, and then classify using majority vote among the k neighbors." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.2]: "In this technique due to Kohonen (1989), prototypes are placed strategically with respect to the decision boundaries in an ad-hoc way. LVQ is an online algorithm-observations are processed one at a time. The idea is that the training points attract prototypes of the correct class, and repel other prototypes. When the iterations settle down, prototypes should be close to the training points in their class." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2.3]: "The Gaussian mixture model can also be thought of as a prototype method, similar in spirit to K-means and LVQ. We discuss Gaussian mixtures in some detail in Sections 6.8, 8.5 and 12.7. Each cluster is described in terms of a Gaussian density, which has a centroid (as in K-means), and a covariance matrix. The comparison becomes crisper if we restrict the component Gaussians to have a scalar covariance matrix (Exercise 13.1)...Similarly, when Gaussian mixture models are used to represent the feature density in each class, it produces smooth posterior probabilities." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
