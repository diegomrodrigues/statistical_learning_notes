## Dist√¢ncia Euclidiana no k-NN: A Necessidade de Padroniza√ß√£o das *Features*

```mermaid
graph LR
    subgraph "k-NN Data Preprocessing"
        A["Raw Data"] --> B["Feature Extraction"]
        B --> C["Feature Standardization"]
        C --> D["Euclidean Distance Calculation"]
        D --> E["k-Nearest Neighbor Search"]
        E --> F["Classification/Regression"]
    end
```

### Introdu√ß√£o

Este cap√≠tulo aborda a import√¢ncia da **dist√¢ncia Euclidiana** como m√©trica de proximidade no algoritmo **k-vizinhos mais pr√≥ximos (k-NN)**, com √™nfase na necessidade de **padronizar** as *features* antes de calcular a dist√¢ncia [^13.3]. A dist√¢ncia Euclidiana √© uma m√©trica amplamente utilizada por sua simplicidade e interpretabilidade, mas sua aplica√ß√£o direta a dados brutos pode levar a resultados enviesados quando as *features* apresentam escalas diferentes. Analisaremos como a padroniza√ß√£o das *features*, que envolve centralizar e normalizar cada dimens√£o para ter m√©dia zero e vari√¢ncia um, √© crucial para garantir que a dist√¢ncia Euclidiana reflita a proximidade real entre os pontos, e como a padroniza√ß√£o √© um passo essencial no pr√©-processamento de dados para o k-NN.

### Dist√¢ncia Euclidiana: Uma M√©trica de Proximidade no Espa√ßo de *Features*

A **dist√¢ncia Euclidiana** √© uma m√©trica de dist√¢ncia amplamente utilizada para medir a proximidade entre dois pontos em um espa√ßo de *features* [^13.3]. No contexto do k-NN, a dist√¢ncia Euclidiana √© usada para identificar os $k$ pontos de treinamento mais pr√≥ximos de um novo ponto de consulta, sendo uma medida direta da dist√¢ncia "em linha reta" entre dois pontos.

Formalmente, a dist√¢ncia Euclidiana entre dois pontos $x = (x_1, x_2, \ldots, x_p)$ e $y = (y_1, y_2, \ldots, y_p)$ em um espa√ßo de *features* de $p$ dimens√µes √© definida como:

$$d(x, y) = \sqrt{\sum_{i=1}^p (x_i - y_i)^2}$$

Onde $x_i$ e $y_i$ s√£o os valores das *features* $i$ dos pontos $x$ e $y$, respectivamente. A dist√¢ncia Euclidiana √© intuitiva e f√°cil de calcular, mas sua aplica√ß√£o direta a dados com diferentes escalas nas *features* pode levar a resultados enviesados, favorecendo *features* com maiores valores absolutos, e diminuindo a import√¢ncia de *features* com escalas menores.

```mermaid
graph LR
    subgraph "Euclidean Distance Formula"
        direction TB
        A["d(x, y) = sqrt(‚àë(x·µ¢ - y·µ¢)¬≤)"]
        B["x·µ¢: Feature 'i' of point 'x'"]
        C["y·µ¢: Feature 'i' of point 'y'"]
        D["‚àë: Sum over all features 'i'"]
        A --> B
        A --> C
        A --> D
    end
```

**Lemma 81:** A dist√¢ncia Euclidiana √© uma m√©trica que quantifica a proximidade entre dois pontos no espa√ßo de *features*, mas √© sens√≠vel √† escala das *features*, o que significa que *features* com maiores valores absolutos podem ter maior influ√™ncia na dist√¢ncia.
*Prova*: A soma das diferen√ßas quadr√°ticas no c√°lculo da dist√¢ncia euclidiana faz com que features em uma escala maior tenham impacto muito maior na dist√¢ncia do que features em escalas menores. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Considere dois pontos em um espa√ßo de duas dimens√µes (features), $x = (1, 100)$ e $y = (2, 102)$.
>
> 1.  **C√°lculo da Dist√¢ncia Euclidiana Sem Padroniza√ß√£o:**
>
>     $d(x, y) = \sqrt{(1-2)^2 + (100-102)^2} = \sqrt{(-1)^2 + (-2)^2} = \sqrt{1 + 4} = \sqrt{5} \approx 2.236$
>
>     Nesse caso, a segunda feature (com valores em torno de 100) domina o c√°lculo da dist√¢ncia, enquanto a primeira feature (com valores em torno de 1) tem pouca influ√™ncia.
>
>     Agora, vamos considerar dois pontos $x = (1, 100)$ e $z = (1, 101)$.
>     $d(x, z) = \sqrt{(1-1)^2 + (100-101)^2} = \sqrt{0^2 + (-1)^2} = \sqrt{1} = 1$.
>     Embora a diferen√ßa na segunda feature entre x e z seja menor que a diferen√ßa entre x e y, a dist√¢ncia entre x e z √© menor, por conta do maior valor absoluto da segunda feature.
>
>     Se as features tivessem sido padronizadas, a dist√¢ncia entre x e y e x e z seria mais representativa da diferen√ßa real entre os pontos.
>
> 2.  **C√°lculo da Dist√¢ncia Euclidiana com Padroniza√ß√£o:**
>     Vamos supor que, ap√≥s a padroniza√ß√£o, os pontos transformados sejam $x' = (-1, -1)$ e $y' = (1, 1)$, e $z' = (-1, 0)$.
>     $d(x', y') = \sqrt{(-1 - 1)^2 + (-1 - 1)^2} = \sqrt{(-2)^2 + (-2)^2} = \sqrt{8} \approx 2.828$
>     $d(x', z') = \sqrt{(-1 - -1)^2 + (-1 - 0)^2} = \sqrt{0^2 + (-1)^2} = \sqrt{1} = 1$
>
>     Com a padroniza√ß√£o, a contribui√ß√£o de cada feature para a dist√¢ncia √© equilibrada. A diferen√ßa entre $x'$ e $y'$ e $x'$ e $z'$ √© mais representativa da diferen√ßa real entre os pontos.
>
>  Este exemplo ilustra como a falta de padroniza√ß√£o pode distorcer a no√ß√£o de proximidade no espa√ßo de features.

**Corol√°rio 81:** A padroniza√ß√£o das *features* √© essencial para garantir que todas as *features* contribuam de forma igual para o c√°lculo da dist√¢ncia Euclidiana e, consequentemente, para a escolha dos vizinhos mais pr√≥ximos no k-NN.

> ‚ö†Ô∏è **Nota Importante**:  A dist√¢ncia Euclidiana √© uma m√©trica amplamente utilizada para calcular a proximidade entre pontos no espa√ßo de *features*, mas sua aplica√ß√£o direta a dados n√£o padronizados pode levar a resultados enviesados.

> ‚ùó **Ponto de Aten√ß√£o**: A padroniza√ß√£o das *features* √© um passo crucial no pr√©-processamento de dados para o k-NN, garantindo que todas as *features* tenham a mesma influ√™ncia no c√°lculo da dist√¢ncia.

### Padroniza√ß√£o das *Features*: M√©dia Zero e Vari√¢ncia Unit√°ria

A **padroniza√ß√£o das *features***, tamb√©m conhecida como *z-score normalization*, √© uma t√©cnica de pr√©-processamento que transforma os dados de forma que cada *feature* tenha m√©dia zero e vari√¢ncia unit√°ria. Essa transforma√ß√£o √© realizada subtraindo a m√©dia de cada *feature* e dividindo pelo seu desvio padr√£o [^13.2].

Formalmente, para cada *feature* $x_i$ no conjunto de dados, o valor padronizado $x_i'$ √© obtido por:

$$x_i' = \frac{x_i - \mu_i}{\sigma_i}$$

Onde $\mu_i$ √© a m√©dia da *feature* $x_i$ no conjunto de treinamento e $\sigma_i$ √© o desvio padr√£o da *feature* $x_i$ no conjunto de treinamento. A m√©dia e o desvio padr√£o s√£o calculados com base no conjunto de treinamento e s√£o usados para padronizar tanto o conjunto de treinamento quanto o conjunto de teste, o que √© crucial para evitar *data leakage*.

A padroniza√ß√£o tem como objetivo remover a influ√™ncia da escala dos *features* no c√°lculo da dist√¢ncia Euclidiana, permitindo que todas as *features* contribuam de forma igual para a medida de proximidade. Sem a padroniza√ß√£o, *features* com escalas maiores podem dominar o c√°lculo da dist√¢ncia, o que leva a resultados enviesados no k-NN e outros m√©todos de classifica√ß√£o que utilizam a dist√¢ncia Euclidiana.

```mermaid
graph LR
    subgraph "Feature Standardization"
        direction TB
        A["x·µ¢' = (x·µ¢ - Œº·µ¢) / œÉ·µ¢"]
        B["x·µ¢': Standardized feature 'i'"]
        C["x·µ¢: Original feature 'i'"]
        D["Œº·µ¢: Mean of feature 'i'"]
        E["œÉ·µ¢: Standard deviation of feature 'i'"]
        A --> B
        A --> C
        A --> D
        A --> E
    end
```

**Lemma 82:** A padroniza√ß√£o de *features* garante que cada *feature* tenha m√©dia zero e vari√¢ncia unit√°ria, o que remove a influ√™ncia da escala das *features* e permite que a dist√¢ncia Euclidiana reflita a proximidade real entre os pontos.
*Prova*: A transforma√ß√£o por padroniza√ß√£o, por defini√ß√£o, garante que a m√©dia dos valores seja 0 e a vari√¢ncia 1, o que faz com que todas as *features* contribuam em igual medida para a dist√¢ncia. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um conjunto de dados com duas *features*, com os seguintes valores para um ponto $x$:
>
> *   *Feature 1*: Valores: \[2, 4, 6, 8, 10]
> *   *Feature 2*: Valores: \[100, 200, 300, 400, 500]
>
> 1.  **C√°lculo da M√©dia e Desvio Padr√£o para cada Feature:**
>
>     *   *Feature 1*:
>         *   M√©dia ($\mu_1$): $(2+4+6+8+10)/5 = 6$
>         *   Desvio Padr√£o ($\sigma_1$): $\sqrt{((2-6)^2 + (4-6)^2 + (6-6)^2 + (8-6)^2 + (10-6)^2)/5} = \sqrt{(16+4+0+4+16)/5} = \sqrt{40/5} = \sqrt{8} \approx 2.83$
>     *   *Feature 2*:
>         *   M√©dia ($\mu_2$): $(100+200+300+400+500)/5 = 300$
>         *   Desvio Padr√£o ($\sigma_2$): $\sqrt{((100-300)^2 + (200-300)^2 + (300-300)^2 + (400-300)^2 + (500-300)^2)/5} = \sqrt{(40000+10000+0+10000+40000)/5} = \sqrt{100000/5} = \sqrt{20000} \approx 141.42$
>
> 2.  **Padroniza√ß√£o de um novo ponto $x = (5, 350)$:**
>
>     *   *Feature 1* padronizada: $x_1' = \frac{5 - 6}{2.83} = \frac{-1}{2.83} \approx -0.35$
>     *   *Feature 2* padronizada: $x_2' = \frac{350 - 300}{141.42} = \frac{50}{141.42} \approx 0.35$
>
>     O ponto $x$ padronizado √© $x' = (-0.35, 0.35)$. Note que os valores foram transformados para uma escala compar√°vel, com m√©dia pr√≥xima de 0 e vari√¢ncia pr√≥xima de 1.

**Corol√°rio 82:** A padroniza√ß√£o √© uma etapa fundamental para utilizar a dist√¢ncia Euclidiana no k-NN, garantindo que todas as *features* tenham o mesmo impacto na escolha dos vizinhos mais pr√≥ximos.

> ‚ö†Ô∏è **Nota Importante**: A padroniza√ß√£o das *features* √© um passo crucial no pr√©-processamento de dados para o k-NN, garantindo que a dist√¢ncia Euclidiana reflita a proximidade real entre os pontos.

> ‚ùó **Ponto de Aten√ß√£o**: A padroniza√ß√£o deve ser feita utilizando a m√©dia e o desvio padr√£o do conjunto de treinamento, evitando que o conjunto de teste influencie a transforma√ß√£o, o que pode levar a resultados enviesados.

### Impacto da Padroniza√ß√£o na Classifica√ß√£o com k-NN

A padroniza√ß√£o das *features* tem um impacto significativo na classifica√ß√£o com o algoritmo k-NN, pois ela influencia a forma como os vizinhos mais pr√≥ximos s√£o identificados [^13.3]. Sem a padroniza√ß√£o, as dist√¢ncias calculadas com a m√©trica Euclidiana podem ser dominadas por *features* que t√™m valores absolutos maiores, o que leva a resultados sub√≥timos.

Com as *features* padronizadas, a dist√¢ncia Euclidiana passa a refletir a proximidade real entre os pontos no espa√ßo de *features*, e o algoritmo k-NN seleciona os vizinhos que s√£o realmente similares ao ponto de consulta, e n√£o os vizinhos que apresentam maiores valores em *features* com escalas maiores. Isso resulta em modelos com melhor capacidade de generaliza√ß√£o e mais robustos a ru√≠do e varia√ß√µes nos dados de treinamento.

A padroniza√ß√£o tamb√©m auxilia a diminuir o impacto da maldi√ß√£o da dimensionalidade, pois ao colocar todas as *features* na mesma escala, a import√¢ncia de cada dimens√£o passa a ser mais compar√°vel.

```mermaid
graph LR
    subgraph "k-NN Classification Impact"
        direction TB
        A["Unstandardized Features"] --> B["Biased Distance Calculation"]
        B --> C["Suboptimal Neighbor Selection"]
        A --> D["Standardized Features"]
        D --> E["Balanced Distance Calculation"]
         E --> F["Optimal Neighbor Selection"]
         C --> G["Lower Generalization"]
         F --> H["Higher Generalization"]
    end
```

**Lemma 83:** A padroniza√ß√£o das *features* melhora o desempenho do k-NN, pois a dist√¢ncia Euclidiana passa a refletir melhor a similaridade entre os pontos, e as *features* contribuem igualmente para a escolha dos vizinhos mais pr√≥ximos.
*Prova*: Sem a padroniza√ß√£o, features com escalas maiores dominam o c√°lculo da dist√¢ncia, enquanto com a padroniza√ß√£o, todos os features passam a ter a mesma import√¢ncia. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos supor um problema de classifica√ß√£o com duas classes, representadas por c√≠rculos azuis e vermelhos, e duas *features*: *idade* (em anos) e *renda* (em reais). Os dados s√£o:
>
> *   Classe Azul: \[ (25, 2000), (30, 2500), (35, 3000) ]
> *   Classe Vermelha: \[ (60, 8000), (65, 8500), (70, 9000) ]
>
> 1.  **Classifica√ß√£o sem Padroniza√ß√£o:**
>     Vamos classificar o ponto de consulta $x = (50, 5000)$ usando k-NN com k=1.
>     *   Dist√¢ncias para os pontos da classe azul:
>         *   $d(x, (25, 2000)) = \sqrt{(50-25)^2 + (5000-2000)^2} = \sqrt{25^2 + 3000^2} \approx 3000.10$
>         *   $d(x, (30, 2500)) = \sqrt{(50-30)^2 + (5000-2500)^2} = \sqrt{20^2 + 2500^2} \approx 2500.08$
>         *  $d(x, (35, 3000)) = \sqrt{(50-35)^2 + (5000-3000)^2} = \sqrt{15^2 + 2000^2} \approx 2000.06$
>     *   Dist√¢ncias para os pontos da classe vermelha:
>          *   $d(x, (60, 8000)) = \sqrt{(50-60)^2 + (5000-8000)^2} = \sqrt{10^2 + 3000^2} \approx 3000.02$
>          *   $d(x, (65, 8500)) = \sqrt{(50-65)^2 + (5000-8500)^2} = \sqrt{15^2 + 3500^2} \approx 3500.03$
>          *   $d(x, (70, 9000)) = \sqrt{(50-70)^2 + (5000-9000)^2} = \sqrt{20^2 + 4000^2} \approx 4000.05$
>
>     O vizinho mais pr√≥ximo √© (35, 3000), da classe azul.
>     No entanto, o ponto de consulta (50, 5000) √© mais pr√≥ximo em termos de idade dos pontos da classe vermelha, mas a renda (que tem valores maiores) acaba dominando a dist√¢ncia, levando a uma classifica√ß√£o incorreta.
>
> 2.  **Classifica√ß√£o com Padroniza√ß√£o:**
>     Ap√≥s a padroniza√ß√£o dos dados, vamos supor que os pontos transformados sejam:
>     *   Classe Azul: \[ (-1.2, -1.2), (-0.8, -0.8), (-0.4, -0.4) ]
>     *   Classe Vermelha: \[ (1.2, 1.2), (1.6, 1.6), (2.0, 2.0) ]
>     *   Ponto de consulta: $x' = (0.4, 0.4)$
>
>     *   Dist√¢ncias para os pontos da classe azul:
>          *   $d(x', (-1.2, -1.2)) = \sqrt{(0.4+1.2)^2 + (0.4+1.2)^2} = \sqrt{1.6^2 + 1.6^2} \approx 2.26$
>          *   $d(x', (-0.8, -0.8)) = \sqrt{(0.4+0.8)^2 + (0.4+0.8)^2} = \sqrt{1.2^2 + 1.2^2} \approx 1.69$
>          *   $d(x', (-0.4, -0.4)) = \sqrt{(0.4+0.4)^2 + (0.4+0.4)^2} = \sqrt{0.8^2 + 0.8^2} \approx 1.13$
>     *   Dist√¢ncias para os pontos da classe vermelha:
>          *   $d(x', (1.2, 1.2)) = \sqrt{(0.4-1.2)^2 + (0.4-1.2)^2} = \sqrt{(-0.8)^2 + (-0.8)^2} \approx 1.13$
>          *   $d(x', (1.6, 1.6)) = \sqrt{(0.4-1.6)^2 + (0.4-1.6)^2} = \sqrt{(-1.2)^2 + (-1.2)^2} \approx 1.69$
>          *   $d(x', (2.0, 2.0)) = \sqrt{(0.4-2.0)^2 + (0.4-2.0)^2} = \sqrt{(-1.6)^2 + (-1.6)^2} \approx 2.26$
>     O vizinho mais pr√≥ximo √© (1.2, 1.2), da classe vermelha, o que √© uma classifica√ß√£o mais coerente com a localiza√ß√£o do ponto de consulta no espa√ßo de features.

**Corol√°rio 83:** Modelos k-NN com dados padronizados tem menor vi√©s e melhor capacidade de generaliza√ß√£o, pois os vizinhos mais pr√≥ximos selecionados s√£o mais representativos da regi√£o local ao ponto de consulta.

> ‚ö†Ô∏è **Nota Importante**: A padroniza√ß√£o das *features* √© essencial para a utiliza√ß√£o da dist√¢ncia Euclidiana no k-NN, garantindo que a escolha dos vizinhos mais pr√≥ximos seja feita de forma correta e equilibrada.

> ‚ùó **Ponto de Aten√ß√£o**: A padroniza√ß√£o dos dados √© um passo essencial para evitar que *features* com escalas maiores dominem o c√°lculo da dist√¢ncia e para garantir o bom funcionamento do k-NN.

### M√©tricas de Dist√¢ncia Alternativas: Adapta√ß√£o √† Estrutura dos Dados

Embora a dist√¢ncia Euclidiana seja a m√©trica mais comum no k-NN, existem outras m√©tricas de dist√¢ncia que podem ser utilizadas em situa√ß√µes espec√≠ficas para adaptar a escolha dos vizinhos mais pr√≥ximos √† estrutura dos dados [^13.3].

Algumas m√©tricas de dist√¢ncia alternativas incluem:

1.  **Dist√¢ncia de Manhattan:** Tamb√©m conhecida como dist√¢ncia da cidade ou dist√¢ncia L1, a dist√¢ncia de Manhattan √© a soma dos valores absolutos das diferen√ßas entre as coordenadas dos pontos:

    $$d(x, y) = \sum_{i=1}^p |x_i - y_i|$$

   A dist√¢ncia de Manhattan pode ser mais apropriada do que a dist√¢ncia Euclidiana em espa√ßos onde as dimens√µes n√£o s√£o uniformemente escaladas, ou para dados categ√≥ricos, sendo menos suscet√≠vel a *outliers* em compara√ß√£o com a dist√¢ncia Euclidiana.

2.  **Dist√¢ncia de Mahalanobis:** A dist√¢ncia de Mahalanobis leva em considera√ß√£o a covari√¢ncia entre as *features*, o que pode ser √∫til em dados onde as *features* s√£o correlacionadas:

    $$d(x, y) = \sqrt{(x - y)^T \Sigma^{-1} (x - y)}$$

   Onde $\Sigma$ √© a matriz de covari√¢ncia das *features*. A dist√¢ncia de Mahalanobis pode ser utilizada quando a distribui√ß√£o dos dados n√£o √© isotr√≥pica, o que implica que as *features* t√™m diferentes vari√¢ncias ou covari√¢ncias.
3.  **Dist√¢ncia Cossenoidal:** A dist√¢ncia cossenoidal mede o √¢ngulo entre dois vetores, e n√£o a dist√¢ncia entre seus pontos finais. A dist√¢ncia cossenoidal √© dada por:

     $$d(x,y) = 1 - \frac{x \cdot y}{||x|| ||y||}$$

    Onde $x \cdot y$ √© o produto interno dos vetores $x$ e $y$, e $||x||$ e $||y||$ s√£o os seus m√≥dulos. Esta m√©trica √© utilizada quando o padr√£o do dado (sua dire√ß√£o) √© mais importante do que sua magnitude, como em documentos de texto ou an√°lise de imagens.

```mermaid
graph LR
    subgraph "Alternative Distance Metrics"
        direction TB
        A["Manhattan Distance: d(x, y) = ‚àë|x·µ¢ - y·µ¢|"]
        B["Mahalanobis Distance: d(x, y) = sqrt((x - y)·µÄ Œ£‚Åª¬π (x - y))"]
        C["Cosine Distance: d(x,y) = 1 - (x ‚ãÖ y) / (||x|| ||y||)"]
       end
```

> üí° **Exemplo Num√©rico:**
>
> Vamos comparar a dist√¢ncia Euclidiana e a dist√¢ncia de Manhattan para dois pontos $x = (3, 4)$ e $y = (7, 1)$.
>
> 1.  **Dist√¢ncia Euclidiana:**
>
>     $d(x, y) = \sqrt{(3-7)^2 + (4-1)^2} = \sqrt{(-4)^2 + (3)^2} = \sqrt{16 + 9} = \sqrt{25} = 5$
>
> 2.  **Dist√¢ncia de Manhattan:**
>
>     $d(x, y) = |3-7| + |4-1| = |-4| + |3| = 4 + 3 = 7$
>
>  A dist√¢ncia de Manhattan √© maior que a dist√¢ncia Euclidiana nesse caso. A dist√¢ncia de Manhattan √© mais sens√≠vel a diferen√ßas nas coordenadas individuais, enquanto a dist√¢ncia Euclidiana leva em considera√ß√£o a dist√¢ncia em linha reta entre os pontos.
>
> Vamos agora calcular a dist√¢ncia cossenoidal entre dois vetores: $x = (1, 0)$ e $y = (1, 1)$.
>
> 1.  **Dist√¢ncia Cossenoidal:**
>     $x \cdot y = (1 * 1) + (0 * 1) = 1$
>     $||x|| = \sqrt{1^2 + 0^2} = 1$
>     $||y|| = \sqrt{1^2 + 1^2} = \sqrt{2}$
>     $d(x,y) = 1 - \frac{1}{1 * \sqrt{2}} = 1 - \frac{1}{\sqrt{2}} \approx 1 - 0.707 = 0.293$
>
> Se calcularmos a dist√¢ncia cossenoidal entre $x = (1, 0)$ e $z = (2, 0)$, que est√£o na mesma dire√ß√£o, temos:
>
> $x \cdot z = (1 * 2) + (0 * 0) = 2$
> $||x|| = 1$
> $||z|| = \sqrt{2^2 + 0^2} = 2$
> $d(x,z) = 1 - \frac{2}{1 * 2} = 1 - 1 = 0$
>
> A dist√¢ncia cossenoidal entre x e z √© 0, pois eles est√£o na mesma dire√ß√£o, independentemente da sua magnitude.

**Lemma 84:** A escolha da m√©trica de dist√¢ncia no k-NN deve levar em considera√ß√£o a estrutura dos dados e a natureza das *features*, o que pode melhorar o desempenho do modelo e a escolha dos vizinhos mais apropriados.
*Prova*: O uso de uma m√©trica de dist√¢ncia apropriada garante que a compara√ß√£o entre os pontos seja feita utilizando crit√©rios adequados para a natureza dos dados, levando a modelos mais precisos e com melhor desempenho. $\blacksquare$

**Corol√°rio 84:** A padroniza√ß√£o √© especialmente importante quando se usa a dist√¢ncia Euclidiana, mas mesmo para outras m√©tricas, o pr√©-processamento dos dados pode aumentar a sua capacidade de discrimina√ß√£o.

> ‚ö†Ô∏è **Nota Importante**: Embora a dist√¢ncia Euclidiana seja a m√©trica mais comum, a escolha da m√©trica de dist√¢ncia no k-NN deve ser feita com base na estrutura dos dados e na necessidade de lidar com especificidades dos dados.

> ‚ùó **Ponto de Aten√ß√£o**: M√©tricas de dist√¢ncia adaptativas podem melhorar o desempenho do k-NN, especialmente em espa√ßos de alta dimens√£o.

### Conclus√£o

A dist√¢ncia Euclidiana √© uma m√©trica de proximidade amplamente utilizada no k-NN, mas sua aplica√ß√£o direta a dados com escalas diferentes nas *features* pode levar a resultados enviesados. A padroniza√ß√£o das *features*, com m√©dia zero e vari√¢ncia um, √© um passo essencial no pr√©-processamento de dados para o k-NN, garantindo que todas as *features* tenham o mesmo impacto no c√°lculo da dist√¢ncia. A escolha de uma m√©trica de dist√¢ncia apropriada, que leve em considera√ß√£o a estrutura dos dados, pode melhorar ainda mais o desempenho do k-NN, especialmente em casos onde a dist√¢ncia Euclidiana n√£o √© a m√©trica mais adequada.

### Footnotes

[^13.3]: "These classifiers are memory-based, and require no model to be fit. Given a query point xo, we find the k training points x(r), r = 1,..., k closest in distance to xo, and then classify using majority vote among the k neighbors." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.2]: "Throughout this chapter, our training data consists of the N pairs (x1,91),...,(xn, 9N) where gi is a class label taking values in {1, 2, . . ., K}. Prototype methods represent the training data by a set of points in feature space. These prototypes are typically not examples from the training sample, except in the case of 1-nearest-neighbor classification discussed later. Each prototype has an associated class label, and classification of a query point x is made to the class of the closest prototype. "Closest" is usually defined by Euclidean distance in the feature space, after each feature has been standardized to have overall mean 0 and variance 1 in the training sample." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
