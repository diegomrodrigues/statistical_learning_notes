## O Problema da Alta Dimensionalidade: A Maldi√ß√£o da Dimensionalidade e a Adapta√ß√£o do k-NN √† Estrutura Local dos Dados

```mermaid
graph LR
    A["High-Dimensional Space"] --> B("Decreased Data Density");
    A --> C("Less Informative Distances");
    B --> D("Impact on k-NN");
    C --> D;
    D --> E("Need for Adaptation");
    E --> F("Local Structure Adaptation Techniques");
    F-->G("Feature Selection");
    F-->H("Dimensionality Reduction");
    F-->I("Adaptive Distance Metrics");
    F-->J("Variable k");
```

### Introdu√ß√£o

Este cap√≠tulo explora o problema da **alta dimensionalidade** em aprendizado de m√°quina, com foco na **maldi√ß√£o da dimensionalidade** e suas implica√ß√µes para o m√©todo de **k-vizinhos mais pr√≥ximos (k-NN)** [^13.4]. Em espa√ßos de *features* de alta dimens√£o, a densidade dos dados diminui e a dist√¢ncia entre os pontos se torna menos informativa, o que afeta o desempenho do k-NN e outros algoritmos que dependem de m√©tricas de dist√¢ncia. Analisaremos como a **adapta√ß√£o do k-NN √† estrutura local dos dados** pode mitigar esses problemas, utilizando t√©cnicas como sele√ß√£o de *features*, redu√ß√£o de dimensionalidade, m√©tricas de dist√¢ncia adaptativas e o uso de k vari√°vel, e como essas adapta√ß√µes podem melhorar a capacidade de generaliza√ß√£o do modelo em cen√°rios com dados de alta dimens√£o.

### A Maldi√ß√£o da Dimensionalidade: Um Desafio em Espa√ßos de Alta Dimens√£o

A **maldi√ß√£o da dimensionalidade** √© um fen√¥meno que ocorre em espa√ßos de *features* com alta dimens√£o, onde a densidade dos dados diminui exponencialmente com o aumento da dimens√£o [^13.4]. √Ä medida que o n√∫mero de *features* (dimens√µes) aumenta, o volume do espa√ßo de *features* tamb√©m aumenta, e a dist√¢ncia entre os pontos se torna mais uniforme, ou seja, as dist√¢ncias entre os pontos se tornam maiores, mas a diferen√ßa entre elas diminui, o que diminui a capacidade do algoritmo k-NN de selecionar os vizinhos mais pr√≥ximos relevantes e a qualidade do modelo.

> üí° **Exemplo Num√©rico:**
>Imagine um conjunto de dados com apenas uma feature, onde os pontos est√£o distribu√≠dos em um intervalo de 0 a 10. Se tivermos 10 pontos, a densidade √© de aproximadamente 1 ponto por unidade de intervalo. Agora, considere adicionar uma segunda feature, tamb√©m variando de 0 a 10. O espa√ßo agora √© um quadrado de 10x10, e se mantivermos os mesmos 10 pontos, eles estar√£o muito mais esparsos. Para manter a mesma densidade, precisar√≠amos de 100 pontos (10^2). Se adicionarmos uma terceira dimens√£o (0 a 10), o espa√ßo se torna um cubo de 10x10x10, e precisar√≠amos de 1000 pontos (10^3) para ter a mesma densidade original. Isso demonstra o crescimento exponencial da necessidade de dados para manter a densidade em espa√ßos de alta dimens√£o.
>
>  Em termos de dist√¢ncia, em uma dimens√£o, a dist√¢ncia m√©dia entre pontos pode ser relativamente pequena. Em duas dimens√µes, a dist√¢ncia m√©dia j√° aumenta, e assim por diante. Al√©m disso, a diferen√ßa entre a dist√¢ncia do vizinho mais pr√≥ximo e o mais distante tende a diminuir, tornando a escolha do vizinho menos informativa.

```mermaid
graph LR
    subgraph "Dimensionality Effects"
        direction TB
        A["1 Dimension: Density 'œÅ'"]
        B["2 Dimensions: Density 'œÅ¬≤'"]
        C["3 Dimensions: Density 'œÅ¬≥'"]
        A --> B
        B --> C
        D["Number of Data Points Needed = 'œÅ^n'"]
        C --> D
    end
```
A maldi√ß√£o da dimensionalidade apresenta os seguintes desafios:

1.  **Densidade de Dados Diminui:** Em espa√ßos de alta dimens√£o, os dados se tornam cada vez mais esparsos, ou seja, a quantidade de dados por unidade de volume diminui exponencialmente com o aumento da dimens√£o.
2.  **Dist√¢ncia Perde Informatividade:** A dist√¢ncia entre os pontos se torna menos informativa, pois a dist√¢ncia entre um ponto e seus vizinhos mais pr√≥ximos tende a aumentar, e a diferen√ßa entre as dist√¢ncias tende a diminuir com o aumento da dimensionalidade, fazendo com que a escolha dos vizinhos n√£o seja t√£o relevante.
3.  **Overfitting:** Modelos de aprendizado de m√°quina, como o k-NN, podem se tornar suscet√≠veis ao *overfitting* em espa√ßos de alta dimens√£o, devido √† sua capacidade de se adaptar ao ru√≠do nos dados.

A maldi√ß√£o da dimensionalidade √© um problema s√©rio em muitas aplica√ß√µes de aprendizado de m√°quina, incluindo reconhecimento de imagens, processamento de linguagem natural, e bioinform√°tica, onde o n√∫mero de *features* pode ser muito grande.

**Lemma 131:** A maldi√ß√£o da dimensionalidade surge do fato de que a densidade de dados diminui exponencialmente com o aumento da dimens√£o, tornando a dist√¢ncia entre os pontos menos informativa, e levando a modelos com menor capacidade de generaliza√ß√£o.
*Prova*: O volume de um espa√ßo de alta dimens√£o aumenta exponencialmente com o n√∫mero de dimens√µes, tornando os dados mais esparsos, e as dist√¢ncias menos discriminativas. $\blacksquare$

**Corol√°rio 131:** A maldi√ß√£o da dimensionalidade imp√µe a necessidade de adaptar m√©todos de aprendizado de m√°quina, como o k-NN, para lidar com dados de alta dimens√£o.

### Adapta√ß√£o do k-NN √† Estrutura Local: Mitigando os Efeitos da Maldi√ß√£o

Para mitigar os efeitos da **maldi√ß√£o da dimensionalidade** no k-NN, existem diversas abordagens que buscam adaptar a classifica√ß√£o √† **estrutura local dos dados** [^13.4]. A ideia central √© que a escolha dos vizinhos mais pr√≥ximos seja feita com base em informa√ß√µes relevantes sobre a regi√£o do espa√ßo de *features* onde o ponto de consulta se encontra. Algumas das abordagens incluem:

```mermaid
graph TD
    subgraph "k-NN Adaptation Strategies"
    direction TB
        A["k-NN Algorithm"]
        B["Feature Selection"]
        C["Dimensionality Reduction"]
        D["Adaptive Distance Metrics"]
        E["Variable k"]
        F["Weighted Neighbors"]
        A --> B
        A --> C
        A --> D
        A --> E
        A --> F
    end
```

1.  **Sele√ß√£o de *Features*:** Selecionar um subconjunto de *features* que sejam relevantes para a classifica√ß√£o em uma determinada regi√£o do espa√ßo de *features*. A sele√ß√£o de *features* pode ser feita por m√©todos de filtro, *wrapper* ou *embedded methods*.
2.  **Redu√ß√£o de Dimensionalidade:** Utilizar t√©cnicas de redu√ß√£o de dimensionalidade, como a an√°lise de componentes principais (PCA) ou outras abordagens lineares e n√£o lineares, para projetar os dados em um subespa√ßo de menor dimens√£o onde as informa√ß√µes relevantes s√£o preservadas.
3.  **M√©tricas de Dist√¢ncia Adaptativas:** Criar m√©tricas de dist√¢ncia que se adaptem √† estrutura local dos dados, dando maior peso √†s *features* que s√£o mais importantes para a classifica√ß√£o em cada regi√£o do espa√ßo de *features*.
4.  **k Vari√°vel:** Utilizar um valor vari√°vel de $k$ que se adapta √† densidade local dos dados. Em regi√µes de alta densidade, um menor n√∫mero de vizinhos pode ser suficiente, enquanto em regi√µes de baixa densidade, um n√∫mero maior de vizinhos √© utilizado.
5.  **Vizinhos Ponderados:** Atribuir pesos diferentes aos vizinhos mais pr√≥ximos, dando maior peso aos vizinhos que s√£o mais relevantes para a classifica√ß√£o, com base na dist√¢ncia ou outras medidas.

**Lemma 132:** A adapta√ß√£o do k-NN √† estrutura local dos dados permite mitigar os efeitos da maldi√ß√£o da dimensionalidade, melhorando a capacidade do modelo de selecionar os vizinhos mais pr√≥ximos que s√£o relevantes para a classifica√ß√£o.
*Prova*: Ao adaptar a vizinhan√ßa do k-NN ou a m√©trica de dist√¢ncia √† estrutura local dos dados, a sele√ß√£o de vizinhos passa a levar em considera√ß√£o informa√ß√µes mais relevantes para a classifica√ß√£o. $\blacksquare$

**Corol√°rio 132:** A combina√ß√£o de t√©cnicas de redu√ß√£o de dimensionalidade, sele√ß√£o de *features* e m√©tricas de dist√¢ncia adaptativas pode resultar em modelos k-NN mais robustos e com melhor capacidade de generaliza√ß√£o em problemas de alta dimens√£o.

### Abordagens de Adapta√ß√£o: Sele√ß√£o de *Features* e Redu√ß√£o de Dimensionalidade

A **sele√ß√£o de *features*** e a **redu√ß√£o de dimensionalidade** s√£o t√©cnicas que buscam reduzir o n√∫mero de *features* utilizadas pelo k-NN, o que pode mitigar o impacto da maldi√ß√£o da dimensionalidade [^13.4].

```mermaid
graph LR
    subgraph "Feature Selection & Dimensionality Reduction"
        direction TB
        A["Original High-Dimensional Data"]
        B["Feature Selection"]
        C["Dimensionality Reduction"]
        A --> B
        A --> C
        B --> D["Reduced Feature Space"]
        C --> D
        D --> E["k-NN Algorithm"]
    end
```

1.  **Sele√ß√£o de *Features*:** A sele√ß√£o de *features* consiste em identificar e utilizar apenas um subconjunto de *features* que s√£o mais relevantes para a classifica√ß√£o. Existem diferentes m√©todos de sele√ß√£o de *features*, incluindo:
    *   **M√©todos de Filtro:** Avaliar a relev√¢ncia das *features* de forma independente do modelo de classifica√ß√£o, utilizando m√©tricas como a correla√ß√£o ou informa√ß√£o m√∫tua.
    *   **M√©todos *Wrapper*:** Selecionar um subconjunto de *features* que maximize o desempenho do modelo, avaliando o desempenho do k-NN com diferentes subconjuntos de *features*.
    *   **M√©todos *Embedded*:** Realizar a sele√ß√£o de *features* como parte do processo de treinamento do modelo, como no caso de algoritmos que utilizam regulariza√ß√£o L1.

> üí° **Exemplo Num√©rico:**
> Suponha que temos um conjunto de dados com 10 *features* para classificar imagens de gatos e cachorros. Algumas dessas *features* podem ser: tamanho da imagem, n√∫mero de cores, m√©dia de brilho, detec√ß√£o de bordas, presen√ßa de orelhas triangulares, presen√ßa de focinho alongado, etc.
>
>  **M√©todo de Filtro:** Poder√≠amos calcular a correla√ß√£o entre cada *feature* e a classe (gato/cachorro). Se a *feature* "presen√ßa de orelhas triangulares" tiver uma alta correla√ß√£o com a classe "gato" e baixa correla√ß√£o com a classe "cachorro", e a *feature* "presen√ßa de focinho alongado" tiver o oposto, essas *features* seriam consideradas mais relevantes. *Features* como "tamanho da imagem" ou "n√∫mero de cores" poderiam ter baixa correla√ß√£o com a classe e ser descartadas.
>
>  **M√©todo *Wrapper*:** Come√ßar√≠amos com um conjunto inicial de *features* (por exemplo, todas as 10) e treinar√≠amos um modelo k-NN. Em seguida, avaliar√≠amos o desempenho (precis√£o, recall, etc.). Poder√≠amos ent√£o remover uma *feature* e avaliar novamente, repetindo esse processo para todas as combina√ß√µes poss√≠veis de *features* ou usando uma busca gulosa. O subconjunto de *features* que resultar no melhor desempenho do k-NN seria selecionado.
>
>  **M√©todo *Embedded*:** Se us√°ssemos um modelo de classifica√ß√£o que inclua regulariza√ß√£o L1 (como um modelo linear com regulariza√ß√£o Lasso), o processo de treinamento j√° selecionaria as *features* mais relevantes, atribuindo pesos pr√≥ximos de zero √†s *features* menos importantes.

```mermaid
graph LR
    subgraph "Feature Selection Methods"
        direction TB
        A["All Features"]
        B["Filter Methods"]
        C["Wrapper Methods"]
        D["Embedded Methods"]
        A --> B
        A --> C
        A --> D
        B --> E["Selected Features"]
        C --> E
        D --> E
    end
```

2.  **Redu√ß√£o de Dimensionalidade:** A redu√ß√£o de dimensionalidade consiste em projetar os dados em um espa√ßo de menor dimens√£o, mantendo a estrutura das *features* relevantes. T√©cnicas de redu√ß√£o de dimensionalidade incluem:
    *   **An√°lise de Componentes Principais (PCA):** Projetar os dados em um subespa√ßo de baixa dimens√£o que capture a maior parte da vari√¢ncia dos dados.
    *   **An√°lise Discriminante Linear (LDA):** Projetar os dados em um subespa√ßo de baixa dimens√£o onde as classes s√£o mais separ√°veis.
    * **Autoencoders:** Redes neurais que podem comprimir os dados em espa√ßos de menor dimens√£o com diferentes graus de n√£o linearidade.

> üí° **Exemplo Num√©rico:**
> Imagine um conjunto de dados com duas *features* altamente correlacionadas, como altura em cent√≠metros e altura em polegadas. Essas *features* essencialmente representam a mesma informa√ß√£o. O PCA identificaria essa correla√ß√£o e criaria uma nova *feature* (componente principal) que captura a vari√¢ncia m√°xima dos dados, eliminando a redund√¢ncia. Se tiv√©ssemos mais *features*, o PCA poderia reduzir a dimensionalidade para um n√∫mero menor de componentes principais, mantendo a maior parte da informa√ß√£o relevante.
>
> ```python
> import numpy as np
> from sklearn.decomposition import PCA
>
> # Dados de exemplo (2 features, 5 amostras)
> X = np.array([[170, 67], [160, 60], [180, 75], [165, 62], [175, 70]])
>
> # Aplicando PCA para reduzir para 1 componente
> pca = PCA(n_components=1)
> X_reduced = pca.fit_transform(X)
>
> print("Dados originais:\n", X)
> print("\nDados reduzidos (PCA):\n", X_reduced)
> print("\nVari√¢ncia explicada:", pca.explained_variance_ratio_)
> ```
>
> Este exemplo mostra como o PCA transforma os dados para um espa√ßo de menor dimens√£o, mantendo a maior parte da vari√¢ncia. A `vari√¢ncia explicada` indica a quantidade de informa√ß√£o retida na nova componente.

```mermaid
graph LR
  subgraph "Dimensionality Reduction Techniques"
    direction TB
    A["Original Data (High Dimension)"]
    B["Principal Component Analysis (PCA)"]
    C["Linear Discriminant Analysis (LDA)"]
    D["Autoencoders"]
    A --> B
    A --> C
    A --> D
    B --> E["Reduced Dimensional Data"]
    C --> E
    D --> E
    end
```

**Lemma 133:** A sele√ß√£o de *features* e a redu√ß√£o de dimensionalidade permitem diminuir o n√∫mero de *features* utilizadas pelo k-NN, o que reduz a complexidade computacional e mitiga o impacto da maldi√ß√£o da dimensionalidade.
*Prova*: A escolha de um subespa√ßo reduzido de *features* faz com que as informa√ß√µes redundantes e ruidosas n√£o sejam mais utilizadas no c√°lculo da dist√¢ncia. $\blacksquare$

**Corol√°rio 133:** T√©cnicas de sele√ß√£o de *features* e redu√ß√£o de dimensionalidade podem ser utilizadas para melhorar o desempenho do k-NN em problemas de alta dimens√£o, ou em problemas onde algumas dimens√µes n√£o cont√©m informa√ß√µes relevantes para a decis√£o.

### Abordagens de Adapta√ß√£o: M√©tricas Adaptativas e k Vari√°vel

Outras abordagens para adaptar o k-NN √† estrutura local dos dados incluem o uso de **m√©tricas de dist√¢ncia adaptativas** e a escolha de um **valor vari√°vel para *k*** [^13.4].

```mermaid
graph LR
    subgraph "Adaptive Metrics & Variable k"
        direction TB
        A["k-NN Algorithm"]
        B["Adaptive Distance Metrics"]
        C["Variable k"]
        A --> B
        A --> C
        B --> D["Improved Local Neighborhood"]
        C --> D
    end
```

1.  **M√©tricas de Dist√¢ncia Adaptativas:** Essas m√©tricas buscam dar maior peso √†s *features* que s√£o mais relevantes para a classifica√ß√£o em uma determinada regi√£o do espa√ßo de *features*. Por exemplo, a dist√¢ncia de Mahalanobis, que considera a covari√¢ncia entre *features*, pode ser utilizada em vez da dist√¢ncia Euclidiana para dar diferentes pesos √†s *features* no c√°lculo da dist√¢ncia.

> üí° **Exemplo Num√©rico:**
>Imagine um conjunto de dados com duas *features*, onde a *feature* 1 varia muito e a *feature* 2 varia pouco. A dist√¢ncia Euclidiana trataria ambas as *features* igualmente, mas a dist√¢ncia de Mahalanobis levaria em conta a vari√¢ncia diferente de cada *feature*. Se a vari√¢ncia da *feature* 1 for muito maior que a da *feature* 2, a dist√¢ncia de Mahalanobis diminuiria o impacto da *feature* 1 no c√°lculo da dist√¢ncia, pois ela √© mais ruidosa.
>
>  A dist√¢ncia de Mahalanobis entre dois pontos $x$ e $y$ √© definida como:
>
>  $$d(x, y) = \sqrt{(x - y)^T \Sigma^{-1} (x - y)}$$
>
> onde $\Sigma$ √© a matriz de covari√¢ncia dos dados.
>
>  Suponha que temos os pontos $x = [1, 2]$ e $y = [3, 4]$ e a matriz de covari√¢ncia √©:
>
> $$\Sigma = \begin{bmatrix} 4 & 1 \\ 1 & 1 \end{bmatrix}$$
>
>  Primeiro, calculamos a inversa da matriz de covari√¢ncia:
>
> $$\Sigma^{-1} = \frac{1}{(4*1 - 1*1)} \begin{bmatrix} 1 & -1 \\ -1 & 4 \end{bmatrix} = \frac{1}{3} \begin{bmatrix} 1 & -1 \\ -1 & 4 \end{bmatrix} = \begin{bmatrix} 1/3 & -1/3 \\ -1/3 & 4/3 \end{bmatrix}$$
>
>  Em seguida, calculamos a diferen√ßa entre os pontos:
>
> $$x - y = \begin{bmatrix} 1 - 3 \\ 2 - 4 \end{bmatrix} = \begin{bmatrix} -2 \\ -2 \end{bmatrix}$$
>
>  Agora, calculamos a dist√¢ncia de Mahalanobis:
>
> $$d(x, y) = \sqrt{\begin{bmatrix} -2 & -2 \end{bmatrix} \begin{bmatrix} 1/3 & -1/3 \\ -1/3 & 4/3 \end{bmatrix} \begin{bmatrix} -2 \\ -2 \end{bmatrix}} = \sqrt{\begin{bmatrix} -2 & -2 \end{bmatrix} \begin{bmatrix} 2/3 \\ -10/3 \end{bmatrix}} = \sqrt{4/3 + 20/3} = \sqrt{24/3} = \sqrt{8} \approx 2.83$$
>
> A dist√¢ncia Euclidiana entre $x$ e $y$ seria $\sqrt{(1-3)^2 + (2-4)^2} = \sqrt{4 + 4} = \sqrt{8} \approx 2.83$. Neste caso espec√≠fico, as dist√¢ncias coincidem, mas ao usar a matriz de covari√¢ncia, a m√©trica de Mahalanobis leva em conta a forma da distribui√ß√£o dos dados. Se a covari√¢ncia fosse diferente, as dist√¢ncias seriam diferentes.

```mermaid
graph LR
    subgraph "Mahalanobis Distance"
        direction TB
        A["d(x, y) = ‚àö(x - y)·µÄ Œ£‚Åª¬π (x - y)"]
        B["(x - y) Difference Vector"]
        C["Œ£‚Åª¬π Inverse Covariance Matrix"]
        A --> B
        A --> C
        D["Adjusted Distance"]
        B & C --> D
    end
```

2.  **k Vari√°vel:** Essa abordagem busca adaptar o n√∫mero de vizinhos ($k$) utilizado pelo k-NN de acordo com a densidade local dos dados. Em regi√µes com alta densidade de dados, um valor menor de $k$ pode ser suficiente para capturar a estrutura local, enquanto em regi√µes de baixa densidade, um valor maior de $k$ pode ser necess√°rio para obter uma boa estima√ß√£o da distribui√ß√£o local das classes.

> üí° **Exemplo Num√©rico:**
> Imagine um conjunto de dados com regi√µes de alta densidade (muitos pontos pr√≥ximos) e regi√µes de baixa densidade (pontos mais esparsos). Em uma regi√£o de alta densidade, usar um $k$ pequeno (por exemplo, 3) pode ser suficiente para classificar um novo ponto, pois os vizinhos mais pr√≥ximos s√£o muito representativos da regi√£o. No entanto, em uma regi√£o de baixa densidade, um $k$ maior (por exemplo, 10) pode ser necess√°rio para incluir vizinhos relevantes e evitar ru√≠do.
>
>  Uma forma de implementar o k vari√°vel √© usando uma fun√ß√£o que relaciona o valor de k com a densidade local. Por exemplo, poder√≠amos definir que $k = \sqrt{n}$, onde $n$ √© o n√∫mero de pontos dentro de um raio de dist√¢ncia pr√©-definido. Em regi√µes mais densas, $n$ seria maior e, portanto, $k$ tamb√©m seria maior (mas o resultado de $k$ seria menor que $n$ por causa da raiz quadrada), e vice versa.

```mermaid
graph LR
    subgraph "Variable k"
        direction TB
        A["Local Data Density"]
        B["High Density Region"]
        C["Low Density Region"]
        A --> B
        A --> C
        B --> D["Small 'k' Value"]
        C --> E["Large 'k' Value"]
        D & E --> F["k-NN Classification"]
    end
```

A combina√ß√£o dessas abordagens pode levar a modelos de k-NN mais robustos e eficazes em problemas com alta dimensionalidade e com diferentes n√≠veis de densidade.

**Lemma 134:** M√©tricas de dist√¢ncia adaptativas e o uso de um valor vari√°vel de $k$ no k-NN permitem que o modelo se adapte √†s caracter√≠sticas locais dos dados, o que √© particularmente √∫til em espa√ßos de alta dimens√£o, onde a distribui√ß√£o de dados n√£o √© uniforme.
*Prova*: A utiliza√ß√£o de m√©tricas de dist√¢ncia adaptativas permite a escolha de vizinhos mais pr√≥ximos em rela√ß√£o a uma medida de proximidade que leva em conta a estrutura local dos dados, e a adapta√ß√£o do valor de $k$ permite que o modelo considere apenas a regi√£o local para fazer a classifica√ß√£o. $\blacksquare$

**Corol√°rio 134:** A combina√ß√£o de m√©todos de adapta√ß√£o local no k-NN resulta em um modelo mais flex√≠vel e que equilibra a influ√™ncia de caracter√≠sticas locais e globais do conjunto de dados.

### Conclus√£o

A maldi√ß√£o da dimensionalidade √© um desafio fundamental em problemas de aprendizado de m√°quina que utilizam m√©tricas de dist√¢ncia, e o k-NN n√£o √© uma exce√ß√£o. A adapta√ß√£o do k-NN √† estrutura local dos dados, seja por meio da sele√ß√£o de *features*, redu√ß√£o de dimensionalidade, m√©tricas adaptativas ou a varia√ß√£o de $k$, permite mitigar os efeitos da maldi√ß√£o da dimensionalidade e obter modelos mais robustos e eficazes para problemas de alta dimens√£o. A escolha da abordagem mais apropriada depende da natureza dos dados, da complexidade do problema e da necessidade de equil√≠brio entre a complexidade do modelo e sua capacidade de generaliza√ß√£o.

### Footnotes

[^13.4]: "When nearest-neighbor classification is carried out in a high-dimensional feature space, the nearest neighbors of a point can be very far away, causing bias and degrading the performance of the rule...In general, this calls for adapting the metric used in nearest-neighbor classification, so that the resulting neighborhoods stretch out in directions for which the class probabilities don't change much." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*
