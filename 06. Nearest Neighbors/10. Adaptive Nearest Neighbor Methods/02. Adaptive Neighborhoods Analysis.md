## Vizinhan√ßas Adaptativas: Estendendo Regi√µes de Proximidade e M√©tricas Transformadas Localmente com An√°lise Discriminante Local

```mermaid
graph TD
    subgraph "Adaptive Neighborhood Formation"
    direction TB
    A["Input Data with Features"]
    B["Select Query Point"]
    C["Initial Neighborhood: k-NN"]
    D["Local Discriminant Analysis (LDA)"]
    E["Transform Local Metric"]
    F["Adaptive Neighborhood Region"]
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora o conceito de **vizinhan√ßas adaptativas** no contexto do m√©todo de **k-vizinhos mais pr√≥ximos (k-NN)**, com foco em como estender as regi√µes de proximidade e utilizar **m√©tricas transformadas localmente**, por meio da **an√°lise discriminante local**, para melhorar a capacidade de o k-NN capturar a estrutura dos dados e realizar classifica√ß√µes mais precisas em diferentes regi√µes do espa√ßo de *features* [^13.4]. Analisaremos como essas abordagens buscam mitigar os efeitos da maldi√ß√£o da dimensionalidade e como a adapta√ß√£o das vizinhan√ßas ao contexto local permite que o k-NN lide melhor com varia√ß√µes na densidade dos dados e com fronteiras de decis√£o complexas.

### Vizinhan√ßas Adaptativas: Estendendo a Proximidade no k-NN

A abordagem padr√£o do **k-NN** utiliza uma regi√£o de proximidade circular ou esf√©rica em torno do ponto de consulta, onde a dist√¢ncia entre o ponto de consulta e seus vizinhos √© medida utilizando uma m√©trica predefinida (geralmente a dist√¢ncia Euclidiana) [^13.3]. No entanto, essa abordagem pode ser limitante em problemas onde a estrutura dos dados varia em diferentes regi√µes do espa√ßo de *features*.

O conceito de **vizinhan√ßas adaptativas** busca estender a no√ß√£o de proximidade no k-NN, utilizando regi√µes de vizinhan√ßa com formas que se adaptem √† estrutura local dos dados. Em vez de considerar vizinhos igualmente em todas as dire√ß√µes, as vizinhan√ßas adaptativas procuram estender a regi√£o de proximidade nas dire√ß√µes em que a distribui√ß√£o dos dados apresenta maior relev√¢ncia para a classifica√ß√£o, e encurtar em dire√ß√µes irrelevantes ou com baixa variabilidade.

Essa abordagem permite capturar melhor as rela√ß√µes entre os dados nas regi√µes locais, e adaptar o processo de classifica√ß√£o √† estrutura dos dados, mitigando os efeitos da maldi√ß√£o da dimensionalidade e melhorando a capacidade de generaliza√ß√£o do modelo. A adapta√ß√£o das vizinhan√ßas pode ser feita por meio de diferentes t√©cnicas, como o estiramento das regi√µes de proximidade, a utiliza√ß√£o de m√©tricas de dist√¢ncia locais ou a combina√ß√£o de diferentes abordagens.

**Lemma 135:** A utiliza√ß√£o de vizinhan√ßas adaptativas permite que o k-NN capture a estrutura local dos dados de forma mais eficiente do que o uso de regi√µes de proximidade fixas, o que pode melhorar a precis√£o das decis√µes de classifica√ß√£o em espa√ßos de alta dimens√£o e com distribui√ß√µes complexas.
*Prova*: A capacidade de adaptar a regi√£o de proximidade permite que o modelo capture as caracter√≠sticas da regi√£o, e que a proximidade seja medida em rela√ß√£o a uma m√©trica adaptada ao contexto. $\blacksquare$

**Corol√°rio 135:** As vizinhan√ßas adaptativas s√£o especialmente √∫teis em problemas onde a densidade e as propriedades das classes variam em diferentes regi√µes do espa√ßo de *features*.

> üí° **Exemplo Num√©rico:**
>
> Imagine um problema de classifica√ß√£o com duas classes, onde os dados da classe A est√£o concentrados em uma regi√£o alongada ao longo do eixo x, e os dados da classe B est√£o concentrados em uma regi√£o alongada ao longo do eixo y. Se usarmos o k-NN padr√£o com dist√¢ncia Euclidiana, um ponto de consulta na borda da classe A pode ter vizinhos da classe B como os mais pr√≥ximos, levando a uma classifica√ß√£o incorreta. No entanto, se usarmos vizinhan√ßas adaptativas que se alongam ao longo do eixo x para pontos na regi√£o da classe A e ao longo do eixo y para pontos na regi√£o da classe B, o k-NN poder√° identificar os vizinhos mais relevantes e melhorar a precis√£o da classifica√ß√£o.
>
> Para ilustrar, vamos gerar dados sint√©ticos e comparar o k-NN padr√£o com uma vers√£o adaptativa:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.neighbors import KNeighborsClassifier
> from sklearn.metrics import accuracy_score
>
> # Gera dados sint√©ticos
> np.random.seed(42)
> class_a_x = np.random.normal(2, 0.5, 100)
> class_a_y = np.random.normal(2, 0.1, 100)
> class_b_x = np.random.normal(2, 0.1, 100)
> class_b_y = np.random.normal(4, 0.5, 100)
>
> X = np.vstack((np.column_stack((class_a_x, class_a_y)), np.column_stack((class_b_x, class_b_y))))
> y = np.array([0] * 100 + [1] * 100)
>
> # k-NN padr√£o
> knn = KNeighborsClassifier(n_neighbors=5)
> knn.fit(X, y)
> y_pred_std = knn.predict(X)
> acc_std = accuracy_score(y, y_pred_std)
>
> # k-NN adaptativo (simplificado, para fins de ilustra√ß√£o)
> # Neste exemplo, vamos alongar manualmente a vizinhan√ßa para os pontos da classe A
> # ao longo do eixo x e para os pontos da classe B ao longo do eixo y.
> # Uma implementa√ß√£o real da vizinhan√ßa adaptativa seria mais complexa.
> def adaptive_distance(x1, x2, class_label):
>     if class_label == 0: # Classe A
>         return np.sqrt((x1[0] - x2[0])**2 + 10*(x1[1] - x2[1])**2)
>     else: # Classe B
>         return np.sqrt(10*(x1[0] - x2[0])**2 + (x1[1] - x2[1])**2)
>
> def predict_adaptive_knn(X_train, y_train, x_test, k):
>     distances = []
>     for i, x_train in enumerate(X_train):
>         distances.append((adaptive_distance(x_test, x_train, y_train[i]), y_train[i]))
>     distances.sort(key=lambda x: x[0])
>     neighbors = distances[:k]
>     labels = [label for _, label in neighbors]
>     return max(set(labels), key=labels.count)
>
> y_pred_adapt = np.array([predict_adaptive_knn(X, y, x, 5) for x in X])
> acc_adapt = accuracy_score(y, y_pred_adapt)
>
> print(f"Acur√°cia k-NN Padr√£o: {acc_std:.2f}")
> print(f"Acur√°cia k-NN Adaptativo (simplificado): {acc_adapt:.2f}")
>
> # Visualiza√ß√£o (simplificada)
> plt.figure(figsize=(8, 6))
> plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')
> plt.title("Dados Sint√©ticos e Classifica√ß√£o (Simplificada)")
> plt.xlabel("Feature 1")
> plt.ylabel("Feature 2")
> plt.show()
> ```
>
> Este exemplo ilustra um caso simplificado onde a vizinhan√ßa adaptativa consegue um desempenho melhor (maior acur√°cia) do que o k-NN padr√£o.

> ‚ö†Ô∏è **Nota Importante**: Vizinhan√ßas adaptativas permitem que o k-NN ajuste a regi√£o de proximidade de acordo com a estrutura local dos dados, o que √© √∫til em espa√ßos de alta dimens√£o.

> ‚ùó **Ponto de Aten√ß√£o**: A implementa√ß√£o de vizinhan√ßas adaptativas pode aumentar a complexidade computacional do k-NN, e a escolha da abordagem mais adequada depende do problema espec√≠fico.

### M√©tricas Transformadas Localmente: An√°lise Discriminante Local

Uma abordagem para construir **m√©tricas de dist√¢ncia adaptativas** √© utilizar a **an√°lise discriminante local**, que consiste em aplicar o conceito da an√°lise discriminante linear (LDA) em vizinhan√ßas locais do espa√ßo de *features*, de forma que as m√©tricas de dist√¢ncia se adaptem ao contexto local [^13.4].

```mermaid
graph TB
    subgraph "Local Discriminant Analysis (LDA)"
        direction TB
        A["Input Data with Features"]
        B["Select Query Point"]
        C["Define Local Neighborhood"]
        D["Apply LDA Locally"]
        E["Compute Transformation"]
        F["Local Metric Transformation"]
        A --> B
        B --> C
        C --> D
        D --> E
        E --> F
    end
```

A **an√°lise discriminante linear (LDA)** √© um m√©todo de redu√ß√£o de dimensionalidade que busca encontrar um subespa√ßo linear onde as classes s√£o mais separ√°veis [^4.3]. A LDA maximiza a dist√¢ncia entre as m√©dias das classes e minimiza a vari√¢ncia dentro das classes. Em vez de aplicar a LDA globalmente, a **an√°lise discriminante local** aplica a LDA em regi√µes locais do espa√ßo de *features*, calculando as transforma√ß√µes que melhor separam as classes em torno de cada ponto de consulta. A ideia central √© adaptar o c√°lculo da dist√¢ncia √†s caracter√≠sticas da distribui√ß√£o local dos dados.

A abordagem de m√©tricas transformadas localmente envolve os seguintes passos:

1.  **Forma√ß√£o da Vizinhan√ßa:** Para cada ponto de consulta, s√£o selecionados os pontos de treinamento mais pr√≥ximos (por exemplo, os $k$ vizinhos mais pr√≥ximos, ou usando um raio fixo).
2.  **An√°lise Discriminante Local:** Aplica-se a LDA localmente na vizinhan√ßa de cada ponto de consulta, calculando a proje√ß√£o linear que maximiza a separa√ß√£o entre classes na regi√£o local.
3.  **Transforma√ß√£o da M√©trica:** A dist√¢ncia entre o ponto de consulta e seus vizinhos √© calculada utilizando uma m√©trica que leva em considera√ß√£o a proje√ß√£o obtida na LDA.

Com essa abordagem, a m√©trica de dist√¢ncia passa a se adaptar √†s caracter√≠sticas locais dos dados e o k-NN passa a utilizar uma m√©trica que considera as rela√ß√µes entre as *features* para selecionar os vizinhos mais relevantes.

**Lemma 136:** A an√°lise discriminante local permite calcular m√©tricas transformadas localmente que consideram a estrutura de dados e a separa√ß√£o entre classes na vizinhan√ßa de um ponto de consulta.
*Prova*: Ao calcular as proje√ß√µes com LDA localmente, √© poss√≠vel ajustar a m√©trica de dist√¢ncia para maximizar a separa√ß√£o entre classes naquela regi√£o do espa√ßo de features. $\blacksquare$

**Corol√°rio 136:** O uso de m√©tricas transformadas localmente aumenta a capacidade do k-NN de se adaptar √† estrutura local dos dados, o que melhora seu desempenho em problemas com alta dimensionalidade e distribui√ß√µes de dados complexas.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados com duas classes, onde a classe 1 tem uma distribui√ß√£o com m√©dia $\mu_1 = [1, 1]^T$ e a classe 2 com m√©dia $\mu_2 = [3, 3]^T$. Vamos criar dados sint√©ticos com covari√¢ncia intra-classe $W$ e calcular a matriz de covari√¢ncia entre classes $B$.
>
> ```python
> import numpy as np
> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>
> # Dados sint√©ticos
> np.random.seed(42)
> mean1 = np.array([1, 1])
> mean2 = np.array([3, 3])
> cov = np.array([[1, 0.8], [0.8, 1]])
> X1 = np.random.multivariate_normal(mean1, cov, 100)
> X2 = np.random.multivariate_normal(mean2, cov, 100)
> X = np.vstack((X1, X2))
> y = np.array([0] * 100 + [1] * 100)
>
> # Calcula a LDA global
> lda = LinearDiscriminantAnalysis()
> lda.fit(X, y)
> X_lda = lda.transform(X)
>
> # Calcula a LDA local (exemplo com um ponto de consulta)
> query_point = X[0]
> k = 10
> distances = np.sqrt(np.sum((X - query_point)**2, axis=1))
> neighbors_indices = np.argsort(distances)[:k]
> X_local = X[neighbors_indices]
> y_local = y[neighbors_indices]
>
> lda_local = LinearDiscriminantAnalysis()
> lda_local.fit(X_local, y_local)
>
> # Projeta o ponto de consulta no espa√ßo transformado
> query_point_lda_local = lda_local.transform(query_point.reshape(1, -1))
>
> print("LDA Global - Proje√ß√£o de todos os pontos")
> print(X_lda[:5])
> print("\nLDA Local - Proje√ß√£o do ponto de consulta")
> print(query_point_lda_local)
> ```
>
> Neste exemplo, a LDA global transforma os dados em um espa√ßo onde as classes s√£o mais separ√°veis. A LDA local, aplicada √† vizinhan√ßa do ponto de consulta, realiza uma transforma√ß√£o que √© espec√≠fica para aquela regi√£o, o que pode ser mais adequado para capturar a estrutura local dos dados. A dist√¢ncia entre pontos √© ent√£o calculada no espa√ßo transformado pela LDA local.

> ‚ö†Ô∏è **Nota Importante**: A utiliza√ß√£o da an√°lise discriminante local permite calcular m√©tricas de dist√¢ncia que se adaptam √† estrutura local dos dados, e que d√£o maior peso a regi√µes de maior discriminabilidade entre as classes.

> ‚ùó **Ponto de Aten√ß√£o**: A implementa√ß√£o de m√©tricas transformadas localmente pode aumentar a complexidade computacional do k-NN, e exige um cuidado na escolha dos par√¢metros do modelo, da janela de vizinhan√ßa e do subespa√ßo de proje√ß√£o.

### A F√≥rmula da Dist√¢ncia Adaptativa: Estendendo e Alongando as Regi√µes de Proximidade

A f√≥rmula da **dist√¢ncia adaptativa** utilizada no m√©todo de **discriminant adaptive nearest-neighbor (DANN)** combina o uso de m√©tricas transformadas localmente e a extens√£o da vizinhan√ßa [^13.4]. Essa f√≥rmula busca estender a regi√£o de proximidade do k-NN nas dire√ß√µes em que a probabilidade das classes varia menos, e contrair a regi√£o nas dire√ß√µes onde as probabilidades mudam mais.

A dist√¢ncia adaptativa entre um ponto $x$ e um ponto de consulta $x_0$ √© dada por:

$$D(x, x_0) = (x - x_0)^T \Sigma (x - x_0)$$

onde $\Sigma$ √© uma matriz que define a forma da vizinhan√ßa, e que √© dada por:

$$ \Sigma = W^{-1/2}[W^{-1/2} B W^{-1/2} + \epsilon I] W^{-1/2} = W^{-1/2}[B^* + \epsilon I]W^{-1/2} $$

Onde $W$ √© a matriz de covari√¢ncia intra-classe, $B$ √© a matriz de covari√¢ncia entre as m√©dias das classes, $B^* = W^{-1/2}BW^{-1/2}$ √© a vers√£o transformada de $B$ e $\epsilon$ √© um par√¢metro de regulariza√ß√£o.

```mermaid
graph LR
    subgraph "Adaptive Distance Calculation"
    direction LR
    A["Input points x, x‚ÇÄ"]
    B["Calculate: diff = x - x‚ÇÄ"]
    C["Compute Sigma Matrix (Œ£)"]
    D["Adaptive Distance: D(x, x‚ÇÄ) = diff·µÄ Œ£ diff"]
    A --> B
    B --> C
    C --> D
    end
    subgraph "Sigma Matrix (Œ£) Components"
        direction TB
        E["Intra-class Covariance Matrix (W)"]
        F["Between-class Covariance Matrix (B)"]
        G["Transformed Between-class Matrix (B*): W‚Åª¬π/¬≤ B W‚Åª¬π/¬≤"]
        H["Regularization Term (ŒµI)"]
        I["Sigma: W‚Åª¬π/¬≤ [B* + ŒµI] W‚Åª¬π/¬≤"]
        E & F --> G
        G & H --> I
    end
    D --> I
```

A matriz $W$ normaliza as *features* e a matriz $B$ estende ou contrai a vizinhan√ßa, de forma a dar maior peso √†s dire√ß√µes em que a informa√ß√£o √© mais relevante para a classifica√ß√£o.

Essa f√≥rmula adapta a m√©trica de dist√¢ncia para estender a vizinhan√ßa em dire√ß√µes ortogonais √† fronteira de decis√£o local, utilizando a informa√ß√£o da variabilidade dos dados na vizinhan√ßa, e as matrizes $W$ e $B$ podem ser definidas localmente.

**Lemma 137:** A f√≥rmula da dist√¢ncia adaptativa do DANN estende a regi√£o de proximidade nas dire√ß√µes onde a varia√ß√£o das probabilidades de classe √© menor, e contrai nas dire√ß√µes onde a varia√ß√£o √© maior, de forma a melhor capturar a estrutura local dos dados e da fronteira de decis√£o.
*Prova*: A matriz $\Sigma$ tem como componentes as matrizes de covari√¢ncia intra e inter-classe, que permitem estender a vizinhan√ßa nas dire√ß√µes de baixa variabilidade entre classes e contrair nas de alta variabilidade. $\blacksquare$

**Corol√°rio 137:** A combina√ß√£o da proje√ß√£o e da extens√£o da vizinhan√ßa permite que o k-NN seja mais robusto e se adapte √†s particularidades de cada regi√£o do espa√ßo de *features*.

> üí° **Exemplo Num√©rico:**
>
> Vamos calcular a dist√¢ncia adaptativa com dados sint√©ticos. Considere duas classes, onde a classe 1 tem m√©dia $\mu_1 = [1, 1]^T$ e a classe 2 tem m√©dia $\mu_2 = [3, 3]^T$. A matriz de covari√¢ncia dentro da classe √© $W$, e entre as classes √© $B$.
>
> Primeiro, vamos gerar dados sint√©ticos e calcular as matrizes $W$ e $B$.
>
> ```python
> import numpy as np
>
> # Dados sint√©ticos
> np.random.seed(42)
> mean1 = np.array([1, 1])
> mean2 = np.array([3, 3])
> cov = np.array([[1, 0.5], [0.5, 1]]) # Covari√¢ncia intra-classe
> X1 = np.random.multivariate_normal(mean1, cov, 100)
> X2 = np.random.multivariate_normal(mean2, cov, 100)
> X = np.vstack((X1, X2))
> y = np.array([0] * 100 + [1] * 100)
>
> # Calcula a matriz de covari√¢ncia intra-classe (W)
> W = np.cov(X.T)
>
> # Calcula as m√©dias das classes
> mean_class1 = np.mean(X[y == 0], axis=0)
> mean_class2 = np.mean(X[y == 1], axis=0)
>
> # Calcula a matriz de covari√¢ncia entre classes (B)
> global_mean = np.mean(X, axis=0)
> B = np.outer(mean_class1 - global_mean, mean_class1 - global_mean) + \
>     np.outer(mean_class2 - global_mean, mean_class2 - global_mean)
>
> # Par√¢metro de regulariza√ß√£o
> epsilon = 0.01
>
> # Calcula a matriz Sigma
> W_sqrt_inv = np.linalg.inv(np.sqrt(W))
> B_star = W_sqrt_inv @ B @ W_sqrt_inv
> Sigma = W_sqrt_inv @ (B_star + epsilon * np.eye(2)) @ W_sqrt_inv
>
> # Ponto de consulta e um ponto vizinho
> x0 = X[0]
> x = X[1]
>
> # Calcula a dist√¢ncia adaptativa
> diff = x - x0
> adaptive_dist = diff.T @ Sigma @ diff
>
> print("Matriz de covari√¢ncia intra-classe (W):")
> print(W)
> print("\nMatriz de covari√¢ncia entre classes (B):")
> print(B)
> print("\nMatriz Sigma:")
> print(Sigma)
> print(f"\nDist√¢ncia adaptativa entre x0 e x: {adaptive_dist:.4f}")
> ```
>
> Neste exemplo, calculamos as matrizes $W$ e $B$ a partir dos dados sint√©ticos. Usamos estas matrizes para calcular $\Sigma$ e, por fim, a dist√¢ncia adaptativa entre dois pontos. A matriz $\Sigma$ ajusta a dist√¢ncia de forma que regi√µes com maior variabilidade entre classes tenham maior peso na dist√¢ncia.

> ‚ö†Ô∏è **Nota Importante**: A f√≥rmula da dist√¢ncia adaptativa no DANN utiliza informa√ß√µes da vizinhan√ßa para estender as regi√µes de proximidade nas dire√ß√µes relevantes e construir uma m√©trica local.

> ‚ùó **Ponto de Aten√ß√£o**: O c√°lculo da dist√¢ncia adaptativa envolve o c√°lculo das matrizes de covari√¢ncia local, o que pode ser computacionalmente custoso, mas a modelagem adaptativa pode trazer melhorias significativas na precis√£o do modelo.

### Conclus√£o

A utiliza√ß√£o de vizinhan√ßas adaptativas e m√©tricas transformadas localmente, por meio da an√°lise discriminante local, permite que o k-NN lide melhor com a maldi√ß√£o da dimensionalidade e com as varia√ß√µes na densidade dos dados em diferentes regi√µes do espa√ßo de *features*. A adapta√ß√£o da vizinhan√ßa e da m√©trica de dist√¢ncia permite que o modelo selecione vizinhos mais relevantes para a classifica√ß√£o e que capture a estrutura local dos dados de forma mais precisa, resultando em modelos mais robustos e com melhor capacidade de generaliza√ß√£o em problemas complexos. A escolha entre as diferentes formas de adapta√ß√£o do k-NN deve ser feita considerando as caracter√≠sticas dos dados e a complexidade do problema de classifica√ß√£o.

### Footnotes

[^13.4]: "When nearest-neighbor classification is carried out in a high-dimensional feature space, the nearest neighbors of a point can be very far away, causing bias and degrading the performance of the rule. To quantify this, consider N data points uniformly distributed in the unit cube [0, 1]P...In general, this calls for adapting the metric used in nearest-neighbor classification, so that the resulting neighborhoods stretch out in directions for which the class probabilities don't change much." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^4.3]: "Linear Discriminant Analysis (LDA) is a classical method for classification. It assumes that the classes are normally distributed and that their covariance matrices are the same." *(Trecho de "4. Linear Methods for Classification")*
