## Combinando T√©cnicas Locais e Globais: Redu√ß√£o de Dimensionalidade e Mitiga√ß√£o do Vi√©s em Espa√ßos Ruidosos

```mermaid
graph LR
    subgraph "Dimensionality Reduction and Bias Mitigation"
        direction LR
        A["High-Dimensional, Noisy Space"] --> B["Global Reduction: Project to lower dimension"]
        B --> C["Local Adaptation: DANN metric refinement"]
        C --> D["Improved Classification"]
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora a combina√ß√£o de **t√©cnicas locais** e **globais** para lidar com os desafios da **alta dimensionalidade** e do **ru√≠do** em problemas de classifica√ß√£o, com foco especial em como a **proje√ß√£o em espa√ßos de menor dimens√£o** e a utiliza√ß√£o de m√©todos adaptativos locais podem mitigar tanto a complexidade computacional quanto o vi√©s que podem surgir em espa√ßos ruidosos [^13.4.2]. Analisaremos como a redu√ß√£o de dimensionalidade, uma t√©cnica global, pode ser combinada com o **Discriminant Adaptive Nearest Neighbors (DANN)**, uma t√©cnica local, para obter modelos de classifica√ß√£o mais robustos, precisos e eficientes. Discutiremos tamb√©m como essa combina√ß√£o permite que o modelo capture melhor a estrutura dos dados e melhore seu desempenho em aplica√ß√µes do mundo real.

### A Complexidade de Espa√ßos de Alta Dimens√£o com Ru√≠do

Problemas de classifica√ß√£o em espa√ßos de alta dimens√£o, onde o n√∫mero de *features* √© grande, s√£o frequentemente afetados por dois desafios principais:

1.  **Maldi√ß√£o da Dimensionalidade:** Conforme abordado em cap√≠tulos anteriores, a **maldi√ß√£o da dimensionalidade** faz com que a densidade dos dados diminua exponencialmente com o aumento da dimens√£o, o que torna a dist√¢ncia entre os pontos menos informativa e dificulta a sele√ß√£o dos vizinhos mais relevantes. Isso afeta a capacidade de modelos como o k-NN (k-vizinhos mais pr√≥ximos) de realizar classifica√ß√µes precisas.
2.  **Ru√≠do:** A presen√ßa de *features* irrelevantes ou ruidosas aumenta a dificuldade de encontrar padr√µes relevantes nos dados, e pode levar a modelos com *overfitting*, ou seja, modelos que aprendem a identificar padr√µes que n√£o existem no mundo real.

```mermaid
graph LR
    subgraph "High Dimensionality Issues"
        direction TB
        A["High Dimensional Space"] --> B["Curse of Dimensionality: Data sparsity increases"]
        A --> C["Noise: Irrelevant features complicate pattern detection"]
        B --> D["Distance becomes less informative"]
        C --> E["Model Overfitting"]
    end
```

A combina√ß√£o desses dois fatores ‚Äì alta dimensionalidade e ru√≠do ‚Äì torna o problema de classifica√ß√£o mais desafiador e exige a utiliza√ß√£o de t√©cnicas que permitam lidar com esses problemas.

Para mitigar esses problemas, √© importante combinar a redu√ß√£o da dimensionalidade com abordagens locais. A redu√ß√£o da dimensionalidade, utilizada como uma abordagem global, projeta os dados em um espa√ßo de menor dimens√£o, o que diminui o n√∫mero de *features*, reduz a complexidade computacional e diminui o ru√≠do. M√©todos locais, como o DANN, fazem um ajuste fino da m√©trica e regi√£o de proximidade para a estrutura local dos dados, melhorando a capacidade do modelo de se adaptar a distribui√ß√µes complexas.

**Lemma 149:** A combina√ß√£o de alta dimensionalidade e ru√≠do torna o problema de classifica√ß√£o mais complexo, e exige a utiliza√ß√£o de t√©cnicas que reduzam a dimens√£o do problema e se adaptem localmente √† estrutura dos dados.
*Prova*: O n√∫mero de dimens√µes aumenta o tamanho do espa√ßo de *features* e o ru√≠do faz com que informa√ß√µes relevantes sejam menos percept√≠veis. $\blacksquare$

**Corol√°rio 149:** A combina√ß√£o de t√©cnicas globais (redu√ß√£o de dimensionalidade) e locais (adapta√ß√£o de m√©trica com DANN) permite lidar com a complexidade e o ru√≠do em problemas de alta dimens√£o.

> ‚ö†Ô∏è **Nota Importante**: Em espa√ßos de alta dimens√£o com ru√≠do, √© necess√°rio combinar t√©cnicas de redu√ß√£o de dimensionalidade com m√©todos adaptativos locais para obter resultados eficazes.

> ‚ùó **Ponto de Aten√ß√£o**:  A escolha das t√©cnicas de redu√ß√£o de dimensionalidade e adapta√ß√£o local deve ser feita com base nas caracter√≠sticas espec√≠ficas do problema em quest√£o.

### Redu√ß√£o de Dimensionalidade: Projetando para um Subespa√ßo Relevante

A **redu√ß√£o de dimensionalidade** √© um processo que busca transformar os dados originais em um espa√ßo de menor dimens√£o, mantendo as informa√ß√µes mais relevantes para a classifica√ß√£o [^13.4.2]. O objetivo √© reduzir a complexidade do espa√ßo de *features* e diminuir o impacto do ru√≠do, projetando os dados em um subespa√ßo que capture a estrutura mais relevante dos dados.

Existem diversas t√©cnicas para redu√ß√£o de dimensionalidade, e algumas das mais comuns incluem:

1.  **An√°lise de Componentes Principais (PCA):** A PCA √© uma t√©cnica linear que busca projetar os dados em um subespa√ßo ortogonal de menor dimens√£o, onde a vari√¢ncia dos dados √© maximizada. A PCA identifica as dimens√µes que mais variam, e elimina aquelas que apresentam baixa vari√¢ncia.
2.  **An√°lise Discriminante Linear (LDA):** A LDA √© uma t√©cnica supervisionada que busca projetar os dados em um subespa√ßo de menor dimens√£o onde as classes s√£o mais separ√°veis. A LDA maximiza a dist√¢ncia entre as m√©dias das classes e minimiza a vari√¢ncia dentro das classes.
3.  **Sele√ß√£o de *Features*:** A sele√ß√£o de *features* consiste em identificar e utilizar apenas um subconjunto das *features* originais que sejam mais relevantes para a classifica√ß√£o.
4. **Autoencoders:** Algoritmos de aprendizado profundo que comprimem os dados em espa√ßos de menor dimens√£o, e que podem ser usados para reconstruir a entrada a partir do espa√ßo comprimido, garantindo que a informa√ß√£o seja preservada.

```mermaid
graph LR
    subgraph "Dimensionality Reduction Techniques"
        direction TB
        A["Original Data: High-Dimensional"] --> B["Principal Component Analysis (PCA)"]
        A --> C["Linear Discriminant Analysis (LDA)"]
        A --> D["Feature Selection"]
        A --> E["Autoencoders"]
        B --> F["Projection to Lower Dimension: Maximize variance"]
        C --> G["Projection to Lower Dimension: Maximize class separation"]
        D --> H["Subset of relevant features"]
        E --> I["Compressed representation"]
    end
```

A redu√ß√£o de dimensionalidade √© uma etapa importante para melhorar o desempenho de modelos de classifica√ß√£o em espa√ßos de alta dimens√£o, pois permite diminuir o n√∫mero de *features*, reduzir o ru√≠do e melhorar a capacidade de generaliza√ß√£o do modelo.

**Lemma 150:** A redu√ß√£o de dimensionalidade permite projetar os dados em um espa√ßo de menor dimens√£o, onde as *features* relevantes s√£o preservadas e a complexidade do problema √© reduzida, atenuando a maldi√ß√£o da dimensionalidade.
*Prova*: A proje√ß√£o em subespa√ßos de baixa dimens√£o reduz a quantidade de *features* ruidosas e irrelevantes. $\blacksquare$

**Corol√°rio 150:** A combina√ß√£o de redu√ß√£o de dimensionalidade e outros m√©todos de classifica√ß√£o podem resultar em modelos mais robustos e eficientes para lidar com problemas de alta dimens√£o.

> ‚ö†Ô∏è **Nota Importante**: A redu√ß√£o de dimensionalidade √© uma t√©cnica importante para lidar com a maldi√ß√£o da dimensionalidade, projetando os dados em um subespa√ßo de menor dimens√£o que preserva informa√ß√µes relevantes.

> ‚ùó **Ponto de Aten√ß√£o**:  A escolha da t√©cnica de redu√ß√£o de dimensionalidade deve levar em considera√ß√£o as caracter√≠sticas dos dados e o objetivo da tarefa de classifica√ß√£o.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um conjunto de dados com 100 amostras e 50 *features*, onde cada amostra pertence a uma de duas classes. As *features* s√£o geradas aleatoriamente, com algumas delas (digamos, 5) sendo mais informativas para a separa√ß√£o das classes e o restante representando ru√≠do.
>
> 1.  **Dados de Entrada:**
>
> ```python
> import numpy as np
> from sklearn.preprocessing import StandardScaler
>
> np.random.seed(42)
> n_samples = 100
> n_features = 50
> n_informative = 5
>
> X = np.random.randn(n_samples, n_features)
> # Gerar classes com base nas primeiras features
> y = np.where(X[:, :n_informative].sum(axis=1) > 0, 1, 0)
>
> # Normalizar os dados
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)
> ```
>
> 2.  **Aplica√ß√£o da PCA:**
>
> ```python
> from sklearn.decomposition import PCA
>
> pca = PCA(n_components=10) # Reduz para 10 dimens√µes
> X_pca = pca.fit_transform(X_scaled)
>
> print("Shape original:", X_scaled.shape)
> print("Shape ap√≥s PCA:", X_pca.shape)
> print("Vari√¢ncia explicada por cada componente:", pca.explained_variance_ratio_)
> ```
>
>   Neste exemplo, a PCA reduz o espa√ßo de 50 para 10 dimens√µes, mantendo a maior parte da vari√¢ncia dos dados. Os valores `explained_variance_ratio_` indicam a import√¢ncia de cada componente principal. Por exemplo, os primeiros componentes tendem a ter valores maiores, indicando que eles capturam mais informa√ß√µes dos dados.
>
> 3.  **Visualiza√ß√£o (Opcional):**
>
>   Poder√≠amos visualizar os dados reduzidos em 2 dimens√µes para ter uma ideia da separabilidade das classes.
>
> ```python
> import matplotlib.pyplot as plt
>
> pca_2d = PCA(n_components=2)
> X_pca_2d = pca_2d.fit_transform(X_scaled)
>
> plt.figure(figsize=(8, 6))
> plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap='viridis', edgecolor='k')
> plt.xlabel('Componente Principal 1')
> plt.ylabel('Componente Principal 2')
> plt.title('PCA 2D')
> plt.colorbar(label='Classe')
> plt.show()
> ```
>
>   Essa visualiza√ß√£o mostra como os dados s√£o projetados em um espa√ßo de menor dimens√£o, potencialmente facilitando a classifica√ß√£o. A PCA ajuda a remover as *features* menos informativas.

### DANN: Adapta√ß√£o Local e Proje√ß√£o Global

O algoritmo **DANN (Discriminant Adaptive Nearest Neighbors)** combina uma abordagem de proje√ß√£o global em subespa√ßos de menor dimens√£o com uma adapta√ß√£o local da m√©trica de dist√¢ncia [^13.4.2]. Essa combina√ß√£o permite que o DANN lide de forma eficaz com dados de alta dimens√£o e com a complexidade das distribui√ß√µes das classes.

1.  **Proje√ß√£o Global:** O DANN utiliza t√©cnicas de redu√ß√£o de dimensionalidade, como PCA ou LDA, para projetar os dados em um subespa√ßo de menor dimens√£o antes de aplicar o k-NN. Essa etapa global diminui a complexidade computacional do modelo e atenua os efeitos da maldi√ß√£o da dimensionalidade.
2.  **Adapta√ß√£o Local:** No subespa√ßo de baixa dimens√£o, o DANN utiliza as informa√ß√µes das matrizes de covari√¢ncia intra e inter-classe para adaptar a m√©trica de dist√¢ncia e a regi√£o de vizinhan√ßa de forma local, como discutido em cap√≠tulos anteriores. Essa adapta√ß√£o local permite que o modelo refine suas decis√µes de classifica√ß√£o.

```mermaid
graph LR
    subgraph "DANN Approach"
        direction LR
        A["Original Data"] --> B["Global Projection: Dimensionality Reduction"]
        B --> C["Local Adaptation: Metric refinement using covariance"]
        C --> D["Improved classification"]
    end
```

A combina√ß√£o dessas abordagens permite que o DANN capture tanto as caracter√≠sticas globais dos dados (com a redu√ß√£o de dimensionalidade) quanto as particularidades locais da distribui√ß√£o de classes (com a adapta√ß√£o da m√©trica), o que resulta em modelos mais robustos e precisos. A redu√ß√£o de dimensionalidade atua na elimina√ß√£o de ru√≠dos e dimens√µes irrelevantes para a classifica√ß√£o, enquanto a adapta√ß√£o local permite o ajuste fino para cada regi√£o.

**Lemma 151:** A combina√ß√£o da redu√ß√£o de dimensionalidade com a adapta√ß√£o local da m√©trica no DANN permite que o modelo capture tanto a estrutura global quanto as particularidades locais dos dados.
*Prova*: A proje√ß√£o global diminui a complexidade do problema de classifica√ß√£o e a adapta√ß√£o local refina a decis√£o usando as informa√ß√µes contextuais dos vizinhos. $\blacksquare$

**Corol√°rio 151:** O DANN equilibra a necessidade de lidar com alta dimensionalidade com a capacidade de se adaptar √†s caracter√≠sticas locais dos dados.

> ‚ö†Ô∏è **Nota Importante**: O DANN combina redu√ß√£o de dimensionalidade com adapta√ß√£o local da m√©trica, o que resulta em modelos mais eficazes em espa√ßos de alta dimens√£o com ru√≠do.

> ‚ùó **Ponto de Aten√ß√£o**:  A ordem em que as abordagens global e local s√£o aplicadas pode influenciar o resultado do modelo, e essa ordem depende da complexidade do problema e das caracter√≠sticas dos dados.

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, vamos ilustrar como a adapta√ß√£o local da m√©trica no DANN funciona, considerando um ponto de consulta e seus vizinhos no espa√ßo reduzido pela PCA.
>
> 1.  **Ponto de Consulta e Vizinhos:**
>
>     Vamos usar o primeiro ponto do conjunto de dados como ponto de consulta e encontrar seus 5 vizinhos mais pr√≥ximos.
>
> ```python
> from sklearn.neighbors import NearestNeighbors
>
> query_point = X_pca[0, :].reshape(1, -1)
> knn = NearestNeighbors(n_neighbors=5)
> knn.fit(X_pca)
> distances, indices = knn.kneighbors(query_point)
> neighbors = X_pca[indices[0]]
> neighbor_labels = y[indices[0]]
>
> print("√çndices dos vizinhos:", indices)
> print("Labels dos vizinhos:", neighbor_labels)
> ```
>
> 2.  **Matrizes de Covari√¢ncia:**
>
>     Agora, calculamos as matrizes de covari√¢ncia intra-classe e inter-classe para os vizinhos.
>
> ```python
> def calculate_covariance_matrices(neighbors, labels):
>     unique_labels = np.unique(labels)
>     Sw = np.zeros((neighbors.shape[1], neighbors.shape[1])) # Within-class scatter
>     Sb = np.zeros((neighbors.shape[1], neighbors.shape[1])) # Between-class scatter
>     overall_mean = np.mean(neighbors, axis=0)
>
>     for label in unique_labels:
>         class_neighbors = neighbors[labels == label]
>         class_mean = np.mean(class_neighbors, axis=0)
>         Sw += np.cov(class_neighbors, rowvar=False) * (len(class_neighbors) - 1)
>         Sb += (class_mean - overall_mean).reshape(-1, 1) @ (class_mean - overall_mean).reshape(1, -1) * len(class_neighbors)
>
>     return Sw, Sb
>
> Sw, Sb = calculate_covariance_matrices(neighbors, neighbor_labels)
>
> print("Matriz de Covari√¢ncia Intra-Classe (Sw):\n", Sw)
> print("Matriz de Covari√¢ncia Inter-Classe (Sb):\n", Sb)
> ```
>
> 3.  **Adapta√ß√£o da M√©trica:**
>
>     A m√©trica de dist√¢ncia √© adaptada usando uma combina√ß√£o das matrizes de covari√¢ncia. No DANN, a m√©trica adaptada √© geralmente dada por:
>     $M = (S_w + \epsilon I)^{-1} S_b$, onde $\epsilon$ √© um pequeno valor para regulariza√ß√£o (por exemplo, 0.001) e $I$ √© a matriz identidade. A dist√¢ncia entre dois pontos $x_i$ e $x_j$ usando essa m√©trica √© dada por $d(x_i, x_j) = (x_i - x_j)^T M (x_i - x_j)$. Essa m√©trica ajusta a import√¢ncia das dimens√µes de acordo com a separabilidade das classes.
>
> ```python
> epsilon = 0.001
> identity_matrix = np.eye(Sw.shape[0])
> M = np.linalg.inv(Sw + epsilon * identity_matrix) @ Sb
>
> def adapted_distance(x1, x2, M):
>    diff = (x1 - x2).reshape(-1,1)
>    return diff.T @ M @ diff
>
> print("M√©trica Adaptada (M):\n", M)
> # Calcular a dist√¢ncia adaptada entre o ponto de consulta e seu primeiro vizinho
> adapted_dist = adapted_distance(query_point, neighbors[0], M)
> print("Dist√¢ncia Adaptada:", adapted_dist)
> ```
>
>    Esta m√©trica adaptada √© usada para recalcular as dist√¢ncias entre o ponto de consulta e seus vizinhos, dando maior peso √†s dimens√µes que melhor separam as classes. O c√°lculo da dist√¢ncia adaptada demonstra como o DANN pondera diferentes dimens√µes do espa√ßo de *features* para levar em conta a estrutura local dos dados.

```mermaid
graph LR
    subgraph "DANN Metric Adaptation"
        direction TB
        A["k-Nearest Neighbors"] --> B["Calculate Within-Class Scatter Matrix (Sw)"]
        A --> C["Calculate Between-Class Scatter Matrix (Sb)"]
        B & C --> D["Adaptive Metric: M = (Sw + ŒµI)^-1 Sb"]
        D --> E["Calculate Adapted Distances"]
    end
```

### Funcionamento Pr√°tico: Combina√ß√£o de T√©cnicas Globais e Locais

O funcionamento pr√°tico do DANN que combina t√©cnicas globais e locais pode ser resumido nos seguintes passos:

1.  **Redu√ß√£o de Dimensionalidade (Global):** O primeiro passo √© aplicar uma t√©cnica de redu√ß√£o de dimensionalidade, como PCA ou LDA, a todo o conjunto de treinamento, projetando os dados em um subespa√ßo de menor dimens√£o.
2.  **Forma√ß√£o da Vizinhan√ßa (Local):** Para cada ponto de consulta no subespa√ßo de menor dimens√£o, s√£o selecionados seus $k$ vizinhos mais pr√≥ximos utilizando alguma m√©trica de dist√¢ncia (normalmente a dist√¢ncia Euclidiana).
3.  **Adapta√ß√£o da M√©trica (Local):** Com base nos $k$ vizinhos selecionados, s√£o calculadas as matrizes de covari√¢ncia intra e entre classes, e a m√©trica de dist√¢ncia √© ajustada localmente utilizando a f√≥rmula do DANN.
4.  **Classifica√ß√£o (Local):** A classifica√ß√£o do ponto de consulta √© realizada utilizando a vota√ß√£o majorit√°ria entre as classes dos $k$ vizinhos, com base na m√©trica de dist√¢ncia adaptada.

```mermaid
graph LR
    subgraph "DANN Workflow"
    direction TB
        A["Training Data"] --> B["Global Reduction: Project to lower dimension using PCA or LDA"]
        B --> C["For each query point: Find k-nearest neighbors in the reduced space"]
        C --> D["Local Adaptation: Adjust distance metric using Sw and Sb"]
        D --> E["Classification: Majority vote of k-neighbors with adapted distance"]
    end
```

Essa combina√ß√£o de t√©cnicas globais (redu√ß√£o de dimensionalidade) e locais (adapta√ß√£o da m√©trica) permite que o DANN se beneficie das vantagens de ambas as abordagens: a redu√ß√£o do ru√≠do e da complexidade computacional, e a capacidade de capturar as caracter√≠sticas espec√≠ficas de cada regi√£o do espa√ßo de *features*.

**Lemma 152:** A combina√ß√£o de t√©cnicas globais, como a redu√ß√£o de dimensionalidade, com t√©cnicas locais como a adapta√ß√£o da m√©trica no DANN, permite criar modelos mais robustos e eficazes em problemas com dados de alta dimens√£o, ru√≠dos e distribui√ß√µes complexas.
*Prova*: A redu√ß√£o de dimensionalidade diminui o efeito da maldi√ß√£o da dimensionalidade, e a adapta√ß√£o local da m√©trica garante que a decis√£o final leve em considera√ß√£o a informa√ß√£o espec√≠fica de cada vizinhan√ßa. $\blacksquare$

**Corol√°rio 152:** A aplica√ß√£o do DANN em problemas do mundo real tem demonstrado o poder da combina√ß√£o de t√©cnicas globais e locais para lidar com a complexidade dos dados.

> ‚ö†Ô∏è **Nota Importante**: A combina√ß√£o de t√©cnicas globais e locais permite que o DANN lide de forma mais eficiente com a complexidade e o ru√≠do presentes em dados de alta dimens√£o.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha da t√©cnica de redu√ß√£o de dimensionalidade e dos par√¢metros da adapta√ß√£o local deve ser feita com base nas caracter√≠sticas do problema e nos dados dispon√≠veis.

### Conclus√£o

A combina√ß√£o de t√©cnicas locais e globais, como a redu√ß√£o de dimensionalidade e a adapta√ß√£o local da m√©trica com o DANN, √© uma abordagem eficaz para lidar com os desafios da alta dimensionalidade e do ru√≠do em problemas de classifica√ß√£o. A proje√ß√£o em subespa√ßos de menor dimens√£o diminui a complexidade computacional e atenua o efeito da maldi√ß√£o da dimensionalidade, enquanto a adapta√ß√£o da m√©trica com as informa√ß√µes da matriz de covari√¢ncia entre classes permite que o modelo se adapte √† distribui√ß√£o local dos dados e realize a classifica√ß√£o de forma mais precisa e robusta. O DANN representa, portanto, uma evolu√ß√£o do k-NN, que combina as vantagens de ambas as abordagens.

### Footnotes

[^13.4.2]: "The discriminant-adaptive nearest-neighbor method carries out local dimension reduction that is, dimension reduction separately at each query point. In many problems we can also benefit from global dimension reduction, that is, apply a nearest-neighbor rule in some optimally chosen subspace of the original feature space...Hastie and Tibshirani (1996a) discuss a variation of the discriminant-adaptive nearest-neighbor method for this purpose. At each training point $x_i$, the between-centroids sum of squares matrix $B_i$ is computed, and then these matrices are averaged over all training points: $B = 1/N * \sum B_i$." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^13.4]: "When nearest-neighbor classification is carried out in a high-dimensional feature space, the nearest neighbors of a point can be very far away, causing bias and degrading the performance of the rule...In general, this calls for adapting the metric used in nearest-neighbor classification, so that the resulting neighborhoods stretch out in directions for which the class probabilities don't change much." *(Trecho de "13. Prototype Methods and Nearest-Neighbors")*

[^4.3]:  "Linear Discriminant Analysis (LDA) is a classical method for classification. It assumes that the classes are normally distributed and that their covariance matrices are the same." * (Trecho de "4. Linear Methods for Classification")*
