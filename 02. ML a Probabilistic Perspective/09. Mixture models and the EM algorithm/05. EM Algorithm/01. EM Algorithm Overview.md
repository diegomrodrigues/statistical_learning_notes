## O Algoritmo EM: Uma Análise Detalhada

### Introdução
Este capítulo se dedica a uma exploração profunda do **algoritmo EM (Expectation-Maximization)**, um método iterativo amplamente utilizado para estimar parâmetros em modelos estatísticos com variáveis latentes ou dados faltantes [^13]. O algoritmo EM é particularmente útil em situações onde a otimização direta da função de verossimilhança é complexa ou intratável [^14]. Como veremos, ele alterna entre duas etapas principais: a etapa de Expectation (E), onde inferimos os valores faltantes ou as variáveis latentes, e a etapa de Maximization (M), onde otimizamos os parâmetros do modelo com base nos dados "completos" inferidos na etapa E [^13].

### Conceitos Fundamentais

O algoritmo EM é uma técnica iterativa que busca maximizar a função de verossimilhança marginal (ou observada) em modelos com variáveis latentes ou dados faltantes [^13]. A ideia central é transformar um problema de otimização complexo em uma sequência de problemas mais simples.

**Definição da Função de Verossimilhança Completa:** O primeiro passo é definir a função de verossimilhança para os dados completos, denotada como $l_c(\theta)$ [^13].  Seja $x_i$ os dados observados e $z_i$ as variáveis latentes ou dados faltantes, a função de verossimilhança completa é definida como:

$$l_c(\theta) = \sum_i \log p(x_i, z_i|\theta)$$ [^13]

onde $\theta$ representa os parâmetros do modelo.

**Função Auxiliar Q:** Como os valores de $z_i$ são desconhecidos, introduzimos a função auxiliar $Q(\theta, \theta^{t-1})$, que representa a esperança da função de verossimilhança completa, condicionada aos dados observados $D$ e aos parâmetros estimados na iteração anterior $\theta^{t-1}$ [^13]:

$$Q(\theta, \theta^{t-1}) = E[l_c(\theta)|D, \theta^{t-1}]$$ [^13]

Essa esperança é calculada em relação à distribuição posterior das variáveis latentes, dada pelos parâmetros anteriores [^13].

**Etapa E (Expectation):** Nesta etapa, calculamos a função auxiliar $Q(\theta, \theta^{t-1})$, identificando os termos dos quais a estimativa de máxima verossimilhança (MLE) depende. Esses termos são conhecidos como **Estatísticas Suficientes Esperadas (ESS)** [^13]. Essencialmente, a etapa E preenche os dados faltantes ou estima as variáveis latentes usando os parâmetros atuais do modelo [^13].

**Etapa M (Maximization):** Nesta etapa, maximizamos a função $Q(\theta, \theta^{t-1})$ em relação a $\theta$, obtendo novas estimativas dos parâmetros [^13]:

$$\theta^t = \arg \max_{\theta} Q(\theta, \theta^{t-1})$$ [^13]

Para a estimativa de Máxima a Posteriori (MAP), a etapa M é modificada para incluir um termo de prior sobre os parâmetros:

$$\theta^t = \arg \max_{\theta} Q(\theta, \theta^{t-1}) + \log p(\theta)$$ [^13]

**Convergência:** O algoritmo EM itera entre as etapas E e M até que a mudança na função de verossimilhança ou nos parâmetros seja menor que um limiar predefinido, indicando convergência [^13].

**Generalizações e Variantes:** O algoritmo EM pode ser generalizado para lidar com diversos modelos, incluindo misturas de especialistas, DGMs com variáveis ocultas e distribuições t de Student, com modificações apropriadas nas etapas E e M [^13].

**Relação com o Algoritmo K-means:** O algoritmo K-means é uma variante do algoritmo EM para GMMs, que faz suposições simplificadoras, como fixar as matrizes de covariância $\Sigma_k = \sigma^2I_D$ e os pesos das misturas $\pi_k = 1/K$ [^13].  Neste caso, apenas os centros dos clusters, $\mu_k \in \mathbb{R}^D$, precisam ser estimados [^13].

**Monotonicidade:** Um aspecto importante do algoritmo EM é que ele aumenta monotonicamente (ou permanece igual) a função de verossimilhança dos dados observados [^13].  Essa propriedade o torna uma ferramenta útil para depuração, pois qualquer diminuição na verossimilhança indica um erro na implementação [^13].

### Exemplo: Mistura de Gaussianas (GMM)

O algoritmo EM é frequentemente usado para ajustar modelos de mistura de Gaussianas (GMMs) [^13, 350]. Um GMM assume que os dados são gerados a partir de uma mistura de distribuições Gaussianas, cada uma com sua própria média e covariância [^339].

**Função Auxiliar para GMM:** No contexto de GMMs, a função auxiliar $Q$ pode ser expressa como:

$$Q(\theta, \theta^{(t-1)}) = \sum_i \sum_{k=1}^K E[I(z_i = k)] \log[\pi_k p(x_i|\theta_k)]$$ [^351]

onde $I(z_i = k)$ é uma função indicadora que vale 1 se a variável latente $z_i$ (indicando o cluster ao qual o ponto $x_i$ pertence) é igual a $k$, e 0 caso contrário, e $\theta_k$ representa os parâmetros da k-ésima Gaussiana (média $\mu_k$ e covariância $\Sigma_k$).

**Etapa E para GMM:** A etapa E envolve o cálculo da responsabilidade $r_{ik}$, que representa a probabilidade posterior de que o ponto $x_i$ pertença ao cluster $k$, dado os parâmetros atuais [^340, 351]:

$$r_{ik} = p(z_i = k|x_i, \theta^{(t-1)}) = \frac{\pi_k p(x_i|\theta_k)}{\sum_{k'=1}^K \pi_{k'} p(x_i|\theta_{k'})}$$ [^340, 351]

**Etapa M para GMM:** Na etapa M, atualizamos os parâmetros do modelo (pesos $\pi_k$, médias $\mu_k$ e covariâncias $\Sigma_k$) para maximizar a função auxiliar $Q$ [^351]:

$$\pi_k = \frac{1}{N} \sum_i r_{ik}$$ [^351]

$$\mu_k = \frac{\sum_i r_{ik} x_i}{\sum_i r_{ik}}$$ [^351]

$$\Sigma_k = \frac{\sum_i r_{ik} (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_i r_{ik}}$$ [^351]

As equações acima são análogas às estimativas de máxima verossimilhança para uma única Gaussiana, mas ponderadas pelas responsabilidades $r_{ik}$ [^351].

### Conclusão

O algoritmo EM é uma ferramenta poderosa e flexível para a estimação de parâmetros em modelos com variáveis latentes ou dados faltantes [^13]. Sua aplicação se estende a uma ampla gama de problemas em estatística e aprendizado de máquina, incluindo modelos de mistura, análise de componentes principais, modelos gráficos e muitos outros [^13, 340, 341]. Embora o algoritmo EM garanta convergência para um máximo local da função de verossimilhança, a escolha de uma boa inicialização e a consideração de possíveis problemas de identificabilidade são cruciais para obter resultados significativos [^346, 347].

### Referências
[^1]: Mixture models and the EM algorithm
[^13]: The EM algorithm is an iterative process that alternates between inferring missing values given parameters (E step) and optimizing parameters given the filled-in data (M step). The EM algorithm defines the complete data log likelihood as lc(θ) = Σ log p(xi, zi|θ) and introduces the expected complete data log likelihood Q(θ, θt-1) = E[lc(θ)|D, θt-1], where the expectation is taken with respect to the old parameters θt-1 and the observed data D. In the E step, the auxiliary function Q(θ, θ^(t-1)) is computed, representing the expected complete data log likelihood given observed data and previous parameters. The M step involves maximizing the Q function with respect to θ, leading to updated parameter estimates. The E step computes Q(θ, θt-1), identifying terms on which the MLE depends, known as expected sufficient statistics (ESS), while the M step optimizes the Q function with respect to θ, i.e., θ = arg max Q(θ, θt-1); for MAP estimation, the M step is modified as θt = argmax Q(θ, θt-1) + log p(θ). The EM algorithm can be generalized to handle various models, including mixtures of experts, DGMs with hidden variables, and Student's t-distributions, with appropriate modifications to the E and M steps. The K-means algorithm is a variant of the EM algorithm for GMMs, which makes assumptions such as Σk = σ^2ID is fixed, and πk = 1/K is fixed, so only the cluster centers, μk ∈ RD, have to be estimated. The EM algorithm monotonically increases the log likelihood of the observed data (or stays the same), serving as a useful debugging tool.
[^14]: In Chapter 10 we showed how graphical models can be used to define high-dimensional joint probability distributions.
[^339]: The most widely used mixture model is the mixture of Gaussians (MOG), also called a Gaussian mixture model or GMM. In this model, each base distribution in the mixture is a multivariate Gaussian with mean μk and covariance matrix Σk. Thus the model has the form
[^340]: We can use mixture models to define density models on many kinds of data. For example, suppose our data consist of D-dimensional bit vectors. In this case, an appropriate class-conditional density is a product of Bernoullis:
[^341]: Here the data vectors xi ∈ R7 represent the expression levels of different genes at 7 different time points. We clustered them using a GMM.
[^346]: The main problem with computing p(0|D) for an LVM is that the posterior may have multiple modes.
[^347]: Unidentifiability can cause a problem for Bayesian inference.
[^350]: EM gets around this problem as follows. Define the complete data log likelihood to be
[^351]: The expected complete data log likelihood is given by
<!-- END -->