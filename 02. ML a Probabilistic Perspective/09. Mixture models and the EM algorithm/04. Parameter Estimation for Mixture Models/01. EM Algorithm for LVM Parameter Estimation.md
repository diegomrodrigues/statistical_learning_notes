## Estimação de Parâmetros em Modelos de Variáveis Latentes para Modelos de Mistura

### Introdução

A estimação de parâmetros em **Modelos de Variáveis Latentes (LVMs)**, como os Modelos de Mistura de Gaussianas (GMMs), apresenta desafios significativos devido à complexidade da função de *log-verossimilhança* e à presença de múltiplas soluções ótimas [^1]. Especificamente, a *posterior* dos parâmetros pode exibir múltiplos modos, cada um correspondendo a diferentes atribuições de rótulos aos *clusters*, resultando na não identificabilidade dos parâmetros. O algoritmo **Expectation-Maximization (EM)** é uma ferramenta fundamental para a estimação de parâmetros nesses modelos, explorando a estrutura dos dados incompletos para iterativamente refinar as estimativas [^1]. Este capítulo aprofunda-se nas nuances da estimação de parâmetros em modelos de mistura, com foco nos desafios de identificabilidade e nas abordagens para mitigar esses problemas, incluindo o uso do algoritmo EM.

### Conceitos Fundamentais

#### Desafios na Estimação de Parâmetros em LVMs
Em LVMs, a correlação entre as variáveis observadas é modelada através da introdução de variáveis latentes, representando uma "causa" oculta comum [^1]. Embora essa abordagem possa reduzir o número de parâmetros em comparação com modelos que representam diretamente a correlação no espaço visível, a estimação dos parâmetros torna-se mais complexa devido à presença dessas variáveis latentes não observadas [^1].

A não identificabilidade dos parâmetros é um problema central na estimação de modelos de mistura. Como mencionado anteriormente, a *posterior* dos parâmetros pode ter múltiplos modos, refletindo diferentes atribuições de rótulos aos *clusters* [^1]. Isso significa que existem múltiplas configurações de parâmetros que podem explicar igualmente bem os dados observados, tornando difícil a identificação de uma solução única e "correta".

#### O Algoritmo Expectation-Maximization (EM)
O algoritmo EM é um método iterativo para encontrar estimativas de máxima verossimilhança (ML) ou máxima a posteriori (MAP) de parâmetros em modelos estatísticos que envolvem variáveis latentes [^1]. O algoritmo alterna entre duas etapas principais:

1.  **Etapa E (Expectation):** Nesta etapa, calcula-se a esperança das variáveis latentes, dado os dados observados e a estimativa atual dos parâmetros. No contexto de modelos de mistura, isso envolve calcular a probabilidade *posterior* de cada ponto de dados pertencer a cada *cluster*. Essa probabilidade é conhecida como a **responsabilidade** do *cluster* $k$ pelo ponto $i$, denotada por $r_{ik}$ [^4]. Matematicamente, a responsabilidade é calculada usando a regra de Bayes:

$$
r_{ik} = p(z_i = k | x_i, \theta) = \frac{p(z_i = k | \theta) p(x_i | z_i = k, \theta)}{\sum_{k'=1}^K p(z_i = k' | \theta) p(x_i | z_i = k', \theta)}
$$

onde $x_i$ é o ponto de dado, $z_i$ é a variável latente indicando o *cluster* ao qual o ponto pertence, $\theta$ representa os parâmetros do modelo, e $K$ é o número de *clusters* [^4].

2.  **Etapa M (Maximization):** Nesta etapa, os parâmetros do modelo são atualizados para maximizar a *verossimilhança* esperada, dado as variáveis latentes inferidas na etapa E. No contexto de GMMs, isso envolve atualizar os pesos de mistura, as médias e as matrizes de covariância dos *clusters* [^4]. As equações de atualização para os parâmetros são:

*   **Peso de mistura:**
    $$
    \pi_k = \frac{\sum_{i=1}^N r_{ik}}{N}
    $$
    onde $N$ é o número total de pontos de dados [^4].
*   **Média:**
    $$
    \mu_k = \frac{\sum_{i=1}^N r_{ik} x_i}{\sum_{i=1}^N r_{ik}}
    $$
*   **Covariância:**
    $$
    \Sigma_k = \frac{\sum_{i=1}^N r_{ik} (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^N r_{ik}}
    $$

O algoritmo EM itera entre as etapas E e M até que a convergência seja alcançada, tipicamente definida como uma pequena mudança na *verossimilhança* ou nos parâmetros do modelo [^4].

#### Abordagens para Mitigar a Não Identificabilidade
Embora o algoritmo EM seja amplamente utilizado para a estimação de parâmetros em modelos de mistura, ele é suscetível a ficar preso em ótimos locais devido à não convexidade da função de *log-verossimilhança* [^4]. Além disso, a não identificabilidade dos parâmetros pode levar a soluções que são difíceis de interpretar ou que generalizam mal para novos dados.

Uma abordagem comum para mitigar a não identificabilidade é computar um único modo local da *posterior*, ou seja, realizar a estimação MAP aproximada [^1]. Isso pode ser feito incorporando conhecimento prévio sobre os parâmetros através de uma distribuição *a priori*, que ajuda a regularizar o modelo e prevenir o *overfitting*, especialmente em casos com tamanhos de amostra limitados [^1]. A estimação MAP modifica a etapa M do algoritmo EM para incluir um termo de penalidade baseado na distribuição *a priori*:

$$
\theta^{t+1} = \underset{\theta}{\text{argmax}} \, Q(\theta, \theta^t) + \log p(\theta)
$$

onde $p(\theta)$ é a distribuição *a priori* sobre os parâmetros [^4].

Outras abordagens para lidar com a não identificabilidade incluem:

*   **Restrições nos parâmetros:** Impor restrições nos parâmetros do modelo, como restringir as matrizes de covariância a serem diagonais ou esféricas, pode reduzir o número de soluções possíveis e melhorar a identificabilidade [^4].
*   **Regularização:** Adicionar termos de regularização à função objetivo pode penalizar soluções complexas e promover soluções mais simples e interpretáveis [^1].
*   **Model selection:** Utilizar critérios de seleção de modelo, como o Critério de Informação Bayesiano (BIC), para escolher o modelo com o melhor compromisso entre ajuste aos dados e complexidade [^34].

#### Algoritmos EM Online
Para lidar com conjuntos de dados grandes ou *streaming*, algoritmos EM online podem ser utilizados para atualizar os parâmetros do modelo incrementalmente à medida que novos dados chegam [^1]. Esses algoritmos permitem o aprendizado contínuo e a adaptação a distribuições de dados variáveis ao longo do tempo.

Existem duas abordagens principais para o EM online:
* **Incremental EM:** Otimiza o *lower bound* $Q(\theta, q_1,..., q_N)$ um $q_i$ por vez [^29].
* **Stepwise EM:** Sempre que uma nova estatística $s_i$ é computada, os parâmetros são movidos na direção dela [^30].

### Conclusão

A estimação de parâmetros em modelos de mistura é um problema desafiador devido à não convexidade da função de *log-verossimilhança* e à não identificabilidade dos parâmetros [^4]. O algoritmo EM é uma ferramenta poderosa para encontrar estimativas de ML ou MAP, mas requer consideração cuidadosa para evitar ficar preso em ótimos locais e para mitigar os efeitos da não identificabilidade [^1]. Abordagens como a estimação MAP, a imposição de restrições nos parâmetros, a regularização e a seleção de modelo podem ajudar a melhorar a qualidade das soluções obtidas. Além disso, algoritmos EM online permitem o aprendizado contínuo e a adaptação a distribuições de dados variáveis ao longo do tempo [^1].

### Referências

[^1]: Mixture models and the EM algorithm.
[^2]: 11.2 Mixture models.
[^3]: 11.1 Latent variable models.
[^4]: 11.3 Parameter estimation for mixture models.
[^5]: 11.4 The EM algorithm.
[^29]: 11.4.8 Online EM.
[^30]: 11.4.8.3 Stepwise EM.
[^34]: 11.5 Model selection for latent variable models.

<!-- END -->