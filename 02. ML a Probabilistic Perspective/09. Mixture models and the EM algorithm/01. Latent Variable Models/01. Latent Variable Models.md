## Latent Variable Models: Unveiling Hidden Structures

### Introdução
Em continuidade ao Capítulo 10, que abordou como modelos gráficos podem ser utilizados para definir distribuições de probabilidade conjuntas de alta dimensionalidade, este capítulo aprofunda o estudo dos **Latent Variable Models (LVMs)** [^1]. Como alternativa à modelagem direta da dependência entre variáveis observadas, os LVMs postulam que as correlações observadas surgem de uma "causa" comum oculta [^1]. Embora a adaptação desses modelos seja mais desafiadora em comparação com modelos sem variáveis latentes, os LVMs oferecem vantagens significativas, incluindo a redução do número de parâmetros e a capacidade de computar representações compactas dos dados por meio de um efeito de *bottleneck* [^1]. Este capítulo explorará a estrutura, os desafios matemáticos e as aplicações dos LVMs, com foco em modelos de mistura e no algoritmo Expectation-Maximization (EM).

### Conceitos Fundamentais

Os LVMs correlacionam variáveis observadas por meio de causas comuns ocultas, oferecendo vantagens como menos parâmetros e representação de dados compactada via um efeito de *bottleneck* para aprendizado não supervisionado [^1]. Estruturas genéricas de LVM envolvem $L$ variáveis latentes e $D$ variáveis visíveis, onde $D >> L$ [^1]. LVMs lidam com mapeamentos muitos-para-muitos, um-para-muitos, muitos-para-um e um-para-um entre variáveis latentes e visíveis, habilitando várias gerações de modelos baseadas nas formas da verossimilhança $p(x_i|z_i)$ e da *prior* $p(z_i)$ [^1].

**Estrutura e Mapeamentos:**
Em geral, os LVMs consistem em $L$ variáveis latentes, denotadas como $z_{i1}, ..., z_{iL}$, e $D$ variáveis visíveis (observadas), denotadas como $x_{i1}, ..., x_{iD}$ [^1]. A relação entre $L$ e $D$ influencia o tipo de mapeamento entre as variáveis latentes e visíveis [^1]:
- **Muitos-para-muitos:** Quando $L > 1$, múltiplos fatores latentes contribuem para cada observação [^1].
- **Um-para-muitos:** Quando $L = 1$, uma única variável latente $z_i$ representa diferentes fatores ou causas para cada variável observada [^1].
- **Muitos-para-um:** Representa diferentes fatores ou causas competindo para cada variável observada [^1]. Estes modelos formam a base da fatoração de matrizes probabilísticas, conforme discutido na Seção 27.6.2 (não presente no contexto, mas referenciado no original) [^1].
- **Um-para-um:** Representado como $z_i \rightarrow x_i$. Ao permitir que $z_i$ e/ou $x_i$ sejam vetoriais, essa representação pode abranger todas as outras [^1].

**Modelos Gráficos Direcionados (DGMs):**
LVMs podem ser representados por DGMs, onde as folhas denotam variáveis observadas (ex: sintomas médicos) e as raízes significam causas primárias (ex: fumar), com variáveis ocultas mediando fatores [^1]. O número de variáveis latentes ($L$) e visíveis ($D$) determina a complexidade do mapeamento do modelo [^1].

**Desafios Matemáticos:**
Os desafios matemáticos em LVMs incluem a dificuldade de ajustar esses modelos em comparação com aqueles sem variáveis latentes, principalmente devido ao aumento da complexidade na estimação de parâmetros e às questões de não identificabilidade decorrentes das variáveis ocultas [^1].

**Modelos de Mistura:**
A forma mais simples de LVM é quando $z_i \in \{1, ..., K\}$, representando um estado latente discreto [^1]. Usamos um *prior* discreto para isso, $p(z_i) = Cat(\pi)$. Para a verossimilhança, usamos $p(x_i|z_i = k) = p_k(x_i)$ [^1].

### Conclusão
Este capítulo introduziu os conceitos fundamentais dos Latent Variable Models, destacando sua estrutura, tipos de mapeamentos, representação em DGMs e desafios matemáticos. A apresentação dos modelos de mistura como um caso especial de LVMs fornece uma base para a compreensão de algoritmos mais avançados, como o EM, que serão explorados em detalhes nas próximas seções [^1].
### Referências
[^1]: Capítulo 11 do livro texto fornecido.
<!-- END -->