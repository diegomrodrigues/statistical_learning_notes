## O Algoritmo EM para Modelos de Mistura Gaussiana (GMMs) e o Algoritmo K-means

### Introdução
Este capítulo aprofunda-se no algoritmo Expectation-Maximization (EM) aplicado a Modelos de Mistura Gaussiana (GMMs), uma técnica fundamental para modelagem de densidade e clustering [^3]. Expandindo o conceito de modelos de variáveis latentes [^1], exploraremos como o algoritmo EM itera entre um passo E (Expectation) e um passo M (Maximization) para estimar os parâmetros de um GMM. Além disso, analisaremos a relação entre o algoritmo EM para GMMs e o algoritmo K-means, destacando as simplificações e suposições que levam ao K-means.

### Conceitos Fundamentais

#### O Algoritmo EM para GMMs
O algoritmo EM é um método iterativo para encontrar estimativas de máxima verossimilhança (MLE) ou estimativas de máximo a posteriori (MAP) de parâmetros em modelos estatísticos que dependem de variáveis latentes não observadas [^14]. No contexto de GMMs, as variáveis latentes representam a associação de cada ponto de dados a um componente gaussiano específico da mistura [^2].

O algoritmo EM para GMMs envolve os seguintes passos iterativos [^14]:

1.  **Passo E (Expectation):** Calcula a *responsabilidade* que o cluster *k* assume pelo ponto de dado *i*, denotada por $r_{ik}$ [^4, 15]. A responsabilidade é a probabilidade posterior de que o ponto de dado *i* tenha sido gerado pelo componente *k* da mistura, dados os parâmetros atuais do modelo.
    $$r_{ik} = \frac{\pi_k p(x_i|\theta^{(t-1)})}{\sum_{k'} \pi_{k'} p(x_i|\theta^{(t-1})} $$
    onde:
    *   $\pi_k$ é o peso de mistura do componente *k* [^4].
    *   $p(x_i|\theta^{(t-1)})$ é a densidade gaussiana do componente *k* avaliada no ponto de dado $x_i$, usando os parâmetros $\theta$ estimados na iteração anterior (*t*-1) [^4].
    *   $\theta$ representa o conjunto de parâmetros do modelo, incluindo pesos de mistura, médias e covariâncias [^4].

2.  **Passo M (Maximization):** Atualiza os pesos de mistura, médias e covariâncias dos componentes gaussianos usando as responsabilidades calculadas no passo E [^4, 15].
    *   **Pesos de Mistura:** Os pesos de mistura são atualizados para refletir a proporção de pontos de dados que são "responsabilizados" por cada componente.
        $$\pi_k = \frac{1}{N} \sum_i r_{ik}$$
        onde *N* é o número total de pontos de dados [^4, 15].

    *   **Médias:** As médias dos componentes gaussianos são atualizadas para serem a média ponderada dos pontos de dados, onde os pesos são as responsabilidades [^4].
        $$\mu_k = \frac{\sum_i r_{ik} x_i}{r_k}$$
        onde $r_k = \sum_i r_{ik}$ é o número ponderado de pontos atribuídos ao cluster *k* [^4].

    *   **Covariâncias:** As matrizes de covariância são atualizadas para refletir a dispersão dos pontos de dados em torno de suas respectivas médias, ponderadas pelas responsabilidades [^4].
        $$\Sigma_k = \frac{\sum_i r_{ik} (x_i - \mu_k)(x_i - \mu_k)^T}{r_k}$$

O algoritmo EM itera entre os passos E e M até que a convergência seja alcançada, tipicamente definida como uma mudança suficientemente pequena na função de log-verossimilhança ou nos parâmetros do modelo [^14].

#### K-means como um Caso Especial do EM para GMMs
O algoritmo K-means pode ser visto como uma variante simplificada do algoritmo EM para GMMs sob certas suposições [^4]. Especificamente, o K-means assume [^4, 16]:

*   **Covariâncias Fixas:** As matrizes de covariância são fixas e iguais para todos os componentes, sendo proporcionais à matriz identidade: $\Sigma_k = \sigma^2I$, onde $\sigma^2$ é uma variância comum e *I* é a matriz identidade [^4]. Isto implica que os clusters são esféricos e têm a mesma dispersão.
*   **Pesos de Mistura Uniformes:** Os pesos de mistura são fixos e iguais para todos os componentes: $\pi_k = 1/K$, onde *K* é o número de clusters [^4]. Isto implica que todos os clusters têm a mesma probabilidade *a priori*.

Sob estas suposições, o passo E do algoritmo EM se simplifica para uma atribuição *hard* [^4, 16]: cada ponto de dado é atribuído ao cluster com a média mais próxima, com base na distância euclidiana [^16]. Isso significa que a responsabilidade $r_{ik}$ é 1 para o cluster mais próximo e 0 para todos os outros [^4].

O passo M também se simplifica: as médias dos clusters são atualizadas calculando a média de todos os pontos atribuídos a cada cluster [^4, 16].

Em resumo, o algoritmo K-means pode ser visto como um caso especial do algoritmo EM para GMMs com as seguintes características [^4, 16]:

*   Atribuição *hard* de pontos de dados a clusters [^4, 16].
*   Clusters esféricos com a mesma variância [^4].
*   Pesos de mistura uniformes [^4].

#### Inicialização e Convergência
Tanto o algoritmo EM para GMMs quanto o algoritmo K-means são sensíveis à inicialização [^17]. Uma má inicialização pode levar a convergência para um ótimo local subótimo [^17]. Algumas estratégias comuns de inicialização incluem [^17]:

*   Escolher *K* pontos de dados aleatoriamente como médias iniciais [^17].
*   Usar o algoritmo K-means para obter uma inicialização razoável para as médias, e então usar essas médias para inicializar o algoritmo EM para GMMs [^17].
*   Farthest point clustering [^17].

### Conclusão
O algoritmo EM para GMMs é uma ferramenta poderosa e flexível para modelagem de densidade e clustering [^3]. Sua capacidade de estimar os parâmetros de componentes gaussianos com diferentes formas e tamanhos o torna adequado para uma ampla gama de aplicações. O algoritmo K-means, como um caso especial simplificado, oferece uma alternativa computacionalmente mais eficiente quando as suposições de clusters esféricos e pesos de mistura uniformes são razoáveis [^4]. É importante notar que tanto o algoritmo EM quanto o K-means convergem para um ótimo local, e a escolha do método de inicialização pode influenciar o resultado final [^17].

### Referências
[^1]: 11.1 Latent variable models
[^2]: 11.2 Mixture models
[^3]: 11.2.3 Using mixture models for clustering
[^4]: 11.4.2 EM for GMMS
[^14]: 11.4 The EM algorithm
[^15]: 11.4.2.1 Auxiliary function
[^16]: 11.4.2.5 K-means algorithm
[^17]: 11.4.2.7 Initialization and avoiding local minima
<!-- END -->