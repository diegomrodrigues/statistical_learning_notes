## Non-Convex Regularizers: Beyond the Laplace Prior

### Introdução
Expandindo as técnicas de regularização $l_1$ previamente discutidas, este capítulo explora métodos de regularização não-convexos que oferecem vantagens em cenários específicos. Embora a regularização $l_1$ seja amplamente utilizada devido à sua convexidade, que facilita a otimização, ela apresenta limitações inerentes em termos de desempenho estatístico. Conforme mencionado em [^429], a regularização $l_1$ resulta de um prior de Laplace, que pode não ser ideal em todas as situações. Este capítulo aborda as limitações do prior de Laplace e introduz priors não-convexos como alternativas promissoras [^457].

### Conceitos Fundamentais
A regularização $l_1$, derivada do prior de Laplace, enfrenta dois desafios principais [^457]:
1. **Insuficiente massa de probabilidade perto de zero:** Isso leva a uma supressão inadequada de ruído.
2. **Insuficiente massa de probabilidade em valores grandes:** Isso resulta em *shrinkage* excessivo de coeficientes relevantes, introduzindo viés.

Esses problemas podem ser mitigados utilizando priors mais flexíveis, que exibam um pico maior em 0 e caudas mais pesadas. Embora a otimização global se torne mais desafiadora devido à não-convexidade, essas abordagens frequentemente superam a regularização $l_1$ tanto em precisão preditiva quanto na detecção de variáveis relevantes [^457].

**Bridge Regression:** Uma generalização natural da regularização $l_1$ é a **bridge regression** [^458], definida como:
$$ \hat{w} = \underset{w}{\text{argmin}} \ NLL(w) + \lambda \sum_j |w_j|^b $$
onde $b \geq 0$. Esta formulação corresponde à estimativa MAP utilizando uma **distribuição de potência exponencial** [^458]:
$$ ExpPower(w|\mu, a, b) \triangleq \frac{b}{2a\Gamma(1 + 1/b)} exp\left( - \left|\frac{w - \mu}{a}\right|^b \right) $$
- $b = 2$: Distribuição Gaussiana (ridge regression)
- $b = 1$: Distribuição de Laplace (lasso)
- $b = 0$: Melhor seleção de subconjunto (não-convexo)

A escolha de $b$ influencia as propriedades de esparsidade e convexidade da solução. A norma $l_1$ ($b=1$) é a aproximação convexa mais "apertada" da norma $l_0$ [^458].

**Hierarchical Adaptive Lasso (HAL):** Uma das principais limitações do lasso é que ele leva a estimativas enviesadas [^458]. Para mitigar isso, o **Hierarchical Adaptive Lasso (HAL)** associa a cada parâmetro um parâmetro de penalidade individual, adaptando a força da regularização a cada coeficiente. Ao invés de ajustar $D$ parâmetros por validação cruzada, HAL trata os parâmetros $\tau_j$ como variáveis aleatórias provenientes de um prior conjugado [^458]:
$$ \gamma_j \sim IG(a, b) \\ \tau_j^2 \sim Ga(1, \frac{\gamma_j^2}{2}) \\ w_j \sim N(0, \tau_j^2) $$
Integrando $\tau_j$, induz-se uma distribuição de Laplace em $w_j$. A distribuição marginal resultante é uma mistura escalonada de Laplace. O modelo HAL pode ser ajustado usando EM [^458].

A função de penalidade resultante é dada por [^461]:
$$ \pi_{\lambda}(w_j) \triangleq -log \ p(w_j) = (a+1)log(1 + \frac{|w_j|}{b}) + const $$

### Conclusão
Enquanto a regularização $l_1$ oferece uma abordagem convexa para promover a esparsidade, priors não-convexos, como os empregados em bridge regression e Hierarchical Adaptive Lasso, podem fornecer soluções mais precisas e adaptativas. Esses métodos permitem maior flexibilidade na modelagem da distribuição dos coeficientes, levando a melhor desempenho preditivo e maior precisão na seleção de variáveis relevantes. A complexidade computacional associada à otimização não-convexa é um desafio, mas os benefícios potenciais em termos de desempenho justificam a exploração dessas técnicas avançadas. <!-- END -->