## Group Lasso: Regularização Estruturada para Esparsidade em Grupos de Variáveis

### Introdução
O capítulo anterior focou na regularização $l_1$ e suas propriedades indutoras de esparsidade. Este capítulo expande esse conceito, introduzindo o **Group Lasso**, uma extensão da regularização $l_1$ que promove a esparsidade em nível de grupo, selecionando ou deselecionando grupos inteiros de variáveis [^29]. Essa abordagem é particularmente útil em modelos onde há múltiplos parâmetros associados a uma única variável, e a seleção individual desses parâmetros não é desejável ou interpretável [^29].

### Conceitos Fundamentais

O Group Lasso estende a regularização $l_1$ para casos onde há muitos parâmetros associados a uma dada variável, utilizando uma 2-norma do vetor de peso do grupo [^29]. A ideia central é penalizar a norma do vetor de pesos associado a cada grupo de variáveis, incentivando que todos os pesos dentro de um grupo sejam simultaneamente zero ou não-zero [^29]. Formalmente, o objetivo a ser minimizado é dado por:

$$
J(w) = NLL(W) + \sum_{g=1}^{G} \lambda_g ||W_g||_2
$$

onde:
*   $NLL(W)$ representa o *negative log-likelihood* do modelo [^29].
*   $W_g$ é o vetor de pesos associado ao *g*-ésimo grupo de variáveis [^29].
*   $||W_g||_2$ denota a *2-norma* (norma Euclidiana) do vetor de pesos do grupo [^29].
*   $\lambda_g$ é o parâmetro de *regularização* para o *g*-ésimo grupo [^29].
*   $G$ é o número total de *grupos* [^29].

A 2-norma do vetor de peso do grupo é definida como [^30]:

$$
||W_g||_2 = \sqrt{\sum_{j \in g} w_j^2}
$$

onde a soma é sobre todos os elementos $j$ no grupo $g$ [^30].

A escolha do parâmetro de regularização $\lambda_g$ pode ser adaptada ao tamanho do grupo $g$ [^30]. Uma prática comum é definir $\lambda_g = \lambda \sqrt{d_g}$, onde $d_g$ é o número de elementos no grupo $g$ [^30]. Isso compensa o fato de que grupos maiores têm maior probabilidade de ter uma norma maior simplesmente devido ao número de elementos [^30].

**Comparação com Ridge Regression:** É importante notar que, se a norma utilizada fosse o quadrado da 2-norma, o modelo se tornaria equivalente à *ridge regression*, que não induz esparsidade em nível de grupo [^30].

**Variações do Group Lasso:** Uma variação dessa técnica substitui a 2-norma pela norma do infinito [^30]:

$$
||W_g||_{\infty} = \max_{j \in g} |w_j|
$$

Essa abordagem também resulta em esparsidade em nível de grupo [^30].

**Interpretação como GSM (Gaussian Scale Mixture):** O Group Lasso pode ser interpretado como uma estimativa MAP (Maximum a Posteriori) usando o seguinte prior [^30]:

$$
p(w|\gamma, \sigma^2) \propto \exp\left(-\frac{\gamma}{2\sigma^2} \sum_{g=1}^{G} ||W_g||_2 \right)
$$

onde $\gamma$ controla a força da regularização [^30]. Esse prior pode ser escrito como uma mistura de escala gaussiana, com uma variância diferente para cada grupo [^31].

### Conclusão

O Group Lasso oferece uma abordagem flexível e poderosa para induzir esparsidade em nível de grupo, permitindo a seleção ou deseleção de grupos inteiros de variáveis [^29]. Sua interpretação como uma estimativa MAP com um prior específico fornece *insights* sobre seu comportamento e permite a derivação de algoritmos eficientes para sua otimização [^30]. Em comparação com a regularização $l_1$ padrão, o Group Lasso é mais adequado para modelos onde a estrutura de grupo das variáveis é conhecida e relevante [^29].

### Referências
[^29]: Chapter 13. Sparse linear models.
[^30]: 13.5 l₁ regularization: extensions
[^31]: 13.5.1 Group Lasso
<!-- END -->