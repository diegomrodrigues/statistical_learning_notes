## Elastic Net: Combining Ridge and Lasso Regularization

### Introdução
O Elastic Net é uma técnica de regularização que combina as penalidades L1 (Lasso) e L2 (Ridge) para superar as limitações de cada método individualmente. Como mencionado no contexto, o Elastic Net utiliza uma função de penalidade estritamente convexa que exibe um efeito de agrupamento, onde variáveis altamente correlacionadas tendem a ter coeficientes de regressão similares [^1]. Este capítulo explorará em detalhes a formulação do Elastic Net, suas propriedades e algoritmos para sua implementação.

### Conceitos Fundamentais

O Elastic Net combina a regularização L1 e L2 utilizando a seguinte função objetivo [^1]:

$$J(w, \lambda_1, \lambda_2) = ||y - Xw||^2 + \lambda_2||w||^2 + \lambda_1||w||_1$$

onde:
- $w$ é o vetor de pesos do modelo.
- $X$ é a matriz de características.
- $y$ é o vetor de variáveis de resposta.
- $\lambda_1$ é o parâmetro de regularização L1 (Lasso).
- $\lambda_2$ é o parâmetro de regularização L2 (Ridge).

A primeira parte da função objetivo, $||y - Xw||^2$, representa o erro quadrático médio, que mede a diferença entre os valores previstos pelo modelo e os valores reais [^1]. A segunda parte, $\lambda_2||w||^2$, é a penalidade L2, que adiciona uma penalidade proporcional ao quadrado da norma L2 dos pesos [^1]. A terceira parte, $\lambda_1||w||_1$, é a penalidade L1, que adiciona uma penalidade proporcional ao valor absoluto da norma L1 dos pesos [^1].

A penalidade L1 promove a **sparsidade** no modelo, forçando alguns dos coeficientes a serem exatamente zero [^1]. Isso é útil para seleção de características, pois as variáveis com coeficientes zero são efetivamente removidas do modelo. No entanto, o Lasso tem a limitação de selecionar apenas uma variável de um grupo de variáveis altamente correlacionadas, escolhendo uma de forma arbitrária [^35].

A penalidade L2, por outro lado, promove a **redução dos coeficientes**, evitando o overfitting sem forçar os coeficientes a serem exatamente zero [^1]. Além disso, a penalidade L2 proporciona **estabilidade** ao modelo, especialmente em casos onde a matriz de características é quase singular. O Elastic Net, ao combinar ambas as penalidades, busca um equilíbrio entre a seleção de características e a redução dos coeficientes [^1].

#### Propriedades do Elastic Net

1.  **Convexidade:** A função objetivo do Elastic Net é estritamente convexa, desde que $\lambda_2 > 0$ [^36]. Isso garante a existência de um único mínimo global, facilitando a otimização.
2.  **Efeito de Agrupamento:** O Elastic Net exibe um efeito de agrupamento, o que significa que variáveis altamente correlacionadas tendem a ter coeficientes de regressão similares [^1, 36]. Isso é uma vantagem em relação ao Lasso, que tende a selecionar apenas uma variável de um grupo de variáveis correlacionadas.
3.  **Número de Variáveis Selecionadas:** Ao contrário do Lasso, o Elastic Net pode selecionar mais variáveis do que o número de amostras ($N$), mesmo no caso em que o número de dimensões ($D$) é maior que $N$ [^35].
4.  **Estabilidade:** A penalidade L2 no Elastic Net proporciona maior estabilidade em comparação com o Lasso, especialmente quando as variáveis são altamente correlacionadas [^35].

#### Algoritmos para Elastic Net

Uma das abordagens para resolver o problema do Elastic Net é reduzir o problema a um problema Lasso em dados modificados [^36]. Definimos:
$$tilde{X} = \frac{c}{\sqrt{\lambda_2}} \begin{pmatrix} X \\\\ \sqrt{\lambda_2}I_D \end{pmatrix}, \quad \tilde{y} =  \begin{pmatrix} y \\\\ 0_{D \times 1} \end{pmatrix}$$
onde $c = (1 + \lambda_2)^{-1/2}$ e $I_D$ é a matriz identidade de dimensão $D$. Então, resolvemos:
$$tilde{w} = \underset{w}{\text{argmin}} ||\tilde{y} - \tilde{X}w||^2 + c\lambda_1||w||_1$$
e definimos $w = c\tilde{w}$ [^36]. Este subproblema pode ser resolvido utilizando o algoritmo LARS-EN [^36].

Uma melhoria para a versão "vanilla" do Elastic Net é escalar as estimativas da versão vanilla para desfazer a contração da penalidade L2 [^36]. Se $w^*$ é a solução da Equação 13.124 [^36], então uma estimativa melhor é:

$$hat{w} = \sqrt{1 + \lambda_2}w^*$$

#### Interpretação GSM do Elastic Net

O Elastic Net também pode ser interpretado como uma estimativa MAP sob um modelo hierárquico [^37]:

$$p(w|\sigma^2) \propto exp \left( -\frac{\gamma}{\sigma} \sum_{j=1}^D |w_j| - \frac{\lambda_2}{2\sigma^2} \sum_{j=1}^D w_j^2 \right)$$

Este prior pode ser escrito como um modelo hierárquico como se segue:

$$w_j|\sigma^2, \tau_j \sim N(0, \sigma^2(\tau_j^{-2} + \gamma_2)^{-1})$$
$$tau_j^2|\gamma_1 \sim Expon(\frac{\gamma_1}{2})$$

Claramente, se $\lambda_1 = 0$, isso reduz ao regular lasso [^37].

### Conclusão
O Elastic Net oferece uma abordagem flexível e poderosa para a regularização, combinando as vantagens das penalidades L1 e L2. Ao promover a sparsidade e o agrupamento, o Elastic Net pode melhorar a precisão e a interpretabilidade dos modelos, especialmente em conjuntos de dados com alta dimensionalidade e multicolinearidade.

### Referências
[^1]: Trecho do contexto fornecido que introduz o Elastic Net.
[^35]: Trecho do contexto que discute as limitações do Lasso e as vantagens do Elastic Net.
[^36]: Trecho do contexto que detalha a formulação matemática e os algoritmos do Elastic Net.
[^37]: Trecho do contexto que apresenta a interpretação GSM do Elastic Net.

<!-- END -->