## Coordinate Descent and the Shooting Algorithm for Lasso

### Introdução
No contexto de algoritmos para modelos lineares esparsos e regularização L1, a busca exaustiva pelo espaço de possíveis modelos é proibitivamente custosa devido ao número exponencial de modelos [^1]. Assim, torna-se necessário recorrer a heurísticas para encontrar soluções adequadas. Este capítulo explora o algoritmo de **coordinate descent** e sua aplicação específica, o **shooting algorithm**, para a resolução do problema **Lasso** [^1].

### Conceitos Fundamentais
O **coordinate descent** é uma técnica iterativa de otimização que minimiza uma função otimizando uma variável de cada vez, mantendo as demais fixas [^1]. A atratividade do método reside na sua simplicidade e eficiência, especialmente quando a otimização unidimensional resultante pode ser resolvida analiticamente [^1].

Para o problema **Lasso**, definido pela minimização de:

$$ f(w) = \frac{1}{N} \sum_{i=1}^{N} (y_i - (w_0 + w^T x_i))^2 + \lambda ||w||_1 $$

onde $w$ é o vetor de pesos, $x_i$ são os vetores de características, $y_i$ são os valores de resposta, e $\lambda$ é o parâmetro de regularização, o **shooting algorithm** oferece uma solução analítica para cada atualização de peso [^1].

A atualização de cada peso $w_j$ é obtida resolvendo a condição de otimalidade para o **Lasso**, que envolve o subgradiente [^1]. A derivada parcial do RSS (Residual Sum of Squares) em relação a $w_j$ é dada por [^1]:

$$ \frac{\partial}{\partial w_j} RSS(w) = a_j w_j - c_j $$

onde

$$ a_j = 2 \sum_{i=1}^{n} x_{ij}^2 $$

$$ c_j = 2 \sum_{i=1}^{n} x_{ij} (y_i - x_{i, -j}^T w_{-j}) $$

Combinando com a penalidade L1, a atualização de $w_j$ no **shooting algorithm** é dada por [^1]:

$$ \hat{w_j} = \begin{cases} \frac{c_j + \lambda}{a_j} & \text{se } c_j < -\lambda \\ 0 & \text{se } c_j \in [-\lambda, \lambda] \\ \frac{c_j - \lambda}{a_j} & \text{se } c_j > \lambda \end{cases} $$

Esta atualização pode ser expressa de forma compacta usando a função de *soft thresholding* [^1]:

$$ \hat{w_j} = \frac{soft(c_j, \lambda)}{a_j} $$

onde

$$ soft(\alpha; \delta) = sign(\alpha) (|\alpha| - \delta)_+ $$

O algoritmo itera sobre cada peso, atualizando-o com base nos valores atuais dos outros pesos, até que a convergência seja alcançada [^1]. Este procedimento é detalhado no Algoritmo 13.1 [^21].

### Conclusão
O **coordinate descent**, exemplificado pelo **shooting algorithm** para o **Lasso**, oferece uma abordagem computacionalmente eficiente para a obtenção de modelos lineares esparsos [^1]. Sua simplicidade e a possibilidade de resoluções analíticas o tornam uma ferramenta valiosa no arsenal de técnicas para problemas de seleção de características e regularização [^1]. Embora outros algoritmos mais complexos possam existir, o **shooting algorithm** permanece relevante devido à sua interpretabilidade e facilidade de implementação [^1].

### Referências
[^1]: Sparse linear models, *Pattern Recognition and Machine Learning*, Christopher Bishop (2006)
[^21]: Section 13.4 l₁ regularization: algorithms, *Pattern Recognition and Machine Learning*, Christopher Bishop (2006)
<!-- END -->