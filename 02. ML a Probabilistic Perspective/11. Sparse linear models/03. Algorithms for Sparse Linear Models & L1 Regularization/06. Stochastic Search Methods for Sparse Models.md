## Métodos Estocásticos e EM para Modelos Lineares Esparsos

### Introdução
Em modelos lineares esparsos, a seleção de variáveis e a regularização $l_1$ são cruciais para evitar overfitting e melhorar a interpretabilidade. Dada a dificuldade de explorar o espaço completo de modelos (2^D possíveis modelos para D dimensões) [^6], torna-se necessário recorrer a métodos heurísticos para encontrar soluções subótimas. Este capítulo explora métodos estocásticos, como MCMC, e o algoritmo EM (Expectation-Maximization) como alternativas para aproximar a distribuição posterior e resolver problemas de regularização $l_1$ [^1, ^6].

### Conceitos Fundamentais
#### Métodos Estocásticos (MCMC)
Quando se deseja aproximar a distribuição posterior em vez de apenas encontrar o modo (e.g., para computar probabilidades de inclusão marginal), uma opção é utilizar MCMC (Markov Chain Monte Carlo) [^9]. A abordagem padrão envolve o uso de Metropolis-Hastings, onde a distribuição proposta simplesmente inverte um único bit [^9]. Isso permite computar eficientemente $p(\gamma'|D)$ dado $p(\gamma|D)$, onde $\gamma$ representa a configuração de bits (modelo) e $D$ os dados [^9]. A probabilidade de um estado (configuração de bits) é estimada contando quantas vezes o passeio aleatório visita este estado [^9]. No entanto, em um espaço de estados discreto, o MCMC é desnecessariamente ineficiente, uma vez que podemos computar diretamente a probabilidade (não normalizada) de um estado usando $p(\gamma, D) = \exp(-f(\gamma))$ [^9]. Uma alternativa mais eficiente é usar algum tipo de algoritmo de busca estocástica para gerar um conjunto $S$ de modelos de alta pontuação e então fazer a seguinte aproximação:

$$p(\gamma|D) \approx \frac{e^{-f(\gamma)}}{\sum_{\gamma' \in S} e^{-f(\gamma')}}$$

#### Algoritmo EM e Inferência Variacional
O algoritmo EM (Expectation-Maximization) é uma técnica iterativa para encontrar estimativas de máxima verossimilhança (MLE) ou máximo a posteriori (MAP) em modelos com variáveis latentes.

##### EM para Lasso
O problema do Lasso pode ser resolvido usando o algoritmo EM através da representação da distribuição de Laplace como uma mistura de escala Gaussiana (Gaussian scale mixture - GSM) [^1, ^27]. A distribuição de Laplace é expressa como:

$$Lap(w_j|0, 1/\lambda) = \frac{\lambda}{2} e^{-\lambda |w_j|} = \int N(w_j|0, \tau_j^2) Ga(\tau_j^2|1, \frac{\lambda^2}{2}) d\tau_j^2$$

onde $N(w_j|0, \tau_j^2)$ é uma distribuição Gaussiana com média 0 e variância $\tau_j^2$, e $Ga(\tau_j^2|1, \frac{\lambda^2}{2})$ é uma distribuição Gama com parâmetros de forma 1 e taxa $\frac{\lambda^2}{2}$.

Usando essa decomposição, o modelo Lasso pode ser representado como um modelo hierárquico com variáveis latentes [^27]. A distribuição conjunta correspondente tem a forma:

$$p(y, w, \tau, \sigma^2|X) = N(y|Xw, \sigma^2 I_N) N(w|0, D_\tau) IG(\sigma^2|a_o, b_o) \prod_j Ga(\tau_j^2|1, \lambda^2/2)$$

Onde $D_\tau$ é uma matriz diagonal com os $\tau_j^2$ na diagonal, $IG$ representa a distribuição Inverse Gamma e $Ga$ representa a distribuição Gama [^27]. Os passos do algoritmo EM são:

*   **Passo E (Expectation):** Inferir as variáveis latentes $\tau_j^2$ e $\sigma^2$ dado os parâmetros atuais.
    *   Calcular $E[\frac{1}{\tau_j^2}|w, D]$ utilizando a distribuição Inverse Gaussian [^28]:

        $$p(1/\tau_j^2|w, D) = \text{InverseGaussian}(\sqrt{\frac{\lambda^2}{w_j^2}}, \lambda^2)$$

        $$E[\frac{1}{\tau_j^2}|w] = \frac{\lambda}{|w_j|}$$

    *   Calcular $E[\frac{1}{\sigma^2}|D, w]$ utilizando a distribuição Inverse Gamma [^28]:

        $$p(\sigma^2|D, w) = IG(a_N, b_N)$$

        $$E[\frac{1}{\sigma^2}] = \frac{a_N}{b_N}$$
*   **Passo M (Maximization):** Estimar os parâmetros $w$ maximizando a função de log-verossimilhança esperada [^28]:

    $$w = \underset{w}{\operatorname{argmax}} -\frac{1}{2}||y - Xw||_2^2 - \frac{1}{2} w^T A w$$

    $$w = (\sigma^2 A + X^T X)^{-1} X^T y$$

    Onde $A = \text{diag}(E[\frac{1}{\tau_1^2}], ..., E[\frac{1}{\tau_D^2}])$

##### EM e Inferência Variacional para o modelo Spike and Slab e Modelo Bernoulli-Gaussiano
O modelo *spike and slab* tem a forma $\gamma_j \rightarrow w_j \rightarrow y$ [^4, ^6]. No passo E, computamos $p(\gamma_j = 1|w_j)$ e otimizamos $w$ no passo M. No entanto, isso não funciona porque, ao computar $p(\gamma_j = 1|w_j)$, estamos comparando uma função delta, $\delta_0(w_j)$, com uma pdf Gaussiana, $N(w_j|0, \sigma^2)$ [^4]. Podemos substituir a função delta por uma Gaussiana estreita, e então o passo E equivale a classificar $w_j$ sob os dois possíveis modelos Gaussianos [^4]. No entanto, é provável que isso sofra de mínimos locais severos [^4].

Uma alternativa é aplicar EM ao modelo Bernoulli-Gaussiano, que tem a forma $\gamma_j \rightarrow y \leftarrow w_j$ [^4, ^6]. Nesse caso, a posterior $p(y|D, w)$ é intratável para computar porque todos os bits se tornam correlacionados devido ao explaining away [^4]. No entanto, é possível derivar uma aproximação de campo médio da forma $\prod_j q(\gamma_j)q(w_j)$ [^4].

### Conclusão
Métodos estocásticos, como MCMC, oferecem uma maneira de explorar a distribuição posterior em modelos esparsos, mas podem ser computacionalmente intensivos [^9]. O algoritmo EM, combinado com a representação da distribuição de Laplace como uma mistura de escala Gaussiana, fornece uma abordagem eficiente para resolver o problema do Lasso, embora possa sofrer de mínimos locais [^27, ^4]. Alternativas como inferência variacional para o modelo Bernoulli-Gaussiano oferecem uma maneira de aproximar a distribuição posterior, mas também requerem considerações cuidadosas para evitar problemas de correlação e mínimos locais [^4]. A escolha do método depende das características específicas do problema e dos recursos computacionais disponíveis [^1, ^6].
<!-- END -->