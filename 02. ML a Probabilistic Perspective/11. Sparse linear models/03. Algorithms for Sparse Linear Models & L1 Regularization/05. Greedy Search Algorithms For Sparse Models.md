## Greedy Search Algorithms for MAP Model Selection in Sparse Linear Models

### Introdução
Este capítulo aprofunda-se no uso de **algoritmos de busca gulosa** para a seleção de modelos Maximum a Posteriori (MAP) em modelos lineares esparsos, um tópico crucial para lidar com a complexidade e o overfitting em problemas de alta dimensão [^1]. Em continuidade à discussão sobre a seleção de variáveis e esparsidade apresentada na seção 13.1 [^1], exploraremos como esses algoritmos iterativamente adicionam ou removem variáveis para otimizar um critério específico.

### Conceitos Fundamentais

A busca pelo modelo MAP em um espaço de modelos de alta dimensão é computacionalmente desafiadora, pois o número de modelos possíveis cresce exponencialmente com o número de dimensões ($2^D$) [^3]. Assim, **algoritmos de busca heurística** são empregados para encontrar soluções subótimas em um tempo razoável [^6]. Os algoritmos de busca gulosa, como o **Single Best Replacement (SBR)** e **Orthogonal Least Squares (OLS)**, são abordagens iterativas que exploram o espaço de modelos, avaliando o custo $f(\gamma)$ em cada ponto [^6].

#### Single Best Replacement (SBR)

O algoritmo **SBR** é uma técnica de *hill climbing* gulosa, onde, a cada passo, o algoritmo explora a vizinhança do modelo atual, definida como todos os modelos que podem ser alcançados invertendo um único bit de $\gamma$ [^7]. Em outras palavras, para cada variável, o algoritmo considera adicioná-la ao modelo se ela estiver ausente, ou removê-la se ela estiver presente [^7]. O SBR é iniciado com o conjunto vazio ($\gamma = 0$) e continua adicionando ou removendo variáveis até que nenhuma melhoria seja possível [^7]. Este processo pode ser visualizado como uma movimentação através do *lattice* de subconjuntos, como ilustrado na Figura 13.2(a) [^7].

#### Orthogonal Least Squares (OLS)

O algoritmo **OLS** é outra abordagem gulosa que, ao contrário do SBR, realiza apenas passos de adição, ou seja, *forward selection*. No contexto da equação 13.27, OLS corresponde a definir $\lambda = 0$, removendo a penalidade de complexidade [^7]. O algoritmo começa com o conjunto vazio e, a cada iteração, adiciona a variável que mais reduz o erro quadrático médio [^7].

A escolha da próxima variável $j^*$ a ser adicionada ao conjunto atual $\gamma^t$ é feita resolvendo o seguinte problema de otimização [^8]:
$$nj^* = \underset{j \notin \gamma^t}{\operatorname{arg\,min}} \,\, \underset{w}{\operatorname{min}} ||y - (X_{\gamma^t}w + X_{:,j}w_j)||^2$$
Após selecionar a melhor variável, o conjunto ativo é atualizado: $\gamma^{(t+1)} = \gamma^{(t)} \cup \{j^*\}$ [^8]. Para escolher a próxima variável, é necessário resolver $D - D_t$ problemas de mínimos quadrados a cada passo $t$, onde $D_t = |\gamma^t|$ é a cardinalidade do conjunto ativo atual [^8].

#### Orthogonal Matching Pursuit (OMP)

**Orthogonal Matching Pursuit (OMP)** é uma alternativa mais rápida ao OLS. A simplificação chave é "congelar" os pesos atuais em seus valores atuais e, em seguida, escolher a próxima característica a ser adicionada resolvendo o seguinte problema de otimização [^8]:
$$nj^* = \underset{j \notin \gamma^t}{\operatorname{arg\,min}} \,\, \underset{\beta}{\operatorname{min}} ||y - Xw_t - \beta x_{:,j}||^2$$
Essa otimização interna tem uma solução analítica: $\beta = \frac{x_{:,j}^T r_t}{||x_{:,j}||^2}$, onde $r_t = y - Xw_t$ é o vetor residual atual [^8]. Se as colunas forem de norma unitária, então [^8]:
$$nj^* = \underset{j}{\operatorname{arg\,max}} \,\, x_{:,j}^T r_t$$
Assim, OMP simplesmente procura a coluna que é mais correlacionada com o residual atual [^8]. Depois, o conjunto ativo é atualizado e a nova estimativa de mínimos quadrados $w_{t+1}$ é computada usando $X_{\gamma_{t+1}}$ [^8]. OMP requer apenas um cálculo de mínimos quadrados por iteração, sendo mais rápido que OLS, mas menos preciso [^8].

### Conclusão

Os algoritmos de busca gulosa, como SBR, OLS e OMP, oferecem abordagens computacionalmente eficientes para encontrar modelos MAP em modelos lineares esparsos [^7]. Embora não garantam a otimalidade global, sua capacidade de explorar o espaço de modelos de forma iterativa e adaptativa os torna ferramentas valiosas em problemas de alta dimensão [^6]. A escolha entre esses algoritmos depende do compromisso entre precisão e custo computacional, com OMP oferecendo uma alternativa mais rápida ao OLS, mas potencialmente menos precisa [^8]. O desenvolvimento de algoritmos mais sofisticados, como os métodos que exploram a estrutura do modelo e utilizam técnicas de aproximação, continua sendo uma área ativa de pesquisa [^8].

### Referências
[^1]: C. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006, p. 421.
[^2]: C. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006, p. 422.
[^3]: C. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006, p. 423.
[^4]: C. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006, p. 424.
[^5]: C. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006, p. 425.
[^6]: C. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006, p. 426.
[^7]: C. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006, p. 427.
[^8]: C. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006, p. 428.
<!-- END -->