## LARS: Least Angle Regression and Shrinkage

### Introdução
Este capítulo explora o algoritmo LARS (Least Angle Regression and Shrinkage) [^442], uma técnica eficiente para computar a solução $\mathbf{w}(\lambda)$ para todos os valores possíveis de $\lambda$ em modelos lineares esparsos. O LARS inicia com um valor grande de $\lambda$ e o diminui iterativamente, começando pela variável mais correlacionada com o vetor de resposta [^442]. Este método se destaca pela sua capacidade de calcular todo o caminho de regularização com um custo computacional relativamente baixo, tornando-o uma ferramenta valiosa no contexto de modelos esparsos e seleção de variáveis.

### Conceitos Fundamentais

O algoritmo LARS, como mencionado, é uma técnica eficiente para calcular $\mathbf{w}(\lambda)$ para todos os valores de $\lambda$ [^442]. Ele opera de forma iterativa, começando com a variável mais correlacionada com o vetor de resposta. A cada passo, o algoritmo diminui $\lambda$ até que uma segunda variável tenha a mesma correlação (em magnitude) com o resíduo atual que a primeira variável [^442].

**Funcionamento Detalhado do LARS**

1.  **Inicialização**: O algoritmo começa com todos os coeficientes definidos como zero, $\mathbf{w} = \mathbf{0}$.
2.  **Identificação da Variável Mais Correlacionada**: Encontra-se a variável $x_j$ que possui a maior correlação absoluta com o vetor de resposta $y$. Matematicamente, isso é expresso como:
    $$\
    j^* = \arg \max_j |\mathbf{x}_j^T \mathbf{y}|
    $$\
3.  **Movimento na Direção do "Least Angle"**: Em vez de adicionar a variável $x_{j^*}$ completamente ao modelo (como no *forward selection*), o LARS move o coeficiente $\mathbf{w}_{j^*}$ na direção do sinal da sua correlação com $\mathbf{y}$ [^442]. Este movimento é feito de tal forma que o ângulo entre o vetor de resposta e o espaço gerado pelas variáveis ativas (aquelas com coeficientes não-zero) seja minimizado.
4.  **Adição de Novas Variáveis**: O algoritmo continua a mover $\mathbf{w}_{j^*}$ até que outra variável $x_k$ tenha a mesma correlação (em magnitude) com o resíduo atual $\mathbf{r} = \mathbf{y} - \mathbf{X}\mathbf{w}$ [^442]. Neste ponto, ambas as variáveis $x_{j^*}$ e $x_k$ são consideradas "ativas".
5.  **Movimento Conjunto**: O LARS então move os coeficientes $\mathbf{w}_{j^*}$ e $\mathbf{w}_k$ *juntos*, na direção que mantém suas correlações com o resíduo o mais igual possível.
6.  **Iteração e Convergência**: Os passos 4 e 5 são repetidos até que todas as variáveis estejam ativas ou até que um critério de parada seja atingido (por exemplo, um valor máximo de $\lambda$ ou um número máximo de iterações).

**Relação com o Lasso**

Uma propriedade notável do LARS é que, com uma pequena modificação, ele pode ser usado para resolver o problema do Lasso [^442]. A modificação envolve a adição de um passo de "shrinkage" que pode remover variáveis do conjunto ativo [^442]. O Lasso impõe uma penalidade $L_1$ na norma dos coeficientes, promovendo soluções esparsas.

**Complexidade Computacional**

O LARS tem uma complexidade computacional comparável a uma única adaptação de mínimos quadrados ordinários (OLS), especificamente $O(ND \min(N, D))$ [^442]. Isso o torna uma alternativa eficiente para outros métodos de seleção de variáveis, especialmente quando se deseja explorar todo o caminho de regularização.

### Conclusão
O algoritmo LARS oferece uma abordagem eficiente e precisa para a seleção de variáveis e regularização em modelos lineares. Sua capacidade de computar todo o caminho de regularização com um custo computacional razoável o torna uma ferramenta valiosa para a construção de modelos esparsos e interpretáveis. Além disso, sua conexão com o Lasso e outros métodos de regularização fornece uma estrutura unificada para entender e aplicar técnicas de seleção de variáveis. Embora existam alternativas, como métodos de descida coordenada [^441], LARS destaca-se pela sua eficiência em percorrer o espaço de soluções, identificando rapidamente as variáveis mais relevantes.

### Referências
[^442]: *Efron et al. 2004*
[^441]: *Yaun et al. 2010*
<!-- END -->