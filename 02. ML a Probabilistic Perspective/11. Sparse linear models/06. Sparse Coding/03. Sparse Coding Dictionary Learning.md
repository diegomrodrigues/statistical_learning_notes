## Aprendizado do Dicionário de Codificação Esparsa

### Introdução
Este capítulo explora o aprendizado de um dicionário em codificação esparsa, um tópico fundamental para representações eficientes de dados. O objetivo é detalhar o processo de otimização envolvido, com ênfase na maximização da verossimilhança (*likelihood*). Conforme mencionado no contexto [^49], o aprendizado do dicionário envolve a maximização da verossimilhança, frequentemente aproximada pela maximização do logaritmo da verossimilhança de cada ponto de dado.

### Conceitos Fundamentais

**Codificação esparsa** busca representar dados como uma combinação linear esparsa de elementos de um dicionário [^49]. Dado um conjunto de dados $X = \{x_i\}_{i=1}^N$, onde $x_i \in \mathbb{R}^D$, o objetivo é encontrar um dicionário $W \in \mathbb{R}^{D \times L}$ e representações esparsas $Z = \{z_i\}_{i=1}^N$, onde $z_i \in \mathbb{R}^L$, tal que $x_i \approx Wz_i$. A esparsidade é imposta para garantir que cada $x_i$ seja representado usando apenas um pequeno número de átomos do dicionário [^1, ^49].

A **verossimilhança** (*likelihood*) dos dados dado o dicionário e as representações esparsas pode ser expressa como:
$$ p(D|W) = \prod_{i=1}^N p(x_i|W, z_i) $$
Assumindo que o ruído é Gaussiano, podemos modelar a verossimilhança de cada ponto de dado como:
$$ p(x_i|W, z_i) = \mathcal{N}(x_i|Wz_i, \sigma^2I) $$
onde $\sigma^2$ é a variância do ruído [^49].

A **função de custo** para o aprendizado do dicionário é geralmente formulada como a minimização da seguinte expressão:
$$ \mathcal{L}(W, Z) = \sum_{i=1}^N \left( ||x_i - Wz_i||_2^2 + \lambda ||z_i||_1 \right) $$
onde $\lambda$ é um parâmetro de regularização que controla a esparsidade das representações $z_i$ [^49].

A **maximização da verossimilhança** é frequentemente abordada maximizando o logaritmo da verossimilhança (*log-likelihood*):
$$ \log p(D|W) = \sum_{i=1}^N \log p(x_i|W, z_i) $$
Para o modelo Gaussiano, isso se torna:
$$ \log p(D|W) = \sum_{i=1}^N \log \mathcal{N}(x_i|Wz_i, \sigma^2I) = -\frac{1}{2\sigma^2} \sum_{i=1}^N ||x_i - Wz_i||_2^2 + \text{constante} $$
O problema de aprendizado do dicionário é tipicamente resolvido alternando entre duas etapas [^49]:
1.  **Inferência esparsa (Sparse Coding):** Dado o dicionário $W$, encontrar as representações esparsas $Z$ que minimizam a função de custo:
    $$     Z^* = \arg \min_Z \sum_{i=1}^N \left( ||x_i - Wz_i||_2^2 + \lambda ||z_i||_1 \right)     $$
    Este passo pode ser resolvido usando algoritmos como o *LARS* [^442] ou métodos de *coordinate descent* [^441].
2.  **Atualização do dicionário (Dictionary Update):** Dado as representações esparsas $Z$, atualizar o dicionário $W$ para melhor ajustar os dados:
    $$     W^* = \arg \min_W \sum_{i=1}^N ||x_i - Wz_i||_2^2 \quad \text{sujeito a} \quad ||w_j||_2 \leq 1, \forall j     $$
    onde $w_j$ é a *j*-ésima coluna de $W$. Esta etapa pode ser resolvida por otimização quadrática ou *stochastic gradient descent* [^49].

Para evitar que os elementos do dicionário se tornem arbitrariamente grandes, é comum impor a restrição $||w_j||_2 \leq 1$ para cada coluna $w_j$ de $W$ [^470].

### Conclusão
O aprendizado de um dicionário esparso é um processo iterativo que envolve a alternância entre a inferência esparsa e a atualização do dicionário. A maximização da verossimilhança, ou equivalentemente a minimização do erro de reconstrução, é o objetivo central. A escolha de algoritmos eficientes para inferência esparsa e atualização do dicionário, bem como a imposição de restrições apropriadas, são cruciais para o sucesso do aprendizado do dicionário. Esse processo resulta em um dicionário adaptado aos dados, permitindo representações esparsas que são úteis para diversas aplicações em processamento de sinais, visão computacional e aprendizado de máquina [^1, ^49].<!-- END -->