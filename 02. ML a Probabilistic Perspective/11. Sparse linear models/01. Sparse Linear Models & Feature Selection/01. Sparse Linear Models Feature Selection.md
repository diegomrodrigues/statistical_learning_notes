## Sparse Linear Models and Feature Selection
### Introduction
Este capítulo aprofunda o tema dos **modelos lineares esparsos** e sua aplicação na **seleção de características**, um tópico introduzido na Seção 3.5.4 [^1]. Modelos esparsos são particularmente úteis quando o número de dimensões *D* excede o número de casos de treinamento *N*, um cenário conhecido como o problema *small N, large D* [^1]. A esparsidade no vetor de pesos **w** é induzida para oferecer vantagens computacionais e prevenir *overfitting* [^1]. Em continuidade ao conceito apresentado, este capítulo explora modelos esparsos como uma abordagem para a seleção simultânea de conjuntos de variáveis, focando na esparsidade do vetor de pesos **w** [^1].

### Conceitos Fundamentais
A **seleção de características** visa identificar variáveis de entrada com alta informação mútua com a saída [^1]. No entanto, essa abordagem míope pode falhar ao capturar efeitos de interação entre variáveis [^1]. Por exemplo, se $y = xor(x_1, x_2)$, nem $x_1$ nem $x_2$ sozinhos podem prever a resposta, mas juntos eles a preveem perfeitamente [^1].

Modelos esparsos abordam essa limitação selecionando conjuntos de variáveis simultaneamente usando uma abordagem baseada em modelo [^1]. Ao induzir esparsidade no vetor de pesos **w**, esses modelos oferecem vantagens computacionais significativas [^1].

A esparsidade pode ser alcançada em *kernel machines* selecionando um subconjunto de exemplos de treinamento [^1]. Isso reduz o *overfitting* e o custo computacional, uma técnica conhecida como *sparse kernel machine* [^1]. No contexto de *kernel machines*, a seleção de características é equivalente a selecionar um subconjunto dos exemplos de treinamento [^1].

Em processamento de sinais, é comum representar sinais (imagens, fala, etc.) em termos de funções de base *wavelet* [^1]. Encontrar uma representação esparsa nesses casos economiza tempo e espaço [^1].

Um caminho natural para formular o problema de seleção de variáveis é definir $\gamma_j = 1$ se a característica *j* é "relevante" e $\gamma_j = 0$ caso contrário [^1]. O objetivo é computar a posteriori sobre modelos:

$$np(\gamma|D) = \frac{e^{-f(\gamma)}}{\sum_{\gamma\'} e^{-f(\gamma\')}}$$

onde $f(\gamma)$ é a função de custo:

$$nf(\gamma) = -[log p(D|\gamma) + log p(\gamma)]$$

Por exemplo, considere gerar $N = 20$ amostras de um modelo de regressão linear dimensional $D = 10$, $y_i \sim N(w^Tx_i,\sigma^2)$, onde $K = 5$ elementos de **w** são diferentes de zero [^2]. Enumerar todos os $2^{10} = 1024$ modelos e computar $p(\gamma|D)$ para cada um revela que a função objetivo é extremamente "irregular" [^2].

Interpretar a posteriori sobre um grande número de modelos é difícil [^2]. Uma estatística de resumo natural é o modo a posteriori, ou estimativa MAP:

$$gamma = argmax p(\gamma|D) = argmin f(\gamma)$$

No entanto, o modo nem sempre é representativo da massa a posteriori completa [^3]. Uma estatística melhor é o *median model*, computado usando

$$gamma = \{j : p(\gamma_j = 1|D) > 0.5\}$$

Isso requer computar as probabilidades de inclusão marginal a posteriori, $p(\gamma_j = 1|D)$ [^3].

O exemplo acima ilustra o "padrão ouro" para a seleção de variáveis: o problema era suficientemente pequeno (apenas 10 variáveis) que foi possível computar a posteriori completa exatamente [^3]. Claro, a seleção de variáveis é mais útil nos casos onde o número de dimensões é grande [^3]. Como existem $2^D$ modelos possíveis (vetores de bits), será impossível computar a posteriori completa em geral, e mesmo encontrar resumos, como a estimativa MAP ou as probabilidades marginais de inclusão, será intratável [^4]. Portanto, a maior parte deste capítulo se concentra em *algorithmic speedups* [^4].

Um modelo comum usado para seleção de variáveis é o **spike and slab model** [^4]. A posteriori é dada por

$$np(\gamma|D) \propto p(\gamma)p(D|\gamma)$$

É comum usar o seguinte prior no vetor de bits:

$$np(\gamma) = \prod_{j=1}^D Ber(\gamma_j|\pi_0) = \pi_0^{\\|\gamma\\|_0}(1-\pi_0)^{D-\\|\gamma\\|_0}$$

onde $\pi_0$ é a probabilidade de uma característica ser relevante, e $\\|\gamma\\|_0 = \sum_{j=1}^D \gamma_j$ é a pseudo-norma $l_0$, que é o número de elementos diferentes de zero do vetor [^4].

<!-- END -->