## Bayesian Variable Selection in Sparse Linear Models

### Introdução
Como introduzido anteriormente [^1], a seleção de variáveis é uma tarefa crucial em muitos problemas de aprendizado de máquina, especialmente em modelos lineares esparsos, onde o objetivo é identificar o subconjunto mais relevante de características (features) que melhor explicam os dados. A seleção de variáveis é particularmente útil em cenários onde o número de dimensões $D$ é grande em relação ao número de amostras de treinamento $N$, conhecido como o problema de *small N, large D* [^1]. Neste capítulo, exploraremos a seleção de variáveis sob uma perspectiva Bayesiana, que oferece uma abordagem natural e probabilística para este problema [^2].

### Conceitos Fundamentais

A **seleção de variáveis Bayesiana** aborda o problema computando a distribuição *a posteriori* sobre modelos, onde cada modelo representa um subconjunto diferente de características [^2]. A ideia central é definir variáveis indicadoras $\gamma_j$ para a relevância de cada característica $j$, onde $\gamma_j = 1$ se a característica $j$ é considerada "relevante" e $\gamma_j = 0$ caso contrário [^2]. O objetivo é calcular a distribuição *a posteriori* sobre os modelos $p(\gamma|D)$ para determinar o conjunto mais provável de características relevantes [^2].

A distribuição *a posteriori* sobre modelos é calculada equilibrando a verossimilhança (likelihood) dos dados dado o modelo, $p(D|\gamma)$, e a probabilidade *a priori* do modelo, $p(\gamma)$ [^2]:

$$p(\gamma|D) = \frac{p(\gamma)p(D|\gamma)}{\sum_{\gamma\'} p(\gamma\')p(D|\gamma\')}$$

onde $D$ representa os dados observados [^2]. O denominador é uma constante de normalização que garante que a distribuição *a posteriori* some um [^2].

O cálculo da verossimilhança marginal $p(D|\gamma)$ envolve a integração sobre os parâmetros do modelo, dados os dados e a estrutura do modelo especificada por $\gamma$ [^4]:

$$p(D|\gamma) = \int p(y|X, w, \gamma)p(w|\gamma, \sigma^2)p(\sigma^2) dw d\sigma^2$$

onde $y$ são as respostas, $X$ é a matriz de design, $w$ são os pesos do modelo, $\sigma^2$ é a variância do ruído, e $p(\sigma^2)$ é a *a priori* sobre a variância do ruído [^4].

Um desafio significativo na seleção de variáveis Bayesiana é a natureza *"bumpy"* da função objetivo, $f(\gamma)$, que é definida como [^2]:

$$f(\gamma) = -[log\\ p(D|\gamma) + log\\ p(\gamma)]$$

Essa função objetivo pode ter múltiplos máximos locais, tornando difícil encontrar o modelo ótimo usando métodos de otimização tradicionais [^2]. Além disso, interpretar a distribuição *a posteriori* sobre um grande número de modelos (2^D, onde D é o número de características) pode ser computacionalmente impraticável [^2].

Para contornar esses desafios, várias técnicas de sumarização são empregadas [^2]:

1.  **Modo *a posteriori* (MAP estimate):** O modo *a posteriori* (MAP) é o modelo com a maior probabilidade *a posteriori* [^2]:
$$ \gamma_{MAP} = \underset{\gamma}{\operatorname{argmax}} p(\gamma|D) = \underset{\gamma}{\operatorname{argmin}} f(\gamma) $$
2.  **Modelo mediano:** O modelo mediano é o conjunto de características que são incluídas com probabilidade *a posteriori* marginal maior que 0.5 [^3]:
$$\hat{\gamma} = \\{j : P(\gamma_j = 1|D) > 0.5\\}$$
3.  **Probabilidades de inclusão marginal:** As probabilidades de inclusão marginal, $p(\gamma_j = 1|D)$, avaliam a confiança na inclusão de cada variável [^2]. Um limiar mais alto captura menos falsos positivos, mas mais falsos negativos [^2]. Estas probabilidades são calculadas como [^3]:
$$p(\gamma_j=1|D) = \sum_{\gamma: \gamma_j=1} p(\gamma|D)$$

Um modelo popular para seleção de variáveis Bayesiana é o **modelo *spike and slab*** [^4]. Este modelo usa um *a priori* que coloca massa de probabilidade tanto em zero (o "spike") quanto em valores diferentes de zero (o "*slab*") para os coeficientes de regressão. Especificamente, o *a priori* para cada coeficiente $w_j$ é dado por [^4]:

$$p(w_j|\sigma^2, \gamma_j) = \begin{cases}\n\delta_0(w_j) & \text{if } \gamma_j = 0 \\\\nN(w_j|0, \sigma^2\sigma_w^2) & \text{if } \gamma_j = 1\n\end{cases}$$

onde $\delta_0(w_j)$ é a função delta de Dirac em zero, $\sigma^2$ é a variância do ruído, e $\sigma_w^2$ controla o quão grandes esperamos que os coeficientes associados com as variáveis relevantes sejam [^4]. A probabilidade *a priori* para o vetor indicador $\gamma$ é frequentemente definida como [^4]:

$$p(\gamma) = \prod_{j=1}^D Ber(\gamma_j|\pi_0) = \pi_0^{\\|\gamma\\|_0}(1-\pi_0)^{D-\\|\gamma\\|_0}$$

onde $\pi_0$ é a probabilidade de que uma característica seja relevante, e $\\|\gamma\\|_0 = \sum_{j=1}^D \gamma_j$ é a pseudo-norma $l_0$, que representa o número de elementos não-nulos no vetor $\gamma$ [^4].

Outro modelo é o modelo de Bernoulli-Gaussiano, também conhecido como modelo de máscara binária [^5]. Neste modelo, a probabilidade *a priori* conjunta tem a forma [^6]:

$$p(\gamma, w) \propto N(w|0, \sigma^2I)\pi_0^{\\|\gamma\\|_0}(1 - \pi_0)^{D - \\|\gamma\\|_0}$$

onde $\lambda \triangleq 2\sigma^2 log(\frac{1-\pi_0}{\pi_0})$ [^6].

Para aproximar a distribuição *a posteriori*, métodos como MCMC (Markov Chain Monte Carlo) podem ser usados [^9]. Uma alternativa mais eficiente é usar um algoritmo de busca estocástica para gerar um conjunto $S$ de modelos de alta pontuação e, em seguida, aproximar a distribuição *a posteriori* como [^9]:

$$p(\gamma|D) \approx \frac{e^{-f(\gamma)}}{\sum_{\gamma\' \in S} e^{-f(\gamma\')}}$$

### Conclusão
A seleção de variáveis Bayesiana oferece uma abordagem flexível e probabilística para identificar características relevantes em modelos lineares esparsos [^2]. Ao calcular a distribuição *a posteriori* sobre modelos, ela fornece uma medida de incerteza na seleção de variáveis e permite a incorporação de conhecimento *a priori* sobre a estrutura do modelo [^2]. Embora o cálculo da distribuição *a posteriori* completa possa ser computacionalmente desafiador, técnicas de sumarização como estimativas MAP e probabilidades de inclusão marginal fornecem informações valiosas sobre a relevância de cada característica [^2]. Como mencionado anteriormente [^1], a seleção de variáveis é uma etapa crucial na construção de modelos esparsos e na obtenção de *insights* científicos a partir de dados de alta dimensão.

### Referências
[^1]: Capítulo 13, Seção 13.1
[^2]: Capítulo 13, Seção 13.2
[^3]: Capítulo 13, Seção 13.2
[^4]: Capítulo 13, Seção 13.2.1
[^5]: Capítulo 13, Seção 13.2.2
[^6]: Capítulo 13, Seção 13.2.2
[^9]: Capítulo 13, Seção 13.2.3.2
<!-- END -->