## Aproximando a Distribuição Preditiva Posterior na Regressão Logística Bayesiana

### Introdução
Na regressão logística Bayesiana, o objetivo é computar a distribuição posterior completa sobre os parâmetros, denotada por $p(w|D)$ [^254]. Essa distribuição é crucial para associar intervalos de confiança às predições, especialmente em cenários como os problemas de contextual bandit mencionados na Seção 5.7.3.1 [^254]. No entanto, ao contrário da regressão linear, a regressão logística não possui um prior conjugado conveniente, o que impede a obtenção exata da distribuição posterior [^254]. Este capítulo explora métodos de aproximação para a distribuição preditiva posterior, $p(y|x, D)$, que é obtida integrando sobre a distribuição posterior. As aproximações incluem a aproximação plug-in e a aproximação de Monte Carlo.

### Conceitos Fundamentais
A distribuição preditiva posterior é dada por:
$$ p(y|x, D) = \int p(y|x, w)p(w|D)dw $$
onde $p(y|x, w)$ é a probabilidade do rótulo $y$ dado a entrada $x$ e os parâmetros $w$, e $p(w|D)$ é a distribuição posterior dos parâmetros $w$ dado o conjunto de dados $D$. Essa integral geralmente é intratável, exigindo o uso de métodos de aproximação [^257].

**Aproximação Plug-in:**
A aproximação mais simples é a **aproximação plug-in**, onde a distribuição posterior $p(w|D)$ é aproximada por um único valor, tipicamente a média posterior $E[w]$ [^257]. A distribuição preditiva posterior é então aproximada por [^257]:
$$ p(y = 1|x, D) \approx p(y = 1|x, E[w]) $$
Nesse contexto, $E[w]$ é chamado de **ponto de Bayes** [^257]. Embora simples, essa aproximação tende a subestimar a incerteza nas predições [^257].

**Aproximação de Monte Carlo:**
Uma abordagem mais precisa é a **aproximação de Monte Carlo (MC)**, que envolve a amostragem de múltiplos valores dos parâmetros $w$ da distribuição posterior $p(w|D)$ [^258]. A distribuição preditiva posterior é então aproximada pela média das predições obtidas com cada amostra [^258]:
$$ p(y = 1|x, D) \approx \frac{1}{S} \sum_{s=1}^{S} \text{sigm}((w^s)^T x) $$
onde $w^s \sim p(w|D)$ são amostras da distribuição posterior e $\text{sigm}(z) = \frac{1}{1 + e^{-z}}$ é a função sigmoide [^258]. Essa técnica pode ser trivialmente estendida para o caso multiclasse [^258].

Se a distribuição posterior foi aproximada usando o método de Monte Carlo, as amostras já podem ser reutilizadas para a predição [^258]. Se uma aproximação Gaussiana foi feita para a posterior, podemos gerar amostras independentes da Gaussiana usando métodos padrão [^258].

**Aproximação Probit (Saída Moderada):**
Se temos uma aproximação Gaussiana para a posterior $p(w|D) \approx N(w|m_v, V_n)$, também podemos computar uma aproximação determinística para a distribuição preditiva posterior [^259]. No caso binário, procedemos da seguinte forma:

$$ p(y=1|x, D) \approx \int \text{sigm}(w^Tx)p(w|D)dw = \int \text{sigm}(\alpha)N(\alpha|\mu_{\alpha},\sigma_{\alpha}^2)d\alpha \quad [^259] $$

Onde

$$ \mu_{\alpha} \triangleq E[\alpha] = m_v^T x  \quad [^259] $$

$$ \sigma_{\alpha}^2 \triangleq \text{var}[\alpha] = \int p(\alpha|D)[\alpha^2 - E[\alpha^2]]d\alpha = \int p(w|D)[(w^Tx)^2 - (m_v^Tx)^2]dw = x^TV_n x \quad [^259] $$

Assim, precisamos avaliar a expectativa da sigmoide com respeito a uma Gaussiana [^260]. Isso pode ser aproximado explorando o fato de que a função sigmoide é similar à função probit, que é dada pela cdf da normal padrão [^260]:

$$ \Phi(a) = \int_{-\infty}^a N(x|0,1) dx \quad [^260] $$

A vantagem de usar a probit é que podemos convoluí-la com uma Gaussiana analiticamente [^260]:

$$ \int \Phi(\lambda \alpha)N(\alpha|\mu, \sigma^2)d\alpha = \Phi \left( \frac{\lambda \mu}{\sqrt{\lambda^2 \sigma^2 + 1}} \right)  \quad [^260] $$

Aplicando isso ao modelo de regressão logística, obtemos a seguinte expressão [^260]:

$$ p(y=1|x, D) \approx \text{sigm}(\kappa(\sigma_{\alpha}^2)\mu_{\alpha})  \quad [^260] $$

Onde $\kappa(\sigma^2) = (1 + \pi \sigma^2/8)^{-1/2}$ [^260]. Usar a Equação 8.71 é algumas vezes chamado de uma *saída moderada*, já que é menos extrema que a estimativa plug-in [^260].

### Conclusão
A aproximação da distribuição preditiva posterior na regressão logística Bayesiana é um passo crucial para obter predições com intervalos de confiança [^254]. Embora a aproximação plug-in seja simples, ela subestima a incerteza [^257]. As aproximações de Monte Carlo e probit oferecem alternativas mais precisas, capturando melhor a incerteza inerente ao modelo [^258, 259]. A escolha do método de aproximação depende do equilíbrio entre precisão e custo computacional, bem como das características específicas do problema em questão [^258].

### Referências
[^254]: Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
[^257]: Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
[^258]: Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
[^259]: Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
[^260]: Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
<!-- END -->