## Gaussian Approximation for Bayesian Logistic Regression

### Introdução
Em Bayesian Logistic Regression, o objetivo é computar a distribuição *a posteriori* completa sobre os parâmetros, $p(w|D)$ [^48]. Diferentemente da regressão linear, a Logistic Regression não possui um *prior* conjugado conveniente, tornando a computação exata da *a posteriori* inviável [^48]. Portanto, recorremos a métodos de aproximação. Este capítulo foca na aproximação Gaussiana, detalhando sua aplicação e limitações no contexto da Logistic Regression Bayesiana.

### Conceitos Fundamentais

A **aproximação Gaussiana** é um termo geral que se refere ao uso de uma distribuição Gaussiana para aproximar a *a posteriori* [^11]. A **aproximação de Laplace** é um método mais sofisticado dentro da comunidade estatística [^11].

Em essência, aproximamos a *a posteriori* $p(\theta|D)$ por uma Gaussiana centrada no modo da *a posteriori*, $\theta^*$, com uma matriz de covariância dada pela inversa do Hessiano da função de energia (negativo do log da *a posteriori* não normalizado) avaliado no modo [^50, 52]:

$$ p(\theta|D) \approx N(\theta|\theta^*, H^{-1}) $$

onde:
*   $\theta$ representa os parâmetros do modelo.
*   $D$ representa os dados observados.
*   $E(\theta) = -\log p(\theta, D)$ é a função de energia, igual ao negativo do log da *a posteriori* não normalizado [^49].
*   $Z = p(D)$ é a constante de normalização [^49].
*   $g = \nabla E(\theta)|_{\theta^*}$ é o gradiente da função de energia avaliado no modo [^50].
*   $H = \nabla^2 E(\theta)|_{\theta^*}$ é o Hessiano da função de energia avaliado no modo [^50].

O procedimento envolve os seguintes passos:

1.  **Identificação do Modo:** Encontrar o modo $\theta^*$ da distribuição *a posteriori*, que corresponde ao ponto de mínimo da função de energia $E(\theta)$ [^50]. Isto pode ser feito usando métodos de otimização, como o gradiente descendente ou o método de Newton (abordados na Seção 8.3 [^2]).
2.  **Expansão de Taylor:** Realizar uma expansão em série de Taylor de segunda ordem da função de energia $E(\theta)$ em torno do modo $\theta^*$ [^50]:

$$ E(\theta) \approx E(\theta^*) + (\theta - \theta^*)^T g + \frac{1}{2} (\theta - \theta^*)^T H (\theta - \theta^*) $$

    Como $\theta^*$ é o modo, o gradiente $g$ é zero [^50, 51].
3.  **Aproximação Gaussiana:** Substituir a expansão de Taylor na expressão da *a posteriori* e normalizar para obter uma Gaussiana [^52, 53]:

$$ p(\theta|D) \approx \frac{1}{Z} e^{-E(\theta^*)} \exp \left[ -\frac{1}{2} (\theta - \theta^*)^T H (\theta - \theta^*) \right] = N(\theta|\theta^*, H^{-1}) $$

    onde $Z = p(D) \approx e^{-E(\theta^*)} (2\pi)^{D/2} |H|^{-\frac{1}{2}}$ [^54].

A aproximação Gaussiana torna-se mais razoável à medida que o tamanho da amostra aumenta, pois as *a posteriori* tendem a se tornar mais Gaussianas, um fenômeno análogo ao Teorema do Limite Central [^11]. Em física, existe uma técnica similar conhecida como aproximação do ponto de sela [^11].

**Derivação do BIC:**
A aproximação Gaussiana pode ser usada para escrever a *log marginal likelihood* da seguinte forma [^55]:
$$ \log p(D) \approx \log p(D|\theta^*) + \log p(\theta^*) - \frac{1}{2} \log |H| $$
Os termos de penalização adicionados a $\log p(D|\theta^*)$ são chamados de **Occam factor**, e são uma medida da complexidade do modelo [^55]. Se tivermos um *prior* uniforme, $p(\theta) \propto 1$, podemos descartar o segundo termo e substituir $\theta^*$ pelo MLE, $\hat{\theta}$ [^55].

**Aplicação à Logistic Regression:**
Para aplicar a aproximação Gaussiana à Logistic Regression, utilizamos um *prior* Gaussiano da forma $p(w) = N(w|0, V_0)$ [^58], como na estimação MAP. A *a posteriori* aproximada é dada por [^58]:

$$ p(w|D) \approx N(w|\hat{w}, H^{-1}) $$

onde $\hat{w} = \arg \min_w E(w)$ é o minimizador da função de energia $E(w) = -(\log p(D|w) + \log p(w))$ [^58], e $H = \nabla^2 E(w)|_{\hat{w}}$ é o Hessiano avaliado em $\hat{w}$ [^58].

**Limitações:**
A aproximação Gaussiana pode ser limitada, especialmente quando a *a posteriori* é significativamente não Gaussiana. Por exemplo, em casos onde os dados são linearmente separáveis, a superfície de *likelihood* pode ser ilimitada, levando a uma *a posteriori* altamente assimétrica [^58]. Nesses casos, a aproximação Gaussiana, por ser simétrica, pode não capturar adequadamente a incerteza nos parâmetros [^58]. Apesar de ser uma aproximação grosseira, ainda é melhor do que aproximar a *a posteriori* por uma função delta, como na estimação MAP [^58].

### Conclusão

A aproximação Gaussiana oferece uma maneira tratável de aproximar a *a posteriori* em Bayesian Logistic Regression, permitindo a computação de intervalos de credibilidade e a incorporação de incerteza nas predições [^48, 59]. Embora possua limitações, especialmente em casos de *a posteriori* não Gaussianas, representa uma melhoria em relação a abordagens pontuais como a estimação MAP [^58]. Métodos mais sofisticados, como MCMC ou inferência variacional, podem ser empregados para obter aproximações mais precisas, mas à custa de maior complexidade computacional [^48].

### Referências

[^11]: Gaussian approximation is a general term referring to the use of a Gaussian distribution to approximate the posterior. The term Laplace approximation is more sophisticated and refers to a specific method in the statistics community. A Gaussian approximation is often reasonable, as posteriors become more Gaussian-like as the sample size increases.
[^48]: It is natural to want to compute the full posterior over the parameters, p(w|D), for logistic regression models. This can be useful for any situation where we want to associate confidence intervals with our predictions (e.g., this is necessary when solving contextual bandit problems, discussed in Section 5.7.3.1). Unfortunately, unlike the linear regression case, this cannot be done exactly, since there is no convenient conjugate prior for logistic regression. We discuss one simple approximation below; some other approaches include MCMC (Section 24.3.3.1), variational inference (Section 21.8.1.1), expectation propagation (Kuss and Rasmussen 2005), etc. For notational simplicity, we stick to binary logistic regression.
[^49]: In this section, we discuss how to make a Gaussian approximation to a posterior distribution. The approximation works as follows. Suppose θ ∈ RD. Let p(0|D) = 1/Z e-E(0) where E(0) is called an energy function, and is equal to the negative log of the unnormalized log posterior, E(0) = -logp(0, D), with Z = p(D) being the normalization constant.
[^50]: Performing a Taylor series expansion around the mode 0* (i.e., the lowest energy state) we get E(0) ≈ E(0*) + (0 – 0*)7g + 1/2 (0 – 0*)H(0 – 0*) where g is the gradient and H is the Hessian of the energy function evaluated at the mode:
[^51]: g ∇E(0)|, H 02E(0) Since 0* is the mode, the gradient term is zero.
[^52]: Hence p(0|D) ≈ e-E(0*) exp [-(0-0)H(0-0)]/Z = N(0|0*, H−1)
[^53]: Z = p(D) ≈ ∫ p(0|D)d0 = e-E(0*) (2π)D/2|H|¯½ = Ν(0|0*, Η−1)
[^54]: The last line follows from normalization constant of the multivariate Gaussian. Equation 8.54 is known as the Laplace approximation to the marginal likelihood. Therefore Equation 8.52 is sometimes called the the Laplace approximation to the posterior. However, in the statistics community, the term “Laplace approximation” refers to a more sophisticated method (see e.g. (Rue et al. 2009) for details). It may therefore be better to use the term "Gaussian approximation” to refer to Equation 8.52.
[^55]: We can use the Gaussian approximation to write the log marginal likelihood as follows, dropping irrelevant constants: log p(D) ≈ log p(D|0*) + log p(0*) -1/2 log |H| The penalization terms which are added to the log p(D|0*) are sometimes called the Occam factor, and are a measure of model complexity. If we have a uniform prior, p(0) x 1, we can drop the second term, and replace 0* with the MLE, θ.
[^58]: Now let us apply the Gaussian approximation to logistic regression. We will use a a Gaussian prior of the form p(w) = N(w|0, Vo), just as we did in MAP estimation. The approximate posterior is given by p(w|D) ≈ N(w|ŵ, H¯¹) where w = arg minw E(w), E(w) = -(log p(D|w) + logp(w)), and H = ∇2E(w)|w. As an example, consider the linearly separable 2D data in Figure 8.5(a). There are many parameter settings that correspond to lines that perfectly separate the training data; we show 4 examples. The likelihood surface is shown in Figure 8.5(b), where we see that the likelihood is unbounded as we move up and to the right in parameter space, along a ridge where w2/w₁ = 2.35 (this is indicated by the diagonal line). The reasons for this is that we can maximize the likelihood by driving ||w|| to infinity (subject to being on this line), since large regression weights make the sigmoid function very steep, turning it into a step function. Consequently the MLE is not well defined when the data is linearly separable. To regularize the problem, let us use a vague spherical prior centered at the origin, N(w|0, 100I). Multiplying this spherical prior by the likelihood surface results in a highly skewed posterior, shown in Figure 8.5(c). (The posterior is skewed because the likelihood function “chops off" regions of parameter space (in a “soft” fashion) which disagree with the data.) The MAP estimate is shown by the blue dot. Unlike the MLE, this is not at infinity. The Gaussian approximation to this posterior is shown in Figure 8.5(d). We see that this is a symmetric distribution, and therefore not a great approximation. Of course, it gets the mode correct (by construction), and it at least represents the fact that there is more uncertainty along the southwest-northeast direction (which corresponds to uncertainty about the orientation of separating lines) than perpendicular to this. Although a crude approximation, this is surely better than approximating the posterior by a delta function, which is what MAP estimation does.
[^59]: Given the posterior, we can compute credible intervals, perform hypothesis tests, etc., just as we did in Section 7.6.3.3 in the case of linear regression. But in machine learning, interest usually focusses on prediction. The posterior predictive distribution has the form p(yx, D) = ∫ p(y|x, w)p(w|D)dw

<!-- END -->