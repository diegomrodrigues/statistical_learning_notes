## Probit Approximation (Moderated Output) in Bayesian Logistic Regression

### Introdução

No contexto da regressão logística Bayesiana, o cálculo da distribuição preditiva posterior $p(y|x, \mathcal{D})$ frequentemente envolve integrais intratáveis [^59]. A solução mais simples, a *plug-in approximation*, subestima a incerteza [^57]. Este capítulo explora uma alternativa determinística conhecida como **probit approximation (moderated output)**, que se baseia em uma aproximação Gaussiana da distribuição posterior e explora a similaridade entre as funções sigmoide e probit [^259]. Esta aproximação resulta em uma previsão menos extrema em comparação com a estimativa *plug-in* [^259].

### Conceitos Fundamentais

A **probit approximation** oferece uma maneira de obter uma estimativa determinística da distribuição preditiva posterior $p(y|x, \mathcal{D})$ quando temos uma aproximação Gaussiana para a distribuição posterior $p(w|\mathcal{D}) \approx \mathcal{N}(w|m_v, V_n)$ [^259]. O ponto de partida é a integral [^259]:

$$ p(y=1|x, \mathcal{D}) = \int \text{sigm}(w^T x) p(w|\mathcal{D}) dw $$

onde $\text{sigm}(w^T x)$ é a função sigmoide e $p(w|\mathcal{D})$ é a distribuição posterior sobre os pesos $w$ dado o conjunto de dados $\mathcal{D}$. Essa integral é geralmente intratável, mas pode ser aproximada explorando a semelhança entre a função sigmoide e a função probit [^259].

A função **probit** $\Phi(a)$ é definida como a função de distribuição cumulativa (CDF) da distribuição normal padrão [^260]:

$$ \Phi(a) = \int_{-\infty}^{a} \mathcal{N}(x|0, 1) dx $$

A chave da aproximação é a observação de que as funções sigmoide e probit têm formas semelhantes [^259, 260]. Podemos aproximar a função sigmoide usando a função probit escalonada [^260]:

$$ \text{sigm}(a) \approx \Phi(\lambda a) $$

onde $\lambda$ é um fator de escala escolhido de forma que as derivadas das duas funções correspondam na origem, $\lambda^2 = \pi/8$ [^260]. A vantagem de usar a função probit é que a convolução de uma probit com uma Gaussiana pode ser avaliada analiticamente [^260]:

$$ \int \Phi(\lambda a) \mathcal{N}(a|\mu, \sigma^2) da = \Phi\left(\frac{\mu}{\sqrt{\lambda^{-2} + \sigma^2}}\right) $$

Assim, podemos aproximar a distribuição preditiva posterior como [^260]:

$$ p(y=1|x, \mathcal{D}) \approx \text{sigm}(\kappa(\sigma_a^2) \mu_a) $$

onde:

*   $\mu_a = m_v^T x$ é o valor esperado de $a = w^T x$ [^259].
*   $\sigma_a^2 = x^T V_n x$ é a variância de $a = w^T x$ [^259].
*   $\kappa(\sigma_a^2) = (1 + \pi\sigma_a^2/8)^{-1/2}$ é um fator de moderação [^260].

O termo $\kappa(\sigma_a^2)$ modera a saída, tornando a previsão menos extrema do que a estimativa *plug-in* [^260]. Isso ocorre porque $0 \leq \kappa(\sigma_a^2) \leq 1$, e, portanto, $\text{sigm}(\kappa(\sigma_a^2) \mu_a) \leq \text{sigm}(\mu_a)$ [^260]. Se $\mu_a > 0$, então $p(y=1|x, \hat{w}) > 0.5$, mas a predição moderada está sempre mais próxima de 0.5, tornando-a menos confiante [^260].

É importante notar que, embora a aproximação suavize a saída, a fronteira de decisão permanece a mesma da estimativa *plug-in*. A fronteira de decisão ocorre quando $p(y=1|x, \mathcal{D}) = 0.5$, o que implica $\mu_a = \hat{w}^T x = 0$ [^260].

### Conclusão

A probit approximation fornece uma alternativa determinística à distribuição preditiva posterior na regressão logística Bayesiana. Ao explorar a similaridade entre as funções sigmoide e probit, essa aproximação permite a avaliação analítica da integral, resultando em previsões menos extremas em comparação com a estimativa *plug-in*. Embora a fronteira de decisão permaneça inalterada, a saída moderada reflete a incerteza na estimativa dos parâmetros, tornando-a uma abordagem mais robusta.

### Referências

[^259]: (Machine Learning: A Probabilistic Perspective, Murphy, Kevin P., 2012)
[^260]: (Machine Learning: A Probabilistic Perspective, Murphy, Kevin P., 2012)
<!-- END -->