## Bayesian Logistic Regression: Posterior Computation and Approximation

### Introdução
O presente capítulo explora a **Bayesian logistic regression**, uma abordagem que computa a distribuição *a posteriori* completa dos parâmetros, $p(w|D)$, para modelos de regressão logística [^254]. Ao contrário das abordagens de estimação pontual, como a Maximum Likelihood Estimation (MLE), a Bayesian logistic regression permite associar **intervalos de confiança** com as predições e é particularmente útil em situações onde a **quantificação da incerteza** é crucial, como em problemas de *contextual bandit* [^254].

### Conceitos Fundamentais
#### Necessidade da Distribuição *a posteriori*
Em muitos cenários práticos, conhecer apenas uma estimativa pontual dos parâmetros do modelo (e.g., o estimador MLE) é insuficiente. A capacidade de quantificar a incerteza associada a essas estimativas é fundamental para a tomada de decisões robustas, especialmente quando as consequências de uma predição incorreta são significativas [^254]. A Bayesian logistic regression fornece uma estrutura para obter essa quantificação da incerteza ao computar a distribuição *a posteriori* completa dos parâmetros.

#### Desafios na Computação da Distribuição *a posteriori*
Ao contrário da regressão linear, a regressão logística não possui uma *prior conjugada* conveniente. Isso significa que a distribuição *a posteriori* não pode ser obtida analiticamente na maioria dos casos [^254]. Portanto, é necessário recorrer a métodos aproximados para estimar $p(w|D)$. As aproximações discutidas incluem:
*   **Aproximação de Laplace**: Uma aproximação Gaussiana para a *a posteriori* [^255].
*   **Métodos de Monte Carlo via Cadeias de Markov (MCMC)**: Métodos computacionalmente intensivos que amostram da *a posteriori* [^254].
*   **Inferência Variacional**: Uma abordagem que aproxima a *a posteriori* por uma distribuição mais simples [^254].
*   **Expectation Propagation (EP)**: Outra técnica de inferência aproximada [^254].

#### Aproximação de Laplace
A **aproximação de Laplace** é um método para aproximar uma distribuição *a posteriori* por uma Gaussiana [^255]. O método envolve expandir a log *a posteriori* em uma série de Taylor em torno do seu máximo (o modo *a posteriori*, $\theta^*$) e truncar a expansão no termo quadrático [^255].

Seja $p(\theta|D) = \frac{1}{Z}e^{-E(\theta)}$ [^255], onde $E(\theta)$ é a **função energia** e $Z = p(D)$ é a constante de normalização. Expandindo $E(\theta)$ em torno de $\theta^*$:\n$$E(\theta) \approx E(\theta^*) + (\theta - \theta^*)^T g + \frac{1}{2}(\theta - \theta^*)^T H (\theta - \theta^*)$$,

onde $g = \nabla E(\theta)|_{\theta^*}$ é o gradiente e $H = \nabla^2 E(\theta)|_{\theta^*}$ é a Hessiana da função energia avaliada no modo [^255]. Como $\theta^*$ é o modo, o termo do gradiente é zero [^255]. Assim, a *a posteriori* aproximada é dada por:

$$p(\theta|D) \approx \frac{1}{Z} e^{-E(\theta^*)} \exp \left[ -\frac{1}{2} (\theta - \theta^*)^T H (\theta - \theta^*) \right] = \mathcal{N}(\theta | \theta^*, H^{-1})$$

A constante de normalização $Z$ pode ser aproximada usando a integral da Gaussiana:

$$Z = p(D) \approx \int p(\theta|D) d\theta = e^{-E(\theta^*)} (2\pi)^{D/2} |H|^{-\frac{1}{2}}$$,

onde $D$ é a dimensão de $\theta$ [^255]. Esta aproximação é conhecida como a **aproximação de Laplace** para a *likelihood marginal* [^255].

#### Bayesian Information Criterion (BIC)
Usando a aproximação Gaussiana, podemos escrever a *log likelihood marginal* como [^255]:

$$log p(D) \approx \log p(D|\theta^*) + \log p(\theta^*) - \frac{1}{2} \log |H|$$

Os termos de penalização adicionados a $\log p(D|\theta^*)$ são chamados de **Occam factor** e medem a complexidade do modelo [^255]. Se temos uma *prior* uniforme, $p(\theta) \propto 1$, podemos descartar o segundo termo e substituir $\theta^*$ pelo estimador de máxima verossimilhança (MLE), $\hat{\theta}$ [^255].

#### Aproximação de Laplace para Regressão Logística
Aplicando a aproximação de Laplace à regressão logística, utilizamos uma *prior* Gaussiana da forma $p(w) = \mathcal{N}(w|0, V_0)$ [^256], como na estimação MAP. A *a posteriori* aproximada é dada por:

$$p(w|D) \approx \mathcal{N}(w|\hat{w}, H^{-1})$$

onde $\hat{w} = \arg \min_w E(w)$, $E(w) = -(\log p(D|w) + \log p(w))$, e $H = \nabla^2 E(w)|_{\hat{w}}$ [^256].

#### Aproximação Preditiva *a posteriori*
Dada a distribuição *a posteriori*, podemos computar *intervalos críveis* e realizar testes de hipóteses [^256]. No entanto, em *machine learning*, o interesse geralmente se concentra na predição. A distribuição preditiva *a posteriori* é dada por [^256]:

$$p(y|x, D) = \int p(y|x, w) p(w|D) dw$$

Esta integral é intratável [^257]. A **aproximação *plug-in*** é a aproximação mais simples, que no caso binário, toma a forma [^257]:

$$p(y=1|x, D) \approx p(y=1|x, \mathbb{E}[w])$$

onde $\mathbb{E}[w]$ é a média *a posteriori*. Neste contexto, $\mathbb{E}[w]$ é chamado de **ponto de Bayes** [^257]. No entanto, essa aproximação subestima a incerteza [^257].

#### Aproximação de Monte Carlo
Uma abordagem melhor é usar uma **aproximação de Monte Carlo**:

$$p(y=1|x, D) \approx \frac{1}{S} \sum_{s=1}^S \sigma((w^s)^T x)$$

onde $w^s \sim p(w|D)$ são amostras da *a posteriori* [^258]. Se aproximamos a *a posteriori* usando uma Gaussiana, podemos gerar amostras independentes da Gaussiana usando métodos padrão [^258].

#### Aproximação Probit (Saída Moderada)
Se tivermos uma aproximação Gaussiana para a *a posteriori* $p(w|D) \approx \mathcal{N}(w|m_v, V_v)$, podemos computar uma aproximação determinística para a distribuição preditiva *a posteriori*, pelo menos no caso binário [^259].

$$p(y=1|x, D) = \int \sigma(w^T x) p(w|D) dw = \int \sigma(a) \mathcal{N}(a|\mu_a, \sigma_a^2) da$$

onde $a = w^T x$, $\mu_a = \mathbb{E}[a] = m_v^T x$ e $\sigma_a^2 = \text{var}[a] = x^T V_v x$ [^259].

Explorando a similaridade entre a função sigmoide e a função *probit* (cdf da normal padrão), $\Phi(a) = \int_{-\infty}^a \mathcal{N}(x|0, 1) dx$ [^260], podemos aproximar a integral acima analiticamente:

$$int \Phi(\lambda a) \mathcal{N}(a|\mu, \sigma^2) da = \Phi\left( \frac{\lambda \mu}{\sqrt{\lambda^2 \sigma^2 + 1}} \right)$$

onde $\lambda^2 = \pi/8$ [^260]. Assim, obtemos:

$$p(y=1|x, D) \approx \sigma(\kappa(\sigma_a^2) \mu_a)$$

onde $\kappa(\sigma_a^2) = (1 + \pi \sigma_a^2 / 8)^{-\frac{1}{2}}$ [^260]. Isso é chamado de uma **saída moderada**, pois é menos extrema que a estimativa *plug-in* [^260].

### Conclusão
A Bayesian logistic regression oferece uma abordagem poderosa para a modelagem de classificação, permitindo a quantificação da incerteza associada às predições. Embora a computação exata da distribuição *a posteriori* seja geralmente intratável, várias técnicas aproximadas, como a aproximação de Laplace, métodos de Monte Carlo e aproximações *probit*, fornecem meios eficazes para obter estimativas precisas e calibradas da incerteza. Essas técnicas são particularmente valiosas em aplicações onde a tomada de decisão informada requer uma compreensão completa das incertezas do modelo.

### Referências
[^254]: Page 254, Chapter 8. Logistic regression
[^255]: Page 255, Chapter 8. Logistic regression
[^256]: Page 256, Chapter 8. Logistic regression
[^257]: Page 257, Chapter 8. Logistic regression
[^258]: Page 258, Chapter 8. Logistic regression
[^259]: Page 259, Chapter 8. Logistic regression
[^260]: Page 260, Chapter 8. Logistic regression
<!-- END -->