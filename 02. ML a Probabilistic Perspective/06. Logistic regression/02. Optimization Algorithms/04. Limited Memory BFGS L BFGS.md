## Limited Memory BFGS (L-BFGS)

### Introdução
O algoritmo BFGS, um método quasi-Newton, oferece uma alternativa eficiente ao método de Newton para otimização, aproximando a matriz Hessiana [^246, 249]. No entanto, o BFGS padrão requer o armazenamento da matriz Hessiana completa, o que se torna inviável para problemas de grande escala [^252]. Para contornar essa limitação, surge o Limited Memory BFGS (L-BFGS), uma variante que aproxima a Hessiana usando uma matriz diagonal mais uma matriz de baixo posto [^252]. Este capítulo explora o L-BFGS em detalhes, abordando sua formulação, implementação e aplicabilidade.

### Conceitos Fundamentais
O L-BFGS se distingue do BFGS tradicional pela forma como a Hessiana (ou sua inversa) é aproximada. Em vez de armazenar uma matriz densa, o L-BFGS armazena apenas um número limitado de vetores que representam as correções de baixo posto à Hessiana [^252]. Essa abordagem reduz drasticamente os requisitos de memória, tornando o L-BFGS adequado para problemas com um grande número de variáveis.

**Aproximação da Hessiana:**
No L-BFGS, a aproximação da Hessiana inversa, $H_k$, não é armazenada explicitamente. Em vez disso, ela é representada implicitamente por meio de um conjunto de *m* vetores (pares)  $(s_k, y_k)$, onde:

*   $s_k = \theta_{k+1} - \theta_k$ representa a diferença entre os pontos em iterações sucessivas.
*   $y_k = g_{k+1} - g_k$ representa a diferença entre os gradientes nas mesmas iterações.

Aqui $\theta_k$ representa os parâmetros no passo *k* e $g_k$ representa o gradiente da função objetivo em $\theta_k$ [^247].

A atualização da Hessiana inversa no BFGS é dada por [^251]:
$$ B_{k+1} = B_k + \frac{y_k y_k^T}{y_k^T s_k} - \frac{(B_k s_k)(B_k s_k)^T}{s_k^T B_k s_k} $$
e sua inversa $C_k \approx H_k$ é dada por [^252]:
$$ C_{k+1} = \left(I - \frac{s_k y_k^T}{y_k^T s_k}\right) C_k \left(I - \frac{y_k s_k^T}{y_k^T s_k}\right) + \frac{s_k s_k^T}{y_k^T s_k} $$

No L-BFGS, em vez de aplicar essa atualização explicitamente, o algoritmo armazena os vetores $s_k$ e $y_k$ para as *m* iterações mais recentes. Para calcular o produto $H_k g_k$, o L-BFGS realiza uma série de multiplicações vetor-vetor usando esses vetores armazenados. Este processo pode ser implementado de forma eficiente usando um algoritmo de duas loops (two-loop recursion), que evita a necessidade de armazenar a matriz Hessiana completa.

**Algoritmo de Duas Loops:**
O algoritmo de duas loops (two-loop recursion) é o núcleo da implementação do L-BFGS. Ele permite calcular o produto $H_k g_k$ sem armazenar explicitamente $H_k$. O algoritmo funciona da seguinte maneira:

1.  **Loop 1 (Backward):** Itera sobre os *m* vetores armazenados $(s_i, y_i)$ em ordem reversa (do mais recente para o mais antigo), calculando coeficientes $\alpha_i$ como:
    $$     \alpha_i = \frac{s_i^T q}{y_i^T s_i}     $$
    onde *q* é inicializado com o gradiente $g_k$. Em cada iteração, *q* é atualizado como:
    $$     q = q - \alpha_i y_i     $$
2.  **Multiplicação pela Matriz Diagonal:** Após o primeiro loop, *q* é multiplicado por uma matriz diagonal $H_0$, que representa uma aproximação inicial da Hessiana inversa. Uma escolha comum para $H_0$ é um múltiplo da matriz identidade, escalonado de forma a aproximar a escala da Hessiana.
3.  **Loop 2 (Forward):** Itera sobre os *m* vetores armazenados $(s_i, y_i)$ na ordem original (do mais antigo para o mais recente), calculando coeficientes $\beta_i$ como:
    $$     \beta_i = \frac{y_i^T q}{y_i^T s_i}     $$
    Em cada iteração, *q* é atualizado como:
    $$     q = q + ( \alpha_i - \beta_i) s_i     $$

Após o segundo loop, o vetor *q* contém o produto aproximado $H_k g_k$. Este vetor é então usado para determinar a direção de busca na iteração atual.

**Seleção do Tamanho do Passo (Line Search):**
Assim como em outros métodos de otimização, o L-BFGS requer uma busca linear (line search) para determinar o tamanho do passo $\eta_k$ ao longo da direção de busca $d_k = -H_k g_k$ [^247, 248]. A busca linear visa encontrar um valor de $\eta_k$ que resulte em uma redução suficiente na função objetivo, ao mesmo tempo em que satisfaz outras condições, como as condições de Wolfe ou de Armijo [^248].

**Vantagens e Desvantagens:**

*   **Vantagens:**
    *   Requisitos de memória reduzidos em comparação com o BFGS padrão, tornando-o adequado para problemas de grande escala [^252].
    *   Convergência rápida em muitos problemas práticos.
*   **Desvantagens:**
    *   Ainda requer o armazenamento de *m* vetores, o que pode ser limitante para problemas extremamente grandes.
    *   A escolha do parâmetro *m* pode afetar o desempenho do algoritmo.
    *   A aproximação da Hessiana pode ser menos precisa do que a do BFGS padrão, o que pode levar a uma convergência mais lenta em alguns casos.

### Conclusão
O L-BFGS é um algoritmo de otimização poderoso e amplamente utilizado, especialmente adequado para problemas de grande escala onde o armazenamento da Hessiana completa é inviável [^252]. Sua capacidade de aproximar a Hessiana usando uma matriz diagonal mais uma matriz de baixo posto, juntamente com o eficiente algoritmo de duas loops, torna-o uma escolha popular em diversas aplicações, incluindo machine learning, visão computacional e processamento de linguagem natural. No entanto, a escolha adequada do parâmetro *m* e a implementação eficiente da busca linear são cruciais para garantir o bom desempenho do algoritmo.

### Referências
[^246]: Seção 8.1, "Logistic regression"
[^247]: Seção 8.3.2, "Steepest descent"
[^248]: Seção 8.3.2, "Steepest descent", página 248
[^249]: Seção 8.3.3, "Newton\'s method"
[^251]: Seção 8.3.5, "Quasi-Newton (variable metric) methods"
[^252]: Seção 8.3.6, "l2 regularization"

<!-- END -->