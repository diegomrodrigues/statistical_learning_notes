## Quasi-Newton Methods for Optimization

### Introdução
Newton's method, um algoritmo de otimização de segunda ordem, utiliza a informação da **matriz Hessiana** para encontrar o mínimo de uma função [^249]. No entanto, o cálculo da Hessiana pode ser computacionalmente caro, especialmente para problemas de alta dimensão [^251]. Os **métodos Quasi-Newton** representam uma alternativa eficiente, construindo iterativamente uma aproximação da Hessiana ou sua inversa usando informações do gradiente obtidas em cada passo [^251]. Este capítulo explora os métodos Quasi-Newton, com foco no popular algoritmo BFGS (Broyden-Fletcher-Goldfarb-Shanno).

### Conceitos Fundamentais

#### A Necessidade de Aproximação da Hessiana
Em **Newton's method**, a atualização dos parâmetros $\theta$ é dada por:

$$ \theta_{k+1} = \theta_k - \eta_k H_k^{-1} g_k $$

onde $H_k$ é a Hessiana e $g_k$ é o gradiente na iteração $k$ [^249]. Calcular e inverter $H_k$ tem complexidade $O(D^3)$, onde $D$ é a dimensão do espaço de parâmetros [^252]. Para problemas de alta dimensão, essa complexidade pode ser proibitiva.

#### Métodos Quasi-Newton: Uma Visão Geral
Os métodos Quasi-Newton abordam esse problema construindo iterativamente uma aproximação $B_k$ da Hessiana ou uma aproximação $C_k$ da sua inversa [^251]. A ideia central é usar informações do gradiente em iterações sucessivas para refinar essas aproximações. Esses métodos garantem que a matriz permaneça definida positiva sob certas condições [^251].

#### BFGS (Broyden-Fletcher-Goldfarb-Shanno)
O algoritmo BFGS é um dos métodos Quasi-Newton mais populares [^251]. Ele atualiza a aproximação da Hessiana $B_k$ usando a seguinte fórmula:

$$ B_{k+1} = B_k + \frac{y_k y_k^T}{y_k^T s_k} - \frac{(B_k s_k)(B_k s_k)^T}{s_k^T B_k s_k} $$

onde:
- $s_k = \theta_{k+1} - \theta_k$ é a diferença entre os parâmetros em iterações sucessivas [^251].
- $y_k = g_{k+1} - g_k$ é a diferença entre os gradientes em iterações sucessivas [^251].

O algoritmo BFGS também pode ser formulado para atualizar diretamente a aproximação da inversa da Hessiana $C_k$ [^252]:

$$ C_{k+1} = \left(I - \frac{s_k y_k^T}{y_k^T s_k}\right) C_k \left(I - \frac{y_k s_k^T}{y_k^T s_k}\right) + \frac{s_k s_k^T}{y_k^T s_k} $$

#### Propriedades e Vantagens do BFGS
- **Construção Iterativa:** BFGS constrói uma aproximação da Hessiana iterativamente, reduzindo o custo computacional em comparação com o cálculo direto [^251].
- **Memória:** O algoritmo requer o armazenamento da matriz $B_k$ ou $C_k$, que tem tamanho $O(D^2)$.
- **Definida Positiva:** BFGS garante que a matriz $B_k$ permaneça definida positiva, o que é crucial para garantir que a direção de busca seja uma direção de descida [^251]. Isso é garantido se $y_k^T s_k > 0$, condição satisfeita com uma busca linear apropriada [^251].
- **Aproximação de Baixa-Patente:** BFGS pode ser visto como uma aproximação "diagonal mais baixa patente" da Hessiana [^251].

#### Limited-Memory BFGS (L-BFGS)
Para problemas com um número muito grande de parâmetros, o armazenamento da matriz $B_k$ ou $C_k$ pode se tornar um gargalo [^252]. O algoritmo L-BFGS (Limited-Memory BFGS) é uma variante que armazena apenas um número limitado $m$ de vetores $s_k$ e $y_k$, usando apenas as $m$ atualizações mais recentes para aproximar a Hessiana [^252]. Isso reduz os requisitos de memória para $O(mD)$, tornando-o adequado para problemas de alta dimensão.

#### Inicialização
O método BFGS tipicamente começa com uma aproximação diagonal, $B_0 = I$, onde $I$ é a matriz identidade [^251]. Isso corresponde a assumir que as variáveis são independentes no início da otimização.

### Conclusão

Os métodos Quasi-Newton, em particular o BFGS e suas variantes, oferecem uma alternativa eficiente para Newton's method, especialmente em problemas de alta dimensão onde o cálculo da Hessiana é proibitivo [^251]. BFGS constrói iterativamente uma aproximação da Hessiana ou sua inversa usando informações do gradiente, garantindo que a matriz permaneça definida positiva sob certas condições [^251]. A variante L-BFGS reduz ainda mais os requisitos de memória, tornando-o adequado para problemas com um grande número de parâmetros [^252]. Esses métodos são amplamente utilizados em diversas aplicações de otimização, incluindo machine learning [^252].

### Referências
[^249]: Page 249, Algorithm 8.1
[^251]: Page 251, Section 8.3.5
[^252]: Page 252, Section 8.3.6
<!-- END -->