## Iteratively Reweighted Least Squares (IRLS) for Logistic Regression

### Introdução
O algoritmo **Iteratively Reweighted Least Squares (IRLS)** é um método utilizado para encontrar a estimativa de máxima verossimilhança (MLE) para regressão logística binária [^8]. Diferentemente da regressão linear, a MLE para regressão logística não possui uma forma fechada [^2]. O IRLS aborda este problema resolvendo iterativamente um problema de mínimos quadrados ponderados, onde a matriz de pesos é ajustada em cada iteração com base nas estimativas de parâmetros atuais [^8]. Este capítulo detalha o funcionamento do algoritmo IRLS, sua derivação matemática e sua aplicação no contexto da regressão logística.

### Conceitos Fundamentais

#### Regressão Logística
Na regressão logística binária, modelamos a probabilidade condicional de uma variável binária $y$ dado um vetor de entrada $x$ e um vetor de pesos $w$ [^1]:
$$ p(y|x, w) = Ber(y|sigm(w^Tx)) $$
onde $Ber(y|μ)$ é a distribuição de Bernoulli e $sigm(a) = \frac{1}{1 + exp(-a)}$ é a função sigmoide [^1]. O objetivo é encontrar o vetor de pesos $w$ que maximize a verossimilhança dos dados observados.

#### Negative Log-Likelihood
A função de **negative log-likelihood (NLL)** para regressão logística é dada por [^2]:
$$ NLL(w) = - \sum_{i=1}^N [y_i log μ_i + (1 - y_i) log (1 - μ_i)] $$
onde $μ_i = sigm(w^Tx_i)$ é a probabilidade predita para a i-ésima amostra [^2]. Essa função é também conhecida como *cross-entropy error function* [^2].

#### Derivação do IRLS
O IRLS utiliza o método de Newton para otimizar a NLL. O método de Newton é um algoritmo iterativo que atualiza os parâmetros da seguinte forma [^5]:
$$ \theta_{k+1} = \theta_k - η_k H_k^{-1} g_k $$
onde $θ_k$ é o vetor de parâmetros na iteração $k$, $η_k$ é o tamanho do passo (step size), $g_k$ é o gradiente e $H_k$ é a matriz Hessiana da função objetivo [^5]. No caso do IRLS para regressão logística, o tamanho do passo $η_k$ é geralmente definido como 1, uma vez que a Hessiana é exata [^6].

O gradiente $g$ e a Hessiana $H$ da NLL para regressão logística são dados por [^3]:
$$ g = \frac{d}{dw} f(w) = \sum_{i=1}^N (μ_i - y_i)x_i = X^T (μ - y) $$
$$ H = \frac{d}{dw} g(w) = \sum_{i=1}^N μ_i(1 - μ_i)x_ix_i^T = X^TSX $$
onde $S$ é uma matriz diagonal com elementos $S_{ii} = μ_i(1 - μ_i)$ [^3].

Substituindo o gradiente e a Hessiana na fórmula de atualização do método de Newton, obtemos a atualização do IRLS [^6]:
$$ w_{k+1} = w_k - (X^TS_kX)^{-1}X^T(μ_k - y) $$
$$ w_{k+1} = w_k + (X^TS_kX)^{-1}X^T(y - μ_k) $$

#### Working Response

Para interpretar melhor a atualização do IRLS, podemos reescrevê-la da seguinte forma [^6]:

$$w_{k+1} = (X^TS_kX)^{-1}X^TS_k z_k$$
onde $z_k$ é o *working response*, definido como [^6]:

$$z_k = Xw_k + S_k^{-1}(y - \mu_k)$$

Essa formulação revela que o IRLS resolve um problema de mínimos quadrados ponderados na forma $w = (X^TSX)^{-1}X^TSz$, onde $S$ é a matriz de pesos e $z$ é o vetor de *working responses* [^7]. A matriz de pesos $S_k$ é atualizada em cada iteração, refletindo a confiança nas estimativas atuais dos parâmetros.

#### Algoritmo IRLS
O algoritmo IRLS pode ser resumido nos seguintes passos [^7]:

1.  Inicializar $w$ com um valor inicial (por exemplo, $w = 0$).
2.  Calcular as probabilidades preditas $μ_i = sigm(w^Tx_i)$ para cada amostra $i$.
3.  Calcular a matriz de pesos $S$ com elementos diagonais $S_{ii} = μ_i(1 - μ_i)$.
4.  Calcular o *working response* $z_i = w^Tx_i + \frac{y_i - μ_i}{μ_i(1 - μ_i)}$.
5.  Atualizar os pesos $w = (X^TSX)^{-1}X^TSz$.
6.  Repetir os passos 2-5 até a convergência (ou seja, até que a mudança em $w$ seja menor que um limiar predefinido).

### Conclusão
O algoritmo IRLS é uma ferramenta eficaz para encontrar a MLE na regressão logística binária. Ele itera entre a ponderação das amostras e a resolução de um problema de mínimos quadrados ponderados. Embora o IRLS seja geralmente rápido e eficiente, é importante notar que ele pode ser sensível a valores iniciais e pode não convergir em alguns casos. Além disso, a necessidade de inverter a matriz $(X^TSX)$ em cada iteração pode ser computacionalmente cara para conjuntos de dados muito grandes, tornando outras técnicas de otimização, como o gradiente descendente estocástico (SGD) [^3], mais apropriadas em tais cenários.

### Referências
[^1]: 8.2 Model specification
[^2]: 8.3.1 MLE
[^3]: 8.3.2 Steepest descent
[^4]: Algorithm 8.1: Newton's method for minimizing a strictly convex function
[^5]: 8.3.3 Newton's method
[^6]: 8.3.4 Iteratively reweighted least squares (IRLS)
[^7]: Algorithm 8.2: Iteratively reweighted least squares (IRLS)
[^8]: Paragraph about IRLS

<!-- END -->