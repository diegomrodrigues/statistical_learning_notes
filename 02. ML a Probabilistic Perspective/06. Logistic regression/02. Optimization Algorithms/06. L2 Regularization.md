## Regularização L2 na Regressão Logística

### Introdução
Este capítulo explora a aplicação da **regularização L2** no contexto da **regressão logística**, uma técnica fundamental para aprimorar a generalização e evitar o *overfitting*, especialmente em cenários onde os dados são linearmente separáveis. A regularização L2, também conhecida como *ridge regularization*, adiciona um termo de penalidade à função objetivo, proporcional ao quadrado da magnitude do vetor de pesos [^8]. Essa abordagem influencia o processo de otimização, favorecendo soluções com pesos menores e, consequentemente, modelos mais simples e robustos.

### Conceitos Fundamentais

A **função objetivo** na regressão logística, geralmente expressa como a *negative log-likelihood* (NLL), busca minimizar o erro entre as previsões do modelo e os valores reais [^8]. Sem regularização, essa minimização pode levar a pesos excessivamente grandes, especialmente quando os dados são linearmente separáveis, resultando em overfitting [^8]. O overfitting ocorre quando o modelo se ajusta excessivamente aos dados de treinamento, capturando ruídos e padrões espúrios, o que compromete sua capacidade de generalização para dados não vistos.

A regularização L2 mitiga esse problema ao adicionar um termo de penalidade à NLL [^8]:

$$
f'(w) = NLL(w) + \lambda w^Tw
$$

onde:
*   $f'(w)$ é a nova função objetivo a ser minimizada.
*   $NLL(w)$ é a *negative log-likelihood* original.
*   $\lambda$ é o parâmetro de regularização, que controla a força da penalidade.
*   $w^Tw$ é o quadrado da norma L2 do vetor de pesos, calculado como a soma dos quadrados dos pesos.

O parâmetro $\lambda$ desempenha um papel crucial. Um valor de $\lambda$ igual a zero elimina a regularização, enquanto valores maiores aumentam a penalidade sobre os pesos grandes, incentivando o modelo a encontrar soluções com pesos menores [^8]. A escolha apropriada de $\lambda$ é fundamental para equilibrar o ajuste aos dados de treinamento e a capacidade de generalização.

A inclusão do termo de regularização L2 também afeta o gradiente e o Hessiano da função objetivo [^8]:

$$
g'(w) = g(w) + \lambda w
$$

$$
H'(w) = H(w) + \lambda I
$$

onde:
*   $g'(w)$ é o novo gradiente.
*   $g(w)$ é o gradiente original.
*   $H'(w)$ é o novo Hessiano.
*   $H(w)$ é o Hessiano original.
*   $I$ é a matriz identidade.

Essas modificações no gradiente e no Hessiano são facilmente incorporadas em qualquer otimizador baseado em gradiente, como o *gradient descent*, *conjugate gradients* ou métodos *Quasi-Newton* como o BFGS (Broyden–Fletcher–Goldfarb–Shanno) [^8].

### Conclusão
A regularização L2 é uma ferramenta valiosa para melhorar a performance da regressão logística, prevenindo o overfitting e promovendo uma melhor generalização [^8]. Ao adicionar um termo de penalidade à função objetivo, a regularização L2 incentiva o modelo a encontrar soluções com pesos menores, resultando em modelos mais simples e robustos. A escolha apropriada do parâmetro de regularização $\lambda$ é crucial para equilibrar o ajuste aos dados de treinamento e a capacidade de generalização.
<!-- END -->