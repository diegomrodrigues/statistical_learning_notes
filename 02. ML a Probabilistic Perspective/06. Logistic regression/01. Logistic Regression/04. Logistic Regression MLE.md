## Maximum Likelihood Estimation in Logistic Regression

### Introdução
Em regressão logística, o objetivo é estimar os parâmetros do modelo que melhor se ajustam aos dados observados. O método de **Maximum Likelihood Estimation (MLE)** [^1] é empregado para encontrar esses parâmetros, maximizando a probabilidade de observar o conjunto de dados fornecido, dado o modelo. Ao contrário da regressão linear, a regressão logística não possui uma solução analítica fechada, o que exige o uso de técnicas de otimização iterativas [^2]. Este capítulo se aprofundará nos detalhes da aplicação do MLE à regressão logística, explorando as funções de likelihood, os métodos de otimização necessários e as nuances envolvidas.

### Conceitos Fundamentais
#### Função de Log-Likelihood
Na regressão logística, o modelo de classificação binária é definido como:
$$ p(y|x, w) = Ber(y|sigm(w^Tx)) $$
onde $y$ representa a variável dependente binária (0 ou 1), $x$ é o vetor de características, $w$ é o vetor de pesos (parâmetros do modelo), e $sigm(w^Tx)$ é a função sigmoide, dada por:
$$ sigm(z) = \frac{1}{1 + e^{-z}} $$
A função de **log-likelihood** (ou, equivalentemente, a negative log-likelihood) é uma medida da qualidade do ajuste do modelo aos dados. A negative log-likelihood (NLL) para regressão logística é dada por [^2]:
$$ NLL(w) = -\sum_{i=1}^{N} [y_i log(\mu_i) + (1 - y_i) log(1 - \mu_i)] $$
onde $\mu_i = sigm(w^Tx_i)$ é a probabilidade prevista de $y_i = 1$ para a $i$-ésima amostra. Esta função também é conhecida como a **cross-entropy error function** [^2]. Uma formulação alternativa, usando $\tilde{y}_i \in \{-1, +1\}$ em vez de $y_i \in \{0, 1\}$, é:
$$ NLL(W) = \sum_{i=1}^{N} log(1 + exp(-\tilde{y}_iw^Tx_i)) $$
O objetivo do MLE é encontrar o vetor de pesos $w$ que minimiza a NLL.

#### Otimização Iterativa
Como não há uma solução de forma fechada para o MLE na regressão logística [^2], é necessário usar algoritmos de otimização iterativos para encontrar o mínimo da função NLL. Isso envolve calcular o gradiente e, em alguns casos, o Hessiano da função NLL.

O gradiente da NLL é dado por [^3]:
$$ g = \frac{d}{dw} f(w) = \sum_{i} (\mu_i - y_i)x_i = X^T(\mu - y) $$
onde $X$ é a matriz de características, $\mu$ é o vetor de probabilidades previstas e $y$ é o vetor de rótulos verdadeiros.

O Hessiano da NLL é dado por [^3]:
$$ H = \frac{d}{dw} g(w) = \sum_{i} (\nabla_w \mu_i)x_i^T = \sum_{i} \mu_i(1 - \mu_i)x_ix_i^T = X^TSX $$
onde $S = diag(\mu_i(1 - \mu_i))$ é uma matriz diagonal. Pode-se demonstrar que $H$ é positivo definido, indicando que a NLL é convexa e possui um mínimo global único [^3].

#### Métodos de Otimização
Vários algoritmos podem ser usados para minimizar a NLL na regressão logística. Alguns dos métodos mais comuns incluem:

1.  **Steepest Descent (Gradient Descent)**: Um dos algoritmos mais simples, que atualiza os parâmetros na direção oposta ao gradiente [^3]:
    $$     \theta_{k+1} = \theta_k - \eta_k g_k     $$
    onde $\eta_k$ é o *step size* ou *learning rate*. A escolha apropriada do learning rate é crucial para garantir a convergência. Se $\eta_k$ for muito pequeno, a convergência será lenta. Se for muito grande, o algoritmo pode não convergir ou oscilar em torno do mínimo [^3].

2.  **Line Search**: Uma abordagem mais estável para escolher o step size envolve minimizar a função ao longo da direção do gradiente [^4]:
    $$     \phi(\eta) = f(\theta_k + \eta d_k)     $$
    onde $d_k$ é a direção de descida. A *line minimization* ou *line search* busca o valor de $\eta$ que minimiza $\phi(\eta)$. No entanto, o steepest descent com line search exibe um comportamento de zig-zag, onde a direção do gradiente é perpendicular à direção de busca [^4].

3.  **Momentum**: Para reduzir o efeito de zig-zag, um termo de momentum pode ser adicionado [^4]:
    $$     \theta_{k+1} = \theta_k - \eta_k g_k + \mu_k (\theta_k - \theta_{k-1})     $$
    onde $\mu_k$ controla a importância do termo de momentum.

4.  **Newton's Method**: Um algoritmo de segunda ordem que usa informações de curvatura (Hessiano) para otimizar [^5]:
    $$     \theta_{k+1} = \theta_k - \eta_k H_k^{-1} g_k     $$
    O método de Newton pode convergir mais rapidamente do que o steepest descent, mas requer o cálculo e a inversão do Hessiano, o que pode ser computacionalmente caro.

5.  **Iteratively Reweighted Least Squares (IRLS)**: Uma aplicação do método de Newton à regressão logística [^6]:
    $$     w_{k+1} = w_k + (X^TS_kX)^{-1}X^T(y - \mu_k)     $$
    Este algoritmo envolve resolver um problema de mínimos quadrados ponderados em cada iteração, onde a matriz de pesos $S_k$ muda a cada iteração.

6.  **Quasi-Newton Methods**: Métodos que constroem iterativamente uma aproximação do Hessiano usando informações do gradiente em cada passo [^7]. Um método comum é o BFGS (Broyden-Fletcher-Goldfarb-Shanno):
    $$     B_{k+1} = B_k + \frac{y_k y_k^T}{y_k^T s_k} - \frac{(B_k s_k)(B_k s_k)^T}{s_k^T B_k s_k}     $$
    onde $s_k = \theta_k - \theta_{k-1}$ e $y_k = g_k - g_{k-1}$.

#### Regularização
Para evitar overfitting, a regularização pode ser adicionada à função de custo. A regularização $l_2$ é uma técnica comum, onde um termo proporcional ao quadrado da norma dos pesos é adicionado à NLL [^8]:
$$ f'(w) = NLL(w) + \frac{\lambda}{2} w^T w $$
onde $\lambda$ é o parâmetro de regularização. O gradiente e o Hessiano da função de custo regularizada são:
$$ g'(w) = g(w) + \lambda w $$
$$ H'(w) = H(w) + \lambda I $$

### Conclusão
A Maximum Likelihood Estimation é uma ferramenta fundamental para estimar os parâmetros em modelos de regressão logística. Dada a ausência de uma solução analítica, métodos de otimização iterativos como gradient descent, Newton's method e variantes quasi-Newton são indispensáveis. A escolha do método de otimização, juntamente com técnicas de regularização apropriadas, desempenha um papel crítico na obtenção de um modelo de regressão logística preciso e generalizável.

### Referências
[^1]: Seção 8.3 do texto fornecido.
[^2]: Seção 8.3.1 do texto fornecido.
[^3]: Seção 8.3.2 do texto fornecido.
[^4]: Seção 8.3.2 do texto fornecido.
[^5]: Seção 8.3.3 do texto fornecido.
[^6]: Seção 8.3.4 do texto fornecido.
[^7]: Seção 8.3.5 do texto fornecido.
[^8]: Seção 8.3.6 do texto fornecido.
<!-- END -->