## Métodos Quasi-Newton para Regressão Logística

### Introdução
Na regressão logística, como em muitos outros problemas de otimização, encontrar os parâmetros que minimizam a função de custo é crucial. Embora o gradiente descendente seja um método popular, ele pode convergir lentamente [^247]. Métodos de segunda ordem, como o método de Newton, podem oferecer convergência mais rápida, mas exigem o cálculo da matriz Hessiana, o que pode ser computacionalmente caro [^249]. Os métodos Quasi-Newton, como o BFGS (Broyden-Fletcher-Goldfarb-Shanno), oferecem um compromisso entre esses dois, aproximando a Hessiana sem calculá-la explicitamente [^251]. Este capítulo explora a aplicação dos métodos Quasi-Newton, especificamente o BFGS, no contexto da regressão logística.

### Conceitos Fundamentais

#### Regressão Logística e Otimização
Na regressão logística, o objetivo é encontrar os pesos **w** que melhor separam as classes, modelando a probabilidade condicional $p(y|x, w)$ usando a função sigmoide [^245]. A função de custo a ser minimizada é a negative log-likelihood (NLL), dada por [^246]:
$$NLL(w) = \sum_{i=1}^N log(1 + exp(-y_i w^T x_i))$$
onde $y_i \in \{-1, +1\}$ são os rótulos das classes e $x_i$ são os vetores de características. Como a NLL não tem uma solução analítica fechada, métodos iterativos de otimização são necessários [^246].

#### Método de Newton
O método de Newton utiliza a curvatura do espaço (i.e., a Hessiana) para acelerar a convergência [^249]. A atualização dos parâmetros é dada por [^249]:
$$theta_{k+1} = \theta_k - \eta_k H_k^{-1} g_k$$
onde $H_k$ é a Hessiana e $g_k$ é o gradiente no passo *k* [^249]. No entanto, calcular e inverter a Hessiana pode ser proibitivamente caro, especialmente em problemas de alta dimensão [^251].

#### Métodos Quasi-Newton: BFGS
Os métodos Quasi-Newton abordam o custo computacional do método de Newton aproximando iterativamente a Hessiana ou sua inversa [^251]. O BFGS é um dos métodos Quasi-Newton mais populares [^251]. Ao invés de calcular a Hessiana diretamente, o BFGS constrói uma aproximação $B_k$ da Hessiana usando informações do gradiente em iterações anteriores [^251].

A atualização da aproximação da Hessiana no BFGS é dada por [^251]:
$$B_{k+1} = B_k + \frac{y_k y_k^T}{y_k^T s_k} - \frac{B_k s_k (B_k s_k)^T}{s_k^T B_k s_k}$$
onde $s_k = \theta_k - \theta_{k-1}$ e $y_k = \nabla f(\theta_k) - \nabla f(\theta_{k-1})$ [^251].

Uma alternativa é atualizar diretamente uma aproximação da inversa da Hessiana, $C_k \approx H_k^{-1}$, usando a seguinte fórmula [^252]:
$$C_{k+1} = \left(I - \frac{s_k y_k^T}{y_k^T s_k}\right) C_k \left(I - \frac{y_k s_k^T}{y_k^T s_k}\right) + \frac{s_k s_k^T}{y_k^T s_k}$$

#### Implementação e Considerações
1.  **Inicialização:** O BFGS geralmente começa com uma aproximação diagonal para a Hessiana, como a matriz identidade $B_0 = I$ [^251].
2.  **Atualização Rank-Two:** A atualização do BFGS é uma atualização de rank-two, o que garante que a matriz permaneça positiva definida sob certas condições (restrições no tamanho do passo) [^251].
3.  **Busca Linear:** Para garantir a convergência, uma busca linear (line search) pode ser usada para encontrar um tamanho de passo $\eta_k$ apropriado [^248, 249].
4.  **BFGS com memória limitada (L-BFGS):** Para problemas de alta dimensão, armazenar a matriz Hessiana (ou sua inversa) pode ser impraticável. O L-BFGS aproxima a Hessiana usando apenas as *m* atualizações mais recentes de *s* e *y*, reduzindo os requisitos de memória [^252].

### Conclusão

Os métodos Quasi-Newton, como o BFGS, representam uma alternativa eficaz ao gradiente descendente e ao método de Newton para otimizar a regressão logística [^251]. Eles oferecem uma convergência mais rápida do que o gradiente descendente sem o custo computacional de calcular a Hessiana explicitamente [^251]. O BFGS e suas variantes, como o L-BFGS, são amplamente utilizados em problemas de aprendizado de máquina devido à sua eficiência e escalabilidade [^252].

### Referências
[^245]: Section 8.2: Model specification
[^246]: Section 8.3.1: MLE
[^247]: Section 8.3.2: Steepest descent
[^248]: Figure 8.3
[^249]: Section 8.3.3: Newton's method
[^251]: Section 8.3.5: Quasi-Newton (variable metric) methods
[^252]: Section 8.3.6: l2 regularization
<!-- END -->