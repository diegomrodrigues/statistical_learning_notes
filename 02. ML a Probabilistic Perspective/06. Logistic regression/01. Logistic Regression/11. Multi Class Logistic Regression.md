## Multi-Class Logistic Regression (Softmax Regression)

### Introdução
Expandindo o conceito de **regressão logística binária** [^1, 8.2], este capítulo aborda a regressão logística multi-classe, também conhecida como regressão logística multinomial ou regressão softmax [^8, 8.3.7]. Este modelo é essencial para problemas de classificação que envolvem mais de duas classes. A regressão logística multi-classe modela a probabilidade de cada classe usando uma função softmax [^8, 8.3.7], que normaliza os *scores* das classes em uma distribuição de probabilidade sobre todas as classes.

### Conceitos Fundamentais
O modelo de regressão logística multi-classe é definido da seguinte forma [^8, 8.3.7]:

$$np(y = c|x, W) = \frac{exp(w_c^Tx)}{\sum_{c'=1}^C exp(w_{c'}^Tx)}$$

onde:
- $y$ é a variável de classe.
- $c$ representa uma classe específica.
- $x$ é o vetor de entrada.
- $W$ é a matriz de pesos, onde cada linha $w_c$ corresponde aos pesos para a classe $c$ [^8, 8.3.7].
- $C$ é o número total de classes.

A função **softmax** garante que a saída seja uma distribuição de probabilidade válida, ou seja, as probabilidades de todas as classes somam 1 e cada probabilidade está entre 0 e 1 [^8, 8.3.7].

**Identificabilidade:** Para garantir a identificabilidade do modelo, uma restrição é necessária. Uma abordagem comum é fixar os pesos de uma das classes (por exemplo, a classe $C$) para zero ($w_C = 0$) [^8, 8.3.7]. Alternativamente, pode-se usar a regularização L2 para evitar que os pesos cresçam sem limites [^8, 8.3.7].

#### Treinamento do Modelo
O treinamento de um modelo de regressão logística multi-classe envolve a estimativa da matriz de pesos $W$ a partir dos dados de treinamento. Isso é tipicamente feito maximizando a função de log-verossimilhança (log-likelihood) ou minimizando a função de custo de entropia cruzada (cross-entropy) [^2, 8.3.1]. A função de custo é dada por:

$$nJ(W) = -\frac{1}{N} \sum_{i=1}^N \sum_{c=1}^C y_{ic} \log(p(y_i = c|x_i, W))$$

onde:
- $N$ é o número de amostras de treinamento.
- $y_{ic}$ é um indicador binário que é 1 se a amostra $i$ pertence à classe $c$ e 0 caso contrário.

A minimização da função de custo pode ser realizada usando algoritmos de otimização como o **gradiente descendente** [^3, 8.3.2], **gradiente descendente estocástico (SGD)** [^18, 8.5.2] ou métodos **Quasi-Newton** [^7, 8.3.5] (por exemplo, BFGS). Dada a estrutura dos dados, pode ser mais apropriado utilizar limited memory BFGS [^8, 8.3.7].

#### Gradient Descent
O gradiente da função de custo em relação aos pesos da classe $c$ é dado por [^9, 8.40]:

$$n\nabla_{w_c} J(W) = -\frac{1}{N} \sum_{i=1}^N (y_{ic} - p(y_i = c|x_i, W))x_i$$

Este gradiente é usado para atualizar os pesos iterativamente até que a convergência seja alcançada [^3, 8.3.2].

#### Regularização
Para evitar o *overfitting*, a regularização L2 pode ser adicionada à função de custo [^8, 8.3.6]:

$$nJ(W) = -\frac{1}{N} \sum_{i=1}^N \sum_{c=1}^C y_{ic} \log(p(y_i = c|x_i, W)) + \frac{\lambda}{2} ||W||^2$$

onde $\lambda$ é o parâmetro de regularização que controla a força da regularização [^8, 8.3.6].

### Conclusão
A regressão logística multi-classe (softmax) é uma ferramenta poderosa para problemas de classificação com múltiplas classes [^8, 8.3.7]. Compreender sua formulação, treinamento e técnicas de regularização é essencial para construir modelos de classificação eficazes. Este capítulo forneceu uma visão detalhada dos conceitos e métodos envolvidos, preparando o terreno para aplicações mais avançadas.

### Referências
[^1]: *Logistic regression*
[^2]: *Chapter 8. Logistic regression*
[^3]: *8.3.2 Steepest descent*
[^7]: *8.3.5 Quasi-Newton (variable metric) methods*
[^8]: *8.3.7 Multi-class logistic regression*
[^9]: *8.40*
[^18]: *8.5.2 Stochastic optimization and risk minimization*
<!-- END -->