## Model Fitting in Logistic Regression: Parameter Estimation and Optimization

### Introdução
Este capítulo se dedica a explorar os métodos para **model fitting** em **logistic regression**, com foco na estimação de parâmetros e otimização [^8]. A regressão logística, conforme discutido na Seção 1.4.6 [^8], é um modelo de classificação binária onde a probabilidade de uma dada observação pertencer a uma classe é modelada usando a função *sigmoid*. Diferentemente da regressão linear, a regressão logística não possui uma solução analítica fechada para a estimação dos parâmetros, tornando necessária a utilização de algoritmos de otimização iterativos [^8].

### Conceitos Fundamentais

O objetivo do **model fitting** em regressão logística é estimar os parâmetros do modelo, geralmente representados por um vetor de pesos **w** [^8]. Este processo é tipicamente realizado maximizando a função de verossimilhança (likelihood function) ou, equivalentemente, minimizando a **negative log-likelihood (NLL)**, que também é conhecida como a função de erro de *cross-entropy* [^8].

#### Negative Log-Likelihood (NLL)
A **NLL** para regressão logística é dada por:
$$
NLL(w) = \sum_{i=1}^{N} log(1 + exp(-y_iw^Tx_i))
$$
onde:
*   $y_i$ é o rótulo da classe da i-ésima observação, geralmente codificada como 0 ou 1 (ou -1 e +1) [^8].
*   $x_i$ é o vetor de características da i-ésima observação [^8].
*   $w$ é o vetor de pesos que desejamos estimar [^8].

> A **NLL** é uma função convexa, o que garante a existência de um mínimo global único. Essa propriedade é crucial, pois assegura que os algoritmos de otimização convergirão para a solução ótima [^8].

#### Algoritmos de Otimização
Devido à ausência de uma solução analítica para a **MLE (Maximum Likelihood Estimate)**, são empregados algoritmos de otimização iterativos [^8]. Os métodos mais comuns incluem:

1.  **Gradient Descent:** Um algoritmo iterativo que atualiza os pesos na direção oposta ao gradiente da **NLL** [^8]. A atualização dos pesos é dada por:
    $$
    \theta_{k+1} = \theta_k - \eta_k g_k
    $$
    onde $\eta_k$ é o *step size* ou *learning rate*, e $g_k$ é o gradiente da **NLL** no ponto $\theta_k$ [^8]. O principal desafio no *gradient descent* é a escolha adequada do *step size*. Um *step size* muito pequeno pode resultar em uma convergência lenta, enquanto um *step size* muito grande pode levar à oscilação e falta de convergência [^8].

    O gradiente da **NLL** é dado por [^8]:
    $$
    g = \frac{d}{dw}f(w) = \sum_i (\mu_i - y_i)x_i = X^T (\mu - y)
    $$
    onde $\mu_i = \frac{1}{1 + exp(-w^Tx_i)}$ é a probabilidade prevista pela regressão logística para a i-ésima observação [^8].

2.  **Newton's Method:** Um algoritmo de segunda ordem que utiliza o gradiente e a matriz Hessiana da **NLL** para encontrar o mínimo [^8]. A atualização dos pesos é dada por:
    $$
    \theta_{k+1} = \theta_k - \eta_k H_k^{-1}g_k
    $$
    onde $H_k$ é a matriz Hessiana da **NLL** no ponto $\theta_k$ [^8].

    A matriz Hessiana da **NLL** é dada por [^8]:
    $$
    H = \frac{d}{dw}g(w) = \sum_i \frac{d}{dw}(\nabla w \mu_i)x_i^T = \sum_i \mu_i(1 - \mu_i)x_ix_i^T = X^TSX
    $$
    onde $S$ é uma matriz diagonal com elementos $S_{ii} = \mu_i(1 - \mu_i)$ [^8].

    O método de Newton geralmente converge mais rapidamente do que o *gradient descent*, mas requer o cálculo e a inversão da matriz Hessiana, o que pode ser computacionalmente caro [^8]. Além disso, o método de Newton requer que a Hessiana seja positiva definida [^8].

#### Steepest Descent
O *steepest descent* é um algoritmo de otimização para problemas não-restritos. Ele também é conhecido como *gradient descent* e pode ser escrito como:
$$
\theta_{k+1} = \theta_k - \eta_k g_k
$$
onde $\eta_k$ é o *step size* ou *learning rate* [^8].

#### Line Search
Para tornar o método mais estável e garantir a convergência para um ótimo local, podemos usar a *line minimization* ou *line search* [^8]. Isso envolve escolher $\eta$ para minimizar:
$$
\phi(\eta) = f(\theta_k + \eta d_k)
$$
onde $d_k$ é a direção de descida [^8].

#### Momentum
Para reduzir o efeito de *zig-zagging*, podemos adicionar um termo de *momentum*:
$$
\theta_{k+1} = \theta_k - \eta_k g_k + \mu_k (\theta_k - \theta_{k-1})
$$
onde $0 \leq \mu_k \leq 1$ controla a importância do termo de *momentum* [^8].

#### Conjugate Gradients
Uma alternativa para minimizar o *zig-zagging* é usar o método dos *conjugate gradients* [^8].

#### Quasi-Newton Methods
Os métodos *Quasi-Newton* iterativamente constroem uma aproximação para a Hessiana usando informações coletadas do vetor gradiente em cada passo [^8]. O método mais comum é o *BFGS* [^8].

#### Iteratively Reweighted Least Squares (IRLS)
O *Iteratively Reweighted Least Squares (IRLS)* é um algoritmo usado para resolver problemas de otimização ponderados dos mínimos quadrados [^8]. Ele pode ser usado para encontrar a *MLE* para regressão logística binária aplicando a atualização de Newton na iteração $k + 1$:
$$
w_{k+1} = w_k + (X^TS_kX)^{-1}X^T(y - \mu_k)
$$
onde $S_k = diag(\mu_i(1 - \mu_i))$ [^8].

#### l2 Regularization
Assim como na regressão *ridge*, preferimos a estimação *MAP* para a regressão logística para calcular a *MLE* [^8]. A regularização é importante na classificação, mesmo que tenhamos muitos dados. Para evitar o problema de separabilidade linear, podemos usar a regularização $l_2$ [^8]:
$$
f'(w) = NLL(w) + \lambda w^Tw
$$

### Conclusão
O *model fitting* em regressão logística envolve a estimação dos parâmetros do modelo através da minimização da **NLL** usando algoritmos de otimização iterativos [^8]. A escolha do algoritmo de otimização e a definição dos hiperparâmetros, como o *learning rate* e os termos de regularização, são cruciais para garantir a convergência e o desempenho do modelo [^8].

### Referências
[^8]: (Documento OCR fornecido)
<!-- END -->