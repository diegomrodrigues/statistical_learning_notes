## Online Learning for Logistic Regression

### Introdução
Este capítulo explora o **aprendizado online (online learning)** no contexto da **regressão logística (logistic regression)**. O aprendizado online é um paradigma de aprendizado de máquina onde o modelo é atualizado sequencialmente à medida que novos pontos de dados chegam [^1]. Isso o torna particularmente adequado para cenários onde os dados são recebidos em *streaming* ou são tão grandes que não cabem na memória [^1]. Discutiremos como algoritmos como o **gradiente descendente estocástico (stochastic gradient descent)** podem ser usados para atualizar os parâmetros do modelo de regressão logística de forma eficiente, iterando à medida que novos dados se tornam disponíveis [^1]. Além disso, abordaremos a visão Bayesiana do aprendizado online, onde a regra de Bayes é aplicada recursivamente para atualizar a distribuição *a posteriori* [^1].

### Conceitos Fundamentais

#### Aprendizado Online e Otimização Estocástica
O aprendizado online, em conjunto com técnicas de **otimização estocástica (stochastic optimization)**, oferece uma abordagem escalável e adaptável para treinar modelos de regressão logística [^1]. Em vez de processar todo o conjunto de dados de uma vez (como no aprendizado em *batch*), os algoritmos de aprendizado online atualizam os parâmetros do modelo iterativamente, ponto de dado por ponto de dado [^1].

#### Gradiente Descendente Online
Um dos algoritmos mais comuns para aprendizado online é o **gradiente descendente online (online gradient descent)** [^1]. A regra de atualização para este algoritmo é dada por:
$$\
\theta_{k+1} = \theta_k - \eta \nabla f(\theta_k, z_k)
$$
onde:
*   $\theta_k$ representa os parâmetros do modelo no passo $k$.
*   $z_k$ é o ponto de dado no passo $k$.
*   $\eta$ é o tamanho do passo (*step size*), também conhecido como taxa de aprendizado (*learning rate*).
*   $\nabla f(\theta_k, z_k)$ é o gradiente da função de perda em relação aos parâmetros $\theta_k$, calculado no ponto de dado $z_k$ [^1].

A escolha do tamanho do passo $\eta$ é crucial para a convergência do algoritmo. Se $\eta$ for muito pequeno, a convergência pode ser lenta. Se $\eta$ for muito grande, o algoritmo pode não convergir e oscilar em torno do ótimo [^3].

#### Tamanho do Passo (Step Size)
A escolha do tamanho do passo, $\eta_k$, é um aspecto crítico no aprendizado online. Algumas condições suficientes para a convergência do gradiente descendente estocástico são dadas pelas condições de Robbins-Monro [^19]:

$$\
\sum_{k=1}^{\infty} \eta_k = \infty, \quad \sum_{k=1}^{\infty} \eta_k^2 < \infty
$$

Uma escolha comum para $\eta_k$ que satisfaz estas condições é $\eta_k = (\tau_0 + k)^{-\kappa}$, onde $\tau_0 \geq 0$ e $\kappa \in (0.5, 1]$ [^19].

#### Visão Bayesiana do Aprendizado Online
Na **visão Bayesiana (Bayesian view)** do aprendizado online, a regra de Bayes é aplicada recursivamente para atualizar a distribuição *a posteriori* dos parâmetros do modelo [^1]:
$$\
p(\theta|D_{1:k}) \propto p(D_k|\theta)p(\theta|D_{1:k-1})
$$
onde:
*   $p(\theta|D_{1:k})$ é a distribuição *a posteriori* dos parâmetros $\theta$ dados os dados observados $D_1$ até $D_k$.
*   $p(D_k|\theta)$ é a *verossimilhança (likelihood)* do novo ponto de dado $D_k$ dado os parâmetros $\theta$.
*   $p(\theta|D_{1:k-1})$ é a distribuição *a priori (prior)* dos parâmetros $\theta$ antes de observar o novo ponto de dado $D_k$ [^1].

A vantagem desta abordagem é que ela retorna uma distribuição *a posteriori* em vez de apenas uma estimativa pontual [^1]. Isso permite quantificar a incerteza sobre os parâmetros do modelo e fazer previsões mais robustas.

#### Aproximação de Laplace
Uma forma de aproximar a distribuição *a posteriori* Bayesiana é através da **aproximação de Laplace (Laplace approximation)** [^11]. Esta técnica aproxima a distribuição *a posteriori* por uma distribuição Gaussiana centrada no modo da distribuição *a posteriori*.

A aproximação funciona da seguinte forma:
1.  Aproxime a função de energia (negativo do log da *a posteriori* não normalizada) por uma expansão de Taylor de segunda ordem em torno do modo $\theta^*$:
    $$\
    E(\theta) \approx E(\theta^*) + (\theta - \theta^*)^Tg + \frac{1}{2}(\theta - \theta^*)^TH(\theta - \theta^*)
    $$
    onde $g$ é o gradiente e $H$ é o Hessiano da função de energia avaliados no modo [^11].
2.  Como $\theta^*$ é o modo, o termo de gradiente é zero. Portanto, a *a posteriori* é aproximada por:
    $$\
    p(\theta|D) \approx \frac{1}{Z} e^{-E(\theta^*)} \exp\left[-\frac{1}{2}(\theta - \theta^*)^TH(\theta - \theta^*)\right]
    $$
    onde $Z$ é a constante de normalização [^11].

A distribuição *a posteriori* aproximada é então uma Gaussiana:
$$\
p(\theta|D) \approx \mathcal{N}(\theta|\theta^*, H^{-1})
$$

#### Limitações da Aproximação de Laplace
Embora a aproximação de Laplace seja uma técnica útil, ela tem algumas limitações:

*   Requer que a *a posteriori* seja aproximadamente Gaussiana, o que nem sempre é o caso.
*   Pode ser computacionalmente caro calcular o Hessiano [^11].

### Conclusão

O aprendizado online oferece uma abordagem flexível e escalável para treinar modelos de regressão logística em cenários onde os dados são recebidos em *streaming* ou são muito grandes para caber na memória [^1]. Algoritmos como o gradiente descendente online permitem atualizar os parâmetros do modelo iterativamente, adaptando-se a mudanças na distribuição dos dados [^1]. A visão Bayesiana do aprendizado online fornece uma maneira de quantificar a incerteza sobre os parâmetros do modelo e fazer previsões mais robustas, embora possa exigir aproximações para tornar os cálculos tratáveis [^1]. Em continuidade ao tópico de otimização, métodos como o de Newton e suas variações (Quasi-Newton, BFGS, L-BFGS) oferecem alternativas para uma convergência mais rápida, embora com maior custo computacional por iteração [^5, 7, 8]. A escolha do método de otimização e do tamanho do passo (learning rate) são cruciais para o sucesso do aprendizado online [^3, 19].

### Referências

[^1]:  Texto fornecido na questão.
[^3]:  Figure 8.2 Gradient descent on a simple function, starting from (0,0), for 20 steps, using a fixed learning rate (step size) η. The global minimum is at (1,1). (a) η = 0.1. (b) η = 0.6. Figure generated by steepestDescentDemo.
[^5]: Algorithm 8.1: Newton's method for minimizing a strictly convex function
[^7]: Section 8.3.5 Quasi-Newton (variable metric) methods
[^8]: Section 8.3.6 l2 regularization
[^11]: Section 8.4.1 Laplace approximation
[^19]: Section 8.5.2.1 Setting the step size
<!-- END -->