## Bayesian Logistic Regression

### Introdução
O presente capítulo explora a aplicação da inferência Bayesiana à regressão logística, uma técnica conhecida como **Bayesian logistic regression** [^254]. Enquanto a regressão logística tradicional se concentra na estimativa de parâmetros por meio da maximização da verossimilhança (MLE), a abordagem Bayesiana permite a incorporação de conhecimento prévio (via *priors*) e a quantificação da incerteza nos parâmetros do modelo [^254]. Este capítulo detalha os fundamentos da Bayesian logistic regression, as dificuldades associadas à sua implementação e as aproximações comuns utilizadas para superar essas dificuldades.

### Conceitos Fundamentais

#### Posterior Distribution
A Bayesian logistic regression difere da regressão logística tradicional ao computar a distribuição *a posteriori* completa sobre os parâmetros do modelo, denotada por $p(w|D)$, dado o conjunto de dados $D$ [^254]. Essa distribuição *a posteriori* reflete a crença atualizada sobre os parâmetros do modelo após a observação dos dados e é proporcional ao produto da função de verossimilhança e da distribuição *a priori*:

$$ p(w|D) \propto p(D|w)p(w) $$

onde:
- $p(w|D)$ é a distribuição *a posteriori* dos parâmetros $w$ dado o conjunto de dados $D$.
- $p(D|w)$ é a função de verossimilhança, que quantifica a probabilidade dos dados dado os parâmetros $w$. Para regressão logística, essa função é geralmente definida com base na distribuição de Bernoulli [^1].
- $p(w)$ é a distribuição *a priori*, que representa o conhecimento prévio sobre os parâmetros $w$ antes de observar os dados.

#### Conjugate Prior e Aproximações
Ao contrário da regressão linear, a regressão logística não possui um *prior* conjugado conveniente, o que significa que a distribuição *a posteriori* não tem uma forma analítica simples [^254]. Portanto, métodos de aproximação são necessários para estimar a distribuição *a posteriori*. Os métodos comuns incluem:

1.  **Laplace Approximation:** Este método aproxima a distribuição *a posteriori* por uma distribuição Gaussiana centrada no modo da *a posteriori* [^255]. A matriz de covariância da Gaussiana é dada pela inversa do Hessiano da função de energia (negativo do log da *a posteriori*) avaliada no modo [^255]:

    $$     p(\theta|D) \approx \mathcal{N}(\theta|\theta^*, H^{-1})     $$

    onde $\theta^*$ é o modo da distribuição *a posteriori* e $H$ é o Hessiano da função de energia avaliada em $\theta^*$.
    A aproximação de Laplace para a evidência marginal é dada por:
    $$     p(D) \approx e^{-E(\theta^*)} (2\pi)^{D/2} |H|^{-1/2}     $$
    onde $E(\theta)$ é a função de energia.

2.  **Markov Chain Monte Carlo (MCMC):** Os métodos MCMC, como o Metropolis-Hastings e o Gibbs sampling, são utilizados para amostrar da distribuição *a posteriori* [^254]. Essas amostras podem ser usadas para aproximar a distribuição e calcular estatísticas de interesse.

3.  **Variational Inference:** A inferência variacional busca aproximar a distribuição *a posteriori* por uma distribuição mais simples (por exemplo, uma Gaussiana) minimizando uma medida de divergência, como a divergência de Kullback-Leibler (KL) [^254].

#### Vantagens da Bayesian Logistic Regression
A Bayesian logistic regression oferece várias vantagens em relação à regressão logística tradicional:

*   **Quantificação da Incerteza:** A capacidade de computar a distribuição *a posteriori* permite associar intervalos de confiança com as previsões, fornecendo uma estimativa mais robusta da incerteza [^254].
*   **Incorporação de Conhecimento Prévio:** A inclusão de *priors* permite incorporar conhecimento prévio sobre os parâmetros do modelo, o que pode melhorar a precisão e a robustez das estimativas, especialmente quando os dados são escassos [^254].
*   **Regularização:** A escolha apropriada de *priors* pode atuar como um mecanismo de regularização, evitando o *overfitting* e melhorando a generalização do modelo [^252]. Por exemplo, usar um *prior* Gaussiano centrado em zero para os pesos $w$ corresponde a regularização $l_2$ [^252].

#### Bayesian Information Criterion (BIC)
O Bayesian Information Criterion (BIC) é uma aproximação para a evidência marginal, que pode ser usada para seleção de modelos. Deriva-se da aproximação de Laplace e penaliza a complexidade do modelo:
$$ \log p(D) \approx \log p(D|\theta^*) + \log p(\theta^*) - \frac{1}{2} \log |H| $$
Os termos de penalização adicionados ao $\log p(D|\theta^*)$ são chamados de fator de Occam e medem a complexidade do modelo [^255].

#### Aproximação Preditiva Posterior
A distribuição preditiva posterior é dada por:
$$ p(y|x, D) = \int p(y|x, w) p(w|D) dw $$
Esta integral é geralmente intratável [^257]. Uma aproximação comum é a aproximação *plug-in*:
$$ p(y=1|x, D) \approx p(y=1|x, E[w]) $$
onde $E[w]$ é a média *a posteriori* [^257].

### Conclusão

A Bayesian logistic regression oferece uma abordagem flexível e robusta para a modelagem de problemas de classificação, permitindo a incorporação de conhecimento prévio e a quantificação da incerteza nas estimativas de parâmetros [^254]. Embora a computação exata da distribuição *a posteriori* seja geralmente intratável, métodos de aproximação como a aproximação de Laplace, MCMC e inferência variacional fornecem alternativas viáveis para a implementação da Bayesian logistic regression em cenários práticos [^254]. A escolha do método de aproximação depende da complexidade do modelo e dos requisitos computacionais, mas o resultado final é uma abordagem mais completa e informativa para a regressão logística.

### Referências
[^1]: Seção 8.2, pág 245
[^254]: Seção 8.4, pág 254
[^255]: Seção 8.4.1, pág 255
[^252]: Seção 8.3.6, pág 252
[^257]: Seção 8.4.4, pág 257
<!-- END -->