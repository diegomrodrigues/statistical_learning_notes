## Gradient Descent na Regressão Logística

### Introdução
Este capítulo explora em detalhes o uso do algoritmo de **gradient descent** para a otimização de modelos de regressão logística. Conforme discutido na Seção 8.3 [^1], a regressão logística é um modelo de classificação binária que estima a probabilidade de uma instância pertencer a uma determinada classe. A estimação dos parâmetros do modelo, tipicamente os pesos associados às features, envolve a minimização de uma função de custo. Em particular, a função de **negative log-likelihood (NLL)** é frequentemente utilizada como função de custo [^2]. Dada a complexidade analítica da NLL na regressão logística, métodos iterativos como o gradient descent são empregados para encontrar os parâmetros ótimos.

### Conceitos Fundamentais
O **gradient descent** é um algoritmo iterativo de otimização que busca o mínimo de uma função, ajustando os parâmetros em direção ao negativo do gradiente [^3]. Em outras palavras, o algoritmo move-se iterativamente na direção oposta àquela em que a função cresce mais rapidamente.

Formalmente, o processo é definido pela seguinte regra de atualização:
$$
\theta_{k+1} = \theta_k - \eta g_k
$$
onde:
*   $\theta_k$ representa o vetor de parâmetros (pesos) na iteração *k*.
*   $\eta$ é a **learning rate** ou tamanho do passo, que controla a magnitude do ajuste em cada iteração [^3].
*   $g_k$ é o gradiente da função de custo (NLL, neste caso) avaliado nos valores atuais dos parâmetros $\theta_k$ [^3].

A escolha da **learning rate** é crucial para o desempenho do gradient descent [^3]. Se $\eta$ for muito pequeno, a convergência será lenta, exigindo um número excessivo de iterações para atingir um mínimo aceitável. Por outro lado, se $\eta$ for muito grande, o algoritmo pode oscilar em torno do mínimo, ou até mesmo divergir, não convergindo para uma solução ótima.

A NLL para regressão logística é dada por [^2]:
$$
NLL(w) = \sum_{i=1}^N \log(1 + \exp(-y_i w^T x_i))
$$
onde:
*   $N$ é o número de instâncias no conjunto de dados.
*   $y_i \in \{-1, +1\}$ é o rótulo da classe da *i*-ésima instância.
*   $x_i$ é o vetor de features da *i*-ésima instância.
*   $w$ é o vetor de pesos do modelo.

O gradiente da NLL em relação a $w$ é [^3]:
$$
g = \frac{d}{dw} f(w) = \sum_{i=1} (\mu_i - y_i)x_i = X^T (\mu - y)
$$
onde $\mu_i = \sigma(w^T x_i)$ e $\sigma(z) = \frac{1}{1 + e^{-z}}$.

A Figura 8.2 [^3] ilustra o comportamento do gradient descent em uma função simples, mostrando como a escolha da learning rate influencia a convergência. Uma learning rate pequena (η = 0.1) resulta em uma convergência lenta, enquanto uma learning rate grande (η = 0.6) causa oscilações e impede a convergência.

Para mitigar os problemas associados à escolha de uma learning rate fixa, técnicas como **line search** podem ser utilizadas [^3]. A line search busca, em cada iteração, o tamanho do passo que minimiza a função de custo ao longo da direção do gradiente. Formalmente, o objetivo é encontrar:
$$
\eta^* = \arg \min_{\eta > 0} \phi(\eta)
$$
onde $\phi(\eta) = f(\theta_k + \eta d_k)$ e $d_k$ é a direção de descida (tipicamente, o negativo do gradiente).

A Figura 8.3 [^3] demonstra que, embora a line search possa melhorar a convergência, o algoritmo pode exibir um comportamento de "zig-zag", onde as direções de busca consecutivas são aproximadamente ortogonais. Para reduzir este efeito, um termo de **momentum** pode ser adicionado à regra de atualização [^3]:
$$
\theta_{k+1} = \theta_k - \eta_k g_k + \mu_k (\theta_k - \theta_{k-1})
$$
onde $\mu_k \in [0, 1]$ controla a importância do termo de momentum.

Além do gradient descent padrão, existem outras variantes mais sofisticadas, como o método de Newton [^3]. No entanto, esses métodos geralmente requerem o cálculo da matriz Hessiana, que pode ser computacionalmente caro, especialmente em problemas de alta dimensão.

### Conclusão
O gradient descent é uma ferramenta fundamental na otimização de modelos de regressão logística. A escolha adequada da learning rate e a utilização de técnicas como line search e momentum são cruciais para garantir uma convergência eficiente e robusta. Métodos mais avançados, como o método de Newton, podem oferecer uma convergência mais rápida, mas à custa de maior complexidade computacional. A escolha do algoritmo de otimização mais adequado depende das características específicas do problema em questão, incluindo o tamanho do conjunto de dados, a dimensionalidade do espaço de features e a precisão desejada.

### Referências
[^1]: Capítulo 8, Introdução [Logistic Regression]
[^2]: Seção 8.3.1, MLE [Logistic Regression]
[^3]: Seção 8.3.2, Steepest descent [Logistic Regression]
<!-- END -->