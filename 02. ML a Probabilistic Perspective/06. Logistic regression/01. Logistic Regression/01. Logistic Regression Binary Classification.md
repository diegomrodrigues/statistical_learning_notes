## Logistic Regression: A Discriminative Approach to Binary Classification

### Introdução
Este capítulo explora a **regressão logística** como um classificador discriminativo para problemas de classificação binária. Diferentemente dos métodos generativos, que modelam $p(x|y)$ e $p(y)$ separadamente, a regressão logística modela diretamente a probabilidade $p(y|x)$ [^8]. Este capítulo detalhará a especificação do modelo, os algoritmos para ajuste do modelo e outras considerações importantes.

### Conceitos Fundamentais

A regressão logística é um modelo de classificação binária que estima a probabilidade de uma instância pertencer a uma determinada classe [^8]. O modelo utiliza uma **função sigmoide** aplicada a uma combinação linear das *features* de entrada [^8]. Matematicamente, a probabilidade é dada por:

$$ p(y|x, w) = \text{Ber}(y|\text{sigm}(w^T x)) $$

onde:
*   $y$ é a variável alvo binária (geralmente 0 ou 1).
*   $x$ é o vetor de *features* de entrada.
*   $w$ é o vetor de pesos do modelo.
*   $\text{sigm}(z) = \frac{1}{1 + e^{-z}}$ é a função sigmoide.
*   $\text{Ber}(y|p)$ representa a distribuição de Bernoulli.

A **função sigmoide** mapeia a combinação linear das *features* para um valor entre 0 e 1, representando a probabilidade de $y = 1$ dado $x$ e $w$ [^8].

**Decisão de Classificação:**

Para classificar uma nova instância, a probabilidade estimada é comparada a um *threshold*, geralmente 0.5 [^8]. Se $\text{sigm}(w^T x) \geq 0.5$, a instância é classificada como pertencente à classe 1; caso contrário, é classificada como pertencente à classe 0 [^8]. Este *threshold* de 0.5 induz uma **fronteira de decisão linear**, onde $w$ é normal (perpendicular) a esta fronteira [^8].

A regressão logística simplifica o ajuste do modelo ao focar na fronteira de decisão, em vez de modelar a distribuição subjacente dos dados [^8].

**Ajuste do Modelo (Model Fitting):**

O objetivo do ajuste do modelo de regressão logística é encontrar o vetor de pesos $w$ que melhor se ajusta aos dados de treinamento [^8]. Isso é geralmente feito maximizando a verossimilhança (likelihood) dos dados, ou equivalentemente, minimizando a **negative log-likelihood (NLL)** [^8]. A NLL para regressão logística é dada por [^8]:

$$ \text{NLL}(w) = -\sum_{i=1}^N \left[ y_i \log \mu_i + (1 - y_i) \log (1 - \mu_i) \right] $$

onde $\mu_i = \text{sigm}(w^T x_i)$ é a probabilidade prevista para a *i*-ésima instância. Esta função também é conhecida como a **cross-entropy error function**.

Uma forma alternativa de escrever a NLL, usando $\tilde{y_i} \in \{-1, +1\}$ em vez de $y_i \in \{0,1\}$, é:

$$ \text{NLL}(w) = \sum_{i=1}^N \log(1 + \exp(-\tilde{y_i} w^T x_i)) $$

Diferentemente da regressão linear, a MLE (Maximum Likelihood Estimate) para regressão logística não tem uma forma fechada [^8]. Portanto, algoritmos de otimização são necessários para encontrar o $w$ que minimiza a NLL [^8].

**Algoritmos de Otimização:**

Vários algoritmos podem ser usados para otimizar a NLL da regressão logística, incluindo:

1.  **Gradient Descent (Steepest Descent):** Um algoritmo iterativo que atualiza os pesos na direção oposta do gradiente da NLL [^8]. A atualização é dada por:

    $$     \theta_{k+1} = \theta_k - \eta_k g_k     $$

    onde $\eta_k$ é o *step size* ou *learning rate*, e $g_k = \frac{d}{dw} f(w)$ é o gradiente [^8]. A escolha do *step size* é crucial para a convergência do algoritmo [^8].
2.  **Newton's Method:** Um algoritmo de segunda ordem que usa a informação da curvatura (Hessiana) da NLL para convergir mais rapidamente para o mínimo [^8]. A atualização é dada por:

    $$     \theta_{k+1} = \theta_k - \eta_k H_k^{-1} g_k     $$

    onde $H_k$ é a matriz Hessiana da NLL [^8].
3.  **Iteratively Reweighted Least Squares (IRLS):** Um algoritmo específico para regressão logística que usa uma aproximação de mínimos quadrados ponderados iterativamente para encontrar o MLE [^8].
4.  **Quasi-Newton (Variable Metric) Methods:** Métodos que iterativamente constroem uma aproximação para a Hessiana usando informações coletadas do vetor gradiente a cada passo [^8].

**Regularização:**

Assim como na regressão linear, a regularização é importante na regressão logística para evitar *overfitting*, especialmente quando os dados são linearmente separáveis [^8]. A **regularização $l_2$** (ridge regression) adiciona um termo de penalidade à NLL, proporcional ao quadrado da norma do vetor de pesos [^8]:

$$ f'(w) = \text{NLL}(w) + \lambda w^T w $$

onde $\lambda$ é o parâmetro de regularização.

### Conclusão

A regressão logística é uma ferramenta poderosa e amplamente utilizada para problemas de classificação binária [^8]. Sua capacidade de modelar diretamente a probabilidade $p(y|x)$, juntamente com sua interpretabilidade e facilidade de implementação, a tornam uma escolha popular em diversas aplicações [^8]. A escolha apropriada do algoritmo de otimização e a inclusão de técnicas de regularização são cruciais para obter um modelo preciso e generalizável [^8].

### Referências
[^8]: Página 245-254 do texto fornecido.
<!-- END -->