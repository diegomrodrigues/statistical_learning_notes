## Line Search in Logistic Regression

### Introdução
Na otimização de modelos como a Regressão Logística, o ajuste do tamanho do passo (*step size* ou *learning rate*) é crucial para uma convergência eficiente e estável. A técnica de **line search** [^1] oferece uma abordagem para determinar o tamanho do passo ideal em cada iteração, minimizando a função objetivo ao longo da direção do gradiente. Este capítulo explora a aplicação e as nuances da *line search* no contexto da Regressão Logística.

### Conceitos Fundamentais

A Regressão Logística, conforme discutido na Seção 1.4.6 [^1], corresponde ao seguinte modelo de classificação binária:

$$p(y|x, w) = Ber(y|sigm(w^Tx))$$

onde $Ber$ representa a distribuição de Bernoulli e $sigm$ é a função sigmoide. O objetivo é estimar os parâmetros $w$ do modelo. Ao contrário da regressão linear, a Regressão Logística não possui uma solução analítica fechada para a estimativa de máxima verossimilhança (MLE). Portanto, recorremos a algoritmos de otimização iterativos.

**Gradient Descent** é um dos algoritmos mais simples para otimização não-restrita [^3], e pode ser escrito como:

$$\theta_{k+1} = \theta_k - \eta_k g_k$$

onde $\eta_k$ é o tamanho do passo (ou *learning rate*) e $g_k$ é o gradiente. A escolha apropriada de $\eta_k$ é fundamental. Se $\eta_k$ for muito pequeno, a convergência será lenta; se for muito grande, o método pode não convergir [^3].

A *line search* surge como uma solução para este problema, buscando otimizar $\eta$ em cada iteração [^1]. Isso envolve minimizar a função $\phi(\eta) = f(\theta_k + \eta d_k)$, onde $d_k$ é a direção de descida (geralmente o negativo do gradiente).

A *line search* busca um $\eta$ que minimize $\phi(\eta) = f(\theta_k + \eta d_k)$ [^4]. Pelo Teorema de Taylor, temos:

$$f(\theta + \eta d) \approx f(\theta) + \eta g^T d$$

onde $d$ é a direção de descida. Se $\eta$ for suficientemente pequeno, então $f(\theta + \eta d) < f(\theta)$, pois o gradiente será negativo. No entanto, não queremos escolher um $\eta$ excessivamente pequeno, pois a convergência seria muito lenta [^4].

Essa técnica também é conhecida como *line minimization* [^4]. Existem vários métodos para resolver este problema de otimização 1D; para detalhes, consulte (Nocedal and Wright 2006) [^4].

A Figura 8.3(a) [^4] demonstra que a *line search* funciona no problema simples apresentado. No entanto, a trajetória de *steepest descent* com *line search* exibe um comportamento característico de **zig-zag**. Isso ocorre porque uma *line search* exata satisfaz $\phi'(\eta) = 0$. Pela regra da cadeia, $\phi'(\eta) = d^T g$, onde $g = f'(\theta + \eta d)$ é o gradiente ao final do passo. Assim, temos $g \perp d$, o que significa que a busca exata termina em um ponto onde o gradiente local é perpendicular à direção de busca. Consequentemente, direções consecutivas serão ortogonais [^4].

Uma heurística simples para reduzir o efeito do *zig-zag* é adicionar um termo de **momentum** [^4], $(\theta_k - \theta_{k-1})$, como segue:

$$\theta_{k+1} = \theta_k - \eta_k g_k + \mu_k(\theta_k - \theta_{k-1})$$

onde $0 \leq \mu_k \leq 1$ controla a importância do termo de momentum. Na comunidade de otimização, isso é conhecido como o método da *heavy ball* (ver e.g., (Bertsekas 1999)) [^5].

### Conclusão

A *line search* é uma ferramenta valiosa na otimização da Regressão Logística, permitindo um ajuste adaptativo do tamanho do passo. Embora possa apresentar um comportamento de *zig-zag*, a adição de um termo de momentum pode mitigar esse efeito. A escolha entre diferentes métodos de *line search* e a calibração do termo de momentum dependem das características específicas do problema e podem ser otimizadas empiricamente.

### Referências
[^1]: Página 245 do documento fornecido.
[^3]: Página 247 do documento fornecido.
[^4]: Página 248 do documento fornecido.
[^5]: Página 249 do documento fornecido.
<!-- END -->