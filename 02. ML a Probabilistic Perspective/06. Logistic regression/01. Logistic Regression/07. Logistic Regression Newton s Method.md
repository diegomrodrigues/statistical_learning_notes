## Newton's Method in Logistic Regression

### Introdução
Este capítulo explora o uso do **método de Newton** para otimizar os parâmetros em modelos de regressão logística. Como vimos anteriormente, a regressão logística é um modelo discriminativo linear nos parâmetros [^1]. Diferentemente da regressão linear, a regressão logística não possui uma solução analítica fechada para a estimativa de máxima verossimilhança (MLE) [^2]. Portanto, algoritmos de otimização iterativos são necessários. O método de Newton oferece uma alternativa ao *gradient descent*, potencialmente convergindo mais rapidamente, embora com um custo computacional maior [^1].

### Conceitos Fundamentais
O **método de Newton** é um algoritmo de otimização de segunda ordem que utiliza o gradiente e a matriz Hessiana (matriz das segundas derivadas) para encontrar o mínimo de uma função [^1]. No contexto da regressão logística, busca-se minimizar a *negative log-likelihood* (NLL) [^1]. A NLL para regressão logística é dada por [^2]:
$$ NLL(w) = \sum_{i=1}^{N} log(1 + exp(-y_i w^T x_i)) $$
onde $y_i \in \{-1, +1\}$ e $w$ representa o vetor de pesos.

O método de Newton utiliza a curvatura (Hessiana) da função NLL para encontrar o mínimo [^1]. O gradiente $g$ e a Hessiana $H$ da NLL são dados por [^3]:
$$ g = \frac{d}{dw} f(w) = \sum_{i} (\mu_i - y_i)x_i = X^T (\mu - y) $$
$$ H = \frac{d}{dw} g(w) = \sum_{i} (\nabla_w \mu_i) x_i^T = \sum_{i} \mu_i (1 - \mu_i) x_i x_i^T = X^T S X $$
onde $S$ é uma matriz diagonal com $S_{ii} = \mu_i (1 - \mu_i)$ [^3].

A regra de atualização iterativa para o método de Newton é dada por [^1]:
$$ \theta_{k+1} = \theta_k - \eta (H^{-1} g_k) $$
onde $\theta_k$ representa os parâmetros na iteração $k$, $\eta$ é a *learning rate*, $H$ é a matriz Hessiana, e $g_k$ é o gradiente [^1].  No contexto da regressão logística, a atualização de Newton na iteração $k+1$ é dada por [^6]:
$$ w_{k+1} = w_k + (X^T S_k X)^{-1} X^T (y - \mu_k) $$
Essa atualização envolve o cálculo da inversa da Hessiana, o que pode ser computacionalmente caro, especialmente para grandes conjuntos de dados [^1]. No entanto, para funções estritamente convexas, o método de Newton geralmente converge mais rapidamente do que o *gradient descent* [^1].

O método de Newton requer que a Hessiana $H_k$ seja positiva definida [^6]. Se a função não for estritamente convexa, $H_k$ pode não ser positiva definida, e a direção de busca $d_k = -H_k^{-1}g_k$ pode não ser uma direção de descida [^6]. Nesses casos, uma estratégia comum é reverter para o *steepest descent* ou usar algoritmos como o Levenberg-Marquardt, que combinam passos de Newton com passos de *steepest descent* [^6]. Outra abordagem é resolver o sistema linear $H_k d_k = -g_k$ para $d_k$ usando o gradiente conjugado (CG), truncando as iterações do CG assim que uma curvatura negativa for detectada; isso é chamado de *truncated Newton* [^6].

Uma aplicação específica do método de Newton na regressão logística leva ao algoritmo *Iteratively Reweighted Least Squares* (IRLS) [^6]. O método IRLS envolve a resolução iterativa de um problema de mínimos quadrados ponderados, onde a matriz de pesos $S_k$ é atualizada a cada iteração [^6, 7].

### Conclusão
O método de Newton oferece uma alternativa poderosa ao *gradient descent* para otimizar modelos de regressão logística, especialmente quando a convergência rápida é crucial e o custo computacional da Hessiana é aceitável. No entanto, é importante estar ciente das limitações do método, como a necessidade de uma Hessiana positiva definida e o custo de calcular sua inversa.  Alternativas como o IRLS e métodos *Quasi-Newton* (BFGS) podem oferecer um bom compromisso entre velocidade de convergência e custo computacional [^7].

### Referências
[^1]: Capítulo 8. Logistic regression
[^2]: Seção 8.3.1 MLE
[^3]: Seção 8.3.2 Steepest descent
[^6]: Seção 8.3.4 Iteratively reweighted least squares (IRLS)
[^7]: Seção 8.3.5 Quasi-Newton (variable metric) methods
<!-- END -->