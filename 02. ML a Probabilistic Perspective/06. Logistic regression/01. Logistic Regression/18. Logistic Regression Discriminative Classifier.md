## Classificadores Generativos vs. Discriminativos na Regressão Logística

### Introdução
Na regressão logística, uma distinção crucial reside entre **classificadores generativos** e **discriminativos** [^1]. Este capítulo explora essa dicotomia, detalhando como cada abordagem modela o problema de classificação e as implicações para o ajuste do modelo e o desempenho. Como veremos, a regressão logística se enquadra na categoria de classificadores discriminativos, modelando diretamente a probabilidade condicional da classe *dado* os atributos de entrada.

### Conceitos Fundamentais

**Classificadores Generativos**

Um classificador generativo constrói um modelo conjunto da forma $p(x, y)$, onde $x$ representa os atributos de entrada e $y$ a classe. A probabilidade condicional $p(y|x)$, usada para a classificação, é então derivada através do teorema de Bayes:

$$p(y|x) = \frac{p(x, y)}{p(x)} = \frac{p(x|y)p(y)}{p(x)}$$

Nesse contexto, $p(x|y)$ representa a distribuição dos atributos *dada* a classe, e $p(y)$ a probabilidade *a priori* da classe. A modelagem de $p(x|y)$ exige assumir uma distribuição específica para os atributos de entrada, como uma Gaussiana no caso da Análise Discriminante Gaussiana (GDA).

**Classificadores Discriminativos**

Em contraste, um classificador discriminativo modela diretamente a probabilidade condicional $p(y|x)$ sem modelar a distribuição conjunta $p(x, y)$ [^1]. A regressão logística é um exemplo paradigmático de um classificador discriminativo. Ela estima diretamente a probabilidade de uma classe *dado* os atributos de entrada através da função sigmoide:

$$p(y|x, w) = \text{Ber}(y|\sigma(w^Tx))$$

onde $\sigma(z) = \frac{1}{1 + e^{-z}}$ é a função sigmoide e $w$ é o vetor de pesos do modelo [^1]. A regressão logística não faz nenhuma suposição sobre a distribuição dos atributos de entrada $x$.

**Regressão Logística como Classificador Discriminativo**

A regressão logística modela a probabilidade de uma classe (por exemplo, $y=1$) *dado* os atributos de entrada ($x$) usando a função sigmoide [^1]:

$$p(y=1|x, w) = \sigma(w^Tx) = \frac{1}{1 + e^{-w^Tx}}$$

O vetor de pesos $w$ é aprendido a partir dos dados de treinamento, maximizando a verossimilhança condicional.

**Ajuste do Modelo**

O ajuste do modelo de regressão logística envolve encontrar os valores ótimos para o vetor de pesos $w$. Isso é tipicamente feito através da maximização da função de verossimilhança condicional, ou equivalentemente, minimizando a função de *negative log-likelihood* (NLL) [^2]:

$$NLL(w) = -\sum_{i=1}^{N} [y_i \log \mu_i + (1 - y_i) \log (1 - \mu_i)]$$

onde $\mu_i = p(y_i = 1|x_i, w)$ e $N$ é o número de amostras de treinamento [^2]. Ao contrário da regressão linear, a regressão logística não possui uma solução analítica fechada para a MLE (estimativa de máxima verossimilhança). Portanto, algoritmos de otimização iterativos, como *gradient descent*, *Newton's method*, ou *iteratively reweighted least squares* (IRLS), são empregados para encontrar o vetor de pesos $w$ que minimiza a NLL [^2].

**Gradient Descent**

O algoritmo de *gradient descent* atualiza os pesos iterativamente na direção oposta do gradiente da função de custo [^3]:

$$theta_{k+1} = \theta_k - \eta_k g_k$$

onde $\theta_k$ representa os pesos na iteração $k$, $\eta_k$ é a *learning rate* (ou *step size*), e $g_k$ é o gradiente da NLL [^3]. A escolha apropriada da *learning rate* é crucial para garantir a convergência do algoritmo.

**Newton's Method**

O *Newton's method* é um algoritmo de otimização de segunda ordem que utiliza a informação da curvatura da função de custo (Hessiana) para convergir mais rapidamente para o mínimo [^5]:

$$theta_{k+1} = \theta_k - \eta_k H_k^{-1} g_k$$

onde $H_k$ é a matriz Hessiana da NLL [^5]. Embora o *Newton's method* possa convergir mais rapidamente que o *gradient descent*, ele é computacionalmente mais caro, pois requer o cálculo e a inversão da matriz Hessiana.

**Iteratively Reweighted Least Squares (IRLS)**

O IRLS é um algoritmo específico para a regressão logística que se baseia na resolução iterativa de um problema de mínimos quadrados ponderados [^7]:

$$w = (X^TSX)^{-1}X^TSz$$

onde $S$ é uma matriz diagonal de pesos e $z$ é uma variável de resposta de trabalho [^7]. O IRLS é equivalente ao *Newton's method* para a regressão logística.

**Regularização**

Para evitar o *overfitting*, a regularização é frequentemente adicionada à função de custo. A regularização $l_2$ adiciona um termo de penalidade proporcional ao quadrado da norma dos pesos [^8]:

$$f'(w) = NLL(w) + \lambda w^T w$$

onde $\lambda$ é o parâmetro de regularização. A regularização $l_2$ tende a produzir pesos menores, o que leva a modelos mais simples e generalizáveis.

### Conclusão

A regressão logística, como um classificador discriminativo, oferece uma abordagem direta e eficiente para a modelagem da probabilidade condicional da classe *dado* os atributos de entrada. Sua flexibilidade, combinada com algoritmos de otimização robustos e técnicas de regularização, a tornam uma ferramenta poderosa para uma ampla gama de problemas de classificação. A escolha entre classificadores generativos e discriminativos depende do problema em questão, das suposições que se pode fazer sobre os dados e do trade-off entre a precisão do modelo e a interpretabilidade.

### Referências
[^1]: Page 245, Section 8.1: Introduction
[^2]: Page 246, Section 8.3.1: MLE
[^3]: Page 247, Section 8.3.2: Steepest descent
[^5]: Page 249, Section 8.3.3: Newton's method
[^7]: Page 251, Section 8.3.4: Iteratively reweighted least squares (IRLS)
[^8]: Page 252, Section 8.3.6: $l_2$ regularization
<!-- END -->