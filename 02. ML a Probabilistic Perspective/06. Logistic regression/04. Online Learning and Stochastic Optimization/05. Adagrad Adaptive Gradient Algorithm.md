## Adagrad: Adaptive Gradient Descent

### Introdução
No contexto de **Online Learning and Stochastic Optimization**, algoritmos de gradiente adaptativos representam uma classe de métodos que ajustam as taxas de aprendizado para cada parâmetro individualmente, permitindo uma convergência mais eficiente. Adagrad é um desses algoritmos, que se destaca por sua capacidade de adaptar as taxas de aprendizado com base na curvatura da função de perda [^263].

### Conceitos Fundamentais

**Adagrad** (abreviação de adaptive gradient) é um método que utiliza tamanhos de passo diferentes para cada parâmetro, adaptando-se à curvatura da função de perda [^263]. A atualização dos parâmetros no Adagrad é dada por [^263]:

$$theta_i(k + 1) = \theta_i(k) - \eta \cdot \frac{g_i(k)}{\tau_0 + \sqrt{s_i(k)}}$$

onde:
- $\theta_i(k)$ é o parâmetro *i* no tempo *k*.
- $\eta$ é a taxa de aprendizado global.
- $g_i(k)$ é o gradiente do parâmetro *i* no tempo *k*.
- $\tau_0$ é um termo de regularização para evitar divisão por zero.
- $s_i(k)$ é a soma dos quadrados dos gradientes até o tempo *k*, que é atualizada recursivamente como [^263]:

$$s_i(k) = s_i(k - 1) + g_i(k)^2$$

A principal vantagem do Adagrad é que ele adapta a taxa de aprendizado para cada parâmetro com base em seu histórico de gradientes. Parâmetros com gradientes maiores no passado terão taxas de aprendizado menores, enquanto parâmetros com gradientes menores terão taxas de aprendizado maiores [^263]. Isso permite que o algoritmo se ajuste à curvatura da função de perda, acelerando a convergência.

**Comparação com SGD:**
Uma desvantagem do Stochastic Gradient Descent (**SGD**) é que ele usa o mesmo tamanho de passo para todos os parâmetros, o que pode ser subótimo [^263]. Adagrad, por outro lado, usa tamanhos de passo por parâmetro que se adaptam à curvatura da função de perda [^263].

**Vantagens e Desvantagens:**
- *Vantagem:* Adaptação automática das taxas de aprendizado por parâmetro.
- *Desvantagem:* A taxa de aprendizado pode diminuir muito rapidamente, especialmente em problemas onde os gradientes iniciais são grandes, o que pode impedir a convergência [^263].

### Conclusão
Adagrad representa uma importante evolução em algoritmos de otimização, oferecendo uma adaptação automática das taxas de aprendizado para cada parâmetro. Embora possua limitações, como a diminuição rápida da taxa de aprendizado, ele serve como base para algoritmos mais sofisticados, como o Adam, que abordam essas desvantagens.

### Referências
[^263]: Page 263, "Pattern Recognition and Machine Learning", Christopher Bishop
<!-- END -->