## Discriminative vs. Generative Classifiers: Modeling Approaches

### Introdução
Este capítulo explora a distinção fundamental entre **classificadores generativos** e **classificadores discriminativos**, com foco especial na modelagem da probabilidade condicional $p(y|x)$ e da probabilidade conjunta $p(x, y)$ [^1]. Classificadores como a regressão logística pertencem à categoria discriminativa, enquanto outros, como o Naive Bayes e o Discriminant Analysis, são generativos. A escolha entre esses dois tipos de classificadores depende das características do problema e dos dados disponíveis.

### Conceitos Fundamentais

A diferença essencial entre classificadores generativos e discriminativos reside na forma como eles abordam a modelagem da relação entre as variáveis de entrada ($x$) e a variável de saída ($y$).

**Classificadores Generativos:**

*   Modelam a **probabilidade conjunta** $p(x, y)$ [^1].
*   Aprendem a distribuição de cada classe e como os dados são gerados dentro de cada classe.
*   Podem gerar novos dados semelhantes aos dados de treinamento.
*   Exemplos: Naive Bayes, Gaussian Discriminant Analysis (GDA).
*   Para fazer uma previsão, usam a regra de Bayes para calcular a probabilidade *a posteriori* $p(y|x)$:

$$p(y|x) = \frac{p(x, y)}{p(x)} = \frac{p(x|y)p(y)}{\sum_{y'} p(x|y')p(y')}$$

**Classificadores Discriminativos:**

*   Modelam diretamente a **probabilidade condicional** $p(y|x)$ [^1].
*   Aprendem a fronteira de decisão entre as classes, sem necessariamente modelar a distribuição dos dados.
*   Não podem gerar novos dados.
*   Exemplos: Regressão Logística, Support Vector Machines (SVMs), árvores de decisão.

A regressão logística, como mencionado na introdução do capítulo, se enquadra na categoria dos classificadores discriminativos [^1]. Conforme discutido na Seção 1.4.6, a regressão logística corresponde ao seguinte modelo de classificação binária:

$$p(y|x, w) = Ber(y|sigm(w^Tx))$$

Onde $Ber$ é a distribuição de Bernoulli e $sigm(w^Tx)$ é a função sigmoide aplicada ao produto interno do vetor de pesos $w$ e do vetor de entrada $x$. Um exemplo 1D é mostrado na Figura 1.19(b) e a regressão logística pode ser facilmente estendida para entradas de dimensões superiores. Por exemplo, a Figura 8.1 mostra gráficos de $p(y = 1|x, w) = sigm(w^Tx)$ para entrada 2D e diferentes vetores de peso $w$. Se definirmos um limiar para essas probabilidades em 0,5, induzimos um limite de decisão linear, cuja normal (perpendicular) é dada por $w$.

**Vantagens e Desvantagens:**

A escolha entre classificadores generativos e discriminativos depende de vários fatores:

*   **Quantidade de dados:** Classificadores generativos podem ter um desempenho melhor com menos dados, pois fazem suposições mais fortes sobre a distribuição dos dados.
*   **Precisão das suposições:** Se as suposições do modelo generativo forem precisas, ele pode superar os classificadores discriminativos. No entanto, se as suposições forem violadas, os classificadores discriminativos tendem a ser mais robustos.
*   **Facilidade de ajuste:** Classificadores generativos, como Naive Bayes e LDA, geralmente são mais fáceis de ajustar do que classificadores discriminativos como a regressão logística [^2].
*   **Flexibilidade:** Classificadores discriminativos permitem pré-processar a entrada de maneiras arbitrárias, por exemplo, substituir $x$ por $\phi(x)$, que poderia ser uma expansão de função de base [^2].

### Conclusão

Classificadores generativos e discriminativos representam abordagens distintas para o problema de classificação. Classificadores discriminativos, como a regressão logística, modelam diretamente a probabilidade condicional $p(y|x)$, aprendendo a fronteira de decisão entre as classes [^1]. Classificadores generativos, por outro lado, modelam a probabilidade conjunta $p(x, y)$, aprendendo a distribuição de cada classe [^1]. A escolha entre esses dois tipos de classificadores depende das características do problema, da quantidade de dados disponíveis e da precisão das suposições feitas sobre a distribuição dos dados.

### Referências
[^1]: Seção 8.1, *Logistic regression*
[^2]: Seção 8.6.1, *Logistic regression*
<!-- END -->