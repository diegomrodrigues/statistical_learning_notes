{
  "topics": [
    {
      "topic": "Latent Variable Models for Discrete Data",
      "sub_topics": [
        "Latent variable models (LVMs) are used for discrete data such as bit vectors, categorical variable sequences, count vectors, graph structures, and relational data. They enable the analysis of voting records, text and document collections, low-intensity images, and movie ratings. The goal is to build joint probability models, denoted as p(yi) or p(ni), using latent variables to capture correlations and provide a compressed representation of the data, enabling interpretation of underlying relationships and reducing data dimensionality. Sparse discrete matrices, such as graphs and relations, can be modeled to represent relationships between entities, like papers citing other papers, using relations denoted by R and categorical data (e.g., text) associated with nodes denoted by Y. The bag of words approach simplifies text analysis by ignoring word order and representing documents as fixed-length vectors of word counts, captured in a sparse N x V count matrix where N is documents and V is vocabulary size."
      ]
    },
    {
      "topic": "Distributed State LVMs for Discrete Data",
      "sub_topics": [
        "Distributed state LVMs construct models of the form p(yi,1:Li) for bags of tokens, p(yi,1:R) for vectors of tokens, and p(ni) for vectors of integer counts. Mixture models associate a discrete latent variable qi with each document to represent cluster membership, using a discrete prior Cat(\u03c0) and defining the probability of a word given a cluster as bkv, where bkv is the probability that cluster k generates word v. The likelihood function is given by P(yi,1:Lz|qi = k) = II Cat(Yil|bk), and the induced distribution on the visible data is P(yi,1:Lz) = \u03a3\u03c0k Cat(Yil|bk), with a generative story involving picking a topic and then picking words from that topic's distribution. Exponential Family PCA (ePCA) uses real-valued continuous latent variables, similar to factor analysis and PCA, but with a multinoulli or multinomial distribution and changing the likelihood to P(yi,1:Li|zi) = \u03a0 Cat(yil|S(Wzi)), where W is a weight matrix and S is the softmax function. Categorical PCA models categorical data using a multinoulli or multinomial distribution, and is an unsupervised analog of naive Bayes classification. Multinomial PCA (mPCA) uses dual parameters, where the dual parameter is the probability vector and the natural parameter is the vector of log odds; it constrains latent variables to live in the appropriate parameter space, ensuring the latent vector lives in SK, the K-dimensional probability simplex. Fitting ePCA models is challenging due to the lack of conjugacy, often requiring coordinate ascent methods or variational EM to marginalize out the latent variables zi, where True EM and MCMC algorithms can be employed."
      ]
    },
    {
      "topic": "Latent Dirichlet Allocation (LDA)",
      "sub_topics": [
        "Latent Dirichlet allocation (LDA) is a probabilistic extension of LSA where latent quantities are non-negative and sum to one, addressing the interpretability issues of LSA by ensuring non-negative latent quantities that sum to one. LDA assigns every document to a distribution over topics, \u03c0i, and every word to a topic-specific distribution, qil, drawn from \u03c0i, creating an admixture or mixed membership model. In LDA, every document is modeled as an admixture over topics, where each document vector defines a distribution over K topics, and each topic vector defines a distribution over V words, enabling a form of dimensionality reduction. LDA addresses polysemy by allowing multiple topics to generate the same word, reflecting the ambiguity in natural language where words can have multiple meanings depending on the context. LDA can be quantitatively evaluated as a language model by assessing its perplexity, which measures its ability to predict sequences of words. The geometric interpretation of LDA involves projecting documents from a V-dimensional simplex (normalized document count vector) onto a K-dimensional simplex (distribution over K topics), effectively reducing dimensionality. The full LDA model incorporates conjugate priors: \u03c0i ~ Dir(\u03b11K), qil ~ Cat(\u03c0i), bkv ~ Dir(\u03b31V), and yil|qil = k, B ~ Cat(bk), allowing for marginalization of the qi variables. Gibbs sampling is a straightforward method for fitting LDA, involving iteratively sampling topics for each word based on the topic's word distribution and the topic's usage in the document. Variational inference provides a faster alternative to MCMC for fitting LDA, using a fully factorized approximation and iteratively updating variational parameters to estimate topic distributions and model parameters."
      ]
    },
    {
      "topic": "Extensions of LDA",
      "sub_topics": [
        "Correlated Topic Model (CTM) extends LDA to capture correlations between topics by replacing the Dirichlet prior with a logistic normal distribution, allowing for a more flexible covariance structure. Dynamic Topic Model allows topic distributions to evolve smoothly over time, capturing how word usage and topic relevance change, using a Gaussian random walk to model topic evolution. LDA-HMM combines LDA with Hidden Markov Models to model sequential dependence between words, using HMM states for syntactic words and LDA for semantic content, capturing both syntax and overall document gist. Supervised LDA generates class labels from topics, enabling sentiment analysis by capturing overall sentiment, and uses empirical topic distribution for document i as a topic distribution. Discriminative categorical PCA expands categorical PCA models with inputs, using linear regression for input-hidden mapping and traditional catPCA for hidden-output mapping, creating a probabilistic neural network for handling exchangeable outputs."
      ]
    },
    {
      "topic": "LVMs for Graph-Structured Data",
      "sub_topics": [
        "Graphs can be represented as adjacency matrices, enabling the modeling of relationships in social networks, protein interactions, and disease transmission. The stochastic block model partitions nodes into groups to reveal underlying structures, where connections go from nodes in one group to another, and this is different from a conventional clustering problem. Mixed membership stochastic block models allow nodes to belong to multiple clusters, capturing the uncertainty in cluster assignments, and use a Dirichlet distribution to model the distribution over blocks. The relational topic model combines text and link structure, predicting links based on document text, and can be used to create a model that explains text and link structure concurrently. The infinite relational model extends stochastic block models to relational data and uses a Dirichlet process to allow the number of clusters for each type to be unbounded. Probabilistic matrix factorization is used for collaborative filtering, predicting entries in a matrix, enabling personalized recommendations. In relational topic models, it is important that Rij depends on the actual topics chosen, qi and qj, and not on the topic distributions, \u03c0i and \u03c0j, to improve predictive performance by capturing graph structure information."
      ]
    },
    {
      "topic": "LVMs for Relational Data",
      "sub_topics": [
        "Relational data represents relations among variables of a certain type, often involving multiple types of objects and relations, which can be defined as a k-ary relation R \u2286 T1 \u00d7 T2 \u00d7 ... \u00d7 Tk. Probabilistic matrix factorization is used for collaborative filtering, predicting entries in a rating matrix, and has been used to predict user rating for movies. Probabilistic Matrix Factorization (PMF) involves embedding users and movies into a low-dimensional continuous space, where the proximity of users to movies indicates the likelihood of high ratings. The infinite relational model (IRM) extends the stochastic block model to relational data by associating a latent variable qt with each entity i of each type t and defining the probability of the relation holding between specific entities. Real-valued latent factors (ui \u2208 RK and vi \u2208 RK) are used to model user and movie preferences, where the likelihood is p(Rij = r|ui, vj) = N(r|uvj, \u03c3\u00b2), and the model is fitted by minimizing the negative log-likelihood using stochastic gradient descent."
      ]
    },
    {
      "topic": "Restricted Boltzmann Machines (RBMs)",
      "sub_topics": [
        "Restricted Boltzmann Machines (RBMs) are undirected graphical models with pairwise Markov Random Fields (MRF), with hidden nodes and visible nodes, and have a bipartite structure. RBMs are a special case of a product of experts (PoE), where experts are potential functions on each edge, and hidden nodes specify which constraints are active. Contrastive Divergence (CD) is a method for approximating the gradient in RBM learning, using brief Gibbs sampling to estimate the unclamped expectations. CD-1 training in RBMs involves initializing weights, iterating through epochs, computing gradients, and updating parameters to learn the underlying data distribution. RBMs can be used for language modeling by defining a generative model for bag-of-words, using a categorical RBM with tied weights and multiplying the hidden activation bias terms. Varieties of RBMs include binary, Gaussian, and categorical RBMs, each suited for different types of data, serving as special cases of the exponential family harmonium. Learning RBMs involves computing maximum likelihood parameter estimates using gradient-based optimizers and contrastive divergence, balancing model fit and computational efficiency."
      ]
    },
    {
      "topic": "LDA and MPCA",
      "sub_topics": [
        "In ePCA, Wzi represents the natural parameters of the exponential family; sometimes using dual parameters like probability vectors is more convenient, requiring constrained latent variables in the K-dimensional probability simplex SK. The Dirichlet distribution, \u03c0i ~ Dir(\u03b1), serves as the natural prior for latent variables, with \u03b1 = \u03b11k encouraging sparsity when \u03b1 \u226a 1, as commonly used in multinomial PCA (mPCA). The likelihood function for mPCA, p(ni|Li, \u03c0i) = Mu(ni|Li, B\u03c0i), models count vectors with a known total sum, where B is a matrix of topic-word distributions, and the corresponding marginal distribution involves an intractable integral. Latent Dirichlet allocation (LDA) extends LSA probabilistically, ensuring non-negative latent quantities that sum to one, unlike LSA's potentially negative values which complicate interpretation. Probabilistic latent semantic indexing (PLSI), a predecessor to LDA, computes a point estimate of \u03c0i for each document, similar to ePCA, without integrating it out or using a prior for \u03c0i. The user rating profile (URP) model and simplex factor model are modifications of LDA to handle a fixed number of different categorical responses, defining the likelihood as p(yi,1:R|\u03c0i) = \u03a0 Cat(yil|B(r)\u03c0i)."
      ]
    }
  ]
}