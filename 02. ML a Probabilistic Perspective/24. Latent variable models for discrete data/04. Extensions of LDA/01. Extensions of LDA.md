## Extensions of LDA: Correlated, Dynamic, and Supervised Topic Models

### Introdução
Este capítulo aprofunda-se em diversas extensões do Latent Dirichlet Allocation (LDA), um modelo fundamental para a modelagem de tópicos em documentos [^949]. As limitações do LDA em capturar correlações entre tópicos, a evolução temporal dos tópicos e a incorporação de informação supervisionada levaram ao desenvolvimento de modelos mais avançados. Exploraremos o Correlated Topic Model (CTM), o Dynamic Topic Model e o Supervised LDA, bem como uma extensão do categorical PCA.

### Conceitos Fundamentais

#### Correlated Topic Model (CTM)
O LDA assume que os tópicos são independentes, o que nem sempre é realista [^961]. Por exemplo, documentos que abordam o tópico de "negócios" frequentemente também mencionam "finanças". O Correlated Topic Model (CTM) supera essa limitação substituindo o prior de Dirichlet por uma distribuição normal logística [^961].

O modelo LDA original define a distribuição dos tópicos $\pi_i$ usando um prior de Dirichlet, o que implica uma matriz de covariância fixa [^961]:
$$\Sigma_{ij} = -\alpha_i\alpha_j$$
onde $\alpha$ é um parâmetro de força.

O CTM, por outro lado, modela os tópicos como vetores Gaussianos que são então transformados em probabilidades via a função softmax [^961]. O modelo CTM pode ser expresso como:
$$\begin{aligned}
\gamma &\sim Dir(\mathbf{\eta}) \\\\
\mathbf{z}_i &\sim \mathcal{N}(\boldsymbol{\mu}, \Sigma) \\\\
\boldsymbol{\pi}_i &= S(\mathbf{z}_i) \\\\
q_{il} &\sim Cat(\boldsymbol{\pi}_i) \\\\
y_{il} | q_{il} = k, \mathbf{B} &\sim Cat(\mathbf{b}_k)
\end{aligned}$$
onde:
*   $\gamma$ é o prior para as distribuições de palavras [^961].
*   $\mathbf{z}_i$ são as variáveis latentes Gaussianas [^961].
*   $\boldsymbol{\mu}$ é a média e $\Sigma$ a matriz de covariância [^961].
*   $S(\mathbf{z}_i)$ é a função softmax que transforma $\mathbf{z}_i$ em uma distribuição de probabilidade $\boldsymbol{\pi}_i$ [^961].
*   $q_{il}$ é a atribuição de tópico para a *l*-ésima palavra no documento *i* [^961].
*   $y_{il}$ é a *l*-ésima palavra no documento *i* [^961].
*   $\mathbf{B}$ é a matriz tópico-palavra [^961].
*   $\mathbf{b}_k$ é a distribuição de palavras para o tópico *k* [^961].

A principal diferença entre o CTM e o categorical PCA é que no CTM, após marginalizar $q_{il}$ e $\pi_i$, temos:
$$y_{il} \sim Cat(BS(\mathbf{z}_i))$$
onde B é uma matriz estocástica. No categorical PCA, temos:
$$y_{il} \sim Cat(S(W\mathbf{z}_i))$$
onde W é uma matriz não-constringida [^961].

A inferência no CTM é mais complexa devido à não-conjugação do prior normal logístico [^961]. Variational inference é frequentemente usado, empregando métodos para lidar com regressão logística multiclasse Bayesiana [^961]. Após ajustar o modelo, a matriz de covariância $\Sigma$ pode ser convertida em uma matriz de precisão esparsa ($\Sigma^{-1}$) por meio da poda de arestas de baixa intensidade, permitindo a visualização da correlação entre tópicos [^961].

#### Dynamic Topic Model (DTM)
O LDA assume que os tópicos são estáticos ao longo do tempo [^962]. No entanto, a relevância e o uso das palavras mudam com o tempo. O Dynamic Topic Model (DTM) permite que as distribuições de tópicos evoluam suavemente ao longo do tempo, capturando essas mudanças [^962].

No DTM, as distribuições de tópicos evoluem de acordo com um passeio aleatório Gaussiano [^962]:
$$\begin{aligned}
\mathbf{b}_{t,k} | \mathbf{b}_{t-1,k} &\sim \mathcal{N}(\mathbf{b}_{t-1,k}, \sigma^2 \mathbf{I}_V) \\\\
\boldsymbol{\pi}_t &\sim Dir(\alpha \mathbf{1}_K) \\\\
q_{il} | \boldsymbol{\pi}_t &\sim Cat(\boldsymbol{\pi}_t) \\\\
y_{il} | q_{il} = k, \mathbf{B}_t &\sim Cat(S(\mathbf{b}_{t,k}))
\end{aligned}$$
onde:
*   $\mathbf{b}_{t,k}$ é a distribuição de palavras para o tópico *k* no tempo *t* [^962].
*   $\sigma^2$ controla a suavidade da evolução do tópico [^962].
*   $S(\mathbf{b}_{t,k})$ é a função softmax [^962].
*   $\boldsymbol{\pi}_t$ é a distribuição de tópicos no tempo t [^962].
*   $q_{il}$ é a atribuição de tópico para a *l*-ésima palavra no documento *i* [^962].
*   $y_{il}$ é a *l*-ésima palavra no documento *i* [^962].

A inferência no DTM pode ser realizada usando métodos de variational inference, aproveitando o algoritmo de Kalman smoothing para inferir sobre a cadeia linear Gaussiana entre os nós $\mathbf{b}_{t,k}$ [^963]. O DTM permite a análise da evolução temporal dos tópicos e pode ser usado para recuperação de documentos corrigida temporalmente [^963].

#### LDA-HMM
O LDA assume que as palavras são trocáveis, o que não é verdade na linguagem natural [^963]. O LDA-HMM combina LDA com Hidden Markov Models (HMMs) para modelar a dependência sequencial entre as palavras [^963].

O modelo LDA-HMM utiliza os estados do HMM para palavras sintáticas e o LDA para conteúdo semântico [^963]. O modelo pode ser definido como:
$$\begin{aligned}
\boldsymbol{\pi}_i &\sim Dir(\alpha \mathbf{1}_K) \\\\
q_{il} | \boldsymbol{\pi}_i &\sim Cat(\boldsymbol{\pi}_i) \\\\
\mathbf{z}_{il} | \mathbf{z}_{i,l-1} &\sim Cat(A_{\mathbf{z}_{i,l-1}}) \\\\
y_{il} | q_{il} = k, \mathbf{z}_{il} = c &\sim
\begin{cases}
Cat(\mathbf{b}_k^{LDA}) & \text{if } c = 0 \\\\
Cat(\mathbf{b}_c^{HMM}) & \text{if } c > 0
\end{cases}
\end{aligned}$$
onde:

*   $\mathbf{z}_{il}$ é o estado do HMM para a *l*-ésima palavra no documento *i* [^963].
*   $A$ é a matriz de transição do HMM [^964].
*   $\mathbf{b}_k^{LDA}$ é a distribuição de palavras para o tópico *k* no LDA [^964].
*   $\mathbf{b}_c^{HMM}$ é a distribuição de palavras para o estado *c* no HMM [^964].

A inferência neste modelo pode ser feita com Gibbs sampling colapsado, integrando analiticamente todas as quantidades contínuas [^964]. O LDA-HMM captura tanto a sintaxe quanto o significado geral do documento [^963].

#### Supervised LDA
O LDA pode ser estendido para incorporar informação supervisionada, permitindo que os tópicos gerem rótulos de classe [^967]. O Supervised LDA (sLDA) gera rótulos de classe a partir dos tópicos, permitindo a análise de sentimentos ao capturar o sentimento geral [^967].

No sLDA, o rótulo de classe $c_i$ é gerado a partir da distribuição de tópicos empírica $\bar{q}_i$ [^967]:
$$p(c_i | \bar{q}_i) = Ber(sigm(\mathbf{w}^T \bar{q}_i))$$
onde:
$$\bar{q}_{ik} = \frac{1}{L_i} \sum_{l=1}^{L_i} q_{ilk}$$
*   $Ber$ é a distribuição de Bernoulli [^967].
*   $sigm$ é a função sigmoide [^967].
*   $\mathbf{w}$ são os pesos a serem aprendidos [^967].

O sLDA permite que o modelo aprenda tópicos que são preditivos do rótulo de classe, tornando-o adequado para tarefas como análise de sentimentos e classificação de documentos [^967].

#### Discriminative Categorical PCA
O Discriminative Categorical PCA expande os modelos de categorical PCA com entradas, usando regressão linear para o mapeamento entrada-oculto e o catPCA tradicional para o mapeamento oculto-saída [^969]. Isso cria uma rede neural probabilística para lidar com saídas intercambiáveis [^969].

O modelo pode ser expresso como:
$$\begin{aligned}
p(\mathbf{z}_i | \mathbf{x}_i, V) &= \mathcal{N}(\mathbf{V}\mathbf{x}_i, \Sigma) \\\\
p(\mathbf{y}_i | \mathbf{z}_i, W) &= \prod_{l=1}^{L_i} Cat(y_{il} | S(W\mathbf{z}_i))
\end{aligned}$$
onde:

*   $\mathbf{x}_i$ é o vetor de entrada [^969].
*   $\mathbf{V}$ é a matriz de mapeamento linear [^969].
*   $\mathbf{z}_i$ é a representação latente [^969].
*   $\Sigma$ é a matriz de covariância [^969].
*   $W$ é a matriz de pesos para o mapeamento oculto-saída [^969].
*   $S$ é a função softmax [^969].

### Conclusão
As extensões do LDA discutidas neste capítulo abordam as limitações do modelo original, permitindo uma modelagem mais rica e flexível de tópicos em documentos [^961, 962, 963, 967, 969]. O CTM captura correlações entre tópicos, o DTM modela a evolução temporal dos tópicos, o LDA-HMM incorpora dependências sequenciais entre palavras, e o Supervised LDA integra informação supervisionada para tarefas como análise de sentimentos. Essas extensões, juntamente com modelos como o Discriminative Categorical PCA, expandem significativamente a aplicabilidade da modelagem de tópicos em diversas áreas.

### Referências
[^949]: Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. *Journal of Machine Learning Research, 3*(4-5), 993-1022.
[^961]: Blei, D. M., & Lafferty, J. D. (2007). Correlated topic models. *Advances in Neural Information Processing Systems*, *18*, 147-154.
[^962]: Blei, D. M., & Lafferty, J. D. (2006b). Dynamic topic models. *Proceedings of the 23rd International Conference on Machine Learning*, 113-120.
[^963]: Griffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2004). Topics in semantic representation. *Psychological Review, 114*(2), 211-244.
[^964]: Griffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2004). Topics in semantic representation. *Psychological Review, 114*(2), 211-244.
[^967]: Blei, D. M., & McAuliffe, J. D. (2010). Supervised topic models. *Advances in Neural Information Processing Systems*, *20*, 121-128.
[^969]: Collins, M., Dasgupta, S., & Schapire, R. E. (2002). A generalization of principal components analysis to the exponential family. *Advances in Neural Information Processing Systems*, *14*, 609-616.
<!-- END -->