## Latent Variable Models for Discrete Data: A Deep Dive into Core Concepts and Advanced Techniques

### Introdução
Este capítulo explora os **Latent Variable Models (LVMs)** aplicados a dados discretos, como vetores de bits, sequências de variáveis categóricas, vetores de contagem, estruturas de grafos e dados relacionais [^1]. LVMs são ferramentas poderosas para analisar dados complexos, permitindo a captura de correlações subjacentes através de variáveis latentes [^1]. O objetivo é construir modelos de probabilidade conjunta, como $p(y_i)$ ou $p(n_i)$, que utilizem variáveis latentes para comprimir a representação dos dados, revelar relações ocultas e reduzir a dimensionalidade [^1]. Serão discutidas diversas abordagens, incluindo mixture models, exponential family PCA, Latent Dirichlet Allocation (LDA) e modelos para dados estruturados em grafos [^1, 27].

### Conceitos Fundamentais

#### Mixture Models
A abordagem mais simples é o uso de um **finite mixture model** [^2]. Este modelo associa uma única variável latente discreta, $q_i \in \{1, ..., K\}$, a cada documento, onde $K$ é o número de clusters [^2]. Define-se um prior discreto, $q_i \sim Cat(\pi)$ [^2]. Para documentos de tamanho variável, define-se $p(y_{il}|q_i = k) = b_{kv}$, onde $b_{kv}$ é a probabilidade de que o cluster $k$ gere a palavra $v$ [^2]. O valor de $q_i$ é conhecido como um **tópico**, e o vetor $b_k$ é a distribuição de palavras do tópico $k$ [^2]. A verossimilhança tem a forma:
$$
P(y_{i,1:L_i}|q_i = k) = \prod_{l=1}^{L_i} Cat(y_{il}|b_k)
$$
A distribuição induzida sobre os dados visíveis é dada por:
$$
P(y_{i,1:L_i}) = \sum_k \pi_k \prod_{l=1}^{L_i} Cat(y_{il}|b_k)
$$
A "generative story" é: para o documento $i$, escolhe-se um tópico $q_i$ a partir de $\pi$, chama-se de $k$, e então para cada palavra $l = 1 : L_i$, escolhe-se uma palavra de $b_k$ [^2].

Se tivermos um conjunto fixo de observações categóricas, podemos usar uma matriz de tópico diferente para cada variável de saída:
$$
P(y_{i,1:R}|q_i = k) = \prod_{r=1}^{R} Cat(y_{ir}|b_k^{(r)})
$$
Isto é um análogo não supervisionado da classificação naive Bayes [^2].

Também podemos modelar vetores de contagem. Se a soma $L_i = \sum_v n_{iv}$ é conhecida, podemos usar uma multinomial:
$$
p(n_i|L_i, q_i = k) = Mu(n_i|L_i, b_k)
$$
Se a soma é desconhecida, podemos usar uma densidade condicional de Poisson para dar:
$$
p(n_i|q_i = k) = \prod_{v=1}^{V} Poi(n_{iv}|\lambda_{vk})
$$
Neste caso, $L_i|q_i = k \sim Poi(\sum_v \lambda_{vk})$ [^2].

#### Exponential Family PCA
Finite mixture models têm poder expressivo limitado [^3]. Um modelo mais flexível é usar um vetor de variáveis latentes contínuas de valor real, similar aos modelos de factor analysis (FA) e PCA [^3]. Em PCA, usamos um prior Gaussiano da forma $p(z_i) = N(\mu, \Sigma)$, onde $z_i \in R^K$, e uma verossimilhança Gaussiana da forma $p(y_i|z_i) = N(Wz_i, \sigma^2I)$ [^3]. Este método pode ser aplicado a dados discretos ou de contagem [^3]. O método conhecido como **latent semantic analysis (LSA)** ou **latent semantic indexing (LSI)** é equivalente a aplicar PCA a uma term by document count matrix [^3].

Um método melhor para modelar dados categóricos é usar uma distribuição multinoulli ou multinomial. Basta mudar a verossimilhança para:
$$
P(y_{i, 1:L_i}|z_i) = \prod_{l=1}^{L_i} Cat(y_{il}|S(Wz_i))
$$
onde $W \in R^{V \times K}$ é uma weight matrix e $S$ é a função softmax [^3]. Se tivermos um número fixo de respostas categóricas, podemos usar:
$$
P(y_{i, 1:R}|z_i) = \prod_{r=1}^{R} Cat(y_{ir}|S(W_rz_i))
$$
onde $W_r \in R^{V \times K}$ é a weight matrix para a r-ésima variável de resposta. Este modelo é chamado de **categorical PCA** [^3]. Se tivermos contagens, podemos usar um modelo multinomial:
$$
p(n_i|L_i, z_i) = Mu(n_i|L_i, S(Wz_i))
$$
ou um modelo de Poisson:
$$
p(n_i|z_i) = \prod_{v=1}^{V} Poi(n_{iv}|\exp(w_v^Tz_i))
$$
Todos esses modelos são exemplos de **exponential family PCA** ou **ePCA** [^3]. A distribuição induzida correspondente sobre as variáveis visíveis tem a forma:
$$
P(y_{i,1:L_i}) = \int \prod_{l=1}^{L_i} P(y_{il}|z_i, W) N(z_i|\mu, \Sigma) dz_i
$$

#### LDA e MPCA
Em ePCA, a quantidade $Wz_i$ representa os parâmetros naturais da família exponencial [^4]. Algumas vezes, é mais conveniente usar os dual parameters [^4]. Por exemplo, para a multinomial, o dual parameter é o vetor de probabilidade, enquanto o natural parameter é o vetor de log odds [^4].

Se quisermos usar os dual parameters, precisamos restringir as variáveis latentes para que vivam no espaço de parâmetros apropriado [^4]. No caso de dados categóricos, precisaremos garantir que o vetor latente vive em $S_K$, o K-dimensional probability simplex [^4]. Para evitar confusão com ePCA, denotaremos tal vetor latente por $\pi_i$ [^4]. Neste caso, o prior natural para as variáveis latentes é o Dirichlet, $\pi_i \sim Dir(\alpha)$ [^4]. Normalmente definimos $\alpha = \alpha_0 1_K$ [^4]. Se definirmos $\alpha \ll 1$, incentivamos $\pi_i$ a ser sparse, como mostrado na Figura 2.14 [^4].

Quando temos um vetor de contagem cuja soma total é conhecida, a verossimilhança é dada por:
$$
p(n_i|L_i, \pi_i) = Mu(n_i|L_i, B\pi_i)
$$
Este modelo é chamado de **multinomial PCA** ou **mPCA** [^4]. Como estamos assumindo $n_{iv} = \sum_k b_{vk}\pi_{iv}$, isso pode ser visto como uma forma de matrix factorization para a count matrix [^4]. A distribuição marginal correspondente tem a forma:
$$
p(n_i|L_i) = \int Mu(n_i|L_i, B\pi_i)Dir(\pi_i|\alpha)d\pi_i
$$
Infelizmente, este integral não pode ser computado analiticamente [^4].

Se tivermos uma sequência de comprimento variável (de comprimento conhecido), podemos usar:
$$
P(y_{i,1:L_i}|\pi_i) = \prod_{l=1}^{L_i} Cat(y_{il}|B\pi_i)
$$
Isto é chamado de **latent Dirichlet allocation** ou **LDA** [^4].

#### GaP model and non-negative matrix factorization
Agora considere modelar vetores de contagem onde não restringimos a soma a ser observada [^5]. Neste caso, as variáveis latentes só precisam ser não negativas, então as denotaremos por $z_i^+$ [^5]. Isso pode ser garantido usando um prior da forma:
$$
p(z_i^+) = \prod_{k=1}^{K} Ga(z_k^+|\alpha_k, \beta_k)
$$
A verossimilhança é dada por:
$$
p(n_i|z_i^+) = \prod_{v=1}^{V} Poi(n_{iv}|b_vz_i^+)
$$
Isto é chamado de **GaP (Gamma-Poisson) model** [^5].

Se definirmos $\alpha_k = \beta_k = 0$ no modelo GaP, recuperamos um método conhecido como **non-negative matrix factorization** ou **NMF** [^5]. NMF não é um probabilistic generative model, uma vez que não especifica um prior apropriado para $z_i^+$ [^5]. Além disso, o algoritmo proposto em (Lee and Seung 2001) é outro algoritmo EM degenerado, então sofre de overfitting [^5].

Para incentivar $z_i^+$ a ser sparse, podemos modificar o prior para ser um prior do tipo spike-and-Gamma da seguinte forma:
$$
P(z_k^+) = \rho_k \mathbb{I}(z_k^+ = 0) + (1 - \rho_k)Ga(z_k^+|\alpha_k, \beta_k)
$$

#### Latent Dirichlet Allocation (LDA)
Em uma mixture of multinoullis, cada documento é atribuído a um único tópico, $q_i \in \{1,..., K\}$, extraído de uma distribuição global $\pi$ [^6]. Em LDA, cada palavra é atribuída ao seu próprio tópico, $q_{il} \in \{1,..., K\}$, extraído de uma distribuição específica do documento $\pi_i$ [^6]. Como um documento pertence a uma distribuição sobre tópicos, em vez de um único tópico, o modelo é chamado de **admixture mixture** ou **mixed membership model** [^6].

Adicionando priors conjugados aos parâmetros, o modelo completo é o seguinte:
$$
\begin{aligned}
\pi_i &\sim Dir(\alpha 1_K) \\
q_{il} &\sim Cat(\pi_i) \\
b_k &\sim Dir(\gamma 1_V) \\
y_{il}|q_{il} = k, B &\sim Cat(b_k)
\end{aligned}
$$
Podemos marginalizar as variáveis $q_i$, criando um arco direto de $\pi_i$ para $y_{il}$, com o seguinte CPD:
$$
p(y_{il} = v|\pi_i) = \sum_k p(y_{il} = v|q_{il} = k)p(q_{il} = k) = \sum_k \pi_{ik}b_{kv}
$$

#### Perplexity
A **perplexity** de um modelo de linguagem $q$ dado um processo estocástico $p$ é definida como:
$$
perplexity(p, q) \triangleq 2^{H(p,q)}
$$
onde $H(p, q)$ é a cross-entropy dos dois processos estocásticos, definida como:
$$
H(p,q) \triangleq \lim_{N \to \infty} - \frac{1}{N} \sum_{y_{1:N}} P(y_{1:N}) \log q(y_{1:N})
$$
A cross entropy (e, portanto, a perplexidade) é minimizada se $q = p$; neste caso, o modelo pode prever tão bem quanto a distribuição "verdadeira" [^9].
No caso de modelos unigram, o termo de cross entropy é dado por:
$$
H = -\frac{1}{N} \sum_{i=1}^{N} \frac{1}{L_i} \sum_{l=1}^{L_i} \log q(y_{il})
$$
onde $N$ é o número de documentos e $L_i$ é o número de palavras no documento $i$ [^10]. Portanto, a perplexidade do modelo $q$ é dada por:
$$
perplexity(p_{emp}, p) = exp(-\frac{1}{N} \sum_{i=1}^{N} \frac{1}{L_i} \sum_{l=1}^{L_i} \log q(y_{il}))
$$

#### Fitting using (collapsed) Gibbs sampling
É direto derivar um algoritmo de Gibbs sampling para LDA [^11]. Os condicionais completos são os seguintes:
$$
\begin{aligned}
p(q_{il} = k|\cdot) &\propto \exp[\log \pi_{ik} + \log b_{k,x_{il}}] \\
p(\pi_i|\cdot) &= Dir(\{\alpha_k + \sum_l \mathbb{I}(z_{il} = k)\}) \\
p(b_k) &= Dir(\{\gamma_v + \sum_i \sum_l \mathbb{I}(x_{il} = v, z_{il} = k)\})
\end{aligned}
$$
No entanto, pode-se obter um melhor desempenho integrando analiticamente os $\pi_i$'s e os $b_k$'s [^11].

### Conclusão
Este capítulo forneceu uma visão abrangente de Latent Variable Models para dados discretos, explorando desde abordagens básicas como mixture models até técnicas mais avançadas como LDA e modelos para dados estruturados em grafos [^1, 27]. A discussão detalhada das formulações matemáticas e dos métodos de inferência, como Gibbs sampling e variational inference, capacita o leitor a aplicar e adaptar esses modelos a uma ampla gama de problemas de análise de dados [^11]. As referências a tópicos relacionados dentro do contexto fornecem uma base sólida para uma compreensão mais profunda e para futuras explorações neste campo [^1, 27].
### Referências
[^1]: Chapter 27.1
[^2]: Chapter 27.2.1
[^3]: Chapter 27.2.2
[^4]: Chapter 27.2.3
[^5]: Chapter 27.2.4
[^6]: Chapter 27.3.1
[^7]: Chapter 27.22
[^8]: Chapter 27.23
[^9]: Chapter 27.3.3
[^10]: Chapter 27.3.3
[^11]: Chapter 27.3.4
[^12]: Chapter 27

<!-- END -->