{
  "topics": [
    {
      "topic": "State Space Models",
      "sub_topics": [
        "State Space Models (SSMs) provide a framework for modeling time series data and systems evolving over time where the system's state is only partially observed. They extend Hidden Markov Models (HMMs) to continuous hidden states, represented generically as \\(z_t = g(u_t, z_{t-1}, \\epsilon_t)\\) and \\(y_t = h(z_t, u_t, \\delta_t)\\), where \\(z_t\\) is the hidden state, \\(u_t\\) is an optional control signal, \\(y_t\\) is the observation, \\(g\\) is the transition model, \\(h\\) is the observation model, and \\(\\epsilon_t\\) and \\(\\delta_t\\) are system and observation noise, respectively; all model parameters \\(\\theta\\) are assumed known or included in the hidden state. The transition model, denoted as g, describes how the hidden state zt evolves from the previous state zt-1 and optional inputs ut, influenced by system noise \\u03b5t. The observation model, denoted as h, defines how the observation yt is generated from the hidden state zt and optional inputs ut, subject to observation noise \\u03b4t.",
        "The primary goal when using SSMs is to recursively estimate the belief state \\(p(z_t | y_{1:t}, u_{1:t}, \\theta)\\), which represents the probability distribution of the hidden state at a given time, conditioned on all past observations and inputs. This involves converting beliefs about hidden states into predictions about future observables by computing the posterior predictive distribution \\(p(y_{t+1} | y_{1:t})\\).",
        "A Linear-Gaussian SSM (LG-SSM), also known as a linear dynamical system (LDS), is a special case where all conditional probability distributions (CPDs) are linear-Gaussian. This simplification allows for efficient inference using the Kalman filter, making LG-SSMs widely applicable in various fields. The transition model is defined as zt = Atzt-1 + Btut + et, and the observation model is defined as yt = Ctzt + Dtut + \\u03b4t, where At is the transition matrix, Bt is the input matrix, ut is the control signal, and et is the system noise, assumed to be Gaussian distributed with zero mean and covariance Qt, i.e., et ~ N(0, Qt). The observation model in LG-SSMs is defined as yt = Ctzt + Dtut + \\u03b4t, where Ct is the observation matrix, Dt is the input matrix, and \\u03b4t is the observation noise, assumed to be Gaussian distributed with zero mean and covariance Rt, i.e., \\u03b4t ~ N(0, Rt); if the parameters \\u03b8t = (At, Bt, Ct, Dt, Qt, Rt) are independent of time, the model is called stationary. Stationarity is achieved when parameters \\u03b8t = (At, Bt, Ct, Dt, Qt, Rt) are time-independent.",
        "In LG-SSMs, if the initial belief state is Gaussian, \\(p(z_1) = N(\\mu_{1|0}, \\Sigma_{1|0})\\), then all subsequent belief states will also be Gaussian, denoted by \\(p(z_t | y_{1:t}) = N(\\mu_{t|t}, \\Sigma_{t|t})\\), which can be efficiently computed using the Kalman filter. Kalman Filtering is a recursive algorithm for exact Bayesian filtering in LG-SSMs, providing a closed-form solution for estimating the posterior distribution of the hidden state given the observed data. It consists of two main steps: prediction and measurement update. The prediction step involves computing the prior distribution of the hidden state at the next time step, p(zt|y1:t-1, u1:t), based on the transition model and the posterior distribution at the previous time step. The measurement update step in Kalman filtering involves combining the prior distribution with the likelihood of the observation, p(yt|zt, ut), to obtain the posterior distribution of the hidden state, p(zt|y1:t, u1:t); this step uses Bayes' rule and results in a Gaussian posterior distribution with updated mean \\u03bct and covariance \\u03a3t; the update equations involve the Kalman gain matrix Kt, which determines the weight given to the new observation in updating the state estimate.",
        "Kalman smoothing is an offline algorithm that provides a more accurate estimate of the hidden states by considering both past and future observations. It involves a forward pass (Kalman filter) and a backward pass to refine the state estimates, reducing uncertainty and providing a smoother trajectory. The RTS smoother is a specific algorithm for Kalman smoothing, named after its inventors Rauch, Tung, and Striebel.",
        "Recursive Least Squares (RLS) is an algorithm for online parameter learning in linear regression models using SSMs, where the regression parameters are treated as the hidden state and updated recursively as new data streams in. The Kalman filter is applied to this model to update the posterior beliefs about the parameters, providing an explicit form for the updates. The RLS algorithm automatically performs step-size adaptation and converges to the optimal posterior in one pass over the data, unlike the Least Mean Squares (LMS) algorithm.",
        "Structural time series models, based on state space representations, decompose time series data into latent processes such as local level, local trend, and seasonality, enabling time series forecasting by integrating out the hidden variables to compute the posterior predictive of the visibles."
      ]
    },
    {
      "topic": "Applications of State Space Models",
      "sub_topics": [
        "State Space Models (SSMs) have a wide range of applications, including object tracking, simultaneous localization and mapping (SLAM), time series forecasting, and online parameter learning.",
        "Object tracking involves estimating the position and velocity of a moving object over time, often from noisy measurements. Kalman filtering is used for tracking objects by estimating position and velocity from noisy radar measurements. By marginalizing out velocity dimensions, the location of the object can be estimated. Measurement noise is statistically removed using Kalman filter. SSMs provide a natural framework for object tracking by modeling the object's dynamics as a state-space model and using Kalman filtering to estimate its state.",
        "Simultaneous Localization and Mapping (SLAM) enables robots to create maps of unknown environments while simultaneously tracking their location within those maps. The challenge is closing the loop, and this problem is solved with Kalman filters and particle filters. SSMs, particularly with Kalman filtering, provide a probabilistic framework for solving SLAM by modeling the robot's motion and sensor measurements over time. The map is represented as 2D locations of fixed landmarks \\(L^1, ..., L^K\\), and the robot's unknown location at time \\(t\\) is \\(x_t\\); a Kalman filter can be used to maintain the belief state about the location of the robot and the landmarks, assuming a linear-Gaussian observation model and a Gaussian motion model.",
        "SSMs are well-suited for time series forecasting by creating a generative model of the data in terms of latent processes that capture different aspects of the signal. Structural time series models explain observed data, basic building blocks include local level model and local linear trend. The goal is to predict future visible variables, \\(\\hat{y}_{t+1} = f(y_{1:t}, \\theta)\\), using the state-space approach to create a generative model of the data in terms of latent processes and integrating out hidden variables to compute the posterior predictive of the visibles. The local level model is a basic time series model that assumes the observed data is equal to some unknown level term plus observation noise, where the level evolves over time subject to system noise. This model can capture gradual changes in the level of a time series. The local linear trend model extends the local level model by adding a trend component, allowing the level to change by an amount that evolves over time. This model can capture linear trends in a time series that may change direction over time. Seasonality in time series can be modeled by adding a latent process consisting of a series of offset terms that sum to zero over a complete cycle. This allows the SSM to capture periodic fluctuations in the time series. ARMA models are a classical approach to time series forecasting that combines autoregressive (AR) and moving average (MA) components. These models can be represented as SSMs, allowing for the use of Kalman filtering for inference and prediction.",
        "Online parameter learning using recursive least squares (RLS) allows for the estimation of model parameters as data streams in. By representing the parameters as hidden states in an SSM, the Kalman filter can be used to update the parameter estimates in real-time. Regression parameters are treated as hidden states and using the Kalman filter to update these states based on incoming data."
      ]
    },
    {
      "topic": "Inference in Linear-Gaussian State Space Models",
      "sub_topics": [
        "The Kalman filter is an algorithm for exact Bayesian filtering in linear-Gaussian state space models. It recursively estimates the marginal posterior distribution of the hidden state at each time step, given the observations up to that time, and represents the marginal posterior at time \\(t\\) by \\(p(z_t | y_{1:t}, u_{1:t}) = N(z_t | \\mu_t, \\Sigma_t)\\). It allows for prediction and update steps to be performed in closed form.",
        "The prediction step involves deriving the predictive distribution of the hidden state at the next time step, given all past observations and inputs. This is computed by integrating over the joint distribution of the current and next hidden states, resulting in \\(p(z_t | y_{1:t-1}, u_{1:t}) = \\int N(z_t | A z_{t-1} + B u_t, Q_t) N(z_{t-1} | \\mu_{t-1}, \\Sigma_{t-1}) dz_{t-1} = N(z_t | \\mu_{t|t-1}, \\Sigma_{t|t-1})\\), where \\(\\mu_{t|t-1} = A \\mu_{t-1} + B u_t\\) and \\(\\Sigma_{t|t-1} = A \\Sigma_{t-1} A^T + Q_t\\). The equations for the predicted mean \\u03bct|t-1 and covariance \\u03a3t|t-1 are derived from the properties of Gaussian distributions and the linear-Gaussian assumptions of the model.",
        "The measurement step involves computing the posterior distribution of the hidden state at the current time step, given the current observation and the predictive distribution from the previous step. This is computed using Bayes' rule, \\(p(z_t | y_t, y_{1:t-1}, u_{1:t}) \\propto p(y_t | z_t, u_t) p(z_t | y_{1:t-1}, u_{1:t}) = N(z_t | \\mu_t, \\Sigma_t)\\), where \\(\\mu_t = \\mu_{t|t-1} + K_t r_t\\) and \\(\\Sigma_t = (I - K_t C_t) \\Sigma_{t|t-1}\\), with \\(r_t = y_t - \\hat{y}_t\\) being the residual or innovation and \\(K_t\\) being the Kalman gain matrix. The residual or innovation rt represents the difference between the actual observation yt and the predicted observation \\u0177t, and is used to update the state estimate. The Kalman gain matrix Kt is a crucial component of the measurement update step, determining the weight given to the new observation in updating the state estimate; it is computed based on the prior covariance \\u03a3t|t-1, the observation matrix Ct, and the observation noise covariance Rt; the magnitude of Kt reflects the relative uncertainty of the prior and the measurement, influencing the amount of correction applied to the state estimate.",
        "The Kalman smoothing algorithm computes p(zt|y1:T) for each t, useful for online inference problems such as tracking. In an offline setting, it can compute \\(p(z_t | y_{1:T})\\) by conditioning on past and future data, reducing uncertainty; this is achieved using the RTS smoother, also known as the Kalman smoothing algorithm. By conditioning on past and future data, uncertainty will be significantly reduced.",
        "The marginal likelihood, p(y1:T|u1:T), can be computed as a byproduct of the Kalman filter, providing a measure of the goodness-of-fit of the model to the observed data. It is calculated as the sum of the log probabilities of the observations given the past data, p(yt|y1:t-1, u1:t), which are Gaussian distributions with means and covariances determined by the Kalman filter equations; the marginal likelihood is useful for model selection and parameter estimation.",
        "The one-step-ahead posterior predictive density for the observations, p(yt|y1:t-1, u1:t), can be computed using the Kalman filter, providing a forecast of future observations based on the current state estimate. This distribution is Gaussian with mean C\\u03bct|t-1 and covariance C\\u03a3t|t-1CT + R, allowing for probabilistic predictions of future observations; it is useful for time series forecasting and decision-making.",
        "Computational efficiency is a key consideration in implementing the Kalman filter, with the matrix inversion to compute the Kalman gain matrix Kt and the matrix-matrix multiply to compute \\u03a3t being the dominant costs. In some applications, sparse approximations can be used to reduce the computational burden. Alternative implementations, such as the information filter and the square root filter, can improve numerical stability."
      ]
    },
    {
      "topic": "Approximate Online Inference for Non-Linear, Non-Gaussian State Space Models",
      "sub_topics": [
        "For non-linear, non-Gaussian SSMs, approximate inference techniques are used. The Extended Kalman Filter (EKF) and the Unscented Kalman Filter (UKF) are common methods for approximating the posterior by a Gaussian.",
        "The Extended Kalman Filter (EKF) is an approximate inference technique for non-linear models, where the non-linear transition and observation models are linearized using a first-order Taylor series expansion around the previous state estimate, and then the standard Kalman filter equations are applied. The EKF linearizes the non-linear functions around the previous state estimate using a first-order Taylor series expansion, and then applies the standard Kalman filter equations; this approach approximates the non-linear system with a time-varying linear system.",
        "The Unscented Kalman Filter (UKF) is an alternative to the EKF that uses a set of deterministically chosen sample points (sigma points) to propagate the state distribution through the non-linear functions, and then fits a Gaussian to the resulting transformed points, providing a more accurate approximation than the EKF, especially when the non-linearities are strong. Instead of linearizing the non-linear functions, it uses the unscented transform to pass a deterministically chosen set of points, known as sigma points, through the non-linear functions; the resulting transformed points are then used to fit a Gaussian distribution, providing a more accurate approximation of the posterior distribution.",
        "Assumed Density Filtering (ADF) is an approximate inference technique where the posterior distribution is approximated by a distribution of a certain convenient form, such as a Gaussian, after each update step, allowing for efficient online inference in complex models. If q is in the exponential family, this KL minimization can be done by moment matching. An exact update step is performed, but the posterior is then approximated by a distribution from a certain convenient family, such as a Gaussian; this involves projecting the exact posterior onto the space of tractable distributions by minimizing the Kullback-Leibler divergence; the predict-update-project cycle is a key component of ADF, where the posterior is predicted, updated, and then projected onto the tractable family.",
        "The Boyen-Koller algorithm is used for online inference in discrete-state dynamic Bayes nets, this algorithm calculates the posterior marginals. The algorithm is accurate, the error remains bounded under certain assumptions. It is a specific instance of ADF for online inference in discrete-state dynamic Bayes nets, where a fully factored approximation is used to address the entanglement problem; this algorithm involves performing a predict-update step using the factored prior and then computing the posterior marginals to update the approximation; the error incurred by this series of approximations remains bounded under certain assumptions about the stochasticity of the system."
      ]
    },
    {
      "topic": "Hybrid Discrete/Continuous State Space Models",
      "sub_topics": [
        "Hybrid systems contain discrete and continuous hidden variables, used when discrete variables indicate sensor issues or regime. A switching linear dynamical system combines HMM and LG-SSM.",
        "Inference in hybrid models is intractable because the number of modes in the belief state grows exponentially over time. Approximate inference methods have been proposed for this model.",
        "A Gaussian sum filter approximates the belief state at each step by a mixture of K Gaussians. This can be implemented by running K Kalman filters in parallel.",
        "Multi-target tracking involves estimating the number and states of multiple objects from noisy measurements, requiring solutions to data association problems; this can be addressed using techniques such as the Hungarian algorithm to solve the maximal weight bipartite matching between detections and objects; extensions of this method handle variable and/or unknown numbers of objects using sequential Monte Carlo or MCMC methods."
      ]
    }
  ]
}