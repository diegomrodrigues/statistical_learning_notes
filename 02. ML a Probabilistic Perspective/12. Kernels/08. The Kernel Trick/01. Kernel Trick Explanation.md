## O Truque do Kernel

### Introdução
O capítulo anterior introduziu o conceito de **funções kernel** e sua aplicação em diversos algoritmos [^1]. Este capítulo aprofunda-se no conceito do **"kernel trick"**, uma técnica fundamental que permite a aplicação de algoritmos lineares em espaços de características de alta dimensão sem a necessidade de calcular explicitamente as coordenadas nesses espaços [^1].

### Conceitos Fundamentais

O "kernel trick" representa uma abordagem alternativa à definição de vetores de características em termos de kernels [^1]. Em vez de definir um vetor de características como $\phi(x) = [\kappa(x, x_1), ..., \kappa(x, x_N)]$, onde $\kappa$ é uma função kernel e $x_i$ são pontos de dados, o "kernel trick" propõe trabalhar diretamente com os vetores de características originais $x$ [^1].

A essência do "kernel trick" reside na modificação do algoritmo, substituindo todos os **produtos internos** da forma $(x, x\')$ por uma chamada à função kernel $\kappa(x, x\')$ [^1]. Matematicamente, isso significa que, em vez de calcular o produto interno no espaço original, utilizamos a função kernel para obter um resultado equivalente no espaço de características induzido pelo kernel.

> O "kernel trick" permite que algoritmos lineares operem em espaços de características de alta dimensão sem calcular explicitamente as coordenadas nesses espaços.

A aplicabilidade do "kernel trick" se estende a uma ampla gama de algoritmos, tornando-o uma ferramenta versátil no aprendizado de máquina [^1]. No entanto, é crucial que o kernel utilizado seja um **Mercer kernel** para garantir que o truque funcione corretamente [^1].

**Mercer Kernels**: Um kernel de Mercer garante que a matriz de Gram (matriz de similaridade entre todos os pares de pontos de dados) seja **positiva semi-definida**. Isso é fundamental para garantir que o algoritmo convirja para uma solução válida no espaço de características induzido pelo kernel.

### Aplicações e Exemplos

Para ilustrar a aplicação do "kernel trick", considere o algoritmo de **K-vizinhos mais próximos (KNN)** [^1]. No KNN, a etapa crucial é o cálculo da distância entre um ponto de teste e os pontos de treinamento. A distância euclidiana ao quadrado entre dois pontos $x_i$ e $x_i\'$ pode ser expressa como:

$$||x_i - x_{i\'}||^2 = (x_i, x_i) + (x_{i\'}, x_{i\'}) - 2(x_i, x_{i\'})$$

Utilizando o "kernel trick", podemos substituir os produtos internos por funções kernel:

$$||x_i - x_{i\'}||^2 = \kappa(x_i, x_i) + \kappa(x_{i\'}, x_{i\'}) - 2\kappa(x_i, x_{i\'})$$

Essa transformação permite aplicar o KNN a dados estruturados e não vetoriais, utilizando funções kernel apropriadas para medir a similaridade entre esses objetos [^1].

Outro exemplo é o **K-medoids clustering** [^1]. Este algoritmo usa a distância Euclidiana para medir a dissimilaridade. Podemos kernelizar o algoritmo K-medoids usando a Equação 14.30 [^1]:

$$||x_i - x_{i\'}||^2 = (x_i, x_i) + (x_{i\'}, x_{i\'}) - 2(x_i, x_{i\'})$$

### Ridge Regression Kernelizado

A regressão de Ridge pode ser facilmente kernelizada mudando das variáveis primárias para as duais [^1].

Seja $x \in R^D$ algum vetor de característica, e $X$ seja a matriz de design $N \times D$ correspondente. Queremos minimizar

$$J(w) = (y - Xw)^T(y - Xw) + \lambda ||w||^2$$

A solução ótima é dada por

$$w = (X^TX + \lambda I_D)^{-1}X^Ty = (\sum_i x_ix_i^T + \lambda I_D)^{-1}X^Ty$$

A Equação 14.34 ainda não está na forma de produtos internos. No entanto, usando o lema de inversão de matriz (Equação 4.107), reescrevemos a estimativa de Ridge como segue

$$w = X^T(XX^T + \lambda I_N)^{-1}y$$

Isso leva tempo para computar $O(N^3 + N^2D)$. Além disso, vemos que podemos kernelizar parcialmente isso, substituindo $XX^T$ pela matriz de Gram $K$. Mas e o termo $X^T$ inicial?

Vamos definir as seguintes variáveis duais:

$$alpha \triangleq (K + \lambda I_N)^{-1}y$$

Então podemos reescrever as variáveis primárias como segue:

$$w = X^T\alpha = \sum_{i=1}^N \alpha_i x_i$$

Isso nos diz que o vetor de solução é apenas uma soma linear dos vetores de treinamento $N$. Quando conectamos isso no momento do teste para calcular a média preditiva, obtemos

$$f(x) = w^Tx = \sum_{i=1}^N \alpha_i x_i^T x = \sum_{i=1}^N \alpha_i \kappa(x, x_i)$$

### Conclusão

O "kernel trick" é uma ferramenta poderosa que permite a aplicação de algoritmos lineares em espaços de características de alta dimensão sem a necessidade de calcular explicitamente as coordenadas nesses espaços [^1]. A escolha de um kernel de Mercer apropriado é crucial para garantir a validade e convergência dos algoritmos kernelizados. O "kernel trick" encontra aplicações em uma ampla gama de algoritmos de aprendizado de máquina, incluindo KNN, K-means e SVMs.

### Referências
[^1]: Machine Learning: A Probabilistic Perspective, Kevin P. Murphy, 2012.

<!-- END -->