## Kernelized K-Medoids Clustering

### Introdução
Como vimos anteriormente [^1], o **truque do kernel** permite que algoritmos lineares operem em espaços de características de alta dimensão implicitamente, substituindo produtos internos por uma função kernel $\kappa(x, x')$. No contexto de clustering, o K-means tradicional utiliza a distância Euclidiana para medir a dissimilaridade entre os pontos de dados, o que pode não ser apropriado para objetos estruturados [^489]. Este capítulo detalha o desenvolvimento de um algoritmo K-medoids kernelizado, que supera essa limitação ao operar diretamente em termos de similaridades definidas por um kernel.

### Conceitos Fundamentais

O algoritmo K-medoids kernelizado é uma extensão do K-medoids, que por sua vez é uma variação do K-means [^489]. A principal diferença entre K-means e K-medoids reside na representação dos **centróides**. Enquanto o K-means representa cada centróide como a média dos vetores de dados atribuídos ao cluster, o K-medoids seleciona um dos próprios vetores de dados como o centróide. Essa abordagem tem a vantagem de sempre lidar com **índices inteiros** em vez de objetos de dados, tornando-a mais adequada para dados estruturados onde a média pode não ser bem definida [^489].

Para implementar o K-medoids kernelizado, seguimos os seguintes passos:

1.  **Inicialização:** Selecionamos aleatoriamente *K* objetos de dados como os medóides iniciais (centróides).
2.  **Atribuição:** Atribuímos cada objeto de dados ao medóide mais próximo. No contexto kernelizado, a proximidade é definida pela **distância kernelizada**.
3.  **Atualização:** Para cada cluster, selecionamos o objeto que minimiza a soma das distâncias kernelizadas a todos os outros objetos no mesmo cluster. Este objeto torna-se o novo medóide para o cluster.
4.  **Iteração:** Repetimos os passos 2 e 3 até convergência, ou seja, até que a atribuição dos objetos aos clusters não se altere.

A **distância kernelizada** entre dois objetos $x_i$ e $x_j$ pode ser expressa em termos do kernel como:
$$d(x_i, x_j) = \sqrt{\kappa(x_i, x_i) + \kappa(x_j, x_j) - 2\kappa(x_i, x_j)}$$
Esta fórmula deriva da expansão da distância Euclidiana no espaço de características induzido pelo kernel [^489]. Ao utilizar esta distância, o algoritmo K-medoids pode operar em dados estruturados sem a necessidade de representá-los explicitamente como vetores de características.

O passo de **atualização** dos medóides envolve a seleção do objeto $m_k$ que minimiza a soma das distâncias kernelizadas a todos os outros objetos no cluster *k* [^490]:
$$m_k = \underset{i: z_i=k}{\operatorname{argmin}} \sum_{i': z_{i'}=k} d(i, i')$$nonde $z_i$ representa o cluster ao qual o objeto *i* está atribuído.

Este processo de atualização garante que o centróide seja sempre um dos pontos de dados originais, o que é crucial para dados estruturados.

### Conclusão

O K-medoids kernelizado representa uma poderosa ferramenta para clustering de dados estruturados, permitindo que algoritmos de clustering operem em espaços de características implícitos definidos por funções kernel [^489]. Ao substituir a distância Euclidiana pela distância kernelizada e ao utilizar medóides em vez de médias, este algoritmo supera as limitações do K-means tradicional em dados não vetoriais. A escolha do kernel apropriado é crucial para o sucesso do algoritmo, e depende da estrutura dos dados e do conhecimento prévio sobre o problema em questão [^1].

### Referências
[^1]: Capítulo 14: Kernels
[^489]: K-means clustering (Section 11.4.2.5) uses Euclidean distance to measure dissimilarity, which is not always appropriate for structured objects. We now describe how to develop a kernelized
[^490]: The first step is to replace the K-means algorithm with the K-medoids algorothm. This is similar to K-means, but instead of representing each cluster's centroid by the mean of all data vectors assigned to this cluster, we make each centroid be one of the data vectors themselves. Thus we always deal with integer indexes, rather than data objects. We assign objects to their closest centroids as before. When we update the centroids, we look at each object that belongs to the cluster, and measure the sum of its distances to all others in the same cluster; we then pick the one which has the smallest such sum:

<!-- END -->