## SVMs para Classificação Multi-classe

### Introdução

Como vimos anteriormente no contexto de regressão logística, a generalização para o caso multi-classe envolveu a substituição da função sigmoide pela função softmax e a distribuição de Bernoulli pela distribuição multinomial. No entanto, a extensão de Support Vector Machines (SVMs) para classificação multi-classe não é tão direta, pois as saídas não estão em uma escala calibrada, tornando difícil a comparação direta entre elas [^1]. Este capítulo explora as abordagens para resolver este problema, focando nas técnicas de "one-versus-one" e "one-versus-the-rest" [^2].

### Conceitos Fundamentais

A dificuldade em estender SVMs para o caso multi-classe reside na natureza binária intrínseca do algoritmo. SVMs são projetados para encontrar um hiperplano que maximize a margem entre duas classes [^19]. Para adaptar este modelo a problemas com mais de duas classes, várias estratégias foram desenvolvidas.

**Abordagem One-Versus-The-Rest (OVR)**:
Esta abordagem envolve treinar $C$ classificadores binários, onde $C$ é o número de classes. Cada classificador $f_c(x)$ é treinado para distinguir a classe $c$ das demais classes [^25]. Durante a classificação, a classe com a maior pontuação é selecionada: $y(x) = \arg \max_c f_c(x)$ [^25].

*Desvantagens*:
1.  *Regiões Ambíguas*: Podem surgir regiões no espaço de entrada que são ambiguamente rotuladas, como ilustrado na Figura 14.14(a) [^25].
2.  *Desbalanceamento de Classes*: Cada subproblema binário é propenso a desbalanceamento de classes. Por exemplo, em um problema com 10 classes, cada classificador é treinado com 10% de exemplos positivos e 90% de exemplos negativos [^25].
3.  *Comparabilidade das Magnitudes*: Não há garantia de que as funções $f_c$ tenham magnitudes comparáveis, o que pode comprometer a escolha da classe com a maior pontuação [^25].

**Abordagem One-Versus-One (OVO)**:
Nesta abordagem, são treinados $C(C-1)/2$ classificadores binários, cada um discriminando entre um par de classes $f_{c,c'}(x)$ [^25]. Um ponto é classificado na classe com o maior número de votos.

*Desvantagens*:
1.  *Ambiguidade*: A Figura 14.14(b) ilustra a possibilidade de ambiguidades mesmo nesta abordagem [^25].
2.  *Custo Computacional*: O treinamento requer $O(C^2N^2)$ tempo, e a classificação de cada ponto de dados requer $O(C^2N_{sv})$ tempo, onde $N_{sv}$ é o número de vetores de suporte [^25].

**Alternativas e Melhorias**:
1. *Treinamento Simultâneo*: É possível treinar todos os classificadores $C$ simultaneamente, embora isso aumente o tempo de treinamento para $O(C^2N^2)$ [^25].
2. *Códigos de Correção de Erro*: Abordagens baseadas em códigos de correção de erro (error-correcting output codes) podem ser utilizadas para melhorar a robustez [^25].
3. *Estruturas DAG*: Para reduzir o tempo de teste, as classes podem ser estruturadas em um Grafo Acíclico Direcionado (DAG), permitindo comparações em tempo $O(C)$ [^25].

### Conclusão

A adaptação de SVMs para classificação multi-classe apresenta desafios significativos devido à sua natureza intrinsecamente binária. As abordagens "one-versus-the-rest" e "one-versus-one" oferecem soluções viáveis, mas ambas apresentam desvantagens em termos de ambiguidade, desbalanceamento de classes e custo computacional [^25]. Embora existam diversas heurísticas e melhorias para mitigar esses problemas, é importante notar que a ausência de um modelo probabilístico intrínseco em SVMs dificulta a comparação direta das saídas entre as classes [^25]. Técnicas alternativas, como LIVM e RVM [^25], que modelam a incerteza usando probabilidades, podem oferecer abordagens mais naturais para classificação multi-classe.

### Referências
[^1]: Página 479: "Upgrading an SVM to the multi-class case is not so easy, since the outputs are not on a calibrated scale, and hence are difficult to compare with each other."
[^2]: Página 503: "The obvious approach is to use a one-versus-the-rest approach (also called one-vs-all)... A common alternative is to pick y(x) = arg max fc(x)."
[^19]: Página 497: "SVMs do not result in probabilistic outputs, which causes various difficulties, especially in the multi-class classification setting (see Section 14.5.2.4 for details)."
[^25]: Página 503: "In Section 8.3.7, we saw how we could “upgrade” a binary logistic regression model to the multi-class case... Upgrading an SVM to the multi-class case is not so easy, since the outputs are not on a calibrated scale and hence are hard to compare to each other... The obvious approach is to use a one-versus-the-rest approach (also called one-vs-all)... A common alternative is to pick y(x) = arg max fc(x)... Another approach is to use the one-versus-one or OVO approach, also called all pairs..."
<!-- END -->