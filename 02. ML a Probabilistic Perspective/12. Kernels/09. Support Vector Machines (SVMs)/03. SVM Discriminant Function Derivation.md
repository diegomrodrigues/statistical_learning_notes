## Derivação da Função Discriminante em SVMs via Abordagem Geométrica

### Introdução
Este capítulo aprofunda a derivação da função discriminante em Support Vector Machines (SVMs) a partir de uma perspectiva geométrica, detalhando a relação entre a distância de um ponto ao hiperplano de decisão e a função discriminante [^501]. O objetivo é fornecer uma compreensão clara de como essa função, linear no espaço de características induzido pelo kernel, é construída e interpretada.

### Conceitos Fundamentais
Considere um ponto **x** no espaço induzido pelo kernel. Podemos decompor este ponto em duas componentes: **x**<sub>⊥</sub>, que representa a projeção ortogonal de **x** no hiperplano de decisão, e um componente escalar *r* multiplicado pelo vetor normalizado **w** / ||**w**||, onde *r* é a distância de **x** ao hiperplano e **w** é o vetor normal a este hiperplano [^501]. Matematicamente, temos:

$$ x = x_{\perp} + r \frac{w}{||w||} $$

A função discriminante *f(x)* é definida como:

$$ f(x) = w^Tx + w_0 $$

onde **w** é o vetor de pesos e *w*<sub>0</sub> é o bias [^501]. Podemos expressar *f(x)* em termos da decomposição geométrica de **x**:

$$ f(x) = w^Tx + w_0 = (w^Tx_{\perp} + w_0) + r \left( w^T \frac{w}{||w||} \right) $$

Observe que *f(x*<sub>⊥</sub>*)* = *w<sup>T</sup>x*<sub>⊥</sub> + *w*<sub>0</sub> = 0, pois **x**<sub>⊥</sub> está no hiperplano de decisão. Portanto, a equação se simplifica para:

$$ f(x) = f(x_{\perp}) + r ||w|| = 0 + r ||w|| = r ||w|| $$

Assim, a distância *r* de **x** ao hiperplano de decisão é dada por:

$$ r = \frac{f(x)}{||w||} $$

Esta equação mostra que a função discriminante *f(x)* está diretamente relacionada à distância do ponto **x** ao hiperplano de decisão, normalizada pela norma do vetor de pesos **w** [^501].

O objetivo do SVM é maximizar a margem, que é a distância mínima entre os pontos de treinamento e o hiperplano de decisão. Formalmente, o SVM busca encontrar **w** e *w*<sub>0</sub> que resolvam o seguinte problema de otimização [^501]:

$$ \min_{w, w_0} \frac{1}{2} ||w||^2 \quad \text{sujeito a} \quad y_i (w^Tx_i + w_0) \geq 1, \quad i = 1, \dots, N $$

onde *y<sub>i</sub>* são os rótulos das classes (+1 ou -1) e *N* é o número de pontos de treinamento. A restrição *y<sub>i</sub>*(*w<sup>T</sup>x<sub>i</sub>* + *w*<sub>0</sub>) ≥ 1 garante que todos os pontos estejam no lado correto do hiperplano de decisão com uma margem de pelo menos 1/||**w**||.

### Conclusão
A derivação geométrica da função discriminante em SVMs fornece uma interpretação intuitiva da sua relação com a distância ao hiperplano de decisão [^501]. Esta formulação é fundamental para entender como os SVMs maximizam a margem e, consequentemente, buscam generalizar bem para dados não vistos. A relação *r* = *f(x)* / ||**w**|| é uma ferramenta poderosa para analisar e interpretar os resultados de um classificador SVM.
### Referências
[^501]: Bishop, Christopher M. *Pattern Recognition and Machine Learning*. Springer, 2006.
<!-- END -->