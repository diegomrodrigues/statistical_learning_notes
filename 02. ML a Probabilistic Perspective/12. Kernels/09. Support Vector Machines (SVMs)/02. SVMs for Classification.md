## SVMs para Classificação: Hinge Loss

### Introdução
Este capítulo explora a aplicação de Support Vector Machines (SVMs) para problemas de classificação, com um foco particular na substituição da Negative Log-Likelihood (NLL) loss pela **hinge loss** [^499]. A hinge loss oferece uma abordagem alternativa para a construção de classificadores, com propriedades distintas em comparação com métodos baseados em probabilidades [^497].

### Conceitos Fundamentais

#### Hinge Loss
Em problemas de classificação binária, onde o objetivo é atribuir uma instância a uma de duas classes (tipicamente representadas por $y = 1$ ou $y = -1$), a função de perda desempenha um papel crucial na determinação do desempenho do modelo. Em vez de usar a NLL loss, como em modelos de regressão logística [^498], SVMs para classificação frequentemente empregam a **hinge loss** [^499], definida como:

$$L_{hinge}(y, \eta) = max(0, 1 - y\eta) = (1 - y\eta)_+$$

onde $\eta = f(x)$ representa a "confiança" do modelo na escolha do rótulo $y = 1$, e $f(x)$ é a função de decisão do modelo [^499]. É importante notar que $\eta$ não precisa ter semântica probabilística [^499].

A **hinge loss** penaliza classificações incorretas e classificações corretas com baixa confiança. Especificamente:
- Se $y\eta \geq 1$, a perda é zero, indicando que a classificação está "corretamente" no lado certo da margem [^499].
- Se $y\eta < 1$, a perda é $1 - y\eta$, penalizando a classificação. Quanto menor o valor de $y\eta$, maior a penalidade [^499].

A função **hinge loss** tem uma forma que se assemelha a uma dobradiça de porta, daí o seu nome [^499]. Esta função não é diferenciável no ponto onde $y\eta = 1$, o que pode exigir técnicas de otimização especializadas [^499].

#### Formulação do Problema de Otimização
O objetivo geral em um SVM é minimizar uma combinação da **hinge loss** e um termo de regularização [^499]. Isso leva à seguinte formulação do problema de otimização:

$$min_{w, w_0} \frac{1}{2} ||w||^2 + C \sum_{i=1}^{N} (1 - y_i f(x_i))_+$$

onde:
- $w$ é o vetor de pesos [^499].
- $w_0$ é o bias [^499].
- $C$ é um parâmetro de regularização que controla o trade-off entre a maximização da margem e a minimização do erro de classificação [^499].
- $N$ é o número de amostras de treinamento [^499].

A não diferenciabilidade da **hinge loss** pode ser tratada introduzindo variáveis *slack* $\xi_i$ [^499], resultando em uma formulação equivalente:

$$min_{w, w_0, \xi} \frac{1}{2} ||w||^2 + C \sum_{i=1}^{N} \xi_i \quad \text{s.t.} \quad \xi_i \geq 0, \quad y_i(w^T x_i + w_0) \geq 1 - \xi_i, \quad i = 1:N$$

Esta é uma programação quadrática (QP) em $N + D + 1$ variáveis, sujeita a $O(N)$ restrições [^499].

#### Dualidade e Vetores de Suporte
O problema de otimização do SVM pode ser resolvido usando técnicas de programação quadrática [^499]. No entanto, é comum resolver o problema dual, que envolve a introdução de multiplicadores de Lagrange $\alpha_i$ [^499]. A solução para o problema dual leva a uma representação esparsa da solução, onde apenas um subconjunto dos pontos de treinamento, chamados **vetores de suporte**, têm $\alpha_i > 0$ [^499].

Os **vetores de suporte** são os pontos de treinamento que estão na margem ou do lado errado da margem [^499]. Estes pontos são cruciais para definir a fronteira de decisão [^499].

#### O Truque do Kernel
Para lidar com dados não linearmente separáveis, SVMs empregam o **truque do kernel** [^488]. Em vez de mapear explicitamente os dados para um espaço de características de alta dimensão, um kernel função $\kappa(x, x')$ calcula o produto interno entre as imagens dos dados nesse espaço [^488].

Exemplos de funções kernel comuns incluem:
- Kernel linear: $\kappa(x, x') = x^T x'$ [^482].
- Kernel polinomial: $\kappa(x, x') = (\gamma x^T x' + r)^M$ [^481].
- Kernel RBF (Gaussiano): $\kappa(x, x') = exp(-\frac{||x - x'||^2}{2\sigma^2})$ [^480].

#### Classificação Multi-classe
SVMs são inerentemente classificadores binários [^497]. Para lidar com problemas de classificação multi-classe, várias estratégias podem ser empregadas, incluindo [^503]:
- **One-versus-all (OVA)**: Treinar um classificador binário para cada classe, tratando os dados dessa classe como positivos e todos os outros como negativos [^503].
- **One-versus-one (OVO)**: Treinar um classificador binário para cada par de classes [^503].

### Conclusão

SVMs para classificação, utilizando a **hinge loss**, oferecem uma abordagem poderosa e flexível para problemas de classificação [^499]. Através do uso do **truque do kernel**, SVMs podem lidar com dados não linearmente separáveis [^488]. A esparsidade da solução, focando em **vetores de suporte**, torna SVMs eficientes em termos computacionais [^499]. Embora SVMs não forneçam diretamente saídas probabilísticas [^497], eles podem ser calibrados para produzir estimativas de probabilidade [^502].

### Referências
[^480]: Capítulo 14, Kernels, 14.2.1 RBF kernels
[^481]: Capítulo 14, Kernels, 14.2.3 Mercer (positive definite) kernels
[^482]: Capítulo 14, Kernels, 14.2.4 Linear kernels
[^488]: Capítulo 14, Kernels, 14.4 The kernel trick
[^497]: Capítulo 14, Kernels, 14.5 Support vector machines (SVMs)
[^498]: Capítulo 14, Kernels, 14.5.2 SVMs for classification
[^499]: Capítulo 14, Kernels, 14.5.2.1 Hinge loss
[^502]: Capítulo 14, Kernels, 14.5.2.3 Probabilistic output
[^503]: Capítulo 14, Kernels, 14.5.2.4 SVMs for multi-class classification
<!-- END -->