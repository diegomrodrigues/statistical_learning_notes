## Kernel Regression: Estimando Expectativas Condicionais com Kernels

### Introdução
Este capítulo explora o conceito de **kernel regression**, também conhecido como *kernel smoothing* ou modelo de *Nadaraya-Watson*, no contexto mais amplo de modelos generativos e funções kernel [^1]. Kernel regression é uma técnica não paramétrica que estima a expectativa condicional de uma variável resposta, dados os valores de uma variável preditora [^1, 14.7.4]. Em continuidade ao conceito de kernels como medidas de similaridade entre objetos [^1], kernel regression utiliza *smoothing kernels* para ponderar as saídas dos pontos de treinamento, baseando-se na similaridade entre os pontos de dados.

### Conceitos Fundamentais

A essência do kernel regression reside na estimação da expectativa condicional $E[y|x]$ [^1, 14.7.4]. Esta estimação é realizada ponderando as saídas (valores alvo) dos pontos de dados vizinhos, usando uma função kernel [^1]. Formalmente, a estimativa de kernel regression é dada por [^14.87, 14.7.4]:

$$
f(x) = \sum_{i=1}^{N} w_i(x) y_i
$$

onde:
- $f(x)$ é a estimativa da expectativa condicional $E[y|x]$;
- $N$ é o número de pontos de treinamento;
- $y_i$ são os valores alvo dos pontos de treinamento;
- $w_i(x)$ são os pesos atribuídos a cada ponto de treinamento, determinados pela função kernel.

Os pesos $w_i(x)$ são calculados usando um *smoothing kernel* [^1]. Um *smoothing kernel* é uma função de um único argumento que satisfaz as seguintes propriedades [^14.69]:

1. $\int \kappa(x) dx = 1$ (normalização);
2. $\int x \kappa(x) dx = 0$ (média zero);
3. $\int x^2 \kappa(x) dx > 0$ (variância positiva).

Exemplos de *smoothing kernels* incluem o kernel Gaussiano [^14.70]:

$$
\kappa(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}
$$
o kernel Epanechnikov [^14.74]:

$$
\kappa(x) = \frac{3}{4}(1 - x^2) \mathbb{I}(|x| \leq 1)
$$
e o kernel tri-cube [^14.75]:

$$
\kappa(x) = \frac{70}{81} (1 - |x|^3)^3 \mathbb{I}(|x| \leq 1)
$$
[^14.76]
Os pesos $w_i(x)$ são então definidos como [^14.88]:

$$
w_i(x) = \frac{\kappa_h(x - x_i)}{\sum_{j=1}^{N} \kappa_h(x - x_j)}
$$

onde $\kappa_h(x) = \frac{1}{h}\kappa(\frac{x}{h})$ e $h$ é o parâmetro de *bandwidth* que controla a largura do kernel [^14.71].

#### Kernel Density Estimation (KDE)

Kernel regression está intimamente relacionado ao **Kernel Density Estimation (KDE)** [^14.72]. Enquanto KDE é usado para estimar a função de densidade de probabilidade de uma variável, kernel regression usa KDE para estimar a expectativa condicional de uma variável, dados os valores de outra. Em KDE, a densidade é estimada como [^14.78]:

$$
\hat{p}(x) = \frac{1}{N} \sum_{i=1}^{N} \kappa_h(x - x_i)
$$

onde $\kappa_h$ é um *smoothing kernel* com *bandwidth* $h$ [^14.71].

#### Escolha do *Bandwidth*

A escolha do *bandwidth* $h$ é crucial para o desempenho do kernel regression [^14.71]. Um *bandwidth* muito pequeno resultará em uma estimativa *noisy*, enquanto um *bandwidth* muito grande resultará em uma estimativa *smoothed* demais [^14.71]. Um método comum para escolher o *bandwidth* é minimizar um estimador do risco frequencista, como validação cruzada [^14.72]. Para dados unidimensionais com densidade Gaussiana e kernels Gaussianos, o *bandwidth* ótimo é dado por [^14.89]:

$$
h = \left( \frac{4}{3N} \right)^{1/5} \hat{\sigma}
$$

onde $\hat{\sigma}$ é uma estimativa robusta do desvio padrão, calculada usando o desvio absoluto mediano (MAD) [^14.90, 14.91]:

$$
MAD = \text{median}(|x - \text{median}(x)|)
$$
$$
\hat{\sigma} = \frac{1.4826 \cdot MAD}{0.6745}
$$

#### Locally Weighted Regression

Uma extensão do kernel regression é a **locally weighted regression** [^14.95]. Em vez de ajustar uma função constante localmente, a locally weighted regression ajusta um modelo de regressão linear localmente [^14.95]. Isto é feito minimizando a seguinte função de perda ponderada [^14.94]:

$$
\min_{\beta(x^*)} \sum_{i=1}^{N} \kappa(x^*, x_i) [y_i - \beta(x^*) \phi(x_i)]^2
$$

onde $\phi(x) = [1, x]$ e $\kappa(x^*, x_i)$ é um kernel que pondera os pontos de dados próximos a $x^*$ [^14.94]. A solução para este problema de mínimos quadrados ponderados é [^14.95]:

$$
\beta(x^*) = (\Phi^T D(x^*) \Phi)^{-1} \Phi^T D(x^*) y
$$

onde $\Phi$ é a matriz de projeto $N \times (D + 1)$ e $D = \text{diag}(\kappa(x^*, x_i))$ [^14.95]. A predição é então dada por [^14.96]:

$$
\hat{f}(x) = \phi(x^*)^T \beta(x^*) = \sum_{i=1}^{N} w_i(x^*) y_i
$$

onde $w_i(x^*)$ é o *equivalent kernel*, combinando o kernel de suavização local com o efeito da regressão linear [^14.96].

### Conclusão

Kernel regression é uma técnica flexível e não paramétrica para estimar a expectativa condicional de uma variável resposta [^1]. Através do uso de kernels, ele pode modelar relações não lineares complexas sem assumir uma forma paramétrica específica [^1]. A escolha do kernel e do *bandwidth* é crucial para o desempenho, e várias técnicas estão disponíveis para selecionar esses parâmetros [^14.72]. Kernel regression está intimamente relacionado a KDE e pode ser estendido para locally weighted regression para melhorar ainda mais a precisão e a flexibilidade [^14.95].

### Referências
[^1]: Capítulo 14, Kernels.
[^14.69]: Seção 14.7.1, Kernels para construir modelos generativos.
[^14.70]: Seção 14.7.1, Kernels para construir modelos generativos.
[^14.71]: Seção 14.7.1, Kernels para construir modelos generativos.
[^14.72]: Seção 14.7.2, Kernel density estimation (KDE).
[^14.74]: Seção 14.7.1, Kernels para construir modelos generativos.
[^14.75]: Seção 14.7.1, Kernels para construir modelos generativos.
[^14.76]: Seção 14.7.1, Kernels para construir modelos generativos.
[^14.78]: Seção 14.7.2, Kernel density estimation (KDE).
[^14.82]: Seção 14.7.4, Kernel regression.
[^14.83]: Seção 14.7.4, Kernel regression.
[^14.84]: Seção 14.7.4, Kernel regression.
[^14.85]: Seção 14.7.4, Kernel regression.
[^14.86]: Seção 14.7.4, Kernel regression.
[^14.87]: Seção 14.7.4, Kernel regression.
[^14.88]: Seção 14.7.4, Kernel regression.
[^14.89]: Seção 14.7.4, Kernel regression.
[^14.90]: Seção 14.7.4, Kernel regression.
[^14.91]: Seção 14.7.4, Kernel regression.
[^14.92]: Seção 14.7.5, Locally weighted regression.
[^14.93]: Seção 14.7.5, Locally weighted regression.
[^14.94]: Seção 14.7.5, Locally weighted regression.
[^14.95]: Seção 14.7.5, Locally weighted regression.
[^14.96]: Seção 14.7.5, Locally weighted regression.
<!-- END -->