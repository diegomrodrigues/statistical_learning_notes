## Locally Weighted Regression: Enhancing Kernel Regression
### Introdução
Este capítulo explora a regressão ponderada localmente (Locally Weighted Regression - LWR), uma técnica que aprimora a regressão kernel ao ajustar modelos de regressão linear localmente em cada ponto. A LWR utiliza uma abordagem de mínimos quadrados ponderados, empregando funções kernel para ponderar os pontos de dados com base em sua proximidade ao ponto de interesse, resultando em um modelo mais flexível e adaptável [^1]. A regressão ponderada localmente é apresentada na seção de kernels para a construção de modelos generativos [^1].

### Conceitos Fundamentais
A regressão ponderada localmente (LWR), também conhecida como *locally weighted scatterplot smoothing (LOWESS)* [^5], representa uma evolução da regressão kernel, onde em vez de ajustar uma constante localmente, ajusta-se um modelo linear.

**Regressão Kernel:** Na regressão kernel, a predição é dada por uma média ponderada dos valores alvo dos pontos de treinamento, onde os pesos são determinados por uma função kernel que mede a similaridade entre o ponto de consulta e os pontos de treinamento. Formalmente, a predição $\hat{f}(x)$ em um ponto $x$ é dada por:
$$ \hat{f}(x) = \sum_{i=1}^{N} w_i(x) y_i $$
onde $w_i(x)$ são os pesos derivados da função kernel e $y_i$ são os valores alvo correspondentes aos pontos de treinamento $x_i$ [^5].

**Regressão Ponderada Localmente:** A LWR generaliza esta abordagem ao ajustar um modelo linear localmente em cada ponto de consulta. Em vez de simplesmente ponderar os valores alvo, a LWR pondera os pontos de dados ao ajustar um modelo linear usando mínimos quadrados ponderados. O objetivo é minimizar a soma dos erros quadrados ponderados:
$$ \min_{\beta(x)} \sum_{i=1}^{N} K(x, x_i) [y_i - \beta(x)^T \phi(x_i)]^2 $$
onde $K(x, x_i)$ é a função kernel, $\phi(x_i)$ são as características (features) dos pontos de dados e $\beta(x)$ são os coeficientes do modelo linear ajustado localmente no ponto $x$ [^5].

**Vantagens da LWR:**
1.  **Flexibilidade:** A LWR é capaz de se adaptar a relações não lineares complexas nos dados, ajustando um modelo linear diferente em cada ponto.
2.  **Adaptabilidade:** A LWR pode ajustar automaticamente a complexidade do modelo com base na densidade dos dados, usando uma largura de banda kernel menor em regiões de alta densidade e uma largura de banda maior em regiões de baixa densidade [^5].

**Implementação da LWR:**
1.  **Seleção do Kernel:** Escolha uma função kernel apropriada, como a gaussiana ou a tricúbica, para ponderar os pontos de dados com base em sua proximidade ao ponto de consulta [^2, 14].
2.  **Ajuste Local:** Para cada ponto de consulta $x$, calcule os pesos $K(x, x_i)$ para todos os pontos de treinamento $x_i$ [^5].
3.  **Mínimos Quadrados Ponderados:** Ajuste um modelo linear usando mínimos quadrados ponderados, minimizando a soma dos erros quadrados ponderados [^5].
4.  **Predição:** Use o modelo linear ajustado para fazer uma predição no ponto de consulta $x$ [^5].

**Equivalência ao Kernel:**
Na seção 14.7.5 [^5] é apresentada a possibilidade de reescrever a predição feita por regressão kernel da seguinte forma:
$$ \hat{f}(x) = \sum_{i=1}^N y_i \frac{\kappa(x, x_i)}{\sum_{i'=1}^N \kappa(x, x_{i'})} $$

É notado que $\kappa(x, x_i)$ não necessita ser um *smoothing kernel* [^5]. Se não for, não é necessário o termo de normalização, portanto, pode-se reescrever:
$$ \hat{f}(x) = \sum_{i=1}^N y_i \kappa(x, x_i) $$

A regressão ponderada localmente é essencialmente ajustar uma função constante localmente [^5]. É possível melhorar este processo ajustando um modelo de regressão linear para cada ponto $x_*$ ao resolver:
$$ \min_{\beta(x_*)} \sum_{i=1}^N \kappa(x_*, x_i) [y_i - \beta(x_*) \phi(x_i)]^2 $$
onde $\phi(x) = [1, x]$ [^5].

Este método é chamado de regressão ponderada localmente [^5].

**Kernel Equivalente:**
O termo $w_i(x*)$, que combina o kernel de suavização local com o efeito da regressão linear, é chamado de *equivalent kernel* [^5].

### Conclusão
A regressão ponderada localmente representa uma técnica poderosa e flexível para modelagem de relações não lineares complexas nos dados. Ao ajustar modelos lineares localmente e ponderar os pontos de dados com base em sua proximidade, a LWR é capaz de se adaptar a diferentes estruturas de dados e fornecer predições precisas e robustas.

### Referências
[^1]: Capítulo 14, Kernels
[^2]: Seção 14.2, Kernel functions
[^3]: Seção 14.2.1, RBF kernels
[^4]: Seção 14.2.2, Kernels for comparing documents
[^5]: Seção 14.7.5, Locally weighted regression
<!-- END -->