## Regressão Localmente Ponderada e Kernels Equivalentes

### Introdução
Este capítulo explora a **regressão localmente ponderada (LWR)** como uma técnica para melhorar a aproximação de funções, complementando a discussão sobre kernels para construção de modelos generativos. A regressão localmente ponderada, ao invés de aproximar uma função constante localmente, ajusta um modelo de regressão linear a cada ponto de interesse, permitindo uma adaptação mais precisa às características locais dos dados. A LWR pode ser entendida como uma combinação entre um **kernel de suavização local** e o efeito da regressão linear, resultando em um **kernel equivalente** [^512].

### Conceitos Fundamentais

A ideia central da regressão localmente ponderada é que, em vez de assumir uma função constante em uma vizinhança local, podemos ajustar um modelo linear para cada ponto de consulta $x^*$. O objetivo é minimizar o erro ponderado dos dados de treinamento, onde o peso de cada ponto de treinamento $x_i$ depende de sua proximidade a $x^*$. Matematicamente, isso é expresso como [^512]:

$$ \min_{\beta(x^*)} \sum_{i} \kappa(x^*, x_i) [y_i - \beta(x^*)^T \phi(x_i)]^2 $$

onde:
- $\kappa(x^*, x_i)$ é um **kernel de ponderação local**, que atribui pesos maiores a pontos $x_i$ mais próximos de $x^*$ e pesos menores a pontos mais distantes.
- $\beta(x^*)$ é o vetor de coeficientes do modelo de regressão linear ajustado para o ponto $x^*$.
- $\phi(x) = [1, x]$ é a função de *features*, que inclui um termo constante e o valor de $x$.

Essa formulação permite que o modelo se adapte localmente aos dados, capturando relações lineares que podem não ser aparentes em uma escala global.

**Kernel de Ponderação Local:**
O kernel $\kappa(x^*, x_i)$ desempenha um papel crucial na LWR. Ele determina a influência de cada ponto de treinamento no ajuste do modelo local. Kernels comuns incluem o **kernel Gaussiano** (RBF) [^480]:

$$ \kappa(x^*, x_i) = \exp\left(-\frac{||x^* - x_i||^2}{2\sigma^2}\right) $$

onde $\sigma$ controla a largura da vizinhança local. Outros kernels, como o **Epanechnikov** ou **Tricube** [^508], também podem ser utilizados, especialmente quando se deseja um suporte compacto.

**Interpretação Matricial e Kernel Equivalente:**
A solução para o problema de minimização pode ser expressa em forma matricial. Definindo [^512]:

$$ \Phi = \begin{bmatrix} \phi(x_1)^T \\ \vdots \\ \phi(x_N)^T \end{bmatrix}, \quad y = \begin{bmatrix} y_1 \\ \vdots \\ y_N \end{bmatrix}, \quad D(x^*) = \text{diag}(\kappa(x^*, x_i)) $$

onde $D(x^*)$ é uma matriz diagonal contendo os pesos do kernel. A solução para $\beta(x^*)$ é:

$$ \beta(x^*) = (\Phi^T D(x^*) \Phi)^{-1} \Phi^T D(x^*) y $$

E a predição para o ponto $x^*$ é:

$$ \hat{f}(x^*) = \phi(x^*)^T \beta(x^*) = \phi(x^*)^T (\Phi^T D(x^*) \Phi)^{-1} \Phi^T D(x^*) y = \sum_{i} w_i(x^*) y_i $$

onde $w_i(x^*)$ é o **kernel equivalente**, que combina o kernel de suavização local $\kappa(x^*, x_i)$ com o efeito da regressão linear. Ele pode ser expresso como [^512]:

$$ w_i(x^*) = \phi(x^*)^T (\Phi^T D(x^*) \Phi)^{-1} \Phi^T D(x^*)_i $$

**LOESS/LOWESS:**
Um exemplo notável de LWR é o método **LOESS (LOcally Estimated Scatterplot Smoothing)** ou **LOWESS (LOcally WEighted Scatterplot Smoothing)** [^512]. Este método utiliza LWR para suavizar gráficos de dispersão, permitindo visualizar tendências em dados ruidosos.

### Conclusão

A regressão localmente ponderada oferece uma abordagem flexível para modelar relações complexas em dados, adaptando-se localmente às características dos dados. Ao combinar kernels de suavização com modelos de regressão linear, a LWR consegue capturar nuances que seriam perdidas por modelos mais rígidos. A interpretação em termos de um kernel equivalente fornece uma visão unificada da técnica, conectando-a com outros métodos baseados em kernels discutidos ao longo deste capítulo. A LWR e suas variantes, como LOESS, são ferramentas valiosas no arsenal de um cientista de dados, oferecendo uma forma poderosa de explorar e modelar dados complexos.

### Referências
[^480]: Chapter 14. Kernels, 14.2.1 RBF kernels
[^508]: Chapter 14. Kernels, 14.7.2 Kernel density estimation (KDE)
[^512]: Chapter 14. Kernels, 14.7.5 Locally weighted regression
<!-- END -->