## Mercer Kernels: Positive Definiteness and Feature Spaces

### Introdução

Este capítulo explora em detalhe os **Mercer kernels**, também conhecidos como *positive definite kernels*, um conceito fundamental na teoria de kernels e em diversas aplicações de machine learning [^481]. A importância dos Mercer kernels reside na sua capacidade de garantir a positividade definida da **Gram matrix**, uma propriedade crucial para a convergência de certos algoritmos e para a interpretação do kernel como um produto interno num espaço de características (feature space) [^481].

### Conceitos Fundamentais

Um **Mercer kernel** $k(x, x')$ é uma função real de dois argumentos, $x$ e $x'$, pertencentes a um espaço abstrato $X$ [^479]. Tipicamente, essa função é simétrica, ou seja, $k(x, x') = k(x', x)$, e não negativa, ou seja, $k(x, x') > 0$, permitindo a sua interpretação como uma medida de similaridade entre os objetos $x$ e $x'$ [^479]. No entanto, a propriedade mais importante de um Mercer kernel é a garantia de que a **Gram matrix** $K$, definida como [^481]:

$$ K = \begin{bmatrix} k(x_1, x_1) & \cdots & k(x_1, x_N) \\ \vdots & \ddots & \vdots \\ k(x_N, x_1) & \cdots & k(x_N, x_N) \end{bmatrix} $$

seja *positive definite* para qualquer conjunto de inputs $\{x_i\}_{i=1}^N$ [^481]. A positividade definida da Gram matrix é uma condição essencial para garantir que o kernel possa ser interpretado como um produto interno num espaço de características de alta dimensão, implicitamente definido pelo kernel [^481].

**Mercer's Theorem**

O teorema de Mercer estabelece uma ligação fundamental entre a positividade definida da Gram matrix e a existência de uma função $\phi$ que mapeia os elementos do espaço de entrada $x \in X$ para um espaço de características $R^D$, tal que [^481]:

$$ k(x, x') = \phi(x)^T \phi(x') $$

Este teorema garante que, se a Gram matrix $K$ for positive definite, ela pode ser decomposta como $K = U \Lambda U^T$, onde $\Lambda$ é uma matriz diagonal de *eigenvalues* positivos $\lambda_i > 0$ [^481]. A função $\phi$ depende das eigenfunções de $k$, e $D$ é a dimensão do espaço de características, que pode ser potencialmente infinita [^481].

**Exemplos de Mercer Kernels**

Existem diversos exemplos de Mercer kernels, incluindo [^481]:

*   **Gaussian kernel (RBF kernel):** definido por $k(x, x') = \exp(-\frac{||x - x'||^2}{2\sigma^2})$, onde $\sigma$ é o *bandwidth* do kernel [^480].
*   **Cosine similarity kernel:** definido por $k(x_i, x_{i'}) = \frac{x_i^T x_{i'}}{||x_i||_2 ||x_{i'}||_2}$, que mede a similaridade entre dois documentos [^480].
*   **Polynomial kernel:** definido por $k(x, x') = (\gamma x^T x' + r)^M$, onde $\gamma > 0$ e $r > 0$, e $M$ é o grau do polinómio [^481].

É importante notar que nem todos os kernels são Mercer kernels. Um exemplo de um kernel que não é um Mercer kernel é o **sigmoid kernel**, definido por $k(x, x') = \tanh(\gamma x^T x' + r)$ [^482].

**Construção de Novos Mercer Kernels**

Em geral, estabelecer que um kernel é um Mercer kernel pode ser uma tarefa complexa, requerendo técnicas de análise funcional [^482]. No entanto, é possível construir novos Mercer kernels a partir de kernels mais simples utilizando um conjunto de regras padronizadas. Por exemplo, se $k_1$ e $k_2$ são ambos Mercer kernels, então $k(x, x') = k_1(x, x') + k_2(x, x')$ também é um Mercer kernel [^482].

### Conclusão

Os Mercer kernels desempenham um papel crucial em diversas aplicações de machine learning, permitindo a utilização de algoritmos lineares em espaços de características de alta dimensão, sem a necessidade de explicitar o mapeamento $\phi$ [^481]. A garantia da positividade definida da Gram matrix assegura a convergência e a validade teórica desses algoritmos [^481].

### Referências
[^479]: Kernel functions - We define a kernel function to be a real-valued function of two arguments, к(x, x') ∈ R, for x, x' ∈ X. Typically the function is symmetric (i.е., к(x, x') = к(x', x)), and non-negative (i.e., к(x, x') > 0), so it can be interpreted as a measure of similarity, but this is not required.
[^480]: RBF kernels, Kernels for comparing documents.
[^481]: Mercer (positive definite) kernels - Some methods that we will study require that the kernel function satisfy the requirement that the Gram matrix, defined by K = [...], be positive definite for any set of inputs {x}1. We call such a kernel a Mercer kernel, or positive definite kernel. It can be shown (Schoelkopf and Smola 2002) that the Gaussian kernel is a Mercer kernel as is the cosine similarity kernel (Sahami and Heilman 2006). The importance of Mercer kernels is the following result, known as Mercer's theorem. If the Gram matrix is positive definite, we can compute an eigenvector decomposition of it as follows K = UTAU where A is a diagonal matrix of eigenvalues 入¿ > 0. Now consider an element of K: [...]. In general, if the kernel is Mercer, then there exists a function & mapping x ∈ X to RD such that к(х, х') = (x)(x') where & depends on the eigen functions of k (so D is a potentially infinite dimensional space). For example, consider the (non-stationary) polynomial kernel (x, x') = (yxx' + r)M, where r > 0.
[^482]: In general, establishing that a kernel is a Mercer kernel is difficult, and requires techniques from functional analysis. However, one can show that it is possible to build up new Mercer kernels from simpler ones using a set of standard rules. For example, if k₁ and K2 are both Mercer, so is к(x, x') = к₁(x, x') + к2(x, x').

<!-- END -->