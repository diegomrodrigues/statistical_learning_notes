## Cosine Similarity Kernel e TF-IDF no Contexto de Kernels

### Introdução
Este capítulo expande sobre o conceito de **kernel functions** introduzido anteriormente [^1], focando especificamente na **cosine similarity** como uma ferramenta para comparar documentos e como a representação **TF-IDF** aprimora essa comparação [^2]. A **cosine similarity** é um kernel útil para dados representados como *bag-of-words*, e o **TF-IDF** é uma técnica de pré-processamento importante para melhorar a performance em tarefas de classificação e recuperação de informação.

### Conceitos Fundamentais

#### Cosine Similarity Kernel

A **cosine similarity** é uma medida de similaridade entre dois vetores que calcula o cosseno do ângulo entre eles [^2]. No contexto de documentos, os vetores representam a frequência de palavras nos documentos (abordagem *bag-of-words*). A fórmula para a **cosine similarity** entre dois documentos representados pelos vetores $x_i$ e $x_{i'}$ é dada por [^2]:

$$ \kappa(x_i, x_{i'}) = \frac{x_i^T x_{i'}}{||x_i||_2 ||x_{i'}||_2} $$

Onde:
- $x_i^T x_{i'}$ é o produto interno dos vetores $x_i$ e $x_{i'}$.
- $||x_i||_2$ e $||x_{i'}||_2$ são as normas euclidianas (L2) dos vetores $x_i$ e $x_{i'}$.

A **cosine similarity** varia entre 0 e 1, onde 0 indica que os vetores são ortogonais (não têm palavras em comum) e 1 indica que os vetores são idênticos (apontam na mesma direção) [^2].

#### TF-IDF (Term Frequency-Inverse Document Frequency)

A representação **TF-IDF** é uma técnica para ponderar palavras em um documento com base em sua frequência no documento (term frequency) e sua raridade no corpus (inverse document frequency) [^2]. Isso ajuda a mitigar o impacto de palavras comuns (stop words) e aumentar a importância de termos discriminativos [^2].

A **term frequency** (*tf*) é definida como uma transformação logarítmica da contagem de palavras:

$$ tf(x_{ij}) = \log(1 + x_{ij}) $$

Onde $x_{ij}$ é o número de vezes que a palavra $j$ ocorre no documento $i$ [^2]. A transformação logarítmica reduz o impacto de palavras que ocorrem muitas vezes em um documento [^2].

A **inverse document frequency** (*idf*) é definida como:

$$ idf(j) = \log \frac{N}{1 + \sum_{i=1}^{N} \mathbb{I}(x_{ij} > 0)} $$

Onde $N$ é o número total de documentos, e $\mathbb{I}(x_{ij} > 0)$ é uma função indicadora que é 1 se a palavra $j$ ocorre no documento $i$ e 0 caso contrário [^2]. O *idf* mede a raridade de uma palavra no corpus [^2].

A representação **TF-IDF** é então definida como [^3]:

$$ tf\text{-}idf(x_i) = [tf(x_{ij}) \times idf(j)]_{j=1}^D $$

Onde $D$ é o número total de termos no vocabulário [^3].

#### Kernel com TF-IDF

Para utilizar **TF-IDF** com a **cosine similarity**, substituímos os vetores de contagem de palavras $x_i$ por vetores **TF-IDF** $\phi(x_i)$ na fórmula da **cosine similarity** [^3]:

$$ \kappa(x_i, x_{i'}) = \frac{\phi(x_i)^T \phi(x_{i'})}{||\phi(x_i)||_2 ||\phi(x_{i'})||_2} $$

Onde $\phi(x_i) = tf\text{-}idf(x_i)$ [^3]. Este kernel melhora significativamente os resultados em tarefas de recuperação de informação [^3].

#### Mercer Kernel e Cosine Similarity

Um **Mercer kernel** (ou *positive definite kernel*) é uma função kernel que satisfaz a condição de que a matriz de Gram, definida por $K_{ij} = \kappa(x_i, x_j)$ para um conjunto de inputs $\{x_i\}_{i=1}^N$, é positiva definida [^3]. A **cosine similarity kernel** é um **Mercer kernel** [^3], o que significa que pode ser usado em algoritmos que requerem essa propriedade, como **Support Vector Machines (SVMs)**.

A importância de **Mercer kernels** reside no **Mercer's theorem**, que afirma que se a matriz de Gram é positiva definida, então podemos decompor a matriz K como $K = U \Lambda U^T$, onde $\Lambda$ é uma matriz diagonal com autovalores positivos $\lambda_i > 0$ [^3].  Se definirmos $\phi(x) = \Lambda^{1/2}U^T$, então $K_{ij} = \phi(x_i)^T \phi(x_j)$, o que significa que podemos calcular os valores do kernel realizando um produto interno em um espaço de características implicitamente definido pelos autovetores $U$ [^3].

### Conclusão
A **cosine similarity**, combinada com a representação **TF-IDF**, oferece uma abordagem eficaz para comparar documentos textuais [^2].  A transformação **TF-IDF** permite que o kernel da **cosine similarity** capture melhor a semântica dos documentos, mitigando o impacto de palavras comuns e enfatizando termos discriminativos [^2]. Além disso, o fato de a **cosine similarity kernel** ser um **Mercer kernel** garante que pode ser usada em uma variedade de algoritmos de aprendizado de máquina que exigem essa propriedade [^3].

### Referências
[^1]: Capítulo 14, Kernels, Introdução.
[^2]: Capítulo 14, Kernels, 14.2.2 Kernels for comparing documents.
[^3]: Capítulo 14, Kernels, 14.2.3 Mercer (positive definite) kernels.
<!-- END -->