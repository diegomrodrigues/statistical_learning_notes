## Kernel Functions: Measuring Similarity in Abstract Spaces

### Introdução
No campo de **kernels**, a capacidade de medir a similaridade entre objetos sem a necessidade de pré-processá-los em vetores de características fixas é fundamental. Este capítulo explora em profundidade as **kernel functions**, também conhecidas como *funções kernel*, que quantificam a similaridade entre dois inputs em um espaço abstrato [^1]. Tais funções mapeiam os inputs para um valor real que serve como medida de sua relação [^1].

### Conceitos Fundamentais

Uma **kernel function**, denotada como $\kappa(x, x\')$, é uma função de valor real que quantifica a similaridade entre dois argumentos $x$ e $x\'$ no espaço de entrada $X$, mapeando-os para um valor em $\mathbb{R}$ [^1]. Frequentemente, assume-se que $\kappa$ é simétrica ($\kappa(x, x\') = \kappa(x\', x)$) e não negativa ($\kappa(x, x\') > 0$), permitindo a interpretação como uma medida de similaridade [^1]. No entanto, essas propriedades não são estritamente requeridas [^1].

**Definição Formal:** Uma função $\kappa: X \times X \rightarrow \mathbb{R}$ é uma função kernel se, para cada conjunto finito $\\{x_1, \dots, x_N\\} \subset X$, a matriz $K$ com entradas $K_{ij} = \kappa(x_i, x_j)$ é simétrica e positiva semi-definida. Tal matriz $K$ é conhecida como a **Gram matrix** [^3].

**Exemplos de Kernel Functions:**
1.  **RBF (Radial Basis Function) Kernels:** Também conhecidos como *squared exponential kernel* (SE kernel) ou *Gaussian kernel*, são definidos por [^2]:
    $$\kappa(x, x\') = \exp\left(-\frac{1}{2}(x - x\')^T \Sigma^{-1} (x - x\')\right) \quad (14.1)$$\n    Se $\Sigma$ é diagonal, a equação pode ser reescrita como [^2]:
    $$\kappa(x, x\') = \exp\left(-\frac{1}{2} \sum_{j=1}^D \frac{(x_j - x\'_j)^2}{\sigma_j^2}\right) \quad (14.2)$$\n    Onde $\sigma_j$ define a *characteristic length scale* da dimensão $j$ [^2]. Se $\sigma_j = \infty$, a dimensão correspondente é ignorada, resultando no **ARD (Automatic Relevance Determination) kernel** [^2]. Se $\Sigma$ é esférica, obtém-se o **isotropic kernel** [^2]:
    $$\kappa(x, x\') = \exp\left(-\frac{||x - x\'||^2}{2\sigma^2}\right) \quad (14.3)$$\n    Aqui, $\sigma^2$ é conhecido como o *bandwidth* [^2].
2.  **Kernels para Comparar Documentos:** No contexto de classificação ou recuperação de documentos, é útil ter uma forma de comparar dois documentos, $x_i$ e $x_{i\'}$ [^2]. Utilizando uma representação "bag of words", onde $x_{ij}$ representa o número de vezes que a palavra $j$ ocorre no documento $i$, pode-se usar a **cosine similarity**, definida por [^2]:
    $$\kappa(x_i, x_{i\'}) = \frac{x_i^T x_{i\'}}{||x_i||_2 ||x_{i\'}||_2} \quad (14.4)$$\n    Esta quantidade mede o cosseno do ângulo entre $x_i$ e $x_{i\'}$, interpretados como vetores [^2].\n    Para melhorar o desempenho, é comum substituir o vetor de contagem de palavras por uma representação **TF-IDF (Term Frequency Inverse Document Frequency)** [^2]. O *term frequency* é definido como [^2]:
    $$tf(x_{ij}) = \log(1 + x_{ij}) \quad (14.5)$$\n    E o *inverse document frequency* é definido como [^2]:
    $$idf(j) \triangleq \log \frac{N}{1 + \sum_{i=1}^N \mathbb{I}(x_{ij} > 0)} \quad (14.6)$$\n    Finalmente, define-se [^3]:
    $$tf\text{-}idf(x_i) \triangleq [tf(x_{ij}) \times idf(j)]_{j=1}^D \quad (14.7)$$\n    O kernel resultante usa a cosine similarity na representação TF-IDF [^3]:
    $$\kappa(x_i, x_{i\'}) = \frac{\phi(x_i)^T \phi(x_{i\'})}{||\phi(x_i)||_2 ||\phi(x_{i\'})||_2} \quad (14.8)$$\n    Onde $\phi(x) = tf\text{-}idf(x)$ [^3].
3.  **Mercer (Positive Definite) Kernels:** Algumas metodologias requerem que a função kernel satisfaça a condição de que a **Gram matrix**, definida por [^3]:
    $$K = \begin{bmatrix} \kappa(x_1, x_1) & \dots & \kappa(x_1, x_N) \\\\ \vdots & \ddots & \vdots \\\\ \kappa(x_N, x_1) & \dots & \kappa(x_N, x_N) \end{bmatrix} \quad (14.9)$$\n    seja positiva definida para qualquer conjunto de inputs $\\{x_i\\}_{i=1}^N$ [^3]. Tais kernels são chamados **Mercer kernels** ou **positive definite kernels** [^3].\n    **Mercer\'s Theorem:** Se a Gram matrix é positiva definida, é possível computar sua decomposição em autovalores [^3]:
    $$K = U \Lambda U^T \quad (14.10)$$\n    Onde $\Lambda$ é uma matriz diagonal com autovalores $\lambda_i > 0$ [^3]. Um elemento de $K$ é dado por [^3]:
    $$k_{ij} = (A^{1/2}U^T)_i (A^{1/2}U^T)_j \quad (14.11)$$\n    Definindo $\phi(x) = A^{1/2}U^Tx$, podemos escrever [^3]:
    $$k_{ij} = \phi(x_i)^T \phi(x_j) \quad (14.12)$$\n    Isso demonstra que as entradas na matriz kernel podem ser computadas realizando um produto interno de vetores de características implicitamente definidos pelos autovetores $U$ [^3]. Em geral, se o kernel é Mercer, existe uma função $\phi$ que mapeia $x \in X$ para $\mathbb{R}^D$ tal que [^3]:
    $$\kappa(x, x\') = \phi(x)^T \phi(x\') \quad (14.13)$$\n    Onde $\phi$ depende das autofunções de $\kappa$ (então $D$ é um espaço potencialmente de dimensão infinita) [^3].
    
    **Exemplo:** Considere o **polynomial kernel** (não estacionário) $\kappa(x, x\') = (\gamma x^T x\' + r)^M$, onde $r > 0$ [^3]. É possível mostrar que o vetor de características correspondente $\phi(x)$ conterá todos os termos até o grau $M$ [^3]. Por exemplo, se $M = 2$, $\gamma = r = 1$ e $x, x\' \in \mathbb{R}^2$, temos [^3]:
    $$(1 + x^T x\')^2 = (1 + x_1x\'_1 + x_2x\'_2)^2 \quad (14.14)$$\n    $$= 1 + 2x_1x\'_1 + 2x_2x\'_2 + (x_1x\'_1)^2 + (x_2x\'_2)^2 + 2x_1x\'_1x_2x\'_2 \quad (14.15)$$\n    Que pode ser escrito como $\phi(x)^T \phi(x\')$, onde [^4]:
    $$\phi(x) = [1, \sqrt{2}x_1, \sqrt{2}x_2, x_1^2, x_2^2, \sqrt{2}x_1x_2]^T \quad (14.16)$$\n4.  **Linear Kernels:** Derivar o vetor de características implícito por um kernel é geralmente difícil, e apenas possível se o kernel for Mercer [^4]. No entanto, derivar um kernel de um vetor de características é fácil: basta usar [^4]\n    $$\kappa(x, x\') = \phi(x)^T \phi(x\') = \langle \phi(x), \phi(x\') \rangle \quad (14.18)$$\n    Se $\phi(x) = x$, obtemos o **linear kernel**, definido por [^4]:
    $$\kappa(x, x\') = x^T x\' \quad (14.19)$$\n5. **Matern Kernels:** O Matern kernel, comumente utilizado na regressão de processos gaussianos, tem a seguinte forma [^4]:\n    $$\kappa(r) = \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{\sqrt{2\nu} r}{l}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu} r}{l}\right) \quad (14.20)$$\n    onde $r = ||x - x\'||$, $\nu > 0$, $l > 0$, e $K_\nu$ é uma função de Bessel modificada [^4].
6. **String Kernels:** A real potência dos kernels surge quando os inputs são objetos estruturados [^5]. Um exemplo notável é a comparação de strings de tamanho variável usando um **string kernel** [^5]. A similaridade entre duas strings pode ser definida como o número de substrings que elas têm em comum [^5].

### Conclusão
As **kernel functions** oferecem uma maneira poderosa e flexível de medir a similaridade entre objetos complexos, sem a necessidade de representá-los explicitamente como vetores de características [^1]. Através de diferentes tipos de kernels, como RBF, cosine similarity, e Mercer kernels, é possível adaptar algoritmos para trabalhar diretamente com as propriedades intrínsecas dos dados [^2]. A escolha do kernel apropriado é crucial para o sucesso de muitas aplicações em machine learning, e a compreensão de suas propriedades é essencial para o desenvolvimento de modelos eficazes [^3].

### Referências
[^1]: Pattern Recognition and Machine Learning, Christopher Bishop, 2006, p. 479-480
[^2]: Pattern Recognition and Machine Learning, Christopher Bishop, 2006, p. 480-481
[^3]: Pattern Recognition and Machine Learning, Christopher Bishop, 2006, p. 481-482
[^4]: Pattern Recognition and Machine Learning, Christopher Bishop, 2006, p. 482
[^5]: Pattern Recognition and Machine Learning, Christopher Bishop, 2006, p. 483
<!-- END -->