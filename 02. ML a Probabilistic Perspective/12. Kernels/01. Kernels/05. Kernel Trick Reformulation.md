## O Truque do Kernel

### Introdução
Em muitos problemas de machine learning, os dados que desejamos classificar ou agrupar não são naturalmente representados como vetores de características de tamanho fixo [^1]. Abordagens alternativas envolvem medir a similaridade entre objetos sem pré-processamento em vetores de características [^1]. O **truque do kernel** oferece uma solução elegante para lidar com essa limitação, permitindo que algoritmos operem em espaços de alta dimensão implicitamente definidos pelo kernel, sem calcular explicitamente o mapeamento de características [^1]. Este capítulo explora em detalhes o truque do kernel, suas aplicações e suas implicações.

### Conceitos Fundamentais

O truque do kernel é uma técnica que reformula algoritmos para usar **funções kernel** em vez de vetores de características explícitos [^1]. Uma função kernel, denotada por $\kappa(x, x')$, é uma função de valor real de dois argumentos, $x$ e $x'$, que mede a similaridade entre esses objetos [^1, 2]. Tipicamente, a função é simétrica, ou seja, $\kappa(x, x') = \kappa(x', x)$, e não negativa, ou seja, $\kappa(x, x') > 0$, podendo ser interpretada como uma medida de similaridade [^2]. No entanto, essas propriedades não são estritamente necessárias [^2].

A essência do truque do kernel reside em modificar algoritmos para substituir **produtos internos** por chamadas de função kernel [^1]. Isso possibilita o uso de métodos kernel sem definir explicitamente vetores de características [^1]. A aplicabilidade do truque do kernel se restringe a algoritmos que podem ser expressos unicamente em termos de produtos internos, como a classificação kernelizada do vizinho mais próximo e o agrupamento kernelizado K-medoids [^1].

Formalmente, se um algoritmo pode ser expresso usando apenas produtos internos da forma $(x, x')$, onde $x$ e $x'$ são vetores no espaço de entrada, então podemos substituir cada produto interno pela função kernel correspondente, $\kappa(x, x')$. Isso tem o efeito de mapear implicitamente os dados para um espaço de características de alta dimensão (possivelmente infinito) sem nunca calcular explicitamente as coordenadas dos dados nesse espaço.

**Mercer Kernels e Espaços de Características**
A escolha de um kernel específico implica a existência de um espaço de características no qual o produto interno entre vetores nesse espaço é igual ao valor do kernel aplicado aos vetores originais. Mais precisamente, se $\kappa(x, x')$ é um **Mercer kernel** (ou kernel definido positivo), então existe uma função $\phi$ que mapeia $x$ para um espaço de características de alta dimensão tal que:

$$\kappa(x, x') = \langle \phi(x), \phi(x') \rangle$$

onde $\langle \cdot, \cdot \rangle$ denota o produto interno no espaço de características.

O **Teorema de Mercer** garante que, se a matriz de Gram definida como $K_{ij} = \kappa(x_i, x_j)$ for positiva definida para qualquer conjunto de entradas $\\{x_i\\}_{i=1}^N$, então existe um mapeamento $\phi$ para um espaço de características tal que a equação acima é satisfeita [^3].

**Exemplo: Kernel Polinomial**
Considere o kernel polinomial não estacionário:
$$\kappa(x, x') = (\gamma x^T x' + r)^M$$
onde $\gamma > 0$, $r > 0$ e $M$ é um inteiro positivo [^3]. Este kernel corresponde a um espaço de características que contém todos os termos até o grau $M$. Por exemplo, se $M = 2$, $\gamma = r = 1$ e $x, x' \in \mathbb{R}^2$, então:

$$\begin{aligned}
(1 + x^T x')^2 &= (1 + x_1 x'_1 + x_2 x'_2)^2 \\\\
&= 1 + 2x_1 x'_1 + 2x_2 x'_2 + (x_1 x'_1)^2 + (x_2 x'_2)^2 + 2x_1 x'_1 x_2 x'_2
\end{aligned}$$

Nesse caso, o espaço de características tem dimensão 6, e o mapeamento $\phi(x)$ é dado por:
$$\phi(x) = [1, \sqrt{2}x_1, \sqrt{2}x_2, x_1^2, x_2^2, \sqrt{2}x_1 x_2]^T$$
Assim, $\kappa(x, x') = \phi(x)^T \phi(x')$ [^3].

**Exemplo: RBF Kernel**
O **kernel RBF** (Radial Basis Function), também conhecido como kernel Gaussiano, é definido como [^2]:
$$\kappa(x, x') = \exp\left(-\frac{||x - x'||^2}{2\sigma^2}\right)$$
onde $\sigma^2$ é o parâmetro de largura de banda [^2]. Ao contrário do kernel polinomial, o espaço de características correspondente ao kernel RBF tem dimensão *infinita* [^3].

**Algoritmos Kernelizados**

1.  **Kernelized Nearest Neighbor Classification:** No classificador do vizinho mais próximo (1NN), a distância euclidiana entre um vetor de teste e todos os pontos de treinamento precisa ser calculada [^11]. Essa distância pode ser kernelizada observando que [^11]:
    $$||x_i - x_{i'}||^2 = \langle x_i, x_i \rangle + \langle x_{i'}, x_{i'} \rangle - 2\langle x_i, x_{i'} \rangle$$
    Substituindo o produto interno pela função kernel, obtemos:
    $$||x_i - x_{i'}||^2 = \kappa(x_i, x_i) + \kappa(x_{i'}, x_{i'}) - 2\kappa(x_i, x_{i'})$$
    Isso nos permite aplicar o classificador do vizinho mais próximo a objetos de dados estruturados [^11].

2.  **Kernelized K-Medoids Clustering:** O algoritmo K-means tradicional usa a distância euclidiana para medir a dissimilaridade, o que nem sempre é apropriado para objetos estruturados [^11]. Para kernelizar o K-means, substituímos a distância euclidiana pela expressão kernelizada acima [^11]. No entanto, em vez de representar o centroide de cada cluster pela média de todos os vetores de dados atribuídos a esse cluster, fazemos com que cada centroide seja um dos vetores de dados em si [^12]. Assim, sempre lidamos com índices inteiros, em vez de objetos de dados [^12]. O objetivo é encontrar o objeto que minimiza a soma das distâncias a todos os outros objetos no mesmo cluster [^12].

### Conclusão

O truque do kernel é uma ferramenta poderosa que permite estender algoritmos lineares a espaços de características de alta dimensão, possibilitando a modelagem de relações complexas nos dados [^1]. Ao substituir produtos internos por funções kernel, podemos evitar o cálculo explícito de mapeamentos de características, tornando o processo computacionalmente eficiente [^1]. O truque do kernel é amplamente utilizado em diversas áreas do aprendizado de máquina, incluindo classificação, regressão e agrupamento [^1, 11, 12].

### Referências
[^1]: Página 479, Seção 14.1
[^2]: Página 479, Seção 14.2
[^3]: Página 481, Seção 14.2.3
[^11]: Página 489, Seção 14.4.1
[^12]: Página 489, Seção 14.4.2
<!-- END -->