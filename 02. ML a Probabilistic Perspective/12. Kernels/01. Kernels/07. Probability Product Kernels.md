## Probability Product Kernels

### Introdução
No contexto de **Kernels**, este capítulo se aprofunda nos **Probability Product Kernels (PPK)**, uma classe específica de kernels derivados de modelos generativos probabilísticos [^485]. Exploraremos a definição formal de PPKs, suas aproximações e a motivação por trás de seu uso na definição de similaridade entre dados. Além disso, discutiremos como modelos generativos probabilísticos podem ser usados para derivar outras funções kernel, como os Fisher Kernels.

### Conceitos Fundamentais
**Probability Product Kernels (PPK)** são uma abordagem para definir funções kernel com base em modelos generativos probabilísticos [^485]. A ideia central é medir a similaridade entre dois objetos $x_i$ e $x_j$ através da integral do produto de suas probabilidades, elevadas a uma potência $\rho$:
$$\
k(x_i, x_j) = \int p(x|x_i)^\rho p(x|x_j)^\rho dx \quad [14.23]
$$\
onde:
*   $k(x_i, x_j)$ é o valor do kernel entre os objetos $x_i$ e $x_j$.
*   $p(x|x_i)$ é a probabilidade de observar $x$ dado $x_i$.
*   $\rho$ é um parâmetro que controla a influência da probabilidade no cálculo da similaridade.

Em muitas aplicações, a probabilidade $p(x|x_i)$ é aproximada por $p(x|\theta(x_i))$, onde $\theta(x_i)$ é uma estimativa de parâmetro computada usando um único vetor de dados [^485]. Essa aproximação simplifica o cálculo do kernel, tornando-o mais tratável computacionalmente.

**Motivação e Interpretação**
A motivação por trás do uso de PPKs reside na capacidade de capturar a similaridade entre objetos com base em suas distribuições de probabilidade [^485]. Se dois objetos $x_i$ e $x_j$ levam a distribuições de probabilidade $p(x|x_i)$ e $p(x|x_j)$ que se sobrepõem significativamente, então a integral do produto dessas probabilidades será alta, indicando alta similaridade.

É importante notar que, embora possa parecer estranho ajustar um modelo a um único ponto de dados, o objetivo não é construir um modelo generativo preciso para cada ponto individualmente [^485]. Em vez disso, o modelo ajustado é usado como uma ferramenta para medir a similaridade entre os pontos de dados.

**Exemplo: Kernel RBF como um PPK**
Um exemplo notável é a derivação do **RBF (Radial Basis Function) kernel** como um caso especial de PPK [^485]. Suponha que $p(x|\theta) = \mathcal{N}(\mu, \sigma^2I)$, onde $\sigma^2$ é fixo. Se $\rho = 1$ e usamos $\mu(x_i) = x_i$ e $\mu(x_j) = x_j$, então:
$$\
k(x_i, x_j) = \frac{1}{(4\pi\sigma^2)^{D/2}} \exp\left(-\frac{1}{4\sigma^2} ||x_i - x_j||^2\right) \quad [14.24]
$$\
Este resultado mostra que o kernel RBF pode ser interpretado como um PPK sob certas condições, conectando assim modelos generativos probabilísticos com funções kernel amplamente utilizadas [^485].

**Vantagens e Aplicações**
PPKs oferecem várias vantagens, incluindo a capacidade de incorporar conhecimento de domínio na definição da função kernel e a capacidade de lidar com sequências de comprimento variável [^485]. Eles podem ser aplicados em uma variedade de tarefas, como classificação, regressão e agrupamento, onde a similaridade entre os dados precisa ser definida de forma flexível e adaptável.

**Fisher Kernels**
Outra abordagem eficiente para usar modelos generativos para definir kernels são os **Fisher kernels** [^485]. Em vez de integrar o produto das probabilidades, os Fisher kernels usam o gradiente do logaritmo da verossimilhança (score vector) para medir a similaridade:
$$\
k(x, x') = g(x)^T F^{-1} g(x') \quad [14.25]
$$\
onde:

*   $g(x) = \nabla_\theta \log p(x|\theta)$ é o score vector, avaliado no MLE $\hat{\theta}$ [^14.26].
*   $F = \nabla \nabla \log p(x|\theta)|_{\hat{\theta}}$ é a matriz de informação de Fisher [^14.27].

A intuição por trás dos Fisher kernels é que eles medem a similaridade entre dois pontos de dados com base em como eles "gostariam" de mover os parâmetros do modelo [^485]. Se os gradientes de dois pontos apontam na mesma direção no espaço de parâmetros, eles são considerados similares.

**String Kernels e Fisher Kernels**
Curiosamente, foi demonstrado que o **string kernel** (Seção 14.2.6) é equivalente ao Fisher kernel derivado de uma cadeia de Markov de ordem L [^481, 14.2.6].

### Conclusão
Probability Product Kernels e Fisher Kernels representam abordagens poderosas para derivar funções kernel de modelos generativos probabilísticos [^485]. Ao definir a similaridade com base na verossimilhança de pontos de dados sob um modelo generativo, esses kernels oferecem uma maneira flexível e adaptável de incorporar conhecimento de domínio e lidar com dados complexos. Sua capacidade de lidar com sequências de comprimento variável e conectar modelos generativos com métodos discriminativos os torna ferramentas valiosas em uma ampla gama de aplicações de aprendizado de máquina.

### Referências
[^485]: Page 485, Kernels derived from probabilistic generative models.
[^14.23]: Page 485, Equation for Probability product kernels.
[^14.24]: Page 485, Equation for RBF kernel derived from PPK.
[^14.25]: Page 485, Equation for Fisher kernels.
[^14.26]: Page 485, Definition of score vector.
[^14.27]: Page 485, Definition of Fisher information matrix.
[^481]: Page 481, Mercer (positive definite) kernels.
[^14.2.6]: Page 483, String kernels.

<!-- END -->