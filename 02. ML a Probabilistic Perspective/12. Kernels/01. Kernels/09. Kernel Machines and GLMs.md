## Kernel Machines como GLMs com Vetores de Características Kernelizados

### Introdução
Este capítulo explora o conceito de **Kernel Machines**, especificamente como elas se encaixam no framework dos **Generalized Linear Models (GLMs)**. Kernel machines oferecem uma abordagem poderosa para modelar relações não lineares nos dados, utilizando funções kernel para mapear os dados de entrada para espaços de características de alta dimensão [^1]. Este mapeamento permite que modelos lineares, como GLMs, capturem complexidades não lineares nos dados originais.

### Conceitos Fundamentais

**Kernel Machines e GLMs**
Kernel machines podem ser vistas como uma classe especial de GLMs onde o vetor de características de entrada é obtido através de uma transformação kernelizada dos dados originais [^4, 14.3]. Formalmente, o vetor de características $\phi(x)$ é definido como:
$$\
\phi(x) = [\kappa(x, \mu_1), ..., \kappa(x, \mu_K)]
$$\
onde:
*   $x$ é o vetor de entrada.
*   $\kappa(x, \mu_k)$ é a função kernel que mede a similaridade entre o vetor de entrada $x$ e o centróide $\mu_k$.
*   $\mu_k \in \mathcal{X}$ são um conjunto de $K$ centróides, que podem ser pontos de dados selecionados ou outros pontos representativos do espaço de entrada.
*   $\mathcal{X}$ é o espaço abstrato onde os objetos $x$ e $x'$ residem [^1].

**Funções Kernel**
A função kernel $\kappa(x, x')$ é uma função real-valorada que mede a similaridade entre dois objetos $x$ e $x'$ [^1, 14.2]. Tipicamente, a função é simétrica ($\kappa(x, x') = \kappa(x', x)$) e não negativa ($\kappa(x, x') > 0$), permitindo que seja interpretada como uma medida de similaridade, embora isso não seja mandatório [^1]. Um exemplo comum de função kernel é o **RBF (Radial Basis Function) kernel** (também conhecido como squared exponential kernel ou Gaussian kernel) [^2, 14.2.1]:
$$\
\kappa(x, x') = \exp\left(-\frac{1}{2}(x - x')^T \Sigma^{-1} (x - x')\right)
$$\
onde $\Sigma$ é uma matriz de covariância. Se $\Sigma$ é diagonal, podemos escrever:
$$\
\kappa(x, x') = \exp\left(-\sum_{j=1}^{D} \frac{(x_j - x'_j)^2}{2\sigma_j^2}\right)
$$\
onde $\sigma_j$ define a escala de comprimento característica da dimensão $j$ [^2]. Se $\Sigma$ é esférica, obtemos o kernel isotrópico:
$$\
\kappa(x, x') = \exp\left(-\frac{||x - x'||^2}{2\sigma^2}\right)
$$\
onde $\sigma^2$ é conhecido como a largura de banda (bandwidth) [^2]. A Equação 14.3 é um exemplo de uma função de base radial ou kernel RBF, já que é apenas uma função de $||x - x'||$ [^2].

**RBF Networks**
Quando a função kernel $\kappa$ é um RBF kernel, a kernel machine resultante é chamada de **RBF network** [^8, 14.3.1]. RBF networks são capazes de aproximar qualquer função contínua, tornando-as uma ferramenta poderosa para modelagem não linear.

**Escolha dos Centróides**
Uma questão crucial no uso de kernel machines é a escolha dos centróides $\mu_k$ [^9, 14.3.2]. Uma abordagem é escolher os centróides para preencher uniformemente o espaço ocupado pelos dados, o que pode ser viável em espaços Euclidianos de baixa dimensão [^9]. No entanto, essa abordagem se torna impraticável em dimensões mais altas devido à maldição da dimensionalidade. Outras abordagens incluem:
*   Otimização numérica dos parâmetros $\mu_k$ [^9].
*   Inferência MCMC [^9].
*   Identificação de clusters nos dados e atribuição de um centróide por cluster [^9].
*   Usar cada exemplo $x_i$ como um protótipo [^10, 14.3.2]:
    $$\
    \phi(x) = [\kappa(x, x_1), ..., \kappa(x, x_N)]
    $$\
    onde $N$ é o número de pontos de dados.

**Sparse Vector Machines**
Quando cada exemplo é usado como protótipo, a dimensionalidade do vetor de características $\phi(x)$ se torna igual ao número de pontos de dados [^10]. Para lidar com essa alta dimensionalidade, técnicas de promoção de esparsidade podem ser aplicadas, resultando em um modelo chamado **sparse vector machine** [^10]. Abordagens comuns incluem:
*   Regularização $l_1$, levando a uma $l_1$-regularized vector machine (LIVM) [^10].
*   Regularização $l_2$, levando a uma $l_2$-regularized vector machine (L2VM) [^10].
*   ARD/SBL (Automatic Relevance Determination/Sparse Bayesian Learning), levando a uma relevance vector machine (RVM) [^10].

**Kernel Trick**
Em vez de definir explicitamente o vetor de características kernelizado $\phi(x)$, podemos usar o **kernel trick**, que consiste em modificar o algoritmo para substituir todos os produtos internos da forma $\langle x, x' \rangle$ por uma chamada à função kernel $\kappa(x, x')$ [^10, 14.4]. Isso permite que o algoritmo opere implicitamente no espaço de características de alta dimensão sem nunca calcular explicitamente os vetores de características.

### Conclusão
Kernel machines, quando vistas como GLMs, oferecem uma maneira flexível e poderosa de modelar relações não lineares nos dados. Ao usar funções kernel para mapear os dados para espaços de características de alta dimensão, modelos lineares podem capturar complexidades não lineares. A escolha da função kernel e dos centróides é crucial para o desempenho do modelo, e técnicas de promoção de esparsidade podem ser aplicadas para lidar com a alta dimensionalidade resultante do uso de muitos centróides. O kernel trick permite que algoritmos operem implicitamente no espaço de características de alta dimensão, evitando a necessidade de calcular explicitamente os vetores de características. As SVMs (Support Vector Machines) usam essa combinação do kernel trick com uma função de perda modificada [^18].

### Referências
[^1]: Seção 14.1
[^2]: Seção 14.2.1
[^3]: Seção 14.2
[^4]: Seção 14.3
[^5]: Seção 14.3.1
[^6]: Seção 14.2.1
[^7]: Seção 14.2.1
[^8]: Seção 14.3.1
[^9]: Seção 14.3.2
[^10]: Seção 14.3.2
[^11]: Seção 14.4
[^12]: Seção 14.4
[^13]: Seção 14.4
[^14]: Seção 14.3
[^15]: Seção 14.3.1
[^16]: Seção 14.2.1
[^17]: Seção 14.3.1
[^18]: Seção 14.5

<!-- END -->