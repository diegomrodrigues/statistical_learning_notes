## Fisher Kernels: Similaridade Baseada no Gradiente do Log-Likelihood

### Introdução
Este capítulo explora os **Fisher kernels**, uma abordagem eficiente para definir kernels usando modelos generativos probabilísticos [^485]. Em contraste com os *probability product kernels* [^485], que requerem a integração do produto de distribuições de probabilidade, os Fisher kernels utilizam o gradiente do log-likelihood para capturar a semelhança entre os dados. Esta abordagem se mostra particularmente útil quando se busca incorporar o conhecimento do domínio através da especificação de um modelo generativo.

### Conceitos Fundamentais

**Definição do Fisher Kernel**

Um Fisher kernel é definido como [^485]:
$$ k(x, x') = g(x)^T F^{-1} g(x') $$
onde:
*   $x$ e $x'$ são os objetos a serem comparados.
*   $g(x)$ é o **score vector** ou gradiente do log-likelihood avaliado no estimador de máxima verossimilhança (MLE) $\hat{\theta}$, definido como [^485]:
    $$     g(x) = \nabla_{\theta} \log p(x|\theta)|_{\hat{\theta}}     $$
*   $F$ é a **Fisher information matrix**, que essencialmente é a Hessiana do negativo do log-likelihood, definida como [^485]:
    $$     F = \nabla \nabla \log p(x|\theta)|_{\hat{\theta}}     $$

**Interpretação e Intuição**

A intuição por trás do Fisher kernel reside na ideia de que $g(x)$ representa a direção no espaço de parâmetros em que $x$ gostaria que os parâmetros se movessem, a partir de $\hat{\theta}$, para maximizar sua própria probabilidade [^485]. Assim, dois vetores $x$ e $x'$ são considerados similares se seus gradientes direcionais são similares em relação à geometria codificada pela curvatura da função de likelihood [^486].

**Cálculo da Fisher Information Matrix**

A Fisher information matrix $F$ desempenha um papel crucial na definição do kernel. Ela quantifica a quantidade de informação que uma variável aleatória $X$ carrega sobre o parâmetro desconhecido $\theta$ do qual a probabilidade de $X$ depende. No contexto de Fisher kernels, $F$ atua como uma métrica no espaço de parâmetros, ponderando a importância das diferentes direções no gradiente.

**Vantagens e Desvantagens**

*   **Vantagens:**
    *   Eficiência computacional em relação aos *probability product kernels* [^485].
    *   Incorporação de conhecimento de domínio através da escolha do modelo generativo.
    *   Aplica-se a uma variedade de modelos generativos [^485].

*   **Desvantagens:**
    *   Dependência da escolha do modelo generativo, que pode influenciar o desempenho do kernel.
    *   Requer o cálculo do gradiente e da Hessiana, que podem ser complexos para certos modelos.

### Conclusão

Os Fisher kernels representam uma ferramenta poderosa para definir medidas de similaridade entre objetos, aproveitando a estrutura de modelos generativos probabilísticos [^485]. Ao capturar a direção em que cada objeto "gostaria" de mover os parâmetros do modelo, eles oferecem uma abordagem intuitiva e eficiente para comparar dados em contextos onde o conhecimento do domínio pode ser expresso através de um modelo generativo. Embora exijam o cálculo do gradiente e da Hessiana, sua capacidade de incorporar informações específicas do problema os torna uma alternativa valiosa aos kernels tradicionais.

### Referências
[^485]: D. Barber, *Bayesian Reasoning and Machine Learning*, Cambridge University Press, 2012, Chapter 14, p. 485.
[^486]: D. Barber, *Bayesian Reasoning and Machine Learning*, Cambridge University Press, 2012, Chapter 14, p. 486.
<!-- END -->