## Hinge Loss em Support Vector Machines

### Introdução
Este capítulo detalha a utilização do **Hinge Loss** em Support Vector Machines (SVMs) [^498]. O Hinge Loss é uma função de custo utilizada em problemas de classificação, especialmente em SVMs, para penalizar classificações incorretas ou classificações corretas que se encontram dentro da margem [^499]. Diferentemente da *negative log likelihood* usada em regressão logística, o Hinge Loss promove uma fronteira de decisão que maximiza a margem entre as classes.

### Conceitos Fundamentais
O Hinge Loss, denotado como $L_{hinge}(y, \eta)$, é definido como [^499]:
$$ L_{hinge}(y, \eta) = \max(0, 1 - y\eta) = (1 - y\eta)_+ $$
onde:
- $y$ é o rótulo verdadeiro da classe, com $y \in \{-1, 1\}$ [^499].
- $\eta = f(x)$ é a "confiança" na escolha do rótulo $y = 1$, que não precisa ter semântica probabilística [^499].

A função Hinge Loss penaliza os pontos que são classificados incorretamente ou que estão corretamente classificados, mas dentro da margem [^499]. Isso incentiva o SVM a encontrar uma fronteira de decisão que maximize a margem entre as classes.

**Interpretação Geométrica**

O objetivo geral do SVM utilizando Hinge Loss tem a forma [^499]:
$$ \min_{w, w_0} \frac{1}{2} ||w||^2 + C \sum_{i=1}^{N} (1 - y_i f(x_i))_+ $$
onde:
- $w$ é o vetor de pesos [^499].
- $C$ é um parâmetro de regularização que controla o trade-off entre a maximização da margem e a minimização do erro de classificação [^499].

Introduzindo *slack variables* $\xi_i$, o problema pode ser reescrito como [^499]:
$$ \min_{w, w_0, \xi} \frac{1}{2} ||w||^2 + C \sum_{i=1}^{N} \xi_i \quad \text{s.t.} \quad \xi_i \geq 0, \quad y_i(w^T x_i + w_0) \geq 1 - \xi_i, \quad i = 1 : N $$
Nesta formulação, $\xi_i$ representa o grau em que a *i*-ésima amostra viola a restrição de margem [^499]. As amostras para as quais $\xi_i > 0$ são chamadas de **support vectors**[^499].

**Propriedades e Solução**

O problema de otimização com Hinge Loss é um *quadratic program* (QP) [^499]. Embora solvers padrão possam ser usados, algoritmos especializados como *Sequential Minimal Optimization (SMO)* são empregados para maior eficiência [^499]. A solução para este problema é esparsa, o que significa que apenas um subconjunto das amostras de treinamento (os *support vectors*) influencia a fronteira de decisão [^499].

A solução tem a forma [^499]:
$$ w = \sum_i \alpha_i x_i $$
onde $\alpha_i = C\lambda_i y_i$ e $\alpha$ é esparso devido ao Hinge Loss [^499].

**Predição**

No tempo de teste, a predição é feita usando [^499]:
$$ \hat{y}(x) = \text{sgn}(f(x)) = \text{sgn}(w_0 + w^T x) $$
Usando o *kernel trick*, a predição se torna [^499]:
$$ \hat{y}(x) = \text{sgn}\left(w_0 + \sum_i \alpha_i \kappa(x_i, x)\right) $$

### Conclusão
O Hinge Loss é uma componente crucial dos SVMs, permitindo a criação de modelos de classificação robustos com margens maximizadas [^499]. Sua natureza esparsa e a capacidade de incorporar o *kernel trick* o tornam uma ferramenta poderosa para uma variedade de problemas de classificação. A escolha do parâmetro de regularização $C$ é fundamental e geralmente realizada através de validação cruzada [^504]. A combinação do Hinge Loss com o *kernel trick*, a esparsidade e o princípio da margem maximizada são os principais ingredientes que tornam os classificadores SVM eficazes [^504].

### Referências
[^498]: Page 498, Chapter 14. Kernels
[^499]: Page 499, Chapter 14. Kernels
[^504]: Page 504, Chapter 14. Kernels
<!-- END -->