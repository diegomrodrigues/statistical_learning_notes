## Epsilon-Insensitive Loss Function in SVM Regression

### Introdução
Este capítulo explora a função de perda *epsilon-insensitive* ($L_\epsilon(y, \hat{y})$) utilizada em Support Vector Machines (SVMs) para regressão. Essa função desempenha um papel crucial na promoção da esparsidade do modelo, criando um tubo de largura $\epsilon$ em torno da predição, dentro do qual os erros não são penalizados [^497]. Pontos fora deste tubo são penalizados linearmente.

### Conceitos Fundamentais

A função de perda *epsilon-insensitive* é definida como [^497]:

$$
L_\epsilon(y, \hat{y}) =
\begin{cases}
0 & \text{se } |y - \hat{y}| < \epsilon \\
|y - \hat{y}| - \epsilon & \text{caso contrário}
\end{cases}
$$

Onde:
*   $y$ é o valor real.
*   $\hat{y}$ é o valor predito.
*   $\epsilon$ é um parâmetro que define a largura do tubo.

Esta função de perda difere da perda quadrática, onde todos os erros são penalizados quadraticamente, e da Huber loss [^497], onde os erros são penalizados quadraticamente até um certo ponto, e linearmente depois. A *epsilon-insensitive loss* ignora erros dentro do tubo, o que leva a soluções mais esparsas, pois apenas os pontos fora do tubo influenciam a solução.

A função objetivo correspondente em SVM para regressão é dada por [^497]:

$$
J = C \sum_{i=1}^{N} L_\epsilon(y_i, \hat{y}_i) + \frac{1}{2} ||w||^2
$$

Onde:
*   $C$ é um parâmetro de regularização que controla o trade-off entre a complexidade do modelo ($||w||^2$) e o erro de treinamento ($L_\epsilon$).
*   $N$ é o número de pontos de treinamento.
*   $w$ representa os pesos do modelo.

Para lidar com a não diferenciabilidade da função de perda, variáveis de *slack* ($\xi_i$) são introduzidas para representar o grau em que cada ponto está fora do tubo [^498]:

$$
\begin{aligned}
&y_i \leq f(x_i) + \epsilon + \xi_i^+ \\
&y_i \geq f(x_i) - \epsilon - \xi_i^-
\end{aligned}
$$

Com as restrições $\xi_i^+ \geq 0$ e $\xi_i^- \geq 0$. A função objetivo pode então ser reescrita como [^498]:

$$
J = C \sum_{i=1}^{N} (\xi_i^+ + \xi_i^-) + \frac{1}{2} ||w||^2
$$

Esta é uma função quadrática de $w$ que deve ser minimizada sujeita às restrições lineares. A solução ótima tem a forma [^498]:

$$
w = \sum \alpha_i x_i
$$

Onde $\alpha_i \geq 0$. Os $x_i$ para os quais $\alpha_i > 0$ são chamados de *support vectors* [^498]; são os pontos para os quais os erros estão fora ou na borda do tubo $\epsilon$. Uma vez que o modelo é treinado, as previsões podem ser feitas usando [^498]:

$$
\hat{y}(x) = w_0 + w^T x
$$

Substituindo $w$, obtemos uma solução kernelizada [^498]:

$$
\hat{y}(x) = w_0 + \sum \alpha_i \kappa(x_i, x)
$$

Onde $\kappa(x_i, x)$ é uma função *kernel* que calcula o produto interno no espaço de *features*.

### Conclusão

A função de perda *epsilon-insensitive* é uma ferramenta fundamental em SVM para regressão, permitindo a criação de modelos esparsos que ignoram pequenos erros e se concentram em capturar as tendências principais nos dados. A introdução de variáveis de *slack* transforma o problema em uma otimização quadrática com restrições lineares, que pode ser resolvida eficientemente. A solução resultante depende apenas de um subconjunto dos dados de treinamento, os *support vectors*, o que torna o modelo mais eficiente e robusto.

### Referências
[^497]: Páginas 497
[^498]: Páginas 498
<!-- END -->