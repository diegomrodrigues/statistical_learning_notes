## Kernelized Ridge Regression

### Introdução
Este capítulo aprofunda o conceito de **kernelized ridge regression**, uma técnica poderosa que combina a **regularização de Ridge** com a **flexibilidade dos kernels** [^1, ^14]. O objetivo é transformar o problema primal em um problema dual, permitindo o uso de kernels para modelar relações não lineares nos dados [^1]. A transformação para o espaço dual é crucial para aplicar o "kernel trick", evitando a necessidade de trabalhar explicitamente em espaços de alta dimensionalidade [^10, ^14].

### Conceitos Fundamentais

A **kernelized ridge regression** é uma extensão do método de **Ridge Regression**, onde a solução é expressa em termos de funções kernel aplicadas aos dados de treinamento. O problema primal da **Ridge Regression** é definido como [^14]:

$$J(w) = (y - Xw)^T(y - Xw) + \lambda ||w||^2$$

onde $X$ é a matriz de design $N \times D$, $y$ é o vetor de resposta, $w$ é o vetor de pesos e $\lambda$ é o parâmetro de regularização. A solução ótima para $w$ é dada por [^14]:

$$w = (X^TX + \lambda I_D)^{-1}X^Ty$$

Para kernelizar a **Ridge Regression**, transformamos o problema primal em um problema dual. As variáveis duais $\alpha$ são definidas como [^14]:

$$\alpha = (K + \lambda I_N)^{-1}y$$

onde $K$ é a matriz de Gram, com $K_{ij} = \kappa(x_i, x_j)$ e $\kappa$ é a função kernel. A solução para o vetor de pesos $w$ é expressa como uma combinação linear dos vetores de treinamento [^14]:

$$w = X^T\alpha = \sum_{i=1}^{N} \alpha_i x_i$$

A função de predição é então dada por [^14]:

$$f(x) = w^Tx = \sum_{i=1}^{N} \alpha_i \kappa(x, x_i)$$

Esta formulação permite realizar regressão em espaços de alta dimensionalidade implicitamente definidos pela função kernel $\kappa$ [^14]. Alguns exemplos de funções kernel comuns incluem os kernels RBF e polinomiais [^1, ^14].

**Kernel Trick:** A kernelização permite substituir todos os produtos internos da forma $(x, x')$ por uma chamada à função kernel $\kappa(x, x')$ [^14]. Isso evita a necessidade de calcular explicitamente as características em espaços de alta dimensão, tornando a abordagem computacionalmente viável [^14].

**Custo computacional:** O cálculo das variáveis duais $\alpha$ tem um custo de $O(N^3)$, enquanto o cálculo das variáveis primais $w$ tem um custo de $O(D^3)$ [^14]. Portanto, o método kernel pode ser vantajoso em configurações de alta dimensão, mesmo com um kernel linear. No entanto, a predição usando as variáveis duais leva um tempo $O(ND)$, enquanto a predição usando as variáveis primais leva apenas um tempo $O(D)$ [^14].

### Conclusão

A **kernelized ridge regression** oferece uma abordagem flexível e poderosa para modelagem não linear, combinando a regularização da **Ridge Regression** com a capacidade de mapear dados para espaços de características de alta dimensão usando kernels [^1, ^14]. A transformação para o problema dual e o uso do "kernel trick" são fundamentais para a eficiência computacional dessa técnica [^14]. Embora o cálculo das variáveis duais possa ser computacionalmente intensivo, a capacidade de modelar relações complexas nos dados torna a **kernelized ridge regression** uma ferramenta valiosa em diversas aplicações [^14].

### Referências
[^1]: Capítulo 14, Kernels
[^10]: Seção 14.4, The kernel trick
[^14]: Seção 14.4.3, Kernelized ridge regression
<!-- END -->