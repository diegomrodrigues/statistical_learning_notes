## Kernel PCA

### Introdução
Este capítulo se aprofunda no Kernel PCA (Principal Component Analysis), uma extensão não linear do PCA que utiliza o *kernel trick* para realizar *embeddings* não lineares [^494]. Como vimos anteriormente [^481], a escolha de um kernel adequado é crucial para o sucesso de métodos baseados em kernels, e o Kernel PCA oferece uma maneira poderosa de explorar relações não lineares nos dados. Este capítulo detalha a formulação matemática do Kernel PCA e sua aplicação.

### Conceitos Fundamentais

O Kernel PCA é uma técnica que permite realizar PCA em um espaço de características de alta dimensão (possivelmente infinito) sem calcular explicitamente as coordenadas neste espaço [^494]. Isso é feito através do *kernel trick*, que substitui todos os produtos internos por uma função kernel [^488].

**Formulação Matemática**

1.  **Matriz de Gram:** Dado um conjunto de dados $\{x_i\}_{i=1}^N$, a matriz de Gram $K$ é definida como [^481]:
    $$     K_{ij} = \kappa(x_i, x_j)     $$
    onde $\kappa(x_i, x_j)$ é uma função kernel que calcula a similaridade entre $x_i$ e $x_j$. Para que o *kernel trick* funcione, $\kappa$ deve ser um kernel de Mercer [^488].
2.  **Autovetores e Autovalores:** Seja $U$ a matriz ortogonal contendo os autovetores de $K$, e $\Lambda$ a matriz diagonal contendo os autovalores correspondentes [^481]:
    $$     K = U \Lambda U^T     $$
3.  **Autovetores no Espaço de Características:** Os autovetores no espaço de características são dados por [^494]:
    $$     V_{kpca} = \Phi U \Lambda^{-\frac{1}{2}}     $$
    onde $\Phi$ é a matriz de design (notional design matrix) implícita pelo kernel, com $\Phi_{ij} = \phi(x_i)_j$, e $\phi(x)$ é o mapeamento para o espaço de características. Como $\Phi$ pode ser de dimensão infinita, não calculamos $V_{kpca}$ explicitamente.
4.  **Projeção de um Vetor de Teste:** A projeção de um novo vetor de teste $x_*$ no espaço definido pelos autovetores do Kernel PCA é dada por [^494]:
    $$     \Phi_*^T V_{kpca} = k_*^T U \Lambda^{-\frac{1}{2}}     $$
    onde $k_*$ é um vetor contendo as similaridades entre $x_*$ e os pontos de treinamento, $k_*^T = [\kappa(x_*, x_1), \dots, \kappa(x_*, x_N)]$.

**Centrando os Dados**

Para garantir que os dados projetados tenham média zero, é necessário centrar a matriz de Gram [^494]. A matriz de Gram centrada $\tilde{K}$ é dada por:
$$ \tilde{K} = H K H $$
onde $H = I - \frac{1}{N} 11^T$ é a matriz de centralização, $I$ é a matriz identidade e $1$ é um vetor de uns [^494]. Os elementos de $\tilde{K}$ são dados por [^494]:
$$ \tilde{K}_{ij} = \kappa(x_i, x_j) - \frac{1}{N} \sum_{k=1}^N \kappa(x_i, x_k) - \frac{1}{N} \sum_{k=1}^N \kappa(x_k, x_j) + \frac{1}{N^2} \sum_{k=1}^N \sum_{l=1}^N \kappa(x_k, x_l) $$

**Algoritmo Kernel PCA**

O algoritmo para Kernel PCA pode ser resumido da seguinte forma [^495]:

1.  Calcular a matriz de Gram $K$ usando uma função kernel adequada.
2.  Centralizar a matriz de Gram $K$ para obter $\tilde{K}$.
3.  Calcular os autovetores $U$ e autovalores $\Lambda$ de $\tilde{K}$.
4.  Normalizar os autovetores $U$ com $\Lambda$
5.  Projetar novos pontos usando a fórmula $\Phi_*^T V_{kpca} = k_*^T U \Lambda^{-\frac{1}{2}}$

### Conclusão

O Kernel PCA é uma ferramenta poderosa para realizar análise de componentes principais em espaços de características não lineares [^494]. Ao utilizar o *kernel trick*, o Kernel PCA evita o cálculo explícito das coordenadas no espaço de características, tornando-o aplicável a problemas com dados complexos e não lineares. A escolha do kernel e a centralização dos dados são passos cruciais para o sucesso do Kernel PCA.

### Referências

[^481]: Chapter 14. Kernels, Section 14.2 Kernel functions
[^488]: Chapter 14. Kernels, Section 14.4 The kernel trick
[^494]: Chapter 14. Kernels, Section 14.4.4 Kernel PCA
[^495]: Chapter 14. Kernels, Algorithm 14.2: Kernel PCA
<!-- END -->