## Máquinas de Vetores Esparsos: LIVMs e RVMs

### Introdução
Este capítulo explora o uso de **máquinas de vetores esparsos** no contexto de modelos lineares generalizados (GLMs) com kernels. Como mencionado anteriormente [^486], a escolha dos *centroids* $\mu_k$ é um desafio fundamental nas *kernel machines*. Expandindo sobre essa questão, focaremos em como **LIVMs** (Máquinas de Vetores com Regularização L1) e **RVMs** (Máquinas de Vetores de Relevância) utilizam *priors* que promovem a **esparsidade** para selecionar um subconjunto dos exemplos de treinamento como vetores de suporte [^488], reduzindo a complexidade computacional e prevenindo o *overfitting*.

### Conceitos Fundamentais

#### Esparsidade em Kernel Machines
A principal dificuldade em usar *kernel machines* é a escolha dos *centroids* $\mu_k$ [^487]. Uma abordagem ingênua de preencher uniformemente o espaço com protótipos torna-se inviável em dimensões mais altas devido à **maldição da dimensionalidade** [^487]. Em vez de otimizar numericamente ou usar inferência MCMC para encontrar esses parâmetros [^487], uma abordagem mais simples é tornar cada exemplo $x_i$ um protótipo [^488]:

$$\phi(x) = [\kappa(x, x_1), ..., \kappa(x, x_N)]$$

Isso leva a um número de parâmetros $D$ igual ao número de pontos de dados $N$ [^488]. No entanto, podemos empregar *priors* que promovem a esparsidade para selecionar eficientemente um subconjunto dos exemplos de treinamento [^488]. Isso é chamado de **sparse vector machine** [^488].

#### LIVMs: Regularização L1 para Esparsidade
A escolha mais natural é usar a **regularização $l_1$** [^488]. Regularização $l_1$ é uma técnica que força alguns dos pesos $w$ a serem exatamente zero, promovendo assim a esparsidade. No contexto de *kernel machines*, isso significa que apenas um subconjunto dos exemplos de treinamento contribui para a previsão final [^488]. Esta abordagem é chamada de **LIVM**, que significa "l1-regularized vector machine" [^488]. Por analogia, o uso de um regularizador $l_2$ é definido como um L2VM ou "l2-regularized vector machine"; isso, obviamente, não será esparso [^488].

#### RVMs: ARD/SBL para Esparsidade Aprimorada
Uma esparsidade ainda maior pode ser obtida usando **ARD/SBL** [^488]. ARD (Automatic Relevance Determination) e SBL (Sparse Bayesian Learning) são técnicas Bayesianas que colocam *priors* nos pesos $w$ que incentivam muitos deles a serem zero. Isso resulta em um método chamado **relevance vector machine** ou **RVM** [^488]. Embora os algoritmos ARD/SBL genéricos possam ser usados para ajustar este modelo, na prática, o método mais comum é o algoritmo *greedy* em (Tipping and Faul 2003) [^488].

#### Comparação com SVMs
Outra abordagem popular para criar uma *kernel machine* esparsa é usar uma **support vector machine** ou **SVM** [^488]. Em vez de usar um *prior* que promova a esparsidade, ele essencialmente modifica o termo de verossimilhança, o que não é natural de um ponto de vista Bayesiano [^488]. No entanto, o efeito é semelhante [^488].

#### Desempenho Empírico
Em um problema de classificação binária em 2D, L2VM, LIVM, RVM e uma SVM usando o mesmo kernel RBF foram comparados [^488]. Para L2VM e LIVM, $\lambda$ foi escolhido manualmente; para RVMs, os parâmetros são estimados usando Bayes empírico; e para a SVM, CV é usado para escolher $C = 1/\lambda$, pois o desempenho da SVM é muito sensível a este parâmetro [^488]. Todos os métodos fornecem desempenho semelhante [^488]. No entanto, RVM é o mais esparso (e, portanto, o mais rápido no tempo de teste), depois LIVM e, em seguida, SVM [^488]. RVM também é o mais rápido para treinar, já que CV para uma SVM é lento [^488]. Este resultado é bastante típico [^488].

Em um problema de regressão 1D, L2VM, LIVM, RVM e uma SVM usando um kernel RBF também foram comparados [^488]. Novamente, as previsões são bastante semelhantes, mas RVM é o mais esparso, depois L2VM e, em seguida, SVM [^488].

### Conclusão
LIVMs e RVMs oferecem abordagens eficazes para criar *kernel machines* esparsas, utilizando *priors* que promovem a esparsidade para selecionar exemplos de treinamento relevantes [^488]. Embora as SVMs também alcancem a esparsidade, elas o fazem modificando o termo de verossimilhança, o que é menos natural de um ponto de vista Bayesiano [^488]. A escolha entre LIVMs, RVMs e SVMs depende de requisitos específicos da aplicação, como a necessidade de inferência Bayesiana completa ou restrições computacionais [^488]. Em geral, RVMs tendem a produzir modelos mais esparsos e podem ser mais rápidos de treinar do que as SVMs, enquanto LIVMs oferecem uma alternativa simples e eficaz usando regularização $l_1$ [^488].

### Referências
[^486]: Christopher M. Bishop. *Pattern Recognition and Machine Learning*. Springer, 2006. Chapter 14.
[^487]: Christopher M. Bishop. *Pattern Recognition and Machine Learning*. Springer, 2006. Page 487.
[^488]: Christopher M. Bishop. *Pattern Recognition and Machine Learning*. Springer, 2006. Page 488.
<!-- END -->