## Kernel Machines como GLMs: Redes RBF e Escolha de Centróides

### Introdução
Este capítulo explora o uso de **kernel machines** dentro do framework de **Generalized Linear Models (GLMs)**, com foco particular em **redes RBF** e a importância da escolha dos **centróides** [^486]. Como vimos anteriormente, os kernels fornecem uma maneira de medir a similaridade entre objetos sem a necessidade de representá-los como vetores de características de tamanho fixo [^479]. Expandindo o conceito apresentado, as kernel machines aproveitam essa capacidade para construir modelos preditivos poderosos e flexíveis [^486].

### Conceitos Fundamentais

Uma **kernel machine** é definida como um GLM onde o vetor de características de entrada é transformado por uma função $\phi(x)$ que utiliza um kernel [^486]. Especificamente,
$$\
\phi(x) = [\kappa(x, \mu_1), ..., \kappa(x, \mu_K)]
$$\
onde $\mu_k \in \mathcal{X}$ são um conjunto de $K$ **centróides**, e $\kappa$ é uma função kernel [^486]. Se $\kappa$ é um kernel RBF (Radial Basis Function), a kernel machine resultante é chamada de **rede RBF** [^486].

**Kernel RBF:** O kernel RBF, também conhecido como **squared exponential kernel** ou **Gaussian kernel**, é definido como [^480]:
$$\
\kappa(x, x\') = \exp\left(-\frac{1}{2}(x - x\')^T \Sigma^{-1} (x - x\')\right)
$$\
onde $\Sigma$ é uma matriz de covariância. Se $\Sigma$ é diagonal, o kernel pode ser escrito como [^480]:
$$\
\kappa(x, x\') = \exp\left(-\frac{1}{2} \sum_{j=1}^{D} \frac{(x_j - x\'_j)^2}{\sigma_j^2}\right)
$$\
Aqui, $\sigma_j$ define a **characteristic length scale** da dimensão $j$. Se $\sigma_j = \infty$, a dimensão correspondente é ignorada, resultando no **ARD kernel** (Automatic Relevance Determination). Se $\Sigma$ é esférica, obtemos o **isotropic kernel**:
$$\
\kappa(x, x\') = \exp\left(-\frac{||x - x\'||^2}{2\sigma^2}\right)
$$\
Neste caso, $\sigma^2$ é conhecido como o **bandwidth** [^480].

**Importância da Escolha dos Centróides:** A escolha dos centróides $\mu_k$ é um aspecto crucial no desempenho das kernel machines [^486]. Uma seleção inadequada pode levar a um ajuste pobre dos dados e, consequentemente, a uma capacidade de generalização limitada [^486].

**Métodos para Escolha dos Centróides:** Existem diversas abordagens para determinar os centróides, incluindo [^486]:
*   **Otimização Numérica:** Os centróides podem ser tratados como parâmetros a serem otimizados numericamente, juntamente com outros parâmetros do modelo, como os pesos em um GLM [^486]. Métodos como gradient descent ou algoritmos de otimização mais sofisticados podem ser empregados [^486].
*   **Inferência MCMC:** Métodos de Monte Carlo via Cadeias de Markov (MCMC) podem ser usados para inferir a distribuição posterior dos centróides, permitindo a incorporação de incerteza na sua estimativa [^486].
*   **Data Clustering:** Algoritmos de agrupamento de dados, como k-means ou hierarchical clustering, podem ser utilizados para identificar grupos de dados e, em seguida, selecionar os centróides como os centros desses grupos [^486].

**Desafios na Escolha dos Centróides:** A escolha dos centróides enfrenta desafios, especialmente em espaços de alta dimensionalidade [^487]. Em espaços de baixa dimensão, é possível "cobrir" o espaço com protótipos uniformemente [^487]. No entanto, essa abordagem se torna impraticável devido à maldição da dimensionalidade [^487].

**Abordagens Alternativas:**
*   **LIVMs, RVMs e Máquinas de Vetores Esparsos:** Estas técnicas visam selecionar um subconjunto esparso de vetores de treinamento como centróides, reduzindo a complexidade do modelo e melhorando a generalização [^487, 488].
*   **Aproximação de Nyström:** Essa técnica usa um subconjunto aleatório dos dados para aproximar a matriz do kernel, reduzindo o custo computacional [^488].
*   **Uso de Todos os Exemplos como Protótipos:** Uma abordagem simplificada é usar cada exemplo de treinamento como um protótipo [^488]. No entanto, isso leva a um número de parâmetros igual ao número de pontos de dados, exigindo o uso de priors que promovam a esparsidade, como a regularização $l_1$, resultando em uma **sparse vector machine** [^488].

### Conclusão
As kernel machines, particularmente as redes RBF, oferecem uma abordagem flexível e poderosa para modelagem preditiva. A escolha cuidadosa dos centróides é essencial para o desempenho do modelo, com diversas técnicas disponíveis para abordar esse problema. A seleção da abordagem apropriada depende das características específicas do conjunto de dados e dos requisitos computacionais. Este capítulo estabeleceu as bases para entender como os kernels podem ser usados dentro de GLMs, e como os centróides podem ser selecionados. Os capítulos posteriores podem expandir esses conceitos, explorando métodos mais avançados para otimização e seleção de modelos.

### Referências
[^479]: "Another approach is to assume that we have some way of measuring the similarity between objects, that doesn\'t require preprocessing them into feature vector format."
[^480]: "The squared exponential kernel (SE kernel) or Gaussian kernel is defined by..."
[^486]: "We define a kernel machine to be a GLM where the input feature vector has the form φ(x) = [κ(x, μ₁), ..., κ(x, μκ)], where με ∈ X are a set of K centroids."
[^487]: "The main issue with kernel machines is: how do we choose the centroids μ? If the input is low-dimensional Euclidean space, we can uniformly tile the space occupied by the data with prototypes, as we did in Figure 14.2(c)."
[^488]: "A simpler approach is to make each example xį be a prototype, so we get ф(x) = [к(Х, Х₁), ..., к(X, XN)]"
<!-- END -->