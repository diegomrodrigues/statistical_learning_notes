## Support Vector Machines (SVMs) e o Truque do Kernel em Modelos Lineares Generalizados
### Introdução
Em continuidade com a discussão sobre o uso de kernels dentro de Modelos Lineares Generalizados (GLMs) [^486], este capítulo se aprofunda em **Support Vector Machines (SVMs)**, uma classe de algoritmos que combina o truque do kernel com uma função de perda modificada para garantir a esparsidade da solução [^texto_original]. SVMs, originalmente projetados para classificação binária, podem ser estendidos para regressão e classificação multiclasse [^497].

### Conceitos Fundamentais
#### Truque do Kernel e Esparsidade
Conforme mencionado anteriormente [^488], ao invés de definir um vetor de características explicitamente em termos de kernels, $\phi(x) = [\kappa(x, x_1), ..., \kappa(x, x_N)]$, podemos trabalhar com os vetores de características originais $x$ e modificar o algoritmo para substituir todos os produtos internos da forma $(x, x\')$ por uma chamada à função kernel, $\kappa(x, x\')$ [^488]. Isso é conhecido como o **truque do kernel** [^488].

SVMs empregam o truque do kernel e uma função de perda modificada para garantir a **esparsidade** da solução, ou seja, a solução depende apenas de um subconjunto dos dados de treinamento, chamados **vetores de suporte** [^488, 496].

#### SVMs para Regressão
Para regressão, SVMs utilizam a **função de perda $\epsilon$-insensitive**, definida como [^texto_original]:
$$ L_\epsilon(y, \eta) =\ \begin{cases}\ 0 & \text{se } |y - \eta| < \epsilon \\\\\ |y - \eta| - \epsilon & \text{caso contrário}\ \end{cases}\ $$
onde $y$ é o valor real e $\eta$ é a previsão. A ideia é que erros menores que $\epsilon$ não são penalizados, criando um "tubo" em torno da previsão [^497]. A função objetivo correspondente é geralmente escrita na forma [^497]:
$$ J = C \sum_{i=1}^N L_\epsilon(y_i, \hat{y}_i) + \frac{1}{2} ||w||^2\ $$
onde $C$ é um parâmetro de regularização e $\hat{y}_i$ é a previsão para a $i$-ésima amostra. Para formular o problema como uma otimização restrita, **variáveis de folga (slack variables)** $\xi_i$ e $\xi_i^*$ são introduzidas para representar o grau em que cada ponto está fora do tubo [^498]:
$$ \begin{aligned}\ y_i &\leq f(x_i) + \epsilon + \xi_i \\\\\ y_i &\geq f(x_i) - \epsilon - \xi_i^*\ \end{aligned}\ $$
O objetivo é minimizar [^498]:
$$ J = C \sum_{i=1}^N (\xi_i + \xi_i^*) + \frac{1}{2} ||w||^2\ $$
sujeito às restrições acima e $\xi_i, \xi_i^* \geq 0$.

#### SVMs para Classificação
Para classificação, SVMs utilizam a **hinge loss**, definida como [^texto_original]:
$$ L_{hinge}(y, \eta) = \max(0, 1 - y\eta) = (1 - y\eta)_+\ $$
onde $y \in \{-1, 1\}$ é o rótulo real e $\eta = f(x)$ é a "confiança" na escolha do rótulo $y = 1$ [^texto_original]. O objetivo geral é minimizar uma combinação da perda e um termo de regularização $||w||^2$ [^texto_original]:
$$ \min_{w, w_0} \frac{1}{2} ||w||^2 + C \sum_{i=1}^N (1 - y_i f(x_i))_+\ $$
onde $C$ é um parâmetro de regularização que controla a penalidade por erros de classificação [^499]. Similarmente ao caso da regressão, variáveis de folga $\xi_i$ são introduzidas [^499]:
$$ \min_{w, w_0, \xi} \frac{1}{2} ||w||^2 + C \sum_{i=1}^N \xi_i \quad \text{s.t.} \quad \xi_i \geq 0, \quad y_i(w^T x_i + w_0) \geq 1 - \xi_i\ $$
A **hinge loss** penaliza classificações incorretas e classificações corretas com baixa confiança [^499]. A esparsidade da solução SVM surge porque apenas as amostras que violam a margem (ou estão no lado errado da fronteira de decisão) contribuem para a solução final [^499].

#### O Problema Dual
Tanto para regressão quanto para classificação, o problema de otimização pode ser reformulado em sua forma **dual**, que envolve a maximização de uma função em termos de **multiplicadores de Lagrange** [^499]. A solução tem a forma [^499]:
$$ w = \sum_i \alpha_i x_i\ $$
onde $\alpha_i$ são os multiplicadores de Lagrange, e apenas os $x_i$ correspondentes a $\alpha_i > 0$ são os **vetores de suporte** [^499]. A previsão é dada por [^499]:
$$ \hat{y}(x) = w_0 + \sum_i \alpha_i \kappa(x_i, x)\ $$
O truque do kernel permite que o SVM opere em um espaço de características de alta dimensão (possivelmente infinito) sem calcular explicitamente as coordenadas dos dados nesse espaço [^499].

#### Maximizando a Margem
Uma interpretação alternativa da formulação do SVM é a de **maximizar a margem** [^501]. A margem é a distância entre a fronteira de decisão e os pontos de dados mais próximos de cada classe [^501]. Maximizar a margem tende a levar a uma melhor generalização [^501].

#### Classificação Multi-classe
Estender SVMs para classificação multiclasse não é trivial [^503]. Duas abordagens comuns são [^503]:
1.  **One-versus-the-rest (OVR)**: Treinar um classificador binário para cada classe, tratando os dados da classe como positivos e todos os outros dados como negativos [^503].
2.  **One-versus-one (OVO)**: Treinar um classificador binário para cada par de classes [^503].

### Conclusão
Support Vector Machines (SVMs) representam uma poderosa classe de algoritmos que combinam o truque do kernel com funções de perda específicas para alcançar esparsidade e maximizar a margem [^496]. Embora originalmente formulados para classificação binária, eles podem ser adaptados para regressão e classificação multiclasse. Apesar de sua natureza não probabilística, SVMs permanecem populares devido à sua eficácia e capacidade de lidar com dados de alta dimensão [^497].

### Referências
[^texto_original]: Trecho do prompt original.
[^486]: Seção 14.3 do texto fornecido.
[^488]: Seção 14.4 do texto fornecido.
[^496]: Seção 14.5 do texto fornecido.
[^497]: Seção 14.5.1 do texto fornecido.
[^498]: Seção 14.5.1 do texto fornecido.
[^499]: Seção 14.5.2 do texto fornecido.
[^501]: Seção 14.5.2.2 do texto fornecido.
[^503]: Seção 14.5.2.4 do texto fornecido.

<!-- END -->