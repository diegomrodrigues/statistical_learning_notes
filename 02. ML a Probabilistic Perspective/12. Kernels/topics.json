{
  "topics": [
    {
      "topic": "Kernels",
      "sub_topics": [
        "Kernel functions measure the similarity between objects without explicitly preprocessing them into feature vector format, enabling algorithms to work directly with the objects' intrinsic properties or relationships. A kernel function, denoted as \\u043a(x, x\"), quantifies the similarity between two inputs x and x' in an abstract space X, typically producing a real-valued output that serves as a measure of their relatedness. Kernel methods allow algorithms to operate on data by only using kernel function computations, which is particularly useful when direct access to the internal representations of objects is limited or undesirable. A kernel function \\u043a(x, x\") is a real-valued function that quantifies the similarity between two arguments x and x' in the input space X, mapping them to a value in R. Frequently, it is assumed that \\u043a is symmetric (\\u03ba(x, x\") = \\u03ba(x', x)) and non-negative (\\u03ba(x, x\")) > 0), allowing interpretation as a measure of similarity.",
        "Common kernels include the linear kernel, defined as \\u043a(x, x\") = x\\u1d40x', which is suitable when the original data is already high-dimensional and the features are individually informative, making it unnecessary to transform the data into a different feature space. RBF (Radial Basis Function) kernels, such as the squared exponential kernel (SE kernel) or Gaussian kernel, define similarity based on the distance between data points, using parameters like the characteristic length scale (\\u03c3j) to determine the influence of each dimension and the bandwidth (\\u03c3\\u00b2) to control the kernel's sensitivity to distance. The squared exponential kernel, also known as the Gaussian kernel, is defined mathematically as \\u043a(x, x\") = exp(-1/2 (x - x')\\u1d40 \\u03a3\\u207b\\u00b9 (x - x')), where \\u03a3 is a covariance matrix. If \\u03a3 is diagonal, the kernel can be written as \\u043a(x, x\") = exp(-1/2 \\u03a3(xj - x'j)\\u00b2 / \\u03c3j\\u00b2), where \\u03c3j represents the characteristic length scale of dimension j. The isotropic kernel is a special case where \\u03a3 is spherical, resulting in \\u043a(x, x\") = exp(-||x - x'||\\u00b2 / (2\\u03c3\\u00b2)), with \\u03c3\\u00b2 as the bandwidth. Matern kernels are defined as \\u03ba(r) = (2^(1-\\u03bd) / \\u0393(\\u03bd)) * (\\u221a(2\\u03bdr) / l)^\\u03bd * K\\u1d65(\\u221a(2\\u03bdr) / l), where r = ||x - x'||, \\u03bd > 0, l > 0, and K\\u1d65 is a modified Bessel function. As \\u03bd approaches infinity, the Matern kernel approaches the SE kernel. When \\u03bd = 1/2, the kernel simplifies to \\u03ba(r) = exp(-r/l).",
        "Cosine similarity, defined as \\u043a(xi, xi\") = (xi\\u1d40xi') / (||xi||\\u2082 ||xi'||\\u2082), is used for comparing documents represented as bag-of-words vectors, measuring the cosine of the angle between the vectors to determine their similarity based on shared word occurrences. The cosine similarity kernel measures the cosine of the angle between two document vectors xi and xi', defined as \\u043a(xi, xi\") = (xi\\u1d40 xi') / (||xi||\\u2082 ||xi'||\\u2082), and is used in document classification and retrieval. Term Frequency-Inverse Document Frequency (TF-IDF) representation improves document comparison by weighting words based on their frequency in a document (term frequency) and rarity across the entire corpus (inverse document frequency), mitigating the impact of common words and boosting the importance of discriminative terms.",
        "A Mercer kernel, also known as a positive definite kernel, ensures that the Gram matrix (a matrix of kernel evaluations for all pairs of inputs) is positive definite, which is crucial for certain algorithms to guarantee convergence and valid interpretations of the kernel as an inner product in some feature space. Mercer kernels, also known as positive definite kernels, satisfy the condition that the Gram matrix is positive definite for any set of inputs, enabling the computation of inner products in a high-dimensional feature space implicitly defined by the kernel, with examples including Gaussian and cosine similarity kernels. Mercer's theorem states that if the Gram matrix is positive definite, it can be decomposed as K = U\\u039bUT, where \\u039b is a diagonal matrix of positive eigenvalues \\u03bb\\u1d62 > 0. This implies that there exists a function \\u03c6 mapping x \\u2208 X to RD such that \\u043a(x, x\") = \\u03c6(x)T\\u03c6(x').",
        "The kernel trick involves reformulating algorithms to use kernel functions instead of explicit feature vectors, enabling the algorithm to operate in high-dimensional spaces implicitly defined by the kernel, without explicitly computing the feature mapping. The kernel trick involves modifying algorithms to replace inner products with kernel function calls, enabling the use of kernel methods without explicitly defining feature vectors, applicable to algorithms that can be expressed solely in terms of inner products, such as kernelized nearest neighbor classification and kernelized K-medoids clustering.",
        "String kernels define similarity based on the substrings that two strings have in common, offering a way to compare variable-length strings without requiring them to be represented as fixed-size feature vectors. String kernels compare variable-length strings by considering the number of substrings they have in common, defined mathematically as \\u03ba(x, x\") = \\u03a3 w\\u209b \\u03c6\\u209b(x) \\u03c6\\u209b(x'), where \\u03c6\\u209b(x) denotes the number of times substring s appears in string x, and w\\u209b is a weight. This is a Mercer kernel and can be computed efficiently using suffix trees.",
        "Probability product kernels define similarity based on the product of probabilities, where a kernel is defined as \\u043a(xi, xj) = \\u222b p(x|xi)^\\u03c1 p(x|xj)^\\u03c1 dx, and p is a parameter, and p(x|xi) is often approximated by p(x|\\u03b8(xi)), where \\u03b8(xi) is a parameter estimate computed using a single data vector. Probabilistic generative models can be used to derive kernel functions, such as probability product kernels and Fisher kernels, by defining similarity based on the likelihood of data points under a generative model, offering a way to incorporate domain knowledge and handle variable-length sequences.",
        "Fisher kernels define similarity based on the gradient of the log likelihood, where a kernel is defined as \\u043a(x, x\") = g(x)\\u1d40F\\u207b\\u00b9g(x'), where g is the gradient of the log likelihood, or score vector, evaluated at the MLE \\u03b8, and F is the Fisher information matrix, which is essentially the Hessian.",
        "Kernel machines, including RBF networks, are GLMs that use a kernelized feature vector of the form \\u03c6(x) = [\\u03ba(x, \\u03bc\\u2081), ..., \\u03ba(x, \\u03bc\\u03ba)], where \\u03bck \\u2208 X are a set of K centroids, with the kernel function mapping the input to a higher dimensional space."
      ]
    },
    {
      "topic": "Support Vector Machines",
      "sub_topics": [
        "SVMs aim to minimize a regularized empirical risk function, using a loss function like hinge loss or epsilon-insensitive loss, to find a decision boundary that maximizes the margin between classes or minimizes prediction errors within a specified tube around the prediction.",
        "The epsilon-insensitive loss function in SVM regression defines an e-tube around the prediction, where points inside the tube are not penalized, allowing for a sparse solution vector w that depends only on a subset of the training data, known as support vectors.",
        "The hinge loss in SVM classification aims to maximize the margin between classes by penalizing points that fall within the margin or are misclassified, resulting in a sparse solution vector and a decision boundary that is robust to noise and outliers.",
        "The large margin principle in SVM seeks to find a decision boundary that maximizes the distance to the closest points from each class, providing better generalization and robustness to overfitting.",
        "SVMs for multi-class classification can be implemented using one-versus-rest or one-versus-one approaches, which involve training multiple binary classifiers to discriminate between classes, but these approaches can suffer from ambiguities and class imbalance problems."
      ]
    },
    {
      "topic": "Kernels for Building Generative Models",
      "sub_topics": [
        "Smoothing kernels are used to create non-parametric density estimates, such as kernel density estimation (KDE), by defining a function that satisfies properties like integrating to one, having zero mean, and positive variance, with examples including Gaussian, Epanechnikov, and tri-cube kernels. Smoothing kernels are functions of one argument that satisfy the properties \\u222b \\u043a(x) dx = 1, \\u222b x \\u043a(x) dx = 0, and \\u222b x\\u00b2 \\u043a(x) dx > 0.",
        "Kernel density estimation (KDE) is a non-parametric density model that estimates the probability density function of a random variable by averaging the contributions of kernel functions centered at each data point, with the bandwidth parameter controlling the smoothness of the resulting density estimate. Kernel density estimation (KDE) is a non-parametric density estimation technique that approximates the probability density function of a random variable by summing smoothing kernels centered at each data point, allowing for flexible modeling of complex distributions without assuming a specific parametric form.",
        "Kernel regression, also known as kernel smoothing or the Nadaraya-Watson model, estimates the conditional expectation of a response variable given a predictor variable by weighting the outputs at the training points based on their similarity to the predictor variable, using smoothing kernels to define the weights. Kernel regression, also known as kernel smoothing or the Nadaraya-Watson model, is a non-parametric regression technique that estimates the conditional expectation of a target variable given an input variable by weighting the target values of nearby data points using a smoothing kernel, allowing for flexible modeling of non-linear relationships without assuming a specific parametric form.",
        "Locally weighted regression improves on kernel regression by fitting a linear regression model locally at each point, using a weighted least squares approach with kernel functions to weight the data points based on their proximity to the point of interest, resulting in a more flexible and adaptive model.",
        "From KDE, we can derive the nearest neighbors classifier. Instead of fixing the bandwidth h, we can 'grow' a volume around x until we find K data points. Then, we can estimate the conditional class density as p(x|y = c, D) = Nc(x) / (N V(x)), where Nc(x) is the number of examples of class c in this volume, and V(x) is the size of the resulting volume.",
        "In locally weighted regression, we approximate a constant function locally. We can improve this by fitting a linear regression model to each point x* by solving min \\u03a3 \\u03ba(x*, xi) [yi - \\u03b2(x*)^T \\u03c6(xi)]\\u00b2, where \\u03c6(x) = [1, x]. This is called locally weighted regression. The equivalent kernel combines the local smoothing kernel with the effect of linear regression."
      ]
    },
    {
      "topic": "Kernel Machines",
      "sub_topics": [
        "Kernel machines are a class of algorithms that use kernel functions to map data into a high-dimensional space where linear models can be applied, effectively creating non-linear models in the original data space, which is achieved by defining the feature vector as \\u03c6(x) = [\\u043a(x, \\u03bc\\u2081), ..., \\u043a(x, \\u03bcK)], where \\u03bck are centroids.",
        "RBF networks are kernel machines that use RBF kernels as their kernel function, where the choice of centroids \\u03bck and bandwidth significantly impacts the model's performance, with uniformly spaced prototypes being a common approach in low-dimensional Euclidean spaces.",
        "Sparse vector machines, including L1-regularized vector machines (LIVM) and relevance vector machines (RVM), promote sparsity by using sparsity-promoting priors, such as L1 regularization or ARD/SBL, to efficiently select a subset of the training exemplars, reducing the number of parameters and improving generalization."
      ]
    },
    {
      "topic": "Support Vector Machines (SVMs)",
      "sub_topics": [
        "SVMs are based on the kernel trick, sparsity, and the large margin principle, where the kernel trick maps data into a high-dimensional feature space, sparsity reduces overfitting by selecting a subset of support vectors, and the large margin principle maximizes the distance between the decision boundary and the closest data points.",
        "The epsilon-insensitive loss function, L\\u03b5(y, \\u0177), is used in SVM regression to create an \\u03b5-tube around the prediction, where points within the tube are not penalized, promoting sparsity by ignoring small errors; points outside the tube are penalized linearly.",
        "Hinge loss, Lhinge(y, \\u03b7) = max(0, 1 \\u2013 yn), is used in SVM classification to penalize points that are incorrectly classified or are correctly classified but lie within the margin, promoting a decision boundary that maximizes the margin between classes.",
        "The large margin principle aims to maximize the distance between the decision boundary and the closest data points, improving generalization by reducing the model's sensitivity to small perturbations in the training data; this is achieved by formulating the optimization problem to maximize the margin subject to the constraint that all points are on the correct side of the boundary with a margin of at least 1.",
        "One-versus-the-rest and one-versus-one are two common approaches for extending SVMs to multi-class classification, where one-versus-the-rest trains C binary classifiers (one for each class), and one-versus-one trains C(C-1)/2 classifiers (one for each pair of classes), with each approach having its own advantages and disadvantages in terms of training time, test time, and ambiguity in classification."
      ]
    },
    {
      "topic": "Using Kernels Inside GLMs",
      "sub_topics": [
        "Kernel machines are GLMs where the input feature vector is defined as \\u03c6(x) = [\\u03ba(x, \\u03bc1), ..., \\u03ba(x, \\u03bcK)], where \\u03bck \\u2208 X are a set of K centroids. If \\u03ba is an RBF kernel, this is called an RBF network. The choice of the centroids \\u03bck is crucial and can be made by numerical optimization, MCMC inference, or data clustering.",
        "Sparse vector machines, such as LIVMs (l1-regularized vector machines) and RVMs (relevance vector machines), utilize priors that promote sparsity to select a subset of the training examples as support vectors, reducing computational complexity and preventing overfitting.",
        "The kernel trick involves replacing all inner products of the form (x, x') with a call to the kernel function \\u03ba(x, x'), allowing linear algorithms to operate in high-dimensional feature spaces without explicitly calculating the coordinates of these spaces. This trick is applicable to a variety of algorithms, including nearest neighbor classification and K-medoids clustering.",
        "In kernelized ridge regression, the primal problem is transformed into a dual problem, where the dual variables \\u03b1 are defined as \\u03b1 = (K + \\u03bbIN)^(-1) y. The solution for the weight vector w is expressed as a linear combination of the training vectors w = \\u03a3 \\u03b1i xi. The prediction function is then given by f(x) = \\u03a3 \\u03b1i \\u03ba(x, xi).",
        "Kernel PCA (Principal Component Analysis) extends PCA to perform a non-linear embedding using the kernel trick. The eigenvectors are given by Vkpca = \\u03a6UA^(-\\u00bd), where U and \\u039b contain the eigenvectors and eigenvalues of the Gram matrix K. The projection of a test vector x* is given by \\u03a6*^T Vkpca = k*^T UA^(-\\u00bd).",
        "Support Vector Machines (SVMs) combine the kernel trick with a modified loss function to ensure sparsity of the solution. SVMs for regression utilize the \\u03b5-insensitive loss function, defined as L\\u03b5(y, \\u0177) = 0 if |y - \\u0177| < \\u03b5, and |y - \\u0177| - \\u03b5 otherwise. SVMs for classification utilize the hinge loss, defined as Lhinge(y, \\u03b7) = max(0, 1 - yn), where \\u03b7 = f(x) is the 'confidence' in choosing the label y = 1. The overall goal is to minimize a combination of the loss and a regularization term ||w||\\u00b2.",
        "The large margin principle in SVMs seeks to maximize the perpendicular distance from the nearest point to the decision boundary. The margin is defined as r = f(x) / ||w||, and the goal is to find a discriminant function f(x) that is linear in the feature space induced by the kernel. The introduction of slack variables allows the SVM to handle non-linearly separable data, penalizing points that are within the margin or on the wrong side of the boundary."
      ]
    },
    {
      "topic": "Kernels Derived from Probabilistic Generative Models",
      "sub_topics": [
        "Probability product kernels are defined as \\u043a(x\\u1d62, x\\u2c7c) = \\u222b p(x|x\\u1d62)\\u1d56 p(x|x\\u2c7c)\\u1d56 dx, where \\u03c1 > 0 and p(x|x\\u1d62) is frequently approximated by p(x|\\u03b8(x\\u1d62)), with \\u03b8(x\\u1d62) being a parameter estimate computed using a single data vector. This approach quantifies the similarity between two objects based on the probability of a model fitted to a single data point.",
        "Fisher kernels are defined as \\u03ba(x, x') = g(x)\\u1d40F\\u207b\\u00b9g(x'), where g is the gradient of the log-likelihood, or score vector, evaluated at the maximum likelihood estimator (MLE) \\u03b8\\u0302, g(x) = \\u2207\\u03b8 log p(x|\\u03b8)|\\u03b8\\u0302, and F is the Fisher information matrix, which is essentially the Hessian: F = \\u2207\\u2207 log p(x|\\u03b8)|\\u03b8\\u0302. The intuition behind the Fisher kernel is that g(x) represents the direction (in parameter space) in which x would like the parameters to move (from \\u03b8\\u0302) to maximize its own likelihood."
      ]
    },
    {
      "topic": "The Kernel Trick",
      "sub_topics": [
        "Instead of defining our feature vector in terms of kernels, \\u03c6(x) = [\\u03ba(x, x\\u2081), ..., \\u03ba(x, x_N)], we can, instead, work with the original feature vectors x, but modify the algorithm so that it replaces all inner products of the form (x, x') with a call to the kernel function, \\u03ba(x, x'). This is called the kernel trick. It turns out that many algorithms can be kernelized in this way. Note that we require the kernel to be a Mercer kernel for this trick to work.",
        "Recall that in a INN classifier, we only need to compute the Euclidean distance of a test vector to all the training points, find the nearest, and look up its label. This can be kernelized by noting that ||x\\u1d62 - x\\u1d62'||\\u00b2 = (x\\u1d62, x\\u1d62) + (x\\u1d62', x\\u1d62') - 2(x\\u1d62, x\\u1d62'). This allows us to apply the nearest neighbor classifier to structured data objects.",
        "K-means clustering uses Euclidean distance to measure dissimilarity, which is not always appropriate for structured objects. We now describe how to develop a kernelized K-medoids algorithm. The first step is to replace the K-means algorithm with the K-medoids algorithm. This is similar to K-means, but instead of representing the centroid of each cluster by the mean of all the data vectors assigned to this cluster, we make each centroid be one of the data vectors themselves. Thus we always deal with integer indices, rather than data objects. We assign objects to their nearest centroids as before. When we update the centroids, we look at each object that belongs to the cluster, and measure the sum of its distances to all others in the same cluster; we then pick the one that has the smallest sum."
      ]
    },
    {
      "topic": "Support Vector Machines (SVMs)",
      "sub_topics": [
        "The \\u03b5-insensitive loss function is defined as L\\u03b5(y, \\u0177) = 0 if |y - \\u0177| < \\u03b5, and |y - \\u0177| - \\u03b5 otherwise. This means that any point lying inside an \\u03b5-tube around the prediction is not penalized. The corresponding objective function is usually written in the form J = C \\u03a3 L\\u03b5(y\\u1d62, \\u0177\\u1d62) + (1/2)||w||\\u00b2, where \\u0177\\u1d62 = w\\u1d40x\\u1d62 + w\\u2080 and C = 1/\\u03bb is a regularization constant.",
        "SVMs for classification. We replace the NLL loss with the hinge loss, defined as Lhinge(y, \\u03b7) = max(0, 1 - y\\u03b7) = (1 - y\\u03b7)\\u208a, where \\u03b7 = f(x) is our 'confidence' in choosing label y = 1; however, it need not have any probabilistic semantics.",
        "We derive the Equation 14.58 from a completely different perspective. Recall that our goal is to derive a discriminant function f(x) that will be linear in the feature space implicit by the choice of kernel. Consider a point x in this induced space. Referring to Figure 14.12(a), we see that x = x\\u22a5 + r (w / ||w||), where r is the distance of x from the decision boundary whose normal vector is w, and x\\u22a5 is the orthogonal projection of x onto this boundary. Hence f(x) = w\\u1d40x + w\\u2080 = (w\\u1d40x\\u22a5 + w\\u2080) + r (w\\u1d40w / ||w||) = f(x) + r ||w||, and r = f(x) / ||w||.",
        "We would like to make this distance r = f(x)/||w|| as large as possible, for the reasons illustrated in Figure 14.11. In particular, there may be many lines that perfectly separate the training data (especially if we work in a high dimensional feature space), but intuitively, the best one to choose is the one that maximizes the margin, that is, the perpendicular distance to the nearest point. Furthermore, we want to ensure that each point is on the correct side of the boundary, so we want f(x\\u1d62)y\\u1d62 > 0. Hence our objective becomes max min y\\u1d62(w\\u1d40x\\u1d62 + w\\u2080) / ||w||.",
        "Probabilistic output. An SVM classifier produces a rigid labeling, \\u0177(x) = sign(f(x)). However, we often want a measure of confidence in our prediction. A heuristic approach is to interpret f(x) as the log-odds ratio, log p(y=0|x) / p(y=1|x). We can then convert the output of an SVM into a probability using p(y = 1|x, \\u03b8) = \\u03c3(af(x) + b), where a, b can be estimated by maximum likelihood on a separate validation set. (Using the training set to estimate a and b leads to severe over-fitting.) This technique was first proposed in (Platt 2000).",
        "SVMs for multi-class classification. We saw how we could 'upgrade' a binary logistic regression model to the multi-class case by replacing the sigmoid function with the softmax, and the Bernoulli distribution with the multinomial. Upgrading an SVM to the multi-class case is not so easy, since the outputs are not on a calibrated scale, and hence are difficult to compare with each other.",
        "The obvious approach is to use a one-versus-the-rest approach, in which we train C binary classifiers, f_c(x), where the data from class c is treated as positive and the data from all other classes is treated as negative. However, this can result in regions of input space that are ambiguously labeled, as shown in Figure 14.14(a)."
      ]
    }
  ]
}