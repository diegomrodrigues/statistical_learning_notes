## Sparse Vector Machines em Kernel Machines
### Introdução
Este capítulo aprofunda o conceito de **sparse vector machines** no contexto mais amplo de **kernel machines**. As sparse vector machines, incluindo L1-regularized vector machines (LIVM) e relevance vector machines (RVM), são técnicas que promovem a *sparsidade* ao utilizar priors que induzem a sparsidade, como a regularização L1 ou ARD/SBL, para selecionar eficientemente um subconjunto dos exemplos de treinamento [^1]. Isso resulta em uma redução no número de parâmetros e, consequentemente, em uma melhor generalização. Este capítulo explorará os princípios fundamentais, as formulações matemáticas e as implicações práticas dessas técnicas, fornecendo uma compreensão abrangente de como elas se encaixam no panorama das kernel machines.

### Conceitos Fundamentais
#### Sparsidade e Regularização
A ideia central por trás das sparse vector machines é a promoção da **sparsidade**. Em termos simples, um modelo esparso é aquele em que muitos dos seus parâmetros são zero. Isso tem várias vantagens, incluindo:
*   **Interpretabilidade:** Modelos esparsos são mais fáceis de interpretar porque apenas um subconjunto dos recursos contribui significativamente para a previsão.
*   **Eficiência computacional:** Modelos com menos parâmetros são mais rápidos para treinar e usar.
*   **Generalização:** Ao evitar o overfitting, modelos esparsos tendem a generalizar melhor para dados não vistos.

A sparsidade é alcançada através do uso de **regularização**. A regularização é uma técnica que adiciona uma penalidade ao termo de perda durante o treinamento, incentivando o modelo a aprender parâmetros menores (ou, no caso da regularização L1, parâmetros iguais a zero).

#### L1-Regularized Vector Machines (LIVM)
As LIVMs utilizam a **regularização L1** para induzir a sparsidade [^488]. A regularização L1 adiciona uma penalidade proporcional ao valor absoluto dos pesos aos objetivos, ou seja:
$$ J(w) = \text{Loss}(w) + \lambda ||w||_1 $$
onde $\text{Loss}(w)$ é o termo de perda padrão (por exemplo, erro quadrático médio para regressão, log-loss para classificação), $w$ é o vetor de pesos, $\lambda$ é o parâmetro de regularização que controla a força da penalidade, e $||w||_1$ é a norma L1 de $w$, definida como a soma dos valores absolutos dos seus elementos:
$$ ||w||_1 = \sum_{i=1}^D |w_i| $$
A regularização L1 tem a propriedade de forçar alguns dos pesos a serem exatamente zero, resultando em um modelo esparso.

#### Relevance Vector Machines (RVM)
As Relevance Vector Machines (RVMs) são um tipo de modelo kernel que utiliza um prior **Automatic Relevance Determination (ARD)** ou **Sparse Bayesian Learning (SBL)** para obter sparsidade [^488]. Em vez de adicionar uma penalidade diretamente aos pesos, o ARD/SBL coloca um prior sobre os pesos que incentiva muitos deles a serem zero.
Especificamente, cada peso $w_i$ é associado a uma variância $\alpha_i$, e o prior sobre os pesos é dado por:
$$ p(w|\alpha) = \prod_{i=1}^D \mathcal{N}(w_i | 0, \alpha_i^{-1}) $$
onde $\mathcal{N}(w_i | 0, \alpha_i^{-1})$ é uma distribuição Gaussiana com média zero e variância $\alpha_i^{-1}$. Se $\alpha_i$ é grande, então a variância de $w_i$ é pequena, o que significa que $w_i$ é forçado a ser próximo de zero. O ARD/SBL aprende os valores de $\alpha_i$ a partir dos dados, e muitos deles acabam sendo grandes, resultando em um modelo esparso.

#### Kernel Trick e Função Base
Tanto LIVMs quanto RVMs se beneficiam do **kernel trick**.  Em vez de definir o vetor de características diretamente, podemos usar uma função kernel $\kappa(x, x')$ para calcular o produto interno das características [^488]:
$$ \kappa(x, x') = \phi(x)^T \phi(x') $$
onde $\phi(x)$ é uma função que mapeia a entrada $x$ para um espaço de características de alta dimensão. O kernel trick nos permite trabalhar em espaços de características de alta dimensão sem realmente calcular as características explicitamente. Exemplos comuns de funções kernel incluem o kernel gaussiano (RBF) [^480]:
$$ \kappa(x, x') = \exp\left(-\frac{||x - x'||^2}{2\sigma^2}\right) $$
e o kernel polinomial [^481]:
$$ \kappa(x, x') = (x^T x' + r)^M $$
O kernel trick é particularmente útil quando as características originais não são linearmente separáveis, pois permite que o modelo aprenda funções de decisão não lineares.

#### Kernel Machines
As sparse vector machines são um tipo de **kernel machine**. Uma kernel machine é um modelo que utiliza uma função kernel para mapear as entradas para um espaço de características de alta dimensão e, em seguida, aprende um modelo linear nesse espaço [^486]. A forma geral do vetor de características em uma kernel machine é [^486]:
$$ \phi(x) = [\kappa(x, \mu_1), ..., \kappa(x, \mu_K)] $$
onde $\mu_k$ são um conjunto de centróides e $\kappa$ é uma função kernel. Se $\kappa$ é um kernel RBF, isso é chamado de rede RBF.

### Comparação entre LIVM, RVM e SVM
É instrutivo comparar LIVM, RVM e Support Vector Machines (SVMs) [^488]. Todas as três são técnicas de kernel que podem ser usadas para classificação e regressão, mas elas diferem na forma como alcançam a sparsidade. As LIVMs usam a regularização L1, as RVMs usam um prior ARD/SBL e as SVMs usam uma função de perda (hinge loss) que incentiva a sparsidade [^488].

Uma comparação dos métodos é resumida na Tabela 14.1 [^505]:

| Método | Opt. w     | Opt. kernel | Sparse | Prob. | Multiclass   | Non-Mercer | Section |
| ------ | ---------- | ----------- | ------ | ----- | ------------ | ---------- | ------- |
| L2VM   | Convex     | EB          | No     | Yes   | Yes          | Yes        | 14.3.2  |
| LIVM   | Convex     | CV          | Yes    | Yes   | Yes          | Yes        | 14.3.2  |
| RVM    | Not convex | EB          | Yes    | Yes   | Yes          | Yes        | 14.3.2  |
| SVM    | Convex     | CV          | Yes    | No    | Indirectly   | No         | 14.5    |

Em geral, as RVMs tendem a ser mais esparsas do que as LIVMs e SVMs, mas também são mais propensas a overfitting. As SVMs são geralmente mais robustas do que as RVMs, mas são também menos esparsas e não fornecem saídas probabilísticas [^497]. As LIVMs oferecem um bom compromisso entre sparsidade e robustez.

### Conclusão
As sparse vector machines, incluindo LIVMs e RVMs, são ferramentas poderosas para aprender modelos esparsos com kernel machines. Ao utilizar priors que induzem a sparsidade, elas são capazes de selecionar eficientemente um subconjunto dos exemplos de treinamento, resultando em modelos mais interpretáveis, eficientes e generalizáveis. A escolha entre LIVM, RVM e SVM depende dos requisitos específicos da aplicação, mas todas as três oferecem vantagens distintas sobre os métodos de kernel não esparsos.

### Referências
[^1]: Página 479: "Sparse vector machines, including L1-regularized vector machines (LIVM) and relevance vector machines (RVM), promote sparsity by using sparsity-promoting priors, such as L1 regularization or ARD/SBL, to efficiently select a subset of the training exemplars, reducing the number of parameters and improving generalization."
[^480]: Página 480: "The squared exponential kernel (SE kernel) or Gaussian kernel is defined by..."
[^481]: Página 481: "For example, consider the (non-stationary) polynomial kernel..."
[^486]: Página 486: "We define a kernel machine to be a GLM where the input feature vector has the form..."
[^488]: Página 488: "The most natural choice is to use l₁ regularization (Krishnapuram et al. 2005). (Note that in the multi-class case, it is necessary to use group lasso, since each exemplar is associated with C weights, one per class.) We call this LIVM, which stands for “l₁-regularized vector machine”. By analogy, we define the use of an l2 regularizer to be a L2VM or “l2-regularized vector machine"; this of course will not be sparse. We can get even greater sparsity by using ARD/SBL, resulting in a method called the relevance vector machine or RVM (Tipping 2001). One can fit this model using generic ARD/SBL algorithms, although in practice the most common method is the greedy algorithm in (Tipping and Faul 2003) (this is the algorithm implemented in Mike Tipping's code, which is bundled with PMTK)."
[^497]: Página 497: "Note that SVMs are very unnatural from a probabilistic point of view. First, they encode sparsity in the loss function rather than the prior. Second, they encode kernels by using an algorithmic trick, rather than being an explicit part of the model. Finally, SVMs do not result in probabilistic outputs, which causes various difficulties, especially in the multi-class classification setting (see Section 14.5.2.4 for details)."
[^505]: Página 505: "Table 14.1 Comparison of various kernel based classifiers. EB = empirical Bayes, CV = cross validation. See text for details."
<!-- END -->