## RBF Networks: Kernel Machines with Radial Basis Functions

### Introdução
As **RBF networks** representam uma classe de **kernel machines** que empregam **RBF kernels** como suas funções de kernel [^486]. A escolha dos **centróides** ($\mu_k$) e da **largura de banda** (bandwidth) $\sigma$ exerce um impacto significativo no desempenho do modelo [^486]. Em espaços Euclidianos de baixa dimensionalidade, a utilização de protótipos uniformemente espaçados é uma abordagem comum [^486]. Este capítulo explora em profundidade as RBF networks, detalhando seus componentes, a importância da escolha dos parâmetros e as diferentes estratégias para sua implementação.

### Conceitos Fundamentais

#### RBF Kernels
Um **RBF kernel** (Radial Basis Function kernel), também conhecido como **squared exponential kernel** ou **Gaussian kernel**, é definido por [^480]:

$$ \kappa(\mathbf{x}, \mathbf{x}\') = \exp\left(-\frac{1}{2}(\mathbf{x} - \mathbf{x}\')^T \Sigma^{-1} (\mathbf{x} - \mathbf{x}\')\right)\ $$

Onde $\Sigma$ é uma matriz que, se diagonal, permite que o kernel seja escrito como [^480]:

$$ \kappa(\mathbf{x}, \mathbf{x}\') = \exp\left(-\frac{1}{2}\sum_{j=1}^{D} \frac{(x_j - x\'_j)^2}{\sigma_j^2}\right)\ $$

Nesta formulação, $\sigma_j$ define a **characteristic length scale** da dimensão *j*. Se $\sigma_j = \infty$, a dimensão correspondente é ignorada. Esse kernel é conhecido como **ARD kernel** (Automatic Relevance Determination kernel). Se $\Sigma$ é esférica, obtém-se o **isotropic kernel** [^480]:

$$ \kappa(\mathbf{x}, \mathbf{x}\') = \exp\left(-\frac{||\mathbf{x} - \mathbf{x}\'||^2}{2\sigma^2}\right)\ $$

Aqui, $\sigma^2$ é conhecido como **bandwidth**. A Equação 14.3 [^480] representa um exemplo de uma **radial basis function** ou **RBF kernel**, pois depende apenas da distância $||\mathbf{x} - \mathbf{x}\'||$ [^480].

#### RBF Networks como GLMs
As RBF networks podem ser vistas como um caso especial de **Generalized Linear Models (GLMs)** [^486], onde o vetor de características de entrada é definido como [^486]:

$$phi(\mathbf{x}) = [\kappa(\mathbf{x}, \mu_1), ..., \kappa(\mathbf{x}, \mu_K)]$$

Nesta equação, $\mu_k \in \mathcal{X}$ representa um conjunto de *K* **centróides** [^486]. A escolha dos centróides $\mu_k$ é um aspecto crucial no design de uma RBF network [^486]. Se o kernel $\kappa$ for um RBF kernel, a rede é chamada de **RBF network** [^486]. A Equação 14.28 é referida como um **kernelized feature vector** [^486]. É importante notar que, nesta abordagem, o kernel não precisa ser um Mercer kernel [^486].

#### Escolha dos Centróides e Largura de Banda
A principal questão no uso de RBF networks é como escolher os centróides $\mu_k$ [^487]. Em espaços Euclidianos de baixa dimensionalidade, é possível preencher uniformemente o espaço ocupado pelos dados com protótipos [^487], como demonstrado na Figura 14.2(c) [^486]. No entanto, essa abordagem se torna inviável em dimensões mais altas devido à maldição da dimensionalidade [^487].

Outras abordagens incluem:
*   **Otimização numérica:** Tentar otimizar numericamente esses parâmetros (Haykin 1998) [^487].
*   **Inferência MCMC:** Usar inferência MCMC (Andrieu et al. 2001; Kohn et al. 2001) [^487].
*   **Clustering:** Encontrar clusters nos dados e atribuir um protótipo por cluster [^487].

A largura de banda $\sigma$ também é crucial. Pequenos valores de $\sigma$ levam a funções muito irregulares, enquanto grandes valores de $\sigma$ podem resultar em uma matriz de design constante, representando uma função linear [^487].

#### Sparse Vector Machines
Uma abordagem para lidar com a alta dimensionalidade é tornar cada exemplo $x_i$ um protótipo [^488]. Isso leva a $D = N$, onde *N* é o número de pontos de dados [^488]. Para lidar com o grande número de parâmetros, pode-se usar priors que promovem a esparsidade, como os discutidos no Capítulo 13 [^488]. Isso leva a uma **sparse vector machine** [^488].

Algumas técnicas para criar sparse kernel machines incluem:
*   **L1 regularization:** Usar regularização $l_1$ (Krishnapuram et al. 2005) [^488], resultando em uma **L1-regularized vector machine (LIVM)** [^488].
*   **L2 regularization:** Usar regularização $l_2$ [^488], resultando em uma **L2-regularized vector machine (L2VM)** [^488].
*   **ARD/SBL:** Usar ARD/SBL (Automatic Relevance Determination/Sparse Bayesian Learning) [^488], resultando em uma **relevance vector machine (RVM)** (Tipping 2001) [^488].
*   **Support Vector Machines (SVM):** Usar uma support vector machine (SVM) [^488], que modifica o termo de likelihood para promover a esparsidade.

### Conclusão
As RBF networks oferecem uma abordagem flexível para modelagem não-linear, utilizando RBF kernels e centróides para mapear dados em um espaço de características onde modelos lineares podem ser aplicados [^486]. A escolha dos centróides e da largura de banda é crucial para o desempenho do modelo [^486], e diversas técnicas podem ser empregadas para otimizar esses parâmetros e lidar com a maldição da dimensionalidade [^487]. As sparse vector machines, como LIVM, RVM e SVM, oferecem abordagens para criar modelos esparsos, selecionando um subconjunto dos dados de treinamento como vetores de suporte [^488].

### Referências
[^486]: "RBF networks are kernel machines that use RBF kernels as their kernel function, where the choice of centroids \u03bck and bandwidth significantly impacts the model\'s performance, with uniformly spaced prototypes being a common approach in low-dimensional Euclidean spaces."
[^480]: "The squared exponential kernel (SE kernel) or Gaussian kernel is defined by ..."
[^487]: "The main issue with kernel machines is: how do we choose the centroids μ? If the input is low-dimensional Euclidean space, we can uniformly tile the space occupied by the data with prototypes, as we did in Figure 14.2(c). However, this approach breaks down in higher numbers of dimensions because of the curse of dimensionality. If με ∈ RD, we can try to perform numerical optimization of these parameters (see e.g., (Haykin 1998)), or we can use MCMC inference, (see e.g., (Andrieu et al. 2001; Kohn et al. 2001)), but the resulting objective function / posterior is highly multimodal. Furthermore, these techniques is hard to extend to structured input spaces, where kernels are most useful."
[^488]: "A simpler approach is to make each example xį be a prototype, so we get ... Now we see D = N, so we have as many parameters as data points. However, we can use any of the sparsity-promoting priors for w discussed in Chapter 13 to efficiently select a subset of the training exemplars. We call this a sparse vector machine."
<!-- END -->