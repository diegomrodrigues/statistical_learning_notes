## SVMs: Regularized Empirical Risk Minimization

### Introdução
Este capítulo se aprofunda na formulação do Support Vector Machine (SVM) como um problema de minimização de risco empírico regularizado [^496]. A ideia central é encontrar uma fronteira de decisão que não apenas separe as classes, mas também maximize a margem entre elas ou minimize os erros de previsão dentro de uma faixa especificada [^496]. Exploraremos os aspectos teóricos e práticos dessa abordagem, com ênfase nas funções de perda utilizadas e na importância da regularização.

### Conceitos Fundamentais

A formulação do SVM como um problema de **minimização de risco empírico regularizado** [^496] envolve a minimização de uma função objetivo que consiste em duas partes: um termo de **perda empírica** e um termo de **regularização**.

#### Função de Perda

A função de perda quantifica o erro cometido pelo modelo ao classificar os dados de treinamento [^496]. No contexto dos SVMs, duas funções de perda são comumente utilizadas:

1.  **Hinge Loss:** A *hinge loss* é definida como $L_{hinge}(y, \eta) = max(0, 1 - y\eta) = (1 - y\eta)_+$ [^499], onde $y \in \\{-1, 1\\}$ é o rótulo verdadeiro e $\eta = f(x)$ é a "confiança" do modelo na escolha do rótulo $y=1$ [^499]. Essa função penaliza classificações incorretas e classificações corretas com baixa confiança [^499]. A *hinge loss* resulta em soluções esparsas, onde apenas um subconjunto dos dados de treinamento (os *support vectors*) influencia a fronteira de decisão [^499].
2.  **Epsilon-Insensitive Loss:** A *epsilon-insensitive loss*, proposta por Vapnik, é uma variante da função de perda de Huber e é definida como:
$$\
L_\epsilon(y, \hat{y}) =\
\begin{cases}\
0 & \text{se } |y - \hat{y}| < \epsilon \\\\\
|y - \hat{y}| - \epsilon & \text{otherwise}\
\end{cases}\
$$[^497]
    onde $\epsilon$ define um tubo em torno da predição dentro do qual os erros não são penalizados [^497]. Essa função é comumente usada em problemas de regressão com SVMs [^497].

#### Termo de Regularização

O termo de regularização penaliza a complexidade do modelo, evitando o *overfitting* [^496]. No contexto dos SVMs, a regularização $l_2$ é comumente utilizada, penalizando a norma Euclidiana do vetor de pesos $w$ [^496]. A função objetivo geral do SVM pode ser expressa como:
$$\
J(w, \lambda) = \sum_{i=1}^{N} L(y_i, \hat{y_i}) + \lambda ||w||^2\
$$[^496]
onde $\lambda$ é o parâmetro de regularização que controla o compromisso entre a minimização do erro empírico e a complexidade do modelo [^496].

#### Maximização da Margem

O SVM busca encontrar a fronteira de decisão que maximize a margem entre as classes [^501]. A margem é definida como a distância mínima entre a fronteira de decisão e os pontos de dados mais próximos de cada classe [^501]. A maximização da margem está intimamente relacionada à minimização do termo de regularização [^501]. Um modelo com grande margem tende a ter menor complexidade e, portanto, menor norma do vetor de pesos $w$ [^501].

#### Slack Variables

Em muitos casos, os dados não são linearmente separáveis [^501]. Para lidar com essa situação, o SVM introduz *slack variables* ($\xi_i$) que permitem que alguns pontos de dados violem a restrição de margem [^501]. A função objetivo modificada é dada por:
$$\
\min_{w, w_0, \xi} \frac{1}{2} ||w||^2 + C \sum_{i=1}^{N} \xi_i\
$$[^501]
sujeito a:
$$\
\xi_i \geq 0, \quad y_i(w^T x_i + w_0) \geq 1 - \xi_i\
$$[^501]
onde $C$ é um parâmetro de penalidade que controla o compromisso entre a maximização da margem e a minimização dos erros de classificação [^501].

#### Kernel Trick

O *kernel trick* permite que o SVM opere em espaços de características de alta dimensão sem calcular explicitamente as coordenadas dos dados nesses espaços [^488]. Em vez disso, o SVM utiliza uma função *kernel* $\kappa(x, x\')$ que calcula o produto interno entre as imagens dos dados em um espaço de características implícito [^488]. Funções *kernel* comuns incluem o *kernel* linear, o *kernel* polinomial e o *kernel* RBF [^488].

#### Dualidade

O problema de otimização do SVM pode ser resolvido tanto no espaço primal quanto no espaço dual [^492]. A formulação dual é particularmente útil quando se utiliza o *kernel trick*, pois permite que o SVM opere em espaços de características de alta dimensão sem calcular explicitamente as coordenadas dos dados nesses espaços [^492].

### Conclusão

O SVM como um problema de minimização de risco empírico regularizado oferece uma estrutura poderosa e flexível para classificação e regressão [^496]. A escolha da função de perda, do termo de regularização e da função *kernel* permite adaptar o SVM a diferentes tipos de dados e problemas [^496]. A capacidade de maximizar a margem e lidar com dados não linearmente separáveis torna o SVM uma ferramenta valiosa no aprendizado de máquina [^501].
### Referências
[^496]: Pág. 496 "In Section 14.3.2, we saw one way to derive a sparse kernel machine, namely by using a GLM with kernel basis functions, plus a sparsity-promoting prior such as l₁ or ARD. An alternative approach is to change the objective function from negative log likelihood to some other loss function, as we discussed in Section 6.5.5. In particular, consider the l2 regularized empirical risk function"
[^497]: Pág. 497 "Vapnik (Vapnik et al. 1997) proposed a variant of the Huber loss function (Section 7.4) called the epsilon insensitive loss function"
[^488]: Pág. 488 "Rather than defining our feature vector in terms of kernels, $(x) = [к(Х, Х₁), ..., κ(X, XN)]$, we can instead work with the original feature vectors x, but modify the algorithm so that it replaces all inner products of the form (x, x\') with a call to the kernel function, к(x, x\'). This is called the kernel trick."
[^499]: Pág. 499 "Here η = f(x) is our “confidence” in choosing label y = 1; however, it need not have any probabilistic semantics. See Figure 6.7 for a plot. We see that the function looks like a door hinge, hence its name. The overall objective has the form"
[^501]: Pág. 501 "The large margin principle"
[^492]: Pág. 492 "Applying the kernel trick to distance-based methods was straightforward. It is not so obvious how to apply it to parametric models such as ridge regression. However, it can be done, as we now explain. This will serve as a good “warm up" for studying SVMs."
<!-- END -->