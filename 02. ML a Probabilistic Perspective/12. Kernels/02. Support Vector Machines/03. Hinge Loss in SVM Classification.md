## Hinge Loss em Support Vector Machines

### Introdução
Este capítulo aprofunda o conceito de **hinge loss** no contexto de **Support Vector Machines (SVMs)**, explorando sua função na maximização da margem entre classes e na obtenção de soluções esparsas [^499]. O SVM, como mencionado anteriormente, busca uma função discriminante $f(x)$ linear no espaço de características induzido pelo kernel [^501]. A hinge loss, como veremos, desempenha um papel crucial na definição do objetivo de otimização do SVM, promovendo a robustez da fronteira de decisão contra ruído e outliers.

### Conceitos Fundamentais

A **hinge loss** é uma função de perda utilizada em algoritmos de classificação, especialmente em SVMs. Formalmente, a hinge loss é definida como [^499]:
$$L_{hinge}(y, \eta) = \max(0, 1 - y\eta) = (1 - y\eta)_+$$
onde:
- $y$ é o rótulo real da classe, assumindo valores em $\\{-1, 1\\}$ [^499]
- $\eta = f(x)$ é a "confiança" na escolha do rótulo $y = 1$, embora não possua uma semântica probabilística direta [^499]
- $(z)_+$ denota a parte positiva de $z$, isto é, $\max(0, z)$

A função hinge loss penaliza pontos de dados que estão dentro da margem ou que são classificados incorretamente. A **margem** é definida como a região entre as linhas de decisão que separam as classes [^500, 501]. Pontos que estão corretamente classificados e fora da margem não contribuem para a perda, enquanto pontos dentro da margem ou classificados incorretamente incorrem em uma penalidade linear.

O objetivo geral do SVM é minimizar a seguinte expressão [^499]:
$$\min_{w, w_0} \frac{1}{2} ||w||^2 + C \sum_{i=1}^{N} (1 - y_i f(x_i))_+$$
onde:
- $w$ é o vetor de pesos
- $w_0$ é o bias
- $C$ é um parâmetro de regularização que controla o trade-off entre a maximização da margem e a minimização do erro de classificação [^499]

A minimização da norma $||w||^2$ busca maximizar a margem [^501], enquanto a minimização da soma das hinge losses busca classificar corretamente os pontos de dados. O parâmetro $C$ controla a importância relativa desses dois objetivos.

A não diferenciabilidade do termo $\max$ na hinge loss pode ser tratada introduzindo **variáveis de folga** $\xi_i$ [^499]:
$$\min_{w, w_0, \xi} \frac{1}{2} ||w||^2 + C \sum_{i=1}^{N} \xi_i$$
sujeito a:
$$\xi_i \geq 0, \quad y_i(w^T x_i + w_0) \geq 1 - \xi_i, \quad i = 1:N$$
onde $\xi_i$ representa o grau de violação da margem pelo ponto $x_i$.

A solução para este problema de otimização resulta em um vetor de pesos $w$ que é uma combinação linear de um subconjunto dos dados de treinamento, chamados **vetores de suporte** [^498, 499]. Esses vetores são os pontos que estão dentro da margem ou que são classificados incorretamente. A esparsidade da solução é uma consequência direta da hinge loss, que zera a contribuição dos pontos bem classificados e fora da margem [^498].

A hinge loss também está relacionada ao conceito de **grande margem** [^501, 502]. Ao maximizar a margem, o SVM busca encontrar uma fronteira de decisão que seja robusta contra pequenas perturbações nos dados de treinamento. Isso leva a uma melhor generalização para dados não vistos.

### Conclusão

A **hinge loss** é um componente fundamental dos **Support Vector Machines (SVMs)**, proporcionando uma maneira eficaz de maximizar a margem entre classes e obter soluções esparsas. Ao penalizar pontos dentro da margem ou classificados incorretamente, a hinge loss promove a robustez da fronteira de decisão contra ruído e outliers, resultando em um modelo com boa capacidade de generalização. Embora não possua uma interpretação probabilística direta, a hinge loss oferece uma alternativa eficaz às funções de perda baseadas em log-verossimilhança, como a log-loss utilizada em regressão logística [^499].

### Referências
[^499]: *Bishop, Christopher. Pattern Recognition and Machine Learning. Springer, 2006.*
[^501]: *Hastie, Trevor, et al. The Elements of Statistical Learning. Springer, 2009.*
[^500]: *Vapnik, Vladimir. The Nature of Statistical Learning Theory. Springer, 1995.*
[^498]: *Schoelkopf, Bernhard, and Alexander J. Smola. Learning with Kernels. MIT Press, 2002.*
[^502]: *Cristianini, Nello, and John Shawe-Taylor. Support Vector Machines and Other Kernel-Based Learning Methods. Cambridge University Press, 2000.*
<!-- END -->