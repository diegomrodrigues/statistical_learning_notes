{
  "topics": [
    {
      "topic": "Naive Bayes Classifiers",
      "sub_topics": [
        "Naive Bayes classifiers (NBCs) classify vectors of discrete-valued features by assuming conditional independence given the class label, simplifying the class conditional density \\\\(p(x|y = c)\\\\) into a product of one-dimensional densities. This allows the class conditional density to be written as a product of one-dimensional densities: \\\\(p(x|y = c, \\\\theta) = \\\\prod_{j=1}^D p(x_j|y = c, \\\\theta_{jc})\\\\).",
        "The form of the class-conditional density in NBC depends on the type of each feature; for real-valued features, a Gaussian distribution can be used, while for binary features, a Bernoulli distribution is appropriate, leading to the multivariate Bernoulli naive Bayes model.",
        "Model fitting for NBC involves computing the Maximum Likelihood Estimate (MLE) or Maximum A Posteriori (MAP) estimate for the parameters. Training a naive Bayes classifier involves computing the MLE or MAP estimate for the parameters, where the log-likelihood decomposes into a series of terms concerning the class priors \\\\(\\\\pi_c\\\\) and the feature likelihoods \\\\(\\\\theta_{jc}\\\\), allowing for separate optimization.",
        "The MLE for the class prior is given by \\\\(\\\\hat{\\\\pi}_c = \\\\frac{N_c}{N}\\\\), where \\\\(N_c\\\\) is the number of examples in class \\\\(c\\\\), and for binary features, the MLE for the feature likelihood is \\\\(\\\\hat{\\\\theta}_{jc} = \\\\frac{N_{jc}}{N_c}\\\\), where \\\\(N_{jc}\\\\) is the number of times feature \\\\(j\\\\) occurs in class \\\\(c\\\\).",
        "Maximum likelihood estimation can lead to overfitting, and a simple solution is to adopt a Bayesian approach with a factored prior, \\\\(p(\\\\theta) = p(\\\\pi) \\\\prod_{j=1}^D \\\\prod_{c=1}^C p(\\\\theta_{jc})\\\\), using a Dirichlet prior for \\\\(\\\\pi\\\\) and a Beta prior for each \\\\(\\\\theta_{jc}\\\\), often with add-one or Laplace smoothing. Bayesian NBC addresses overfitting by using a factored prior, \\\\(p(0) = p(\\\\pi)\\\\prod\\\\prod p(\\\\theta_{jc})\\\\, where \\\\(\\pi\\\\) is the class prior and \\\\(\\theta_{jc}\\\\) represents the parameters for feature \\\\(j\\\\) in class \\\\(c\\\\), combined with a Dirichlet prior for \\\\(\\pi\\\\) and a Beta prior for each \\\\(\\theta_{jc}\\\\).",
        "At test time, the goal is to compute the posterior probability \\\\(p(y = c|x, D)\\\\), which involves integrating out the unknown parameters; if the posterior is Dirichlet, the posterior predictive density can be obtained by simply plugging in the posterior mean parameters. At test time, NBC computes the posterior probability p(y = c|x, D) using Bayes' theorem, plugging in the posterior mean parameters \\\\(\\theta\\\\), where the posterior predictive density is obtained by multiplying the class prior with the product of feature likelihoods.",
        "The log-sum-exp trick is a practical detail that arises when using generative classifiers, addressing numerical underflow by taking logs when applying Bayes rule and factoring out the largest term to represent remaining numbers relative to that. The log-sum-exp trick is used to prevent numerical underflow when computing the posterior over class labels, where the log of the sum of exponentials is approximated by factoring out the largest term and representing the remaining numbers relative to that.",
        "Feature selection can tackle overfitting and reduce run-time cost by removing irrelevant features, using mutual information to measure relevance between feature \\\\(X_j\\\\) and class label \\\\(Y\\\\), computed as \\\\(I(X, Y) = \\\\sum_{x_j} \\\\sum_y p(x_j, y) \\\\log \\\\frac{p(x_j, y)}{p(x_j)p(y)}\\\\). Feature selection in NBC tackles overfitting and high run-time costs by removing irrelevant features, using mutual information to measure relevance between feature \\\\(X_j\\\\) and the class label \\\\(Y\\\\), where \\\\(I(X,Y) = \\\\sum\\\\sum p(x_j, y)log[p(x_j, y) / p(x_j)p(y)]\\\\).",
        "In document classification, documents can be represented as binary vectors indicating word presence, and the class conditional density can be modeled using a Bernoulli product model, or a multinomial distribution, which counts the number of occurrences of each word. Replacing the multinomial class conditional density with the Dirichlet Compound Multinomial (DCM) density captures the burstiness phenomenon, where seeing one occurrence of a word updates posterior counts, making another occurrence more likely, in contrast to the multinomial model where word occurrences are independent."
      ]
    },
    {
      "topic": "Likelihood and Conjugate Priors",
      "sub_topics": [
        "The likelihood function quantifies the probability of observing the given data \\\\(D\\\\), assuming that the data are independent and identically distributed (iid), given the parameter \\\\(\\theta\\\\), and is mathematically represented as \\\\(p(D|\\\\theta) = \\\\theta^{N_1} * (1 - \\\\theta)^{N_0}\\\\), where \\\\(N_1\\\\) is the number of 'heads' and \\\\(N_0\\\\) is the number of 'tails'.",
        "Sufficient statistics are derived from the data, such as \\\\(N_1\\\\) (number of heads) and \\\\(N_0\\\\) (number of tails), which encapsulate all the necessary information from the dataset \\\\(D\\\\) to infer the parameter \\\\(\\theta\\\\), such that \\\\(p(\\\\theta|D) = p(\\\\theta|s(data))\\\\, where \\\\(s(D)\\\\) represents the sufficient statistics.",
        "The binomial sampling model is used when the data consists of the count of the number of heads \\\\(N_1\\\\) observed in a fixed number \\\\(N\\\\) of trials, and its likelihood is the same as the Bernoulli model because the inferences about \\\\(\\theta\\\\) will be the same whether we observe the counts or a sequence of trials.",
        "Conjugate priors maintain the same functional form as the likelihood function, which simplifies the computation of the posterior distribution; for the Bernoulli distribution, the conjugate prior is the beta distribution, represented as \\\\(Beta(\\\\theta|a, b) \\\\propto \\\\theta^(a-1) * (1 - \\\\theta)^(b-1)\\\\)."
      ]
    }
  ]
}