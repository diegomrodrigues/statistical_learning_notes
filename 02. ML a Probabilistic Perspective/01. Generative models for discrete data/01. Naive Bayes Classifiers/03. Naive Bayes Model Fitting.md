## Model Fitting in Naive Bayes Classifiers

### Introdução

Este capítulo explora o processo de **model fitting** para **Naive Bayes Classifiers (NBC)**, um método fundamental no aprendizado de máquina generativo para dados discretos. O objetivo principal é estimar os parâmetros do modelo, permitindo a classificação de novas instâncias com base nos dados de treinamento observados. A simplicidade e eficiência do NBC o tornam uma ferramenta valiosa, mesmo que suas suposições de independência nem sempre se sustentem na prática. O foco aqui é derivar e entender os estimadores de **Maximum Likelihood Estimate (MLE)** e **Maximum A Posteriori (MAP)** para os parâmetros do modelo, explorando as implicações de diferentes escolhas de prior e suas influências no desempenho do classificador.

### Conceitos Fundamentais

O treinamento de um classificador Naive Bayes envolve a computação do **MLE** ou **MAP** para os parâmetros [^19]. No contexto do NBC, a probabilidade de log se decompõe em uma série de termos relativos aos **class priors** $\pi_c$ e às **feature likelihoods** $\theta_{jc}$, permitindo a otimização separada [^19]. Matematicamente, podemos expressar a probabilidade para um único caso de dados como:

$$np(x_i, y_i|\theta) = p(y_i|\pi) \prod_j p(x_{ij}|\theta_j)$$

Onde $x_i$ representa o *i*-ésimo vetor de características, $y_i$ é a classe correspondente, $\pi$ são os class priors e $\theta_j$ são os parâmetros para a *j*-ésima característica.

A função de log-likelihood para um conjunto de dados $D$ é dada por [^19]:

$$log p(D|\theta) = \sum_{c=1}^C N_c \log \pi_c + \sum_{j=1}^D \sum_{c=1}^C \sum_{i:y_i=c} \log p(x_{ij}|\theta_{jc})$$

Aqui, $N_c$ é o número de exemplos na classe $c$, $C$ é o número de classes, e $D$ é o número de features. A decomposição da log-likelihood permite otimizar separadamente os class priors $\pi_c$ e as feature likelihoods $\theta_{jc}$ [^19].

#### Maximum Likelihood Estimation (MLE)

O estimador de **MLE** para os class priors $\pi_c$ é simplesmente a fração de exemplos na classe $c$ [^19]:

$$hat{\pi}_c = \frac{N_c}{N}$$

Onde $N$ é o número total de exemplos.

Para features binárias, onde $x_{j}|y=c \sim Ber(\theta_{jc})$, o estimador de **MLE** para $\theta_{jc}$ é a fração de vezes que a feature *j* ocorre na classe *c* [^19]:

$$hat{\theta}_{jc} = \frac{N_{jc}}{N_c}$$

Onde $N_{jc}$ é o número de vezes que a feature *j* ocorre na classe *c*.

#### Maximum A Posteriori (MAP) Estimation

Para incorporar conhecimento prévio, podemos usar a estimativa **MAP**. Assumindo um prior Dirichlet para $\pi$ e um prior Beta para $\theta_{jc}$, os estimadores **MAP** são:

$$np(\theta) = p(\pi) \prod_{j=1}^D \prod_{c=1}^C p(\theta_{jc})$$

$$np(\pi|D) = Dir(N_1 + \alpha_1, ..., N_C + \alpha_C)$$

$$np(\theta_{jc}|D) = Beta((N_c - N_{jc}) + \beta_0, N_{jc} + \beta_1)$$

Onde $\alpha_c$ e $\beta_i$ são os hiperparâmetros dos priors Dirichlet e Beta, respectivamente. Os estimadores **MAP** são então:

$$hat{\pi}_c = \frac{N_c + \alpha_c}{N + \sum_c \alpha_c}$$

$$hat{\theta}_{jc} = \frac{N_{jc} + \beta_1}{N_c + \beta_0 + \beta_1}$$

Estes estimadores incorporam a influência dos priors, suavizando as estimativas e evitando problemas de overfitting, especialmente quando os dados são escassos. A escolha de $\alpha_c = 1$ e $\beta_i = 1$ corresponde ao **add-one smoothing** ou **Laplace smoothing** [^20].

### Conclusão

O processo de model fitting em Naive Bayes Classifiers envolve a estimativa dos parâmetros do modelo a partir dos dados de treinamento. As estimativas MLE e MAP oferecem abordagens diferentes para este problema, com a estimativa MAP permitindo a incorporação de conhecimento prévio através de priors. A escolha entre MLE e MAP, bem como a seleção dos hiperparâmetros dos priors, depende das características específicas do problema e da quantidade de dados disponíveis. A simplicidade e eficiência do NBC, juntamente com a flexibilidade de incorporar priors, tornam-no uma ferramenta valiosa no aprendizado de máquina.

### Referências

[^19]: Seção 3.5.1 de "Generative models for discrete data"
[^20]: Seção 3.5.1.2 de "Generative models for discrete data"
<!-- END -->