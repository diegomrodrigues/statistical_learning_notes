## Maximum Likelihood Estimation in Naive Bayes Classifiers

### Introdução
Este capítulo aprofunda a estimação de parâmetros no contexto dos classificadores Naive Bayes, com foco particular na estimação de máxima verossimilhança (MLE) para o classificador *a priori* e a verossimilhança de *features* binárias. O classificador Naive Bayes, introduzido na Seção 3.5 [^82], simplifica o problema de classificação ao assumir independência condicional entre os *features* dado o rótulo da classe. Essa suposição permite uma estimação de parâmetros mais tratável, embora possa comprometer a precisão em cenários onde a independência condicional não é válida.

### Conceitos Fundamentais

#### Estimador de Máxima Verossimilhança (MLE) para o Classificador *a priori*

No contexto de um classificador Naive Bayes, o classificador *a priori* representa a probabilidade de observar uma determinada classe independentemente dos *features*. O MLE para o classificador *a priori*, denotado como $\hat{\pi}_c$, é estimado pela proporção de exemplos na classe *c* em relação ao número total de exemplos [^83]:

$$\hat{\pi}_c = \frac{N_c}{N}$$

onde $N_c$ é o número de exemplos na classe *c* e *N* é o número total de exemplos no conjunto de dados. Este estimador é intuitivamente apelativo, uma vez que atribui probabilidades mais elevadas às classes que são mais frequentemente observadas nos dados de treinamento.

#### Estimador de Máxima Verossimilhança (MLE) para a Verossimilhança de *Features* Binárias

Para *features* binárias, a verossimilhança de *features* representa a probabilidade de observar um determinado valor de *feature* dado o rótulo da classe. O MLE para a verossimilhança de *features* binárias, denotado como $\hat{\theta}_{jc}$, é estimado pela proporção de vezes que a *feature* *j* ocorre na classe *c* em relação ao número total de exemplos na classe *c* [^83]:

$$\hat{\theta}_{jc} = \frac{N_{jc}}{N_c}$$

onde $N_{jc}$ é o número de vezes que a *feature* *j* ocorre na classe *c*. Este estimador quantifica a associação entre a presença da *feature* *j* e a classe *c*.

#### Derivação Formal

Para derivar formalmente o MLE para $\pi_c$, começamos com a função de verossimilhança para os dados:

$$L(\pi) = \prod_{i=1}^N \pi_{y_i}$$

onde $y_i$ é o rótulo da classe para o *i*-ésimo exemplo. Para maximizar esta função de verossimilhança, aplicamos uma restrição de que as probabilidades somam 1:

$$\sum_{c=1}^C \pi_c = 1$$

Usando multiplicadores de Lagrange, formamos o Lagrangiano:

$$L(\pi, \lambda) = \prod_{i=1}^N \pi_{y_i} - \lambda \left( \sum_{c=1}^C \pi_c - 1 \right)$$

Tomando a derivada com respeito a $\pi_c$ e definindo-a como zero, obtemos:

$$\frac{\partial L}{\partial \pi_c} = \frac{N_c}{\pi_c} - \lambda = 0$$

Resolvendo para $\pi_c$, obtemos:

$$\pi_c = \frac{N_c}{\lambda}$$

Somando sobre todas as classes, obtemos:

$$\sum_{c=1}^C \pi_c = \sum_{c=1}^C \frac{N_c}{\lambda} = 1$$

Portanto, $\lambda = N$ e o MLE para $\pi_c$ é:

$$\hat{\pi}_c = \frac{N_c}{N}$$

Para derivar formalmente o MLE para $\theta_{jc}$, começamos com a função de verossimilhança para os dados:

$$L(\theta) = \prod_{i=1}^N \prod_{j=1}^D \theta_{jc}^{x_{ij} \mathbb{I}(y_i = c)} (1 - \theta_{jc})^{(1 - x_{ij}) \mathbb{I}(y_i = c)}$$

onde $x_{ij}$ é o valor da *feature* *j* para o *i*-ésimo exemplo, e $\mathbb{I}(y_i = c)$ é uma função indicadora que é 1 se o rótulo da classe $y_i$ é igual a *c* e 0 caso contrário.

Tomando o logaritmo da função de verossimilhança, obtemos:

$$\log L(\theta) = \sum_{i=1}^N \sum_{j=1}^D \left[ x_{ij} \mathbb{I}(y_i = c) \log \theta_{jc} + (1 - x_{ij}) \mathbb{I}(y_i = c) \log (1 - \theta_{jc}) \right]$$

Para maximizar esta função de log-verossimilhança, tomamos a derivada com respeito a $\theta_{jc}$ e definimos como zero:

$$\frac{\partial \log L(\theta)}{\partial \theta_{jc}} = \sum_{i=1}^N \left[ \frac{x_{ij} \mathbb{I}(y_i = c)}{\theta_{jc}} - \frac{(1 - x_{ij}) \mathbb{I}(y_i = c)}{1 - \theta_{jc}} \right] = 0$$

Resolvendo para $\theta_{jc}$, obtemos:

$$\hat{\theta}_{jc} = \frac{\sum_{i=1}^N x_{ij} \mathbb{I}(y_i = c)}{\sum_{i=1}^N \mathbb{I}(y_i = c)} = \frac{N_{jc}}{N_c}$$

onde $N_{jc}$ é o número de vezes que a *feature* *j* ocorre na classe *c*, e $N_c$ é o número de exemplos na classe *c*. $\blacksquare$

#### Exemplo Ilustrativo

Considere um conjunto de dados com 100 exemplos, onde 60 exemplos pertencem à classe A e 40 exemplos pertencem à classe B. Para a classe A, a *feature* 1 ocorre em 30 exemplos. Para a classe B, a *feature* 1 ocorre em 10 exemplos.

O MLE para o classificador *a priori* seria:
- $\hat{\pi}_A = \frac{60}{100} = 0.6$
- $\hat{\pi}_B = \frac{40}{100} = 0.4$

O MLE para a verossimilhança de *features* binárias seria:
- $\hat{\theta}_{1A} = \frac{30}{60} = 0.5$
- $\hat{\theta}_{1B} = \frac{10}{40} = 0.25$

#### Implicações

Esses estimadores MLE fornecem uma maneira direta e computacionalmente eficiente de estimar os parâmetros de um classificador Naive Bayes. No entanto, é importante notar que esses estimadores são propensos a *overfitting*, especialmente quando o tamanho do conjunto de dados é pequeno ou quando existem *features* raras. Para mitigar o *overfitting*, técnicas de suavização, como a suavização de Laplace (também conhecida como *add-one smoothing*) podem ser aplicadas [^83].

### Conclusão

A estimação de máxima verossimilhança fornece um *framework* simples, mas eficaz, para estimar os parâmetros de um classificador Naive Bayes. Ao estimar o classificador *a priori* e a verossimilhança de *features* binárias usando MLE, podemos construir um classificador que pode prever a classe de novos exemplos com base em seus valores de *feature*. No entanto, é crucial estar ciente das limitações do MLE, particularmente sua propensão a *overfitting*, e considerar técnicas de suavização para melhorar o desempenho de generalização.

### Referências
[^82]: Seção 3.5, "Naive Bayes classifiers"
[^83]: Seção 3.5.1, "Model fitting"
<!-- END -->