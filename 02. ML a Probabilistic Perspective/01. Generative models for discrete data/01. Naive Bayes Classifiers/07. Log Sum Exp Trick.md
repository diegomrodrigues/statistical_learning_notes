## O Truque Log-Sum-Exp em Classificadores Naive Bayes

### Introdução
Em classificadores generativos, o cálculo da probabilidade posterior sobre rótulos de classe frequentemente envolve produtos de pequenas probabilidades, o que pode levar a *numerical underflow*. O **truque log-sum-exp** é uma técnica utilizada para mitigar este problema, transformando multiplicações em somas no domínio logarítmico e evitando assim a perda de precisão numérica. Este capítulo explora o truque log-sum-exp em detalhes, dentro do contexto dos classificadores Naive Bayes.

### Conceitos Fundamentais
O truque log-sum-exp é uma técnica essencial para evitar o *numerical underflow* ao calcular a probabilidade posterior sobre rótulos de classe em modelos generativos, como os classificadores Naive Bayes [^86]. A probabilidade posterior é calculada usando a regra de Bayes [^1], que envolve multiplicar a probabilidade *a priori* da classe pela verossimilhança dos dados dada a classe. Quando os dados são de alta dimensão, a verossimilhança pode ser um número muito pequeno, levando ao *underflow*.

Para combater isso, o truque log-sum-exp aplica logaritmos às probabilidades e transforma a multiplicação em uma soma [^86]. A probabilidade posterior é dada por:

$$
p(y = c|x) \propto p(x|y = c)p(y = c)
$$

Tomando o logaritmo de ambos os lados, obtemos:

$$
\log p(y = c|x) = \log p(x|y = c) + \log p(y = c) + \text{constante}
$$

Onde a "constante" é um termo de normalização que garante que as probabilidades posteriores somem 1.

No entanto, ao calcular a probabilidade posterior normalizada, ainda precisamos somar exponenciais de log-probabilidades, o que pode causar problemas numéricos. Para resolver isso, o truque log-sum-exp subtrai o maior termo antes de calcular a exponencial [^86]:

$$
\log \sum_{c=1}^C e^{b_c} = B + \log \sum_{c=1}^C e^{b_c - B}
$$

onde $B = \max_c b_c$ e $b_c = \log p(x|y = c) + \log p(y = c)$ [^86]. Esta transformação garante que o maior termo dentro da exponencial seja 1, evitando o *underflow*.

**Implementação do Truque Log-Sum-Exp:**

1.  Calcule $b_c = \log p(x|y = c) + \log p(y = c)$ para todas as classes $c$.
2.  Encontre $B = \max_c b_c$.
3.  Calcule $\log \sum_{c=1}^C e^{b_c - B}$.
4.  Adicione $B$ ao resultado para obter $\log \sum_{c=1}^C e^{b_c}$.

**Exemplo:**

Suponha que tenhamos duas classes e as seguintes log-probabilidades não normalizadas:

$$
b_1 = -120, \quad b_2 = -121
$$

Sem o truque log-sum-exp, calcularíamos $e^{-120} + e^{-121}$, o que pode resultar em *underflow*. Usando o truque, temos:

$$
B = \max(-120, -121) = -120
$$

$$
\log(e^{-120} + e^{-121}) = -120 + \log(e^0 + e^{-1}) = -120 + \log(1 + e^{-1})
$$

Este cálculo é numericamente estável e evita o *underflow*.

O truque log-sum-exp é usado no Algoritmo 1 para calcular $p(y_i|x_i, \theta)$ [^86]. No entanto, não é necessário se quisermos apenas calcular $\hat{y}_i$, pois podemos simplesmente maximizar a quantidade não normalizada $\log p(y_i = c) + \log p(x_i|y = c)$ [^86].

### Conclusão
O truque log-sum-exp é uma ferramenta prática e essencial para garantir a estabilidade numérica em classificadores generativos, como os Naive Bayes, especialmente quando lidamos com dados de alta dimensão. Ao transformar multiplicações em somas no domínio logarítmico e ao fatorar o maior termo, evitamos o problema de *numerical underflow* e garantimos uma precisão maior nos cálculos da probabilidade posterior.

### Referências
[^1]: Seção 2.2.3.2
[^86]: Seção 3.5.3
<!-- END -->