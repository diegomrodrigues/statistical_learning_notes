## Naive Bayes Classifiers: Conditional Independence and Density Estimation

### Introdução
Em continuidade ao estudo de modelos generativos para dados discretos, este capítulo aprofunda-se nos **Naive Bayes Classifiers (NBCs)**, uma ferramenta fundamental para a classificação de dados com características discretas [^82]. Como mencionado anteriormente, NBCs simplificam a complexidade da modelagem da densidade condicional da classe ao assumir a *independência condicional* entre as características, dado o rótulo da classe [^82]. Esta simplificação permite uma modelagem mais eficiente e robusta, especialmente em cenários com alta dimensionalidade e dados esparsos.

### Conceitos Fundamentais
A essência dos Naive Bayes Classifiers reside na simplificação da densidade condicional da classe [^82]:
$$\
p(x|y = c, \theta) = \prod_{j=1}^D p(x_j|y = c, \theta_{jc})
$$
onde:
- $x$ é o vetor de características (features).
- $y$ é o rótulo da classe.
- $c$ é um valor específico para o rótulo da classe.
- $\theta$ representa os parâmetros do modelo.
- $D$ é o número de características.
- $x_j$ é a *j*-ésima característica.
- $\theta_{jc}$ são os parâmetros específicos para a *j*-ésima característica na classe *c*.

Essa formulação assume que cada característica $x_j$ é *condicionalmente independente* das outras características, dado o rótulo da classe *y* [^82]. Essa suposição "naive" simplifica drasticamente o processo de modelagem, pois em vez de modelar a densidade conjunta $p(x|y=c, \theta)$, modelamos *D* densidades unidimensionais $p(x_j|y=c, \theta_{jc})$ [^82].

A escolha da forma para $p(x_j|y = c, \theta_{jc})$ depende do tipo de característica [^82]. O texto apresenta os seguintes casos:

*   **Características Reais:** A distribuição Gaussiana pode ser utilizada [^82]:
    $$\
    p(x_j|y = c, \theta_{jc}) = \mathcal{N}(x_j|\mu_{jc}, \sigma_{jc}^2)
    $$
    onde $\mu_{jc}$ é a média da característica *j* na classe *c*, e $\sigma_{jc}^2$ é sua variância.
*   **Características Binárias:** A distribuição de Bernoulli é apropriada [^82]:
    $$\
    p(x_j|y = c, \theta_{jc}) = \text{Ber}(x_j|\theta_{jc}) = \theta_{jc}^{x_j} (1 - \theta_{jc})^{(1 - x_j)}
    $$
    onde $\theta_{jc}$ é a probabilidade de que a característica *j* ocorra na classe *c*. Este modelo é chamado de *multivariate Bernoulli naive Bayes model* [^82].
*   **Características Categóricas:** A distribuição multinoulli (categórica) é utilizada [^83]:
    $$\
    p(x_j|y = c, \theta) = \text{Cat}(x_j|\mu_{jc})
    $$
    onde $\mu_{jc}$ é um histograma sobre os *K* possíveis valores para $x_j$ na classe *c*.

#### Model Fitting
O processo de *model fitting*, ou treinamento, em um NBC envolve estimar os parâmetros $\theta$ a partir dos dados de treinamento [^83]. Isso geralmente é feito maximizando a *verossimilhança (likelihood)* dos dados, resultando nas estimativas de *Maximum Likelihood Estimation (MLE)* ou *Maximum a Posteriori (MAP)* [^83].

Considerando o caso de características binárias, o MLE para os parâmetros do modelo é dado por [^83]:
$$\
\hat{\theta}_{jc} = \frac{N_{jc}}{N_c}
$$
onde $N_{jc}$ é o número de vezes que a característica *j* ocorre na classe *c*, e $N_c$ é o número total de exemplos na classe *c* [^83].
A estimativa do class prior $\pi_c$ é dada por [^83]:
$$\
\hat{\pi}_c = \frac{N_c}{N}
$$
onde *N* é o número total de amostras [^83].

#### Bayesian Naive Bayes
O texto menciona que o MLE pode sofrer de *overfitting*, especialmente quando os dados são escassos [^84]. Para mitigar este problema, o texto propõe uma abordagem Bayesiana, utilizando priors sobre os parâmetros [^84]. Para o class prior $\pi$, um prior de Dirichlet é usado, e para cada $\theta_{jc}$, um prior Beta é usado [^84]. As equações resultantes para o posterior são [^85]:
$$\
p(\pi|D) = \text{Dir}(N_1 + \alpha_1, ..., N_C + \alpha_C)
$$
$$\
p(\theta_{jc}|D) = \text{Beta}((N_c - N_{jc}) + \beta_0, N_{jc} + \beta_1)
$$

#### Uso do Modelo para Predição
Uma vez que o modelo é treinado, ele pode ser usado para classificar novas instâncias [^85]. Dada uma nova instância *x*, a probabilidade de pertencer a uma classe *c* é dada por [^85]:
$$\
p(y = c|x, D) \propto p(y = c|D) \prod_{j=1}^D p(x_j|y = c, D)
$$
onde $p(y = c|D)$ é o class prior e $p(x_j|y = c, D)$ é a densidade condicional da característica *j* na classe *c*, estimada a partir dos dados de treinamento.

No caso de características binárias e utilizando a abordagem Bayesiana, a equação se torna [^85]:
$$\
p(y = c|x, D) \propto \pi_c \prod_{j=1}^D \theta_{jc}^{x_j} (1 - \theta_{jc})^{(1 - x_j)}
$$
onde
$$\
\theta_{jc} = \frac{N_{jc} + \beta_1}{N_c + \beta_0 + \beta_1}
$$
$$\
\pi_c = \frac{N_c + \alpha_c}{N + \alpha_0}
$$
e $\alpha_0 = \sum_c \alpha_c$ [^85].

#### Log-Sum-Exp Trick
O texto aborda um problema prático comum ao usar classificadores generativos: o *numerical underflow* [^86]. Como as probabilidades $p(x|y = c)$ podem ser muito pequenas, especialmente em espaços de alta dimensão, o produto dessas probabilidades pode resultar em valores tão pequenos que excedem a precisão da máquina [^86]. Para evitar isso, o texto sugere o uso do *log-sum-exp trick*, que opera no domínio logarítmico para evitar o underflow [^86].

#### Feature Selection
Outro problema abordado é o *overfitting* e o custo computacional elevado de NBCs em espaços de alta dimensão [^86]. Para mitigar esses problemas, o texto sugere o uso de *feature selection* para remover características irrelevantes [^86]. Uma abordagem simples é usar a *mutual information* entre cada característica e o rótulo da classe para avaliar a relevância da característica [^86].

### Conclusão
Os Naive Bayes Classifiers são uma ferramenta poderosa e eficiente para a classificação de dados com características discretas [^82]. Sua simplicidade e interpretabilidade os tornam uma escolha popular em muitas aplicações [^82]. Apesar da suposição "naive" de independência condicional, os NBCs frequentemente apresentam um bom desempenho na prática [^82]. As técnicas de regularização Bayesiana e feature selection podem ser usadas para melhorar ainda mais o desempenho dos NBCs e mitigar os problemas de overfitting e custo computacional [^84, 86].

### Referências
[^82]: Seção 3.5, p. 82.
[^83]: Seção 3.5.1, p. 83.
[^84]: Seção 3.5.1.2, p. 84.
[^85]: Seção 3.5.2, p. 85.
[^86]: Seção 3.5.3, p. 86.

<!-- END -->