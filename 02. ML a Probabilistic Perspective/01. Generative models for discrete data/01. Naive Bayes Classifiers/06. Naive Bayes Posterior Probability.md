## Predição e Inferência com Classificadores Naive Bayes

### Introdução
Este capítulo explora o uso de classificadores Naive Bayes (NBC) para a classificação de dados discretos. Em particular, focaremos na fase de teste, onde o objetivo é calcular a probabilidade *a posteriori* de uma classe dado um vetor de características e um conjunto de dados de treinamento. Este processo envolve a integração dos parâmetros desconhecidos do modelo, com particular atenção ao caso em que a distribuição *a posteriori* é Dirichlet [^85].

### Conceitos Fundamentais

Em um classificador Naive Bayes, o objetivo no tempo de teste é calcular a probabilidade *a posteriori* $p(y = c|x, D)$ [^85], onde $y$ é a classe, $c$ é um valor específico de classe, $x$ é o vetor de características, e $D$ é o conjunto de dados de treinamento.

A abordagem Bayesiana correta envolve integrar os parâmetros desconhecidos [^85]:

$$ p(y = c|x, D) \propto \int p(x|y = c, \theta) p(\theta|D) d\theta $$

onde $\theta$ representa os parâmetros do modelo.

No contexto do NBC, assumimos que as características são condicionalmente independentes dada a classe [^82]. Isso permite expressar a probabilidade condicional da classe como [^82]:

$$ p(x|y = c, \theta) = \prod_{j=1}^{D} p(x_j|y = c, \theta_{jc}) $$

onde $D$ é o número de características e $\theta_{jc}$ são os parâmetros específicos para a característica $j$ na classe $c$.

O texto menciona que, se a *a posteriori* é Dirichlet, a densidade preditiva *a posteriori* pode ser obtida simplesmente inserindo os parâmetros da média *a posteriori* [^85]. Isso se baseia na Equação 3.51, que fornece a distribuição preditiva *a posteriori* para um único ensaio multinoulli usando uma prior Dirichlet [^81]:

$$ p(X = j|D) = \frac{\alpha_j + N_j}{\sum_{k}(\alpha_k + N_k)} $$

onde $\alpha_j$ são os hiperparâmetros da prior Dirichlet e $N_j$ são as contagens empíricas para a classe $j$.

Para características binárias, a probabilidade *a posteriori* é dada por [^85]:

$$ p(y = c|x, D) \propto \pi_c \prod_{j=1}^{D} (\theta_{jc})^{(x_j=1)} (1 - \theta_{jc})^{(x_j=0)} $$

onde $\pi_c$ é a probabilidade *a priori* da classe $c$ e $\theta_{jc}$ é a probabilidade da característica $j$ estar presente na classe $c$.

Os parâmetros da média *a posteriori* são calculados como [^85]:

$$ \theta_{jk} = \frac{N_{jc} + \beta_1}{N_c + \beta_0 + \beta_1} $$

$$ \pi_c = \frac{N_c + \alpha_c}{N + \alpha_0} $$

onde $\beta_0$ e $\beta_1$ são os parâmetros da prior Beta para cada característica, e $\alpha_c$ são os parâmetros da prior Dirichlet para as classes. $N_c$ é o número de exemplos na classe $c$, e $N_{jc}$ é o número de vezes que a característica $j$ ocorre na classe $c$.

Se a *a posteriori* for aproximada por um único ponto, $p(\theta|D) \approx \delta_{\hat{\theta}}(\theta)$ [^85], onde $\hat{\theta}$ pode ser a estimativa ML ou MAP, então a densidade preditiva *a posteriori* é obtida simplesmente substituindo os parâmetros, resultando em uma regra virtualmente idêntica [^85]:

$$ p(y = c|x, D) \propto \pi_c \prod_{j=1}^{D} (\theta_{jc})^{(x_j=1)} (1 - \theta_{jc})^{(x_j=0)} $$

### Conclusão

Em resumo, a fase de teste em NBC envolve o cálculo da probabilidade *a posteriori* de uma classe dado um vetor de características. A abordagem Bayesiana correta integra os parâmetros desconhecidos, e quando a *a posteriori* é Dirichlet, a densidade preditiva *a posteriori* pode ser obtida simplesmente inserindo os parâmetros da média *a posteriori*. Se a *a posteriori* é aproximada por um único ponto (ML ou MAP), uma regra virtualmente idêntica é obtida.

### Referências
[^85]: Capítulo 3 do texto fornecido.
[^82]: Capítulo 3 do texto fornecido.
[^81]: Capítulo 3 do texto fornecido.

<!-- END -->