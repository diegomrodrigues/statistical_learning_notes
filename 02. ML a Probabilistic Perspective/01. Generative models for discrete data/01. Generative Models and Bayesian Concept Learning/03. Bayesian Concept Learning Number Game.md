## Capítulo 3.2: O Jogo dos Números como Ilustração da Aprendizagem Bayesiana de Conceitos

### Introdução

Conforme discutido na introdução deste capítulo sobre modelos generativos para dados discretos, uma tarefa fundamental é a classificação e a inferência de parâmetros a partir de observações. Expandindo a ideia de classificadores generativos introduzida anteriormente, focaremos agora na **aprendizagem de conceitos (concept learning)**, um processo cognitivo que pode ser modelado com ferramentas Bayesianas. A aprendizagem de conceitos é análoga à classificação binária, onde o objetivo é aprender uma função indicadora $f(x)$ que define se um elemento $x$ pertence a um conceito $C$ [^1]. Pesquisas psicológicas indicam que humanos podem aprender conceitos eficazmente apenas a partir de exemplos positivos, um cenário que desafia técnicas de classificação binária padrão que geralmente requerem exemplos positivos e negativos [^1]. Para explorar este cenário pedagogicamente, utilizaremos o **number game**, um exemplo simples proposto na tese de doutorado de Josh Tenenbaum [^1]. Neste jogo, um conceito aritmético $C$ (como "números primos" ou "números entre 1 e 10") é escolhido, e uma série de exemplos positivos $D = \\{x_1, ..., x_N\\}$ extraídos de $C$ é fornecida. A tarefa consiste em classificar se um novo caso de teste $\hat{x}$ pertence a $C$ [^1]. Este capítulo detalhará como a abordagem Bayesiana modela este processo, focando na inferência a partir de exemplos positivos.

### Conceitos Fundamentais

**O Jogo dos Números: Configuração e Intuição**

No *number game*, assumimos, por simplicidade, que o universo de discurso são os inteiros entre 1 e 100 [^2]. Quando apenas um exemplo positivo é fornecido, digamos $D = \\{16\\}$, a tarefa de predição é inerentemente vaga. Humanos tendem a julgar que números *similares* a 16 são mais prováveis de pertencer ao conceito $C$, mas a noção de similaridade é multifacetada: 17 é similar por ser próximo, 6 por compartilhar um dígito, 32 por ser par e uma potência de 2, enquanto 99 parece dissimilar [^2]. As predições humanas, neste caso, refletem essa incerteza, atribuindo probabilidade a números similares a 16 sob várias métricas de similaridade, resultando numa distribuição preditiva difusa, como ilustrado experimentalmente na Figura 3.1 (linha superior) [^2].

Formalmente, as predições são representadas por uma distribuição de probabilidade $p(\hat{x} \in C | D)$, conhecida como a **distribuição preditiva posterior (posterior predictive distribution)** [^2].

> A distribuição preditiva posterior $p(\hat{x} \in C | D)$ captura a probabilidade de um novo item $\hat{x}$ pertencer ao conceito $C$, dados os exemplos observados $D$.

Se mais exemplos positivos são fornecidos, como $D = \\{16, 8, 2, 64\\}$, a natureza das predições muda drasticamente. Observadores humanos tendem a inferir uma regra subjacente, neste caso, "potências de dois". Este é um exemplo de **indução (induction)**, onde uma hipótese geral é formada a partir de dados específicos [^2]. Com esta hipótese, a distribuição preditiva torna-se muito mais específica, concentrando a maior parte de sua massa nas potências de dois (Figura 3.1, terceira linha) [^2]. Um conjunto diferente de exemplos, como $D = \\{16, 23, 19, 20\\}$, levaria a um gradiente de generalização diferente, focado em números próximos a 20 (Figura 3.1, linha inferior), ilustrando a sensibilidade da indução aos dados observados [^2].

**A Estrutura Bayesiana para Aprendizagem de Conceitos**

Para explicar e emular este comportamento indutivo numa máquina, a abordagem clássica é supor um **espaço de hipóteses (hypothesis space)** $\mathcal{H}$ de conceitos possíveis [^3]. No *number game*, $\mathcal{H}$ poderia incluir conceitos como "números ímpares", "números pares", "todos os números entre 1 e 100", "potências de dois", "números terminados em $j$" (para $0 \le j \le 9$), etc. [^3, ^4]. O subconjunto de $\mathcal{H}$ que é consistente com os dados $D$ é chamado de **espaço de versões (version space)** [^4]. À medida que mais exemplos são observados, o *version space* encolhe, aumentando a certeza sobre o conceito (Mitchell 1997) [^4].

No entanto, o *version space* por si só não explica todo o processo. Por exemplo, após ver $D = \\{16, 8, 2, 64\\}$, tanto "potências de dois" quanto "todos os números pares" são consistentes, mas as pessoas preferem a primeira. Por quê? A inferência Bayesiana fornece uma explicação quantitativa através da combinação de *likelihood*, *prior* e *posterior*.

**Likelihood e o Princípio da Parcimônia (Size Principle)**

A intuição chave para preferir "potências de dois" ($h_{two}$) sobre "números pares" ($h_{even}$) após observar $D = \\{16, 8, 2, 64\\}$ é evitar **coincidências suspeitas (suspicious coincidences)** [^5]. Se o conceito verdadeiro fosse $h_{even}$, seria muito coincidência observar apenas potências de dois. Para formalizar isso, Tenenbaum introduz a **strong sampling assumption**: assume-se que os exemplos $D$ são amostrados uniformemente e de forma independente (com reposição) da **extensão (extension)** do conceito $h$ (o conjunto de números pertencentes a $h$) [^5]. Sob esta suposição, a probabilidade de observar os dados $D$ dado uma hipótese $h$, conhecida como **likelihood**, é:

$$\np(D|h) = \left[ \frac{1}{\text{size}(h)} \right]^N = \frac{1}{|h|^N} \quad \text{se } D \subseteq h \text{ e } 0 \text{ caso contrário}
\tag{3.2}
$$ [^5]

onde $|h|$ é o tamanho da extensão de $h$ (e.g., $|h_{two}| = 6$ para números entre 1 e 100, $|h_{even}| = 50$) e $N$ é o número de exemplos. A equação (3.2) incorpora o que Tenenbaum chama de **size principle**: o modelo favorece a hipótese mais simples (menor extensão) que é consistente com os dados [^5]. Este princípio é uma manifestação da **Navalha de Occam (Occam\'s razor)** [^5].

Vejamos como funciona:
*   Se $D = \\{16\\}$ ($N=1$), então $p(D|h_{two}) = 1/6$ e $p(D|h_{even}) = 1/50$. O *likelihood* favorece $h_{two}$.
*   Se $D = \\{16, 8, 2, 64\\}$ ($N=4$), então $p(D|h_{two}) = (1/6)^4 \approx 7.7 \times 10^{-4}$ e $p(D|h_{even}) = (1/50)^4 = 1.6 \times 10^{-7}$. A razão de *likelihoods* é de quase 5000:1 a favor de $h_{two}$, quantificando a intuição de que os dados seriam uma coincidência muito suspeita se gerados por $h_{even}$ [^5].

**Prior e Conhecimento Prévio**

O *likelihood* sozinho não resolve todos os problemas. Considere $D = \\{16, 8, 2, 64\\}$. A hipótese $h\'$ = "potências de dois exceto 32" tem uma extensão $|h\'|=5$, que é menor que $|h_{two}|=6$. Portanto, $p(D|h\') = (1/5)^4 > p(D|h_{two})$. O *likelihood* favorece $h\'$, embora $h\'$ pareça "conceptualmente não natural" [^6].

Aqui entra a **probabilidade a priori (prior probability)** $p(h)$. O *prior* captura crenças sobre a plausibilidade dos conceitos antes de observar os dados. Hipóteses "não naturais" como $h\'$ podem receber uma probabilidade a priori baixa [^6]. A subjetividade do *prior* é uma fonte de controvérsia no raciocínio Bayesiano, mas é também o mecanismo pelo qual o **conhecimento prévio (background knowledge)** é incorporado, permitindo aprendizado rápido a partir de poucos exemplos [^6]. Para fins ilustrativos, pode-se usar um *prior* simples que atribui probabilidade uniforme a 30 conceitos aritméticos simples (pares, ímpares, primos, etc.), talvez com pesos maiores para pares/ímpares, e pesos baixos para conceitos "não naturais" como "potências de 2 mais 37" ou "potências de 2 exceto 32" (ver Figura 3.2(a)) [^6].

**Posterior: Combinando Dados e Crenças**

A **probabilidade a posteriori (posterior probability)** $p(h|D)$ representa a crença atualizada na hipótese $h$ após observar os dados $D$. É calculada usando a regra de Bayes, combinando o *likelihood* e o *prior*, e normalizando:

$$\np(h|D) = \frac{p(D|h)p(h)}{\sum_{h\' \in \mathcal{H}} p(D|h\')p(h\')} \propto p(h) p(D|h)
$$

Substituindo a expressão do *likelihood* (Eq. 3.2) e usando uma função indicadora $I(D \subseteq h)$ que é 1 se todos os dados em $D$ estão na extensão de $h$ e 0 caso contrário:

$$\np(h|D) = \frac{p(h) I(D \subseteq h) / |h|^N}{\sum_{h\' \in \mathcal{H}} p(h\') I(D \subseteq h\') / |h\'|^N}
\tag{3.3}
$$ [^7]

A Figura 3.2 mostra o *prior*, *likelihood* e *posterior* para $D = \\{16\\}$. O *posterior* é uma combinação de ambos. Conceitos "não naturais" têm *posterior* baixo devido ao *prior* baixo, apesar do *likelihood* potencialmente alto. Conceitos como "números ímpares" têm *posterior* baixo devido ao *likelihood* zero (pois 16 é par), apesar de um *prior* alto [^7].
A Figura 3.3 mostra o mesmo para $D = \\{16, 8, 2, 64\\}$. Agora, o *likelihood* está muito mais concentrado em "potências de dois", dominando o *posterior*. Essencialmente, o aprendiz tem um **momento "aha" (aha moment)** e identifica o conceito verdadeiro (assumindo que o *prior* baixo nos conceitos não naturais previne o *overfitting* aos dados, como escolher "potências de 2 exceto 32") [^7].

**Estimativas Pontuais: MAP e MLE**

Quando há dados suficientes, o *posterior* $p(h|D)$ tende a se concentrar em uma única hipótese. Uma forma de sumarizar a inferência é encontrar a hipótese com a maior probabilidade a posteriori, conhecida como a estimativa **maximum a posteriori (MAP)**:

$$\n\hat{h}_{MAP} = \underset{h \in \mathcal{H}}{\text{argmax}} \, p(h|D) = \underset{h \in \mathcal{H}}{\text{argmax}} \, [p(D|h) p(h)] = \underset{h \in \mathcal{H}}{\text{argmax}} \, [\log p(D|h) + \log p(h)]
\tag{3.4, 3.6}
$$ [^8, ^9]

À medida que a quantidade de dados $N$ aumenta, o termo do *likelihood* $p(D|h)$, que depende exponencialmente de $N$, domina o termo do *prior* $p(h)$, que permanece constante. Neste caso, a estimativa MAP converge para a estimativa de **maximum likelihood estimate (MLE)**:

$$\n\hat{h}_{MLE} = \underset{h \in \mathcal{H}}{\text{argmax}} \, p(D|h) = \underset{h \in \mathcal{H}}{\text{argmax}} \, \log p(D|h)
\tag{3.7}
$$ [^9]

Este fenômeno é frequentemente descrito como **os dados sobrepujam o prior (data overwhelms the prior)** [^10]. Se a verdadeira hipótese geradora dos dados estiver em $\mathcal{H}$, tanto a inferência Bayesiana quanto a estimação por ML são **estimadores consistentes (consistent estimators)**, convergindo para a verdade. Diz-se também que o espaço de hipóteses é **identificável no limite (identifiable in the limit)** [^10].

**Predição: Distribuição Preditiva Posterior vs. Aproximação Plug-in**

O estado de crença interno é o *posterior* $p(h|D)$. Para testar essas crenças, usamos a **distribuição preditiva posterior (posterior predictive distribution)** para prever quantidades objetivamente observáveis, como se um novo item $\hat{x}$ pertence a $C$ [^11]. Esta é dada por uma média das predições de cada hipótese individual, ponderada pela sua probabilidade a posteriori:

$$\np(\hat{x} \in C | D) = \sum_{h \in \mathcal{H}} p(\hat{x} \in C | h) p(h|D)
\tag{3.8}
$$ [^11]

Assumindo que a pertença a um conceito $h$ é determinística ($p(\hat{x} \in C | h) = 1$ se $\hat{x} \in h$, 0 caso contrário), a equação torna-se $p(\hat{x} \in C | D) = \sum_{h: \hat{x} \in h} p(h|D)$. Este processo é chamado de **Bayes model averaging (BMA)** [^11]. A Figura 3.4 ilustra isso: as predições de cada hipótese (representadas por pontos) são ponderadas pelos seus pesos posteriores (curva à direita) para formar a distribuição preditiva geral (gráfico superior) [^11].

Quando temos poucos dados ou dados ambíguos, o *posterior* $p(h|D)$ é vago, resultando numa distribuição preditiva ampla. Conforme aprendemos mais e "descobrimos" o conceito, o *posterior* se concentra, e a distribuição preditiva também [^11].

Uma alternativa mais simples ao BMA é a **aproximação plug-in (plug-in approximation)**, que usa apenas a estimativa MAP (ou MLE) para fazer predições:

$$\np(\hat{x} \in C | D) \approx p(\hat{x} \in C | \hat{h}_{MAP})
\tag{3.9}
$$ [^12]

Embora simples, a aproximação *plug-in* geralmente sub-representa a incerteza e as predições não são tão "suaves" quanto com BMA [^12]. Crucialmente, ela não captura a mudança gradual do raciocínio baseado em similaridade (com *posterior* incerto) para o raciocínio baseado em regras (com *posterior* certo) [^13]. Por exemplo, com $D = \\{16\\}$ e o *prior* simples, $\hat{h}_{MAP}$ poderia ser "potências de 4", levando a predições muito restritas (apenas 4 e 16), um caso de *overfitting*. Com $D = \\{16, 8, 2, 64\\}$, $\hat{h}_{MAP}$ é "potências de dois". A predição *plug-in* começa estreita e é forçada a alargar à medida que mais dados chegam. Em contraste, a abordagem Bayesiana completa (BMA) começa ampla com $D = \\{16\\}$ (muitas hipóteses com suporte posterior não negligenciável) e estreita à medida que o *posterior* se concentra em uma hipótese com $D = \\{16, 8, 2, 64\\}$, o que é mais intuitivo [^13]. As predições das duas abordagens podem ser bem diferentes no regime de pequenas amostras, embora convirjam à medida que mais dados são vistos [^13].

**Um Prior Mais Complexo**

Para modelar o comportamento humano de forma mais realista, Tenenbaum utilizou um *prior* mais sofisticado, derivado de dados experimentais sobre similaridade percebida entre números [^14]. Este *prior* inclui conceitos aritméticos e também todos os intervalos $[n, m]$ para $1 \le n, m \le 100$. O *prior* é uma mistura de um *prior* sobre regras aritméticas e um *prior* sobre intervalos:

$$\np(h) = \pi_0 p_{\text{rules}}(h) + (1 - \pi_0) p_{\text{interval}}(h)
\tag{3.10}
$$ [^14]

O único parâmetro livre é o peso relativo $\pi_0$, geralmente definido como $\pi_0 > 0.5$ para refletir a preferência humana por conceitos definidos por regras. A distribuição preditiva resultante deste modelo (Figura 3.5) é notavelmente similar às distribuições preditivas humanas empíricas (Figura 3.1), mesmo sem ajuste fino aos dados humanos (além da escolha do espaço de hipóteses) [^14].

### Conclusão

O *number game* serve como uma excelente ilustração dos princípios fundamentais da **aprendizagem Bayesiana de conceitos**. Ele demonstra como a inferência Bayesiana pode modelar a aprendizagem a partir de exemplos positivos, integrando **conhecimento prévio (prior)** com a **evidência dos dados (likelihood)** para formar **crenças atualizadas (posterior)** sobre hipóteses concorrentes. O **size principle**, derivado da *strong sampling assumption*, fornece uma justificação formal para a preferência por hipóteses mais simples (Navalha de Occam). Além disso, o quadro Bayesiano, através do **Bayes Model Averaging**, oferece um mecanismo para gerar predições robustas que refletem a incerteza e transitam naturalmente de um raciocínio baseado em similaridade para um raciocínio baseado em regras à medida que mais dados são acumulados, superando limitações das abordagens *plug-in* baseadas em estimativas pontuais como MAP ou MLE. A capacidade de modelar nuances do aprendizado humano com uma estrutura matemática coerente destaca o poder da abordagem Bayesiana para entender a cognição e construir sistemas de inteligência artificial mais eficazes.

### Referências

[^1]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 1, Section 3.2, Paragraph 4.
[^2]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 2, Paragraph 1 & Figure 3.1.
[^3]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 2, Paragraph 2.
[^4]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 3, Paragraph 1.
[^5]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 3, Section 3.2.1.
[^6]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 3, Section 3.2.2 & Page 4, Paragraphs 1-3.
[^7]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 4, Section 3.2.3 & Figures 3.2/3.3.
[^8]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 4, Equations 3.4, 3.5.
[^9]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 5, Equations 3.6, 3.7.
[^10]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 5, Paragraph 2 & Page 6, Paragraph 2.
[^11]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 7, Section 3.2.4, Equation 3.8 & Figure 3.4.
[^12]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 8, Equation 3.9.
[^13]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 8, Paragraph 2.
[^14]: Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. Page 8, Section 3.2.5, Equation 3.10 & Figure 3.5.

<!-- END -->