## Capítulo 3.2.4: A Distribuição Preditiva Posterior e Bayes Model Averaging

### Introdução

Nas seções anteriores, exploramos como a inferência Bayesiana permite atualizar nossas crenças sobre hipóteses subjacentes a partir de dados observados, culminando na distribuição posterior $p(h|D)$ [^12]. Vimos como, no contexto do **Bayesian concept learning** [^2], essa distribuição captura nossa incerteza sobre qual conceito $C$ gerou os exemplos $D = \\{x_1, ..., x_N\\}$. A distribuição posterior $p(h|D)$ representa nosso *internal belief state* sobre o mundo [^18]. No entanto, um objetivo fundamental da modelagem, alinhado com o método científico, é usar essas crenças para fazer previsões sobre novas observações, quantidades objetivamente observáveis [^18]. Esta seção aprofunda-se na **distribuição preditiva posterior** (posterior predictive distribution), o mecanismo Bayesiano para realizar tais previsões, e introduz o conceito de **Bayes Model Averaging (BMA)**.

### Conceitos Fundamentais: A Distribuição Preditiva Posterior

A questão central após observar os dados $D$ é prever se um novo caso de teste $\tilde{x}$ pertence ao conceito $C$. A resposta Bayesiana a esta questão é dada pela **distribuição preditiva posterior**, definida como a probabilidade de $\tilde{x}$ pertencer a $C$ (ou seja, $y=1$ para a função indicadora $f(\tilde{x})$ [^2]) dado o conjunto de dados $D$:

> A distribuição preditiva posterior é dada por:
> $$\
> p(\tilde{x} \in C|D) = \sum_{h \in \mathcal{H}} p(y = 1|\tilde{x}, h)p(h|D)\
> $$ [^19]

Nesta equação, $p(y = 1|\tilde{x}, h)$ representa a predição feita pela hipótese individual $h$ sobre se $\tilde{x}$ pertence ao conceito $C$ definido por $h$. A distribuição preditiva posterior, portanto, calcula a probabilidade de $\tilde{x} \in C$ marginalizando sobre todas as hipóteses possíveis no espaço $\mathcal{H}$ [^5]. Cada hipótese $h$ contribui para a predição final, ponderada pela sua probabilidade posterior $p(h|D)$, que reflete quão plausível a hipótese $h$ é à luz dos dados $D$ e do nosso conhecimento prévio (prior) [^12].

**Bayes Model Averaging (BMA)**

A formulação acima exemplifica um processo conhecido como **Bayes Model Averaging (BMA)** [^20]. Em vez de selecionar uma única hipótese "melhor" (como a estimativa **MAP** [^14] ou **MLE** [^15]) e basear as previsões apenas nela, o BMA considera um *ensemble* de hipóteses.

> *A distribuição preditiva posterior é uma média ponderada das predições de cada hipótese individual, conhecida como Bayes model averaging* [^20].

Este processo de média captura inerentemente a incerteza sobre qual hipótese é a verdadeira. Se a distribuição posterior $p(h|D)$ for vaga (***vague posterior***), indicando considerável incerteza sobre a hipótese correta (como pode ocorrer com conjuntos de dados pequenos ou ambíguos, por exemplo, $D=\\{16\\}$ no number game [^4]), muitas hipóteses terão pesos $p(h|D)$ não negligenciáveis. A média resultante $p(\tilde{x} \in C|D)$ será ampla (***broad predictive distribution***), refletindo essa incerteza nas previsões [^22]. A Figura 3.4 ilustra isso: a distribuição preditiva no topo é obtida pela soma ponderada das predições (pontos) de cada hipótese na base, usando os pesos da posterior $p(h|D)$ mostrados na curva à direita [^21].

Por outro lado, à medida que mais dados são coletados e a posterior $p(h|D)$ se torna mais concentrada em torno de uma única hipótese (***focused posterior***), como a estimativa MAP $\hat{h}_{MAP}$ [^14], a distribuição preditiva também se torna mais concentrada. No limite de dados infinitos, assumindo que o espaço de hipóteses é identificável [^17], a posterior converge para uma função delta na hipótese verdadeira (ou na melhor aproximação dentro de $\mathcal{H}$), e a distribuição preditiva converge para a predição feita por essa hipótese específica [^22].

**Comparação com a Aproximação Plug-in**

Uma alternativa comum ao BMA é a **aproximação plug-in** (plug-in approximation). Nesta abordagem, seleciona-se uma única estimativa pontual da hipótese, tipicamente a estimativa MAP $\hat{h}_{MAP} = \text{argmax}_h p(h|D)$ [^14], e substituímo-la (plug-in) na expressão de predição:

$$\
p(\tilde{x} \in C|D) \approx p(y = 1|\tilde{x}, \hat{h}_{MAP})\
$$ [^23]

Esta abordagem é computacionalmente mais simples, pois evita a soma sobre todas as hipóteses. No entanto, ela tem uma desvantagem significativa: *sub-representa a incerteza* [^23]. Ao basear as previsões numa única hipótese, ignora-se a possibilidade de outras hipóteses serem corretas, mesmo que tenham suporte posterior considerável. Isso pode levar a previsões excessivamente confiantes, especialmente com dados limitados.

O texto contrasta as duas abordagens no contexto da aprendizagem de conceitos [^24]. O BMA permite uma transição gradual do raciocínio baseado em similaridade (quando a posterior é vaga e muitas hipóteses contribuem) para o raciocínio baseado em regras (quando a posterior se concentra numa hipótese específica). Em contraste, a aprendizagem MAP pode sofrer de *overfitting* com dados limitados. Por exemplo, com $D=\\{16\\}$ e um prior simples, a hipótese MAP mínima consistente pode ser "potências de 4", levando a previsões muito restritas (apenas 4 e 16) [^24]. À medida que mais dados chegam (e.g., $D=\\{16, 8, 2, 64\\}$), a hipótese MAP muda para "potências de 2", e a distribuição preditiva plug-in pode alargar-se. O BMA, por outro lado, começa amplo com $D=\\{16\\}$ (pois várias hipóteses são plausíveis) e depois se estreita à medida que a evidência aponta mais claramente para "potências de 2" [^24]. Essa dinâmica do BMA é considerada mais intuitiva [^24]. A diferença entre as previsões Bayesianas (BMA) e a aproximação plug-in MAP é particularmente pronunciada em regimes de pequenas amostras, embora ambas convirjam para a mesma resposta à medida que mais dados são vistos [^24]. Uma ilustração visual dessa diferença também é fornecida posteriormente no contexto do modelo beta-binomial na Figura 3.7 [^55].

### Conclusão

A **distribuição preditiva posterior** é um componente central da inferência e previsão Bayesiana. Através do **Bayes Model Averaging**, ela fornece um mecanismo principialista para gerar previsões sobre dados futuros que levam em conta a incerteza sobre as hipóteses subjacentes, representada pela distribuição posterior $p(h|D)$. Ao ponderar as previsões de todas as hipóteses consideradas pela sua plausibilidade posterior, o BMA oferece uma representação mais robusta e honesta da incerteza preditiva em comparação com as aproximações **plug-in** baseadas em estimativas pontuais como o **MAP**. Essa capacidade de quantificar e propagar a incerteza é especialmente crucial em cenários com dados limitados ou ambíguos, alinhando-se com a natureza gradual da aprendizagem observada em sistemas cognitivos e científicos.

### Referências

[^1]: Seção 2.2.3.2, mencionada na Introdução (p. 65).
[^2]: Seção 3.2, Bayesian concept learning (p. 65).
[^3]: Definição de $p(\tilde{x}|D)$ como a probabilidade de $\tilde{x} \in C$ dado $D$ (p. 66).
[^4]: Figura 3.1, mostrando distribuições preditivas empíricas (p. 66).
[^5]: Introdução do espaço de hipóteses $\mathcal{H}$ (p. 66).
[^6]: Assunção de amostragem forte (strong sampling assumption) (p. 67).
[^7]: Derivação da likelihood $p(D|h)$ (Equação 3.2, p. 67).
[^8]: Princípio do tamanho (size principle) / Navalha de Occam (Occam\'s razor) (p. 67).
[^9]: Comparação de likelihoods para $h_{two}$ e $h_{even}$ (p. 67).
[^10]: Discussão sobre o prior $p(h)$ e sua subjetividade (p. 67-68).
[^11]: Exemplo de prior para o number game (p. 68).
[^12]: Definição da posterior $p(h|D)$ (Equação 3.3, p. 68).
[^13]: Figura 3.2, mostrando prior, likelihood e posterior para $D=\\{16\\}$ (p. 69).
[^14]: Definição da estimativa MAP $\hat{h}_{MAP}$ (Equação 3.4, p. 68).
[^15]: Definição da estimativa MLE $\hat{h}_{mle}$ (Equação 3.7, p. 69) e sua relação com MAP (Equação 3.6, p. 69).
[^16]: Figura 3.3, mostrando prior, likelihood e posterior para $D=\\{16, 8, 2, 64\\}$ (p. 70).
[^17]: Conceitos de consistência e identificabilidade no limite (p. 70).
[^18]: Propósito da distribuição preditiva posterior: testar crenças prevendo quantidades observáveis (p. 71).
[^19]: Fórmula da distribuição preditiva posterior (Equação 3.8, p. 71).
[^20]: Definição de Bayes Model Averaging (BMA) (p. 71).
[^21]: Figura 3.4, ilustrando BMA (p. 71).
[^22]: Relação entre a concentração da posterior e a largura da preditiva (p. 71).
[^23]: Definição da aproximação plug-in (Equação 3.9, p. 72) e sua desvantagem (sub-representação da incerteza) (p. 72).
[^24]: Comparação entre BMA e MAP plug-in, incluindo o exemplo de overfitting e a transição de raciocínio (p. 72).
[^25]: Seção 3.2.5, A more complex prior (p. 72).
[^26]: Seção 3.3, The beta-binomial model (p. 72).
[^27]: Suficientes estatísticas (p. 74).
[^28]: Distribuição Binomial (Equação 3.12, p. 74).
[^29]: Prior conjugado (p. 74).
[^30]: Distribuição Beta (Equação 3.15, p. 74).
[^31]: Hiper-parâmetros (p. 74).
[^32]: Prior não informativo (p. 74).
[^33]: Posterior para Beta-Binomial (Equação 3.16, p. 75).
[^34]: Pseudo-contagens (pseudo counts) (p. 75).
[^35]: Tamanho efetivo da amostra (effective sample size) (p. 75).
[^36]: Figura 3.6, exemplos de atualização da posterior Beta (p. 75).
[^37]: Aprendizagem online (online learning) (p. 75).
[^38]: Média e modo da posterior Beta (Equações 3.21, 3.23, p. 76).
[^39]: Média posterior como combinação convexa (Equação 3.24, p. 76).
[^40]: Variância da posterior Beta (Equação 3.25, p. 76).
[^41]: Aproximação da variância posterior (Equação 3.26, p. 76).
[^42]: Barra de erro (error bar) / desvio padrão posterior (Equação 3.27, p. 76).
[^43]: Incerteza vs valor de $\theta$ (p. 77).
[^44]: Distribuição preditiva posterior para Beta-Binomial (Equações 3.28, 3.29, p. 77).
[^45]: Média da preditiva posterior vs plug-in da média posterior (p. 77).
[^46]: Seção 3.3.4.1, Overfitting and the black swan paradox (p. 77).
[^47]: Problema da contagem zero (zero count problem) / dados esparsos (sparse data problem) (p. 77).
[^48]: Paradoxo do cisne negro (black swan paradox) (p. 77).
[^49]: Problema da indução (induction) (p. 77).
[^50]: Regra de sucessão de Laplace (Laplace\'s rule of succession) (Equação 3.30, p. 77).
[^51]: Suavização add-one (add-one smoothing) (p. 77).
[^52]: Plug-in do MAP não tem o mesmo efeito de suavização (p. 78).
[^53]: Ver [^51].
[^54]: Seção 3.3.4.2, Predicting the outcome of multiple future trials (p. 78), distribuição beta-binomial (Equação 3.34, p. 78).
[^55]: Figura 3.7, comparando predição Bayesiana e plug-in MAP para beta-binomial (p. 79).
[^56]: Seção 3.4, The Dirichlet-multinomial model (p. 78).
[^57]: Likelihood Multinomial (Equação 3.36, p. 79).
[^58]: Prior de Dirichlet (Equação 3.37, p. 79).
[^59]: Posterior de Dirichlet (Equação 3.40, p. 79).
[^60]: Obtenção da posterior pela adição de pseudo-contagens (p. 80).
[^61]: Derivação do MAP para Dirichlet-Multinomial (Equação 3.47, p. 80).
[^62]: Multiplicador de Lagrange (p. 80).
[^63]: MLE para Multinomial (Equação 3.48, p. 80).
[^64]: Seção 3.4.4, Posterior predictive (p. 81).
[^65]: Preditiva posterior para Dirichlet-Multinomial (Equação 3.51, p. 81).
[^66]: Suavização Bayesiana evita problema de contagem zero (p. 81).
[^67]: Seção 3.4.4.1, Worked example: language models using bag of words (p. 81).
[^68]: Modelo bag of words (p. 81).
[^69]: Ver [^67].
[^70]: Ver [^66].
[^71]: Exemplo de predição de palavras com suavização (Equação 3.53, p. 82).
[^72]: Seção 3.5, Naive Bayes classifiers (p. 82).
[^73]: Suposição de independência condicional (p. 82).
[^74]: Modelo Naive Bayes Classifier (NBC) (Equação 3.54, p. 82).
[^75]: Tipos de características (real, binária, categórica) (p. 82-83).
[^76]: Seção 3.5.1, Model fitting (p. 83).
[^77]: Seção 3.5.1.1, MLE for NBC (p. 83).
[^78]: Log-likelihood para NBC (Equação 3.56, p. 83).
[^79]: MLE para prior de classe e parâmetros de característica (Equações 3.57, 3.58, p. 83).
[^80]: Figura 3.8, densidades condicionais de classe para modelo bag-of-words (p. 84).
[^81]: Seção 3.5.1.2, Bayesian naive Bayes (p. 84), prior fatorizado (Equação 3.59, p. 84), posterior fatorizada (Equações 3.60-3.62, p. 85).
[^82]: Seção 3.5.2, Using the model for prediction (p. 85).
[^83]: Predição com NBC (Equação 3.63, p. 85).
[^84]: Procedimento Bayesiano correto: integrar parâmetros (Equações 3.64, 3.65, p. 85).
[^85]: Densidade preditiva posterior via plug-in da média posterior (Equações 3.66-3.68, p. 85).
[^86]: Comparação com plug-in do ML/MAP (Equação 3.69, p. 85).
[^87]: Seção 3.5.3, The log-sum-exp trick (p. 86).
[^88]: Problema de underflow numérico (p. 86).
[^89]: Fórmula do log-sum-exp (Equação 3.74, p. 86).
[^90]: Seção 3.5.4, Feature selection using mutual information (p. 86).
[^91]: Informação Mútua (Mutual Information) (Equação 3.75, p. 87).
[^92]: Cálculo de MI para características binárias (Equação 3.76, p. 87).
[^93]: Seção 3.5.5, Classifying documents using bag of words (p. 87).
[^94]: Modelo produto de Bernoulli / independência binária (Equação 3.77, p. 87). Comparação com modelo Multinomial (Equação 3.78, p. 88).
[^95]: Problema de burstiness (p. 88).
[^96]: Dirichlet Compound Multinomial (DCM) (Equação 3.79, p. 89).
[^97]: Exercício 3.1 (p. 89).
[^98]: Exercício 3.2 (p. 89).
[^99]: Exercício 3.3 (p. 90).
[^100]: Exercício 3.4 (p. 90).
[^101]: Exercício 3.5 (p. 90).
[^102]: Exercício 3.6 (p. 90).
[^103]: Exercício 3.7 (p. 90).
[^104]: Exercício 3.8 (p. 90).
[^105]: Exercício 3.9 (p. 91).
[^106]: Exercício 3.10 (p. 91).
[^107]: Exercício 3.11 (p. 92).
[^108]: Exercício 3.12 (p. 92).
[^109]: Exercício 3.13 (p. 92).
[^110]: Exercício 3.14 (p. 93).
[^111]: Exercício 3.15 (p. 93).
[^112]: Exercício 3.16 (p. 93).
[^113]: Exercício 3.17 (p. 93).
[^114]: Exercício 3.18 (p. 93).
[^115]: Exercício 3.19 (p. 93).
[^116]: Exercício 3.20 (p. 94).
[^117]: Exercício 3.21 (p. 95).
[^118]: Exercício 3.22 (p. 95).

<!-- END -->