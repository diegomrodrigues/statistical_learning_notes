## 3.2.1 Likelihood: Amostragem Forte e o Princípio do Tamanho

### Introdução

Como introduzido na seção anterior sobre **Bayesian concept learning** [^1], a tarefa de aprender o significado de um conceito, como no *number game* [^1], envolve inferir um conceito latente $C$ a partir de exemplos positivos $D = \{x_1, ..., x_N\}$ [^1]. Vimos que, após observar um único exemplo, como $D=\{16\}$, as predições sobre outros números são vagas, refletindo uma alta incerteza sobre o conceito subjacente [^2]. No entanto, com mais exemplos, como $D=\{16, 8, 2, 64\}$, a inferência torna-se mais específica, favorecendo fortemente hipóteses como "potências de dois" [^2]. Surge então uma questão fundamental: por que, dados $D=\{16, 8, 2, 64\}$, preferimos a hipótese $h_{two}$ = "potências de dois" em detrimento de outras hipóteses consistentes, como $h_{even}$ = "números pares" ou $h'$ = "potências de dois exceto 32"? [^3]. A resposta reside na avaliação da **likelihood** de observar os dados $D$ sob cada hipótese $h$, $p(D|h)$. Este componente crucial da inferência Bayesiana permite quantificar a plausibilidade de cada hipótese à luz dos dados observados.

### Conceitos Fundamentais: Likelihood, Amostragem Forte e o Princípio do Tamanho

A intuição central para a seleção de hipóteses é a necessidade de **evitar coincidências suspeitas** (*avoid suspicious coincidences*) [^3]. Se o verdadeiro conceito fosse $h_{even}$ ("números pares"), seria extremamente improvável observar apenas exemplos que, coincidentemente, também são potências de dois, como em $D=\{16, 8, 2, 64\}$ [^3]. Nossa inferência deve favorecer hipóteses que tornem os dados observados o mais típicos e menos coincidentes possível.

Para formalizar essa intuição, adotamos a **strong sampling assumption** (pressuposto de amostragem forte), conforme denominado por Tenenbaum [^3]. Este pressuposto estabelece que os exemplos $D$ são amostrados **uniformemente ao acaso** a partir da **extensão** da hipótese $h$ [^3]. A *extensão* de um conceito é simplesmente o conjunto de todos os elementos (neste caso, números entre 1 e 100 [^2]) que pertencem a esse conceito [^3]. Por exemplo, a extensão de $h_{even}$ é o conjunto $\{2, 4, 6, ..., 98, 100\}$, enquanto a extensão de "números terminados em 9" é $\{9, 19, ..., 99\}$ [^3].

Sob a *strong sampling assumption*, a probabilidade de amostrar (com reposição) $N$ itens independentes, formando o conjunto de dados $D$, a partir da extensão de uma hipótese $h$ é dada por:

$$ p(D|h) = \left[ \frac{1}{size(h)} \right]^N = \frac{1}{|size(h)|^N} \quad (3.2) [^3] $$

onde $|size(h)|$ representa o cardinalidade (tamanho) da extensão da hipótese $h$ [^3]. Esta equação é fundamental e incorpora o que Tenenbaum chama de **size principle** (princípio do tamanho) [^3].

> O **size principle** afirma que o modelo favorece a hipótese mais simples (com menor extensão) que seja consistente com os dados [^3].

Este princípio é uma manifestação quantitativa da navalha de **Occam's razor**¹, que prefere explicações mais simples [^3]. Hipóteses com extensões menores (mais específicas) atribuem maior *likelihood* aos dados consistentes do que hipóteses com extensões maiores (mais gerais).

Vamos ilustrar o funcionamento do *size principle* com o exemplo do *number game* [^3]. Suponha que observamos $D=\{16\}$ ($N=1$).
A hipótese $h_{two}$ = "potências de dois" (menores que 100) tem extensão $\{2, 4, 8, 16, 32, 64\}$, então $|size(h_{two})| = 6$ [^3]. A *likelihood* é $p(D|h_{two}) = (1/6)^1 = 1/6$ [^3].
A hipótese $h_{even}$ = "números pares" (menores que 100) tem extensão $\{2, 4, ..., 100\}$, então $|size(h_{even})| = 50$ [^3]. A *likelihood* é $p(D|h_{even}) = (1/50)^1 = 1/50$ [^3].
Neste caso, a *likelihood* já favorece $h_{two}$ sobre $h_{even}$ [^3].

Agora, considere $D=\{16, 8, 2, 64\}$ ($N=4$). Ambas as hipóteses ainda são consistentes com os dados.
Para $h_{two}$, a *likelihood* é $p(D|h_{two}) = (1/6)^4 = 1/1296 \approx 7.7 \times 10^{-4}$ [^3].
Para $h_{even}$, a *likelihood* é $p(D|h_{even}) = (1/50)^4 = 1/6,250,000 = 1.6 \times 10^{-7}$ [^3].

A diferença agora é substancial. O **likelihood ratio** (razão de verossimilhanças) $p(D|h_{two}) / p(D|h_{even})$ é aproximadamente $ (7.7 \times 10^{-4}) / (1.6 \times 10^{-7}) \approx 4812.5 $, que é quase 5000:1 em favor de $h_{two}$ [^3]. Este valor quantifica nossa intuição inicial: observar $D=\{16, 8, 2, 64\}$ seria uma coincidência muito suspeita se a hipótese verdadeira fosse $h_{even}$ [^3]. O *size principle*, portanto, penaliza fortemente a hipótese $h_{even}$ por sua generalidade excessiva face aos dados específicos observados.

### Conclusão

A função de **likelihood**, $p(D|h)$, desempenha um papel central na avaliação e seleção de hipóteses dentro do paradigma Bayesiano de aprendizagem de conceitos. Derivada da **strong sampling assumption**, que postula uma amostragem uniforme da extensão do conceito, a *likelihood* incorpora naturalmente o **size principle**. Este princípio favorece hipóteses mais simples (com extensões menores) que são consistentes com os dados, fornecendo uma base matemática para a navalha de **Occam's razor**. Ao quantificar quão "coincidente" ou "suspeita" a observação dos dados seria sob diferentes hipóteses, a *likelihood* permite uma comparação rigorosa, como demonstrado pela alta razão de verossimilhanças favorecendo "potências de dois" sobre "números pares" após observar $D=\{16, 8, 2, 64\}$. Contudo, como veremos na próxima seção, a *likelihood* é apenas um dos componentes da inferência Bayesiana; a probabilidade a priori da hipótese, $p(h)$, também é crucial.

### Referências

[^1]: Page 1, Section 3.2 Bayesian concept learning.
[^2]: Page 2, Figure 3.1 and surrounding text.
[^3]: Page 3, Section 3.2.1 Likelihood.

<!-- END -->