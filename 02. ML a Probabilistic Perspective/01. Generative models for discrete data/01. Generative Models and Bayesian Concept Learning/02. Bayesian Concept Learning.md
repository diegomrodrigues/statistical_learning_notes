## Capítulo 3.2: Bayesian Concept Learning como Classificação Binária Probabilística a partir de Exemplos Positivos

### Introdução

Como discutido brevemente na Seção 3.1 sobre modelos generativos para dados discretos [^1], o **Bayesian concept learning** oferece um framework poderoso para modelar como conceitos são adquiridos. Um aspecto fundamental, explorado neste capítulo, é a perspectiva de que aprender o significado de um conceito, como a palavra "cachorro", pode ser formalizado como **concept learning** [^1]. Este processo, por sua vez, é intrinsecamente equivalente à **binary classification** [^1]. O objetivo central é aprender uma **indicator function** $f(x)$, que define os elementos pertencentes a um conceito $C$, tal que $f(x) = 1$ se $x$ pertence a $C$, e $f(x) = 0$ caso contrário [^1]. Uma característica notável desta abordagem é sua capacidade de operar utilizando **apenas exemplos positivos** do conceito, um cenário plausível em muitos contextos de aprendizagem, como o aprendizado infantil de linguagem, onde pais frequentemente apontam exemplos ("olhe o cachorro!") mas raramente fornecem contra-exemplos explícitos ("olhe aquele não-cachorro") [^1]. Pesquisas psicológicas corroboram a ideia de que humanos podem aprender conceitos apenas com exemplos positivos [^1].

Este capítulo detalhará como a estrutura Bayesiana permite essa forma de aprendizado. Ao incorporar incerteza sobre a definição exata da função $f(x)$ — ou, equivalentemente, sobre os elementos exatos do conjunto $C$ — podemos efetivamente emular a **fuzzy set theory** utilizando o cálculo de probabilidade padrão [^1]. Isso contrasta com técnicas padrão de classificação binária, que tipicamente requerem tanto exemplos positivos quanto negativos [^1]. Utilizaremos o **number game**, introduzido por Tenenbaum (1999), como um exemplo pedagógico para ilustrar os mecanismos subjacentes [^1]. Neste jogo, um conceito aritmético $C$ (e.g., "números primos") é escolhido, exemplos positivos $D = \\{x_1, \dots, x_N\\}$ são sorteados de $C$, e o aprendiz deve determinar se um novo caso de teste $\tilde{x}$ pertence a $C$, ou seja, classificar $\tilde{x}$ [^1].

### De Conceitos a Classificação Probabilística

A ligação entre concept learning e classificação binária é estabelecida através da função indicadora $f(x)$ [^1]. Em uma abordagem clássica, aprender o conceito $C$ significaria determinar $f(x)$ de forma definitiva. No entanto, a abordagem Bayesiana introduz uma camada de incerteza probabilística. Em vez de aprender uma única função $f$, mantemos uma distribuição de probabilidade sobre um **hypothesis space** $\mathcal{H}$ de possíveis conceitos [^2]. Cada hipótese $h \in \mathcal{H}$ corresponde a um conceito específico e, portanto, a uma função indicadora específica $f_h(x)$, onde $f_h(x) = 1$ se $x \in h$ e $f_h(x) = 0$ caso contrário.

A incerteza sobre qual hipótese $h$ representa o verdadeiro conceito $C$ é capturada pela distribuição **posterior** $p(h|D)$, calculada após observar os dados $D$ [^7]. Esta incerteza leva a previsões probabilísticas sobre a pertinência de um novo item $\tilde{x}$ ao conceito $C$. A probabilidade de $\tilde{x}$ pertencer a $C$, dado o conjunto de exemplos positivos $D$, é dada pela **posterior predictive distribution** [^2]:

> $$ p(\tilde{x} \in C | D) = p(f(\tilde{x}) = 1 | D) $$

Esta probabilidade é calculada marginalizando sobre todas as hipóteses no espaço $\mathcal{H}$, ponderadas por sua probabilidade posterior. Este processo é conhecido como **Bayes model averaging (BMA)** [^11]:

> $$ p(\tilde{x} \in C | D) = \sum_{h \in \mathcal{H}} p(\tilde{x} \in C | h) p(h|D) = \sum_{h \in \mathcal{H}} I(\tilde{x} \in h) p(h|D) $$
> onde $I(\tilde{x} \in h)$ é 1 se $\tilde{x}$ pertence à extensão da hipótese $h$ e 0 caso contrário. A equação (3.8) no texto original apresenta a forma geral [^11].

O resultado $p(\tilde{x} \in C | D)$ é um valor entre 0 e 1, refletindo o grau de crença de que $\tilde{x}$ é um membro do conceito $C$. É neste sentido que a abordagem Bayesiana emula a *fuzzy set theory*: a pertinência não é necessariamente binária (0 ou 1), mas graduada, embora essa graduação surja diretamente do cálculo de probabilidade padrão, sem necessidade de uma lógica fuzzy separada [^1]. A Figura 3.4 ilustra como a soma ponderada das previsões de hipóteses individuais (pontos na parte inferior, ponderados pela curva $p(h|D)$ à direita) resulta na distribuição preditiva na parte superior [^11].

### Aprendizado Apenas com Exemplos Positivos

Uma questão central é como o framework Bayesiano consegue aprender e generalizar a partir de exemplos exclusivamente positivos [^1]. A resposta reside na interação entre a **likelihood** e o **prior**.\
\nA função de **likelihood**, $p(D|h)$, quantifica a probabilidade de observar os dados $D$ se a hipótese $h$ fosse verdadeira. Sob a **strong sampling assumption** — que postula que os exemplos $D = \\{x_1, \dots, x_N\\}$ são amostrados uniformemente ao acaso da extensão do conceito $h$ — a likelihood é dada por [^4]:

> $$ p(D|h) = \left[ \frac{1}{\text{size}(h)} \right]^N = \left[ \frac{1}{|h|} \right]^N \quad \text{(Eq. 3.2)} $$
> onde $|h|$ é o número de elementos na extensão da hipótese $h$.

Esta formulação incorpora o **size principle**: hipóteses com extensões menores (mais específicas) que são consistentes com os dados recebem uma likelihood maior do que hipóteses com extensões maiores (mais gerais) [^4]. Isso ocorre porque seria uma "coincidência suspeita" observar apenas exemplos que se encaixam em um conceito muito específico se o conceito verdadeiro fosse muito mais amplo [^4]. Por exemplo, ao observar $D = \\{16, 8, 2, 64\\}$, a hipótese $h_{two}$ ("potências de dois") tem $|h_{two}| = 6$ (considerando números até 100), enquanto $h_{even}$ ("números pares") tem $|h_{even}| = 50$. A likelihood $p(D|h_{two}) = (1/6)^4$ é muito maior que $p(D|h_{even}) = (1/50)^4$, favorecendo fortemente $h_{two}$ com base apenas nos exemplos positivos [^4]. Este princípio é uma manifestação do **Occam\'s razor** [^4].

A **prior probability**, $p(h)$, captura crenças prévias sobre a plausibilidade das hipóteses antes de observar os dados [^5]. Ela permite incorporar conhecimento de fundo ou um viés indutivo para conceitos "naturais" ou simples, atribuindo baixa probabilidade a hipóteses consideradas "conceptualmente não naturais", como "potências de dois exceto 32" [^5]. Embora a **subjetividade** dos priors seja controversa, eles são um mecanismo crucial pelo qual o conhecimento prévio pode ser trazido para o problema, permitindo aprendizado rápido a partir de amostras pequenas [^6]. A Figura 3.2(a) mostra um exemplo de prior que favorece conceitos aritméticos simples [^6].

A distribuição **posterior**, $p(h|D)$, combina a likelihood e o prior através da regra de Bayes [^7]:

> $$ p(h|D) = \frac{p(D|h)p(h)}{\sum_{h\' \in \mathcal{H}} p(D|h\')p(h\')} \propto p(D|h)p(h) \quad \text{(Eq. 3.3)} $$
> A condição $I(D \subseteq h)$ é implicitamente necessária, pois $p(D|h)=0$ se $D$ não estiver contido em $h$ [^7].

O posterior representa a crença atualizada sobre cada hipótese $h$ após considerar os exemplos positivos $D$. As Figuras 3.2 e 3.3 mostram como o posterior evolui, combinando o prior e a likelihood, e como ele pode se concentrar fortemente em uma única hipótese (o "aha moment") quando os dados são suficientemente informativos [^7, ^9].

### A Perspectiva da Classificação

Retornando à tarefa de classificação, o objetivo é decidir se um novo item $\tilde{x}$ pertence ao conceito $C$ (classe 1) ou não (classe 0) [^1]. O framework Bayesiano fornece diretamente a probabilidade necessária para esta decisão: $p(\tilde{x} \in C | D)$ [^11].

Uma regra de decisão simples seria classificar $\tilde{x}$ como pertencente a $C$ se $p(\tilde{x} \in C | D) > 0.5$. No entanto, a saída do modelo é inerentemente probabilística, refletindo a incerteza residual sobre o verdadeiro conceito $C$. Esta incerteza é maior quando os dados $D$ são escassos ou ambíguos, resultando em uma distribuição preditiva ampla (como na Figura 3.1, topo, para $D=\\{16\\}$) [^2, ^11]. À medida que mais dados são observados e o posterior $p(h|D)$ se concentra em poucas (ou uma) hipóteses (como na Figura 3.3 para $D=\\{16, 8, 2, 64\\}$), a distribuição preditiva torna-se mais nítida [^7, ^9, ^11].

É instrutivo contrastar a predição via **Bayes model averaging (BMA)** com a **plug-in approximation** [^12]. A aproximação plug-in utiliza uma estimativa pontual da hipótese, tipicamente a estimativa **Maximum A Posteriori (MAP)** $\hat{h}_{MAP} = \text{argmax}_h p(h|D)$ [^7], e faz predições como se essa fosse a hipótese verdadeira:

> $$ p(\tilde{x} \in C | D) \approx p(\tilde{x} \in C | \hat{h}_{MAP}) = I(\tilde{x} \in \hat{h}_{MAP}) \quad \text{(Eq. 3.9)} $$

Embora mais simples, a aproximação plug-in ignora a incerteza sobre as hipóteses, o que pode levar a previsões excessivamente confiantes e comportamento menos intuitivo, especialmente em regimes de poucos dados [^12]. O BMA, ao promediar sobre todas as hipóteses, fornece uma representação mais robusta e fiel da incerteza do modelo [^11]. Com dados suficientes, o posterior $p(h|D)$ tende a se concentrar em torno da hipótese MAP (ou **MLE** se o prior for dominado pela likelihood), e as previsões de BMA e plug-in convergem [^7, ^8, ^10, ^12].

### Conclusão

Este capítulo demonstrou a equivalência formal entre **Bayesian concept learning** a partir de exemplos positivos e **binary classification** probabilística. A estrutura Bayesiana aborda o desafio de aprender sem exemplos negativos através da interação da função de **likelihood**, que implementa o **size principle** [^4], e da distribuição **prior**, que codifica vieses indutivos [^5, ^6]. O resultado é uma distribuição **posterior** sobre um espaço de hipóteses [^7], que por sua vez induz uma **posterior predictive distribution** [^11]. Esta distribuição preditiva $p(\tilde{x} \in C | D)$ fornece uma medida probabilística da pertinência de um novo item $\tilde{x}$ ao conceito $C$, funcionando efetivamente como a saída de um classificador binário que naturalmente lida com incerteza e emula características da **fuzzy set theory** usando o cálculo de probabilidade padrão [^1]. Este framework não apenas fornece uma solução computacional elegante, mas também oferece insights sobre como a aprendizagem de conceitos pode ocorrer em sistemas naturais, como humanos [^1].

### Referências

[^1]: Página 1, Seção 3.2
[^2]: Página 2
[^3]: Página 3, início
[^4]: Página 3, Seção 3.2.1
[^5]: Página 3, Seção 3.2.2
[^6]: Página 4, início
[^7]: Página 4, Seção 3.2.3
[^8]: Página 5
[^9]: Página 6, Figura 3.3
[^10]: Página 6, final
[^11]: Página 7, Seção 3.2.4
[^12]: Página 8, início
[^13]: Página 8, Seção 3.2.5
[^14]: Página 9, Figura 3.5

<!-- END -->