## Capítulo 3.2.4.1: Aproximação Plug-in versus Bayesian Model Averaging na Previsão Preditiva Posterior

### Introdução

Como estabelecido na Seção 3.2 [^2], um objetivo central da aprendizagem de conceitos Bayesiana é prever se um novo caso de teste $\tilde{x}$ pertence a um conceito $C$, dados os exemplos observados $D = \{x_1, \dots, x_N\}$. A ferramenta formal para esta previsão é a **distribuição preditiva posterior**, $p(\tilde{x} \in C|D)$ [^5]. Conforme detalhado na Seção 3.2.4, esta distribuição é obtida marginalizando sobre a incerteza acerca da hipótese correta $h$, representada pela **distribuição posterior** $p(h|D)$ [^16, ^17], calculada como na Equação 3.3 [^17]. Este processo de média ponderada sobre todas as hipóteses é conhecido como **Bayesian Model Averaging (BMA)** [^26].

No entanto, calcular a soma completa exigida pelo BMA pode ser computacionalmente intensivo, especialmente com um grande espaço de hipóteses $\mathcal{H}$ [^7]. Uma alternativa comum e mais simples é a **aproximação plug-in**, que utiliza uma única estimativa pontual da hipótese, tipicamente a hipótese **Maximum A Posteriori (MAP)**, $h_{MAP}$ [^19]. Este capítulo aprofunda a comparação entre a aproximação plug-in baseada em MAP e a abordagem completa de BMA, contrastando suas propriedades, implicações para a representação da incerteza e sua capacidade de explicar dinâmicas de aprendizagem observadas, com base nas discussões apresentadas anteriormente neste capítulo.

### Conceitos Fundamentais

#### Bayesian Model Averaging (BMA)

A distribuição preditiva posterior, conforme definida na Equação 3.8 [^25], é calculada através do BMA:
$$ p(\tilde{x} \in C|D) = \sum_{h \in \mathcal{H}} p(y=1|\tilde{x}, h) p(h|D) $$
Esta formulação representa uma média das previsões $p(y=1|\tilde{x}, h)$ de cada hipótese individual $h$, ponderada pela sua respetiva probabilidade posterior $p(h|D)$ [^27]. Uma característica fundamental do BMA é que ele **incorpora integralmente a incerteza posterior** sobre qual hipótese $h$ é a verdadeira geradora dos dados.

Esta abordagem alinha-se intuitivamente com o processo de aprendizagem. *Quando temos um conjunto de dados pequeno e/ou ambíguo, a posterior $p(h|D)$ é vaga, distribuída por muitas hipóteses* [^28]. Consequentemente, o BMA produz uma distribuição preditiva ampla, refletindo a incerteza e capturando um raciocínio baseado em similaridade difusa, como observado na Figura 3.1 (primeiras duas linhas) [^6] e ilustrado na Figura 3.4 [^27]. À medida que mais dados são observados, a posterior tende a concentrar-se numa ou em poucas hipóteses [^28], como visto na Figura 3.3 [^18]. Neste caso, o BMA produz uma distribuição preditiva mais estreita, refletindo maior certeza e um raciocínio baseado em regras [^6] (Figura 3.1, terceira linha). Crucialmente, esta característica permite ao BMA explicar a **mudança gradual** do raciocínio baseado em similaridade para o raciocínio baseado em regras à medida que a aprendizagem progride [^31].

#### A Aproximação Plug-in via MAP

Em contraste com o BMA, a aproximação plug-in simplifica drasticamente o cálculo da distribuição preditiva posterior. Ela baseia-se na seleção de uma única hipótese "melhor", geralmente a estimativa MAP, $h_{MAP} = \text{argmax}_h p(h|D)$ [^19], e utiliza as previsões *apenas* dessa hipótese. A distribuição preditiva plug-in é, portanto, aproximada como [^29]:
$$ p(\tilde{x} \in C|D) \approx p(\tilde{x} \in C | h_{MAP}) $$
A principal vantagem desta abordagem é a sua **simplicidade computacional** [^30, ^31]. Em vez de somar sobre todo o espaço de hipóteses $\mathcal{H}$, necessita apenas de encontrar o modo da distribuição posterior (via Equação 3.6 [^22], por exemplo) e usar essa hipótese para fazer previsões.

No entanto, esta simplicidade tem um custo significativo:

> *[...] a plug-in approximation [...] under-represents our uncertainty, and our predictions will not be as “smooth” as when using BMA.* [^30]

Ao descartar todas as hipóteses exceto $h_{MAP}$, a aproximação plug-in ignora a incerteza residual representada pela massa de probabilidade posterior atribuída a outras hipóteses. Esta sub-representação da incerteza é particularmente problemática quando os dados são limitados e a posterior $p(h|D)$ ainda não está fortemente concentrada. Pode levar a previsões excessivamente confiantes e a um comportamento menos robusto, semelhante aos problemas de *overfitting* e ao *paradoxo do cisne negro* discutidos no contexto do MLE no modelo beta-binomial (Seção 3.3.4.1 [^41]). A Figura 3.7(b) [^44] ilustra como uma aproximação plug-in (usando MAP, que é próximo do MLE com prior fraco ou muitos dados) pode ter caudas mais curtas do que a previsão Bayesiana completa (Figura 3.7(a) [^44]), refletindo menor dispersão de probabilidade.

Além disso, a aprendizagem MAP, sendo simples, *não consegue explicar a mudança gradual do raciocínio baseado em similaridade para o baseado em regras* [^31]. A transição nas previsões tende a ser mais abrupta, ocorrendo quando a identidade da $h_{MAP}$ muda. Ela não captura naturalmente a fase inicial de raciocínio amplo e baseado em similaridade que surge da incerteza sobre múltiplas hipóteses plausíveis. O comportamento contrastante descrito é que a abordagem MAP/plug-in *começa estreita* (baseada na $h_{MAP}$ inicial, que pode ser resultado de overfitting com poucos dados, como o exemplo "all powers of 4" para D={16} [^31]) e é *forçada a alargar* se dados subsequentes contradisserem fortemente a $h_{MAP}$ inicial, enquanto a abordagem Bayesiana (BMA) *começa ampla e depois estreita à medida que aprende mais*, o que faz mais sentido intuitivo [^31].

#### Comparação e Convergência

A Tabela abaixo resume as principais diferenças discutidas:

| Característica             | Bayesian Model Averaging (BMA)                     | Aproximação Plug-in (MAP)                       |
| :------------------------- | :------------------------------------------------- | :---------------------------------------------- |
| **Incerteza Posterior**  | Totalmente representada [^26]                      | Sub-representada [^30]                          |
| **Complexidade**         | Alta (soma sobre $\mathcal{H}$)                    | Baixa (requer $h_{MAP}$) [^31]                    |
| **Explicação da Aprendizagem** | Captura mudança gradual (similaridade→regra) [^31] | Não captura mudança gradual [^31]               |
| **Comportamento Inicial**  | Amplo, reflete incerteza [^31]                     | Estreito, pode sofrer overfitting inicial [^31] |
| **Suavidade Previsões**  | Mais suave [^30]                                   | Menos suave / mais abrupta [^30]                |

Apesar destas diferenças significativas, especialmente no regime de dados pequenos ou ambíguos, as duas abordagens convergem na presença de dados suficientes [^32]. À medida que $N$ aumenta, a distribuição posterior $p(h|D)$ torna-se cada vez mais concentrada em torno de uma única hipótese, que é a $h_{MAP}$ (assumindo identificabilidade [^24]), aproximando-se de uma medida de Dirac $\delta_{h_{MAP}}(h)$ [^21]. Neste limite, a soma no BMA (Equação 3.8 [^25]) é dominada pelo termo correspondente a $h_{MAP}$, e a distribuição preditiva do BMA torna-se virtualmente idêntica à da aproximação plug-in (Equação 3.9 [^29]):
$$ \lim_{N \to \infty} \sum_{h \in \mathcal{H}} p(y=1|\tilde{x}, h) p(h|D) = p(y=1|\tilde{x}, h_{MAP}) $$
Isto justifica o uso da aproximação plug-in em cenários com grandes volumes de dados onde a incerteza sobre a hipótese é mínima e a eficiência computacional é prioritária. No entanto, para muitos problemas de interesse, especialmente aqueles que mimetizam a aprendizagem humana a partir de poucos exemplos [^4], a capacidade do BMA de lidar com a incerteza é crucial.

### Conclusão

A escolha entre **Bayesian Model Averaging (BMA)** [^26] e a **aproximação plug-in** baseada na estimativa **MAP** [^19, ^29] representa um compromisso fundamental na aplicação de modelos generativos Bayesianos para previsão. O BMA oferece uma abordagem teoricamente completa, integrando toda a incerteza posterior sobre as hipóteses [^27], o que resulta em previsões mais robustas e suaves [^30], e alinha-se melhor com as dinâmicas graduais observadas na aprendizagem humana [^31]. Por outro lado, a aproximação plug-in oferece uma alternativa computacionalmente mais simples [^31], mas fá-lo ao custo de sub-representar a incerteza [^30], o que pode ser problemático com dados limitados. Embora ambas as abordagens convirjam com dados suficientes [^32], a superioridade teórica e prática do BMA em regimes de alta incerteza sublinha a importância de considerar a distribuição posterior completa ao fazer previsões no quadro Bayesiano explorado neste capítulo. A compreensão destas nuances é essencial para a aplicação criteriosa de modelos Bayesianos em problemas de aprendizagem de conceitos e classificação.

### Referências

[^1]: Page 1, Equation 3.1: $p(y = c|\mathbf{x}, \theta) \propto p(\mathbf{x}|y = c, \theta)p(y = c|\theta)$
[^2]: Page 1, Section 3.2: Bayesian concept learning introduction.
[^3]: Page 1, Section 3.2: Concept learning as binary classification.
[^4]: Page 1, Section 3.2: Learning from positive examples alone.
[^5]: Page 2, Definition of posterior predictive distribution $p(\tilde{x} \in C|D)$.
[^6]: Page 2, Discussion of similarity vs. rule-based reasoning and Figure 3.1.
[^7]: Page 2, Introduction of hypothesis space $\mathcal{H}$.
[^8]: Page 3, Section 3.2.1: Avoiding suspicious coincidences.
[^9]: Page 3, Section 3.2.1: Strong sampling assumption.
[^10]: Page 3, Equation 3.2: Likelihood $p(D|h) = [1/\text{size}(h)]^N$.
[^11]: Page 3, Section 3.2.1: Size principle / Occam\'s razor.
[^12]: Page 3, Section 3.2.1: Likelihood example $h_{two}$ vs $h_{even}$.
[^13]: Page 3, Section 3.2.2: Prior probability $p(h)$ for conceptual naturalness.
[^14]: Page 4, Section 3.2.2: Subjectivity and utility of the prior.
[^15]: Page 4, Section 3.2.2: Example priors.
[^16]: Page 4, Section 3.2.3: Definition of posterior probability $p(h|D)$.
[^17]: Page 4, Equation 3.3: Posterior formula $p(h|D) \propto p(h)I(D \subseteq h)/|h|^N$.
[^18]: Page 4, Section 3.2.3: Posterior examples Figures 3.2 and 3.3.
[^19]: Page 4, Section 3.2.3: Definition of MAP estimate $h_{MAP}$.
[^20]: Page 4, Equation 3.5: Dirac measure $\delta$.
[^21]: Page 4, Equation 3.4: Posterior converges to Dirac measure at MAP.
[^22]: Page 5, Equation 3.6: MAP as argmax of log posterior.
[^23]: Page 5, Section 3.2.3: Convergence of MAP to MLE, data overwhelms prior.
[^24]: Page 6, Section 3.2.3: Consistency and identifiability in the limit.
[^25]: Page 7, Equation 3.8: Posterior predictive distribution formula.
[^26]: Page 7, Section 3.2.4: Naming as Bayes Model Averaging (BMA).
[^27]: Page 7, Section 3.2.4: Explanation as weighted average and Figure 3.4.
[^28]: Page 7, Section 3.2.4: Effect of vague vs. peaked posterior on predictive distribution.
[^29]: Page 8, Equation 3.9: Plug-in approximation definition $p(\tilde{x}|h_{MAP})$.
[^30]: Page 8, Section 3.2.4: Plug-in under-represents uncertainty, less smooth than BMA.
[^31]: Page 8, Section 3.2.4: MAP simplicity, inability to explain gradual shift, contrast with Bayesian approach (broad-to-narrow).
[^32]: Page 8, Section 3.2.4: Convergence of plug-in and BMA with sufficient data.
[^33]: Page 8, Section 3.2.5: More complex prior.
[^34]: Page 8, Section 3.2.5: Comparison of model predictions (Fig 3.5) with human data (Fig 3.1).
[^35]: Page 8-9, Section 3.3: Beta-binomial model introduction.
[^36]: Page 10, Definition of sufficient statistics.
[^37]: Page 10, Definition of conjugate prior.
[^38]: Page 11, Definitions of hyper-parameters, pseudo counts, effective sample size.
[^39]: Page 11, Mention of online learning suitability.
[^40]: Page 12, Equation 3.24: Posterior mean as convex combination.
[^41]: Page 13, Section 3.3.4.1: Overfitting, zero count problem, black swan paradox.
[^42]: Page 13, Equation 3.30: Laplace\'s rule of succession / add-one smoothing.
[^43]: Page 14, Equation 3.34: Beta-binomial distribution.
[^44]: Page 14, Section 3.3.4.2 and Figure 3.7: Comparison of Bayesian prediction vs plug-in MAP.
[^45]: Page 14, Section 3.4: Dirichlet-multinomial model introduction.
[^46]: Page 16, Equation 3.47: MAP estimate for Dirichlet-multinomial.
[^47]: Page 17, Section 3.4.4: Posterior predictive avoids zero-count problem.
[^48]: Page 17, Section 3.4.4.1: Bag of words model.
[^49]: Page 18, Equation 3.54: Naive Bayes conditional independence assumption.
[^50]: Page 19, Section 3.5.1: Model fitting for NBC (MLE/MAP).
[^51]: Page 20, Equation 3.59: Factored prior for Bayesian NBC.
[^52]: Page 21, Section 3.5.2: Prediction using posterior predictive (Eq 3.66) vs plug-in (Eq 3.69).
[^53]: Page 22, Section 3.5.3: Log-sum-exp trick.
[^54]: Page 22-23, Section 3.5.4: Feature selection using mutual information.
[^55]: Page 23-24, Section 3.5.5: Bernoulli vs Multinomial models for document classification.
[^56]: Page 24, Section 3.5.5: Burstiness problem.
[^57]: Page 25, Equation 3.79: Dirichlet Compound Multinomial (DCM) / Polya Urn model.

<!-- END -->