## Capítulo 3: Classificadores Generativos Baseados na Regra de Bayes para Dados Discretos

### Introdução

Como discutido brevemente na Seção 2.2.3.2 do material de referência, uma abordagem fundamental para a classificação de vetores de características $x$ reside na aplicação da regra de Bayes através de um classificador generativo [^1]. A forma geral deste classificador é expressa como:

$$ p(y = c|x, \theta) \propto p(x|y = c, \theta)p(y = c|\theta) \quad (3.1) [^1] $$

Aqui, $y$ representa a variável de classe que desejamos prever, $c$ é uma classe específica, $x$ é o vetor de características observado, e $\theta$ denota o conjunto de parâmetros desconhecidos do modelo. A essência desta abordagem reside em modelar explicitamente como os dados são gerados. Isso envolve dois componentes cruciais: o **prior de classe** $p(y = c|\theta)$, que reflete a probabilidade *a priori* de uma instância pertencer à classe $c$, e a **densidade condicional de classe** $p(x|y = c, \theta)$, que define a distribuição dos dados que esperamos observar para cada classe específica [^2].

A chave para a utilização eficaz de tais modelos generativos é a especificação de uma forma funcional adequada para a densidade condicional de classe $p(x|y = c, \theta)$ [^2]. Esta escolha determina *que tipo de dados esperamos ver em cada classe* [^2]. Subsequentemente, torna-se necessário inferir os parâmetros desconhecidos $\theta$ a partir dos dados observados [^2]. Este capítulo aprofunda-se nesses modelos, com foco particular no cenário onde os dados observados $x$ consistem em **símbolos discretos** [^2], um caso prevalente em muitas aplicações, como processamento de linguagem natural e bioinformática [^50]. Discutiremos como especificar $p(x|y=c, \theta)$ para dados discretos e como realizar a inferência dos parâmetros $\theta$ neste contexto, construindo sobre os princípios da inferência Bayesiana e da estimação de máxima verossimilhança.

### Conceitos Fundamentais

#### A Densidade Condicional de Classe $p(x|y=c, \theta)$ para Dados Discretos

O coração de um classificador generativo é a densidade condicional de classe, $p(x|y=c, \theta)$, que modela a distribuição dos dados dentro de cada classe. Especificar esta distribuição de forma adequada é crucial, mas pode ser desafiador, especialmente quando o vetor de características $x$ é de alta dimensionalidade.

Uma simplificação comum e poderosa é a **suposição Naive Bayes**, que assume que as características $x_j$ (componentes do vetor $x = (x_1, ..., x_D)$) são **condicionalmente independentes** dada a classe $y=c$ [^64]. Sob esta suposição, a densidade condicional de classe pode ser fatorada como um produto de densidades unidimensionais:

$$ p(x|y = c, \theta) = \prod_{j=1}^D p(x_j|y = c, \theta_{jc}) \quad (3.54) [^64] $$

onde $\theta_{jc}$ são os parâmetros associados à distribuição da característica $j$ para a classe $c$. Este modelo é denominado **Naive Bayes Classifier (NBC)** [^64]. A denominação "naive" (ingênua) advém do fato de que a suposição de independência condicional raramente se sustenta na prática [^65]. No entanto, os classificadores NBC frequentemente apresentam bom desempenho, em parte devido à sua simplicidade [^65]. O modelo possui um número relativamente pequeno de parâmetros, da ordem de $O(CD)$ para $C$ classes e $D$ características, tornando-o menos propenso a *overfitting* [^65].

A forma específica de $p(x_j|y=c, \theta_{jc})$ depende da natureza da característica $j$. Para dados discretos, as seguintes modelagens são comuns:

1.  **Características Binárias:** Se $x_j \in \{0, 1\}$, podemos usar a distribuição de Bernoulli: $p(x_j|y=c, \theta_{jc}) = Ber(x_j|\theta_{jc})$, onde $\theta_{jc}$ é a probabilidade da característica $j$ ocorrer (ter valor 1) na classe $c$ [^67]. Este modelo é por vezes referido como **multivariate Bernoulli naive Bayes model** [^67].
2.  **Características Categóricas:** Se $x_j \in \{1, ..., K\}$, onde $K$ é o número de valores possíveis, podemos usar a distribuição Categórica (ou Multinoulli): $p(x_j|y=c, \theta_{jc}) = Cat(x_j|\theta_{jc})$, onde $\theta_{jc}$ é um vetor de probabilidades (um histograma) sobre os $K$ valores possíveis para a característica $j$ na classe $c$ [^68].

No contexto de classificação de documentos usando a representação **bag of words** [^60], onde a ordem das palavras é ignorada e apenas suas ocorrências são contadas, duas abordagens principais para $p(x|y=c, \theta)$ emergem:

*   **Modelo de Produto Bernoulli (Binary Independence Model):** Representa cada documento $x_i$ como um vetor binário indicando a presença ou ausência de cada palavra do vocabulário [^90]. A densidade condicional de classe é então modelada como um produto de distribuições de Bernoulli independentes para cada palavra, como no caso de características binárias acima [^90].
*   **Modelo Multinomial:** Representa cada documento $x_i$ como um vetor de contagens $x_{ij}$ (número de ocorrências da palavra $j$ no documento $i$), com $N_i = \sum_j x_{ij}$ sendo o número total de palavras no documento $i$ [^91]. A densidade condicional de classe é modelada usando a distribuição Multinomial:
    $$     p(x_i|y_i = c, \theta_c) = Mu(x_i|N_i, \theta_c) = \frac{N_i!}{\prod_{j=1}^D x_{ij}!} \prod_{j=1}^D \theta_{jc}^{x_{ij}} \quad (3.78) [^91]     $$
    Aqui, $\theta_{jc}$ é a probabilidade de gerar a palavra $j$ em documentos da classe $c$, com $\sum_{j=1}^D \theta_{jc} = 1$ [^91]. Embora fácil de treinar e usar, o modelo Multinomial não captura a **burstiness** (explosividade) do uso de palavras – o fenômeno onde palavras, se aparecem uma vez em um documento, são propensas a aparecer mais vezes [^92].
*   **Modelo Dirichlet Compound Multinomial (DCM):** Uma alternativa mais sofisticada que substitui a densidade Multinomial pela densidade DCM (também conhecida como modelo Polya Urn) [^93]:
    $$     p(x_i|y_i = c, \alpha_c) = \int Mu(x_i|N_i, \theta_c) Dir(\theta_c|\alpha_c) d\theta_c = \frac{N_i!}{\prod_{j=1}^D x_{ij}!} \frac{B(x_i + \alpha_c)}{B(\alpha_c)} \quad (3.79) [^93]     $$
    onde $B(\cdot)$ é a função Beta multinomial e $\alpha_c$ são os parâmetros (pseudo-contagens) do prior Dirichlet. Este modelo captura a *burstiness* e frequentemente apresenta melhor desempenho [^94]. A intuição é que, após observar uma ocorrência da palavra $j$, a contagem posterior para $\theta_{jc}$ é atualizada, tornando mais provável a ocorrência subsequente da mesma palavra [^94].

#### O Prior de Classe $p(y=c|\theta)$

O prior de classe $p(y=c|\theta)$ representa nossa crença inicial sobre a frequência relativa das classes, antes de observarmos o vetor de características $x$. Geralmente, assume-se que a classe $y$ segue uma distribuição Categórica parametrizada por um vetor $\pi = (\pi_1, ..., \pi_C)$, onde $\pi_c = p(y=c|\pi)$ e $\sum_{c=1}^C \pi_c = 1$. A estimação de $\pi$ é discutida abaixo.

#### Inferência de Parâmetros $\theta$

Os parâmetros $\theta$ do modelo generativo, que tipicamente incluem os parâmetros do prior de classe ($\pi$) e das densidades condicionais de classe ($\theta_{jc}$), precisam ser inferidos a partir de um conjunto de dados de treinamento $D = \{(x_1, y_1), ..., (x_N, y_N)\}$. Duas abordagens principais são a Estimação de Máxima Verossimilhança (MLE) e a Inferência Bayesiana.

**Maximum Likelihood Estimation (MLE):**

A abordagem MLE visa encontrar os valores dos parâmetros $\theta$ que maximizam a probabilidade (verossimilhança) dos dados observados $D$: $\hat{\theta}_{MLE} = \arg\max_\theta p(D|\theta)$. Para classificadores Naive Bayes, a log-verossimilhança geralmente se decompõe em termos independentes para os parâmetros do prior de classe e para os parâmetros de cada característica dentro de cada classe [^69], facilitando a otimização.

*   **Prior de Classe:** O MLE para $\pi_c$ é simplesmente a frequência relativa da classe $c$ nos dados de treinamento:
    $$     \hat{\pi}_c = \frac{N_c}{N} \quad (3.57) [^70]     $$
    onde $N_c = \sum_{i=1}^N I(y_i = c)$ é o número de exemplos da classe $c$ e $N$ é o número total de exemplos [^70].
*   **Densidades Condicionais de Classe (Exemplo Bernoulli):** Para características binárias modeladas por $Ber(\theta_{jc})$, o MLE é a frequência relativa da característica $j$ ocorrendo (sendo 1) dentro dos exemplos da classe $c$:
    $$     \hat{\theta}_{jc} = \frac{N_{jc}}{N_c} \quad (3.58) [^71]     $$
    onde $N_{jc} = \sum_{i:y_i=c} I(x_{ij} = 1)$ é o número de vezes que a característica $j$ é 1 nos exemplos da classe $c$ [^71].

> Uma limitação significativa do MLE é sua tendência a sofrer de *overfitting*, especialmente com dados esparsos. Um problema crítico é o **zero count problem** (ou **sparse data problem**): se um determinado valor de característica (e.g., uma palavra específica) nunca foi observado para uma classe $c$ nos dados de treinamento, o MLE para sua probabilidade será zero ($\hat{\theta}_{jc} = 0$) [^45, 74]. Isso leva à previsão de probabilidade zero para qualquer nova instância que contenha essa característica, independentemente das outras características, um fenômeno relacionado ao **black swan paradox** [^45, 74].

**Inferência Bayesiana:**

A abordagem Bayesiana trata os parâmetros $\theta$ como variáveis aleatórias, atribuindo-lhes distribuições *a priori* $p(\theta)$ que refletem nossas crenças iniciais. A inferência consiste em calcular a distribuição *a posteriori* $p(\theta|D)$ usando a regra de Bayes: $p(\theta|D) \propto p(D|\theta)p(\theta)$.

*   **Priors Conjugados:** Para simplificar os cálculos, frequentemente utilizamos **priors conjugados**, que são famílias de distribuições *a priori* tais que a distribuição *a posteriori* resultante pertence à mesma família [^29].
    *   Para a probabilidade $\theta$ de um modelo Bernoulli (características binárias), o prior conjugado é a distribuição **Beta**, Beta$(\theta|a, b) \propto \theta^{a-1}(1-\theta)^{b-1}$ [^30]. Os parâmetros $a$ e $b$ são chamados **hiperparâmetros** [^31] e podem ser interpretados como **pseudo counts** (contagens fictícias) [^34]. A posteriori, após observar $N_1$ sucessos e $N_0$ falhas, é Beta$(\theta|a+N_1, b+N_0)$ [^33]. A soma $a+b$ é a **effective sample size** do prior [^34]. Um prior uniforme Beta(1,1) corresponde a adicionar uma pseudo-contagem a cada resultado [^32].
    *   Para o vetor de probabilidades $\theta$ de um modelo Categórico/Multinomial (prior de classe ou características categóricas), o prior conjugado é a distribuição **Dirichlet**, Dir$(\theta|\alpha) \propto \prod_{k=1}^K \theta_k^{\alpha_k-1}$ [^53, 54]. Os hiperparâmetros $\alpha = (\alpha_1, ..., \alpha_K)$ atuam como pseudo-contagens [^55]. A posteriori, após observar contagens $N = (N_1, ..., N_K)$, é Dir$(\theta|\alpha+N)$ [^55]. Um prior Dirichlet com $\alpha_k=1$ para todo $k$ é um prior uniforme sobre o simplex [^57].
*   **Priors Fatorados para NBC:** No Naive Bayes, é comum usar um prior fatorado que reflete a estrutura do modelo:
    $$     p(\theta) = p(\pi) \prod_{j=1}^D \prod_{c=1}^C p(\theta_{jc}) \quad (3.59) [^75]     $$
    onde $p(\pi)$ é tipicamente um prior Dirichlet e $p(\theta_{jc})$ é um prior apropriado para o tipo de característica (e.g., Beta para Bernoulli) [^76]. A posteriori também será fatorada [^60].
*   **Smoothing Bayesiano:** Uma consequência direta do uso de priors (especialmente com pseudo-contagens positivas, como em Beta(1,1) ou Dir(1,...,1)) é que as estimativas de probabilidade *a posteriori* nunca são zero, mesmo para eventos não observados no treinamento. Isso resolve o *zero count problem* e é conhecido como **smoothing** (suavização) Bayesiano [^59]. A **add-one smoothing** (ou **Laplace smoothing**) é um caso particular que surge de priors uniformes (Beta(1,1) ou Dir(1,...,1)) [^46, 76].
*   **Maximum A Posteriori (MAP) Estimation:** Uma alternativa à computação da posteriori completa é encontrar o modo da distribuição *a posteriori*, conhecido como estimativa MAP: $\hat{\theta}_{MAP} = \arg\max_\theta p(\theta|D) = \arg\max_\theta [\log p(D|\theta) + \log p(\theta)]$ [^14, 16]. A estimativa MAP incorpora a influência do prior, tendendo a ser mais robusta que o MLE com poucos dados. Para grandes volumes de dados, a verossimilhança domina o prior, e a estimativa MAP converge para a estimativa MLE [^17]. As fórmulas para MAP com priors Beta e Dirichlet são:
    *   Beta Posterior: $\hat{\theta}_{MAP} = (a+N_1-1) / (a+b+N-2)$ [^37].
    *   Dirichlet Posterior: $\hat{\theta}_{k, MAP} = (N_k+\alpha_k-1) / (N+\alpha_0-K)$, onde $\alpha_0 = \sum_k \alpha_k$ [^56].

#### Predição com Classificadores Generativos

Uma vez treinado o modelo (ou seja, obtida uma estimativa pontual $\hat{\theta}$ ou a posteriori $p(\theta|D)$), podemos usá-lo para classificar novas instâncias $x_{new}$. O objetivo é calcular a probabilidade posterior da classe $p(y=c|x_{new}, D)$.

**Abordagem Plug-in:**

A forma mais simples é usar uma estimativa pontual dos parâmetros (MLE ou MAP) na regra de Bayes:

$$ p(y=c|x_{new}, \hat{\theta}) \propto p(x_{new}|y=c, \hat{\theta}) p(y=c|\hat{\theta}) \quad [^21, 81] $$

Esta abordagem é computacionalmente eficiente, mas herda as limitações da estimativa pontual utilizada, como a sensibilidade ao *overfitting* (especialmente com MLE) [^48, 74]. A predição pode sub-representar a incerteza [^21].

**Abordagem Bayesiana Completa (Posterior Predictive):**

A abordagem Bayesiana correta envolve integrar (marginalizar) sobre a incerteza dos parâmetros representada pela distribuição *a posteriori*:

$$ p(y=c|x_{new}, D) = \int p(y=c|x_{new}, \theta) p(\theta|D) d\theta \propto \int p(x_{new}|y=c, \theta) p(y=c|\theta) p(\theta|D) d\theta \quad [^78] $$

Calculando a probabilidade preditiva para a classe $p(y=c|D)$ e para a densidade condicional $p(x_{new}|y=c, D) = \int p(x_{new}|y=c, \theta) p(\theta|D) d\theta$, obtemos:

$$ p(y=c|x_{new}, D) \propto p(y=c|D) p(x_{new}|y=c, D) \quad [^77] $$

Para modelos Naive Bayes com priors conjugados (Dirichlet/Beta), a distribuição preditiva posterior $p(x_j|y=c, D)$ pode ser convenientemente calculada usando a média da distribuição posterior dos parâmetros $\theta_{jc}$ [^79]. Por exemplo, para Bernoulli/Beta, $p(x_j=1|y=c, D) = E[\theta_{jc}|D] = (a+N_{jc}) / (a+b+N_c)$ [^44, 79]. O prior preditivo $p(y=c|D)$ também é a média posterior de $\pi_c$, e.g., $(\alpha_c+N_c)/(\alpha_0+N)$ para prior Dirichlet [^58, 80].

> A predição Bayesiana, ao considerar a incerteza nos parâmetros, tende a ser mais robusta e menos propensa a *overfitting* e ao paradoxo do cisne negro, especialmente com poucos dados [^22, 48]. As distribuições preditivas Bayesianas geralmente têm caudas mais pesadas do que as obtidas por plug-in com estimativas MAP ou MLE [^48]. Na prática, usar a média posterior como plug-in $\bar{\theta}$ [^79, 80] é frequentemente um bom compromisso, sendo computacionalmente similar ao plug-in MAP/MLE, mas resultando em menor *overfitting* [^81].

**Detalhe Prático: The Log-Sum-Exp Trick:**

Ao implementar a predição, especialmente com Naive Bayes, o cálculo de $p(x|y=c, \theta) = \prod_j p(x_j|y=c, \theta_{jc})$ pode resultar em valores extremamente pequenos, levando a **numerical underflow** [^82]. Para evitar isso, os cálculos são realizados em escala logarítmica. A normalização da probabilidade posterior $p(y=c|x, D)$ requer o cálculo do logaritmo da evidência $\log p(x|D) = \log \sum_{c'} p(x|y=c', D)p(y=c'|D)$. Isso envolve somar exponenciais, o que pode ser numericamente instável. O **log-sum-exp trick** é usado para calcular $\log \sum_{c'} e^{b_{c'}}$ de forma estável [^83]:

$$ \log \sum_{c'=1}^C e^{b_{c'}} = B + \log \sum_{c'=1}^C e^{b_{c'} - B} \quad (3.74) [^84] $$

onde $b_{c'} = \log p(x|y=c', D) + \log p(y=c'|D)$ e $B = \max_{c'} b_{c'}$ [^84].

### Conclusão

Os classificadores generativos oferecem uma abordagem probabilística fundamentada para a classificação, baseada na modelagem explícita da geração dos dados através da regra de Bayes. A especificação da densidade condicional de classe $p(x|y=c, \theta)$ e do prior de classe $p(y=c|\theta)$, juntamente com a inferência dos parâmetros $\theta$, são os passos centrais. Para dados discretos, modelos como Naive Bayes (com variantes Bernoulli, Multinomial ou DCM) fornecem arcabouços práticos e eficazes. A escolha entre estimação MLE e inferência Bayesiana (completa ou via MAP) envolve um trade-off entre simplicidade computacional e robustez contra overfitting, especialmente em regimes de dados esparsos. A abordagem Bayesiana, particularmente através do uso de priors conjugados e da média posterior para predição, oferece uma solução elegante para o problema de contagens zero e captura a incerteza paramétrica, levando a predições mais robustas. Técnicas como o *log-sum-exp trick* são essenciais para a implementação numérica estável desses modelos.

### Referências

[^1]: Page 1, Section 3.1: In Section 2.2.3.2, we discussed how to classify a feature vector x by applying Bayes rule to a generative classifier of the form $p(y = c|x, \theta) \propto p(x|y = c, \theta)p(y = c|\theta)$ (3.1)
[^2]: Page 1, Section 3.1: The key to using such models is specifying a suitable form for the class-conditional density $p(x|y = c, \theta)$, which defines what kind of data we expect to see in each class. In this chapter, we focus on the case where the observed data are discrete symbols. We also discuss how to infer the unknown parameters $\theta$ of such models.
[^3]: Page 1, Section 3.2: We can think of learning the meaning of a word as equivalent to concept learning, which in turn is equivalent to binary classification.
[^4]: Page 1, Section 3.2: For pedagogical purposes, we will consider a very simple example of concept learning called the number game...
[^5]: Page 2: We can represent this as a probability distribution, $p(\tilde{x}|D)$, which is the probability that $\tilde{x} \in C$ given the data $D$... This is called the posterior predictive distribution.
[^6]: Page 2: The classic approach to induction is to suppose we have a hypothesis space of concepts, $\mathcal{H}$...
[^7]: Page 3, Section 3.2.1: To formalize this, let us assume that examples are sampled uniformly at random from the extension of a concept. ... Tenenbaum calls this the strong sampling assumption. Given this assumption, the probability of independently sampling N items (with replacement) from h is given by $p(D|h) = [1 / \text{size}(h)]^N = [1 / |h|]^N$ (3.2). This crucial equation embodies what Tenenbaum calls the size principle, which means the model favors the simplest (smallest) hypothesis consistent with the data. This is more commonly known as Occam's razor.
[^8]: Page 3, Section 3.2.1: To see how it works, let $D = \{16\}$. Then $p(D|h_{two}) = 1/6$, since there are only 6 powers of two less than 100, but $p(D|h_{even}) = 1/50$, since there are 50 even numbers. ... After 4 examples, the likelihood of $h_{two}$ is $(1/6)^4 = 7.7 \times 10^{-4}$, whereas the likelihood of $h_{even}$ is $(1/50)^4 = 1.6 \times 10^{-7}$. This is a likelihood ratio of almost 5000:1 in favor of $h_{two}$.
[^9]: Page 3, Section 3.2.2: However, the hypothesis $h' = \text{“powers of two except 32”}$ seems “conceptually unnatural”. We can capture such intution by assigning low prior probability to unnatural concepts.
[^10]: Page 4: Although the subjectivity of the prior is controversial, it is actually quite useful. ... Thus we see that the prior is the mechanism by which background knowledge can be brought to bear on a problem. Without this, rapid learning (i.e., from small samples sizes) is impossible.
[^11]: Page 4: For illustration purposes, let us use a simple prior which puts uniform probability on 30 simple arithmetical concepts...
[^12]: Page 4, Section 3.2.3: The posterior is simply the likelihood times the prior, normalized. In this context we have $p(h|D) = p(D|h)p(h) / \sum_{h'} p(D, h')$ (3.3)
[^13]: Page 4, Section 3.2.3: Figure 3.2 plots the prior, likelihood and posterior after seeing $D = \{16\}$. We see that the posterior is a combination of prior and likelihood.
[^14]: Page 4, Section 3.2.3: In general, when we have enough data, the posterior $p(h|D)$ becomes peaked on a single concept, namely the MAP estimate, i.e., $p(h|D) \rightarrow \delta_{\hat{h}_{MAP}}(h)$ (3.4) where $\hat{h}_{MAP} = \text{argmax}_h p(h|D)$ is the posterior mode...
[^15]: Page 4, Section 3.2.3: ...where $\delta$ is the Dirac measure defined by $\delta_x(A) = \{1 \text{ if } x \in A, 0 \text{ if } x \notin A\}$ (3.5)
[^16]: Page 5: Note that the MAP estimate can be written as $\hat{h}_{MAP} = \text{argmax}_h p(D|h)p(h) = \text{argmax}_h [\log p(D|h) + \log p(h)]$ (3.6)
[^17]: Page 5: Since the likelihood term depends exponentially on N, and the prior stays constant, as we get more and more data, the MAP estimate converges towards the maximum likelihood estimate or MLE: $\hat{h}_{mle} \triangleq \text{argmax}_h p(D|h) = \text{argmax}_h \log p(D|h)$ (3.7). In other words, if we have enough data, we see that the data overwhelms the prior.
[^18]: Page 6: If the true hypothesis is in the hypothesis space, then the MAP/ ML estimate will converge upon this hypothesis. Thus we say that Bayesian inference (and ML estimation) are consistent estimators... We also say that the hypothesis space is identifiable in the limit...
[^19]: Page 7, Section 3.2.4: Specifically, the posterior predictive distribution in this context is given by $p(\tilde{x} \in C|D) = \sum_h p(y = 1|\tilde{x}, h)p(h|D)$ (3.8)
[^20]: Page 7, Section 3.2.4: This is just a weighted average of the predictions of each individual hypothesis and is called Bayes model averaging (BMA).
[^21]: Page 8: $p(\tilde{x} \in C|D) = \sum_h p(\tilde{x}|h)\delta_{\hat{h}}(h) = p(\tilde{x}|\hat{h})$ (3.9). This is called a plug-in approximation... However, in general, this under-represents our uncertainty...
[^22]: Page 8: In contrast, in the Bayesian approach, we start broad and then narrow down as we learn more... So the predictions made by a plug-in approach and a Bayesian approach are quite different in the small sample regime...
[^23]: Page 8, Section 3.2.5: To model human behavior, Tenenbaum used a slightly more sophisticated prior...
[^24]: Page 8, Section 3.3: However, in many applications, the unknown parameters are continuous, so the hypothesis space is (some subset) of $\mathbb{R}^K$, where K is the number of parameters.
[^25]: Page 9, Section 3.3: We will illustrate this by considering the problem of inferring the probability that a coin shows up heads...
[^26]: Page 9, Section 3.3.1: Suppose $X_i \sim Ber(\theta)$... If the data are iid, the likelihood has the form $p(D|\theta) = \theta^{N_1}(1-\theta)^{N_0}$ (3.11)
[^27]: Page 10: where we have $N_1 = \sum_{i=1}^N I(x_i = 1)$ heads and $N_0 = \sum_{i=1}^N I(x_i = 0)$ tails. These two counts are called the sufficient statistics of the data... (An alternative set of sufficient statistics are $N_1$ and $N = N_0 + N_1$.)
[^28]: Page 10: Now suppose the data consists of the count of the number of heads $N_1$ observed in a fixed number $N = N_1 + No$ of trials. In this case, we have $N_1 \sim Bin(N, \theta)$... the likelihood for the binomial sampling model is the same as the likelihood for the Bernoulli model.
[^29]: Page 10, Section 3.3.2: When the prior and the posterior have the same form, we say that the prior is a conjugate prior for the corresponding likelihood.
[^30]: Page 10, Section 3.3.2: In the case of the Bernoulli, the conjugate prior is the beta distribution... Beta$(\theta|a, b) \propto \theta^{a-1}(1-\theta)^{b-1}$ (3.15)
[^31]: Page 10, Section 3.3.2: The parameters of the prior are called hyper-parameters.
[^32]: Page 10, Section 3.3.2: The uniform distribution can be represented by a beta distribution with $a = b = 1$.
[^33]: Page 11, Section 3.3.3: If we multiply the likelihood by the beta prior we get the following posterior ... $p(\theta|D) \propto Bin(N_1|\theta, N_0+N_1)Beta(\theta|a, b) \propto Beta(\theta|N_1+a, N_0+b)$ (3.16)
[^34]: Page 11, Section 3.3.3: In particular, the posterior is obtained by adding the prior hyper-parameters to the empirical counts. For this reason, the hyper-parameters are known as pseudo counts. The strength of the prior, also known as the effective sample size of the prior, is the sum of the pseudo counts, $a+b$.
[^35]: Page 11, Section 3.3.3: Note that updating the posterior sequentially is equivalent to updating in a single batch.
[^36]: Page 11, Section 3.3.3: This makes Bayesian inference particularly well-suited to online learning...
[^37]: Page 12, Section 3.3.3.1: From Equation 2.62, the MAP estimate is given by $\hat{\theta}_{MAP} = (a+N_1-1) / (a+b+N-2)$ (3.21)
[^38]: Page 12, Section 3.3.3.1: If we use a uniform prior, then the MAP estimate reduces to the MLE, which is just the empirical fraction of heads: $\hat{\theta}_{MLE} = N_1/N$ (3.22)
[^39]: Page 12, Section 3.3.3.1: By contrast, the posterior mean is given by, $\bar{\theta} = (a+N_1) / (a+b+N)$ (3.23)
[^40]: Page 12, Section 3.3.3.1: We will now show that the posterior mean is convex combination of the prior mean and the MLE... $E[\theta|D] = \lambda m_1 + (1-\lambda)\hat{\theta}_{MLE}$ (3.24) where $\lambda = \alpha_0 / (N+\alpha_0)$...
[^41]: Page 12, Section 3.3.3.2: The variance of the Beta posterior is given by var$[\theta|D] = (a+N_1)(b+N_0) / [(a+N_1+b+N_0)^2 (a+N_1+b+N_0+1)]$ (3.25)
[^42]: Page 12, Section 3.3.3.2: We can simplify this formidable expression in the case that $N \gg a, b$, to get var$[\theta|D] \approx N_1 N_0 / (N N N) = \hat{\theta}(1-\hat{\theta}) / N$ (3.26)
[^43]: Page 13: We see that the uncertainty goes down at a rate of $1/\sqrt{N}$.
[^44]: Page 13, Section 3.3.4: Consider predicting the probability of heads in a single future trial... $p(\tilde{x}=1|D) = \int_0^1 p(\tilde{x}=1|\theta)p(\theta|D)d\theta = \int_0^1 \theta Beta(\theta|a,b)d\theta = E[\theta|D] = a / (a+b)$ (3.28, 3.29, using posterior parameters a,b implicitly) -> Using posterior parameters N1+a, N0+b: $E[\theta|D] = (N_1+a)/(N_1+a+N_0+b) = (N_1+a)/(N+a+b)$.
[^45]: Page 13, Section 3.3.4.1: Suppose instead that we plug-in the MLE... Unfortunately, this approximation can perform quite poorly when the sample size is small. For example, suppose we have seen N = 3 tails in a row. The MLE is $\hat{\theta} = 0/3 = 0... using this estimate, we predict that heads are impossible. This is called the zero count problem or the sparse data problem... analogous to a problem in philosophy called the black swan paradox.
[^46]: Page 13, Section 3.3.4.1: We will use a uniform prior, so $a = b = 1$. In this case, plugging in the posterior mean gives Laplace