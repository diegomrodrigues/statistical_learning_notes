## A Probabilidade a Priori e a Naturalidade Conceitual na Aprendizagem Bayesiana

### Introdução

Como explorado anteriormente no contexto da aprendizagem de conceitos Bayesianos, exemplificado pelo *number game* [^5], o objetivo central é inferir um conceito subjacente $C$ a partir de um conjunto de exemplos positivos $D = \{x_1, ..., x_N\}$. A inferência Bayesiana culmina na distribuição posterior sobre o espaço de hipóteses $\mathcal{H}$, $p(h|D)$, que quantifica a plausibilidade de cada hipótese $h \in \mathcal{H}$ após a observação dos dados $D$. Vimos que a verossimilhança $p(D|h)$, governada pelo **princípio do tamanho** (*size principle*) [^14], favorece hipóteses mais simples ou menores que explicam os dados. Contudo, a verossimilhança por si só não captura todas as nuances da inferência indutiva humana, especialmente a preferência por conceitos que são "naturalmente" concebíveis. Este capítulo aprofunda o papel crucial da **probabilidade a priori**, $p(h)$, na modelagem Bayesiana da aprendizagem de conceitos, focando em como ela codifica a **naturalidade conceitual** das hipóteses e permite a incorporação de conhecimento prévio.

### Conceitos Fundamentais

#### A Limitação da Verossimilhança e a Necessidade da Priori

A análise da verossimilhança, $p(D|h)$, baseada na **strong sampling assumption** [^12] onde $p(D|h) = [1/\text{size}(h)]^N$ [^13], implementa uma forma de **navalha de Occam** (*Occam's razor*), favorecendo hipóteses com menor extensão [^14]. Embora poderosa, essa abordagem pode levar a conclusões contraintuitivas. Consideremos novamente o exemplo $D = \{16, 8, 2, 64\}$. A hipótese $h = \text{"potências de dois"}$ é consistente com os dados. No entanto, a hipótese alternativa $h' = \text{"potências de dois exceto 32"}$ [^16] também é consistente. Dado que o número 32 não está presente em $D$, a hipótese $h'$ tem uma extensão menor que $h$ (size($h'$) < size($h$)). Consequentemente, a verossimilhança de $h'$ é *maior* que a de $h$, pois $p(D|h') = [1/\text{size}(h')]^N > [1/\text{size}(h)]^N = p(D|h)$. A verossimilhança, isoladamente, preferiria $h'$.

Contudo, a hipótese $h'$ parece *"conceptually unnatural"* [^17]. Intuitivamente, preferimos a regra mais geral "potências de dois" em vez de uma regra ad-hoc que exclui um único elemento ausente nos dados. Essa intuição não é capturada pela verossimilhança, que se concentra apenas na adequação aos dados observados sob o princípio do tamanho. É aqui que a probabilidade a priori $p(h)$ se torna essencial.

#### A Priori como Medida de Naturalidade Conceitual

A estrutura Bayesiana permite incorporar essa intuição sobre a naturalidade dos conceitos através da **probabilidade a priori**, $p(h)$. A ideia central é atribuir uma probabilidade a priori baixa a conceitos considerados "não naturais" [^18].

> A probabilidade a priori captura a 'naturalidade conceitual' das hipóteses, reconhecendo o aspecto subjetivo do raciocínio Bayesiano, onde diferentes priors podem levar a diferentes respostas. Um prior uniforme atribui probabilidade igual a conceitos aritméticos simples, enquanto dá menor peso a conceitos 'não naturais'. [^19]

No exemplo ilustrativo fornecido no contexto [^25], um prior simples é construído sobre um espaço de hipóteses $\mathcal{H}$ contendo cerca de 30 conceitos aritméticos simples (como "números pares", "números ímpares", "números primos", "múltiplos de k", "potências de k", etc.). Este prior atribui probabilidade (quase) uniforme a esses conceitos "naturais", mas pode incorporar nuances, como atribuir pesos ligeiramente maiores a conceitos muito básicos como "par" e "ímpar". Crucialmente, hipóteses consideradas "não naturais", como $h' = \text{"potências de dois exceto 32"}$ ou $h'' = \text{"potências de 2, mais 37"}$, recebem um peso a priori significativamente baixo [^25]. Uma visualização desse tipo de prior é mostrada na Figura 3.2(a) do material de referência [^26].

Ao fazer isso, mesmo que uma hipótese "não natural" como $h'$ tenha uma alta verossimilhança $p(D|h')$, seu baixo valor a priori $p(h')$ resultará em uma baixa probabilidade posterior $p(h'|D) \propto p(D|h')p(h')$, prevenindo o **superajuste (overfitting)** aos dados específicos de uma maneira conceitualmente implausível [^31].

#### Subjetividade e Incorporação de Conhecimento Prévio

A atribuição de probabilidades a priori introduz um elemento de **subjetividade** no raciocínio Bayesiano [^19]. Diferentes agentes (por exemplo, uma criança e um professor de matemática) podem ter priors distintos, refletindo diferentes espaços de hipóteses ou diferentes julgamentos sobre a naturalidade dos conceitos, levando potencialmente a conclusões distintas a partir dos mesmos dados [^21]. O texto sugere que essa diferença pode ser modelada definindo um espaço de hipóteses comum e atribuindo peso a priori zero a conceitos "avançados" para o agente menos experiente (a criança) [^22].

Embora controversa [^21], essa subjetividade é descrita como *útil* [^20]. Ela fornece o mecanismo pelo qual o **conhecimento prévio (background knowledge)** pode ser trazido para influenciar a inferência [^23]. O exemplo dos números {1200, 1500, 900, 1400} ilustra isso: se informados que os números seguem uma regra aritmética, podemos achar 400 provável e 1183 improvável; se informados que são níveis de colesterol saudáveis, o julgamento se inverte [^23]. O prior é o que permite essa diferenciação baseada no contexto ou conhecimento prévio.

> Sem isso [o prior], o aprendizado rápido (i.e., a partir de amostras pequenas) é impossível. [^24]

A capacidade de incorporar conhecimento prévio via prior é fundamental para explicar a **rápida aprendizagem a partir de tamanhos de amostra pequenos** [^20], uma característica marcante da cognição humana que modelos baseados puramente em verossimilhança lutam para replicar. Com um prior adequado que favorece conceitos prováveis ou naturais, mesmo poucos exemplos podem levar a uma forte inferência indutiva [^8].

#### O Impacto da Priori na Inferência Posterior

Matematicamente, a influência da priori é formalizada pela regra de Bayes para a probabilidade posterior de uma hipótese $h$ dados os dados $D$:
$$ p(h|D) = \frac{p(D|h)p(h)}{\sum_{h' \in \mathcal{H}} p(D|h')p(h')} \propto p(D|h)p(h) $$ [^27]

A probabilidade posterior é, portanto, proporcional ao produto da verossimilhança e da priori. A Figura 3.2 do material de referência [^28] ilustra isso para $D=\{16\}$. Vemos que a posterior é uma combinação da priori e da verossimilhança [^29]. Conceitos "não naturais" como "potências de 2, mais 37" ou "potências de 2, exceto 32" podem ter alta verossimilhança (se consistentes com $D=\{16\}$), mas seu baixo peso a priori resulta em baixo suporte posterior. Inversamente, um conceito como "números ímpares" tem alta priori (no prior ilustrativo), mas sua verossimilhança para $D=\{16\}$ é zero (ou muito baixa se considerarmos a extensão completa), resultando também em baixo suporte posterior [^29].

Quando mais dados são observados, como em $D=\{16, 8, 2, 64\}$ (Figura 3.3 [^30]), a verossimilhança $p(D|h)$ torna-se muito mais acentuada, especialmente porque o termo $[1/\text{size}(h)]^N$ decai exponencialmente com $N$ para hipóteses com extensões maiores [^15]. Para a hipótese correta ("potências de dois"), a verossimilhança torna-se extremamente alta em comparação com outras hipóteses consistentes mas maiores (como "números pares"). Nesse cenário, a verossimilhança pode dominar a priori, levando a uma posterior fortemente concentrada na hipótese mais provável [^30], resultando no que o texto chama de *"aha moment"* [^30]. No entanto, mesmo aqui, a priori desempenha o papel vital de eliminar hipóteses "não naturais" (como "potências de dois exceto 32") que também podem ter alta verossimilhança, evitando assim o overfitting [^31].

À medida que a quantidade de dados $N$ cresce, o termo da verossimilhança $p(D|h)$ domina cada vez mais o termo da priori $p(h)$ (assumindo que $p(h) > 0$) [^34]. Nesse limite, a estimativa **MAP (Maximum A Posteriori)**, $\hat{h}_{MAP} = \text{argmax}_h p(h|D)$, converge para a estimativa de **Máxima Verossimilhança (MLE - Maximum Likelihood Estimate)**, $\hat{h}_{mle} = \text{argmax}_h p(D|h)$ [^34]. Isso reflete a ideia de que, com dados suficientes, *os dados sobrepujam a priori* (*data overwhelms the prior*) [^35]. Contudo, em regimes de poucos dados, característicos de muitas situações de aprendizagem humana, a priori é indispensável.

#### Priors Mais Complexos

Embora o prior simples discutido [^25] sirva para ilustração, priors mais sofisticados podem ser desenvolvidos para modelar melhor o comportamento humano. O texto menciona um prior mais complexo usado por Tenenbaum [^44], derivado de dados experimentais sobre similaridade numérica. Esse prior é uma mistura de duas componentes: uma sobre regras aritméticas e outra sobre intervalos $[n, m]$ para $1 \le n, m \le 100$. A forma é $p(h) = \pi_0 p_{rules}(h) + (1-\pi_0)p_{interval}(h)$ [^44], onde $\pi_0$ é um peso relativo. O facto de $\pi_0 > 0.5$ ser um bom ajuste reflete a preferência humana por conceitos definidos por regras [^45]. As distribuições preditivas geradas com este prior e espaço de hipóteses mais amplo mostraram-se notavelmente similares aos dados humanos (comparar Figura 3.5 [^46] com Figura 3.1 [^6]), mesmo sem ajuste fino aos dados humanos específicos (exceto pela escolha do espaço de hipóteses) [^46]. Isso reforça a ideia de que priors bem estruturados, capturando a "naturalidade conceitual", são fundamentais para modelos Bayesianos de aprendizagem de conceitos.

### Conclusão

A probabilidade a priori $p(h)$ é um componente fundamental e indispensável da abordagem Bayesiana para a aprendizagem de conceitos. Indo além da mera adequação aos dados proporcionada pela verossimilhança $p(D|h)$, a priori permite codificar noções intuitivas sobre a **naturalidade conceitual** das hipóteses [^18, ^19]. Ao atribuir pesos mais baixos a conceitos "não naturais" ou excessivamente complexos [^18, ^25], a priori ajuda a guiar a inferência para explicações mais plausíveis e gerais, mitigando o risco de superajuste (overfitting) [^31], especialmente quando os dados são escassos. A **subjetividade** inerente à escolha da priori [^19], embora fonte de debate [^21], é também sua força, pois fornece o mecanismo para incorporar **conhecimento prévio** [^23] e permite explicar a capacidade humana de **aprendizagem rápida a partir de poucos exemplos** [^20, ^24]. Embora a influência da priori diminua à medida que mais dados se tornam disponíveis [^35], seu papel é crítico nos estágios iniciais da aprendizagem e na seleção entre hipóteses que, de outra forma, seriam igualmente prováveis sob a ótica da verossimilhança. A construção de priors adequados, que reflitam a estrutura e as preferências do domínio de conhecimento, continua a ser um aspecto central no desenvolvimento de modelos generativos Bayesianos para a aprendizagem e cognição.

### Referências

[^1]: Seção 3.1, Equação (3.1): $p(y = c|x, \theta) \propto p(x|y = c, \theta)p(y = c|\theta)$
[^2]: Seção 3.2: Bayesian concept learning ... example ... called the number game, based on part of Josh Tenenbaum's PhD thesis (Tenenbaum 1999).
[^3]: Seção 3.2: We can think of learning the meaning of a word as equivalent to concept learning, which in turn is equivalent to binary classification. To see this, define f(x) = 1 if x is an example of the concept C, and f(x) = 0 otherwise. Then the goal is to learn the indicator function f...
[^4]: Seção 3.2: ...psychological research has shown that people can learn concepts from positive examples alone (Xu and Tenenbaum 2007). ... By contrast, we will devise a way to learn from positive examples alone.
[^5]: Seção 3.2: The game proceeds as follows. I choose some simple arithmetical concept C ... I then give you a series of randomly chosen positive examples D = {x1,...,xN} drawn from C, and ask you whether some new test case $\tilde{x}$ belongs to C...
[^6]: Seção 3.2, Figura 3.1: Empirical predictive distribution averaged over 8 humans in the number game. First two rows: after seeing D = {16} and D = {60}. ... Third row: after seeing D = {16, 8, 2, 64}. ... Bottom row: after seeing D = {16, 23, 19, 20}.
[^7]: Seção 3.2: We can represent this as a probability distribution, p($\tilde{x}$|D), which is the probability that $\tilde{x} \in C$ given the data D ... This is called the posterior predictive distribution.
[^8]: Seção 3.2: Now suppose I tell you that 8, 2 and 64 are also positive examples. Now you may guess that the hidden concept is “powers of two”. This is an example of induction.
[^9]: Seção 3.2: The classic approach to induction is to suppose we have a hypothesis space of concepts, $\mathcal{H}$, such as: odd numbers, even numbers, ... powers of two...
[^10]: Seção 3.2: The subset of $\mathcal{H}$ that is consistent with the data D is called the version space. ... However, the version space is not the whole story.
[^11]: Seção 3.2.1: We must explain why we chose $h_{two}$ ... and not, say, $h_{even}$ ... The key intuition is that we want to avoid suspicious coincidences.
[^12]: Seção 3.2.1: To formalize this, let us assume that examples are sampled uniformly at random from the extension of a concept. ... Tenenbaum calls this the strong sampling assumption.
[^13]: Seção 3.2.1, Equação (3.2): $p(D|h) = [\frac{1}{\text{size}(h)}]^N = [\frac{1}{|h|}]^N$
[^14]: Seção 3.2.1: This crucial equation embodies what Tenenbaum calls the size principle, which means the model favors the simplest (smallest) hypothesis consistent with the data. This is more commonly known as Occam's razor.
[^15]: Seção 3.2.1: After 4 examples, the likelihood of $h_{two}$ is $(1/6)^4 = 7.7 \times 10^{-4}$, whereas the likelihood of $h_{even}$ is $(1/50)^4 = 1.6 \times 10^{-7}$. This is a likelihood ratio of almost 5000:1 in favor of $h_{two}$.
[^16]: Seção 3.2.2: Suppose $D = \{16, 8, 2, 64\}$. Given this data, the concept $h' = \text{“powers of two except 32”}$ is more likely than $h = \text{“powers of two”}$, since $h'$ does not need to explain the coincidence that 32 is missing from the set of examples.
[^17]: Seção 3.2.2: However, the hypothesis $h' = \text{“powers of two except 32”}$ seems “conceptually unnatural”.
[^18]: Seção 3.2.2: We can capture such intuition by assigning low prior probability to unnatural concepts.
[^19]: Seção 3.2.2: Of course, your prior might be different than mine. This subjective aspect of Bayesian reasoning is a source of much controversy, since it means, for example, that a child and a math professor will reach different answers.
[^20]: Seção 3.2.2: Although the subjectivity of the prior is controversial, it is actually quite useful. ... Thus we see that the prior is the mechanism by which background knowledge can be brought to bear on a problem. Without this, rapid learning (i.e., from small samples sizes) is impossible.
[^21]: Seção 3.2.2: This subjective aspect of Bayesian reasoning is a source of much controversy, since it means, for example, that a child and a math professor will reach different answers.
[^22]: Seção 3.2.2: In fact, they presumably not only have different priors, but also different hypothesis spaces. However, we can finesse that by defining the hypothesis space of the child and the math professor to be the same, and then setting the child's prior weight to be zero on certain “advanced” concepts.
[^23]: Seção 3.2.2: If you are told the numbers are from some arithmetic rule, then given 1200, 1500, 900 and 1400, you may think 400 is likely but 1183 is unlikely. But if you are told that the numbers are examples of healthy cholesterol levels, you would probably think 400 is unlikely and 1183 is likely. Thus we see that the prior is the mechanism by which background knowledge can be brought to bear on a problem.
[^24]: Seção 3.2.2: Without this [the prior], rapid learning (i.e., from small samples sizes) is impossible.
[^25]: Seção 3.2.2: For illustration purposes, let us use a simple prior which puts uniform probability on 30 simple arithmetical concepts, such as “even numbers”, “odd numbers”, “prime numbers”, “numbers ending in 9", etc. To make things more interesting, we make the concepts even and odd more likely apriori. We also include two “unnatural” concepts, namely "powers of 2, plus 37” and “powers of 2, except 32", but give them low prior weight.
[^26]: Seção 3.2.2: See Figure 3.2(a) for a plot of this prior.
[^27]: Seção 3.2.3, Equação (3.3): $p(h|D) = \frac{p(D|h)p(h)}{\sum_{h' \in \mathcal{H}} p(D|h')p(h')} = \frac{p(h)I(D \subseteq h)/|h|^N}{\sum_{h' \in \mathcal{H}} p(h')I(D \subseteq h')/|h'|^N}$
[^28]: Seção 3.2.3: Figure 3.2 plots the prior, likelihood and posterior after seeing $D = \{16\}$.
[^29]: Seção 3.2.3: We see that the posterior is a combination of prior and likelihood. ... However, the “unnatural” concepts of “powers of 2, plus 37” and “powers of 2, except 32” have low posterior support, despite having high likelihood, due to the low prior. Conversely, the concept of odd numbers has low posterior support, despite having a high prior, due to the low likelihood.
[^30]: Seção 3.2.3: Figure 3.3 plots the prior, likelihood and posterior after seeing $D = \{16, 8, 2, 64\}$. Now the likelihood is much more peaked on the powers of two concept, so this dominates the posterior. Essentially the learner has an aha moment, and figures out the true concept.
[^31]: Seção 3.2.3: (Here we see the need for the low prior on the unnatural concepts, otherwise we would have overfit the data and picked "powers of 2, except for 32".)
[^32]: Seção 3.2.3, Equação (3.4), (3.5): $p(h|D) \to \delta_{\hat{h}_{MAP}}(h)$ where $\hat{h}_{MAP} = \text{argmax}_h p(h|D)$ is the posterior mode, and where $\delta$ is the Dirac measure...
[^33]: Seção 3.2.3, Equação (3.6): $\hat{h}_{MAP} = \text{argmax}_h p(D|h)p(h) = \text{argmax}_h [\log p(D|h) + \log p(h)]$
[^34]: Seção 3.2.3, Equação (3.7): Since the likelihood term depends exponentially on N, and the prior stays constant, as we get more and more data, the MAP estimate converges towards the maximum likelihood estimate or MLE: $\hat{h}_{mle} \triangleq \text{argmax}_h p(D|h) = \text{argmax}_h \log p(D|h)$
[^35]: Seção 3.2.3: In other words, if we have enough data, we see that the data overwhelms the prior.
[^36]: Seção 3.2.3: Thus we say that Bayesian inference (and ML estimation) are consistent estimators...
[^37]: Seção 3.2.3: We also say that the hypothesis space is identifiable in the limit...
[^38]: Seção 3.2.3: If our hypothesis class is not rich enough to represent the “truth”... we will converge on the hypothesis that is as close as possible to the truth.
[^39]: Seção 3.2.4, Equação (3.8): $p(\tilde{x} \in C|D) = \sum_h p(y=1|\tilde{x}, h)p(h|D)$. This is just a weighted average ... and is called Bayes model averaging (BMA).
[^40]: Seção 3.2.4: This is illustrated in Figure 3.4.
[^41]: Seção 3.2.4: When we have a small and/or ambiguous dataset, the posterior $p(h|D)$ is vague, which induces a broad predictive distribution. However, once we have “figured things out”, the posterior becomes a delta function centered at the MAP estimate.
[^42]: Seção 3.2.4, Equação (3.9): $p(\tilde{x} \in C|D) = \sum_h p(\tilde{x}|h)\delta_{\hat{h}}(h) = p(\tilde{x}|\hat{h})$. This is called a plug-in approximation...
[^43]: Seção 3.2.4: Although MAP learning is simple, it cannot explain the gradual shift... In contrast, in the Bayesian approach, we start broad and then narrow down as we learn more...
[^44]: Seção 3.2.5, Equação (3.10): To model human behavior, Tenenbaum used a slightly more sophisticated prior ... The result is a set of arithmetical concepts similar to those mentioned above, plus all intervals between n and m ... Thus the prior is a mixture of two priors... $p(h) = \pi_0 p_{rules}(h) + (1 - \pi_0) p_{interval}(h)$
[^45]: Seção 3.2.5: The only free parameter in the model is the relative weight, $\pi_0$... The results are not very sensitive to this value, so long as $\pi_0 > 0.5$, reflecting the fact that people are more likely to think of concepts defined by rules.
[^46]: Seção 3.2.5: The predictive distribution of the model, using this larger hypothesis space, is shown in Figure 3.5. It is strikingly similar to the human predictive distribution, shown in Figure 3.1, even though it was not fit to human data (modulo the choice of hypothesis space).

<!-- END -->