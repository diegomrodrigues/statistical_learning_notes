## A Função de Verossimilhança para o Modelo Beta-Binomial

### Introdução

Como introduzido na Seção 3.3 [^1], o modelo **beta-binomial** representa uma mudança fundamental de espaços de hipóteses discretos, como explorado no exemplo do *number game* (Seção 3.2) [^2], para cenários onde os parâmetros desconhecidos são contínuos. Especificamente, este modelo aborda o problema de inferir a probabilidade $\theta$ de um evento binário (como uma moeda dar "heads") com base em uma série de observações [^1]. Este modelo serve como alicerce para muitos métodos mais complexos abordados posteriormente, incluindo classificadores naive Bayes e modelos de Markov [^1]. Seguindo a abordagem familiar de especificar a verossimilhança (*likelihood*) e a priori (*prior*), este capítulo foca exclusivamente na derivação e compreensão da função de verossimilhança para o modelo beta-binomial, um componente crucial para a inferência subsequente [^1]. Analisaremos sua forma, sua relação com as estatísticas suficientes dos dados e sua conexão intrínseca com a distribuição Binomial.

### Conceitos Fundamentais

#### Definição do Modelo e Derivação da Verossimilhança

Consideremos um cenário onde realizamos uma sequência de $N$ ensaios de Bernoulli independentes e identicamente distribuídos (iid). Seja $X_i$ a variável aleatória representando o resultado do $i$-ésimo ensaio, onde $X_i=1$ denota "heads" (sucesso) e $X_i=0$ denota "tails" (falha) [^3]. Assumimos que cada $X_i$ segue uma distribuição de Bernoulli com um parâmetro de taxa $\theta \in [0, 1]$, que representa a probabilidade de sucesso (heads) [^3]:
$$ X_i \sim \text{Ber}(\theta) $$
O conjunto de dados observados é $D = \{x_1, x_2, ..., x_N\}$. A função de verossimilhança, $p(D|\theta)$, quantifica a probabilidade de observar esses dados $D$ dado um valor específico do parâmetro $\theta$. Devido à suposição iid, a verossimilhança da sequência completa é o produto das probabilidades individuais:
$$ p(D|\theta) = \prod_{i=1}^N p(x_i|\theta) $$
Para um único ensaio de Bernoulli, a função de massa de probabilidade é $p(x_i|\theta) = \theta^{x_i} (1-\theta)^{1-x_i}$. Substituindo na expressão do produto, obtemos:
$$ p(D|\theta) = \prod_{i=1}^N \theta^{x_i} (1-\theta)^{1-x_i} = \theta^{\sum_{i=1}^N x_i} (1-\theta)^{\sum_{i=1}^N (1-x_i)} $$
Agora, definimos $N_1$ como o número total de sucessos ("heads") e $N_0$ como o número total de falhas ("tails") na sequência $D$ [^4]. Matematicamente:
$$ N_1 = \sum_{i=1}^N I(x_i=1) \quad \text{e} \quad N_0 = \sum_{i=1}^N I(x_i=0) $$
onde $I(\cdot)$ é a função indicadora [^4]. Note que $N = N_0 + N_1$ é o número total de ensaios. Com estas definições, a função de verossimilhança simplifica-se para a forma apresentada na Equação (3.11) do texto original [^5]:
> $$ p(D|\theta) = \theta^{N_1} (1 - \theta)^{N_0} $$
Esta expressão é central para o modelo beta-binomial, pois encapsula como a probabilidade dos dados observados varia em função do parâmetro desconhecido $\theta$.

#### Estatísticas Suficientes

As contagens $N_1$ e $N_0$ são denominadas **estatísticas suficientes** (*sufficient statistics*) dos dados $D$ para o parâmetro $\theta$ [^6]. Isso significa que $N_1$ e $N_0$ contêm toda a informação relevante presente nos dados $D$ para a inferência sobre $\theta$ [^6]. Formalmente, uma estatística $s(D)$ é suficiente para $\theta$ se a distribuição posterior de $\theta$ depender dos dados $D$ apenas através de $s(D)$, ou seja, $p(\theta|D) = p(\theta|s(D))$ [^7]. Alternativamente, se utilizarmos uma prior uniforme, a suficiência implica que a verossimilhança depende dos dados apenas através da estatística suficiente: $p(D|\theta) \propto p(s(D)|\theta)$ [^7].
A consequência prática é que, para realizar inferências sobre $\theta$, não precisamos armazenar toda a sequência $D = \{x_1, ..., x_N\}$; basta manter as contagens $N_1$ e $N_0$ [^6]. Dois conjuntos de dados diferentes que resultem nas mesmas contagens $N_1$ e $N_0$ levarão exatamente à mesma inferência sobre $\theta$ [^7]. Um conjunto alternativo de estatísticas suficientes é $(N_1, N)$, onde $N = N_0 + N_1$ é o número total de ensaios [^6].

#### Conexão com a Distribuição Binomial

Existe uma ligação íntima entre a função de verossimilhança derivada dos ensaios de Bernoulli e a distribuição Binomial. Suponha que, em vez de observar a sequência completa $D$, observamos apenas o número total de sucessos $N_1$ num número fixo $N = N_1 + N_0$ de ensaios [^8]. Neste caso, a variável aleatória $N_1$ segue uma distribuição Binomial com parâmetros $N$ e $\theta$, denotada como $N_1 \sim \text{Bin}(N, \theta)$ [^8]. A função de massa de probabilidade (pmf) da distribuição Binomial é dada por [^9]:
$$ \text{Bin}(k|n, \theta) \equiv \binom{n}{k} \theta^k (1 - \theta)^{n-k} $$
Aplicando isso ao nosso caso, onde $k=N_1$ e $n=N=N_1+N_0$:\
$$ p(N_1|N, \theta) = \binom{N}{N_1} \theta^{N_1} (1 - \theta)^{N_0} $$
Esta é a probabilidade de observar exatamente $N_1$ sucessos em $N$ ensaios. Se considerarmos esta pmf como uma função de $\theta$ para $N_1$ e $N$ fixos, ela representa a verossimilhança baseada no modelo de amostragem Binomial. É crucial notar que o termo combinatório $\binom{N}{N_1}$ (ou $\binom{n}{k}$) é uma constante que *não depende* de $\theta$ [^10]. Portanto, a função de verossimilhança para o modelo de amostragem Binomial é proporcional à função de verossimilhança que derivamos para o modelo de Bernoulli [^10]:
$$ p(N_1|N, \theta) \propto \theta^{N_1} (1 - \theta)^{N_0} $$
Isso implica que qualquer inferência sobre $\theta$ (seja via Maximum Likelihood Estimation - MLE, ou inferência Bayesiana) será idêntica, quer observemos a sequência completa de ensaios $D = \{x_1, ..., x_N\}$ ou apenas as contagens agregadas $(N_1, N)$ [^10]. A essência da informação sobre $\theta$ está contida nas estatísticas suficientes $N_1$ e $N_0$.

#### Propriedades e Implicações para Inferência

A função de verossimilhança $p(D|\theta) = \theta^{N_1} (1 - \theta)^{N_0}$ é o ponto de partida para estimar $\theta$. A estimativa de máxima verossimilhança (MLE) é obtida maximizando esta função (ou, mais convenientemente, seu logaritmo) em relação a $\theta$. Como mencionado no texto e detalhado no Exercício 3.1 [^11, ^12], a MLE para $\theta$ é simplesmente a fração empírica de sucessos:
$$ \hat{\theta}_{MLE} = \frac{N_1}{N} = \frac{N_1}{N_1 + N_0} $$ [^11]
Na inferência Bayesiana, a função de verossimilhança é combinada com uma distribuição a priori $p(\theta)$ para obter a distribuição a posteriori $p(\theta|D)$ através da regra de Bayes:
$$ p(\theta|D) \propto p(D|\theta) p(\theta) $$ [^13]
Como discutido na Seção 3.3.2 [^13], se a prior $p(\theta)$ tiver a mesma forma funcional da verossimilhança, ou seja, $p(\theta) \propto \theta^{\gamma_1} (1-\theta)^{\gamma_2}$ (como a distribuição Beta, $\text{Beta}(\theta|a,b) \propto \theta^{a-1}(1-\theta)^{b-1}$ [^14]), ela é chamada de **prior conjugada** [^13]. A conjugação simplifica enormemente os cálculos, pois a posterior também pertencerá à mesma família de distribuições. Neste caso, a posterior é obtida simplesmente somando os expoentes da verossimilhança e da prior [^13]:
$$ p(\theta|D) \propto \theta^{N_1} (1 - \theta)^{N_0} \times \theta^{a-1} (1 - \theta)^{b-1} = \theta^{N_1+a-1} (1 - \theta)^{N_0+b-1} $$
Isso corresponde a uma distribuição $\text{Beta}(\theta|N_1+a, N_0+b)$ [^15]. A forma específica da função de verossimilhança $\theta^{N_1} (1 - \theta)^{N_0}$ é, portanto, fundamental para a tratabilidade analítica da inferência Bayesiana no modelo beta-binomial.

### Conclusão

A função de verossimilhança $p(D|\theta) = \theta^{N_1} (1 - \theta)^{N_0}$ é um componente central do modelo beta-binomial, quantificando a plausibilidade do parâmetro $\theta$ à luz dos dados observados $D$. Demonstramos sua derivação a partir de ensaios de Bernoulli iid e destacamos sua dependência exclusiva das **estatísticas suficientes** $N_1$ (número de sucessos) e $N_0$ (número de falhas). Além disso, estabelecemos sua equivalência funcional (a menos de uma constante de normalização) com a verossimilhança derivada da distribuição Binomial, garantindo que as inferências sobre $\theta$ sejam as mesmas independentemente do formato dos dados (sequência completa vs. contagens agregadas) [^10]. Esta função não só permite a estimação por máxima verossimilhança [^11], mas também se combina elegantemente com a prior conjugada Beta na inferência Bayesiana [^13], facilitando o cálculo da distribuição posterior (Seção 3.3.3 [^15]) e da distribuição preditiva posterior (Seção 3.3.4 [^16]). Sua estrutura serve de base para a generalização encontrada no modelo Dirichlet-Multinomial (Seção 3.4 [^17]).

### Referências

[^1]: Página 9: "We will illustrate this by considering the problem of inferring the probability that a coin shows up heads, given a series of observed coin tosses... It is historically important... We will follow our now-familiar recipe of specifying the likelihood and prior, and deriving the posterior and posterior predictive."
[^2]: Página 8: "The number game involved inferring a distribution over a discrete variable drawn from a finite hypothesis space... However, in many applications, the unknown parameters are continuous..."
[^3]: Página 9: "Suppose $X_i \sim \text{Ber}(\theta)$, where $X_i = 1$ represents “heads”, $X_i = 0$ represents “tails”, and $\theta \in [0, 1]$ is the rate parameter (probability of heads)."
[^4]: Página 10: "where we have $N_1 = \sum_{i=1}^N I(x_i = 1)$ heads and $N_0 = \sum_{i=1}^N I(x_i = 0)$ tails."
[^5]: Página 9: "If the data are iid, the likelihood has the form $p(D|\theta) = \theta^{N_1} (1 - \theta)^{N_0}$ (3.11)"
[^6]: Página 10: "These two counts are called the sufficient statistics of the data, since this is all we need to know about D to infer $\theta$. (An alternative set of sufficient statistics are $N_1$ and $N = N_0 + N_1$.)"
[^7]: Página 10: "More formally, we say $s(D)$ is a sufficient statistic for data $D$ if $p(\theta|D) = p(\theta|s(\text{data}))$. If we use a uniform prior, this is equivalent to saying $p(D|\theta) \propto p(s(D)|\theta)$. Consequently, if we have two datasets with the same sufficient statistics, we will infer the same value for $\theta$."
[^8]: Página 10: "Now suppose the data consists of the count of the number of heads $N_1$ observed in a fixed number $N = N_1 + N_0$ of trials. In this case, we have $N_1 \sim \text{Bin}(N, \theta)$..."
[^9]: Página 10: "...where Bin represents the binomial distribution, which has the following pmf: $\text{Bin}(k|n, \theta) \triangleq \binom{n}{k} \theta^k (1-\theta)^{n-k}$ (3.12)"
[^10]: Página 10: "Since $\binom{n}{k}$ is a constant independent of $\theta$, the likelihood for the binomial sampling model is the same as the likelihood for the Bernoulli model. So any inferences we make about $\theta$ will be the same whether we observe the counts, $D = (N_1, N)$, or a sequence of trials, $D = \{x_1, ..., x_N\}$."
[^11]: Página 12: "If we use a uniform prior, then the MAP estimate reduces to the MLE, which is just the empirical fraction of heads: $\hat{\theta}_{MLE} = N_1/N$ (3.22)"
[^12]: Página 25: "Exercise 3.1 MLE for the Bernoulli/ binomial model. Derive Equation 3.22 by optimizing the log of the likelihood in Equation 3.11."
[^13]: Página 10: "...if the prior had the same form as the likelihood... $p(\theta) \propto \theta^{\gamma_1} (1-\theta)^{\gamma_2}$... then we could easily evaluate the posterior by simply adding up the exponents: $p(\theta|D) \propto p(D|\theta)p(\theta) = \theta^{N_1} (1 - \theta)^{N_0} \theta^{\gamma_1} (1 - \theta)^{\gamma_2} = \theta^{N_1+\gamma_1} (1 - \theta)^{N_0+\gamma_2}$ (3.14). When the prior and the posterior have the same form, we say that the prior is a conjugate prior..."
[^14]: Página 10: "In the case of the Bernoulli, the conjugate prior is the beta distribution... $\text{Beta}(\theta|a, b) \propto \theta^{a-1}(1-\theta)^{b-1}$ (3.15)"
[^15]: Página 11: "If we multiply the likelihood by the beta prior we get the following posterior... $p(\theta|D) \propto \text{Bin}(N_1|\theta, N_0+N_1)\text{Beta}(\theta|a, b) \propto \text{Beta}(\theta|N_1+a, N_0+b)$ (3.16)"
[^16]: Página 13: "Thus we see that the mean of the posterior predictive distribution is equivalent (in this case) to plugging in the posterior mean parameters: $p(\tilde{x}|D) = \text{Ber}(\tilde{x}|E[\theta|D])$."
[^17]: Página 15: "In this section, we generalize these results to infer the probability that a dice with K sides comes up as face k... The likelihood for the multinomial model has the same form [as Dirichlet-Multinomial likelihood]..."

<!-- END -->