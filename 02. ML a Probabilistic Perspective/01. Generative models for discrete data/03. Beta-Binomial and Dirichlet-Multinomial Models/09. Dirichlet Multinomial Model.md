## O Modelo Dirichlet-Multinomial: Generalização e Inferência Bayesiana

### Introdução

Em continuidade à nossa exploração de modelos generativos para dados discretos [^1], e expandindo o conceito apresentado na Seção 3.3 sobre o **modelo Beta-Binomial** [^8], este capítulo foca no **modelo Dirichlet-Multinomial**. Como vimos anteriormente, o modelo Beta-Binomial é utilizado para inferir a probabilidade de sucesso $\theta$ em ensaios de Bernoulli (e.g., a probabilidade de uma moeda dar "cara"), utilizando uma distribuição Beta como prior conjugada para a likelihood Binomial [^9][^10]. O modelo Dirichlet-Multinomial representa uma generalização direta desse cenário, estendendo a análise para inferir as probabilidades de múltiplos resultados possíveis (e.g., a probabilidade de cada face $k$ de um dado de $K$ faces aparecer) [^14]. Discutiremos a especificação da likelihood Multinomial, a utilização da distribuição de Dirichlet como prior conjugada, a derivação da distribuição posterior e da distribuição preditiva posterior, mantendo a abordagem Bayesiana para a inferência dos parâmetros desconhecidos $\theta$ [^1].

### Conceitos Fundamentais

#### Likelihood Multinomial

Consideremos um experimento com $K$ resultados categóricos mutuamente exclusivos, análogo a lançar um dado de $K$ faces $N$ vezes. Seja $D = \{x_1, ..., x_N\}$ o conjunto de dados observados, onde cada $x_i \in \{1, ..., K\}$ representa o resultado do $i$-ésimo lançamento [^15]. Assumindo que os lançamentos são independentes e identicamente distribuídos (iid), a probabilidade de observar a face $k$ é denotada por $\theta_k$, onde $\theta = (\theta_1, ..., \theta_K)$ é o vetor de parâmetros. Este vetor reside no **simplex de probabilidade $K$-dimensional**, $S_K$, satisfazendo $\theta_k \ge 0$ para todo $k$ e $\sum_{k=1}^K \theta_k = 1$. A **likelihood** de observar o conjunto de dados $D$ dado o vetor de parâmetros $\theta$ é dada pela função de verossimilhança Multinomial [^15]:

$$np(D|\theta) = \prod_{k=1}^K \theta_k^{N_k} \quad \quad (3.36) \text{[^15]}$$

Aqui, $N_k = \sum_{i=1}^N \mathbb{I}(x_i = k)$ representa o número de vezes que o resultado $k$ ocorreu nos $N$ ensaios [^15]. Os contadores $N_1, ..., N_K$ são as **estatísticas suficientes** para este modelo, significando que eles encapsulam toda a informação relevante dos dados $D$ para a inferência de $\theta$ [^10][^15]. É importante notar que a forma funcional da likelihood para o modelo Multinomial é a mesma, a menos de uma constante de normalização (o coeficiente multinomial), que é irrelevante para a inferência Bayesiana de $\theta$ quando $N$ é fixo [^15].

#### Prior Dirichlet

Para realizar a inferência Bayesiana sobre $\theta$, necessitamos de uma distribuição **prior** $p(\theta)$ que atribua densidade de probabilidade sobre o simplex $S_K$ [^15]. Seguindo a abordagem adotada no modelo Beta-Binomial [^10], é computacionalmente conveniente escolher uma **prior conjugada** para a likelihood Multinomial. A distribuição de **Dirichlet** satisfaz essa propriedade [^15]. A densidade de probabilidade da distribuição de Dirichlet é definida como:

$$text{Dir}(\theta|\alpha) = \frac{1}{B(\alpha)} \prod_{k=1}^K \theta_k^{\alpha_k - 1} \quad \quad (3.37) \text{[^15]}$$

onde $\theta \in S_K$, $\alpha = (\alpha_1, ..., \alpha_K)$ é o vetor de **hiperparâmetros** (com $\alpha_k > 0$), e $B(\alpha)$ é a função Beta multinomial, que atua como constante de normalização:

$$B(\alpha) = \frac{\prod_{k=1}^K \Gamma(\alpha_k)}{\Gamma(\sum_{k=1}^K \alpha_k)}$$

Os hiperparâmetros $\alpha_k$ podem ser interpretados como **pseudo-counts** (contagens fictícias) que representam nosso conhecimento prévio sobre a ocorrência de cada face $k$, de forma análoga aos hiperparâmetros $a$ e $b$ na distribuição Beta [^11]. A soma $\alpha_0 = \sum_{k=1}^K \alpha_k$ pode ser vista como o **tamanho amostral efetivo** da prior [^11][^16]. Uma escolha comum para uma prior não informativa (ou fracamente informativa) é a prior uniforme, que corresponde a $\alpha_k = 1$ para todo $k$ [^16], similar ao Beta(1,1) no caso binomial [^10].

#### Inferência Posterior

Dada a likelihood Multinomial (3.36) e a prior Dirichlet (3.37), a distribuição **posterior** $p(\theta|D)$ é obtida através do Teorema de Bayes: $p(\theta|D) \propto p(D|\theta)p(\theta)$. Devido à conjugação entre a Dirichlet e a Multinomial, a posterior também é uma distribuição de Dirichlet [^15]:

$$np(\theta|D) \propto \left( \prod_{k=1}^K \theta_k^{N_k} \right) \left( \frac{1}{B(\alpha)} \prod_{k=1}^K \theta_k^{\alpha_k - 1} \right)$$

$$np(\theta|D) \propto \prod_{k=1}^K \theta_k^{N_k + \alpha_k - 1} \quad \quad (3.39) \text{[^15]}$$

Reconhecemos esta forma funcional como sendo proporcional a uma distribuição de Dirichlet com parâmetros atualizados. Portanto, a distribuição posterior é:

$$np(\theta|D) = \text{Dir}(\theta|\alpha_1 + N_1, ..., \alpha_K + N_K) \quad \quad (3.40) \text{[^15]}$$

> A inferência posterior no modelo Dirichlet-Multinomial consiste simplesmente em adicionar as contagens empíricas observadas ($N_k$) aos pseudo-counts da prior ($\alpha_k$) para obter os parâmetros da distribuição posterior [^16]. Este processo é análogo à atualização no modelo Beta-Binomial, onde $N_1$ e $N_0$ são adicionados a $a$ e $b$, respectivamente [^11].

#### Estimativas Pontuais (MAP e MLE)

Embora a inferência Bayesiana completa utilize a distribuição posterior inteira, frequentemente desejamos estimativas pontuais para o vetor de parâmetros $\theta$. Uma estimativa comum é a **Maximum A Posteriori (MAP)**, que corresponde à moda da distribuição posterior. Para derivar a moda da Dirichlet posterior, precisamos maximizar $\log p(\theta|D)$ sujeito à restrição $\sum_{k=1}^K \theta_k = 1$. Utilizamos um **multiplicador de Lagrange**, $\lambda$, formando a função Lagrangiana [^16]:

$$mathcal{L}(\theta, \lambda) = \log \left( \prod_{k=1}^K \theta_k^{N_k + \alpha_k - 1} \right) + \lambda \left( 1 - \sum_{k=1}^K \theta_k \right)$$

$$mathcal{L}(\theta, \lambda) = \sum_{k=1}^K (N_k + \alpha_k - 1) \log \theta_k + \lambda \left( 1 - \sum_{k=1}^K \theta_k \right) \quad \quad (\approx 3.41) \text{[^16]}$$

Derivando em relação a $\theta_k$ e igualando a zero [^16]:

$$frac{\partial \mathcal{L}}{\partial \theta_k} = \frac{N_k + \alpha_k - 1}{\theta_k} - \lambda = 0 \implies N_k + \alpha_k - 1 = \lambda \theta_k \quad \quad (\approx 3.43, 3.44) \text{[^16]}$$

Somando sobre $k$ e usando a restrição $\sum \theta_k = 1$:

$$sum_{k=1}^K (N_k + \alpha_k - 1) = \lambda \sum_{k=1}^K \theta_k \implies N + \alpha_0 - K = \lambda \quad \quad (\approx 3.45, 3.46) \text{[^16]}$$

onde $N = \sum N_k$ é o número total de observações e $\alpha_0 = \sum \alpha_k$ é a soma dos hiperparâmetros da prior. Substituindo $\lambda$ de volta na equação da derivada, obtemos a estimativa MAP para $\theta_k$:

$$hat{\theta}_{k, MAP} = \frac{N_k + \alpha_k - 1}{N + \alpha_0 - K} \quad \quad (3.47) \text{[^16]}$$

Esta fórmula é consistente com a estimativa MAP para o modelo Beta-Binomial (Equação 3.21 [^12]) quando $K=2$.

Se utilizarmos uma prior uniforme, $\alpha_k = 1$ para todo $k$, então $\alpha_0 = K$. Neste caso, a estimativa MAP se reduz à **Maximum Likelihood Estimate (MLE)** [^16]:

$$hat{\theta}_{k, MLE} = \frac{N_k + 1 - 1}{N + K - K} = \frac{N_k}{N} \quad \quad (3.48) \text{[^16]}$$

A MLE é simplesmente a fração empírica das vezes que o resultado $k$ foi observado [^16], análoga à MLE no caso binomial (Equação 3.22 [^12]).

#### Distribuição Preditiva Posterior

Um objetivo central da inferência Bayesiana é a predição de futuras observações. A **distribuição preditiva posterior** para um único novo ensaio $X$ (e.g., o resultado do próximo lançamento do dado), dado os dados observados $D$, é calculada marginalizando a likelihood do novo dado sobre a posterior dos parâmetros:

$$np(X=j|D) = \int p(X=j|\theta) p(\theta|D) d\theta \quad \quad (3.49) \text{[^17]}$$

Como $p(X=j|\theta) = \theta_j$, a distribuição preditiva é a esperança do parâmetro $\theta_j$ sob a distribuição posterior:

$$np(X=j|D) = \int \theta_j \text{Dir}(\theta|\alpha + N) d\theta = E[\theta_j|D] \quad \quad (\approx 3.50) \text{[^17]}$$

A média da distribuição de Dirichlet $\text{Dir}(\theta|\alpha\')$ é conhecida como $E[\theta_j] = \frac{\alpha\'_j}{\sum_{k=1}^K \alpha\'_k}$. No nosso caso, os parâmetros da posterior são $\alpha\'_k = \alpha_k + N_k$. Portanto, a média posterior é:

$$E[\theta_j|D] = \frac{\alpha_j + N_j}{\sum_{k=1}^K (\alpha_k + N_k)} = \frac{\alpha_j + N_j}{\alpha_0 + N} \quad \quad (3.51) \text{[^17]}$$

Assim, a probabilidade preditiva posterior para o resultado $j$ é:

$$np(X=j|D) = \frac{\alpha_j + N_j}{\alpha_0 + N}$$

Esta expressão é fundamental pois fornece um método de suavização Bayesiana. Mesmo que uma face $j$ nunca tenha sido observada ($N_j = 0$), a probabilidade preditiva $p(X=j|D)$ será $\frac{\alpha_j}{\alpha_0 + N}$, que é maior que zero (assumindo $\alpha_j > 0$). Isso evita o **problema da contagem zero (zero-count problem)**, que pode ocorrer com estimativas MLE plug-in, onde $p(X=j|D) \approx \hat{\theta}_{j, MLE} = 0$ se $N_j=0$ [^13][^17]. Esta suavização é particularmente importante no caso multinomial, onde a esparsidade dos dados (muitos $N_k$ sendo zero) é mais provável do que no caso binomial, especialmente quando $K$ é grande [^17]. A forma de suavização resultante, especialmente com uma prior uniforme ($\alpha_k=1$), é análoga à **regra de sucessão de Laplace** (add-one smoothing) discutida para o modelo Beta-Binomial [^13].

Como exemplo prático, o texto menciona a aplicação deste modelo em **modelagem de linguagem usando bag of words** [^17]. Nesse contexto, $K$ seria o tamanho do vocabulário, $N_k$ a contagem da palavra $k$ em um corpus $D$, e $\theta_k$ a probabilidade de ocorrência da palavra $k$. A distribuição preditiva posterior permite estimar a probabilidade de palavras que não foram vistas no corpus de treinamento, um aspecto crucial para modelos de linguagem robustos [^18].

### Conclusão

O modelo Dirichlet-Multinomial oferece uma estrutura Bayesiana coerente e computacionalmente tratável para a análise de dados categóricos com mais de dois resultados. Ele generaliza naturalmente o modelo Beta-Binomial [^14], substituindo a likelihood Binomial pela Multinomial e a prior Beta pela Dirichlet [^15]. A propriedade de conjugação simplifica a obtenção da distribuição posterior, que também é uma Dirichlet, com parâmetros atualizados pela incorporação das contagens observadas [^15]. A derivação da estimativa MAP [^16] e, mais importante, da distribuição preditiva posterior [^17], demonstra a capacidade do modelo de fornecer estimativas suavizadas que evitam problemas como a contagem zero, sendo particularmente útil em domínios com dados esparsos, como processamento de linguagem natural [^17][^18]. Este modelo serve como bloco de construção fundamental para modelos mais complexos, como classificadores Naive Bayes para features categóricas [^18] e o modelo Dirichlet Compound Multinomial (DCM) [^25].

### Referências

[^1]: Página 1, Seção 3.1, Introdução aos modelos generativos para dados discretos e inferência de parâmetros.
[^8]: Página 8, Seção 3.3, Introdução ao modelo Beta-Binomial.
[^9]: Página 9, Seção 3.3.1, Likelihood Bernoulli/Binomial $p(D|\theta) = \theta^{N_1}(1-\theta)^{N_0}$.
[^10]: Página 10, Seção 3.3.2, Suficiência estatística, Prior Beta $\text{Beta}(\theta|a, b) \propto \theta^{a-1}(1-\theta)^{b-1}$, Conceito de prior conjugada, Prior uniforme Beta(1,1).
[^11]: Página 11, Seção 3.3.3, Posterior Beta $\text{Beta}(\theta|N_1+a, N_0+b)$, Hiperparâmetros como pseudo-counts, Tamanho amostral efetivo $a+b$, Atualização sequencial.
[^12]: Página 12, Seção 3.3.3.1, Estimativas MAP e MLE para Beta-Binomial, Posterior média como combinação convexa, Posterior variância.
[^13]: Página 13, Seção 3.3.4, Distribuição preditiva posterior (ensaio único, média), Problema da contagem zero/paradoxo do cisne negro, Regra de Laplace (add-one smoothing).
[^14]: Página 14, Seção 3.3.4.2 e 3.4, Distribuição preditiva Beta-Binomial para múltiplos ensaios, Introdução ao Dirichlet-Multinomial como generalização para K faces.
[^15]: Página 15, Seção 3.4.1 e 3.4.2, Likelihood Multinomial $p(D|\theta) = \prod \theta_k^{N_k}$, Estatísticas suficientes $N_k$, Parâmetros no simplex, Prior Dirichlet $\text{Dir}(\theta|\alpha)$, Dirichlet como prior conjugada, Posterior Dirichlet $\text{Dir}(\theta|\alpha+N)$.
[^16]: Página 16, Derivação da moda (MAP) da posterior Dirichlet usando multiplicador de Lagrange, Fórmula MAP $\hat{\theta}_k = (N_k+\alpha_k-1)/(N+\alpha_0-K)$, MLE como caso especial $\hat{\theta}_k=N_k/N$.
[^17]: Página 17, Seção 3.4.4, Distribuição preditiva posterior para ensaio único $p(X=j|D) = (\alpha_j+N_j)/(\alpha_0+N)$, Evita problema da contagem zero, Importância no caso multinomial, Exemplo de modelagem de linguagem bag-of-words.
[^18]: Página 18, Seção 3.4.4.1 e 3.5, Continuação do exemplo bag-of-words, Introdução aos classificadores Naive Bayes.
[^25]: Página 25, Menção ao Dirichlet Compound Multinomial (DCM) para capturar "burstiness".

<!-- END -->