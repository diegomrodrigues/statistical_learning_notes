## Estatísticas Suficientes em Modelos de Verossimilhança e Priors Conjugados

### Introdução
Em modelos estatísticos, a inferência sobre parâmetros desconhecidos a partir de dados observados é uma tarefa central. A noção de **estatísticas suficientes** simplifica esse processo, condensando as informações relevantes dos dados em um conjunto menor de valores [^74]. Este capítulo explora o conceito de estatísticas suficientes no contexto de *verossimilhança* e *priors conjugados*, utilizando o exemplo do modelo Beta-Binomial [^73].

### Conceitos Fundamentais
**Estatística Suficiente:** Uma estatística $s(D)$ é dita *suficiente* para um parâmetro $\theta$ se a distribuição posterior de $\theta$ dado os dados $D$ depende apenas de $s(D)$ e não dos dados originais em si [^74]. Matematicamente, isso é expresso como:
$$\np(\theta|D) = p(\theta|s(D))$$
onde $D$ representa o conjunto de dados. Isso implica que $s(D)$ encapsula toda a informação necessária dos dados para inferir $\theta$.

**Exemplo: Modelo de Bernoulli:** Considere uma sequência de lançamentos de moeda, onde $X_i \sim Ber(\theta)$ [^73]. Sejam $N_1$ o número de caras e $N_0$ o número de coroas observadas. A função de verossimilhança é dada por:
$$\np(D|\theta) = \theta^{N_1}(1-\theta)^{N_0}$$
Neste caso, $s(D) = (N_1, N_0)$ são estatísticas suficientes [^74]. Isso significa que, para inferir $\theta$, apenas precisamos saber o número de caras e coroas, não a ordem específica em que ocorreram.

**Prior Conjugado:** Um *prior conjugado* é uma distribuição de probabilidade que, quando combinada com a função de verossimilhança, resulta em uma distribuição posterior da mesma família [^74]. Isso simplifica significativamente os cálculos Bayesianos.

**Exemplo: Beta-Binomial:** No caso do modelo de Bernoulli, o prior conjugado é a distribuição Beta [^74]. A distribuição Beta é dada por:
$$\np(\theta|\alpha, \beta) \propto \theta^{\alpha-1}(1-\theta)^{\beta-1}$$
onde $\alpha$ e $\beta$ são *hiperparâmetros* que codificam nosso conhecimento prévio sobre $\theta$ [^74].

A distribuição posterior é então proporcional ao produto da verossimilhança e do prior:
$$\np(\theta|D) \propto p(D|\theta)p(\theta) \propto \theta^{N_1}(1-\theta)^{N_0} \theta^{\alpha-1}(1-\theta)^{\beta-1} = \theta^{N_1+\alpha-1}(1-\theta)^{N_0+\beta-1}$$
Observe que a posterior também é uma distribuição Beta, com parâmetros atualizados:
$$\np(\theta|D) \sim Beta(N_1 + \alpha, N_0 + \beta)$$

**Interpretação dos Hiperparâmetros:** Os hiperparâmetros $\alpha$ e $\beta$ podem ser interpretados como *pseudo-contagens* [^75]. Eles representam o número de caras e coroas que teríamos observado *antes* de ver os dados reais. A soma $\alpha + \beta$ representa a *força* do prior, também conhecida como o *tamanho efetivo da amostra* do prior [^75].

**Inferência Bayesiana:** A inferência Bayesiana envolve o cálculo da distribuição posterior e o uso dessa distribuição para fazer previsões [^75]. Por exemplo, a média da distribuição posterior é uma estimativa razoável para $\theta$:
$$\n\hat{\theta} = E[\theta|D] = \frac{N_1 + \alpha}{N_1 + N_0 + \alpha + \beta}$$
Essa estimativa é uma combinação convexa da estimativa de máxima verossimilhança (MLE) e da média do prior, ponderada pela força relativa do prior e dos dados [^76].

**Distribuição Preditiva Posterior:** Para prever o resultado de um novo lançamento de moeda, usamos a *distribuição preditiva posterior* [^77]:
$$\np(x=1|D) = \int p(x=1|\theta)p(\theta|D)d\theta = \frac{N_1 + \alpha}{N_1 + N_0 + \alpha + \beta}$$
Neste caso, a distribuição preditiva posterior é equivalente a usar a média da posterior como uma estimativa pontual de $\theta$.

**Oversmoothing e o Paradoxo do Cisne Negro:** Usar a estimativa de máxima verossimilhança (MLE) pode levar ao *oversmoothing*, especialmente quando o tamanho da amostra é pequeno [^77]. Por exemplo, se observarmos apenas coroas, a MLE será $\hat{\theta} = 0$, o que significa que prevemos que caras são impossíveis. Isso é conhecido como o *problema da contagem zero* ou o *problema de dados esparsos*, que é análogo ao *paradoxo do cisne negro* na filosofia [^77].

A inferência Bayesiana com um prior informativo pode mitigar esse problema. Por exemplo, usando um prior uniforme ($\alpha = \beta = 1$), obtemos a *regra de sucessão de Laplace*:
$$\np(x=1|D) = \frac{N_1 + 1}{N_1 + N_0 + 2}$$
Isso justifica a prática comum de adicionar 1 às contagens empíricas, normalizar e então usar essas contagens ajustadas para fazer previsões.

### Conclusão
O conceito de *estatísticas suficientes* simplifica a inferência estatística, condensando informações relevantes dos dados. No contexto de *priors conjugados*, como o modelo Beta-Binomial, as estatísticas suficientes permitem uma atualização fácil do conhecimento prévio, levando a distribuições posteriores analiticamente tratáveis. A inferência Bayesiana, usando a distribuição preditiva posterior, fornece uma abordagem robusta para fazer previsões, mitigando problemas como *oversmoothing* e o *paradoxo do cisne negro* [^77].

### Referências
[^74]:  Chapter 3, page 74
[^73]:  Chapter 3, page 73
[^74]:  Chapter 3, page 74
[^75]:  Chapter 3, page 75
[^76]:  Chapter 3, page 76
[^77]:  Chapter 3, page 77
<!-- END -->