## Suavização em Modelos de Markov Ocultos: O Algoritmo Forward-Backward

### Introdução
Em continuidade ao estudo de Hidden Markov Models (HMMs), este capítulo se aprofunda no conceito de *suavização*, um processo crucial para inferir o estado oculto em um determinado instante, considerando toda a sequência de observações. Em particular, exploraremos o algoritmo *forward-backward*, uma ferramenta fundamental para computar as marginais suavizadas de nós e arestas em um HMM [^610]. Este algoritmo, também conhecido como algoritmo de *inferência offline*, permite estimar a probabilidade de um estado oculto em um determinado momento, dado todo o conjunto de dados observados.

### Conceitos Fundamentais

O algoritmo forward-backward é projetado para calcular as probabilidades marginais suavizadas de nós e arestas em um HMM. Essas probabilidades fornecem uma visão abrangente do estado do sistema em cada ponto no tempo, levando em consideração tanto as observações passadas quanto as futuras [^607].

As quantidades computadas pelo algoritmo são:

1.  **Marginais Suavizadas de Nós:** Representadas por $\gamma_{i,t}(j)$, estas indicam a probabilidade de que o estado oculto $z_t$ seja igual a $j$ no instante $t$, dado a sequência de observações $x_{i,1:T_i}$ e os parâmetros do modelo $\theta$ [^619]. Matematicamente:
    $$     \gamma_{i,t}(j) = p(z_t = j | x_{i,1:T_i}, \theta)\     $$
    onde:
    *   $i$ indexa a sequência de observações.
    *   $t$ representa o instante de tempo.
    *   $j$ denota o estado oculto específico.
    *   $x_{i,1:T_i}$ é a sequência de observações da sequência $i$ do instante 1 até $T_i$.
    *   $\theta$ representa os parâmetros do modelo HMM.

2.  **Marginais Suavizadas de Arestas:** Denotadas por $\xi_{i,t}(j, k)$, estas expressam a probabilidade conjunta de que o estado oculto no instante $t-1$ seja $j$ e o estado oculto no instante $t$ seja $k$, dado a sequência de observações $x_{i,1:T_i}$ e os parâmetros do modelo $\theta$ [^619]. Formalmente:
    $$     \xi_{i,t}(j, k) = p(z_{t-1} = j, z_t = k | x_{i,1:T_i}, \theta)\     $$
    onde:
    *   $i$ indexa a sequência de observações.
    *   $t$ representa o instante de tempo.
    *   $j$ denota o estado oculto no instante $t-1$.
    *   $k$ denota o estado oculto no instante $t$.
    *   $x_{i,1:T_i}$ é a sequência de observações da sequência $i$ do instante 1 até $T_i$.
    *   $\theta$ representa os parâmetros do modelo HMM.

O algoritmo forward-backward calcula essas marginais em duas etapas:

1.  **Passagem Forward (Para Frente):** Calcula as probabilidades *forward* $\alpha_t(j) = p(z_t = j | x_{1:t})$ recursivamente [^609].
2.  **Passagem Backward (Para Trás):** Calcula as probabilidades *backward* $\beta_t(j) = p(x_{t+1:T} | z_t = j)$ recursivamente [^611].

Finalmente, as marginais suavizadas são calculadas combinando as probabilidades *forward* e *backward* [^611]:

$$ \gamma_t(j) \propto \alpha_t(j) \beta_t(j)\ $$

$$ \xi_{t,t+1}(i,j) \propto \alpha_t(i) p(x_{t+1}|z_{t+1}=j) p(z_{t+1}=j|z_t=i) \beta_{t+1}(j)\ $$

Essas estatísticas são *suficientes* para estimar os parâmetros do HMM usando o algoritmo Expectation-Maximization (EM) [^619].

### Conclusão

O algoritmo forward-backward é uma ferramenta poderosa para inferência em HMMs, permitindo a computação das marginais suavizadas de nós e arestas. Essas marginais são cruciais para diversas tarefas, incluindo a estimativa de parâmetros do modelo e a análise do comportamento do sistema ao longo do tempo. Ao considerar tanto as observações passadas quanto as futuras, o algoritmo fornece uma visão mais precisa do estado oculto do sistema do que a simples filtragem ou predição.

### Referências
[^607]: Capítulo 17 "Markov and hidden Markov models", Seção 17.4, "Inference in HMMS".
[^609]: Capítulo 17 "Markov and hidden Markov models", Seção 17.4.2, "The forwards algorithm".
[^610]: Capítulo 17 "Markov and hidden Markov models", Seção 17.4.3, "The forwards-backwards algorithm".
[^611]: Capítulo 17 "Markov and hidden Markov models", Seção 17.4.3, "The forwards-backwards algorithm".
[^619]: Capítulo 17 "Markov and hidden Markov models", Seção 17.5, "Learning for HMMs".

<!-- END -->