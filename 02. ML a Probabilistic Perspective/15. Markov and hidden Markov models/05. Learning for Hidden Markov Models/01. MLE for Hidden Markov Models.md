## Estimação de Parâmetros em HMMs com Dados Completamente Observados

### Introdução
Este capítulo explora a estimação de parâmetros em Hidden Markov Models (HMMs) quando os dados observados estão completamente disponíveis. Em continuidade ao que foi apresentado em seções anteriores sobre HMMs [^603], focaremos em como estimar os parâmetros do modelo, como a matriz de transição e as probabilidades iniciais, através de Maximum Likelihood Estimation (MLE) e inferência Bayesiana com priors conjugados. Também abordaremos como a forma do modelo de observação impacta a estimação dos parâmetros, conectando-o ao processo de ajuste de um classificador generativo.

### Conceitos Fundamentais
Em um HMM, o objetivo é estimar os parâmetros $\theta = (\pi, A, B)$, onde:
*   $\pi(i) = p(z_1 = i)$ é a distribuição do estado inicial.
*   $A(i,j) = p(z_t = j | z_{t-1} = i)$ é a matriz de transição, representando a probabilidade de transição entre estados.
*   $B$ representa os parâmetros das densidades condicionais de classe $p(x_t | z_t = j)$. [^617]

**Estimação com Dados Completamente Observados**
Quando as sequências de estados ocultos $z_{1:T}$ são observadas no conjunto de treinamento, a estimação dos parâmetros se torna mais direta [^617].

**Maximum Likelihood Estimation (MLE)**
Como mencionado na referência [^617], os MLEs para $A$ e $\pi$ podem ser calculados como na Seção 17.2.2.1. Isso significa que podemos estimar esses parâmetros contando as frequências de transições e estados iniciais nos dados observados e normalizando-os.

*   Para a matriz de transição $A$, cada elemento $A_{jk}$ é estimado como a proporção de transições do estado $j$ para o estado $k$ em relação ao número total de transições que saem do estado $j$ [^593]. Matematicamente:

    $$A_{jk} = \frac{N_{jk}}{\sum_{k'} N_{jk'}}$$

    onde $N_{jk}$ é o número de vezes que a transição de $j$ para $k$ é observada nos dados de treinamento [^593].
*   Para a distribuição do estado inicial $\pi$, cada elemento $\pi_j$ é estimado como a proporção de vezes que o estado $j$ é o estado inicial em relação ao número total de sequências de treinamento [^593]. Matematicamente:

    $$\pi_j = \frac{N_j}{\sum_j N_j}$$

    onde $N_j$ é o número de vezes que o estado $j$ é o estado inicial nas sequências de treinamento [^593].

**Inferência Bayesiana com Priors Conjugados**
Se utilizarmos um *prior conjugado*, podemos facilmente calcular a distribuição *posterior* dos parâmetros [^617]. O uso de priors conjugados simplifica o cálculo da *posterior*, pois a *posterior* pertence à mesma família de distribuições que o *prior*.

*   Para a matriz de transição $A$, um prior de Dirichlet é frequentemente usado [^594]. Cada linha da matriz de transição tem um prior de Dirichlet associado:

    $$A_j \sim Dir(\alpha_{j1}, ..., \alpha_{jK})$$

    onde $A_j$ é a j-ésima linha da matriz de transição, $Dir$ denota a distribuição de Dirichlet, e $\alpha_{jk}$ são os hiperparâmetros do prior [^594]. A *posterior* também será uma distribuição de Dirichlet, com hiperparâmetros atualizados com base nos dados observados [^594]:

    $$A_j | D \sim Dir(\alpha_{j1} + N_{j1}, ..., \alpha_{jK} + N_{jK})$$

    onde $D$ representa os dados observados e $N_{jk}$ é o número de transições observadas de $j$ para $k$ [^594].
*   Para a distribuição do estado inicial $\pi$, também podemos usar um prior de Dirichlet:

    $$\pi \sim Dir(\alpha_1, ..., \alpha_K)$$

    A *posterior* é calculada de forma similar, atualizando os hiperparâmetros do *prior* com as contagens dos estados iniciais observados.

**Estimando o Modelo de Observação (B)**
Os detalhes de como estimar $B$ dependem da forma do modelo de observação, de forma idêntica ao ajuste de um classificador generativo [^617]. Por exemplo:

*   **Observações Discretas:** Se as observações são discretas, $B$ pode ser uma matriz de observação onde $B(k, l) = p(x_t = l | z_t = k)$ é a probabilidade de observar o símbolo $l$ no estado $k$ [^604]. A estimativa de $B$ pode ser feita contando as ocorrências de cada símbolo em cada estado e normalizando [^617].

$$B_{jl} = \frac{N_{jl}}{N_j}$$

onde $N_{jl}$ é o número de vezes que o símbolo $l$ é observado no estado $j$, e $N_j$ é o número total de vezes que o estado $j$ é visitado [^618].
*   **Observações Contínuas:** Se as observações são contínuas e modeladas por Gaussianas, então $B$ consiste nos parâmetros da Gaussiana (média $\mu_k$ e covariância $\Sigma_k$) para cada estado $k$ [^604]. Os parâmetros podem ser estimados usando as equações padrão para estimar a média e a covariância de uma distribuição Gaussiana, ponderadas pelas probabilidades de cada ponto de dados pertencer a cada estado [^618].

    $$\mu_k = \frac{\sum_{t=1}^T \mathbb{I}(z_t = k) x_t}{\sum_{t=1}^T \mathbb{I}(z_t = k)}$$

    $$\Sigma_k = \frac{\sum_{t=1}^T \mathbb{I}(z_t = k) (x_t - \mu_k)(x_t - \mu_k)^T}{\sum_{t=1}^T \mathbb{I}(z_t = k)}$$

    onde $\mathbb{I}(z_t = k)$ é uma função indicadora que é 1 se o estado no tempo $t$ é $k$ e 0 caso contrário [^618].

### Conclusão
A estimação de parâmetros em HMMs com dados completamente observados é um processo relativamente direto, especialmente quando se utiliza MLE ou inferência Bayesiana com priors conjugados. A escolha do modelo de observação influencia os detalhes da estimação de $B$, mas o princípio geral de contar ocorrências e normalizar permanece o mesmo. Este capítulo forneceu uma visão geral desses métodos, preparando o terreno para a exploração de cenários mais complexos onde os dados de estado oculto não são observados, conforme discutido nas seções subsequentes [^617].

### Referências
[^593]: Seção 17.2.2.1 do texto.
[^594]: Seção 17.3.2.2 do texto.
[^603]: Seção 17.3 do texto.
[^604]: Seção 17.3 do texto.
[^617]: Seção inicial do contexto fornecido.
[^618]: Seção 17.5 do texto.
<!-- END -->