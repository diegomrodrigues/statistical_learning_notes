## Empirical Bayes Deleted Interpolation in Markov Language Models

### Introdução

A modelagem de linguagem utilizando modelos de Markov enfrenta desafios significativos quando lidamos com dados esparsos [^5]. A esparsidade dos dados ocorre quando certas sequências de palavras (n-gramas) são raras ou ausentes no conjunto de treinamento, levando a estimativas de probabilidade imprecisas. Uma técnica comum para mitigar esse problema é a **interpolação deletada**, que combina as frequências de n-gramas de diferentes ordens para obter estimativas mais robustas. Uma variação dessa técnica é a **Empirical Bayes version of deleted interpolation**, que será o foco deste capítulo.

### Conceitos Fundamentais

A Empirical Bayes version of deleted interpolation aborda o problema de dados esparsos definindo a matriz de transição como uma combinação convexa das frequências de bigramas e unigramas [^5]:

$$ A_{jk} = (1 - \lambda) f_{jk} + \lambda f_k $$

onde:
*   $A_{jk}$ representa a probabilidade de transição do estado $j$ para o estado $k$.
*   $f_{jk}$ é a frequência do bigrama, que é a frequência normalizada de ocorrência da sequência de palavras $j$ seguida por $k$. É calculada como $f_{jk} = N_{jk} / N_j$, onde $N_{jk}$ é o número de vezes que o bigrama $(j, k)$ aparece nos dados de treinamento, e $N_j$ é o número de vezes que a palavra $j$ aparece.
*   $f_k$ é a frequência do unigrama, que é a frequência normalizada de ocorrência da palavra $k$. É calculada como $f_k = N_k / N$, onde $N_k$ é o número de vezes que a palavra $k$ aparece nos dados de treinamento, e $N$ é o número total de palavras no conjunto de treinamento.
*   $\lambda$ é um parâmetro de interpolação que controla o peso relativo das frequências de bigramas e unigramas [^5]. Este parâmetro é tipicamente definido por validação cruzada ou técnicas de *backoff smoothing* [^5].

A intuição por trás dessa abordagem é que, se a frequência do bigrama $f_{jk}$ é baixa (devido à esparsidade dos dados), a probabilidade de transição $A_{jk}$ é influenciada pela frequência do unigrama $f_k$, que representa uma estimativa mais generalizada e robusta [^5]. O parâmetro $\lambda$ permite ajustar a importância relativa dessas duas estimativas, equilibrando a especificidade dos bigramas com a generalidade dos unigramas.

**Justificativa Bayesiana Hierárquica**

A técnica de *deleted interpolation* pode ser vista como uma aproximação das predições feitas por um modelo Bayesiano hierárquico simples [^5]. Nesse modelo, assume-se um *prior* de Dirichlet independente em cada linha da matriz de transição:

$$ \Delta_j \sim Dir(\alpha_0 m_1, ..., \alpha_0 m_K) = Dir(\alpha_0 \mathbf{m}) = Dir(\mathbf{\alpha}) $$

onde:
*   $\Delta_j$ é a linha $j$ da matriz de transição.
*   $\mathbf{m}$ é a média *a priori*, satisfazendo $\sum_k m_k = 1$.
*   $\alpha_0$ é a força *a priori*.

O *posterior* é dado por:

$$ \Delta_j \sim Dir(\mathbf{\alpha} + \mathbf{N}_j) $$

onde $\mathbf{N}_j = (N_{j1}, ..., N_{jK})$ é o vetor que registra o número de vezes que transitamos do estado $j$ para cada um dos outros estados [^5]. A densidade preditiva *posterior* é:

$$ p(X_{t+1} = k | X_t = j, \mathcal{D}) = \mathbb{E}[A_{jk} | \mathcal{D}, \alpha] = \frac{N_{jk} + \alpha m_k}{N_j + \alpha_0} = (1 - \lambda_j) f_{jk} + \lambda_j m_k $$

onde $\lambda_j = \frac{\alpha}{N_j + \alpha_0}$ [^5].

Essa formulação é semelhante à equação da *deleted interpolation*, mas com algumas diferenças importantes:

1.  O peso $\lambda_j$ é dependente do contexto, ao invés de fixo [^5].
2.  Em vez de fazer *backoff* para as frequências marginais empíricas $f_k$, fazemos *backoff* para o parâmetro do modelo $m_k$ [^5].

A questão que permanece é como escolher os valores para $\alpha$ e $\mathbf{m}$ [^5]. Uma abordagem é usar o Empirical Bayes, onde os *priors* são estimados a partir dos dados [^5]. Assumindo que cada linha da matriz de transição é *a priori* independente dado $\alpha$, a verossimilhança marginal para nosso modelo de Markov é encontrada aplicando a Equation 5.24 em cada linha [^5]:

$$ p(\mathcal{D} | \alpha) = \prod_j \frac{B(\mathbf{N}_j + \mathbf{\alpha})}{B(\mathbf{\alpha})} $$

onde $\mathbf{N}_j = (N_{j1}, ..., N_{jK})$ são as contagens para sair do estado $j$ e $B(\mathbf{\alpha})$ é a função beta generalizada [^5].

Podemos ajustar isso usando os métodos discutidos em (Minka 2000e) [^5]. No entanto, também podemos usar a seguinte aproximação (McKay and Peto 1995, p12) [^5]:

$$ m_k \propto |\{j: N_{jk} > 0\}| $$

Isso significa que a probabilidade *a priori* da palavra $k$ é dada pelo número de diferentes contextos em que ela ocorre, em vez do número de vezes que ela ocorre [^5].

### Conclusão

A Empirical Bayes version of deleted interpolation oferece uma abordagem eficaz para lidar com o problema de dados esparsos em modelos de linguagem de Markov. Ao combinar as frequências de bigramas e unigramas de forma ponderada, essa técnica permite obter estimativas de probabilidade mais robustas e generalizáveis. A interpretação Bayesiana hierárquica fornece uma justificativa teórica para essa abordagem, permitindo uma melhor compreensão dos *trade-offs* entre especificidade e generalidade. Embora existam outras técnicas mais sofisticadas, como o *interpolated Kneser-Ney* [^5], a *deleted interpolation* continua sendo uma ferramenta útil e amplamente utilizada na modelagem de linguagem [^5].

### Referências

[^5]: Trechos extraídos do contexto fornecido no prompt.
<!-- END -->