## Lidando com Contagens Nulas em Modelos N-grama: Técnicas de Suavização

### Introdução
Modelos de linguagem Markovianos, particularmente os modelos n-grama, são ferramentas poderosas para modelar sequências de palavras [^5, ^17]. No entanto, quando o número de estados \\(K\\) (tamanho do vocabulário) ou a ordem da cadeia \\(n\\) (tamanho do n-grama) é grande, surge o problema das **contagens nulas**, levando à *esparsidade de dados* e *overfitting* [^5]. Este capítulo explora este problema em profundidade e discute técnicas de suavização para mitigar seus efeitos.

### Conceitos Fundamentais

#### O Problema das Contagens Nulas
Em modelos n-grama, estimamos a probabilidade de uma palavra dada uma sequência de \\(n-1\\) palavras precedentes. A probabilidade de uma sequência de palavras \\(x_{1:T}\\) é expressa como [^17]:

$$ p(x_{1:T}|\theta) = \prod_{t=2}^T p(x_t | x_{t-1}) $$

onde \\(\theta\\) representa os parâmetros do modelo. A estimativa de máxima verossimilhança (MLE) para as probabilidades de transição é dada pelas contagens normalizadas [^17]:

$$ A_{jk} = \frac{N_{jk}}{\sum_k N_{jk}} $$

onde \\(N_{jk}\\) é o número de vezes que a palavra \\(k\\) segue a palavra \\(j\\) no conjunto de treinamento [^17].

O problema surge quando um determinado n-grama não aparece no conjunto de treinamento, resultando em \\(N_{jk} = 0\\) para algum par \\(j, k\\). Isso leva a uma probabilidade estimada de zero para esse n-grama, o que pode ser problemático por várias razões [^5]:

1.  **Probabilidade Zero para Sequências Novas:** Se um n-grama com probabilidade zero fizer parte de uma sequência, toda a sequência receberá uma probabilidade de zero, mesmo que seja uma sequência plausível [^5].
2.  **Overfitting:** O modelo se torna excessivamente específico para o conjunto de treinamento e não generaliza bem para dados não vistos [^5].

#### Técnicas de Suavização
As técnicas de suavização são projetadas para ajustar as estimativas de probabilidade para evitar contagens nulas e melhorar a generalização [^5]. O objetivo é alocar uma pequena quantidade de probabilidade para n-gramas não vistos, "suavizando" a distribuição de probabilidade [^5].

##### Add-One Smoothing (Laplace Smoothing)
Uma das técnicas de suavização mais simples é o **add-one smoothing**, também conhecido como **Laplace smoothing** [^5]. Esta técnica adiciona um a todas as contagens empíricas antes da normalização [^5]:

$$ A_{jk} = \frac{N_{jk} + 1}{\sum_k (N_{jk} + 1)} = \frac{N_{jk} + 1}{\sum_k N_{jk} + K} $$

onde \\(K\\) é o número de estados (tamanho do vocabulário) [^5].

**Justificativa Bayesiana:** O add-one smoothing pode ser justificado a partir de uma perspectiva Bayesiana, como discutido na Seção 3.3.4.1 (não disponível no contexto atual). Essencialmente, ele corresponde a usar um prior de Dirichlet com todos os parâmetros iguais a um [^5].

**Limitações:** Embora simples, o add-one smoothing tem limitações [^5]:

1.  **Suposição Irrealista:** Assume que todos os n-gramas são igualmente prováveis *a priori*, o que geralmente não é verdade [^5].
2.  **Suavização Excessiva:** Pode suavizar demais a distribuição, atribuindo muita probabilidade a n-gramas não vistos e pouca probabilidade a n-gramas frequentes [^5].

##### Abordagens Bayesianas Mais Sofisticadas
Para superar as limitações do add-one smoothing, abordagens Bayesianas mais sofisticadas podem ser empregadas. Estas abordagens envolvem o uso de priors mais informativos que refletem melhor a distribuição esperada dos n-gramas [^5].

Uma dessas abordagens é o uso de um prior de Dirichlet com parâmetros diferentes de um. Por exemplo, poderíamos usar um prior de Dirichlet onde os parâmetros são proporcionais às frequências unigramas das palavras. Isso alocaria mais probabilidade *a priori* para n-gramas que contêm palavras comuns [^5].

Outra abordagem é usar modelos hierárquicos Bayesianos, onde os parâmetros do prior são eles próprios amostrados de uma distribuição superior. Isso permite que o modelo aprenda a forma apropriada do prior a partir dos dados [^5].

##### Métodos Empíricos de Bayes
**Empirical Bayes** é uma abordagem onde os parâmetros do prior são estimados a partir dos dados [^6]. Isso permite que o modelo se adapte às características específicas do conjunto de dados [^6].

Um exemplo de uma técnica Empirical Bayes é a **deleted interpolation**, que combina as probabilidades de n-gramas de diferentes ordens [^5, ^6]. A ideia é que se um n-grama de ordem superior tiver uma contagem baixa, podemos "retroceder" para um n-grama de ordem inferior, que terá uma estimativa mais confiável [^6].

A deleted interpolation pode ser expressa como [^6]:

$$ A_{jk} = (1 - \lambda) f_{jk} + \lambda f_k $$

onde \\(f_{jk}\\) é a frequência do bigrama, \\(f_k\\) é a frequência do unigrama e \\(\lambda\\) é um peso que controla a importância relativa dos dois [^6]. O valor de \\(\lambda\\) é geralmente definido por validação cruzada [^6].

#### Outras Abordagens
##### Acúmulo de Dados
Uma alternativa para usar priors inteligentes é simplesmente coletar muitos dados [^5]. Por exemplo, o Google ajustou modelos n-grama (para \\(n = 1:5\\)) com base em um trilhão de palavras extraídas da web [^5]. Seus dados, que têm mais de 100 GB quando descompactados, estão disponíveis publicamente [^5].

##### Lidando com Palavras Fora do Vocabulário
As técnicas de suavização discutidas acima lidam com o caso em que as contagens são pequenas ou mesmo zero, mas nenhuma delas lida com o caso em que o conjunto de teste pode conter uma palavra completamente nova [^7]. Em particular, todos eles assumem que as palavras no vocabulário (isto é, o espaço de estados de \\(X_t\\)) são fixas e conhecidas (normalmente é o conjunto de palavras únicas nos dados de treinamento ou em algum dicionário) [^7].

Uma heurística padrão para resolver esse problema é substituir todas as novas palavras pelo símbolo especial **UNK**, que significa "desconhecido" [^8]. Uma certa quantidade de massa de probabilidade é reservada para este evento [^8].

Uma solução mais fundamentada seria usar um processo de Dirichlet, que pode gerar um espaço de estados contavelmente infinito, à medida que a quantidade de dados aumenta (ver Seção 25.2.2, não disponível no contexto atual) [^8].

### Conclusão
O problema das contagens nulas é um desafio fundamental em modelos de linguagem n-grama. As técnicas de suavização, como add-one smoothing e abordagens Bayesianas mais sofisticadas, fornecem meios de mitigar este problema, melhorando a generalização e evitando probabilidades zero para sequências não vistas. A escolha da técnica de suavização apropriada depende das características específicas do conjunto de dados e dos requisitos da aplicação.

### Referências
[^5]: Capítulo 17, Markov and hidden Markov models, p. 593.
[^6]: Capítulo 17, Markov and hidden Markov models, p. 594.
[^7]: Capítulo 17, Markov and hidden Markov models, p. 595.
[^8]: Capítulo 17, Markov and hidden Markov models, p. 596.
[^17]: Capítulo 17, Markov and hidden Markov models, p. 592.
<!-- END -->