## Markov Models para Modelagem de Linguagem

### Introdução
Este capítulo explora a aplicação de **modelos de Markov** na **modelagem de linguagem**, um tópico fundamental no processamento de linguagem natural (NLP). Como vimos anteriormente [^1], os modelos de Markov são modelos probabilísticos para sequências de observações. Aqui, vamos detalhar como esses modelos são usados para criar modelos estatísticos de linguagem e suas diversas aplicações [^3].

### Conceitos Fundamentais
Um **modelo de linguagem estatístico** é uma distribuição de probabilidade sobre sequências de palavras [^3]. O espaço de estados em um modelo de Markov para linguagem é definido como o conjunto de palavras em um determinado idioma [^3]. A ideia central é estimar a probabilidade de uma sequência de palavras com base nas probabilidades de transição entre palavras adjacentes na sequência.

**N-gramas:**
Os modelos de Markov de ordem inferior são usados para aproximar a distribuição de probabilidade de sequências de palavras. Os *n-gramas* são sequências contínuas de *n* itens de uma dada amostra de texto ou fala. Em modelos de linguagem, esses itens podem ser palavras, letras ou fonemas.

*   **Unigramas:** As probabilidades marginais $p(X_1 = k)$ são chamadas de **estatísticas de unigramas**. Elas representam a frequência de cada palavra individualmente no corpus [^3].
*   **Bigramas:** Modelos de Markov de primeira ordem $p(X_t = k | X_{t-1} = j)$ são chamados de **modelos de bigramas**. Eles consideram a probabilidade de uma palavra ocorrer dado o conhecimento da palavra anterior [^3]. A matriz de transição **A** [^1], onde $A_{ij} = p(X_t = j | X_{t-1} = i)$, define essas probabilidades.
*   **Trigramas:** Modelos de Markov de segunda ordem $p(X_t = k | X_{t-1} = j, X_{t-2} = i)$ são chamados de **modelos de trigramas**. Eles estendem a dependência para as duas palavras anteriores [^3].
*   **Generalização para n-gramas:** O conceito se generaliza para modelos de *n-gramas*, que consideram as *n-1* palavras anteriores para prever a palavra atual.

**Aplicações de Modelos de Linguagem:**
Os modelos de n-gramas têm uma ampla gama de aplicações [^3]:

*   **Completar Frases (Sentence Completion):** Prever a próxima palavra em uma frase, dado o contexto das palavras anteriores.
*   **Compressão de Dados (Data Compression):** Atribuir códigos mais curtos a sequências de palavras mais prováveis, reduzindo o tamanho dos dados.
*   **Classificação de Texto (Text Classification):** Usar modelos de linguagem como densidades condicionais de classe para classificar documentos. Modelos de 0-gramas (apenas estatísticas de unigramas) são equivalentes a classificadores Naive Bayes [^3].
*   **Escrita Automática de Ensaios (Automatic Essay Writing):** Gerar texto artificial amostrando da distribuição de probabilidade do modelo de linguagem.

**Exemplo:**
Um exemplo de saída de um modelo de 4-gramas treinado em um corpus de notícias é apresentado na Tabela 17.1 [^3]. As primeiras 4 palavras são especificadas manualmente e o modelo gera a 5ª palavra, alimentando os resultados de volta ao modelo.

**Estimativa de Máxima Verossimilhança (MLE) para Modelos de Linguagem de Markov:**
A probabilidade de uma sequência particular de comprimento $T$ é dada por [^4]:
$$ p(x_{1:T}|\theta) = \pi(x_1)A(x_1, x_2) ... A(x_{T-1}, x_T) = \prod_{j=1}^K (\pi_j)^{I(x_1=j)} \prod_{t=2}^T \prod_{j=1}^K \prod_{k=1}^K (A_{jk})^{I(x_t=k, x_{t-1}=j)} $$
Onde $\pi(x_1)$ é a probabilidade da primeira palavra e $A(x_{t-1}, x_t)$ é a probabilidade de transição da palavra $x_{t-1}$ para $x_t$.
A função de log-verossimilhança para um conjunto de sequências $D = (x_1, ..., x_N)$, onde $x_i = (x_{i1}, ..., x_{i,T_i})$ é uma sequência de comprimento $T_i$, é dada por [^4]:
$$ log \, p(D|\theta) = \sum_{i=1}^N log \, p(x_i|\theta) = \sum_j N_j log \, \pi_j + \sum_j \sum_k N_{jk} log \, A_{jk} $$
Onde $N_j$ é o número de vezes que a palavra $j$ aparece como a primeira palavra em uma sequência, e $N_{jk}$ é o número de vezes que a palavra $k$ segue a palavra $j$ em uma sequência.
As estimativas de máxima verossimilhança são então dadas pelas contagens normalizadas [^5]:
$$ \hat{\pi}_j = \frac{N_j}{\sum_j N_j}, \quad \hat{A}_{jk} = \frac{N_{jk}}{\sum_k N_{jk}} $$

**Problemas com Contagens Zero e Suavização:**
Um problema significativo com modelos de n-gramas é a ocorrência de contagens zero. Se um determinado n-grama não aparecer nos dados de treinamento, sua probabilidade será estimada como zero, o que pode levar a problemas ao avaliar a probabilidade de novas sentenças. Uma solução simples é usar a suavização "adicionar um" (add-one smoothing), onde adicionamos um a todas as contagens empíricas antes de normalizar [^5]. No entanto, isso assume que todos os n-gramas são igualmente prováveis, o que nem sempre é realista. Técnicas de suavização mais sofisticadas, como interpolação deletada (deleted interpolation) e suavização de Kneser-Ney, são usadas para abordar essa limitação.

**Interpolação Deletada:**
A interpolação deletada é uma heurística comum usada para lidar com o problema de dados esparsos [^5]. A matriz de transição é definida como uma combinação convexa do bigrama:
$$ A_{jk} = (1 - \lambda) f_{jk} + \lambda f_k $$
Onde $f_{jk} = N_{jk} / N_j$ são as frequências do bigrama e $f_k = N_k / N$ são as frequências do unigrama [^6]. O termo $\lambda$ é normalmente definido por validação cruzada [^6].

### Conclusão
A modelagem de linguagem usando modelos de Markov, particularmente n-gramas, é uma técnica poderosa com diversas aplicações em NLP [^3]. Embora modelos mais simples como unigramas e bigramas forneçam uma base, modelos de n-gramas de ordem superior podem capturar dependências contextuais mais complexas [^3]. No entanto, lidar com o problema de dados esparsos é crucial e várias técnicas de suavização foram desenvolvidas para mitigar esse problema [^5]. Em suma, os modelos de Markov oferecem um *framework* flexível e eficiente para modelar a estrutura estatística da linguagem.

### Referências
[^1]: Section 17.2 Markov models
[^3]: Section 17.2.2 Application: Language modeling
[^4]: Section 17.2.2.1 MLE for Markov language models
[^5]: Section 17.2.2.2 Empirical Bayes version of deleted interpolation
[^6]: Page 594
<!-- END -->