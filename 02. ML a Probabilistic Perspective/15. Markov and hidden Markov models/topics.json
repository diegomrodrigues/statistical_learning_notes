{
  "topics": [
    {
      "topic": "Markov Models",
      "sub_topics": [
        "Markov models are probabilistic models for sequences of observations, where the future state depends only on the current state, a property known as the Markov assumption, making it a sufficient statistic for prediction. This means that the current state encapsulates all relevant information for predicting the future, simplifying the joint distribution of a sequence of states. In mathematical terms, this can be expressed as \\( p(X_{1:T}) = p(X_1) \\prod_{t=2}^{T} p(X_t | X_{t-1}) \\).",
        "Homogeneous Markov chains, also known as stationary or time-invariant chains, assume that the transition function \\( p(X_t | X_{t-1}) \\) is independent of time, allowing the modeling of an arbitrary number of variables with a fixed number of parameters, leading to stochastic processes. This is achieved through parameter tying, where the same parameters are shared across multiple variables.",
        "A transition matrix \\( A \\\\), where \\( A_{ij} \\\\) represents the probability of transitioning from state \\\\( i \\\\) to state \\\\( j \\\\), denoted as \\\\( p(X_t = j | X_{t-1} = i) \\\\), represents the conditional distribution between discrete states. This matrix is a stochastic matrix, meaning each row sums to one (\\\\( \\\\sum_j A_{ij} = 1 \\\\)), ensuring that it accurately describes the probabilities of transitioning between states.",
        "Markov chains can be visualized as state transition diagrams, where nodes represent states, arrows represent legal transitions (non-zero elements of matrix A), and weights on arcs represent transition probabilities, offering a clear graphical representation of the model's dynamics. This visualization is equivalent to a stochastic automaton.",
        "The n-step transition matrix \\( A(n) \\\\), with elements \\\\( A_{ij}(n) = p(X_{t+n} = j | X_t = i) \\\\), specifies the probability of moving from state \\\\( i \\\\) to state \\\\( j \\\\) in exactly \\\\( n \\\\) steps. It can be computed using the Chapman-Kolmogorov equations: \\\\( A_{ij}(m + n) = \\\\sum_{k=1}^{K} A_{ik}(m) A_{kj}(n) \\\\), simplifying to \\\\( A(m + n) = A(m)A(n) \\\\), and simulated by powering up the transition matrix, \\\\( A(n) = A A(n - 1) = A^n \\\\), enabling the analysis of Markov chain behavior over multiple steps.",
        "Markov models are applied in language modeling to create statistical language models, where the state space consists of words in a language. Marginal probabilities \\\\( p(X_1 = k) \\\\) are called unigram statistics; first-order Markov models \\\\( p(X_t = k | X_{t-1} = j) \\\\) are called bigram models; and second-order Markov models \\\\( p(X_t = k | X_{t-1} = j, X_{t-2} = i) \\\\) are called trigram models, generalizing to n-gram models. These models can be used for sentence completion, data compression, text classification, and automatic essay writing.",
        "Maximum Likelihood Estimation (MLE) is used to estimate the transition matrix from training data by calculating normalized counts of transitions between states. However, the problem of zero-counts can arise, especially with large state spaces, leading to techniques like add-one smoothing or more sophisticated Bayesian approaches to address data sparsity. The MLE for the normalized counts can be written as \\\\( \\\\hat{\\\\pi}_j = \\\\frac{N_j}{\\\\sum_j N_j} \\\\) and \\\\( \\\\hat{A}_{jk} = \\\\frac{N_{jk}}{\\\\sum_k N_{jk}} \\\\), where \\\\( N_j \\\\) is the count of initial occurrences of state \\\\( j \\\\) and \\\\( N_{jk} \\\\) is the count of transitions from state \\\\( j \\\\) to state \\\\( k \\\\).",
        "To mitigate the problem of zero-counts in n-gram models, add-one smoothing adds one to all empirical counts before normalizing. A more sophisticated approach is deleted interpolation, which addresses the sparse data problem by defining the transition matrix as a convex combination of bigram and unigram frequencies: \\\\( A_{jk} = (1 - \\\\lambda) f_{jk} + \\\\lambda f_k \\\\), where \\\\( \\\\lambda \\\\) is typically set by cross-validation or using backoff smoothing, and \\\\( f_{jk} = N_{jk} / N_j \\\\) and \\\\( f_k = N_k / N \\\\) are the bigram and unigram frequencies, respectively."
      ]
    },
    {
      "topic": "Stationary Distribution of a Markov Chain",
      "sub_topics": [
        "The stationary distribution, denoted as \\\\( \\\\pi \\\\), represents the long-term distribution of states in a Markov chain, where the probability of being in a particular state remains constant over time. It satisfies the equation \\\\( \\\\pi = \\\\pi A \\\\), also called the invariant distribution or equilibrium distribution, meaning once the chain enters this distribution, it will not leave. It represents the long-term distribution over states.",
        "The global balance equations, \\\\( \\\\pi_i \\\\sum_{j \\\\neq i} A_{ij} = \\\\sum_{j \\\\neq i} \\\\pi_j A_{ji} \\\\), state that the probability of being in state \\\\( i \\\\) times the net flow out of state \\\\( i \\\\) must equal the probability of being in each other state \\\\( j \\\\) times the net flow from that state into \\\\( i \\\\). These equations can be solved subject to the constraint that \\\\( \\\\sum_j \\\\pi_j = 1 \\\\).",
        "To find the stationary distribution, one can solve the eigenvector equation \\\\( A^T v = v \\\\), and then set \\\\( \\\\pi = v^T \\\\), where \\\\( v \\\\) is an eigenvector with eigenvalue 1, normalized to sum to one. A more general approach involves solving \\\\( \\\\pi(I - A) = 0 \\\\) with the constraint \\\\( \\\\pi 1_{K \\\\times 1} = 0 \\\\), replacing any column of \\\\( I - A \\\\) with 1 to get a new matrix \\\\( M \\\\), and solving \\\\( \\\\pi M = r \\\\), where \\\\( r = [0, 0, ..., 1] \\\\), with the 1 in the last position.",
        "A limiting distribution exists if \\\\( \\\\pi_j = \\\\lim_{n \\\\to \\\\infty} A_{ij}^n \\\\) exists and is independent of \\\\( i \\\\) for all \\\\( j \\\\), implying the long-run distribution over states will be independent of the starting state. For a limiting distribution to exist, the Markov chain must be irreducible (singly connected), aperiodic (no cyclical behavior), and recurrent (every state is visited infinitely often).",
        "A state \\\\( i \\\\) is aperiodic if its period \\\\( d(i) = gcd\\\\{t : A_{ii}(t) > 0\\\\} \\\\) is 1, where \\\\( gcd \\\\) stands for greatest common divisor. A chain is aperiodic if all its states are aperiodic. A state is recurrent if the chain will return to that state with probability 1, and ergodic if it is aperiodic, recurrent, and non-null (finite expected time to return).",
        "A Markov chain \\\\( A \\\\) is time reversible if there exists a distribution \\\\( \\\\pi \\\\) such that \\\\( \\\\pi_i A_{ij} = \\\\pi_j A_{ji} \\\\), satisfying the detailed balance equations, which state that the flow from \\\\( i \\\\) to \\\\( j \\\\) must equal the flow from \\\\( j \\\\) to \\\\( i \\\\), weighted by the appropriate source probabilities. If a Markov chain with transition matrix \\\\( A \\\\) is regular and satisfies detailed balance with respect to distribution \\\\( \\\\pi \\\\), then \\\\( \\\\pi \\\\) is a stationary distribution of the chain."
      ]
    },
    {
      "topic": "Google's PageRank Algorithm",
      "sub_topics": [
        "The PageRank algorithm uses the theoretical underpinnings of Markov chains to rank web pages, treating the web as a giant directed graph where nodes represent web pages and edges represent hyperlinks. The algorithm performs web crawling, starting at root nodes and following links to store encountered pages, and then enters words from each page into an inverted index, which lists documents where each word occurs.",
        "The authoritativeness of page \\\\( j \\\\), also called its PageRank, is given by \\\\( \\\\pi_j = \\\\sum_i A_{ij} \\\\pi_i \\\\), where \\\\( A_{ij} \\\\) is the probability of following a link from \\\\( i \\\\) to \\\\( j \\\\), and this is recognized as the stationary distribution of a Markov chain. In the simplest setting, \\\\( A_{ij} \\\\) is a uniform distribution over all states that \\\\( i \\\\) is connected to.",
        "To ensure the distribution is unique, the chain is made regular by allowing each state \\\\( i \\\\) to jump to any other state (including itself) with some small probability, making the transition matrix aperiodic and fully connected, even if the adjacency matrix of the web itself is highly sparse.",
        "To efficiently compute the PageRank vector, let \\\\( G_{ij} = 1 \\\\) iff there is a link from \\\\( j \\\\) to \\\\( i \\\\), and with probability \\\\( p = 0.85 \\\\) you follow one of the outlinks uniformly at random, and with probability \\\\( 1 - p \\\\) you jump to a random node, again chosen uniformly at random, defining the transition matrix as \\\\( M_{ij} = \\\\begin{cases} p G_{ij} / c_j + \\\\delta & \\\\text{if } c_j \\\\neq 0 \\\\\\\\ 1/n & \\\\text{if } c_j = 0 \\\\end{cases} \\\\), where \\\\( n \\\\) is the number of nodes, \\\\( \\\\delta = (1 - p) / n \\\\), and \\\\( c_j = \\\\sum_i G_{ij} \\\\).",
        "The transition matrix can be represented compactly as \\\\( M = pGD + 1z^T \\\\), where \\\\( D \\\\) is a diagonal matrix with entries \\\\( d_{jj} = \\\\begin{cases} 1/c_j & \\\\text{if } c_j \\\\neq 0 \\\\\\\\ 0 & \\\\text{if } c_j = 0 \\\\end{cases} \\\\), and \\\\( z_j = \\\\begin{cases} \\\\delta & \\\\text{if } c_j \\\\neq 0 \\\\\\\\ 1/n & \\\\text{if } c_j = 0 \\\\end{cases} \\\\), and the leading eigenvector of \\\\( M \\\\) can be found using the power method, which consists of repeated matrix-vector multiplication, followed by normalization, \\\\( v \\\\propto Mv = pGDv + 1z^T v \\\\), or by using a Monte Carlo approximation by sampling from the transition matrix."
      ]
    },
    {
      "topic": "Hidden Markov Models (HMMs)",
      "sub_topics": [
        "Hidden Markov Models (HMMs) consist of a discrete-time, discrete-state Markov chain with hidden states \\\\( z_t \\\\in \\\\{1, ..., K\\\\} \\\\) and an observation model \\\\( p(x_t | z_t) \\\\), where the joint distribution has the form \\\\( p(z_{1:T}, x_{1:T}) = p(z_1) \\\\prod_{t=2}^{T} p(z_t | z_{t-1}) \\\\prod_{t=1}^{T} p(x_t | z_t) \\\\). The observations can be discrete, in which case the observation model is commonly an observation matrix \\\\( p(x_t = l | z_t = k, \\\\theta) = B(k, l) \\\\), or continuous, often modeled as a conditional Gaussian \\\\( p(x_t | z_t = k, \\\\theta) = N(x_t | \\\\mu_k, \\\\Sigma_k) \\\\).",
        "HMMs can be used as black-box density models on sequences to represent long-range dependencies between observations mediated via latent variables. They are useful for time-series prediction and defining class-conditional densities inside a generative classifier. Examples of applications include automatic speech recognition, activity recognition, and part-of-speech tagging. Profile HMMs are used in protein sequence alignment, where the model captures consensus sequences of different lengths.",
        "Inference in HMMs involves computing the hidden state sequence given the observed data. Different types of inference include filtering, smoothing, and MAP estimation. Filtering involves computing the belief state \\\\( p(z_t | x_{1:t}) \\\\) online, or recursively, as the data streams in, by applying Bayes' rule in a sequential fashion. Smoothing involves computing \\\\( p(z_t | x_{1:T}) \\\\) offline, given all the evidence, which reduces uncertainty by conditioning on past and future data. MAP estimation involves computing \\\\( \\\\arg \\\\max_{z_{1:T}} p(z_{1:T} | x_{1:T}) \\\\), which is a most probable state sequence, also known as Viterbi decoding.",
        "The forwards algorithm recursively computes the filtered marginals \\\\( p(z_t | x_{1:t}) \\\\) in an HMM using a predict-update cycle. The prediction step computes the one-step-ahead predictive density, and the update step absorbs the observed data using Bayes' rule. The forwards-backwards algorithm computes the smoothed marginals \\\\( p(z_t = j | x_{1:T}) \\\\) using offline inference, combining the filtered belief state with the conditional likelihood of future evidence given the hidden state. This algorithm involves passing messages from left to right (forwards) and then from right to left (backwards), and is also known as belief propagation.",
        "The Viterbi algorithm computes the most probable sequence of states in a chain-structured graphical model, \\\\( z^* = \\\\arg \\\\max_{z_{1:T}} p(z_{1:T} | x_{1:T}) \\\\), by finding the shortest path through the trellis diagram, where the nodes are possible states at each time step and the node and edge weights are log probabilities.",
        "Learning for HMMs involves estimating the parameters \\\\( \\\\theta = (\\\\pi, A, B) \\\\), where \\\\( \\\\pi(i) \\\\) is the initial state distribution, \\\\( A(i,j) \\\\) is the transition matrix, and \\\\( B \\\\) are the parameters of the class-conditional densities \\\\( p(x_t | z_t = j) \\\\). If the hidden state sequences are observed, MLEs can be computed. If not, the EM algorithm (Baum-Welch) is used."
      ]
    },
    {
      "topic": "Learning for Hidden Markov Models",
      "sub_topics": [
        "Given fully observed data in HMMs, MLEs for \\\\( A \\\\) and \\\\( \\\\pi \\\\) can be computed as in Section 17.2.2.1, and if using a conjugate prior, one can easily compute the posterior. Details on how to estimate \\\\( B \\\\) depend on the form of the observation model, identical to fitting a generative classifier.",
        "The EM algorithm for HMMs, also known as the Baum-Welch algorithm, is used when the \\\\( z_t \\\\) variables are not observed, analogous to fitting a mixture model. The E step computes the expected complete data log likelihood, and the M step maximizes this likelihood, normalizing the expected counts for A and \\\\( \\\\pi \\\\), and updating the parameters for the observation model B.",
        "These expected sufficient statistics can be computed by running the forwards-backwards algorithm on each sequence, computing the following smoothed node and edge marginals: \\\\( \\\\gamma_{i,t}(j) = p(z_t = j | x_{i,1:T_i}, \\\\theta) \\\\) and \\\\( \\\\xi_{i,t}(j, k) = p(z_{t-1} = j, z_t = k | x_{i,1:T_i}, \\\\theta) \\\\)."
      ]
    },
    {
      "topic": "MLE for Markov Language Models and Smoothing Techniques",
      "sub_topics": [
        "Estimating the transition matrix from training data involves maximizing the log-likelihood of the observed sequences, leading to normalized counts where \\\\( \\\\pi_j \\\\) (initial state probability) is estimated as \\\\( N_j / \\\\sum_j N_j \\\\) and \\\\( A_{jk} \\\\) (transition probability) is estimated as \\\\( N_{jk} / \\\\sum_k N_{jk} \\\\), with \\\\( N_j \\\\) representing the count of initial occurrences of state \\\\( j \\\\) and \\\\( N_{jk} \\\\) representing the count of transitions from state \\\\( j \\\\) to state \\\\( k \\\\).",
        "The problem of zero-counts in n-gram models arises when the number of states \\\\( K \\\\) or the order of the chain \\\\( n \\\\) is large, leading to data sparsity and overfitting. Solutions include add-one smoothing (Bayesian justification in Section 3.3.4.1), which adds one to all empirical counts before normalizing, and more sophisticated Bayesian approaches or gathering more data.",
        "Empirical Bayes version of deleted interpolation addresses the sparse data problem by defining the transition matrix as a convex combination of bigram and unigram frequencies, \\\\( A_{jk} = (1 - \\\\lambda) f_{jk} + \\\\lambda f_k \\\\), where \\\\( \\\\lambda \\\\) is typically set by cross-validation or backoff smoothing, and \\\\( f_{jk} \\\\) and \\\\( f_k \\\\) are the bigram and unigram frequencies, respectively."
      ]
    },
    {
      "topic": "Conditions for the Existence of a Stationary Distribution",
      "sub_topics": [
        "A necessary condition for a unique stationary distribution is that the state transition diagram be a singly connected component, i.e., we can get from any state to any other state, and such chains are called irreducible.",
        "A chain has a limiting distribution if \\\\( \\\\pi_j = \\\\lim_{n \\\\to \\\\infty} A_{ij}^n \\\\) exists and is independent of \\\\( i \\\\), for all \\\\( j \\\\), and the long-run distribution over states will be independent of the starting state, \\\\( P(X_t = j) = \\\\sum_i P(X_0 = i) A_{ij}(t) \\\\to \\\\pi_j \\\\) as \\\\( t \\\\to \\\\infty \\\\).",
        "The period of state \\\\( i \\\\) is defined as \\\\( d(i) = gcd\\\\{t : A_{ii}(t) > 0\\\\} \\\\), where gcd stands for greatest common divisor, and a state \\\\( i \\\\) is aperiodic if \\\\( d(i) = 1 \\\\), and a chain is aperiodic if all its states are aperiodic.",
        "Every irreducible (singly connected), aperiodic finite state Markov chain has a limiting distribution, which is equal to \\\\( \\\\pi \\\\), its unique stationary distribution, and a regular chain, where \\\\( A_{ij}^n > 0 \\\\) for some integer \\\\( n \\\\) and all \\\\( i, j \\\\), has a unique stationary distribution.",
        "For a stationary distribution to exist, we require irreducibility (singly connected) and aperiodicity, as before, and that each state is recurrent, meaning that you will return to that state with probability 1, and a chain in which all states are recurrent is called a recurrent chain.",
        "A state is ergodic if it is aperiodic, recurrent and non-null, and a chain is ergodic if all its states are ergodic, and every irreducible (singly connected), ergodic Markov chain has a limiting distribution, which is equal to \\\\( \\\\pi \\\\), its unique stationary distribution.",
        "A Markov chain \\\\( A \\\\) is time reversible if there exists a distribution \\\\( \\\\pi \\\\) such that \\\\( \\\\pi_i A_{ij} = \\\\pi_j A_{ji} \\\\), and these are called the detailed balance equations, and if a Markov chain with transition matrix \\\\( A \\\\) is regular and satisfies detailed balance wrt distribution \\\\( \\\\pi \\\\), then \\\\( \\\\pi \\\\) is a stationary distribution of the chain."
      ]
    }
  ]
}