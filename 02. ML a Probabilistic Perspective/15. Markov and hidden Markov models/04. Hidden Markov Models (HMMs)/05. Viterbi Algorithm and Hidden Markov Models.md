## O Algoritmo de Viterbi para Decodificação em HMMs

### Introdução
Em modelos de Markov ocultos (HMMs), um problema fundamental é encontrar a sequência mais provável de estados ocultos dado uma sequência de observações. O algoritmo de Viterbi é uma solução eficiente para este problema, computando a sequência de estados mais provável através da identificação do caminho mais curto em um diagrama de treliça [^1]. Este capítulo detalha o algoritmo de Viterbi, explorando seus fundamentos teóricos e aplicações práticas no contexto de HMMs, com ênfase em sua relação com outros algoritmos de inferência e suas complexidades computacionais.

### Conceitos Fundamentais

O algoritmo de Viterbi é um algoritmo de programação dinâmica utilizado para encontrar a sequência mais provável de estados ocultos em um HMM, dado uma sequência de observações [^1]. Formalmente, o objetivo é encontrar:

$$ z^* = \arg \max_{z_{1:T}} p(z_{1:T} | x_{1:T}) $$

onde $z_{1:T}$ representa a sequência de estados ocultos e $x_{1:T}$ representa a sequência de observações. O algoritmo opera em um **diagrama de treliça**, onde os nós representam os possíveis estados em cada passo de tempo, e as arestas representam as transições entre os estados [^1]. Os pesos dos nós e das arestas são definidos como log-probabilidades, o que permite transformar o problema de maximização em um problema de minimização (caminho mais curto).

#### Funcionamento do Algoritmo
O algoritmo de Viterbi pode ser implementado substituindo o operador *sum* pelo operador *max* no algoritmo *forwards-backwards*, resultando no algoritmo *max-product* [^1]. No entanto, o algoritmo de Viterbi não é tão simples quanto substituir a soma pelo máximo, pois o *backwards pass* usa um procedimento de *traceback* para recuperar o caminho mais provável através da treliça de estados [^1].

O algoritmo de Viterbi é dividido em duas etapas principais:
1. **Inicialização e Recursão (Forwards Pass):**
   - Inicializa a probabilidade do melhor caminho até o primeiro estado [^1].
   - Calcula recursivamente a probabilidade do melhor caminho até cada estado em cada instante de tempo $t$, armazenando também o estado precedente mais provável [^1].
2. **Traceback (Backwards Pass):**
   - Começa no último passo de tempo e retrocede, selecionando o estado precedente que maximiza a probabilidade do caminho [^1].
   - Constrói a sequência de estados mais provável através da recuperação dos estados precedentes armazenados [^1].

#### Detalhes do Algoritmo
1. **Inicialização**:
   - Define $\delta_1(j)$ como a probabilidade de estar no estado $j$ no tempo $t=1$, dado o primeiro sinal de observação. Isso é calculado como:
   $$    \delta_1(j) = \pi_j \phi_1(j)    $$
   onde $\pi_j$ é a probabilidade inicial do estado $j$ e $\phi_1(j)$ é a probabilidade de observar $x_1$ no estado $j$ [^1].

2. **Recursão (Forwards Pass)**:
   - Para cada instante de tempo $t = 2, \dots, T$ e para cada estado $j$, calcula a probabilidade máxima de terminar no estado $j$ no tempo $t$, dado os sinais de observação até $t$:
   $$    \delta_t(j) = \max_i \delta_{t-1}(i) \psi(i, j) \phi_t(j)    $$
   onde $\psi(i, j)$ é a probabilidade de transição do estado $i$ para o estado $j$, e $\phi_t(j)$ é a probabilidade de observar $x_t$ no estado $j$ [^1].
   - Mantém um rastreamento do estado precedente mais provável $\alpha_t(j)$ para cada estado $j$ no tempo $t$:
   $$    \alpha_t(j) = \arg \max_i \delta_{t-1}(i) \psi(i, j) \phi_t(j)    $$
   Este passo garante que, no final, possamos reconstruir o caminho mais provável [^1].

3. **Terminação**:
   - Encontra o estado final mais provável no tempo $T$:
   $$    z_T^* = \arg \max_i \delta_T(i)    $$

4. **Traceback (Backwards Pass)**:
   - Recupera a sequência de estados mais provável começando do tempo $T$ e retrocedendo até $t=1$:
   $$    z_t^* = \alpha_{t+1}(z_{t+1}^*)    $$
   Este passo reconstrói o caminho que maximiza a probabilidade da sequência de estados ocultos dado as observações [^1].

#### Complexidade Computacional
A complexidade de tempo do algoritmo de Viterbi é $O(K^2T)$ em geral, e a complexidade de espaço é $O(KT)$, onde $K$ é o número de estados e $T$ é o comprimento da sequência [^1]. Se a matriz de transição é esparsa, o algoritmo pode ser implementado em $O(TK)$ [^1].

### Relação com MAP e MPE
É importante notar que a sequência de estados mais provável (encontrada pelo algoritmo de Viterbi) não é necessariamente a mesma que a sequência de estados marginalmente mais prováveis [^1]. O algoritmo de Viterbi computa o **MAP (Maximum a Posteriori)**, que é a sequência de estados que maximiza a probabilidade conjunta $p(z_{1:T} | x_{1:T})$, enquanto a sequência de estados marginalmente mais prováveis é dada pelo **MPM (Marginal Posterior Maximization)**, que é o maximizador das marginais posteriores [^1]:

$$ \hat{z} = (\arg \max_{z_1} p(z_1 | x_{1:T}), \dots, \arg \max_{z_T} p(z_T | x_{1:T})) $$

O MAP garante uma sequência globalmente consistente, enquanto o MPM pode resultar em uma sequência inconsistente [^1].

### Conclusão

O algoritmo de Viterbi é uma ferramenta essencial para a decodificação em HMMs, fornecendo a sequência de estados mais provável dado uma sequência de observações [^1]. Sua eficiência e aplicabilidade o tornam fundamental em diversas áreas, como reconhecimento de fala, bioinformática e processamento de linguagem natural [^17]. Embora o algoritmo de Viterbi forneça a sequência de estados mais provável, é importante considerar as diferenças entre MAP e MPM e escolher o método apropriado dependendo da aplicação específica [^1].

### Referências
[^1]: Murphy, Kevin P. *Machine Learning: A Probabilistic Perspective*. MIT Press, 2012.
[^17]: Durbin, R., Eddy, S., Krogh, A., & Mitchison, G. *Biological sequence analysis: probabilistic models of proteins and nucleic acids*. Cambridge university press, 1998.

<!-- END -->