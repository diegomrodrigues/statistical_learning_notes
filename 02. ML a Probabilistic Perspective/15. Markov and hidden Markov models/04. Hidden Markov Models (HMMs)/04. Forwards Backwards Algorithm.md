## Forward-Backward Algorithm in Hidden Markov Models

### Introdução
Este capítulo aprofunda o algoritmo *forwards-backwards* em Hidden Markov Models (HMMs), explorando sua aplicação na inferência *offline* para calcular as marginais suavizadas [^1]. O algoritmo combina as informações do estado de crença filtrado com a verossimilhança condicional das evidências futuras, dado o estado oculto [^1]. Ele envolve a passagem de mensagens da esquerda para a direita (forwards) e da direita para a esquerda (backwards), sendo também conhecido como propagação de crença [^1].

### Conceitos Fundamentais

O algoritmo *forwards-backwards* é uma técnica fundamental para realizar inferência em HMMs quando se deseja obter uma estimativa do estado oculto em um determinado instante, utilizando todas as informações disponíveis ao longo da sequência observada [^1]. Diferentemente do algoritmo *forwards*, que calcula as marginais filtradas *online*, o algoritmo *forwards-backwards* realiza uma inferência *offline*, processando toda a sequência de observações antes de fornecer uma estimativa [^1].

**Algoritmo Forwards (Passagem para Frente):**

O algoritmo *forwards* calcula recursivamente as marginais filtradas $p(z_t | x_{1:t})$ em um HMM usando um ciclo de predição-atualização [^1].

1.  **Predição:** Calcula a densidade preditiva de um passo à frente:

    $$p(z_t = j | x_{1:t-1}) = \sum_i p(z_t = j | z_{t-1} = i) p(z_{t-1} = i | x_{1:t-1})$$ [^1]
2.  **Atualização:** Absorve os dados observados usando a regra de Bayes:

    $$\alpha_t(j) \propto p(z_t = j | x_{1:t}) = \frac{1}{Z_t} p(x_t | z_t = j, x_{1:t-1}) p(z_t = j | x_{1:t-1})$$ [^1]

    onde $Z_t$ é a constante de normalização:

    $$Z_t = p(x_t | x_{1:t-1}) = \sum_j p(z_t = j | x_{1:t-1}) p(x_t | z_t = j)$$ [^1]

Esse processo é conhecido como o ciclo de predição-atualização [^1]. A distribuição $p(z_t | x_{1:t})$ é chamada de estado de crença (filtrado) no tempo $t$ e é um vetor de $K$ números, frequentemente denotado por $\alpha_t$ [^1].

**Algoritmo Backwards (Passagem para Trás):**

O algoritmo *backwards* calcula a verossimilhança condicional das evidências futuras dado o estado oculto, $p(x_{t+1:T} | z_t = j)$, denotada por $\beta_t(j)$ [^1]. Ele é inicializado no final da sequência e retrocede no tempo, atualizando a estimativa com base nas observações futuras [^1]. A base é o caso é:

$$\
\beta_T(i) = p(x_{T+1:T} | z_T = i) = p(\emptyset | z_T = i) = 1
$$

que é a probabilidade de um não-evento [^1].
Recursivamente, podemos computar $\beta_{t-1}$ como segue:

$$\
\beta_{t-1}(i) = p(x_{t:T} | z_{t-1} = i) = \sum_j p(z_t = j, x_t, x_{t+1:T} | z_{t-1} = i)
$$
$$\
= \sum_j p(x_{t+1:T} | z_t = j, z_{t-1} = i, x_t) p(z_t = j, x_t | z_{t-1} = i)
$$
$$\
= \sum_j p(x_{t+1:T} | z_t = j) p(x_t | z_t = j, z_{t-1} = i) p(z_t = j | z_{t-1} = i)
$$
$$\
= \sum_j \beta_t(j) \psi_t(j) \Psi(i,j)
$$

Podemos escrever a equação resultante na forma vetor-matriz como:
$$\
\beta_{t-1} = \Psi (\psi_t \odot \beta_t)
$$
[^1]
onde $\psi_t(j) = p(x_t | z_t = j)$ é a evidência local no tempo $t$, $\Psi(i,j) = p(z_t = j | z_{t-1} = i)$ é a matriz de transição [^1].

**Marginais Suavizadas:**

As marginais suavizadas, $p(z_t = j | x_{1:T})$, combinam as informações dos passos *forwards* e *backwards* [^1]:

$$\gamma_t(j) \propto p(z_t = j | x_{1:T}) \propto p(z_t = j | x_{1:t}) p(x_{t+1:T} | z_t = j) = \alpha_t(j) \beta_t(j)$$ [^1]

**Estimativa de Duas Fatias Suavizadas:**

Ao estimar os parâmetros da matriz de transição usando EM (Expectation-Maximization), é necessário computar o número esperado de transições do estado $i$ para o estado $j$ [^1]:

$$N_{ij} = \sum_{t=1}^{T-1} E[I(z_t = i, z_{t+1} = j) | x_{1:T}] = \sum_{t=1}^{T-1} p(z_t = i, z_{t+1} = j | x_{1:T})$$ [^1]

O termo $p(z_t = i, z_{t+1} = j | x_{1:T})$ é chamado de marginal de duas fatias (suavizada) e pode ser computado como segue [^1]:

$$\
\xi_{t,t+1}(i,j) \propto p(z_t = i, z_{t+1} = j | x_{1:T}) \propto p(z_t | x_{1:t}) p(z_{t+1} | z_t, x_{t+1:T})
$$
$$\
\propto p(z_t | x_{1:t}) p(x_{t+1:T} | z_t, z_{t+1}) p(z_{t+1} | z_t)
$$
$$\
\propto p(z_t | x_{1:t}) p(x_{t+1} | z_{t+1}) p(x_{t+2:T} | z_{t+1}) p(z_{t+1} | z_t)
$$
$$\
= \alpha_t(i) \psi_{t+1}(j) \beta_{t+1}(j) \Psi(i,j)
$$

**Complexidade Temporal e Espacial:**

A implementação direta do algoritmo *forwards-backwards* tem complexidade temporal de $O(K^2T)$, pois envolve uma multiplicação de matriz $K \times K$ em cada passo [^1]. A complexidade espacial é de $O(KT)$ [^1].

### Conclusão

O algoritmo *forwards-backwards* é uma ferramenta poderosa para inferência em HMMs, permitindo a estimativa *offline* das marginais suavizadas [^1]. Ao combinar as informações das passagens *forwards* e *backwards*, o algoritmo fornece uma estimativa mais precisa do estado oculto em cada instante, considerando todas as evidências disponíveis [^1]. Esse algoritmo é fundamental para diversas aplicações, como reconhecimento de fala, bioinformática e análise de séries temporais [^1].

### Referências
[^1]: (Chapter 17, Markov and hidden Markov models)
<!-- END -->