## Spectral Clustering: Graph Cuts and Partitioning

### Introdução
Este capítulo explora o *spectral clustering*, uma técnica de clustering que reformula o problema de agrupamento como um problema de corte de grafo [^901]. Em vez de depender diretamente de medidas de distância no espaço de características, o *spectral clustering* constrói um grafo a partir de uma matriz de similaridade e, em seguida, particiona este grafo de forma a minimizar o corte entre os clusters. Este método tem se mostrado eficaz em situações onde os clusters não são convexos ou quando a estrutura dos dados é melhor capturada pelas relações de conectividade do que pelas distâncias euclidianas [^891].

### Conceitos Fundamentais

#### Grafos e Matrizes de Similaridade
O *spectral clustering* começa com a construção de um **grafo não direcionado ponderado** $W$ a partir da **matriz de similaridade** $S$ [^901]. Cada nó no grafo corresponde a um ponto de dado, e o peso das arestas entre os nós representa a similaridade entre os pontos correspondentes. A matriz de similaridade $S$ pode ser construída usando várias medidas de similaridade, como o kernel Gaussiano [^893]:

$$S_{ij} = exp(-\frac{||x_i - x_j||^2}{2\sigma^2})$$

onde $x_i$ e $x_j$ são os pontos de dados, e $\sigma$ é um parâmetro de escala. Alternativamente, uma matriz de dissimilaridade $D$ pode ser convertida em uma matriz de similaridade $S$ aplicando uma função monotonicamente decrescente, como $D = max(S) - S$ [^875].

Para garantir a **esparsidade do grafo** e acelerar os cálculos, é comum usar os *k-vizinhos mais próximos* de cada ponto para determinar as arestas do grafo [^901]. Isso significa que apenas os *k* pontos mais similares a cada ponto são conectados por arestas, e os pesos das arestas restantes são definidos como zero.

#### O Problema do Corte de Grafo
O objetivo do *spectral clustering* é encontrar uma partição do grafo em *K* clusters ($A_1, A_2, ..., A_K$) de forma a **minimizar o corte entre os clusters** [^901]. O corte entre dois conjuntos de nós $A$ e $B$ é definido como a soma dos pesos das arestas que conectam os nós em $A$ aos nós em $B$:

$$cut(A, B) = \sum_{i \in A, j \in B} W_{ij}$$

Um critério natural para minimizar é:

$$cut(A_1, \dots, A_K) = \frac{1}{2} \sum_{k=1}^K cut(A_k, \bar{A_k})$$

onde $\bar{A_k}$ é o complemento de $A_k$ [^901]. No entanto, minimizar diretamente o corte pode levar a soluções triviais, onde um ou mais clusters contêm apenas um único ponto de dado [^891].

#### Corte Normalizado (Normalized Cut)
Para evitar soluções triviais, o *spectral clustering* usa frequentemente o **corte normalizado** (*Normalized Cut* ou *Ncut*) como critério de otimização [^891]. O *Ncut* é definido como:

$$Ncut(A_1, \dots, A_K) = \sum_{k=1}^K \frac{cut(A_k, \bar{A_k})}{vol(A_k)}$$

onde $vol(A)$ é o volume do conjunto $A$, definido como a soma dos graus ponderados dos nós em $A$:

$$vol(A) = \sum_{i \in A} d_i$$

e $d_i$ é o grau ponderado do nó *i*:

$$d_i = \sum_{j=1}^N W_{ij}$$

O *Ncut* busca equilibrar a minimização do corte entre os clusters com a maximização da densidade dos clusters [^891].

#### O Laplaciano do Grafo
A minimização do *Ncut* pode ser reformulada como um problema de autovalores usando o **Laplaciano do grafo** [^891]. O Laplaciano do grafo $L$ é definido como:

$$L = D - W$$

onde $D$ é uma matriz diagonal com os graus ponderados dos nós na diagonal. O Laplaciano normalizado pode ser definido de duas maneiras:

1.  **Laplaciano Normalizado da Caminhada Aleatória:**
    $$L_{rw} = D^{-1}L = I - D^{-1}W$$
2.  **Laplaciano Normalizado Simétrico:**
    $$L_{sym} = D^{-\frac{1}{2}}LD^{-\frac{1}{2}} = I - D^{-\frac{1}{2}}WD^{-\frac{1}{2}}$$

O *spectral clustering* envolve o cálculo dos autovetores correspondentes aos menores autovalores do Laplaciano normalizado [^891].

#### Algoritmo Geral do Spectral Clustering
O algoritmo geral do *spectral clustering* pode ser resumido da seguinte forma:

1.  **Construção do Grafo:** Construir um grafo ponderado não direcionado a partir da matriz de similaridade $S$.
2.  **Cálculo do Laplaciano:** Calcular o Laplaciano do grafo $L$ (ou um Laplaciano normalizado $L_{rw}$ ou $L_{sym}$).
3.  **Autodecomposição:** Calcular os *K* autovetores $u_1, ..., u_K$ correspondentes aos menores autovalores de $L$ (ou $L_{rw}$ ou $L_{sym}$).
4.  **Formação da Matriz de Características:** Formar a matriz $U = [u_1, ..., u_K]$, onde cada coluna é um autovetor.
5.  **Normalização (para $L_{sym}$):** Normalizar as linhas de $U$ para criar a matriz $T$, onde $T_{ij} = U_{ij} / (\sum_k U_{ik}^2)^{1/2}$.
6.  **Clustering:** Tratar cada linha de $U$ (ou $T$) como um ponto no espaço $\mathbb{R}^K$ e clusterizar os pontos usando o algoritmo *K-means*.
7.  **Atribuição de Clusters:** Atribuir o ponto original $i$ ao cluster *j* se a linha *i* da matriz $U$ (ou $T$) foi atribuída ao cluster *j*.

### Conclusão
O *spectral clustering* oferece uma abordagem poderosa para o agrupamento de dados, especialmente quando a estrutura dos clusters não é bem capturada por métodos tradicionais como o *K-means* [^893]. Ao reformular o problema de clustering como um problema de corte de grafo, o *spectral clustering* é capaz de identificar clusters complexos com base nas relações de conectividade entre os pontos de dados. A escolha do tipo de Laplaciano (não normalizado, normalizado da caminhada aleatória ou normalizado simétrico) e a escolha dos parâmetros (como o parâmetro de escala $\sigma$ no kernel Gaussiano) podem afetar o desempenho do algoritmo.

### Referências
[^875]: Clustering, Measuring (dis)similarity
[^891]: Spectral clustering
[^901]: Spectral clustering, graph cuts
[^893]: Spectral clustering vs K-means

<!-- END -->