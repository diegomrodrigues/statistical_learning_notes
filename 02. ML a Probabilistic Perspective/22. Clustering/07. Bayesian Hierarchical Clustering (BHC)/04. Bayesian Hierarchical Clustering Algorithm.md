## Bayesian Hierarchical Clustering: Algorithm and DPMM Connection

### Introdução
Este capítulo explora o algoritmo de **Bayesian Hierarchical Clustering (BHC)** e sua conexão com **Dirichlet Process Mixture Models (DPMMs)**. O BHC oferece uma abordagem probabilística para o clustering hierárquico, superando algumas limitações das abordagens heurísticas tradicionais [^894]. A ligação com DPMMs fornece uma base teórica sólida e um meio de calcular probabilidades a priori para decisões de merge [^900]. Este capítulo detalha o algoritmo BHC, sua justificativa Bayesiana e sua relação com os DPMMs.

### Conceitos Fundamentais

#### O Algoritmo BHC
O BHC é um algoritmo bottom-up, similar ao clustering aglomerativo padrão, mas com uma diferença crucial: ele usa testes de hipóteses Bayesianas para decidir quais clusters devem ser unidos, em vez de calcular a similaridade entre grupos de pontos de forma *ad hoc* [^899].

O algoritmo começa considerando cada ponto de dados como um cluster individual (singletons) [^900]. Em cada passo, o algoritmo avalia todos os pares possíveis de clusters e calcula a probabilidade de que eles devam ser unidos em um novo cluster [^899]. Essa probabilidade é dada por:

$$
r_{ij} = \frac{p(D_{ij}|M_{ij} = 1)p(M_{ij} = 1)}{p(D_{ij}|M_{ij} = 1)p(M_{ij} = 1) + p(D_{ij}|M_{ij} = 0)p(M_{ij} = 0)}
$$

onde:

*   $D_{i}$ e $D_{j}$ representam os conjuntos de dados nos subárvores $T_i$ e $T_j$, respectivamente [^899].
*   $D_{ij}$ é o conjunto de dados combinado se $T_i$ e $T_j$ forem unidos [^899].
*   $M_{ij} = 1$ indica que os clusters $i$ e $j$ devem ser unidos, e $M_{ij} = 0$ indica que não devem [^899].
*   $p(D_{ij}|M_{ij} = 1)$ é a probabilidade marginal dos dados combinados sob um modelo (por exemplo, um modelo de mistura Gaussiana) [^899].
*   $p(D_{ij}|M_{ij} = 0) = p(D_i|T_i)p(D_j|T_j)$ é a probabilidade dos dados nos dois clusters separados, já computada no processo bottom-up [^899].
*   $p(M_{ij} = 1)$ é a probabilidade *a priori* de um merge, que é crucial para o desempenho do algoritmo [^899].

O par de clusters com a maior probabilidade de merge ($r_{ij}$) é então unido, e o processo é repetido até que todos os clusters sejam unidos [^900]. O resultado é uma árvore hierárquica (dendrograma) que representa a estrutura de cluster dos dados.

#### Conexão com DPMMs
A principal inovação do BHC é sua conexão com DPMMs [^900]. Esta conexão permite que o algoritmo compute a probabilidade *a priori* de um merge, $p(M_{ij} = 1)$, de uma forma Bayesiana consistente. A probabilidade marginal de um DPMM, somando sobre todas as $2^N - 1$ partições, é dada por [^900]:

$$
p(D_k) = \sum_{\nu \in \mathcal{V}} p(\nu) p(D_\nu)
$$

onde:

*   $\mathcal{V}$ é o conjunto de todas as partições possíveis de $D_k$ [^900].
*   $\nu$ representa uma partição específica [^900].
*   $p(\nu)$ é a probabilidade da partição $\nu$ [^900].
*   $p(D_\nu)$ é a probabilidade dos dados sob a partição $\nu$ [^900].

As expressões para $p(\nu)$ e $p(D_\nu)$ são dadas por [^900]:

$$
p(\nu) = \frac{\alpha^{m_\nu} \prod_{l=1}^{m_\nu} \Gamma(n_l^\nu)}{\Gamma(n_k + \alpha)}
$$

$$
p(D_\nu) = \prod_{l=1}^{m_\nu} p(D_l^\nu)
$$

onde:

*   $m_\nu$ é o número de clusters na partição $\nu$ [^900].
*   $n_l^\nu$ é o número de pontos no cluster $l$ da partição $\nu$ [^900].
*   $D_l^\nu$ são os pontos no cluster $l$ da partição $\nu$ [^900].
*   $n_k$ é o número de pontos em $D_k$ [^900].

Heller e Ghahramani (2005) mostraram que $p(D_k|T_k)$ computado pelo algoritmo BHC é similar a $p(D_k)$ acima, exceto que ele soma apenas sobre partições que são consistentes com a árvore $T_k$ [^900]. Isso permite usar o BHC para computar um limite inferior na verossimilhança marginal dos dados de um DPMM [^900].

Para computar $\pi_k = p(M_k = 1)$ para cada nó $k$ com filhos $i$ e $j$, inicializamos $d_i = \alpha$ e $\pi_i = 1$ para cada folha $i$ [^900]. Então, à medida que construímos a árvore, para cada nó interno $k$, computamos [^900]:

$$
d_k = \alpha \Gamma(n_k) + d_i d_j
$$

$$
\pi_k = \frac{\alpha \Gamma(n_k)}{\alpha \Gamma(n_k) + d_i d_j}
$$

onde $i$ e $j$ são os filhos esquerdo e direito de $k$ [^900].

#### Vantagens do BHC
O BHC oferece várias vantagens sobre os métodos de clustering hierárquico tradicionais:

1.  **Base Bayesiana:** Fornece uma estrutura probabilística para decisões de merge, em vez de depender de heurísticas *ad hoc* [^899].
2.  **Seleção Automática de Modelo:** Não requer a especificação prévia do número de clusters [^875]. O algoritmo pode determinar automaticamente o número apropriado de clusters com base nos dados [^875].
3.  **Conexão com DPMMs:** Fornece uma maneira de computar probabilidades *a priori* de merges, levando a decisões de clustering mais informadas [^900].
4.  **Desempenho Empírico:** Demonstra melhor desempenho em vários conjuntos de dados em comparação com algoritmos de clustering aglomerativos tradicionais [^901].

### Conclusão
O Bayesian Hierarchical Clustering oferece uma abordagem poderosa e flexível para o clustering hierárquico [^899]. Sua base Bayesiana, combinada com sua conexão com DPMMs, fornece uma estrutura teórica sólida e permite a seleção automática de modelos [^899]. Embora o algoritmo seja computacionalmente intensivo, suas vantagens o tornam uma escolha atraente para muitas aplicações de clustering [^899].

### Referências
[^875]: Clustering is the process of grouping similar objects together.
[^894]: Note that agglomerative and divisive clustering are both just heuristics, which do not optimize any well-defined objective function.
[^899]: There are several ways to make probabilistic models which produce results similar to hierarchical clustering
[^900]: In this section, we will establish the connection between BHC and DPMMs.
[^901]: (Heller and Ghahramani 2005) compared BHC with traditional agglomerative clustering algorithms on various data sets in terms of purity scores.
<!-- END -->