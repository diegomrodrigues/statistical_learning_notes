## Bayesian Hierarchical Clustering: A Probabilistic Approach to Hierarchical Clustering
### Introdução
O clustering é uma técnica fundamental no campo de para agrupar objetos similares. Conforme introduzido em [^1], existem duas abordagens principais: *similarity-based clustering* e *feature-based clustering*. Além disso, o resultado do clustering pode ser *flat* ou *hierárquico* [^1]. Este capítulo se concentrará no *Bayesian hierarchical clustering (BHC)*, uma abordagem probabilística para o clustering hierárquico baseada em testes de hipóteses Bayesianas [^25]. O BHC utiliza testes de hipóteses Bayesianas para decidir quais clusters devem ser unidos, fornecendo uma estrutura probabilística para o clustering hierárquico baseado em uma matriz de dados [^25]. Esta abordagem supera as limitações dos métodos heurísticos tradicionais de clustering hierárquico, oferecendo uma maneira de avaliar a qualidade do clustering de forma probabilística [^25].

### Conceitos Fundamentais
#### Abordagens Hierárquicas Tradicionais
Os métodos tradicionais de clustering hierárquico, como *agglomerative clustering* e *divisive clustering*, são abordagens heurísticas que não otimizam uma função objetivo bem definida [^25]. O *agglomerative clustering* começa com cada ponto de dados como um cluster separado e, em seguida, mescla iterativamente os clusters mais similares até que todos os pontos de dados pertençam a um único cluster [^25]. O *divisive clustering*, por outro lado, começa com todos os dados em um único cluster e, em seguida, divide recursivamente os clusters em clusters filhos [^25].

#### Bayesian Hierarchical Clustering
O BHC, em contraste, oferece uma abordagem probabilística para o clustering hierárquico [^25]. Ele utiliza testes de hipóteses Bayesianas para decidir quais clusters devem ser mesclados, fornecendo uma estrutura probabilística para o clustering hierárquico baseado em uma matriz de dados [^25]. O BHC é algoritmicamente semelhante ao *agglomerative clustering* padrão, mas usa testes de hipóteses Bayesianas para decidir quais clusters mesclar, em vez de calcular a similaridade entre grupos de pontos de maneira *ad-hoc* [^25].

#### O Algoritmo BHC
O algoritmo BHC pode ser resumido da seguinte forma [^25]:
1. **Inicialização:** Comece com cada ponto de dados como um cluster separado.
2. **Iteração:** Em cada passo, compare todos os pares de clusters e calcule a probabilidade de que eles devam ser mesclados usando um teste de hipótese Bayesiano.
3. **Mesclagem:** Mesclar os dois clusters com a maior probabilidade de mesclagem.
4. **Repetição:** Repita os passos 2 e 3 até que todos os clusters sejam mesclados em um único cluster.

A probabilidade de uma mesclagem entre dois clusters $T_i$ e $T_j$ é dada por [^25]:
$$ r_{ij} = \frac{p(D_{ij}|M_{ij} = 1)p(M_{ij} = 1)}{p(D_{ij}|T_{ij})} $$
onde:
*   $D = \{x_1, ..., x_N\}$ representa todos os dados.
*   $D_i$ é o conjunto de pontos de dados nas folhas da subárvore $T_i$.
*   $D_{ij}$ representa os dados mesclados de $T_i$ e $T_j$.
*   $M_{ij} = 1$ indica que $T_i$ e $T_j$ devem ser mesclados.
*   $p(M_{ij} = 1)$ é a probabilidade *a priori* de uma mesclagem.
*   $p(D_{ij}|M_{ij} = 1)$ é a verossimilhança marginal dos dados combinados sob um único modelo.
*   $p(D_{ij}|M_{ij} = 0) = p(D_i|T_i)p(D_j|T_j)$ é a verossimilhança marginal dos dados sob árvores separadas.

Se $M_{ij} = 1$, os dados em $D_{ij}$ são considerados provenientes do mesmo modelo. Portanto [^25]:
$$ p(D_{ij}|M_{ij} = 1) = \int \prod_{x_n \in D_{ij}} p(x_n|\theta) p(\theta|\lambda) d\theta $$
Se $M_{ij} = 0$, os dados em $D_{ij}$ são considerados gerados por cada árvore independentemente [^25]:
$$ p(D_{ij}|M_{ij} = 0) = p(D_i|T_i) p(D_j|T_j) $$

Esses dois termos já teriam sido computados pelo processo *bottom-up* [^25].

#### Conexão com Dirichlet Process Mixture Models (DPMMs)
O BHC tem uma conexão íntima com os *Dirichlet Process Mixture Models* (DPMMs) [^25]. A verossimilhança marginal de um DPMM, somando sobre todas as $2^N - 1$ partições, é dada por [^25]:
$$ p(D_k) = \sum_{\nu \in \mathcal{V}} p(\nu) p(D_{\nu}) $$
onde $\mathcal{V}$ é o conjunto de todas as partições possíveis de $D_k$, $p(\nu)$ é a probabilidade da partição $\nu$ e $D_{\nu}$ é a probabilidade dos dados dada a partição $\nu$.

A probabilidade da partição $\nu$ é dada por [^25]:
$$ p(\nu) = \alpha^{m_{\nu}} \frac{\prod_{l=1}^{m_{\nu}} \Gamma(n_l^{\nu})}{\Gamma(n_k + \alpha)} $$
onde $m_{\nu}$ é o número de clusters na partição $\nu$, $n_l^{\nu}$ é o número de pontos no cluster $l$ da partição $\nu$ e $n_k$ é o número de pontos em $D_k$.

A probabilidade dos dados dada a partição $\nu$ é dada por [^25]:
$$ p(D_{\nu}) = \prod_{l=1}^{m_{\nu}} p(D_l^{\nu}) $$
onde $D_l^{\nu}$ são os pontos no cluster $l$ da partição $\nu$.

O BHC computa $p(D_k|T_k)$, que é similar a $p(D_k)$, exceto que ele soma apenas sobre partições consistentes com a árvore $T_k$ [^25]. O número de partições consistentes com a árvore é exponencial no número de pontos de dados para árvores binárias balanceadas, mas este é obviamente um subconjunto de todas as partições possíveis [^25]. Desta forma, podemos usar o algoritmo BHC para computar um limite inferior na verossimilhança marginal dos dados de um DPMM [^25]. Além disso, podemos interpretar o algoritmo como uma busca *greedy* através do espaço exponencialmente grande de partições consistentes com a árvore para encontrar as melhores de um determinado tamanho em cada passo [^25].

Para computar $\pi_k = p(M_k = 1)$ para cada nó $k$ com filhos $i$ e $j$, isto é igual à probabilidade do cluster $D_k$ vir do DPMM, relativo a todas as outras partições de $D_k$ consistentes com a árvore atual [^25]. Isso pode ser computado da seguinte forma: inicializar $\alpha_i = \alpha$ e $\pi_i = 1$ para cada folha $i$; então, enquanto construímos a árvore, para cada nó interno $k$, computar $d_k = \alpha \Gamma(n_k) + d_i d_j$, e $\pi_k = \frac{\alpha \Gamma(n_k)}{d_k}$, onde $i$ e $j$ são os filhos esquerdo e direito de $k$ [^25].

#### Resultados Experimentais
Heller e Ghahramani (2005) compararam o BHC com algoritmos de clustering aglomerativos tradicionais em vários conjuntos de dados em termos de pontuações de pureza [^25]. Os resultados são mostrados na Tabela 25.1 [^25]. Vemos que o BHC teve um desempenho muito melhor do que os outros métodos em todos os conjuntos de dados, exceto no conjunto de dados de vidro forense [^25].

### Conclusão
O Bayesian hierarchical clustering (BHC) oferece uma abordagem probabilística para o clustering hierárquico que supera as limitações dos métodos heurísticos tradicionais [^25]. Ao utilizar testes de hipóteses Bayesianas para decidir quais clusters devem ser mesclados, o BHC fornece uma estrutura probabilística para o clustering hierárquico que pode ser usada para avaliar a qualidade do clustering de forma mais rigorosa [^25]. Além disso, a conexão entre o BHC e os DPMMs permite que o algoritmo seja interpretado como uma busca *greedy* através do espaço de partições consistentes com a árvore, fornecendo *insights* adicionais sobre o comportamento do algoritmo [^25].

### Referências
[^1]: Capítulo 25, Seção 25.1
[^25]: Capítulo 25, Seção 25.5.4

<!-- END -->