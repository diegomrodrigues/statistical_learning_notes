## Bayesian Hierarchical Clustering: The Merge Algorithm

### Introdução
Expandindo o conceito de clustering hierárquico introduzido anteriormente [^1], este capítulo explora o algoritmo de **Bayesian Hierarchical Clustering (BHC)**, focando especificamente no processo de decisão de merge entre árvores [^25]. O BHC oferece uma abordagem probabilística para o clustering hierárquico, superando as limitações dos métodos heurísticos tradicionais [^894]. A principal diferença reside na utilização de testes de hipóteses Bayesianas para determinar quais clusters devem ser unidos, em vez de calcular a similaridade entre grupos de pontos de maneira *ad-hoc* [^899]. Este processo de decisão é fundamental para a construção da hierarquia de clusters.

### Conceitos Fundamentais
O algoritmo BHC, de forma similar ao clustering aglomerativo padrão [^899], inicia com cada ponto de dado representando um cluster individual. Em cada passo, o algoritmo compara duas árvores (clusters) para determinar se elas devem ser unidas [^899]. A probabilidade de merge, denotada por $r_{ij}$, é calculada com base nos dados e nas probabilidades *a priori* [^25]:
$$\nr_{ij} = \frac{p(D_{ij}|M_{ij} = 1)p(M_{ij} = 1)}{p(D_{ij}|T_{ij})}\n$$
Onde:
*   $D$ representa todos os dados [^899].
*   $D_i$ é o conjunto de *datapoints* nas folhas da subárvore $T_i$ [^899].
*   $D_{ij}$ representa os dados unidos de $T_i$ e $T_j$ [^899].
*   $M_{ij} = 1$ indica que as árvores $T_i$ e $T_j$ devem ser unidas [^899].
*   $T_{ij}$ representa a união das árvores $T_i$ e $T_j$ [^899].
*   $p(M_{ij} = 1)$ é a probabilidade *a priori* de um merge [^899].

A probabilidade *a priori* de um merge, $p(M_{ij} = 1)$, é crucial. Ela pode ser calculada usando uma abordagem *bottom-up*, conforme descrito a seguir [^899]. Se $M_{ij} = 1$, assume-se que os dados em $D_{ij}$ vêm do mesmo modelo [^899]:
$$\np(D_{ij}|M_{ij} = 1) = \int \prod_{x_n \in D_{ij}} p(x_n|\theta) p(\theta|\lambda) d\theta\n$$
Onde:
*   $x_n$ representa um *datapoint* individual [^899].
*   $\theta$ representa os parâmetros do modelo [^899].
*   $\lambda$ são os *hyperparameters* [^899].

Se $M_{ij} = 0$, assume-se que os dados em $D_{ij}$ foram gerados independentemente por cada árvore [^899]:
$$\np(D_{ij}|M_{ij} = 0) = p(D_i|T_i)p(D_j|T_j)\n$$
Estes termos já foram computados no processo *bottom-up* [^899]. Assim, todas as quantidades necessárias para decidir quais árvores unir estão disponíveis [^899].

**Cálculo da Probabilidade *a Priori* $p(M_{ij} = 1)$**

A conexão com Dirichlet Process Mixture Models (DPMMs) permite computar as probabilidades *a priori* $p(M_{ij} = 1)$ [^900]. A *marginal likelihood* de um DPMM, somando sobre todas as $2^N - 1$ partições, é dada por [^900]:

$$\np(D_k) = \sum_{\nu \in \mathcal{V}} p(\nu) p(D_\nu)\n$$

onde $\mathcal{V}$ é o conjunto de todas as partições possíveis de $D_k$ [^900].

A probabilidade da partição $\nu$ é dada por [^900]:
$$\np(\nu) = \frac{\alpha^{m_\nu} \prod_{l=1}^{m_\nu} \Gamma(n_l^\nu)}{\Gamma(n_k + \alpha)}\n$$
onde:
*   $m_\nu$ é o número de clusters na partição $\nu$ [^900].
*   $n_l^\nu$ é o número de pontos no cluster $l$ da partição $\nu$ [^900].
*   $n_k$ é o número de pontos em $D_k$ [^900].
*   $\alpha$ é o parâmetro de concentração [^900].

A probabilidade dos dados dados a partição $\nu$ é dada por [^900]:
$$\np(D_\nu) = \prod_{l=1}^{m_\nu} p(D_l^\nu)\n$$
onde $D_l^\nu$ são os pontos no cluster $l$ da partição $\nu$ [^900].

A probabilidade $p(D_k|T_k)$ computada pelo algoritmo BHC é similar a $p(D_k)$, exceto que soma apenas sobre partições consistentes com a árvore $T_k$ [^900].
Para cada nó $k$ com filhos $i$ e $j$, a probabilidade *a priori* $π_k = p(M_k = 1)$ é igual à probabilidade do cluster $D_k$ vindo do DPMM, relativo a todas as outras partições de $D_k$ consistentes com a árvore atual [^900].

**Algoritmo**
O algoritmo BHC pode ser resumido da seguinte forma [^900]:
1.  Inicializar cada *datapoint* como um cluster individual: $D_i = \{x_i\}$ [^900].
2.  Calcular $p(D_i|T_i)$ para cada cluster individual [^900].
3.  Repetir [^900]:
    a.  Para cada par de clusters $i$ e $j$, calcular $p(D_{ij}|T_{ij})$ [^900].
    b.  Encontrar o par de clusters $D_i$ e $D_j$ com a maior probabilidade de merge $r_{ij}$ [^900].
    c.  Unir os clusters: $D_k := D_i \cup D_j$ [^900].
    d.  Remover $D_i$ e $D_j$ [^900].
4.  Continuar até que todos os clusters sejam unidos [^900].

### Conclusão
O algoritmo BHC oferece uma abordagem probabilística robusta para o clustering hierárquico [^25]. Ao utilizar testes de hipóteses Bayesianas para determinar merges, ele supera as limitações dos métodos heurísticos tradicionais [^899]. A conexão com DPMMs permite o cálculo das probabilidades *a priori* de merge, tornando o BHC uma ferramenta poderosa para análise de dados complexos [^900]. Além disso, a capacidade de aprender os *hyperparameters* do modelo [^901] e a obtenção de resultados superiores em diversos *datasets* [^901] demonstram a eficácia do BHC em comparação com outras técnicas de clustering hierárquico [^901].

### Referências
[^1]: 25.1 Introduction
[^25]: The algorithm compares two trees to see if they should be merged, defining the probability of a merge based on the data and prior probabilities.
[^894]: Note that agglomerative and divisive clustering are both just heuristics, which do not optimize any well-defined objective function.
[^899]: There are several ways to make probabilistic models which produce results similar to hierarchical clustering, e.g., (Williams 2000; Neal 2003b; Castro et al. 2004; Lau and Green 2006). Here we present one particular approach called Bayesian hierarchical clustering (Heller and Ghahramani 2005). Algorithmically it is very similar to standard bottom-up agglomerative clustering, and takes comparable time, whereas several of the other techniques referenced above are much slower. However, it uses Bayesian hypothesis tests to decide which clusters to merge (if any), rather than computing the similarity between groups of points in some ad-hoc way.
[^900]: In this section, we will establish the connection between BHC and DPMMs. This will in turn give us an algorithm to compute the prior probabilities $p(M_{ij} = 1)$.
[^901]: (Heller and Ghahramani 2005) compared BHC with traditional agglomerative clustering algorithms on various data sets in terms of purity scores.
<!-- END -->