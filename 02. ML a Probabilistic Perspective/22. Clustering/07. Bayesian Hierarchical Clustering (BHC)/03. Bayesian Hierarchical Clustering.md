## Bayesian Hierarchical Clustering: Marginal Likelihood Computation

### Introdução
Este capítulo explora o cálculo da **marginal likelihood** no contexto do **Bayesian Hierarchical Clustering (BHC)**, um método que combina a flexibilidade da inferência bayesiana com a estrutura hierárquica do clustering. O BHC computa a marginal likelihood de um *Dirichlet process mixture model*, somando sobre partições consistentes com a árvore, fornecendo um limite inferior na marginal likelihood dos dados [^1]. Esta abordagem é crucial para a seleção de modelos e para a inferência sobre a estrutura hierárquica dos dados.

### Conceitos Fundamentais
O Bayesian Hierarchical Clustering (BHC) oferece uma abordagem probabilística para o clustering hierárquico, superando as limitações dos métodos heurísticos tradicionais [^899]. Em vez de otimizar uma função objetivo ad-hoc, o BHC utiliza testes de hipóteses bayesianas para decidir quais clusters devem ser unidos, baseando-se na probabilidade marginal dos dados sob diferentes estruturas de árvore [^899].

A **marginal likelihood** desempenha um papel central no BHC. Dado um conjunto de dados $D$ e uma estrutura de árvore $T$, a marginal likelihood $p(D|T)$ quantifica a probabilidade dos dados dada a árvore [^899]. No BHC, a marginal likelihood é calculada somando sobre todas as partições dos dados que são consistentes com a estrutura da árvore.

O BHC estabelece uma conexão entre o clustering hierárquico e os *Dirichlet process mixture models (DPMMs)* [^900]. O DPMM fornece uma estrutura probabilística para modelar a distribuição dos dados, permitindo que o número de clusters cresça à medida que mais dados são observados [^879]. Ao integrar o DPMM com o BHC, é possível calcular a marginal likelihood de forma eficiente, aproveitando a estrutura hierárquica para reduzir a complexidade computacional [^900].

No BHC, a probabilidade de uma junção entre duas árvores $T_i$ e $T_j$ é dada por:

$$
r_{ij} = \frac{p(D_{ij}|M_{ij} = 1)}{p(D_{ij}|T_{ij})} = \frac{p(D_{ij}|M_{ij} = 1)p(M_{ij} = 1)}{p(D_{ij}|M_{ij} = 1)p(M_{ij} = 1) + p(D_{ij}|M_{ij} = 0)p(M_{ij} = 0)}
$$

onde $D_{ij}$ representa os dados combinados das subárvores $T_i$ e $T_j$, e $M_{ij}$ é uma variável indicadora que denota se as árvores devem ser unidas ($M_{ij} = 1$) ou não ($M_{ij} = 0$) [^899, 900]. O termo $p(M_{ij} = 1)$ representa a probabilidade *a priori* de uma junção, que pode ser calculada usando um algoritmo bottom-up [^899].

A **marginal likelihood** $p(D_{ij}|M_{ij} = 1)$ é calculada integrando sobre todos os parâmetros do modelo:

$$
p(D_{ij}|M_{ij} = 1) = \int \prod_{x_n \in D_{ij}} p(x_n|\theta) p(\theta|\lambda) d\theta
$$

onde $p(x_n|\theta)$ é a probabilidade de um ponto de dados $x_n$ dado os parâmetros $\theta$, e $p(\theta|\lambda)$ é a *a priori* sobre os parâmetros [^899]. Se $M_{ij} = 0$, os dados são assumidos como gerados independentemente por cada árvore, e a marginal likelihood é dada por:

$$
p(D_{ij}|M_{ij} = 0) = p(D_i|T_i)p(D_j|T_j)
$$

Estes termos já foram calculados pelo processo bottom-up [^899].

A conexão entre o BHC e o DPMM permite calcular as probabilidades *a priori* $p(M_{ij} = 1)$ [^900]. Dado um nó $k$ com filhos $i$ e $j$, inicializamos $d_i = \alpha$ e $\pi_i = 1$ para cada folha $i$. À medida que construímos a árvore, para cada nó interno $k$, calculamos:

$$
d_k = \alpha\Gamma(n_k) + d_i d_j
$$

$$
\pi_k = \frac{\alpha\Gamma(n_k)}{d_k}
$$

onde $n_k$ é o número de pontos de dados em $D_k$, e $\alpha$ é o parâmetro de concentração do DPMM [^900].

### Conclusão
O BHC oferece uma abordagem bayesiana para o clustering hierárquico, utilizando a marginal likelihood como critério para a seleção de modelos e para a inferência sobre a estrutura da árvore. A conexão com o DPMM permite calcular a marginal likelihood de forma eficiente, somando sobre partições consistentes com a árvore. Este método fornece um limite inferior na marginal likelihood dos dados, oferecendo uma ferramenta poderosa para a análise exploratória de dados e para a descoberta de estruturas hierárquicas em conjuntos de dados complexos.

### Referências
[^1]: Capítulo 25 do texto fornecido
<!-- END -->