## Clustering Fundamentals: In-Depth Exploration

### Introdução
O presente capítulo visa aprofundar o entendimento sobre **Clustering**, uma técnica fundamental de aprendizado não supervisionado [^1]. Exploraremos em detalhes os conceitos, métodos e métricas associadas a essa abordagem, com o objetivo de fornecer uma base sólida para a análise de dados complexos. O clustering, como técnica de agrupamento de objetos semelhantes, desempenha um papel crucial na identificação de padrões e estruturas em dados não rotulados [^1]. Abordaremos tanto o clustering baseado em similaridade quanto o baseado em características, além de discutir as diferentes formas de saída, como clustering plano e hierárquico [^1].

### Conceitos Fundamentais

#### Definição e Tipos de Clustering
**Clustering** é o processo de agrupar objetos similares [^1]. Existem duas abordagens principais quanto ao tipo de entrada utilizada:
*   **Clustering baseado em similaridade:** Utiliza uma matriz de dissimilaridade ou distância $N \times N$ como entrada [^1]. Essa matriz, denotada por $D$, quantifica a "distância" entre cada par de objetos.
*   **Clustering baseado em características:** Utiliza uma matriz de características ou design $N \times D$ como entrada, denotada por $X$ [^1]. Cada linha representa um objeto, e cada coluna representa uma característica.

O clustering baseado em similaridade oferece a flexibilidade de incorporar funções de similaridade ou kernel específicas do domínio [^1]. O clustering baseado em características, por outro lado, é aplicável a dados "brutos" e potencialmente ruidosos [^1].

Além do tipo de entrada, o clustering pode ser classificado de acordo com o tipo de saída:
*   **Clustering plano (particional):** Divide os objetos em conjuntos disjuntos [^1].
*   **Clustering hierárquico:** Cria uma árvore aninhada de partições [^1].

Em geral, o clustering plano é mais rápido (complexidade $O(ND)$) do que o hierárquico (complexidade $O(N^2 \log N)$), mas o clustering hierárquico pode ser mais útil em muitas situações [^1]. Além disso, a maioria dos algoritmos de clustering hierárquico é determinística e não requer a especificação do número de clusters $K$, ao contrário da maioria dos algoritmos de clustering plano [^1].

#### Medindo (Dis)similaridade
A **matriz de dissimilaridade** $D$ é uma matriz onde $d_{i,i} = 0$ e $d_{i,j} \geq 0$ representa uma medida de "distância" entre os objetos $i$ e $j$ [^1]. É importante notar que as dissimilaridades subjetivas raramente são distâncias no sentido estrito, pois a desigualdade triangular ($d_{i,j} \leq d_{i,k} + d_{j,k}$) nem sempre se mantém [^1].

Se tivermos uma **matriz de similaridade** $S$, podemos convertê-la em uma matriz de dissimilaridade aplicando uma função monotonicamente decrescente, como $D = \max(S) - S$ [^1].

A forma mais comum de definir a dissimilaridade entre objetos é em termos da dissimilaridade de seus atributos. A dissimilaridade entre os objetos $x_i$ e $x_{i\'}$ pode ser calculada como a soma das dissimilaridades dos atributos individuais [^2]:

$$ \Delta(x_i, x_{i\'}) = \sum_{j=1}^D \Delta_j(x_{ij}, x_{i\'j}) $$

onde $\Delta_j(x_{ij}, x_{i\'j})$ é a dissimilaridade entre os valores do atributo $j$ para os objetos $i$ e $i\'$.

Algumas funções de dissimilaridade de atributo comuns são [^2]:

*   **Distância Euclidiana Quadrada:** $\Delta_j(x_{ij}, x_{i\'j}) = (x_{ij} - x_{i\'j})^2$. Essa métrica só faz sentido se o atributo $j$ for de valor real.
*   **Distância $l_1$ (City Block):** $\Delta_j(x_{ij}, x_{i\'j}) = |x_{ij} - x_{i\'j}|$. Em 2D, a distância pode ser calculada contando quantas linhas e colunas precisamos mover horizontal e verticalmente para ir de $x_i$ para $x_{i\'}$.
*   **Correlação:** Se $x$ for um vetor (e.g., uma série temporal de dados de valor real), é comum usar o coeficiente de correlação. Se os dados forem padronizados, então $\text{corr}[x_i, x_{i\'}] = \sum_j x_{ij} x_{i\'j}$, e $\sum_j (x_{ij} - x_{i\'j})^2 = 2(1 - \text{corr}[x_i, x_{i\'}])$. Assim, o clustering baseado em correlação (similaridade) é equivalente ao clustering baseado na distância quadrada (dissimilaridade).
*   **Variáveis ordinais:** Para variáveis ordinais, como {baixo, médio, alto}, é padrão codificar os valores como números de valor real, digamos 1/3, 2/3, 3/3 se houver 3 valores possíveis. Podemos então aplicar qualquer função de dissimilaridade para variáveis quantitativas, como a distância quadrada.
*   **Variáveis categóricas:** Para variáveis categóricas, como {vermelho, verde, azul}, geralmente atribuímos uma distância de 1 se as características forem diferentes e uma distância de 0 caso contrário. Somando todas as características categóricas, obtemos a **distância de Hamming**:

$$ \Delta(x_i, x_{i\'}) = \sum_{j=1}^D \mathbb{I}(x_{ij} \neq x_{i\'j}) $$

#### Avaliando a Saída de Métodos de Clustering
A validação das estruturas de clustering é a parte mais difícil e frustrante da análise de clusters [^2]. Sem um forte esforço nessa direção, a análise de clusters permanecerá uma arte obscura acessível apenas àqueles verdadeiros crentes que têm experiência e grande coragem.

Como o clustering é uma técnica de aprendizado não supervisionado, é difícil avaliar a qualidade da saída de qualquer método [^3]. Se usarmos modelos probabilísticos, sempre podemos avaliar a verossimilhança de um conjunto de teste, mas isso tem duas desvantagens: primeiro, não avalia diretamente nenhum clustering que seja descoberto pelo modelo; e segundo, não se aplica a métodos não probabilísticos [^3].

Intuitivamente, o objetivo do clustering é atribuir pontos que são similares ao mesmo cluster e garantir que pontos que são dissimilares estejam em clusters diferentes [^3]. Existem várias maneiras de medir essas quantidades. Uma alternativa é confiar em alguma forma externa de dados com a qual validar o método. Por exemplo, suponha que tenhamos rótulos para cada objeto. Equivalentemente, podemos ter um clustering de referência; dado um clustering, podemos induzir um conjunto de rótulos e vice-versa [^3]. Podemos então comparar o clustering com os rótulos usando várias métricas que descreveremos abaixo [^3].

##### Pureza
Seja $N_{ij}$ o número de objetos no cluster $i$ que pertencem à classe $j$, e seja $N_i = \sum_{j=1}^C N_{ij}$ o número total de objetos no cluster $i$ [^3]. Defina $p_{ij} = N_{ij}/N_i$; esta é a distribuição empírica sobre rótulos de classe para o cluster $i$ [^3]. Definimos a pureza de um cluster como $p_i = \max_j p_{ij}$, e a pureza geral de um clustering como [^3]
$$ \text{purity} = \sum_i \frac{N_i}{N} p_i $$
A pureza varia entre 0 (ruim) e 1 (bom) [^3]. No entanto, podemos trivialmente alcançar uma pureza de 1 colocando cada objeto em seu próprio cluster, então esta medida não penaliza o número de clusters [^3].

##### Índice de Rand
Sejam $U = \{u_1, ..., u_R\}$ e $V = \{v_1, ..., v_C\}$ duas partições diferentes dos $N$ pontos de dados, i.e., dois clusterings (planos) diferentes [^3]. Por exemplo, $U$ pode ser o clustering estimado e $V$ é o clustering de referência derivado dos rótulos de classe. Agora defina uma tabela de contingência 2 × 2, contendo os seguintes números: $TP$ é o número de pares que estão no mesmo cluster em ambos $U$ e $V$ (verdadeiros positivos); $TN$ é o número de pares que estão em clusters diferentes em ambos $U$ e $V$ (verdadeiros negativos); $FN$ é o número de pares que estão em clusters diferentes em $U$ mas no mesmo cluster em $V$ (falsos negativos); e $FP$ é o número de pares que estão no mesmo cluster em $U$ mas em clusters diferentes em $V$ (falsos positivos) [^4]. Uma estatística de resumo comum é o **índice de Rand**:
$$ R = \frac{TP + TN}{TP + FP + FN + TN} $$
Isto pode ser interpretado como a fração de decisões de clustering que estão corretas [^4]. Claramente, $0 \leq R \leq 1$ [^4].

O índice de Rand só atinge seu limite inferior de 0 se $TP = TN = 0$, o que é um evento raro [^4]. Podemos definir um **índice de Rand ajustado** como [^4]:
$$ AR = \frac{\text{index} - \text{expected index}}{\text{max index} - \text{expected index}} $$
O modelo de aleatoriedade é baseado no uso da distribuição hipergeométrica generalizada, i.e., as duas partições são escolhidas aleatoriamente sujeitas a terem o número original de classes e objetos em cada uma, e então o valor esperado de $TP + TN$ é calculado [^4]. Este modelo pode ser usado para computar a significância estatística do índice de Rand [^4].

O índice de Rand pondera falsos positivos e falsos negativos igualmente [^4]. Várias outras estatísticas de resumo para problemas de decisão binária, como o escore F, também podem ser usadas [^4]. Podemos computar sua distribuição de amostragem frequentista e, portanto, sua significância estatística, usando métodos como bootstrap [^4].

##### Informação Mútua
Outra maneira de medir a qualidade do cluster é calcular a informação mútua entre $U$ e $V$ [^4]. Para fazer isso, seja $p_{UV}(i, j) = \frac{|u_i \cap v_j|}{N}$ a probabilidade de que um objeto escolhido aleatoriamente pertença ao cluster $u_i$ em $U$ e $v_j$ em $V$ [^4]. Além disso, seja $p_U(i) = \frac{|u_i|}{N}$ a probabilidade de que um objeto escolhido aleatoriamente pertença ao cluster $u_i$ em $U$; defina $p_V(j) = \frac{|v_j|}{N}$ similarmente [^5]. Então temos [^5]
$$ I(U, V) = \sum_{i=1}^R \sum_{j=1}^C p_{UV}(i, j) \log \frac{p_{UV}(i, j)}{p_U(i) p_V(j)} $$
Isto fica entre 0 e $\min\{H(U), H(V)\}$ [^5]. Infelizmente, o valor máximo pode ser alcançado usando muitos clusters pequenos, que têm baixa entropia [^5]. Para compensar isso, podemos usar a **informação mútua normalizada**,\
$$ NMI(U, V) = \frac{I(U, V)}{\sqrt{H(U) + H(V)}/2} $$
Isto fica entre 0 e 1 [^5].

### Conclusão

Neste capítulo, exploramos os fundamentos do clustering, incluindo os diferentes tipos de entradas (similaridade e características), tipos de saídas (plano e hierárquico) e métricas para avaliar a qualidade dos resultados do clustering [^1]. A compreensão desses conceitos é crucial para aplicar clustering de forma eficaz e interpretar os resultados de maneira significativa. Os conceitos apresentados aqui servem como base para a exploração de algoritmos de clustering específicos e suas aplicações em diversos domínios.

### Referências
[^1]: Clustering
[^2]: 25.1.2 Measuring (dis)similarity
[^3]: 25.1.2.1 Purity
[^4]: 25.1.2.2 Rand index
[^5]: 25.1.2.3 Mutual information
<!-- END -->