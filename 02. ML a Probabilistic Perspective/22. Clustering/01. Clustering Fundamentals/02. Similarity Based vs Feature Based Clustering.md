## Clustering Baseado em Similaridade vs. Clustering Baseado em Features

### Introdução
O processo de **clustering** consiste em agrupar objetos similares [^1]. Existem diferentes abordagens para realizar essa tarefa, dependendo do tipo de entrada utilizada [^1]. Este capítulo explora as diferenças entre o *clustering baseado em similaridade* e o *clustering baseado em features*, detalhando suas características, vantagens e desvantagens [^1].

### Conceitos Fundamentais

#### Clustering Baseado em Similaridade
No *clustering baseado em similaridade*, o algoritmo recebe como entrada uma **matriz de dissimilaridade** (ou matriz de distância) *D* de dimensão *N x N* [^1]. Cada elemento *di,j* dessa matriz representa a dissimilaridade entre os objetos *i* e *j* [^1]. Uma matriz de dissimilaridade *D* é uma matriz onde *di,i* = 0 e *di,j* ≥ 0 é uma medida de "distância" entre os objetos *i* e *j* [^1]. Dissimilaridades subjetivamente julgadas raramente são distâncias no sentido estrito, uma vez que a desigualdade triangular, *di,j* ≤ *di,k* + *dj,k*, frequentemente não se mantém [^1]. Alguns algoritmos requerem que *D* seja uma verdadeira matriz de distância, mas muitos não [^1]. Se tivermos uma matriz de similaridade *S*, podemos convertê-la em uma matriz de dissimilaridade aplicando qualquer função monotonicamente decrescente, por exemplo, *D* = max(*S*) – *S* [^1].

Uma vantagem crucial dessa abordagem é a facilidade de incorporar **funções de similaridade ou kernel** específicas do domínio [^1]. Isso permite que o clustering capture relações complexas entre os objetos, que podem não ser evidentes ao analisar diretamente os atributos [^1].

#### Clustering Baseado em Features
Em contraste, o *clustering baseado em features* utiliza como entrada uma **matriz de features** *X* de dimensão *N x D* [^1]. Cada linha dessa matriz representa um objeto, e cada coluna representa um atributo ou feature [^1].

Essa abordagem permite aplicar o clustering a dados "brutos" e potencialmente ruidosos [^1]. A principal vantagem é a capacidade de lidar com dados diretamente observados, sem a necessidade de definir explicitamente uma função de similaridade [^1]. A forma mais comum de definir a dissimilaridade entre objetos é em termos da dissimilaridade de seus atributos:

$$\
\Delta(x_i, x_{i\'}) = \sum_{j=1}^D \Delta_j(x_{ij}, x_{i\'j})
$$
[^1]

Algumas funções de dissimilaridade de atributos comuns são as seguintes:

*   **Distância Euclidiana Quadrada:**

$$\
\Delta_j(x_{ij}, x_{i\'j}) = (x_{ij} - x_{i\'j})^2
$$

[^1]

Claro, isso só faz sentido se o atributo *j* for de valor real [^1].

*   **Distância Quadrada:**

A distância quadrada enfatiza fortemente grandes diferenças (porque as diferenças são quadradas) [^1]. Uma alternativa mais robusta é usar uma distância *l₁*:\

$$\
\Delta_j(x_{ij}, x_{i\'j}) = |x_{ij} - x_{i\'j}|
$$

[^1]

Isso também é chamado de **distância do bloco da cidade**, já que, em 2D, a distância pode ser computada contando quantas linhas e colunas temos que mover horizontal e verticalmente para ir de *xi* para *xi\'* [^1].

*   Se **x** é um vetor (por exemplo, uma série temporal de dados de valor real), é comum usar o coeficiente de correlação (ver Seção 2.5.1) [^1]. Se os dados forem padronizados, então corr[*xi*, *xi\'*] = ∑*j* *xij* *xi\'j*, e, portanto, ∑*j*(*xij* - *xi\'j*)² = 2(1 - corr[*xi*, *xi\'*]) [^1]. Portanto, o clustering baseado na correlação (similaridade) é equivalente ao clustering baseado na distância quadrada (dissimilaridade) [^1].
*   Para variáveis ordinais, como {baixo, médio, alto}, é padrão codificar os valores como números de valor real, digamos 1/3, 2/3, 3/3 se houver 3 valores possíveis [^1]. Pode-se então aplicar qualquer função de dissimilaridade para variáveis quantitativas, como distância quadrada [^1].
*   Para variáveis categóricas, como {vermelho, verde, azul}, geralmente atribuímos uma distância de 1 se as features forem diferentes e uma distância de 0 caso contrário [^1]. Somar todas as features categóricas fornece:

$$\
\Delta(x_i, x_{i\'}) = \sum_{j=1}^D I(x_{ij} \neq x_{i\'j})
$$

[^1]

Isso é chamado de **distância de Hamming** [^1].

#### Comparação Direta
A escolha entre as duas abordagens depende da natureza dos dados e do conhecimento prévio disponível [^1]. Se a similaridade entre os objetos é bem definida e pode ser expressa por uma função de similaridade apropriada, o *clustering baseado em similaridade* é preferível [^1]. No entanto, se os dados são "brutos" e a definição de similaridade não é trivial, o *clustering baseado em features* é mais adequado [^1].

### Conclusão
O *clustering baseado em similaridade* e o *clustering baseado em features* representam duas abordagens distintas para a tarefa de clustering [^1]. A escolha entre elas depende da disponibilidade de uma função de similaridade específica do domínio e da natureza dos dados [^1]. Ambas as abordagens têm suas vantagens e desvantagens, e a seleção apropriada é crucial para obter resultados significativos [^1].

### Referências
[^1]: Página 1, Clustering Fundamentals.
<!-- END -->