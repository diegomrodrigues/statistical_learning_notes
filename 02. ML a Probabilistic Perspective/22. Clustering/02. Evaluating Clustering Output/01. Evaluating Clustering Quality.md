## Avaliação da Qualidade do Clustering

### Introdução
A avaliação da qualidade do clustering é um desafio inerente devido à sua natureza de aprendizado não supervisionado. Ao contrário de problemas de classificação supervisionada, onde rótulos verdadeiros estão disponíveis para comparação, no clustering, não há uma "resposta correta" predefinida. A dificuldade reside em quantificar a qualidade de agrupamentos descobertos sem um padrão ouro externo [^877]. Este capítulo explora as complexidades da avaliação de clustering, focando em métricas baseadas em probabilidade e suas limitações.

### Conceitos Fundamentais

A avaliação do clustering é complexa devido à ausência de rótulos predefinidos. No entanto, modelos probabilísticos oferecem uma via para avaliar a qualidade do clustering por meio da avaliação da *likelihood* [^877].

*Avaliação via Likelihood:*

Modelos probabilísticos de clustering, como os modelos de mistura finitos (abordados na Seção 11.2.3 [^879]) e os modelos de mistura de processos de Dirichlet (abordados na Seção 25.2 [^879]), permitem calcular a *likelihood* dos dados sob o modelo aprendido. Uma *likelihood* mais alta sugere que o modelo se ajusta melhor aos dados, indicando potencialmente um clustering de maior qualidade.

**Desvantagens da Avaliação via Likelihood:**

1.  *Não Avalia Diretamente o Clustering:* A *likelihood* avalia o ajuste do modelo aos dados, mas não mede diretamente a qualidade dos clusters descobertos [^877]. Um modelo pode ter uma *likelihood* alta, mas ainda assim gerar clusters que não são intuitivos ou úteis.
2.  *Aplicabilidade Limitada:* A avaliação via *likelihood* é inerentemente restrita a métodos de clustering probabilísticos [^877]. Métodos não probabilísticos, como K-means ou *affinity propagation* (abordado na Seção 25.3 [^887]), não fornecem uma *likelihood* que possa ser usada para avaliação.

Como alternativa, o texto apresenta medidas de desempenho não baseadas em *likelihood*, como a *purity* e o *Rand index* [^877].

*Purity:*

A *purity* mede a extensão em que cada cluster contém principalmente objetos de uma única classe [^877]. É calculada como a média ponderada da *purity* de cada cluster. A *purity* de um cluster é definida como a fração do tamanho total do cluster que a classe mais comum representa.

$$
\text{purity} = \sum_i \frac{N_i}{N} p_i
$$

onde $N_{ij}$ é o número de objetos no cluster $i$ que pertencem à classe $j$, $N_i = \sum_{j=1}^C N_{ij}$ é o número total de objetos no cluster $i$, $p_{ij} = N_{ij}/N_i$ é a distribuição empírica sobre rótulos de classe para o cluster $i$, $p_i = \max_j p_{ij}$ é a *purity* de um cluster e $N$ é o número total de pontos de dados.

Uma *purity* alta indica que os clusters correspondem bem às classes verdadeiras, mas essa métrica pode ser enganosa, pois não penaliza o número de clusters [^877]. Por exemplo, atribuir cada objeto ao seu próprio cluster trivialmente resulta em uma *purity* de 1.

*Rand Index:*

O *Rand index* (RI) mede a similaridade entre duas partições de dados, como o clustering estimado e um clustering de referência derivado de rótulos de classe [^877]. Ele quantifica a fração de decisões de clustering que estão corretas. O RI é calculado usando uma tabela de contingência 2x2 que contabiliza pares de pontos que são:

*   No mesmo cluster em ambas as partições (verdadeiros positivos - TP)
*   Em diferentes clusters em ambas as partições (verdadeiros negativos - TN)
*   No mesmo cluster na primeira partição, mas em clusters diferentes na segunda (falsos positivos - FP)
*   Em clusters diferentes na primeira partição, mas no mesmo cluster na segunda (falsos negativos - FN)

$$
R = \frac{TP + TN}{TP + FP + FN + TN}
$$

O *Rand index* varia de 0 a 1, com valores mais altos indicando maior similaridade entre as partições. No entanto, o RI pode atingir seu limite inferior de 0 apenas em casos raros [^878]. Para abordar essa limitação, pode-se usar o *adjusted Rand index* [^878].

*Mutual Information:*

A *mutual information* (MI) quantifica a quantidade de informação que duas variáveis aleatórias (neste caso, as partições de clustering) compartilham [^878]. Em termos de clustering, mede o grau em que o conhecimento de uma partição reduz a incerteza sobre a outra. A *mutual information* é calculada como:

$$
I(U, V) = \sum_{i=1}^R \sum_{j=1}^C p_{UV}(i, j) \log \frac{p_{UV}(i, j)}{p_U(i)p_V(j)}
$$

onde $U$ e $V$ são duas partições dos dados, $p_{UV}(i, j)$ é a probabilidade de um objeto escolhido aleatoriamente pertencer ao cluster $u_i$ em $U$ e ao cluster $v_j$ em $V$, e $p_U(i)$ e $p_V(j)$ são as probabilidades de um objeto pertencer ao cluster $u_i$ em $U$ e ao cluster $v_j$ em $V$, respectivamente.

A *mutual information* varia entre 0 e o mínimo da entropia de $U$ e $V$. Para compensar o fato de que a *mutual information* pode ser maximizada usando muitos clusters pequenos, a *normalized mutual information* (NMI) é frequentemente usada [^879]:

$$
NMI(U, V) = \frac{I(U, V)}{\sqrt{H(U) + H(V)}/2}
$$

A *normalized mutual information* varia entre 0 e 1.

### Conclusão

A avaliação da qualidade do clustering é uma tarefa complexa e multifacetada. Embora os modelos probabilísticos ofereçam uma maneira de avaliar a *likelihood*, essa métrica não avalia diretamente a qualidade do clustering e é aplicável apenas a métodos probabilísticos. Métricas como *purity*, *Rand index* e *mutual information* fornecem medidas alternativas da qualidade do clustering, mas também têm suas próprias limitações. A escolha da métrica de avaliação apropriada depende do contexto específico do problema de clustering e dos objetivos da análise.

### Referências
[^877]: Clustering is an unupervised learning technique, so it is hard to evaluate the quality of the output of any given method. If we use probabilistic models, we can always evaluate the likelihood of a test set, but this has two drawbacks: first, it does not directly assess any clustering that is discovered by the model; and second, it does not apply to non-probabilistic methods.
[^878]: The Rand index only achieves its lower bound of 0 if TP = TN = 0, which is a rare event. One can define an adjusted Rand index (Hubert and Arabie 1985) as follows:
[^879]: This lies between 0 and min{H (U), H(V)}. Unfortunately, the maximum value can be achieved by using lots of small clusters, which have low entropy. To compensate for this, we can use the normalized mutual information,

<!-- END -->