## Multi-view Clustering with Dirichlet Process Priors

### Introdução
Este capítulo expande o conceito de **biclustering** [^903] para **multi-view clustering**, onde um objeto pode pertencer a múltiplos clusters, dependendo do subconjunto de *features* utilizado. Em continuidade ao conceito de **biclustering** ou *co-clustering* [^903], que envolve o agrupamento simultâneo de linhas e colunas de uma matriz de dados, o *multi-view clustering* oferece uma abordagem mais flexível, permitindo que um objeto seja associado a diferentes grupos, dependendo da perspectiva ou conjunto de *features* considerado. Para modelar a distribuição de probabilidade dos clusters, emprega-se um *Dirichlet process prior* [^905], que possibilita o crescimento automático do número de *views* ($V$).

### Conceitos Fundamentais
O *multi-view clustering* surge como uma resposta à limitação do *biclustering*, que restringe cada objeto (linha) a pertencer a um único cluster [^903]. Em muitas situações reais, um objeto pode desempenhar múltiplos papéis ou apresentar características que o associam a diferentes grupos. Por exemplo, no *dataset* de animais [^904], um animal pode ser agrupado com base em características anatômicas (e.g., mamíferos são de sangue quente) ou comportamentais (e.g., predadores vs. presas).

Para modelar essa flexibilidade, o *multi-view clustering* particiona as colunas (features) em $V$ grupos ou *views* [^904, 905], onde $c_j \in \{1, ..., V\}$ indica a qual *view* a *feature* $j$ pertence. Um *Dirichlet process prior* é utilizado para $p(c)$ [^905], permitindo que o número de *views* ($V$) cresça automaticamente. Para cada partição das colunas (ou seja, cada *view* $v$), as linhas são particionadas novamente usando um *Dirichlet process*, onde $r_{iv} \in \{1,...,K^{(v)}\}$ representa o cluster ao qual a linha $i$ pertence na *view* $v$ [^905].

Após particionar as linhas e colunas, os dados são gerados assumindo que todas as linhas e colunas dentro de um bloco são *iid* [^905]. O modelo pode ser definido de forma mais precisa como:
$$np(c, r, D) = p(c)p(r|c)p(D|r, c) \quad [^905]$$
onde
$$np(c) = DP(\alpha) \quad [^905]$$
$$np(r|c) = \prod_{v=1}^{V(c)} DP(r_v|\beta) \quad [^905]$$
$$np(D|r, c, \theta) = \prod_{v=1}^{V(c)} \prod_{j: c_j=v} \prod_{k=1}^{K^{(r_v)}} [\int \prod_{i: r_{iv}=k} p(x_{ij}|\theta_{jk})p(\theta_{jk})d\theta_{jk}] \quad [^95]$$

onde $DP(\alpha)$ representa o *Dirichlet Process* com parâmetro $\alpha$, $V(c)$ é o número de *views*, $r_v$ é a partição das linhas para a *view* $v$, $K^{(r_v)}$ é o número de clusters na partição das linhas para a *view* $v$, e $\theta_{jk}$ são os parâmetros para o cluster $j$ na *view* $k$.

Para dados binários, utilizando um *Beta(γ, γ)* prior para $\theta_{jk}$, a *likelihood* se reduz a:
$$np(D|r, c, \gamma) = \prod_{v=1}^{V(c)} \prod_{j: c_j=v} \prod_{k=1}^{K^{(r_v)}} \frac{Beta(n_{j,k,v} + \gamma, \bar{n}_{j,k,v} + \gamma)}{Beta(\gamma, \gamma)} \quad [^905]$$

onde $n_{j,k,v} = \sum_{i: r_{iv}=k} I(x_{ij}=1)$ conta o número de *features* que estão ativas na coluna $j$ para a *view* $v$ e para o cluster de linhas $k$, e $\bar{n}_{j,k,v}$ conta quantas *features* estão inativas [^905].

A estimação aproximada de MAP pode ser feita usando busca estocástica [^905]. A inferência aproximada pode ser realizada usando *Variational Bayes* ou *Gibbs sampling* [^905].

### Conclusão
O *multi-view clustering* oferece uma abordagem flexível e poderosa para o agrupamento de dados, permitindo que objetos pertençam a múltiplos clusters dependendo do subconjunto de *features* considerado. A utilização de um *Dirichlet process prior* para modelar a distribuição dos clusters possibilita o crescimento automático do número de *views*, adaptando-se à complexidade dos dados. Modelos como o *crosscat/multi-clust* [^905] demonstram a capacidade de lidar com dados complexos e identificar padrões relevantes, superando as limitações de abordagens mais tradicionais.

### Referências
[^903]: Capítulo 25, Seção 25.6.1
[^904]: Capítulo 25, Seção 25.6.2
[^905]: Capítulo 25, Seção 25.6.2

<!-- END -->