## Biclustering via Gibbs Sampling

### Introdução
Este capítulo se aprofunda na técnica de **biclustering**, também conhecida como *co-clustering*, que envolve o agrupamento simultâneo de linhas e colunas de uma matriz de dados [^903]. Essa abordagem é particularmente útil em bioinformática, onde as linhas representam genes e as colunas representam condições experimentais. Ela também encontra aplicações em filtragem colaborativa, onde linhas representam usuários e colunas representam itens. A técnica que será explorada envolve associar cada linha e coluna com um indicador latente e assumir que os dados são *iid* através de amostras e *features* dentro de cada bloco, ajustando o modelo usando amostragem de Gibbs [^903].

### Conceitos Fundamentais

A ideia central é associar cada linha $i$ a um indicador latente $r_i \in \{1, ..., K^{\sim}\}$ e cada coluna $j$ a um indicador latente $c_j \in \{1, ..., K^{\circ}\}$, onde $K^{\sim}$ e $K^{\circ}$ representam o número de clusters de linhas e colunas, respectivamente [^903]. Assume-se então que os dados são independentes e identicamente distribuídos (iid) dentro de cada bloco definido pela combinação de clusters de linhas e colunas.

O modelo probabilístico pode ser expresso como:

$$
p(x|r, c, \theta) = \prod_{i} \prod_{j} p(x_{ij}|r_i, c_j, \theta) = \prod_{i} \prod_{j} p(x_{ij}|\theta_{r_i, c_j})
$$

onde:
*   $x$ representa a matriz de dados observada.
*   $r$ é o vetor de indicadores latentes para as linhas.
*   $c$ é o vetor de indicadores latentes para as colunas.
*   $\theta$ representa os parâmetros do modelo.
*   $\theta_{r_i, c_j}$ são os parâmetros para o bloco definido pelo cluster de linha $r_i$ e o cluster de coluna $c_j$ [^903].

Em vez de usar um número finito de clusters para linhas e colunas, pode-se empregar um *Dirichlet process*, como na modelagem relacional infinita [^903]. Isso permite que o modelo aprenda o número apropriado de clusters a partir dos dados.

O ajuste do modelo pode ser realizado usando amostragem de Gibbs colapsada [^903]. Isso envolve iterativamente amostrar os indicadores latentes $r_i$ e $c_j$ condicionalmente aos dados e aos outros indicadores latentes.

#### Amostragem de Gibbs Colapsada
A amostragem de Gibbs colapsada para este modelo envolve as seguintes etapas:

1.  **Inicialização:** Inicializar aleatoriamente os indicadores latentes $r_i$ e $c_j$ para cada linha e coluna.
2.  **Iteração:** Para cada linha $i$:
    *   Remover a linha $i$ de seu cluster atual.
    *   Calcular a probabilidade de a linha $i$ pertencer a cada cluster de linha existente e a um novo cluster de linha.
    *   Amostrar um novo cluster para a linha $i$ com base nessas probabilidades.
    *   Adicionar a linha $i$ ao seu novo cluster.
3.  **Iteração:** Para cada coluna $j$:
    *   Remover a coluna $j$ de seu cluster atual.
    *   Calcular a probabilidade de a coluna $j$ pertencer a cada cluster de coluna existente e a um novo cluster de coluna.
    *   Amostrar um novo cluster para a coluna $j$ com base nessas probabilidades.
    *   Adicionar a coluna $j$ ao seu novo cluster.
4.  **Repetição:** Repetir as etapas 2 e 3 até que a convergência seja alcançada.

As probabilidades de atribuição de cluster na etapa 2 e 3 são derivadas da distribuição posterior do modelo, integrando os parâmetros $\theta$. Por exemplo, se os dados forem binários e uma distribuição de Bernoulli for usada para modelar os dados dentro de cada bloco, a probabilidade de uma linha $i$ pertencer a um cluster de linha $k$ será proporcional a:

$$
p(r_i = k | x, r_{-i}, c) \propto \frac{\Gamma(n_{k} + \alpha) \prod_{j=1}^{D} \Gamma(n_{k,j} + \beta) \Gamma(n_{k,j}' + \beta')}{\Gamma(n_{k} + D\beta + D\beta') \prod_{j=1}^{D} \Gamma(\beta) \Gamma(\beta')}
$$

onde:

*   $r_{-i}$ representa todos os indicadores de cluster de linha, exceto o da linha $i$.
*   $n_k$ é o número de linhas no cluster $k$.
*   $n_{k,j}$ é o número de linhas no cluster $k$ onde a coluna $j$ tem valor 1.
*   $n_{k,j}'$ é o número de linhas no cluster $k$ onde a coluna $j$ tem valor 0.
*   $\alpha$, $\beta$ e $\beta'$ são hiperparâmetros da distribuição a priori.

Uma equação semelhante pode ser derivada para a probabilidade de uma coluna pertencer a um determinado cluster.

### Conclusão
A técnica de biclustering usando amostragem de Gibbs oferece uma abordagem flexível e eficaz para descobrir padrões em dados complexos. Ao associar indicadores latentes a linhas e colunas e usar um *Dirichlet process*, o modelo pode aprender o número apropriado de clusters e identificar blocos de dados com padrões distintos. A amostragem de Gibbs colapsada fornece uma maneira computacionalmente eficiente de ajustar o modelo e inferir os indicadores latentes.

### Referências
[^903]: Clustering datapoints and features. *Biclustering*.
<!-- END -->