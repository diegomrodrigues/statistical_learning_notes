## Affinity Propagation: Exemplar-Based Clustering

### Introdução
Este capítulo explora a técnica de **Affinity Propagation**, um algoritmo de *clustering* que se distingue por sua capacidade de identificar automaticamente o número de *clusters* e seus respectivos centros, chamados de **exemplares**. Diferentemente dos algoritmos K-medoids ou K-centers, o Affinity Propagation não exige a especificação prévia do número de *clusters* [^887]. Ele opera com uma matriz de similaridade como entrada, o que o torna adequado para dados onde as relações de similaridade são mais facilmente definidas do que as características intrínsecas dos objetos [^887].

### Conceitos Fundamentais
O Affinity Propagation se baseia na ideia de que cada ponto de dados deve escolher outro ponto de dados como seu **exemplar** ou **centróide** [^887]. Alguns pontos de dados podem até escolher a si mesmos como exemplares, determinando assim os centros dos *clusters* [^887]. Mais formalmente, seja $c_i \in \{1, ..., N\}$ o centróide para o ponto de dados $i$, onde $N$ é o número total de pontos de dados [^887]. O objetivo do algoritmo é maximizar a seguinte função [^888]:

$$
S(c) = \sum_{i=1}^{N} s(i, c_i) + \sum_{k=1}^{N} \delta_k(c)
$$

onde:
- $s(i, c_i)$ representa a similaridade entre o ponto de dados $i$ e seu centróide $c_i$. O primeiro termo mede a similaridade de cada ponto ao seu centróide [^888].
- $\delta_k(c)$ é um termo de penalidade definido como [^888]:

$$
\delta_k(c) =
\begin{cases}
-\infty & \text{se } c_k \neq k \text{ mas } \exists i: c_i = k \\
0 & \text{caso contrário}
\end{cases}
$$

O segundo termo é uma penalidade que é $-\infty$ se algum ponto de dados $i$ escolheu $k$ como seu exemplar (i.e., $c_i = k$), mas $k$ não se escolheu como exemplar (i.e., não temos $c_k = k$) [^888]. Mais formalmente, $\delta_k(c)$ garante que, se algum ponto escolher $k$ como seu exemplar, então $k$ deve ser um exemplar de si mesmo [^888].

A função objetivo pode ser representada como um *factor graph* [^888]. Existem duas abordagens para resolver o problema de otimização: usar $N$ nós com $N$ valores possíveis ou usar $N^2$ nós binários [^889]. O algoritmo Affinity Propagation utiliza **loopy belief propagation** (max-product) para encontrar um máximo local da função objetivo [^889]. No *factor graph*, cada nó de variável $c_i$ envia uma mensagem para cada nó de fator $\delta_k$ [^889]. Essa mensagem é reduzida a um escalar, denotado como $r_{i \rightarrow k}$, conhecido como **responsabilidade**. A responsabilidade é uma medida de quão bom o ponto $k$ seria como exemplar para o ponto $i$, em comparação com outros exemplares potenciais que $i$ considerou [^889]. Adicionalmente, cada nó de fator $\delta_k$ envia uma mensagem para cada nó de variável $c_i$ [^889]. Essa mensagem é reduzida a um escalar, denotado como $a_{i \leftarrow k}$, conhecido como **disponibilidade**. A disponibilidade é uma medida de quão fortemente $k$ acredita que deveria ser um exemplar para $i$, com base em outros pontos de dados que $k$ considerou [^889].

Assim como em *loopy belief propagation*, o método pode oscilar e a convergência não é garantida [^889]. No entanto, na prática, o uso de amortecimento torna o método bastante confiável [^890]. Se o grafo for densamente conectado, a passagem de mensagens leva tempo $O(N^2)$, mas com matrizes de similaridade esparsas, leva apenas tempo $O(E)$, onde $E$ é o número de arestas ou entradas não nulas em $S$ [^890].

O número de *clusters* pode ser controlado escalonando os termos diagonais $S(i, i)$, que refletem o quanto cada ponto de dados quer ser um exemplar [^890]. Os valores $S(i, i)$ são definidos como a mediana de todas as similaridades aos pares [^890]. O resultado é 3 *clusters* [^890].

### Conclusão
O Affinity Propagation oferece uma abordagem interessante para o *clustering*, eliminando a necessidade de especificar o número de *clusters* antecipadamente [^887]. Sua capacidade de identificar exemplares e operar com matrizes de similaridade o torna uma ferramenta valiosa em diversos cenários de análise de dados [^887]. Embora o algoritmo possa apresentar desafios de convergência, o uso de técnicas como amortecimento pode mitigar esses problemas [^890]. Além disso, o controle sobre o número de *clusters* através da manipulação dos termos diagonais da matriz de similaridade oferece flexibilidade na aplicação do algoritmo [^890].

### Referências
[^887]: Seção 25.3, parágrafo 1
[^888]: Seção 25.3, parágrafo 2
[^889]: Seção 25.3, parágrafo 5
[^890]: Seção 25.3, parágrafo 7
<!-- END -->