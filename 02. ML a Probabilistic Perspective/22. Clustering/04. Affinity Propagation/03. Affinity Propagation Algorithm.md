## Affinity Propagation: Maximizing Similarity and Exemplar Selection

### Introdução
Este capítulo explorará o algoritmo de **Affinity Propagation**, uma técnica de clustering que difere das abordagens tradicionais ao identificar *exemplares* (exemplars) que representam os centros dos clusters [^887]. Em vez de requerer a especificação prévia do número de clusters, o Affinity Propagation determina automaticamente esse número, selecionando pontos de dados que melhor representam a estrutura dos dados. O algoritmo equilibra a similaridade de cada ponto ao seu centroide e um termo de penalidade que garante que os exemplares se auto-selecionem, alcançado através da troca de mensagens [^888].

### Conceitos Fundamentais
O Affinity Propagation recebe como entrada uma matriz de similaridade $S$, onde $s(i, k)$ representa a similaridade entre o ponto $i$ e o ponto $k$ [^887]. O objetivo do algoritmo é maximizar a função $S(c)$ [^888]:
$$S(c) = \sum_{i=1}^{N} s(i, c_i) + \sum_{k=1}^{N} \delta_k(c)$$
onde:
- $c_i$ representa o centroide (exemplar) atribuído ao ponto $i$.
- $s(i, c_i)$ mede a similaridade do ponto $i$ ao seu centroide $c_i$. Este termo busca agrupar pontos similares.
- $\delta_k(c)$ é um termo de penalidade definido como [^888]:
$$delta_k(c) = \begin{cases} -\infty & \text{se } c_k \neq k \text{ mas } \exists i : c_i = k \\ 0 & \text{caso contrário} \end{cases}$$
Este termo de penalidade garante que um ponto $k$ só pode ser um exemplar se ele se escolher como exemplar, ou seja, $c_k = k$. Isso evita que pontos sejam designados como exemplares se eles próprios não demonstrarem essa preferência.

**Message Passing:** O algoritmo Affinity Propagation atinge a maximização de $S(c)$ através de um processo iterativo de troca de mensagens (message passing) entre os pontos de dados [^888]. Duas matrizes de mensagens são utilizadas:
1. **Responsibility (Responsabilidade)** $r(i, k)$: Representa o quão adequado o ponto $k$ é para ser um exemplar do ponto $i$, considerando outros potenciais exemplares para $i$.
2. **Availability (Disponibilidade)** $a(i, k)$: Representa o quão "disponível" o ponto $k$ está para ser um exemplar, considerando o suporte de outros pontos que o escolhem como exemplar.

As mensagens são atualizadas iterativamente até a convergência ou um número máximo de iterações. As equações de atualização das mensagens (não fornecidas no contexto, mas essenciais para a compreensão completa) são projetadas para refletir a busca por um equilíbrio entre a similaridade e a auto-seleção de exemplares.

O texto menciona que o vetor de $N$ números pode ser reduzido a uma mensagem escalar, denotada como $r_{i \rightarrow k}$, conhecida como a responsabilidade [^889]. Esta é uma medida de o quanto $i$ pensa que $k$ faria um bom exemplar, comparado com todos os outros exemplares que $i$ tem olhado [^889].

Além disso, cada nó de fator $\delta_k$ envia uma mensagem para cada nó de variável $c_i$ [^889]. Novamente, isto pode ser reduzido a uma mensagem escalar, $a_{i \leftarrow k}$, conhecida como disponibilidade [^889]. Esta é uma medida de o quanto $k$ acredita que deveria ser um exemplar para $i$, baseado em todos os outros pontos de dados que $k$ tem olhado [^889].

**Loopy Belief Propagation:** O algoritmo emprega o max-product loopy belief propagation (Seção 22.2) para encontrar um máximo local forte do objetivo [^889].

**Damping:** Para evitar oscilações e garantir a convergência, uma técnica de *damping* é frequentemente utilizada nas atualizações das mensagens.

### Conclusão
O Affinity Propagation oferece uma abordagem inovadora para clustering, eliminando a necessidade de especificar o número de clusters *a priori* e identificando automaticamente exemplares representativos. O algoritmo equilibra a similaridade e a auto-seleção de exemplares através da troca de mensagens, tornando-o uma ferramenta poderosa para análise de dados exploratória. Apesar da complexidade do processo de troca de mensagens e do potencial de oscilações, o Affinity Propagation tem demonstrado um desempenho superior em comparação com outras técnicas de clustering, como o K-medoids [^890].

### Referências
[^887]: Clustering is the process of grouping similar objects together...
[^888]: The algorithm maximizes a function that balances the similarity of each point to its centroid and a penalty term ensuring exemplars choose themselves, achieved through message passing.
[^889]: Referring to the model in Figure 25.9, each variable nodes ci sends a message to each factor node dk.
[^890]: Many other results are reported in (Frey and Dueck 2007), who show that the method significantly outperforms K-medoids.
<!-- END -->