## Dirichlet Process Mixture Models: Finite Mixture Model Representation

### Introdução
Os Dirichlet Process Mixture Models (DPMMs) oferecem uma abordagem flexível e poderosa para o clustering, superando algumas das limitações dos modelos de mistura finitos tradicionais [^1]. Em continuidade ao conceito de *clustering* como o processo de agrupar objetos similares [^1], este capítulo aprofunda-se na representação dos DPMMs como modelos de mistura finitos, onde a escolha do número de componentes é tratada de forma não paramétrica.

### Conceitos Fundamentais
DPMMs envolvem uma representação de modelo de mistura finita, onde os pontos de dados são gerados a partir de distribuições específicas de cluster com pesos de mistura, e uma *prior* não paramétrica governa a distribuição sobre esses pesos [^1].

**Modelo de Mistura Finita:** Um modelo de mistura finita define um modelo probabilístico dos dados e otimiza um objetivo bem definido (a *likelihood* ou *posterior*) [^1]. A representação usual de um modelo de mistura finita é dada por [^5]:
$$\
p(x_i | z_i = k, \theta) = p(x_i | \theta_k) \quad \text{[25.13]}
$$
$$\
p(z_i = k | \pi) = \pi_k \quad \text{[25.14]}
$$
$$\
p(\pi | \alpha) = Dir(\pi | (\alpha / K) 1_K) \quad \text{[25.15]}
$$
Onde $x_i$ é o i-ésimo ponto de dado, $z_i$ é a atribuição do cluster para o i-ésimo ponto de dado, $\theta_k$ são os parâmetros do k-ésimo cluster, $\pi_k$ é o peso de mistura para o k-ésimo cluster, $\alpha$ é o parâmetro de concentração, e $K$ é o número de componentes.

**Escolha do Número de Componentes (K):** O número de componentes $K$ é um parâmetro chave, mas escolher o $K$ correto pode ser desafiador e subjetivo, potencialmente levando a um *clustering* subótimo [^1]. A dificuldade em escolher o valor apropriado de $K$ em modelos de mistura finitos é uma motivação central para o uso de modelos de mistura infinitos baseados em processos de Dirichlet [^5]. A seleção inadequada de $K$ pode resultar em *clustering* subótimo [^1]. Uma das vantagens dos DPMMs é que eles eliminam a necessidade de especificar um limite *a priori* para o número de *clusters* [^5].

**Prior Não Paramétrica:** Os DPMMs utilizam uma *prior* não paramétrica baseada no processo de Dirichlet (DP) [^5]. Isso permite que o número de *clusters* cresça conforme a quantidade de dados aumenta [^5]. O DP é uma distribuição sobre medidas de probabilidade $G$ [^8]:
$$\
G: \Theta \rightarrow \mathbb{R}^+ \text{, onde } G(\theta) \geq 0 \text{ e } \int G(\theta) d\theta = 1
$$
A distribuição de Dirichlet é definida implicitamente pela exigência de que $(G(T_1), ..., G(T_K))$ tenha uma distribuição de Dirichlet conjunta [^8]:
$$\
Dir(\alpha H(T_1), ..., \alpha H(T_K)) \quad \text{[25.17]}
$$
para qualquer partição finita $(T_1, ..., T_K)$ de $\Theta$. Aqui, $\alpha$ é chamado de parâmetro de concentração e $H$ é chamado de medida base [^8].

**Representação como Mistura Finita:** Embora os DPMMs sejam modelos de mistura infinitos, eles podem ser representados como uma mistura finita em qualquer amostra finita de dados [^5]. Isso é alcançado através da construção *stick-breaking* do DP [^9].

**Construção Stick-Breaking:** A construção *stick-breaking* fornece uma definição construtiva do DP. Seja $\pi = \{\pi_k\}_{k=1}^\infty$ uma sequência infinita de pesos de mistura derivados do seguinte processo [^9]:
$$\
\beta_k \sim Beta(1, \alpha) \quad \text{[25.23]}
$$
$$\
\pi_k = \beta_k \prod_{l=1}^{k-1} (1 - \beta_l) = \beta_k (1 - \sum_{l=1}^{k-1} \pi_l) \quad \text{[25.24]}
$$
onde $\beta_k$ são variáveis aleatórias independentes da distribuição Beta.

**Processo do Restaurante Chinês (CRP):** O CRP é uma representação alternativa útil para entender o *clustering* induzido pelo DP [^10]. Se $\theta_i \sim G$ são $N$ observações de $G \sim DP(\alpha, H)$, tomando $K$ valores distintos $\Theta_k$, então a distribuição preditiva da próxima observação é dada por [^10]:
$$\
p(\theta_{N+1} = \theta | \theta_{1:N}, \alpha, H) = \frac{1}{\alpha + N} (\alpha H(\theta) + \sum_{k=1}^K N_k \delta_{\theta_k} (\theta)) \quad \text{[25.27]}
$$
onde $N_k$ é o número de observações anteriores iguais a $\theta_k$.

### Conclusão
Os DPMMs oferecem uma abordagem flexível para *clustering* ao eliminar a necessidade de especificar o número de componentes *a priori* [^5]. Ao empregar um *prior* não paramétrico baseado no processo de Dirichlet [^5], esses modelos podem adaptar-se à complexidade dos dados e descobrir automaticamente a estrutura de *clustering* apropriada. A representação dos DPMMs como modelos de mistura finitos, juntamente com técnicas como a construção *stick-breaking* e o processo do restaurante chinês [^9, 10], fornecem ferramentas poderosas para inferência e análise nesses modelos.

### Referências
[^1]: Capítulo 25, Clustering
[^5]: Seção 25.2, Dirichlet process mixture models
[^8]: Seção 25.2.2, The Dirichlet process
[^9]: Seção 25.2.2.1, Stick breaking construction of the DP
[^10]: Seção 25.2.2.2, The Chinese restaurant process (CRP)
<!-- END -->