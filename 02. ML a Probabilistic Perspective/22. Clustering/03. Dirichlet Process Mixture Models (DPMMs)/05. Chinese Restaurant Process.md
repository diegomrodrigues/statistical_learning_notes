## Dirichlet Process Mixture Models e o Processo do Restaurante Chinês

### Introdução
Este capítulo aprofunda o entendimento dos **Dirichlet Process Mixture Models (DPMMs)**, explorando a analogia do **Chinese Restaurant Process (CRP)** para facilitar a compreensão de como os dados são agrupados em *clusters*. O CRP oferece uma maneira intuitiva de visualizar a formação de *clusters* em DPMMs, onde "mesas" representam *clusters* e "clientes" representam pontos de dados [^1].

### Conceitos Fundamentais

#### A Analogia do Chinese Restaurant Process (CRP)
O **Chinese Restaurant Process (CRP)** é uma analogia útil para entender como os DPMMs realizam o *clustering* [^1]. Imagine um restaurante chinês com um número infinito de mesas. Os clientes entram no restaurante um de cada vez. O primeiro cliente senta-se na primeira mesa. Cada cliente subsequente escolhe uma mesa seguindo estas regras:
*   Com probabilidade proporcional ao número de clientes já sentados na mesa, o novo cliente junta-se a uma mesa existente.
*   Com probabilidade proporcional a um parâmetro $\alpha$ (o parâmetro de concentração), o novo cliente senta-se numa nova mesa.

Nesta analogia:
*   Cada mesa representa um *cluster*.
*   Cada cliente representa um ponto de dados.
*   O número de clientes em uma mesa representa o tamanho do *cluster*.
*   O parâmetro $\alpha$ controla a tendência de criar novos *clusters*. Um valor maior de $\alpha$ implica uma maior probabilidade de criar novas mesas (e, portanto, novos *clusters*).

#### Distribuição Preditiva no DPMM
Formalmente, considere $N$ observações $\theta_i$ amostradas de uma distribuição $G$, onde $G \sim DP(\alpha, H)$. Aqui, $DP(\alpha, H)$ denota um Processo de Dirichlet com parâmetro de concentração $\alpha$ e distribuição base $H$ [^1]. Se essas $N$ observações assumem $K$ valores distintos $\theta_k$, a distribuição preditiva para a próxima observação $\theta_{N+1}$ é dada por [^1]:

$$\np(\theta_{N+1} = \theta | \theta_{1:N}, \alpha, H) = \frac{\alpha}{\alpha + N} H(\theta) + \sum_{k=1}^{K} \frac{N_k}{\alpha + N} \delta_{\theta_k}(\theta)$$

Onde:
*   $H(\theta)$ é a distribuição base, representando a probabilidade *a priori* de um novo valor de $\theta$.
*   $N_k$ é o número de observações que já assumiram o valor $\theta_k$.
*   $\delta_{\theta_k}(\theta)$ é a função delta de Dirac, que é 1 se $\theta = \theta_k$ e 0 caso contrário.
*   $\alpha$ é o parâmetro de concentração, controlando a probabilidade de amostrar um novo valor a partir da distribuição base $H$.

Esta equação descreve como a próxima observação é alocada a um *cluster* existente ou a um novo *cluster*. O primeiro termo, $\frac{\alpha}{\alpha + N} H(\theta)$, representa a probabilidade de $\theta_{N+1}$ ser amostrado da distribuição base $H$, efetivamente criando um novo *cluster*. O segundo termo, $\sum_{k=1}^{K} \frac{N_k}{\alpha + N} \delta_{\theta_k}(\theta)$, representa a probabilidade de $\theta_{N+1}$ ser alocado a um *cluster* existente, com probabilidade proporcional ao tamanho do *cluster*.

#### Processo de Quebra de Varetas (Stick-Breaking Process)
Uma construção alternativa para o DP é o **processo de quebra de varetas** [^24]. Este processo define os pesos de mistura $\pi_k$ como se segue:

$$\beta_k \sim Beta(1, \alpha)$$

$$\pi_k = \beta_k \prod_{l=1}^{k-1} (1 - \beta_l) = \beta_k \left(1 - \sum_{l=1}^{k-1} \pi_l\right)$$

Onde $\beta_k$ são variáveis aleatórias amostradas de uma distribuição Beta com parâmetros 1 e $\alpha$. A intuição aqui é que começamos com uma "vareta" de comprimento 1, e iterativamente quebramos um pedaço proporcional a $\beta_k$, definindo esse pedaço como o peso $\pi_k$. Os pesos $\pi_k$ são usados na representação do DP como uma soma ponderada de deltas de Dirac [^26]:

$$G(\theta) = \sum_{k=1}^{\infty} \pi_k \delta_{\theta_k}(\theta)$$

Onde $\theta_k \sim H$ são amostras da distribuição base $H$ [^26].

#### Amostragem de Gibbs Colapsada (Collapsed Gibbs Sampling)

A amostragem de Gibbs colapsada é um método comum para ajustar um DPMM [^86]. A ideia principal é amostrar as atribuições de *cluster* $z_i$ para cada ponto de dados $x_i$, condicional nas atribuições de *cluster* de todos os outros pontos de dados $z_{-i}$ e os dados $x$. A probabilidade condicional é dada por [^86]:

$$p(z_i = k | z_{-i}, x, \alpha, \lambda) \propto p(z_i = k | z_{-i}, \alpha) p(x_i | x_{-i}, z_i = k, z_{-i}, \lambda)$$

O primeiro termo, $p(z_i = k | z_{-i}, \alpha)$, é a probabilidade *a priori* de que o ponto de dados $i$ pertença ao *cluster* $k$, dada pelas equações do CRP [^86]:

$$p(z_i = k | z_{-i}, \alpha) = \begin{cases}
\frac{N_{k, -i}}{\alpha + N - 1} & \text{se } k \text{ foi visto antes} \\\\
\frac{\alpha}{\alpha + N - 1} & \text{se } k \text{ é um novo cluster}
\end{cases}$$

Onde $N_{k, -i}$ é o número de pontos de dados atribuídos ao *cluster* $k$, excluindo o ponto de dados $i$. O segundo termo, $p(x_i | x_{-i}, z_i = k, z_{-i}, \lambda)$, é a verossimilhança de $x_i$ dada a atribuição do *cluster* e os dados restantes. Este termo depende da escolha da distribuição base $H$. Por exemplo, se $H$ é uma distribuição Gaussiana, então este termo é a distribuição preditiva posterior para o *cluster* $k$ avaliada em $x_i$ [^86].

### Conclusão
O **Dirichlet Process Mixture Model (DPMM)**, facilitado pela analogia do **Chinese Restaurant Process (CRP)**, oferece uma abordagem flexível e poderosa para o *clustering* não paramétrico. Ao permitir que o número de *clusters* cresça com os dados, o DPMM evita a necessidade de especificar o número de *clusters* *a priori*, tornando-o adequado para uma ampla gama de aplicações. A distribuição preditiva do DPMM, juntamente com métodos de inferência como a amostragem de Gibbs colapsada, fornece um *framework* estatístico sólido para aprender a estrutura do *cluster* a partir dos dados.

### Referências
[^1]: Clustering
[^24]: Stick breaking construction of the DP
[^26]: Now define
[^86]: Fitting a DP mixture model
<!-- END -->