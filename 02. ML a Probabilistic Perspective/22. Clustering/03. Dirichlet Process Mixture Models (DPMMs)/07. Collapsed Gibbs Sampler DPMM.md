## Collapsed Gibbs Sampler para Modelos de Mistura de Processos de Dirichlet

### Introdução
Este capítulo explora a aplicação do *collapsed Gibbs sampler* para ajustar modelos de mistura de Processos de Dirichlet (DPMMs). Em continuidade aos conceitos de clustering introduzidos anteriormente [^1], e estendendo a discussão sobre modelos de mistura finitos [^8], focamos aqui numa abordagem não-paramétrica que permite inferir o número de clusters a partir dos dados. O *collapsed Gibbs sampler* é uma técnica de inferência eficiente que se adapta bem à estrutura do DPMM, permitindo uma exploração eficaz do espaço de configurações possíveis.

### Conceitos Fundamentais
A forma mais simples de ajustar um DPMM é modificar o *collapsed Gibbs sampler* [^12]. A probabilidade condicional $p(z_i = k | z_{-i}, x, \alpha, \lambda)$ é proporcional a $p(z_i = k | z_{-i}, \alpha) * p(x_i | x_{-i}, z_i = k, z_{-i}, \lambda)$ [^12], onde:
*   $z_i$ representa a atribuição do ponto de dados $x_i$ ao cluster $k$.
*   $z_{-i}$ representa as atribuições de todos os outros pontos de dados (excluindo $x_i$).
*   $x$ representa todos os pontos de dados.
*   $x_{-i}$ representa todos os pontos de dados exceto $x_i$.
*   $\alpha$ é o parâmetro de concentração do Processo de Dirichlet.
*   $\lambda$ representa os hiperparâmetros das distribuições de base.

O primeiro termo, $p(z_i = k | z_{-i}, \alpha)$, é determinado pelo *Chinese Restaurant Process* (CRP) [^10]. Este processo estocástico define a probabilidade de um novo cliente (ponto de dados) se juntar a uma mesa existente (cluster) ou sentar-se numa nova mesa. A probabilidade de se juntar a uma mesa existente é proporcional ao número de clientes já sentados nessa mesa, enquanto a probabilidade de se sentar numa nova mesa é influenciada pelo parâmetro de concentração $\alpha$.

O segundo termo, $p(x_i | x_{-i}, z_i = k, z_{-i}, \lambda)$, envolve a partição dos dados em clusters [^12]. Este termo avalia a verossimilhança do ponto de dados $x_i$ dado que ele pertence ao cluster $k$, considerando os outros pontos de dados já atribuídos a esse cluster e os hiperparâmetros $\lambda$.

Formalmente, por *exchangeability*, podemos assumir que $z_i$ é o último cliente a entrar no restaurante [^12]. Assim, o primeiro termo é dado por [^12]:

$$ p(z_i | z_{-i}, \alpha) = \frac{\alpha \mathbb{I}(z_i = k^*) + \sum_{k=1}^K N_{k,-i} \mathbb{I}(z_i = k)}{\alpha + N - 1} $$

onde:

*   $K$ é o número de clusters usados por $z_{-i}$.
*   $k^*$ é um novo cluster [^12].
*   $N_{k,-i}$ é o número de pontos no cluster $k$, excluindo $x_i$.
*   $\mathbb{I}(\cdot)$ é a função indicadora.

Equivalentemente, podemos escrever [^12]:

$$ p(z_i = k | z_{-i}, \alpha) = \begin{cases} \frac{N_{k,-i}}{\alpha + N - 1} & \text{se } k \text{ já foi visto} \\\\ \frac{\alpha}{\alpha + N - 1} & \text{se } k \text{ é um novo cluster} \end{cases} $$

Interessantemente, isto é equivalente à equação 24.26, que tem a forma $p(z_i = k | z_{-i}, \alpha) = \frac{N_{k,-i} + \alpha/K}{\alpha + N - 1}$, no limite $K \rightarrow \infty$ (Rasmussen 2000; Neal 2000) [^12].

Para computar o segundo termo, $p(x_i | x_{-i}, z_i = k, z_{-i}, \lambda)$, particionamos os dados $x_{-i}$ em clusters baseados em $z_{-i}$ [^12]. Seja $x_{i,k} = \{x_j : z_j = k, j \neq i\}$ os dados atribuídos ao cluster $k$. Se $z_i = k$, então $x_i$ é condicionalmente independente de todos os pontos de dados exceto aqueles atribuídos ao cluster $k$. Portanto [^12]:

$$ p(x_i | x_{-i}, z_{-i}, z_i = k, \lambda) = p(x_i | x_{i,k}, \lambda) = \frac{p(x_i, x_{i,k} | \lambda)}{p(x_{i,k} | \lambda)} $$

onde [^12]:

$$ p(x_i, x_{i,k} | \lambda) = \int p(x_i | \theta_k) \prod_{j \neq i : z_j = k} p(x_j | \theta_k) H(\theta_k | \lambda) d\theta_k $$

é a verossimilhança marginal de todos os dados atribuídos ao cluster $k$, incluindo $i$, e $p(x_{i,k} | \lambda)$ é uma expressão análoga excluindo $i$. Assim, vemos que o termo $p(x_i | x_{-i}, z_{-i}, z_i = k, \lambda)$ é a distribuição preditiva posterior para o cluster $k$ avaliada em $x_i$ [^12].

Se $z_i = k^*$, correspondendo a um novo cluster, temos [^12]:

$$ p(x_i | x_{-i}, z_{-i}, z_i = k^*, \lambda) = p(x_i | \lambda) = \int p(x_i | \theta) H(\theta | \lambda) d\theta $$

que é apenas a distribuição preditiva *a priori* para um novo cluster avaliada em $x_i$.

O Algoritmo 25.1 fornece o pseudocódigo para o *collapsed Gibbs sampler* para DPMMs [^13]. Este algoritmo é muito similar ao *collapsed Gibbs* para misturas finitas, exceto que temos de considerar o caso $z_i = k^*$.

### Conclusão
O *collapsed Gibbs sampler* oferece uma abordagem elegante e eficiente para ajustar DPMMs. Ao integrar analiticamente os parâmetros $\theta_k$, o sampler opera diretamente sobre as atribuições de cluster $z_i$, reduzindo a dimensionalidade do espaço de busca e acelerando a convergência [^12]. A capacidade de inferir o número de clusters a partir dos dados torna o DPMM uma ferramenta valiosa para problemas de clustering onde o número ideal de clusters é desconhecido *a priori*. Adicionalmente, a conexão com o *Chinese Restaurant Process* fornece uma interpretação intuitiva da dinâmica de atribuição de clusters [^10].

### Referências
[^1]: Clustering is the process of grouping similar objects together.
[^8]: The simplest approach to (flat) clustering is to use a finite mixture model, as we discussed in Section 11.2.3.
[^10]: This is called the Chinese restaurant process or CRP, based on the seemingly infinite supply of tables at certain Chinese restaurants.
[^12]: The simplest way to fit a DPMM is to modify the collapsed Gibbs sampler of Section 24.2.4. From Equation 24.23 we have $p(z_i = k|z_{-i}, x, \alpha, \lambda) \propto p(z_i = k|z_{-i}, \alpha) * p(x_i|x_{-i}, z_i = k, z_{-i}, \lambda)$, where the first term is determined by the CRP and the second term involves partitioning the data into clusters.
[^13]: Algorithm 25.1: Collapsed Gibbs sampler for DP mixtures
<!-- END -->