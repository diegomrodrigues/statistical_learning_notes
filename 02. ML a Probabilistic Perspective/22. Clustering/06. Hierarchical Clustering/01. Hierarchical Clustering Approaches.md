## Hierarchical Clustering: Building a Nested Hierarchy of Clusters

### Introdução
O processo de **clustering** visa agrupar objetos similares. Uma abordagem fundamental para este fim é o **hierarchical clustering**, que constrói uma hierarquia aninhada de clusters [^1]. Essa abordagem difere do *flat clustering*, que particiona os objetos em conjuntos disjuntos [^1]. O hierarchical clustering oferece uma perspectiva rica sobre a estrutura dos dados, permitindo a identificação de clusters em diferentes níveis de granularidade.

### Conceitos Fundamentais
O hierarchical clustering opera com base em uma **matriz de dissimilaridade** entre os objetos [^1]. Essa matriz, denotada por *D*, contém medidas de "distância" entre cada par de objetos [^1]. O elemento *d<sub>i,j</sub>* da matriz representa a dissimilaridade entre os objetos *i* e *j* [^1].

Existem duas abordagens principais para hierarchical clustering:

1.  **Agglomerative (Bottom-up):** Inicialmente, cada objeto é considerado um cluster individual [^1]. Em cada iteração, os dois clusters mais similares são combinados em um novo cluster [^1]. Esse processo é repetido até que todos os objetos pertençam a um único cluster [^1].

2.  **Divisive (Top-down):** Inicialmente, todos os objetos pertencem a um único cluster [^1]. Em cada iteração, o cluster mais heterogêneo é dividido em dois novos clusters [^1]. Esse processo é repetido recursivamente até que cada objeto pertença a um cluster individual [^1].

**Algoritmos Agglomerativos**
Os algoritmos agglomerativos são mais comuns devido à sua simplicidade e facilidade de implementação [^1]. A escolha de como medir a similaridade entre clusters é crucial e leva a diferentes variantes do hierarchical clustering [^21]. As três abordagens mais comuns são:

*   **Single Linkage:** A distância entre dois clusters é definida como a distância entre os dois membros mais próximos dos clusters [^21].
$$d_{SL}(G, H) = \min_{i \in G, i' \in H} d_{i,i'}$$
    Onde *G* e *H* representam dois clusters.

*   **Complete Linkage:** A distância entre dois clusters é definida como a distância entre os dois membros mais distantes dos clusters [^21].
$$d_{CL}(G, H) = \max_{i \in G, i' \in H} d_{i,i'}$$
    O complete linkage tende a produzir clusters mais compactos [^23].

*   **Average Linkage:** A distância entre dois clusters é definida como a distância média entre todos os pares de membros dos clusters [^21].
$$d_{avg}(G, H) = \frac{1}{n_G n_H} \sum_{i \in G} \sum_{i' \in H} d_{i,i'}$$
    Onde *n<sub>G</sub>* e *n<sub>H</sub>* são o número de elementos nos grupos *G* e *H* respectivamente. O average linkage representa um compromisso entre o single linkage e o complete linkage [^23].

O resultado do hierarchical clustering pode ser visualizado como um **dendrograma**, uma representação em árvore da hierarquia de clusters [^21]. As folhas do dendrograma representam os objetos individuais, e a altura dos ramos representa a dissimilaridade entre os clusters que estão sendo unidos [^21]. Ao cortar o dendrograma em uma determinada altura, obtemos uma partição dos dados em clusters [^21].

**Algoritmos Divisivos**
Os algoritmos divisivos são menos comuns, mas oferecem a vantagem de tomar decisões de divisão no contexto de todos os dados [^24]. Um exemplo é o **dissimilarity analysis**, que remove iterativamente os objetos mais dissimilares de um cluster até que um critério de parada seja satisfeito [^24].

### Conclusão
O hierarchical clustering oferece uma abordagem flexível e informativa para a análise de dados, permitindo a exploração de estruturas de cluster em diferentes escalas [^1]. A escolha entre as abordagens agglomerativa e divisiva, bem como a escolha da métrica de similaridade entre clusters, dependem das características específicas dos dados e dos objetivos da análise [^21, 24]. Embora o hierarchical clustering não otimize uma função objetivo bem definida, como os modelos de mistura [^21], ele fornece uma representação visual e interpretável da estrutura dos dados. Abordagens Bayesianas para hierarchical clustering superam essa limitação [^25].

### Referências
[^1]: Clustering
[^2]: Measuring (dis)similarity
[^21]: Hierarchical clustering
[^23]: Single link
[^24]: Divisive clustering
[^25]: Choosing the number of clusters
<!-- END -->