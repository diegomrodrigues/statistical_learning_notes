## Single Link Clustering: A Minimum Spanning Tree Approach

### Introdução
Em continuidade ao estudo de **Hierarchical Clustering**, este capítulo se aprofunda em uma de suas variantes fundamentais: o *single link clustering*, também conhecido como *nearest neighbor clustering* [^897]. Exploraremos como este método define a distância entre grupos, sua relação intrínseca com a construção de uma *minimum spanning tree* (MST) e suas implicações práticas. Os métodos de *hierarchical clustering* produzem um agrupamento "plano" [^893], e são especialmente úteis quando se deseja aprender um agrupamento hierárquico, onde *clusters* podem estar aninhados uns dentro dos outros [^893].

### Conceitos Fundamentais
O *single link clustering* é um método *agglomerative* (bottom-up) [^893] que se distingue pela forma como calcula a distância entre *clusters*. Ao invés de considerar a distância média ou a distância entre os pontos mais distantes, o *single link clustering* define a distância entre dois *clusters* como a distância entre seus dois membros mais próximos [^897]. Formalmente, se temos dois grupos $G$ e $H$, a distância entre eles, $d_{SL}(G, H)$, é definida como:

$$d_{SL}(G, H) = \min_{i \in G, i' \in H} d_{i,i'}$$ [^897]

onde $d_{i,i'}$ representa a distância entre os objetos $i$ e $i'$.

**Construção da Minimum Spanning Tree (MST)**
Uma característica marcante do *single link clustering* é sua íntima relação com a *minimum spanning tree* (MST) [^897]. A MST de um conjunto de dados é uma árvore que conecta todos os objetos de forma a minimizar a soma dos pesos das arestas (distâncias) [^897]. O algoritmo de *single link clustering* constrói implicitamente uma MST ao longo de suas iterações [^897].

*Prova:*
Para demonstrar essa relação, considere o processo de *merging* de dois *clusters* $G$ e $H$ no *single link clustering*. Ao unirmos esses *clusters*, estamos essencialmente conectando seus dois membros mais próximos, $i$ e $i'$, com uma aresta de peso $d_{i,i'}$ [^897]. Esta aresta é garantidamente a aresta de menor peso que une os *clusters* $G$ e $H$. Como o algoritmo sempre une os *clusters* com a menor distância entre seus membros, o resultado final é uma árvore que minimiza a soma dos pesos das arestas, que é a definição de uma MST [^897]. $\blacksquare$

**Algoritmo:**
O algoritmo *agglomerative clustering* inicia com $N$ grupos, cada um contendo um único objeto [^895]. Em cada passo, ele une os dois grupos mais similares até que reste um único grupo contendo todos os dados [^895]. O processo de *merging* pode ser representado por uma *binary tree*, chamada *dendrogram* [^895]. A altura dos galhos representa a dissimilaridade entre os grupos que estão sendo unidos [^895].

**Complexidade:**
A complexidade de tempo para selecionar os dois *clusters* mais similares para *merge* é $O(N^2)$ [^895], e existem $O(N)$ passos no algoritmo [^895]. Portanto, o tempo total de execução é $O(N^3)$ [^895]. No entanto, usando uma fila de prioridade, isto pode ser reduzido para $O(N^2 \log N)$ [^895]. Diferentemente de outras variantes, o *single link clustering* pode ser implementado em $O(N^2)$ [^897].

**Propriedades e Implicações**
O *single link clustering* possui algumas propriedades distintas que o diferenciam de outros métodos de *hierarchical clustering* [^897]:
1.  **Sensibilidade ao Efeito Chain:** Devido à sua definição de distância baseada nos vizinhos mais próximos, o *single link clustering* tende a formar *clusters* alongados e irregulares, suscetíveis ao chamado "efeito chain" [^897]. Isso significa que um único par de objetos próximos pode "ligar" *clusters* que, de outra forma, seriam considerados distintos.
2.  **Violação da Compactness Property:** O *single link clustering* pode gerar *clusters* que não satisfazem a propriedade de *compactness*, que afirma que todas as observações dentro de um grupo devem ser similares entre si [^897].

### Conclusão
O *single link clustering* oferece uma abordagem interessante e intuitiva para o *hierarchical clustering*, com sua forte ligação à *minimum spanning tree*. No entanto, sua sensibilidade ao "efeito chain" e a potencial violação da propriedade de *compactness* exigem cautela em sua aplicação. Em situações onde a estrutura dos dados é bem definida e os *clusters* são naturalmente compactos, métodos alternativos como o *complete link clustering* ou o *average link clustering* [^897] podem ser mais adequados.

### Referências
[^893]: Capítulo 25, Clustering, Seção 25.5 Hierarchical clustering.
[^895]: Capítulo 25, Clustering, Seção 25.5.1 Agglomerative clustering.
[^897]: Capítulo 25, Clustering, Seção 25.5.1.1 Single link.
<!-- END -->