## Divisive Hierarchical Clustering

### Introdução
Em contraste com a abordagem *bottom-up* da **aglomeração hierárquica**, a **divisão hierárquica** adota uma estratégia *top-down* para a construção de hierarquias de clusters [^893]. Em vez de começar com cada ponto de dados em seu próprio cluster e, em seguida, iterativamente mesclar os clusters mais semelhantes, a divisão hierárquica começa com todos os dados em um único cluster e, recursivamente, divide cada cluster em dois clusters filhos. Este capítulo detalha o processo de divisão hierárquica, explorando as metodologias e heurísticas empregadas para realizar a divisão de clusters, incluindo o algoritmo *bisecting K-means* e a construção de *minimum spanning trees*.

### Conceitos Fundamentais
A **divisão hierárquica** representa uma abordagem complementar à aglomeração hierárquica, oferecendo uma perspectiva *top-down* na análise de agrupamentos. Ao contrário dos métodos aglomerativos, que iniciam com *N* grupos singulares e iterativamente os fundem, a divisão hierárquica começa com um único grupo contendo todos os dados e, recursivamente, divide esse grupo em subgrupos menores [^895, 898].

#### Metodologia Geral
O processo de divisão hierárquica envolve os seguintes passos:
1. **Inicialização:** Começar com um único cluster contendo todos os *N* pontos de dados [^898].
2. **Divisão Recursiva:** Recursivamente dividir cada cluster em dois clusters filhos, utilizando uma heurística para determinar a melhor divisão [^898].
3. **Critério de Parada:** Continuar o processo de divisão até que um critério de parada seja satisfeito, como um número desejado de clusters, um limite de dissimilaridade dentro de cada cluster ou quando todos os clusters forem singulares [^898].

#### Heurísticas para Divisão
Dado que existem $2^{N-1}-1$ maneiras de dividir um conjunto de *N* itens em dois grupos, a computação da divisão ótima é impraticável. Portanto, várias heurísticas são empregadas para aproximar a melhor divisão [^898].

##### Algoritmo Bisecting K-means
Uma abordagem comum é selecionar o cluster com o maior diâmetro e dividi-lo em dois utilizando o algoritmo **K-means** ou **K-medoids** com *K=2* [^898]. Este processo é conhecido como o algoritmo **bisecting K-means** [(Steinbach et al. 2000)][^898]. Este algoritmo pode ser repetido até que se tenha o número desejado de clusters.

##### Minimum Spanning Tree (MST)
Outro método envolve a construção de uma **minimum spanning tree (MST)** a partir do grafo de dissimilaridade e, em seguida, remover a aresta correspondente à maior dissimilaridade [^898]. Curiosamente, este método produz os mesmos resultados que a **single link agglomerative clustering** [^898]. A árvore construída utilizando **single link clustering** é uma *minimum spanning tree* dos dados [^897].

##### Dissimilarity Analysis
Outro método para a divisão de clusters é a **dissimilarity analysis** [(Macnaughton-Smith et al. 1964)][^898]. Este método começa com um único cluster contendo todos os dados, $G = \\{1, ..., N\\}$. Em seguida, mede-se a dissimilaridade média de $i \in G$ para todos os outros $i' \in G$:\n\n$$d_i^G = \frac{1}{n_G} \sum_{i' \in G} d_{i,i'}$$\n\nRemove-se então o objeto mais dissimilar e coloca-o em seu próprio cluster $H$:\n\n$$i^* = \arg \max_{i \in G} d_i^G, \quad G = G \setminus \\{i^*\\}, \quad H = \\{i^*\\}$$\n\nContinuamos então a mover objetos de $G$ para $H$ até que algum critério de parada seja atendido. Especificamente, escolhemos um ponto $i^*$ para mover que maximiza a dissimilaridade média para cada $i' \in G$, mas minimiza a dissimilaridade média para cada $i' \in H$:\n\n$$d_i^H = \frac{1}{n_H} \sum_{i' \in H} d_{i,i'}, \quad i^* = \arg \max_{i \in G} d_i^G - d_i^H$$\n\nContinuamos a fazer isso até que $d_i^G - d_i^H$ seja negativo. O resultado final é que dividimos $G$ em dois clusters filhos, $G$ e $H$. Podemos então chamar recursivamente o algoritmo em $G$ e/ou $H$, ou em qualquer outro nó da árvore. Por exemplo, podemos escolher dividir o nó $G$ cuja dissimilaridade média é a mais alta, ou cuja dissimilaridade máxima (isto é, diâmetro) é a mais alta. Continuamos o processo até que a dissimilaridade média dentro de cada cluster esteja abaixo de algum limite, e/ou todos os clusters sejam singletons [^898].

#### Vantagens e Desvantagens
A divisão hierárquica, embora menos popular do que a aglomeração hierárquica, oferece vantagens distintas [^899]:
- **Eficiência:** Pode ser mais rápida se apenas um número constante de níveis for dividido, resultando em uma complexidade de tempo de $O(N)$ [^898].
- **Decisões Globais:** As decisões de divisão são tomadas no contexto de todos os dados, ao contrário dos métodos *bottom-up*, que tomam decisões míopes de mesclagem [^898].

### Conclusão
A divisão hierárquica oferece uma alternativa valiosa à aglomeração hierárquica, especialmente em cenários onde a eficiência e a consideração global dos dados são cruciais. Ao empregar heurísticas como o algoritmo *bisecting K-means* e a construção de *minimum spanning trees*, a divisão hierárquica permite a descoberta de estruturas de cluster complexas e significativas em conjuntos de dados diversos. Embora menos popular do que a aglomeração hierárquica, a divisão hierárquica mantém seu lugar como uma ferramenta essencial no arsenal de técnicas de clustering, oferecendo uma perspectiva *top-down* que complementa as abordagens *bottom-up* e enriquece a análise de dados [^893, 898].

### Referências
[^893]: Capítulo 25, Clustering, Seção 25.5, Hierarchical clustering.
[^895]: Capítulo 25, Clustering, Seção 25.5, Hierarchical clustering, Algoritmo 25.2.
[^897]: Capítulo 25, Clustering, Seção 25.5.1.1, Single link.
[^898]: Capítulo 25, Clustering, Seção 25.5.2, Divisive clustering.
[^899]: Capítulo 25, Clustering, Seção 25.5.2, Divisive clustering.

<!-- END -->