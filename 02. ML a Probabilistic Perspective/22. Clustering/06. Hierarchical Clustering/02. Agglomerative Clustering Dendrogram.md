## Agglomerative Clustering: A Bottom-Up Approach to Hierarchical Structures

### Introdução
Em continuidade ao conceito de **hierarchical clustering** [^1], exploraremos agora um método específico: o **agglomerative clustering** [^21]. Este método, também conhecido como *bottom-up clustering*, oferece uma abordagem intuitiva e determinística para a criação de hierarquias de clusters. Diferentemente dos métodos de *partitional clustering*, que requerem a especificação prévia do número de clusters ($K$) e são sensíveis às condições iniciais [^1], o agglomerative clustering constrói uma hierarquia completa de clusters, representada por um **dendrograma**, sem a necessidade de especificar $K$ [^21].

### Conceitos Fundamentais
O agglomerative clustering inicia com cada objeto do dataset em seu próprio cluster individual [^21]. Assim, se temos $N$ objetos, começamos com $N$ clusters, cada um contendo um único objeto [^21]. O algoritmo então itera, repetidamente unindo os dois clusters mais similares até que reste apenas um único cluster contendo todos os dados [^21].

O processo de união é definido da seguinte forma [^21]:
1. **Inicialização:** Cada objeto é um cluster individual.
2. **Iteração:**
    *   Encontrar os dois clusters mais similares, $C_j$ e $C_k$.
    *   Unir $C_j$ e $C_k$ para formar um novo cluster $C_l = C_j \cup C_k$.
    *   Remover $C_j$ e $C_k$ do conjunto de clusters disponíveis e adicionar $C_l$.
3. **Repetição:** Repetir o passo 2 até que reste apenas um cluster.

A principal diferença entre as variantes do agglomerative clustering reside na definição de **dissimilaridade entre grupos de objetos** [^21]. Três métodos comuns são [^21]:

*   **Single Linkage (Nearest Neighbor):** A dissimilaridade entre dois clusters é definida como a menor distância entre qualquer par de objetos, um de cada cluster [^21]. Matematicamente:
    $$d_{SL}(G, H) = \min_{i \in G, i' \in H} d_{i,i'}$$
    onde $G$ e $H$ são dois clusters e $d_{i,i'}$ é a dissimilaridade entre os objetos $i$ e $i'$. A árvore gerada por single linkage é uma *minimum spanning tree* [^21].
*   **Complete Linkage (Furthest Neighbor):** A dissimilaridade entre dois clusters é definida como a maior distância entre qualquer par de objetos, um de cada cluster [^21]. Matematicamente:
    $$d_{CL}(G, H) = \max_{i \in G, i' \in H} d_{i,i'}$$
    Complete linkage tende a produzir clusters mais compactos [^21].
*   **Average Linkage:** A dissimilaridade entre dois clusters é definida como a média das distâncias entre todos os pares de objetos, um de cada cluster [^21]. Matematicamente:
    $$d_{avg}(G, H) = \frac{1}{n_G n_H} \sum_{i \in G} \sum_{i' \in H} d_{i,i'}$$
    onde $n_G$ e $n_H$ são o número de elementos nos grupos $G$ e $H$, respectivamente [^21]. Average linkage busca um compromisso entre single e complete linkage [^21].

O algoritmo 25.2 [^21] resume o pseudocódigo do agglomerative clustering.

**Complexidade Computacional:** A escolha dos dois clusters mais similares para mesclar tem complexidade $O(N^2)$, com $N$ sendo o número de passos do algoritmo, resultando em um tempo total de execução de $O(N^3)$ [^21]. No entanto, o uso de uma *priority queue* pode reduzir a complexidade para $O(N^2 \log N)$ [^21].

**Dendrogramas:** O processo de merging é representado por uma **binary tree**, chamada **dendrogram** [^21]. Os grupos iniciais (objetos) estão nas folhas (na parte inferior da figura), e cada vez que dois grupos são mesclados, nós os juntamos na árvore [^21]. A altura dos ramos representa a dissimilaridade entre os grupos que estão sendo unidos [^21]. A raiz da árvore (que está no topo) representa um grupo contendo todos os dados [^21].

### Conclusão
O agglomerative clustering oferece uma forma flexível de explorar a estrutura hierárquica dos dados. A escolha do método de linkage (single, complete ou average) influencia a forma dos clusters resultantes, permitindo adaptar o algoritmo às características específicas do dataset [^21]. A representação em dendrograma facilita a visualização da hierarquia e a escolha do número de clusters, embora esta última ainda dependa de critérios subjetivos ou da aplicação de métodos Bayesianos [^21].

### Referências
[^1]: 25.1 Introduction
[^21]: 25.5.1 Agglomerative clustering
<!-- END -->