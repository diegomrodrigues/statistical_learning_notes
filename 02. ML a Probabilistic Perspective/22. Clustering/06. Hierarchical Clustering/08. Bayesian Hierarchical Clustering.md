## Bayesian Hierarchical Clustering: A Probabilistic Approach to Dendrogram Construction

### Introdução
O *clustering hierárquico* é uma técnica fundamental na análise de dados, permitindo a organização de objetos em uma estrutura hierárquica de clusters aninhados [^1]. Tradicionalmente, os algoritmos de clustering hierárquico, tanto aglomerativos (bottom-up) quanto divisivos (top-down), empregam métricas de similaridade *ad hoc* para determinar quais clusters devem ser unidos ou divididos [^19]. No entanto, essa abordagem pode levar a decisões subótimas e dificulta a avaliação da qualidade do clustering resultante [^19]. O *Bayesian hierarchical clustering* (BHC) surge como uma alternativa probabilística, utilizando testes de hipóteses Bayesianas para guiar o processo de fusão de clusters, oferecendo uma estrutura mais robusta e interpretável [^25].

### Conceitos Fundamentais
O BHC emprega uma abordagem aglomerativa, similar aos algoritmos tradicionais de clustering hierárquico bottom-up [^25]. No entanto, em vez de depender de métricas de similaridade predefinidas, o BHC utiliza testes de hipóteses Bayesianas para avaliar a probabilidade de que dois clusters devam ser unidos [^25].

A probabilidade de fusão entre dois clusters $T_i$ e $T_j$, denotada por $r_{ij}$, é dada por [^25]:

$$ r_{ij} = p(D_{ij} | T_{ij}) = \frac{p(D_{ij} | M_{ij} = 1)p(M_{ij} = 1)}{p(D_{ij} | M_{ij} = 1)p(M_{ij} = 1) + p(D_{ij} | M_{ij} = 0)p(M_{ij} = 0)} $$

onde:
- $D_{ij}$ representa os dados combinados dos clusters $T_i$ e $T_j$ [^25].
- $M_{ij} = 1$ indica que os clusters $T_i$ e $T_j$ devem ser fundidos [^25].
- $M_{ij} = 0$ indica que os clusters $T_i$ e $T_j$ não devem ser fundidos [^25].
- $p(D_{ij} | M_{ij} = 1)$ é a probabilidade dos dados $D_{ij}$ sob a hipótese de que eles se originam do mesmo modelo [^25].
- $p(D_{ij} | M_{ij} = 0)$ é a probabilidade dos dados $D_{ij}$ sob a hipótese de que eles se originam de modelos independentes [^25].
- $p(M_{ij} = 1)$ é a probabilidade *a priori* de que os clusters $T_i$ e $T_j$ devam ser fundidos [^25].

A probabilidade *a priori* de uma fusão, $p(M_{ij} = 1)$, pode ser computada utilizando um algoritmo bottom-up [^25]. Se $M_{ij} = 1$, assume-se que os dados em $D_{ij}$ vêm do mesmo modelo, logo [^25]:

$$ p(D_{ij} | M_{ij} = 1) = \int \left[ \prod_{x_n \in D_{ij}} p(x_n | \theta) \right] p(\theta | \lambda) d\theta $$

onde $\theta$ representa os parâmetros do modelo e $\lambda$ representa os hiperparâmetros [^25].

Se $M_{ij} = 0$, assume-se que os dados em $D_{ij}$ foram gerados independentemente por cada árvore, logo [^25]:

$$ p(D_{ij} | M_{ij} = 0) = p(D_i | T_i) p(D_j | T_j) $$

Estes dois termos já foram computados pelo processo bottom-up [^25].

**Conexão com Modelos de Mistura do Processo de Dirichlet (DPMM)**

Existe uma conexão entre BHC e DPMMs. A verossimilhança marginal de um DPMM, somando todas as $2^N-1$ partições, é dada por [^26]:

$$ p(D_k) = \sum_{\nu \in \mathcal{V}} p(\nu) p(D_\nu) $$

onde $\mathcal{V}$ é o conjunto de todas as partições possíveis de $D_k$, $p(\nu)$ é a probabilidade da partição $\nu$, $m_\nu$ é o número de clusters na partição $\nu$, $n_l^\nu$ é o número de pontos no cluster $l$ da partição $\nu$, $D_l^\nu$ são os pontos no cluster $l$ da partição $\nu$ e $n_k$ é o número de pontos em $D_k$ [^26].

A probabilidade da partição $p(\nu)$ é dada por [^26]:
$$ p(\nu) = \frac{\alpha^{m_\nu} \prod_{l=1}^{m_\nu} \Gamma(n_l^\nu)}{\Gamma(n_k + \alpha)} $$

onde $\alpha$ é um parâmetro de concentração [^26].

A probabilidade dos dados dado a partição $p(D_\nu)$ é dada por [^26]:
$$ p(D_\nu) = \prod_{l=1}^{m_\nu} p(D_l^\nu) $$

O BHC calcula $p(D_k | T_k)$, o qual é similar a $p(D_k)$, exceto que ele só soma partições que são consistentes com a árvore $T_k$ [^26].

### Conclusão
O BHC oferece uma abordagem Bayesiana rigorosa para o clustering hierárquico, superando as limitações das métricas de similaridade *ad hoc* utilizadas nos métodos tradicionais [^25]. Ao empregar testes de hipóteses Bayesianas para guiar o processo de fusão de clusters, o BHC fornece uma estrutura mais robusta e interpretável, permitindo a avaliação da probabilidade de diferentes estruturas hierárquicas [^25]. A conexão com os DPMMs fornece uma base teórica sólida e permite o uso de algoritmos eficientes para a computação das probabilidades de fusão [^26]. Embora o BHC possa ser computacionalmente mais intensivo do que os métodos tradicionais, sua capacidade de fornecer uma representação probabilística da estrutura de clusters o torna uma ferramenta valiosa para a análise exploratória de dados e a inferência estatística [^25].

### Referências
[^1]: Clustering is the process of grouping similar objects together.
[^19]: Note that agglomerative and divisive clustering are both just heuristics, which do not optimize any well-defined objective function.
[^25]: Here we present one particular approach called Bayesian hierarchical clustering (Heller and Ghahramani 2005).
[^26]: In this section, we will establish the connection between BHC and DPMMs.
<!-- END -->