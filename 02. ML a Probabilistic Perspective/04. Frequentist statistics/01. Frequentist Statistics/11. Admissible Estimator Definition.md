## Admissibility of Estimators in Frequentist Statistics

### Introdução
No contexto da teoria da decisão frequentista, onde o objetivo é escolher o melhor estimador ou regra de decisão, um conceito fundamental é o de **admissibilidade**. Este conceito surge da necessidade de comparar diferentes estimadores, mesmo quando a verdadeira distribuição dos dados é desconhecida [^1]. A admissibilidade fornece um critério para eliminar estimadores que são estritamente piores do que outros, independentemente do valor do parâmetro verdadeiro. Este capítulo explora em detalhe o conceito de admissibilidade, fornecendo definições precisas e exemplos ilustrativos, baseando-se nos princípios da estatística frequentista.

### Conceitos Fundamentais

Em estatística frequentista, a avaliação de um estimador $\delta$ é feita através da sua **função de risco** $R(\theta^*, \delta)$, que representa a perda esperada ao usar $\delta$ quando o verdadeiro valor do parâmetro é $\theta^*$ [^1, 5]. A função de risco é definida como:
$$R(\theta^*, \delta) = E_{p(\mathcal{D}|\theta^*)}[L(\theta^*, \delta(\mathcal{D}))] = \int L(\theta^*, \delta(\mathcal{D}))p(\mathcal{D}|\theta^*)d\mathcal{D}$$
onde $\mathcal{D}$ representa os dados amostrados, $L(\theta^*, \delta(\mathcal{D}))$ é a função de perda que quantifica a penalidade por usar a estimativa $\delta(\mathcal{D})$ quando o verdadeiro valor é $\theta^*$, e $p(\mathcal{D}|\theta^*)$ é a distribuição dos dados dado o parâmetro verdadeiro $\theta^*$ [^5].

**Definição de Dominância:** Um estimador $\delta_1$ domina um estimador $\delta_2$ se, para todo $\theta \in \Theta$,
$$R(\theta, \delta_1) \leq R(\theta, \delta_2)$$
e existe pelo menos um $\theta_0 \in \Theta$ tal que
$$R(\theta_0, \delta_1) < R(\theta_0, \delta_2)$$
onde $\Theta$ é o espaço de parâmetros [^7]. Em outras palavras, $\delta_1$ tem um risco menor ou igual a $\delta_2$ para todos os valores possíveis de $\theta$, e tem um risco estritamente menor para pelo menos um valor de $\theta$.

**Definição de Admissibilidade:** Um estimador $\delta$ é dito admissível se não é estritamente dominado por nenhum outro estimador [^7]. Formalmente, $\delta$ é admissível se não existe outro estimador $\delta\'$ tal que
$$R(\theta, \delta\') \leq R(\theta, \delta) \quad \forall \theta \in \Theta$$
e
$$R(\theta_0, \delta\') < R(\theta_0, \delta) \quad \text{para algum } \theta_0 \in \Theta$$

**Exemplo:** Considere o problema de estimar a média de uma distribuição Gaussiana com variância conhecida [^7]. Sejam $X_1, ..., X_N$ amostras i.i.d. de uma distribuição $N(\theta^*, \sigma^2=1)$. Algumas possíveis regras de decisão (estimadores) são:
*   $\delta_1(x) = \bar{x}$, a média amostral.
*   $\delta_2(x) = \tilde{x}$, a mediana amostral.
*   $\delta_3(x) = \theta_0$, um valor fixo.
*   $\delta_\kappa(x) = \frac{N}{N+\kappa}\bar{x} + \frac{\kappa}{N+\kappa}\theta_0$, a média posterior sob uma priori $N(\theta|\theta_0, \sigma^2/\kappa)$.

A média amostral $\bar{x}$ é um estimador não viesado, e sua função de risco (MSE) é $\frac{\sigma^2}{N}$ [^7]. Para $\delta_3(x)=\theta_0$, o MSE é $(\theta^* - \theta_0)^2$ [^7].  O estimador $\delta_\kappa(x)$ é uma combinação da média amostral e um valor fixo $\theta_0$ [^7].  A admissibilidade desses estimadores depende das circunstâncias.

**Teorema 6.3.3 [^10]:** Seja $X \sim N(\theta, 1)$ e considere a estimação de $\theta$ sob perda quadrática. Seja $\delta_1(x) = \theta_0$, uma constante independente dos dados. Este é um estimador admissível.

*Prova:* Suponha que não seja. Então existe algum outro estimador $\delta_2$ com risco menor, tal que $R(\theta^*, \delta_2) \leq R(\theta^*, \delta_1)$, onde a desigualdade deve ser estrita para algum $\theta^*$. Suponha que o verdadeiro parâmetro seja $\theta^* = \theta_0$. Então $R(\theta^*, \delta_1) = 0$ e
$$R(\theta^*, \delta_2) = \int (\delta_2(x) - \theta_0)^2 p(x|\theta_0)dx$$
Como $0 \leq R(\theta^*, \delta_2) \leq R(\theta^*, \delta_1)$ para todo $\theta^*$, e $R(\theta_0, \delta_1) = 0$, temos $R(\theta_0, \delta_2) = 0$ e, portanto, $\delta_2(x) = \theta_0 = \delta_1(x)$. Assim, a única maneira de $\delta_2$ evitar ter um risco maior que $\delta_1$ em algum ponto específico $\theta_0$ é sendo igual a $\delta_1$. Portanto, não existe outro estimador $\delta_2$ com risco estritamente menor, e $\delta_1$ é admissível. $\blacksquare$

**Paradoxo de Stein [^9]:** Suponha que temos $N$ variáveis aleatórias i.i.d. $X_i \sim N(\theta_i, 1)$, e queremos estimar os $\theta_i$. O estimador óbvio é o MLE, que neste caso define $\hat{\theta}_i = x_i$. Acontece que este é um estimador inadmissível sob perda quadrática, quando $N > 4$.

**Estimador de James-Stein [^9]:** Para mostrar isso, basta construir um estimador que seja melhor. O estimador de James-Stein é um desses estimadores e é definido como:
$$\hat{\theta}_i = B\bar{x} + (1 - B)x_i = \bar{x} + (1 - B)(x_i - \bar{x})$$
onde $\bar{x} = \frac{1}{N}\sum_{i=1}^N x_i$ e $0 < B < 1$ é alguma constante de ajuste. Este estimador "encolhe" os $\theta_i$ em direção à média geral.

### Conclusão
A admissibilidade é um critério importante na estatística frequentista para selecionar estimadores que não são dominados por outros [^7]. No entanto, como demonstrado pelo paradoxo de Stein [^9], um estimador admissível nem sempre é o melhor em todas as situações. A escolha do estimador ideal depende do contexto específico e dos objetivos da análise [^1].

### Referências
[^1]: Capítulo 6 do texto fornecido.
[^5]: Seção 6.3 do texto fornecido.
[^7]: Seção 6.3.3 do texto fornecido.
[^9]: Seção 6.3.3.2 do texto fornecido.
[^10]: Teorema 6.3.3 do texto fornecido.
<!-- END -->