## Distribuição Amostral de um Estimador em Estatística Frequentista

### Introdução
Em estatística frequentista, a inferência sobre parâmetros populacionais é realizada sem atribuir distribuições de probabilidade a esses parâmetros. Em vez disso, o foco está na variabilidade dos estimadores sob repetidas amostragens. Este capítulo explora a **distribuição amostral de um estimador**, um conceito central na abordagem frequentista [^1]. A distribuição amostral permite quantificar a incerteza associada a uma estimativa de parâmetro. Este capítulo também abordará o método de Bootstrap para aproximar a distribuição amostral e a teoria assintótica para o estimador de máxima verossimilhança (MLE).

### Conceitos Fundamentais

Em estatística frequentista, um **parâmetro** $\theta$ é considerado um valor fixo, mas desconhecido, da população [^1]. Uma **estimativa** de $\theta$, denotada por $\hat{\theta}$, é obtida aplicando um **estimador** $\delta$ a um conjunto de dados $D$, tal que $\hat{\theta} = \delta(D)$ [^1]. A principal diferença em relação à abordagem Bayesiana é que, na frequentista, os dados $D$ são considerados aleatórios, enquanto o parâmetro $\theta$ é fixo, o oposto da visão Bayesiana [^1].

A **distribuição amostral** de um estimador $\delta$ é a distribuição de probabilidade dos valores de $\delta(D)$ que seriam observados se amostras aleatórias repetidas de tamanho $N$ fossem retiradas da população [^1]. Formalmente, considere $S$ diferentes conjuntos de dados $D^{(s)}$ amostrados de um modelo verdadeiro, $p(\cdot|\theta^*)$, onde $\theta^*$ é o parâmetro verdadeiro [^2]. Assim, $D^{(s)} = \\{x_i\\}_{i=1}^N$, onde $x_i \sim p(\cdot|\theta^*)$, e $s = 1:S$ indexa o conjunto de dados amostrado [^2]. Aplicando o estimador $\delta(\cdot)$ a cada $D^{(s)}$, obtemos um conjunto de estimativas $\\{\delta(D^{(s)})\\}_{s=1}^S$ [^2]. Quando $S \to \infty$, a distribuição induzida em $\delta(\cdot)$ é a distribuição amostral do estimador [^2].

**Bootstrap:**
O *bootstrap* é uma técnica de Monte Carlo que aproxima a distribuição amostral [^2]. É particularmente útil quando o estimador é uma função complexa dos parâmetros verdadeiros. Se conhecêssemos os parâmetros verdadeiros $\theta^*$, poderíamos gerar $S$ conjuntos de dados falsos, cada um de tamanho $N$, da distribuição verdadeira, $x \sim p(\cdot|\theta^*)$, para $s = 1:S$, $i = 1:N$ [^2]. Poderíamos então calcular nosso estimador a partir de cada amostra, $\theta^s = \delta(x_{1:N})$, e usar a distribuição empírica das amostras resultantes como nossa estimativa da distribuição amostral [^2]. Como $\theta$ é desconhecido, o *bootstrap paramétrico* gera as amostras usando $\hat{\theta}(D)$ [^2]. Uma alternativa, chamada *bootstrap não paramétrico*, é amostrar o $x_i$ (com substituição) dos dados originais $D$, e então computar a distribuição induzida como antes [^2].

**Teoria Assintótica para o MLE:**
Em alguns casos, a distribuição amostral pode ser computada analiticamente. Sob certas condições, quando o tamanho da amostra tende ao infinito, a distribuição amostral do MLE se torna Gaussiana [^3]. O centro da Gaussiana será o MLE $\hat{\theta}$ [^3]. A variância do estimador estará inversamente relacionada à curvatura da superfície de verossimilhança em seu pico [^3]. Se a curvatura é grande, o pico será "nítido" e a variância baixa; neste caso, a estimativa é "bem determinada" [^3]. Por outro lado, se a curvatura é pequena, o pico será quase "plano", então a variância é alta [^3].

Formalizando essa intuição, defina a **função de escore** como o gradiente do log da verossimilhança avaliado em algum ponto $\theta$:\n$$s(\theta) \triangleq \nabla_\theta \log p(D|\theta)|_{\hat{\theta}}$$ [^3]\nA **matriz de informação observada** é definida como o gradiente negativo da função de escore, ou equivalentemente, o Hessiano do NLL:\n$$J(\hat{\theta}(D)) \triangleq -\nabla s(\theta) = -\nabla^2_\theta \log p(D|\theta)|_{\hat{\theta}}$$ [^3]\nEm 1D, isso se torna\n$$J(\hat{\theta}(D)) = -\frac{d^2}{d\theta^2} \log p(D|\theta)|_{\hat{\theta}}$$ [^3]\nComo estamos estudando a distribuição amostral, $D = (x_1, ..., x_N)$ é um conjunto de variáveis aleatórias [^3]. A **matriz de informação de Fisher** é definida como o valor esperado da matriz de informação observada:\n$$I_N(\theta|\theta^*) \triangleq E_{\theta^*}[J(\hat{\theta}|D)]$$ [^3]

Sob condições de regularidade, pode-se mostrar que\n$$\hat{\theta} \to N(\theta^*, I_N(\theta^*)^{-1})$$ [^4]\nquando $N \to \infty$ [^4]. Isso significa que a distribuição amostral do MLE é *assintoticamente normal* [^4].

### Conclusão

A distribuição amostral de um estimador é um conceito fundamental em estatística frequentista, fornecendo uma maneira de quantificar a incerteza associada a uma estimativa de parâmetro [^1]. Embora computar a distribuição amostral possa ser desafiador, técnicas como o bootstrap e a teoria assintótica do MLE fornecem ferramentas valiosas para aproximá-la [^2, 3]. Compreender a distribuição amostral é essencial para realizar inferências estatísticas válidas e tomar decisões informadas com base em dados amostrais.

### Referências
[^1]: Page 1, Section 6.2
[^2]: Page 2, Section 6.2.1
[^3]: Page 3, Section 6.2.2
[^4]: Page 4, Section 6.2.2
<!-- END -->