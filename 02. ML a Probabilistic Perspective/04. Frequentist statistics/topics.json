{
  "topics": [
    {
      "topic": "Frequentist Statistics",
      "sub_topics": [
        "Frequentist statistics, also known as classical or orthodox statistics, is an approach to statistical inference that avoids treating parameters as random variables, differing from the Bayesian approach which uses priors and Bayes' rule. It relies on the concept of a sampling distribution, which is the distribution of an estimator when applied to multiple datasets sampled from the same true but unknown distribution, thus forming the basis for modeling uncertainty based on variation across repeated trials.",
        "In frequentist statistics, a parameter estimate \u03b8 is computed by applying an estimator \u03b4 to data D, such that \u03b8 = \u03b4(D), where the parameter is viewed as fixed and the data as random, contrasting with the Bayesian view where the parameter is treated as a random variable and the data as fixed. The uncertainty in the parameter estimate is measured by computing the sampling distribution of the estimator.",
        "The bootstrap method is a Monte Carlo technique used to approximate the sampling distribution, particularly useful when the estimator is a complex function of the true parameters. It involves generating multiple fake datasets from either the true distribution (parametric bootstrap) or by resampling from the original data (non-parametric bootstrap) to estimate the sampling distribution empirically.",
        "Large sample theory for the Maximum Likelihood Estimator (MLE) states that, under certain conditions, as the sample size tends to infinity, the sampling distribution of the MLE becomes Gaussian. The center of the Gaussian is the MLE, and the variance is inversely related to the curvature of the likelihood surface at its peak, formalized by the score function and Fisher information matrix. This requires each parameter to \"see\" an infinite amount of data and for the model to be identifiable, which is formalized by the concept of asymptotic normality.",
        "The score function is defined as the gradient of the log-likelihood evaluated at a point \u03b8 (s(\u03b8) = \u2207\u03b8 log p(D|\u03b8)), while the observed information matrix measures the curvature of the log-likelihood function (J(\u03b8(D)) = \u2212\u2207s(\u03b8) = \u2212\u2207\u00b2\u03b8 log p(D|\u03b8)), both used in assessing the precision of parameter estimates.",
        "The Fisher information matrix, IN(\u03b8|\u03b8*) = E\u03b8\u2217[J(\u03b8\u0302|D)], represents the expected value of the observed information matrix and is crucial for determining the asymptotic variance of the MLE. The approximate standard errors of the parameter estimates are derived from the inverse of the Fisher information matrix, providing a measure of the precision of the estimates.",
        "Frequentist decision theory involves a loss function and a likelihood but lacks a prior, thus precluding the derivation of an optimal estimator based on posterior expected loss. Instead, any estimator or decision procedure \u03b4 : X \u2192 A can be chosen, necessitating alternative criteria for comparing estimators, such as risk and minimaxity.",
        "The expected loss or risk of an estimator is defined as the expected value of the loss function with respect to the sampling distribution of the estimator, where the expectation is taken over the data sampled from nature's distribution represented by the true parameter \u03b8*.",
        "Bayes risk, or integrated risk, is obtained by putting a prior on \u03b8* and then defining the risk as the expected value of the risk with respect to the prior, leading to a Bayes estimator or Bayes decision rule that minimizes the expected risk.",
        "Minimax risk is an alternative approach that defines the maximum risk of an estimator as the maximum value of the risk over all possible values of \u03b8*, and a minimax rule is one that minimizes the maximum risk.",
        "An estimator is admissible if it is not strictly dominated by any other estimator, meaning that there is no other estimator that has lower risk for all values of \u03b8*.",
        "The Mean Squared Error (MSE) can be decomposed into squared bias plus variance, which creates the bias-variance trade-off, where it might be wise to use a biased estimator if it reduces the variance, assuming the goal is to minimize squared error."
      ]
    },
    {
      "topic": "Empirical Risk Minimization",
      "sub_topics": [
        "Empirical risk minimization (ERM) is a method used in frequentist decision theory to estimate the risk function when the true data distribution is unknown. ERM seeks to minimize the empirical risk,  R\u2091mp(D, \u03b4) = (1/N) \u03a3\u1d62\u208c\u2081\u1d3a L(y\u1d62, \u03b4(x\u1d62)), calculated using the training data, where L is a loss function and \u03b4(x) is the prediction function. This is done by using the empirical distribution, derived from training data, to approximate the true distribution and minimize the empirical risk, often estimating the misclassification rate or mean squared error. However, this often results in overfitting.",
        "Regularized risk minimization (RRM) extends ERM by adding a complexity penalty to the objective function, controlling overfitting by penalizing complex prediction functions. RRM adds a complexity penalty to the objective function, R'(D, \u03b4) = R\u2091mp(D, \u03b4) + \u03bbC(\u03b4), where C(\u03b4) measures the complexity of the prediction function and \u03bb controls the strength of the penalty. This is equivalent to MAP estimation when the loss function is negative log likelihood and the regularizer is a negative log prior.",
        "Structural risk minimization (SRM) is a principle for selecting the model complexity penalty by estimating the risk using methods like cross-validation or theoretical upper bounds. SRM aims to pick the complexity penalty \u03bb by minimizing an estimate of the risk, \u03bb\u0302 = argmin\u03bb R(\u03b4\u03bb), using techniques like cross-validation or theoretical upper bounds on the risk to avoid underestimating the true risk due to optimism of the training error.",
        "Cross-validation (CV) is a technique for estimating the risk of an estimator by dividing the data into folds and using each fold as a validation set while training on the remaining data. Cross-validation (CV) estimates the risk of an estimator by dividing the training data into K folds and averaging the loss over each fold, providing a more robust estimate of the generalization error compared to using a single validation set.",
        "The one standard error rule is a heuristic for model selection in cross-validation, picking the simplest model whose risk is no more than one standard error above the best model's risk.",
        "Statistical learning theory (SLT) provides bounds on the generalization error, relating the risk to the empirical risk, sample size, and the size of the hypothesis space. SLT provides theoretical bounds on the risk R(p*, h) in terms of the empirical risk R\u2091mp(D, h), the sample size N, and the size of the hypothesis space H, offering theoretical guarantees on the generalization error."
      ]
    },
    {
      "topic": "Pathologies of Frequentist Statistics",
      "sub_topics": [
        "Frequentist statistics can exhibit counter-intuitive behavior in confidence intervals, where the interval derived from the sampling distribution may not align with the actual knowledge of the parameter given the observed data. A confidence interval is an interval derived from the sampling distribution of an estimator such that, if hypothetical future data \\\\( \\\\tilde{D} \\\\) is sampled from \\\\( \\\\theta \\\\), the interval \\\\( (l(\\\\tilde{D}), u(\\\\tilde{D})) \\\\) contains \\\\( \\\\theta \\\\) with probability \\\\( 1 - \\\\alpha \\\\), but this definition can lead to counter-intuitive results, such as knowing that \u03b8 must be a certain value but only having a certain level of \"confidence\" in this fact.",
        "P-values, used in null hypothesis significance testing (NHST), are defined as the probability of observing a test statistic as large or larger than the one actually observed under the null hypothesis, but they tend to overstate evidence against the null and are sensitive to stopping rules. P-values are often considered harmful due to their tendency to overstate the evidence against the null hypothesis and their dependence on decisions about when to stop collecting data.",
        "Null hypothesis significance testing (NHST) is an approach that rejects the null hypothesis if the p-value is less than some threshold, but p-values tend to overstate the evidence against the null and are thus very trigger happy. NHST can overstate evidence against the null and never favor the null.",
        "The likelihood principle states that inference should be based on the likelihood of the observed data, not based on hypothetical future data that you have not observed. The likelihood principle, which says that inference should be based on the likelihood of the observed data, not on hypothetical future data, is violated by frequentist inference and is satisfied by Bayesian inference. The likelihood principle is violated by frequentist inference, leading to various pathologies. Bayesian methods satisfy the likelihood principle and do not suffer from these pathologies."
      ]
    },
    {
      "topic": "Frequentist Decision Theory",
      "sub_topics": [
        "Frequentist decision theory uses a loss function and a likelihood, but lacks a prior, thus not providing an automatic way to derive an optimal estimator; instead, any estimator or decision procedure \u03b4 : X \u2192 A can be chosen.",
        "The frequentist risk, R(\u03b8*, \u03b4), is the expected loss of an estimator given data sampled from nature's distribution, represented by parameter \u03b8*, contrasting with the Bayesian approach that averages over \u03b8 and conditions on the observed data D. The risk function R(\u03b8*, \u03b4) = E\u209a(D|\u03b8\u2217)[L(\u03b8*, \u03b4(D))] defines the expected loss of an estimator \u03b4 given data sampled from the true parameter \u03b8*, representing the average loss over the sampling distribution of the estimator. This contrasts with Bayesian posterior expected loss, which averages over the posterior distribution of \u03b8 given the observed data D.",
        "Bayes risk, RB(\u03b4), is the integrated risk of an estimator, calculated by putting a prior on \u03b8* and then integrating R(\u03b8*, \u03b4) over p(\u03b8*)d\u03b8*, and a Bayes estimator minimizes this expected risk. Bayes risk, defined as RB(\u03b4) = E\u209a(\u03b8\u2217)[R(\u03b8*, \u03b4)] = \u222b R(\u03b8*, \u03b4)p(\u03b8*)d\u03b8*, integrates the frequentist risk over a prior distribution p(\u03b8*) to provide a single measure of estimator quality that does not depend on knowing the true parameter \u03b8*. A Bayes estimator minimizes this integrated risk.",
        "A minimax rule minimizes the maximum risk, Rmax(\u03b4), which is the maximum value of the risk function R(\u03b8*, \u03b4) over all possible values of \u03b8*, providing a conservative approach to decision-making. Minimax risk provides an alternative approach by defining the maximum risk of an estimator as Rmax(\u03b4) = max\u03b8\u2217 R(\u03b8*, \u03b4) and selecting the minimax rule \u03b4MM = argmin\u03b4 Rmax(\u03b4), which minimizes the maximum possible risk. This approach is conservative, aiming to minimize the worst-case performance of the estimator.",
        "An admissible estimator is one that is not strictly dominated by any other estimator, meaning there is no other estimator that performs better for all values of \u03b8, ensuring a baseline level of performance. An estimator is considered admissible if no other estimator dominates it, meaning there is no other estimator \u03b4\u2081 such that R(\u03b8, \u03b4\u2081) \u2264 R(\u03b8, \u03b4\u2082) for all \u03b8 \u2208 \u0398, with strict inequality for some \u03b8. Admissibility restricts the search for good estimators to those that are not strictly worse than any other estimator.",
        "The James-Stein estimator, given by \\\\( \\\\hat{\\\\theta}_i = B\\\\bar{x} + (1 - B)x_i \\\\), shrinks individual estimates towards the overall mean \\\\( \\\\bar{x} \\\\), and it has lower frequentist risk (MSE) than the MLE (sample mean) for \\\\( N > 4 \\\\), illustrating Stein's paradox.",
        "Stein's paradox illustrates that the MLE can be inadmissible under quadratic loss when estimating multiple parameters, as demonstrated by the James-Stein estimator, which shrinks individual estimates towards the overall mean to reduce frequentist risk."
      ]
    },
    {
      "topic": "Desirable Properties of Estimators",
      "sub_topics": [
        "A consistent estimator recovers the true parameters that generated the data as the sample size goes to infinity, i.e., \u03b8\u0302(D) \u2192 \u03b8* as |D| \u2192 \u221e, which is a useful theoretical property. A consistent estimator is one that converges in probability to the true parameter value as the sample size goes to infinity, allowing the estimator to recover the true parameters. The MLE is a consistent estimator, which means maximizing the likelihood is equivalent to minimizing the KL divergence between the true distribution and the estimated distribution.",
        "The bias of an estimator is defined as bias(\u03b8\u0302) = E\u209a(D|\u03b8\u2217)[\u03b8\u0302(D) - \u03b8*], where \u03b8* is the true parameter value; an unbiased estimator has a bias of zero, meaning its sampling distribution is centered on the true parameter. An unbiased estimator has a bias of zero, meaning that the sampling distribution is centered on the true parameter, but this is not always a desirable property. An unbiased estimator has a bias of zero, meaning its sampling distribution is centered on the true parameter value, formally defined as bias(\u03b8\u0302(D)) = E\u209a(D|\u03b8\u2217)[\u03b8\u0302(D) \u2212 \u03b8\u2217] = 0. While unbiasedness is desirable, it is not always the most important criterion, as biased estimators can sometimes have lower overall risk.",
        "The Cramer-Rao lower bound provides a lower bound on the variance of any unbiased estimator, stating that var(\u03b8\u0302) \u2265 1/(nI(\u03b8\u2080)), where I(\u03b8\u2080) is the Fisher information matrix. The Cramer-Rao lower bound provides a lower limit on the variance of any unbiased estimator, offering a benchmark for how efficiently an unbiased estimator can estimate the parameter. The MLE is asymptotically optimal because it achieves this lower bound.",
        "The bias-variance tradeoff illustrates that the mean squared error (MSE) can be decomposed into variance plus squared bias, MSE = variance + bias\u00b2, indicating that it might be wise to use a biased estimator if it reduces variance enough to minimize squared error. The bias-variance tradeoff highlights that minimizing squared error (MSE) involves balancing bias and variance, as MSE = variance + bias\u00b2. It may be beneficial to use a biased estimator if it substantially reduces variance, leading to a lower overall MSE.",
        "In ridge regression, the bias-variance tradeoff is managed by using a Gaussian prior, p(w) = N(w|0, \u03bb\u207b\u00b9I), which encourages smaller weights to reduce overfitting; the precision term \u03bb controls the strength of this prior, resulting in a biased estimate when \u03bb > 0.",
        "Empirical risk minimization (ERM) is the task of finding a decision procedure to minimize the empirical risk, which is the average loss over the training data, but this can lead to overfitting.",
        "Regularized risk minimization (RRM) adds a complexity penalty to the objective function to prevent overfitting, where the complexity penalty is controlled by a parameter \u03bb, and this is equivalent to MAP estimation if the loss function is negative log likelihood and the regularizer is a negative log prior. Regularized risk minimization (RRM) addresses overfitting by adding a complexity penalty to the empirical risk, R'(D, \u03b4) = R\u2091mp(D, \u03b4) + \u03bbC(\u03b4), where C(\u03b4) measures the complexity of the prediction function and \u03bb controls the strength of the penalty. This is equivalent to MAP estimation when the loss function is negative log likelihood and the regularizer is a negative log prior.",
        "Structural risk minimization (SRM) is a principle that says we should fit the model by minimizing the regularized risk, but we cannot use the training set to pick \u03bb because this will underestimate the true risk, so we can use cross-validation or theoretical upper bounds on the risk. Cross-validation (CV) is used to estimate the risk of an estimator by partitioning the data into folds, training the model on a subset of the data, and evaluating its performance on the remaining fold. Structural risk minimization (SRM) uses CV to select the model complexity that minimizes the estimated risk.",
        "Cross-validation (CV) is a technique to estimate the risk of some estimator by using a validation set, and if we don't have a separate validation set, we can use k-fold cross-validation.",
        "The one standard error rule is a heuristic for picking a model from noisy estimates of the risk, which picks the simplest model whose risk is no more than one standard error above the risk of the best model.",
        "Statistical learning theory (SLT) tries to bound the risk in terms of the empirical risk, the sample size, and the size of the hypothesis space, and for finite hypothesis spaces, the probability that our estimate of the error rate will be more than \u03b5 wrong is upper bounded by a function of these quantities. Statistical learning theory (SLT) provides theoretical bounds on the generalization error, relating the risk to the empirical risk, sample size, and complexity of the hypothesis space. For finite hypothesis spaces, the probability that the error rate is more than \u03b5 wrong is bounded by P(maxh\u2208H |R\u2091mp(D, h) \u2212 R(p\u2217, h)| > \u03b5) \u2264 2|H|e^(\u22122N\u03b5\u00b2).",
        "The Vapnik-Chervonenkis (VC) dimension is used to measure the complexity of infinite hypothesis spaces, and the key intuition behind statistical learning theory is that a model with low empirical risk is evidence of a low true risk if the hypothesis space is sufficiently constrained in size."
      ]
    }
  ]
}