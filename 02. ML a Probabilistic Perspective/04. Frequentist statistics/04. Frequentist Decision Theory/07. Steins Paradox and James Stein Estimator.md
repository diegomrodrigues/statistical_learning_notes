## Stein's Paradox: Inadmissibility of MLE under Quadratic Loss

### Introdução
Na teoria da decisão frequentista, a escolha de um estimador ideal é um problema central. Vimos [^6] que a teoria frequentista da decisão, ao contrário da Bayesiana, não fornece um método automático para derivar estimadores ótimos. Em vez disso, dependemos de funções de perda e da distribuição de amostragem do estimador. Este capítulo explora um fenômeno intrigante conhecido como o **paradoxo de Stein**, que demonstra que o estimador de máxima verossimilhança (MLE) pode ser *inadmissível* sob a perda quadrática quando se estimam múltiplos parâmetros [^1]. Este resultado, exemplificado pelo **estimador de James-Stein**, desafia a intuição de que o MLE é sempre uma boa escolha.

### Conceitos Fundamentais
O **paradoxo de Stein** surge no contexto da estimação de múltiplos parâmetros Gaussianos. Considere o cenário onde temos $N$ variáveis aleatórias independentes e identicamente distribuídas (iid) $X_i \sim \mathcal{N}(\theta_i, 1)$, e desejamos estimar os $\theta_i$ [^3]. O estimador óbvio é o MLE, que simplesmente define $\hat{\theta}_i = x_i$. No entanto, sob perda quadrática, este estimador é inadmissível quando $N \geq 4$ [^3].

Para demonstrar isso, precisamos construir um estimador que seja *melhor* que o MLE. O **estimador de James-Stein** é um desses estimadores, definido como [^3]:

$$\hat{\theta}_i^{JS} = \bar{x} + (1 - B)(x_i - \bar{x})$$

onde $\bar{x} = \frac{1}{N} \sum_{i=1}^{N} x_i$ é a média amostral e $0 < B < 1$ é uma constante de ajuste. Este estimador "encolhe" os $\theta_i$ em direção à média geral $\bar{x}$ [^3].  A escolha do valor de $B$ é crucial para o desempenho do estimador.

O paradoxo reside no fato de que, embora o MLE seja o melhor estimador para cada $\theta_i$ individualmente, o estimador de James-Stein, que *combina* informações de todas as amostras, possui um risco frequentista menor (erro quadrático médio - MSE) para $N > 4$ [^3]. Isso significa que, no geral, estimamos o vetor $\theta = (\theta_1, \theta_2, ..., \theta_N)$ de forma mais precisa ao "pooling" as informações.

O texto [^3] ilustra o paradoxo de Stein com um exemplo de QI de estudantes e a média de suas notas em um teste. Outro exemplo usa a média de chuva em Vancouver. A questão crucial é que, mesmo que as dimensões sejam qualitativamente diferentes, o "encolhimento" em direção à média geral ainda pode melhorar a precisão da estimativa conjunta.

Para entender por que o estimador de James-Stein funciona, considere o problema de estimar a norma do vetor $\theta$, isto é, $||\theta||^2$, a partir de uma única amostra $x \sim \mathcal{N}(\theta, I)$ [^3]. Um estimador simples é $||x||^2$, mas este sobrestimará o resultado, pois [^3]:

$$E[||x||^2] = E\Big[\sum_{i=1}^{N} x_i^2\Big] = \sum_{i=1}^{N} (1 + \theta_i^2) = N + ||\theta||^2$$

Consequentemente, podemos reduzir o risco ao "pooling" informações e "encolhendo" em direção à média geral [^3].

### Conclusão
O paradoxo de Stein demonstra que o MLE pode ser inadmissível sob perda quadrática na estimação de múltiplos parâmetros. O estimador de James-Stein, ao "encolher" as estimativas individuais em direção à média geral, pode reduzir o risco frequentista. Este resultado tem implicações significativas para a prática estatística, destacando a importância de considerar estimadores alternativos ao MLE, especialmente em problemas de alta dimensão. O texto [^3] aponta que no capítulo 5.6.2 é dada uma explicação Bayesiana para este resultado, e também cita (Efron and Morris 1975) como referência adicional.

### Referências
[^1]: Página 1
[^3]: Página 199
[^6]: Página 195
<!-- END -->