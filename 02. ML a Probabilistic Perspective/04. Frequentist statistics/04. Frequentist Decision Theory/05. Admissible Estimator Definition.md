## Admissibilidade de Estimadores na Teoria da Decisão Frequentista

### Introdução
Na teoria da decisão frequentista, o objetivo é selecionar o melhor estimador ou regra de decisão sem o uso de *priors* ou perdas esperadas *a posteriori* [^6]. Uma das propriedades desejáveis de um estimador é a **admissibilidade**, que garante que não há outro estimador que seja uniformemente melhor. Este capítulo se concentrará na admissibilidade de estimadores, explorando sua definição, implicações e exemplos.

### Conceitos Fundamentais

Um **estimador admissível** é aquele que não é estritamente dominado por nenhum outro estimador [^texto base]. Em outras palavras, não existe outro estimador que tenha um desempenho melhor para todos os valores de $\theta$ [^texto base]. Formalmente, um estimador $\delta_2$ é considerado admissível se não existe outro estimador $\delta_1$ tal que:

$$R(\theta, \delta_1) \leq R(\theta, \delta_2) \quad \forall \theta \in \Theta$$

com desigualdade estrita para algum $\theta$ [^texto base]. Aqui, $R(\theta, \delta)$ representa a função de risco do estimador $\delta$ para o valor do parâmetro $\theta$, e $\Theta$ é o espaço de parâmetros [^texto base].

**Importância da Admissibilidade:** A admissibilidade restringe a busca por bons estimadores àqueles que não são estritamente piores do que qualquer outro estimador [^texto base]. Isso garante um nível básico de desempenho e evita a seleção de estimadores que podem ser significativamente piores em certas regiões do espaço de parâmetros.

**Exemplo:**
Considere o problema de estimar a média de uma distribuição Gaussiana [^7]. Os possíveis estimadores incluem a média amostral ($\delta_1(x) = \bar{x}$), a mediana amostral ($\delta_2(x) = \tilde{x}$), um valor fixo ($\delta_3(x) = \theta_0$), e a média *a posteriori* sob um *prior* Gaussiano ($\delta_\kappa(x)$) [^7]. O risco correspondente à perda quadrática (MSE) pode ser calculado para cada um destes [^7]. Conforme mencionado em [^8], a mediana amostral é um estimador inadmissível para este problema, pois a média amostral sempre apresenta um risco menor.

**Teorema:** (Wald, 1950) Toda regra de decisão admissível é uma regra de decisão de Bayes com respeito a alguma distribuição *a priori*, possivelmente imprópria [^6].

Este teorema demonstra uma conexão profunda entre as abordagens frequentista e Bayesiana [^6]. Ele implica que, para minimizar o risco frequentista, é necessário adotar uma perspectiva Bayesiana [^6].

**Estimadores Minimax:** Um estimador minimax é aquele que minimiza o risco máximo [^6]. Formalmente, o risco máximo de um estimador $\delta$ é definido como:

$$R_{max}(\delta) = \max_{\theta \in \Theta} R(\theta, \delta)$$

Um estimador minimax $\delta_{MM}$ é então:

$$\delta_{MM} = \underset{\delta}{\operatorname{argmin}} R_{max}(\delta)$$

Embora os estimadores minimax tenham um certo apelo, eles podem ser muito pessimistas [^7]. De fato, todos os estimadores minimax são equivalentes aos estimadores de Bayes sob um *prior* menos favorável [^7].

**Paradoxo de Stein:** O paradoxo de Stein demonstra que, em certas situações, é possível construir estimadores que dominam uniformemente o estimador de máxima verossimilhança (MLE) [^9]. Isso ocorre mesmo quando os parâmetros a serem estimados são independentes.

**Admissibilidade Não é Suficiente:** Conforme mencionado em [^9], a admissibilidade não é suficiente para garantir um bom estimador. É possível construir estimadores admissíveis que ainda tenham um desempenho ruim.

**Exemplo:** Seja $X \sim N(\theta, 1)$ e considere estimar $\theta$ sob perda quadrática [^10]. Seja $\delta_1(x) = \theta_0$, uma constante independente dos dados [^10]. Este é um estimador admissível [^10]. Para provar isto, suponha que não seja. Então existe algum outro estimador $\delta_2$ com risco menor, tal que $R(\theta^*, \delta_2) \leq R(\theta^*, \delta_1)$ [^10].

### Conclusão

A admissibilidade é um critério importante na seleção de estimadores na teoria da decisão frequentista [^texto base]. No entanto, não é o único critério a ser considerado [^9]. Estimadores admissíveis garantem que não haja outro estimador que seja uniformemente melhor, mas podem ainda ter um desempenho ruim em certas situações [^9]. Outras propriedades, como consistência, não-viesamento e variância mínima, também devem ser consideradas ao escolher um estimador [^11, ^10]. A conexão entre admissibilidade e regras de decisão de Bayes fornece uma ligação valiosa entre as abordagens frequentista e Bayesiana [^6].

### Referências
[^texto base]: Trecho do prompt que define o conceito de admissibilidade.
[^6]: Seção 6.3 de [^1], Frequentist decision theory.
[^7]: Seção 6.3.3 de [^1], Admissible estimators.
[^8]: Seção 6.3.3 de [^1], Exemplo de estimadores admissíveis.
[^9]: Seção 6.3.3.2 de [^1], Stein's paradox *.
[^10]: Seção 6.3.3.3 de [^1], Admissibility is not enough.
[^11]: Seção 6.4 de [^1], Desirable properties of estimators.

<!-- END -->