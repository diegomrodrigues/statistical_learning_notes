## Consistência de Estimadores

### Introdução
No campo da estatística, uma das propriedades desejáveis para um **estimador** é a sua capacidade de convergir para o verdadeiro valor do parâmetro à medida que o tamanho da amostra aumenta. Essa propriedade é conhecida como **consistência** [^200]. Este capítulo explora em detalhe o conceito de consistência, sua importância teórica e a relação com outros conceitos estatísticos, como a **verossimilhança máxima (MLE)**.

### Conceitos Fundamentais
Um estimador $\hat{\theta}(D)$ é dito **consistente** se, à medida que o tamanho da amostra $|D|$ tende ao infinito, o estimador converge em probabilidade para o verdadeiro valor do parâmetro $\theta^*$, ou seja, $\hat{\theta}(D) \rightarrow \theta^*$ quando $|D| \rightarrow \infty$ [^200].

*Em termos mais formais, um estimador consistente é aquele que converge em probabilidade para o verdadeiro valor do parâmetro à medida que o tamanho da amostra tende ao infinito, permitindo que o estimador recupere os parâmetros verdadeiros* [^200].

A consistência é uma propriedade teórica útil, pois garante que, com dados suficientes, o estimador será capaz de fornecer uma estimativa precisa do parâmetro de interesse [^200]. No entanto, é importante notar que a consistência é uma propriedade assintótica, o que significa que ela só se aplica quando o tamanho da amostra tende ao infinito. Em amostras finitas, um estimador consistente pode não ser o melhor estimador possível.

**Estimador de Máxima Verossimilhança (MLE)**

O **estimador de máxima verossimilhança (MLE)** é um estimador consistente [^200]. Isso significa que, sob certas condições, o MLE converge em probabilidade para o verdadeiro valor do parâmetro à medida que o tamanho da amostra aumenta.

*A razão intuitiva para a consistência do MLE é que maximizar a verossimilhança é equivalente a minimizar a divergência de Kullback-Leibler (KL) entre a distribuição verdadeira e a distribuição estimada* [^200].

A **divergência KL** é uma medida da diferença entre duas distribuições de probabilidade. Minimizar a divergência KL entre a distribuição verdadeira $p(\cdot|\theta^*)$ e a distribuição estimada $p(\cdot|\hat{\theta})$ significa que estamos encontrando a distribuição estimada que é mais próxima da distribuição verdadeira. No limite, quando o tamanho da amostra tende ao infinito, o MLE irá convergir para o verdadeiro valor do parâmetro, alcançando a divergência KL mínima possível, que é zero [^200].

É importante notar que a consistência do MLE depende de certas condições, como a identificabilidade do modelo e a suavidade da função de verossimilhança. Se essas condições não forem satisfeitas, o MLE pode não ser consistente.

### Conclusão
A consistência é uma propriedade desejável para um estimador, pois garante que, com dados suficientes, o estimador será capaz de fornecer uma estimativa precisa do parâmetro de interesse. O MLE é um estimador consistente, o que o torna uma escolha popular em muitas aplicações estatísticas. No entanto, é importante notar que a consistência é uma propriedade assintótica e que o MLE pode não ser o melhor estimador possível em amostras finitas.

### Referências
[^200]: Pattern Recognition and Machine Learning, Christopher Bishop, Springer, 2006, página 200.

<!-- END -->