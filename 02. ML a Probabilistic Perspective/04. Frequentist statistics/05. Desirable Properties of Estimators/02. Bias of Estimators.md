## O Viés de um Estimador

### Introdução

Em estatística frequentista, um dos objetivos principais é encontrar **estimadores** que possuam boas propriedades. Já exploramos diversas propriedades desejáveis, como consistência, e agora nos aprofundaremos no conceito de viés, uma característica crucial na avaliação da qualidade de um estimador [^6.4]. Este capítulo se concentrará no viés de um estimador, explorando sua definição, implicações e o *trade-off* entre viés e variância.

### Conceitos Fundamentais

O **viés de um estimador** $\hat{\theta}$ é definido como a diferença entre o valor esperado do estimador e o valor verdadeiro do parâmetro $\theta^*$:

$$\
\text{bias}(\hat{\theta}(D)) = E_{p(D|\theta^*)}[\hat{\theta}(D) - \theta^*]
$$

onde:
*   $D$ representa os dados observados.
*   $\theta^*$ é o valor verdadeiro do parâmetro que se deseja estimar.
*   $E_{p(D|\theta^*)}$ denota o valor esperado sobre todas as possíveis amostras $D$ geradas a partir da distribuição verdadeira $p(D|\theta^*)$ [^6.32].

Um **estimador não viesado** é aquele cujo viés é zero, ou seja, sua distribuição amostral está centrada no valor verdadeiro do parâmetro [^6.4]. Formalmente:

$$\
\text{bias}(\hat{\theta}(D)) = E_{p(D|\theta^*)}[\hat{\theta}(D) - \theta^*] = 0
$$

Embora a não-tendenciosidade seja uma propriedade desejável, nem sempre é o critério mais importante na escolha de um estimador. Estimadores viesados podem, em algumas situações, apresentar um risco geral menor [^6.4].

**Exemplo:** O estimador de máxima verossimilhança (MLE) para a variância de uma distribuição Gaussiana é viesado [^6.4]. Dado um conjunto de dados $D = \\{x_1, ..., x_N\\}$ amostrados de uma distribuição Gaussiana com média $\mu$ e variância $\sigma^2$, o MLE para $\sigma^2$ é:

$$\
\hat{\sigma}^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})^2
$$

onde $\bar{x}$ é a média amostral. No entanto, o valor esperado deste estimador não é igual a $\sigma^2$:

$$\
E[\hat{\sigma}^2] = \frac{N-1}{N} \sigma^2
$$

Isso significa que o MLE para a variância gaussiana *subestima* o valor verdadeiro de $\sigma^2$. Contudo, o estimador não viesado

$$\
\hat{\sigma}^2_{unbiased} = \frac{1}{N-1} \sum_{i=1}^{N} (x_i - \bar{x})^2
$$

pode ter uma variância maior, resultando em um erro quadrático médio (MSE) maior em certas circunstâncias.

### Trade-off Viés-Variância

O **erro quadrático médio (MSE)** é uma medida comum para avaliar a qualidade de um estimador, e pode ser decomposto em duas componentes: o viés ao quadrado e a variância [^6.4]:

$$\
MSE(\hat{\theta}) = E[(\hat{\theta} - \theta^*)^2] = \text{var}(\hat{\theta}) + \text{bias}(\hat{\theta})^2
$$

Esta equação demonstra o **trade-off viés-variância**: um estimador com viés pequeno pode ter uma variância grande, e vice-versa [^6.4]. Em muitas situações práticas, é preferível aceitar um pequeno viés em troca de uma redução significativa na variância, resultando em um MSE menor.

**Exemplo:** A **regressão de *ridge*** é uma técnica que introduz um viés deliberado em um modelo de regressão linear para reduzir a variância e melhorar a performance preditiva [^6.4]. Ao adicionar um termo de penalidade à função de custo, a regressão de *ridge* encolhe os coeficientes do modelo em direção a zero, o que pode aumentar o viés, mas também diminuir a variância, especialmente quando há multicolinearidade entre as variáveis preditoras [^6.4].

### Conclusão

O viés é uma propriedade importante a ser considerada na escolha de um estimador, mas não deve ser o único critério. O *trade-off* viés-variância demonstra que, em muitas situações, um estimador viesado com baixa variância pode superar um estimador não viesado com alta variância. A escolha do estimador ideal depende do contexto específico do problema e dos objetivos da análise. Técnicas como regularização (e.g., regressão de *ridge*) exploram este *trade-off* para obter um melhor desempenho preditivo.

### Referências

[^6.4]: Seção 6.4 do texto original.
[^6.32]: Seção 6.4.2 do texto original.

<!-- END -->