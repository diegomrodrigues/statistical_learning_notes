## Cross-Validation para Estimação de Risco de Estimadores

### Introdução
Em estatística frequentista, a avaliação do desempenho de um estimador é crucial. Uma das técnicas para estimar o risco de um estimador é a **cross-validation (CV)** [^61], que utiliza um *validation set*. Quando não dispomos de um *validation set* separado, podemos recorrer à **k-fold cross-validation** [^653]. Este capítulo explorará em detalhes a técnica de *cross-validation*, seus fundamentos e aplicações na estimação do risco de estimadores.

### Conceitos Fundamentais
A **cross-validation** é uma técnica utilizada para estimar o quão bem um modelo estatístico generaliza para um *dataset* independente [^61]. É particularmente útil quando a quantidade de dados disponíveis é limitada, impedindo a separação de um conjunto de dados de treinamento grande o suficiente e um *validation set* representativo [^653].

**K-Fold Cross-Validation**
Na **k-fold cross-validation**, o *dataset* original é particionado em *k* subconjuntos (folds) mutuamente exclusivos de aproximadamente o mesmo tamanho. Um dos *k* subconjuntos é usado como *validation set* para testar o modelo, enquanto os *k-1* subconjuntos restantes são usados como dados de treinamento. O processo é repetido *k* vezes, com cada um dos *k* subconjuntos sendo usado exatamente uma vez como o *validation set*. Os resultados das *k* iterações são então combinados (por exemplo, calculando a média) para produzir uma única estimativa do desempenho do modelo [^653].

Formalmente, seja $D$ o *dataset* completo com $N$ casos de dados, e seja $D_k$ o $k$-ésimo fold, onde $k = 1, \dots, K$. O risco estimado usando a *k-fold cross-validation* é dado por [^659]:

$$
R(m, D, K) = \frac{1}{N} \sum_{k=1}^{K} \sum_{i \in D_k} L(y_i, \mathcal{P}(x_i, \mathcal{F}(D_{-k}, m)))
$$

Onde:
*   $R(m, D, K)$ é o risco estimado do modelo $m$ utilizando *k-fold cross-validation*.
*   $N$ é o número total de casos de dados no *dataset* $D$.
*   $K$ é o número de folds.
*   $D_k$ é o conjunto de dados no *k*-ésimo fold.
*   $D_{-k}$ é o conjunto de dados em todos os folds exceto o *k*-ésimo fold.
*   $L(y_i, \hat{y}_i)$ é a função de perda, que mede a diferença entre o valor real $y_i$ e a predição $\hat{y}_i$.
*   $\mathcal{F}(D_{-k}, m)$ é a função de aprendizado que treina o modelo $m$ utilizando os dados $D_{-k}$, retornando um vetor de parâmetros $\theta_m$.
*   $\mathcal{P}(x_i, \theta_m)$ é a função de predição que utiliza o modelo treinado $\theta_m$ para predizer o valor $\hat{y}_i$ para a entrada $x_i$.

**Leave-One-Out Cross-Validation (LOOCV)**
Um caso especial de *k-fold cross-validation* é o **leave-one-out cross-validation (LOOCV)**, onde $k = N$, ou seja, cada fold contém apenas um único caso de dado [^660]. Neste caso, o risco estimado é dado por:

$$
R(m, D, N) = \frac{1}{N} \sum_{i=1}^{N} L(y_i, f_{m}^{-i}(x_i))
$$

Onde $f_{m}^{-i}(x)$ é o modelo treinado em todos os dados exceto o $i$-ésimo ponto.

**Generalized Cross-Validation (GCV)**
Para alguns modelos e funções de perda, é possível calcular analiticamente o efeito de remover o $i$-ésimo caso de treinamento sem realmente re-treinar o modelo $N$ vezes. Isso é conhecido como **generalized cross-validation (GCV)** [^661].

**Aplicações da Cross-Validation**
A *cross-validation* é amplamente utilizada para [^653]:

1.  **Avaliação de Modelos:** Estimar o desempenho de diferentes modelos e selecionar o melhor.
2.  **Seleção de Hiperparâmetros:** Ajustar os hiperparâmetros de um modelo para otimizar seu desempenho.
3.  **Estimativa de Erro:** Obter uma estimativa confiável do erro de generalização de um modelo.

**Exemplo: Escolhendo λ para Ridge Regression**
Um exemplo prático do uso da *cross-validation* é na escolha do parâmetro de regularização $\lambda$ na *ridge regression* [^653]. O objetivo é encontrar o valor de $\lambda$ que minimize o risco estimado por *k-fold cross-validation*:

$$
\hat{\lambda} = \underset{\lambda \in [\lambda_{min}, \lambda_{max}]}{\text{argmin}} \ R(\lambda, D_{train}, K)
$$

Onde $R(\lambda, D_{train}, K)$ é o risco estimado usando a *k-fold CV* para um dado valor de $\lambda$.

**One-Standard-Error Rule**
Ao selecionar um modelo usando *cross-validation*, é comum utilizar a **one-standard-error rule** [^665]. Esta regra sugere escolher o modelo mais simples cujo risco estimado não seja mais do que um desvio padrão acima do risco mínimo estimado. Isso ajuda a evitar o *overfitting* e a selecionar um modelo mais generalizável [^665].

### Conclusão
A *cross-validation* é uma ferramenta essencial na estatística frequentista para estimar o risco de estimadores e selecionar modelos [^61]. Ao particionar os dados em múltiplos folds e iterativamente treinar e validar o modelo, a *cross-validation* fornece uma estimativa robusta do desempenho do modelo em dados não vistos [^653]. Técnicas como *k-fold cross-validation* e *LOOCV* permitem uma avaliação abrangente do modelo, enquanto a *one-standard-error rule* ajuda a selecionar modelos mais simples e generalizáveis [^665].

### Referências
[^61]: Frequentist statistics
[^653]: Estimating the risk using cross validation
[^659]: The K-fold CV estimate of the risk of $f_m$ is defined by
[^660]: where k(i) is the fold in which i is used as test data.
[^661]: known as generalized cross validation or GCV.
[^665]: The one standard error rule

<!-- END -->