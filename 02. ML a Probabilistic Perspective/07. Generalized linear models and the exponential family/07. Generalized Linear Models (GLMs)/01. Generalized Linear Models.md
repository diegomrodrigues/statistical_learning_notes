## Modelos Lineares Generalizados (GLMs) e a Família Exponencial

### Introdução
Este capítulo explora os Modelos Lineares Generalizados (GLMs) no contexto da família exponencial, expandindo a discussão sobre distribuições de probabilidade encontradas anteriormente, como a Gaussiana, Bernoulli, Student t, uniforme e gama [^1]. Os GLMs generalizam a regressão linear, permitindo modelar variáveis de resposta que não seguem uma distribuição normal [^1]. A família exponencial desempenha um papel central nos GLMs, fornecendo uma base para a construção de modelos generativos e discriminativos [^1].

### Conceitos Fundamentais

Um **Modelo Linear Generalizado (GLM)** é uma extensão dos modelos lineares que permite que a média da variável de resposta esteja relacionada a um preditor linear através de uma função de ligação. Isso possibilita a modelagem de variáveis de resposta não-normais. Formalmente, um GLM consiste em três componentes:

1.  **Componente Aleatório:** Uma distribuição de probabilidade da família exponencial.
2.  **Componente Sistemático:** Um preditor linear $\eta_i = w^T x_i$, onde $w$ é o vetor de pesos e $x_i$ é o vetor de variáveis preditoras.
3.  **Função de Ligação:** Uma função $g$ que relaciona os componentes aleatórios e sistemáticos, definida como $g(\mu_i) = \eta_i$, onde $\mu_i$ é a média da variável de resposta.

A **família exponencial** desempenha um papel crucial na construção de GLMs. Uma distribuição $p(x|\theta)$ pertence à família exponencial se puder ser escrita na forma [^2]:

$$np(x|\theta) = \frac{1}{Z(\theta)} h(x) \exp[\theta^T \phi(x)] = h(x) \exp[\theta^T \phi(x) - A(\theta)]$$

onde:

*   $\theta$ são os **parâmetros naturais** ou **canônicos**.
*   $\phi(x) \in \mathbb{R}^d$ é um vetor de **estatísticas suficientes**.
*   $Z(\theta)$ é a **função de partição**.
*   $A(\theta) = \log Z(\theta)$ é a **função log-partição** ou **função cumulante**.
*   $h(x)$ é uma função de escala [^2].

Se $\phi(x) = x$, dizemos que é uma **família exponencial natural** [^2]. A equação pode ser generalizada para:

$$np(x|\theta) = h(x) \exp[\eta(\theta)^T \phi(x) - A(\eta(\theta))]$$

onde $\eta$ é uma função que mapeia os parâmetros $\theta$ para os parâmetros canônicos $\eta = \eta(\theta)$ [^2]. Se $\text{dim}(\theta) < \text{dim}(\eta(\theta))$, é chamada de **família exponencial curva** [^2]. Se $\eta(\theta) = \theta$, o modelo está na **forma canônica** [^2].

**Exemplos de Distribuições da Família Exponencial:**

*   **Bernoulli:** A distribuição de Bernoulli para $x \in \{0, 1\}$ pode ser escrita na forma da família exponencial como [^2]:

$$text{Ber}(x|\mu) = (1-\mu) \exp\left[ x \log \left(\frac{\mu}{1-\mu}\right) \right]$$

Neste caso, $\phi(x) = x$ e $\theta = \log(\frac{\mu}{1-\mu})$, que é o *log-odds ratio* [^3].
*   **Multinoulli:** A distribuição Multinoulli pode ser representada como uma família exponencial mínima [^3]:

$$text{Cat}(x|\mu) = \left(1 - \sum_{k=1}^{K-1} \mu_k \right) \exp \left[ \sum_{k=1}^{K-1} x_k \log \left( \frac{\mu_k}{1 - \sum_{j=1}^{K-1} \mu_j} \right) \right]$$
*   **Gaussiana Univariada:** A distribuição Gaussiana univariada pode ser escrita como [^4]:

$$N(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[ -\frac{1}{2\sigma^2}(x-\mu)^2 \right]$$

**Função de Ligação Canônica:**

Uma forma particularmente simples de função de ligação é usar $g = \psi$, onde $\psi$ é uma função. Isso é chamado de **função de ligação canônica**. Neste caso, $\theta_i = \eta_i = w^T x_i$ [^11].

**Estimação de Máxima Verossimilhança (MLE) para a Família Exponencial:**

A verossimilhança de um modelo da família exponencial tem a forma [^6]:

$$np(\mathcal{D}|\theta) = \left( \prod_{i=1}^N h(x_i) \right) Z(\theta)^{-N} \exp \left[ \eta(\theta)^T \sum_{i=1}^N \phi(x_i) \right]$$

As estatísticas suficientes são $N$ e $\sum_{i=1}^N \phi(x_i)$ [^6]. Sob certas condições de regularidade, a família exponencial é a única família de distribuições com estatísticas suficientes finitas (**Teorema de Pitman-Koopman-Darmois**) [^6].

Para encontrar o MLE para um modelo canônico da família exponencial, maximizamos a log-verossimilhança [^6]:

$$log p(\mathcal{D}|\theta) = \theta^T \phi(\mathcal{D}) - N A(\theta)$$

Como $-A(\theta)$ é côncava em $\theta$ e $\theta^T \phi(\mathcal{D})$ é linear em $\theta$, a log-verossimilhança é côncava e, portanto, possui um máximo global único [^6]. Derivando em relação a $\theta$ e igualando a zero:

$$nabla_\theta \log p(\mathcal{D}|\theta) = \phi(\mathcal{D}) - N \mathbb{E}[\phi(X)] = 0$$

No MLE, a média empírica das estatísticas suficientes deve ser igual às estatísticas suficientes esperadas teoricamente pelo modelo [^6]:

$$mathbb{E}[\phi(X)] = \frac{1}{N} \sum_{i=1}^N \phi(x_i)$$

Este processo é chamado de *moment matching* [^7].

### Conclusão
Os Modelos Lineares Generalizados fornecem uma estrutura flexível para modelar uma ampla gama de tipos de dados, estendendo a regressão linear tradicional. A família exponencial fornece a base teórica para esses modelos, permitindo que sejam construídos de forma eficiente e interpretados com relativa facilidade. O MLE fornece um método para estimar os parâmetros do modelo, e a função de ligação permite que a média da variável de resposta seja relacionada a um preditor linear. Modelos como Bernoulli, Multinoulli e Gaussiana se encaixam na estrutura da família exponencial.

### Referências
[^1]: Page 281, Section 9.1
[^2]: Page 282, Section 9.2.1
[^3]: Page 283, Section 9.2.2.2
[^4]: Page 284, Section 9.2.2.3
[^5]: Page 285, Section 9.2
[^6]: Page 286, Section 9.2.4
[^7]: Page 287, Section 9.2.5
[^11]: Page 291, Section 9.3
<!-- END -->