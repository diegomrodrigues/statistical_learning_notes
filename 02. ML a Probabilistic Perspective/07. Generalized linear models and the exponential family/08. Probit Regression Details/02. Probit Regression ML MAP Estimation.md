## ML/MAP Estimation Using Gradient-Based Optimization in Probit Regression

### Introdução
Este capítulo aprofunda a estimação de Maximum Likelihood (ML) e Maximum a Posteriori (MAP) em regressão probit, utilizando métodos de otimização baseados em gradiente. A regressão probit, como alternativa à regressão logística, modela a probabilidade de uma variável binária usando a função de distribuição cumulativa normal padrão (CDF). A estimação de parâmetros em regressão probit envolve a maximização da função de log-verossimilhança, um processo que frequentemente requer técnicas iterativas de otimização.

### Conceitos Fundamentais
A regressão probit modela a probabilidade de uma variável binária $y_i$ como:
$$p(y_i = 1|x_i, w) = \Phi(w^T x_i)$$
onde $\Phi$ é a função de distribuição cumulativa (CDF) da normal padrão, $x_i$ é o vetor de características e $w$ é o vetor de pesos. O objetivo é encontrar o vetor de pesos $w$ que melhor se ajusta aos dados observados.

#### Log-Verossimilhança para Regressão Probit
Para um conjunto de dados com $N$ observações, a função de log-verossimilhança é dada por:
$$l(w) = \sum_{i=1}^{N} \log p(y_i|x_i, w) = \sum_{i=1}^{N} \log \Phi(\tilde{y}_i w^T x_i)$$
onde $\tilde{y}_i = 2y_i - 1 \in \{-1, 1\}$ é uma transformação de $y_i$ para simplificar a notação.

#### Gradiente da Log-Verossimilhança
Para otimizar a função de log-verossimilhança, é necessário calcular o gradiente em relação aos parâmetros $w$. O gradiente para uma observação específica $i$ é dado por [^9]:
$$g_i = \frac{\partial}{\partial w} \log p(y_i|x_i, w) = \frac{\tilde{y}_i \phi(\mu_i)}{\Phi(\tilde{y}_i \mu_i)} x_i$$
onde $\mu_i = w^T x_i$ e $\phi$ é a função de densidade de probabilidade (PDF) da normal padrão. O gradiente total é a soma dos gradientes individuais:
$$g = \sum_{i=1}^{N} g_i$$
#### Hessiana da Log-Verossimilhança
Além do gradiente, o método de Newton e outros métodos de otimização de segunda ordem requerem o cálculo da Hessiana. A Hessiana para uma observação específica $i$ é dada por [^9]:
$$H_i = -\left(\frac{\tilde{y}_i \mu_i \phi(\mu_i)}{\Phi(\tilde{y}_i \mu_i)} + \frac{\phi(\mu_i)^2}{\Phi(\tilde{y}_i \mu_i)^2}\right) x_i x_i^T$$
A Hessiana total é a soma das Hessianas individuais:
$$H = \sum_{i=1}^{N} H_i$$
#### Implementação de Métodos de Otimização
Com o gradiente e a Hessiana, podemos implementar vários métodos de otimização:
1.  **Gradiente Descendente:**
    $$w_{t+1} = w_t - \eta g_t$$
    onde $\eta$ é a taxa de aprendizado.
2.  **Método de Newton:**
    $$w_{t+1} = w_t - H_t^{-1} g_t$$
    Este método geralmente converge mais rapidamente, mas requer o cálculo da inversa da Hessiana, que pode ser computacionalmente caro.
#### Regularização MAP
Para a estimação MAP, incorporamos um prior sobre os parâmetros $w$. Se usarmos um prior Gaussiano $p(w) = N(0, V_0)$, a função de log-verossimilhança penalizada é:
$$l_{MAP}(w) = l(w) - \frac{1}{2} w^T V_0^{-1} w$$
O gradiente e a Hessiana da função de log-verossimilhança penalizada são modificados da seguinte forma [^9]:
$$g_{MAP} = g - V_0^{-1} w$$
$$H_{MAP} = H - V_0^{-1}$$

### Conclusão
A estimação ML/MAP em regressão probit pode ser efetuada utilizando métodos de otimização baseados em gradiente. O gradiente e a Hessiana da função de log-verossimilhança (penalizada) são essenciais para implementar esses métodos. A escolha do método de otimização (gradiente descendente, Newton, etc.) depende da complexidade computacional e da taxa de convergência desejada. A regressão probit oferece uma alternativa valiosa à regressão logística, especialmente em cenários onde a interpretação da CDF normal padrão é preferível. $\blacksquare$

### Referências
[^9]: Capítulo 9 do texto fornecido
<!-- END -->