## Densidade Preditiva Posterior para a Família Exponencial

### Introdução
Este capítulo explora a derivação da densidade preditiva posterior para observáveis futuros, dado um conjunto de dados passado, no contexto da família exponencial. Partindo do conceito de **família exponencial** [^1], que engloba distribuições como a Gaussiana, Bernoulli e Gamma, e da sua propriedade de possuir estatísticas suficientes de tamanho finito [^1], facilitando a análise Bayesiana, vamos focar na obtenção de previsões sobre dados futuros. A utilização de priors conjugados, que simplificam a computação da posterior [^1], é crucial para essa derivação.

### Conceitos Fundamentais

A **densidade preditiva posterior** para observáveis futuros $D' = (x'_1, ..., x'_{N'})$ dado os dados passados $D = (x_1, ..., x_N)$ pode ser derivada utilizando uma expressão genérica [^8]. Para simplificar a notação, as estatísticas suficientes são combinadas com o tamanho dos dados: $\tau_0 = (\nu_0, \tau_0)$, $s(D) = (N, s(D))$, e $s(D') = (N', s(D'))$ [^8]. A verossimilhança (likelihood) e a posterior têm uma forma similar, permitindo a simplificação da expressão [^8].

A **prior** se torna [^8]:
$$
p(\theta|\tau_0) = \frac{1}{Z(\tau_0)} g(\theta)^{\nu_0} \exp(\eta(\theta)^T \tau_0) \qquad (9.56)
$$

A verossimilhança e a posterior têm uma forma similar [^8]. Portanto,

$$
p(D'|D) = \int p(D'|\theta) p(\theta|D) d\theta \qquad (9.57)
$$

Expandindo a integral, obtemos [^8]:
$$
p(D'|D) = \int \left[ \prod_{i=1}^{N'} h(x'_i) \right] Z(\tau_0 + s(D))^{-1} g(\theta)^{\nu_0 + N + N'} d\theta \times \exp \left[ \sum_k \eta_k(\theta) \left( \tau_k + \sum_{i=1}^{N} s_k(x_i) + \sum_{i=1}^{N'} s_k(x'_i) \right) \right] d\theta \qquad (9.58, 9.59)
$$

Simplificando, obtemos [^8]:
$$
p(D'|D) = \left[ \prod_{i=1}^{N'} h(x'_i) \right] \frac{Z(\tau_0 + s(D) + s(D'))}{Z(\tau_0 + s(D))} \qquad (9.60)
$$

Se $N = 0$, isto se torna a verossimilhança marginal de $D'$ [^8], que se reduz à forma familiar do normalizador da posterior dividido pelo normalizador da prior, multiplicado por uma constante [^8].

**Exemplo: Distribuição de Bernoulli**

Como um exemplo simples, vamos revisitar o modelo Beta-Bernoulli em nossa nova notação [^8]. A verossimilhança é dada por [^8]:
$$
p(D|\theta) = (1-\theta)^N \exp \left( \log \left( \frac{\theta}{1-\theta} \right) \sum_i x_i \right) \qquad (9.61)
$$

Portanto, o prior conjugado é dado por [^8]:
$$
p(\theta|\nu_0, \tau_0) \propto (1-\theta)^{\nu_0} \exp \left( \log \left( \frac{\theta}{1-\theta} \right) \tau_0 \right) = \theta^{\tau_0} (1-\theta)^{\nu_0 - \tau_0} \qquad (9.62, 9.63)
$$
Se definirmos $\alpha = \tau_0 + 1$ e $\beta = \nu_0 - \tau_0 + 1$, vemos que esta é uma distribuição Beta [^8]. Podemos derivar a posterior como se segue, onde $s = \sum_i I(x_i = 1)$ é a estatística suficiente [^8]:
$$
p(\theta|D) \propto \theta^{\tau_0 + s} (1-\theta)^{\nu_0 - \tau_0 + N - s} = \theta^{\nu_n} (1-\theta)^{\nu_n - \tau_n} \qquad (9.64, 9.65)
$$
Podemos derivar a distribuição preditiva posterior como se segue [^8]. Assumimos que $p(\theta) = Beta(\theta|\alpha, \beta)$ [^8], e seja $s = s(D)$ o número de "caras" nos dados passados [^8]. Podemos prever a probabilidade de uma dada sequência de "caras" futuras, $D' = (x'_1, ..., x'_{N'})$ [^9], com estatística suficiente $s' = \sum_{i=1}^{N'} I(x'_i = 1)$ [^9], como se segue [^9]:
$$
p(D'|D) = \int p(D'|\theta) Beta(\theta|\alpha_n, \beta_n) d\theta \qquad (9.66)
$$
$$
= \frac{\Gamma(\alpha_n) \Gamma(\beta_n)}{\Gamma(\alpha_n + \beta_n)} \int \theta^{\alpha_n + s' - 1} (1-\theta)^{\beta_n + N' - s' - 1} d\theta \qquad (9.67)
$$
$$
= \frac{\Gamma(\alpha_n) \Gamma(\beta_n)}{\Gamma(\alpha_n + \beta_n)} \frac{\Gamma(\alpha_n + s') \Gamma(\beta_n + N' - s')}{\Gamma(\alpha_n + s' + \beta_n + N' - s')} \qquad (9.68)
$$
Onde [^9]:
$$
\alpha_{n+m} = \alpha_n + s' = \alpha + s + s' \qquad (9.69)
$$
$$
\beta_{n+m} = \beta_n + (N' - s') = \beta + (N - s) + (N' - s') \qquad (9.70)
$$

### Conclusão
A derivação da densidade preditiva posterior é fundamental para a inferência Bayesiana, permitindo fazer previsões sobre observáveis futuros com base nos dados passados e no conhecimento prévio codificado na prior. O exemplo da distribuição de Bernoulli ilustra como essa derivação pode ser realizada de forma analítica, aproveitando a propriedade de conjugação da prior Beta. Este processo é essencial para a tomada de decisões informadas em diversas aplicações, desde a modelagem de eventos discretos até a previsão de séries temporais.

### Referências
[^1]: Capítulo 9. Generalized linear models and the exponential family, páginas 1-7.
[^8]: Capítulo 9. Generalized linear models and the exponential family, página 288.
[^9]: Capítulo 9. Generalized linear models and the exponential family, página 289.
<!-- END -->