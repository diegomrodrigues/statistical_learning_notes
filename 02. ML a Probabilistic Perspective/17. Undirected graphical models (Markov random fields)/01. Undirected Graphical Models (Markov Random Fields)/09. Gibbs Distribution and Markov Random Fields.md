## A Distribuição de Gibbs em Modelos Gráficos Não Direcionados

### Introdução
Este capítulo explora a **distribuição de Gibbs**, um modelo intimamente relacionado com os Modelos Gráficos Não Direcionados (UGMs), também conhecidos como Campos Aleatórios de Markov (MRFs) [^1]. A distribuição de Gibbs fornece uma maneira poderosa e flexível de definir distribuições de probabilidade conjuntas sobre um conjunto de variáveis, com aplicações em diversas áreas, como física estatística, bioquímica e aprendizado de máquina [^6]. Compreender a distribuição de Gibbs é fundamental para aprofundar o conhecimento sobre a representação e parametrização de MRFs.

### Conceitos Fundamentais
A distribuição de Gibbs é definida como [^6]:
$$ p(y|\theta) = \frac{1}{Z(\theta)} \exp\left(-\sum_{c} E_c(y_c|\theta_c)\right) $$
onde:
*   $y$ representa uma configuração do vetor de variáveis aleatórias.
*   $\theta$ representa os parâmetros do modelo.
*   $c$ indexa os cliques no grafo não direcionado.
*   $E_c(y_c|\theta_c) > 0$ é a **energia** associada às variáveis no clique $c$ [^6]. Estados de alta probabilidade correspondem a configurações de baixa energia.
*   $Z(\theta)$ é a **função de partição**, que garante que a distribuição de probabilidade seja normalizada [^6]. Ela é definida como:
    $$     Z(\theta) = \sum_{y} \exp\left(-\sum_{c} E_c(y_c|\theta_c)\right)     $$
    A função de partição é crucial para garantir que a distribuição some 1, mas seu cálculo é geralmente intratável para grafos grandes.

**Funções de Potencial e Modelos de Máxima Entropia:**

As funções de potencial podem ser representadas como tabelas de números não negativos ou, mais geralmente, como uma função log-linear dos parâmetros [^6]:
$$ \log \psi_c(y_c) = \phi_c(y_c)^T \theta_c $$
onde:
*   $\psi_c(y_c)$ é a função de potencial para o clique $c$.
*   $\phi_c(y_c)$ é um vetor de características derivado dos valores das variáveis $y_c$.
*   $\theta_c$ é o vetor de parâmetros associado ao clique $c$.

Essa representação log-linear também é conhecida como um **modelo de máxima entropia** [^6]. Modelos de máxima entropia são uma forma geral de representar distribuições de probabilidade, sujeitas a certas restrições (como as características $\phi_c(y_c)$).

**Energia e Probabilidade:**

A relação entre energia e probabilidade na distribuição de Gibbs é inversa. Configurações de baixa energia têm alta probabilidade, e vice-versa [^6]. Isso permite que o modelo capture dependências complexas entre as variáveis, favorecendo configurações que minimizam a energia total do sistema.

**Teorema de Hammersley-Clifford:**

O **Teorema de Hammersley-Clifford** [^6] estabelece uma conexão fundamental entre as propriedades de independência condicional de um grafo não direcionado e a forma da distribuição de probabilidade conjunta. O teorema afirma que uma distribuição positiva $p(y) > 0$ satisfaz as propriedades de independência condicional de um grafo não direcionado $G$ se e somente se $p$ pode ser representada como um produto de fatores, um por clique maximal, ou seja,
$$ p(y|\theta) = \frac{1}{Z(\theta)} \prod_{c \in C} \psi_c(y_c|\theta_c) $$
onde $C$ é o conjunto de todos os cliques (maximals) de $G$.

**Restrições de Parametrização:**

É importante notar que podemos restringir a parametrização aos vértices do grafo, em vez dos cliques maximais [^6]. Isso é chamado de **MRF pairwise**.  Em um MRF pairwise, a distribuição conjunta é proporcional ao produto de potenciais definidos nos pares de nós adjacentes no grafo [^6]:
$$ p(y|\theta) \propto \prod_{(s,t) \in E} \psi_{st}(y_s, y_t) $$
onde $E$ representa o conjunto de arestas no grafo. Embora mais simples, essa forma pode ser menos geral do que usar cliques maximais [^6].

### Conclusão
A distribuição de Gibbs oferece uma estrutura flexível e poderosa para modelar distribuições de probabilidade conjuntas em UGMs [^6]. Através da definição de funções de energia ou potenciais sobre os cliques do grafo, podemos capturar dependências complexas entre as variáveis. O Teorema de Hammersley-Clifford fornece uma base teórica para essa representação, garantindo a consistência entre as propriedades de independência condicional do grafo e a forma da distribuição [^6]. A escolha da parametrização (cliques maximais vs. arestas) e a representação das funções de potencial (tabelas vs. modelos log-lineares) dependem das características específicas do problema em questão. <!-- END -->

### Referências
[^1]: Koller, D., & Friedman, N. (2009). *Probabilistic graphical models: principles and techniques*. MIT press.
[^6]: Chapter 19. Undirected graphical models (Markov random fields)
<!-- END -->