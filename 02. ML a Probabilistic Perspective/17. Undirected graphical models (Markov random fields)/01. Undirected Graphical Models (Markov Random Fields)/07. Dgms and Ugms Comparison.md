## Mapas Perfeitos e Comparação entre DGMs e UGMs

### Introdução
Este capítulo explora a capacidade de **Directed Graphical Models (DGMs)** e **Undirected Graphical Models (UGMs)** em representar diferentes conjuntos de distribuições [^1]. DGMs, como as redes Bayesianas, e UGMs, também conhecidos como campos aleatórios de Markov (MRFs) ou redes de Markov [^1], oferecem abordagens distintas para modelagem probabilística. Exploraremos como a escolha entre esses modelos impacta a representação de dependências condicionais e a expressividade do modelo.

### Conceitos Fundamentais
Tanto DGMs quanto UGMs são capazes de representar diferentes conjuntos de distribuições perfeitamente [^1]. DGMs podem modelar relações de causa e efeito mais naturalmente, enquanto UGMs são mais adequados para representar dependências simétricas [^1].

**Relações de Causa e Efeito (DGMs)**
Os DGMs são inerentemente direcionados, o que os torna adequados para representar relações de causa e efeito [^1]. Em um DGM, as arestas direcionadas indicam dependências condicionais, onde o nó pai influencia o nó filho. Essa direcionalidade torna os DGMs intuitivos para modelar sistemas onde as relações causais são conhecidas ou assumidas.

**Dependências Simétricas (UGMs)**
Os UGMs, por outro lado, são não direcionados, o que os torna mais adequados para representar dependências simétricas [^1]. Em um UGM, a ausência de direcionalidade permite que o modelo capture dependências mútuas entre variáveis sem impor uma relação de causa e efeito. Essa característica é particularmente útil em domínios onde as dependências são simétricas por natureza, como a correlação entre pixels vizinhos em uma imagem [^1].

**Mapas Perfeitos**
Um grafo $G$ é um **mapa perfeito** de uma distribuição $p$ se $I(G) = I(p)$, significando que o grafo pode representar todas e somente as propriedades de independência condicional (CI) da distribuição [^1]. Essa definição formaliza a ideia de que um mapa perfeito captura precisamente as dependências condicionais presentes na distribuição.

**Independência Condicional**
UGMs definem relações de CI através de separação no grafo. Para conjuntos de nós $A$, $B$ e $C$, dizemos que $X_A \perp X_B | X_C$ se $C$ separa $A$ de $B$ no grafo $G$ [^1]. Isso significa que, ao remover todos os nós em $C$, não há caminhos conectando qualquer nó em $A$ a qualquer nó em $B$ [^1]. Essa propriedade é chamada de **propriedade de Markov global** para UGMs [^1].

**Propriedades de Markov Monotônicas e Não Monotônicas**
As propriedades de independência condicional em UGMs são **monotônicas**, enquanto em DGMs elas podem ser **não monotônicas** [^1]. A monotonicidade em UGMs significa que se $A \perp B | C$, então $A \perp B | (C \cup D)$ [^1]. Em outras palavras, adicionar mais variáveis ao conjunto de condicionamento não destrói a independência condicional. Em contraste, DGMs podem exibir não-monotonicidade devido a fenômenos como "explaining away", onde condicionar em uma variável adicional pode criar dependência entre variáveis que eram previamente independentes.

**Modelos Decomponíveis ou Chordais**
Distribuições que podem ser perfeitamente modeladas por um DGM ou um UGM são chamadas de **decomponíveis** ou **chordais**, implicando que se colapsarmos todas as variáveis em cada clique maximal, para fazer "mega-variáveis", o grafo resultante será uma árvore [^1].

### Conclusão
DGMs e UGMs são mapas perfeitos para diferentes conjuntos de distribuições, tornando nenhum mais poderoso que o outro como uma linguagem de representação [^1]. A escolha entre DGMs e UGMs depende das propriedades específicas da distribuição que se deseja modelar, com DGMs favorecidos para relações causais e UGMs para dependências simétricas. Entender as nuances de cada modelo é crucial para uma modelagem probabilística eficaz. $\blacksquare$
<!-- END -->