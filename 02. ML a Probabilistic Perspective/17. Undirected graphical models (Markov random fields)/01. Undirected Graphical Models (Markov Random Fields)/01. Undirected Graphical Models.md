## Undirected Graphical Models (Markov Random Fields)

### Introdução
Este capítulo dedica-se ao estudo dos **Undirected Graphical Models (UGMs)**, também conhecidos como **Markov Random Fields (MRFs)** ou **Markov Networks**. UGMs representam uma alternativa aos Directed Graphical Models (DGMs) quando a orientação das arestas não é naturalmente definida [^1]. Eles são particularmente adequados para problemas onde as relações entre variáveis são simétricas, como em análise de imagens e estatística espacial [^1]. O capítulo explora as propriedades de independência condicional, a parametrização, e exemplos de MRFs, culminando em uma discussão sobre Conditional Random Fields (CRFs) e Structural SVMs.

### Conceitos Fundamentais

#### Propriedades de Independência Condicional
UGMs definem relações de **independência condicional (CI)** através da separação no grafo [^1]. Dados conjuntos de nós A, B, e C, dizemos que $X_A \perp X_B | X_C$ se C separa A de B no grafo G [^1]. Isso significa que, ao remover todos os nós em C, não existem caminhos conectando qualquer nó em A a qualquer nó em B [^1]. Essa propriedade é chamada de **global Markov property** para UGMs [^1].

Por exemplo, na Figura 19.2(b) [^1], temos que $\{1,2\} \perp \{6,7\} | \{3,4,5\}$.

O conjunto de nós que torna um nó *t* condicionalmente independente de todos os outros nós no grafo é chamado de **Markov blanket** de *t*, denotado por mb(*t*) [^1]. Formalmente, o Markov blanket satisfaz a seguinte propriedade:

$$t \perp V \setminus cl(t) | mb(t) \quad (19.1)$$

onde $cl(t) = mb(t) \cup \{t\}$ é o *closure* do nó *t* [^1]. Em um UGM, o Markov blanket de um nó é o conjunto de seus vizinhos imediatos, conhecido como **undirected local Markov property** [^1]. Por exemplo, na Figura 19.2(b) [^1], temos mb(5) = {2, 3, 4, 6}.

Duas variáveis são condicionalmente independentes dado o resto se não houver uma aresta direta entre elas, o que é chamado de **pairwise Markov property** [^1]. Em símbolos:

$$s \perp t | V \setminus \{s, t\} \iff G_{st} = 0 \quad (19.2)$$

É óbvio que a propriedade global de Markov implica a propriedade local de Markov, que por sua vez implica a propriedade pairwise de Markov [^1]. Assumindo que $p(x) > 0$ para todo *x* (densidade positiva), a propriedade pairwise implica a propriedade global [^1].

#### Moralização

A determinação de relações de CI em UGMs é mais simples do que em DGMs, pois não é necessário considerar a direcionalidade das arestas [^1]. Para determinar relações de CI para um DGM usando um UGM, não basta simplesmente remover a orientação das arestas [^1]. Isso porque uma v-estrutura $A \rightarrow B \leftarrow C$ tem propriedades de CI diferentes de uma cadeia não direcionada $A - B - C$ [^1]. Para evitar afirmações de CI incorretas, podemos adicionar arestas entre os pais "não casados" A e C, e então remover as setas das arestas [^1]. Esse processo é chamado de **moralização** [^1]. A Figura 19.2(b) [^1] fornece um exemplo maior de moralização.

#### Modelos Direcionados vs. Não Direcionados

DGMs e UGMs são mapas perfeitos para diferentes conjuntos de distribuições [^1] (ver Figura 19.5 [^1]). Um v-estrutura $A \rightarrow C \leftarrow B$ afirma que $A \perp B$ e $A \not\perp B|C$. Se removermos as setas, obtemos $A - C - B$, que afirma que $A \perp B|C$ e $A \not\perp B$, o que está incorreto [^1].

Em geral, as propriedades de CI em UGMs são **monotônicas**: se $A \perp B|C$, então $A \perp B|(C \cup D)$ [^1]. Em DGMs, as propriedades de CI podem ser não-monotônicas, pois o condicionamento em variáveis extras pode eliminar independências condicionais devido à explicação [^1].

#### Parameterização de MRFs

Como não há ordenação topológica associada a um grafo não direcionado, não podemos usar a regra da cadeia para representar *p(y)* [^5]. Em vez disso, associamos **funções de potencial** ou **fatores** a cada clique maximal no grafo [^5]. Denotamos a função de potencial para o clique *c* por $\psi_c(y_c|\theta_c)$ [^5]. A função de potencial pode ser qualquer função não negativa de seus argumentos [^5]. A distribuição conjunta é então definida como proporcional ao produto dos potenciais de clique [^5]. Surpreendentemente, qualquer distribuição positiva cujas propriedades de CI podem ser representadas por um UGM pode ser representada dessa forma [^5]. Este resultado é conhecido como o **Teorema de Hammersley-Clifford** [^6].

**Teorema 19.3.1 (Hammersley-Clifford)**. Uma distribuição positiva $p(y) > 0$ satisfaz as propriedades de CI de um grafo não direcionado G se e somente se *p* pode ser representada como um produto de fatores, um por clique maximal, i.e.,

$$p(y|\theta) = \frac{1}{Z(\theta)} \prod_{c \in C} \psi_c(y_c|\theta_c) \quad (19.3)$$

onde C é o conjunto de todos os cliques (maximais) de G, e Z(θ) é a **função de partição** dada por

$$Z(\theta) = \sum_x \prod_{c \in C} \psi_c(y_c|\theta_c) \quad (19.4)$$

A função de partição garante que a distribuição some para 1 [^6].

Existe uma profunda conexão entre UGMs e a física estatística. Em particular, existe um modelo conhecido como a **distribuição de Gibbs**, que pode ser escrita como:

$$p(y|\theta) = \frac{1}{Z(\theta)} exp(-\sum_c E(y_c|\theta_c)) \quad (19.7)$$

onde $E(y_c) > 0$ é a energia associada com as variáveis no clique *c* [^6]. Podemos converter isso para um UGM definindo:

$$psi_c(y_c|\theta_c) = exp(-E(y_c|\theta_c)) \quad (19.8)$$

Estados de alta probabilidade correspondem a configurações de baixa energia [^6]. Modelos desta forma são conhecidos como **modelos baseados em energia** [^6].

Podemos restringir a parametrização para as arestas do grafo, em vez dos cliques maximais. Isso é chamado de **pairwise MRF** [^6].

#### Representando Funções de Potencial
Se as variáveis são discretas, podemos representar as funções de potencial como tabelas de números não negativos [^7]. De forma mais geral, podemos definir os log potenciais como uma função linear dos parâmetros:

$$log \psi_c(y_c) \triangleq \phi_c(y_c)^T \theta_c \quad (19.11)$$

onde $\phi_c(y_c)$ é um vetor de características derivado dos valores das variáveis $y_c$ [^7]. A probabilidade logarítmica resultante tem a forma:

$$log p(y|\theta) = \sum_c \phi_c(y_c)^T \theta_c - Z(\theta) \quad (19.12)$$

Isto é também conhecido como um **modelo de máxima entropia** ou **log-linear** [^7].

### Exemplos de MRFs

#### Modelo de Ising
O **modelo de Ising** é um exemplo de um MRF que surgiu da física estatística [^8]. Ele foi originalmente usado para modelar o comportamento de ímãs [^8]. Seja $y_s \in \{-1, +1\}$ representando o spin de um átomo [^8]. Em alguns ímãs, chamados de ferromagnéticos, os spins vizinhos tendem a se alinhar na mesma direção, enquanto em outros tipos de ímãs, chamados de antiferromagnéticos, os spins "querem" ser diferentes de seus vizinhos [^8].

Podemos modelar isso como um MRF da seguinte forma. Criamos um grafo na forma de uma rede 2D ou 3D e conectamos variáveis vizinhas [^8]. Definimos o seguinte potencial de clique pairwise:

$$psi_{st}(y_s, y_t) = \begin{pmatrix} e^{w_{st}} & e^{-w_{st}} \\\\ e^{-w_{st}} & e^{w_{st}} \end{pmatrix} \quad (19.17)$$

onde $w_{st}$ é a força de acoplamento entre os nós *s* e *t* [^8]. Se dois nós não estão conectados no grafo, definimos $w_{st} = 0$ [^8]. Assumimos que a matriz de peso W é simétrica, então $w_{st} = w_{ts}$ [^8]. Frequentemente assumimos que todas as arestas têm a mesma força, então $w_{st} = J$ [^8].

Se todos os pesos são positivos, $J > 0$, então é provável que os spins vizinhos estejam no mesmo estado; isso pode ser usado para modelar ferromagnetos, e é um exemplo de uma **rede Markov associativa** [^8]. Se os pesos são suficientemente fortes, a distribuição de probabilidade correspondente terá dois modos, correspondendo ao estado all +1 e ao estado all -1 [^8]. Estes são chamados de **estados fundamentais** do sistema [^8].

Se todos os pesos são negativos, $J < 0$, então os spins querem ser diferentes de seus vizinhos; isso pode ser usado para modelar um antiferromagneto e resulta em um **sistema frustrado**, no qual nem todas as restrições podem ser satisfeitas ao mesmo tempo [^8]. A distribuição de probabilidade correspondente terá múltiplos modos [^8]. Curiosamente, calcular a função de partição Z(J) pode ser feito em tempo polinomial para redes Markov associativas, mas é NP-difícil em geral (Cipra 2000) [^8].

#### Redes de Hopfield
Uma **rede de Hopfield** (Hopfield 1982) é um modelo de Ising totalmente conectado com uma matriz de peso simétrica, $W = W^T$ [^9]. Esses pesos, mais os termos de bias *b*, podem ser aprendidos a partir de dados de treinamento usando (aproximada) máxima verossimilhança [^9].

A principal aplicação das redes de Hopfield é como uma **memória associativa** ou **memória endereçável por conteúdo** [^9]. A ideia é a seguinte: suponha que treinamos em um conjunto de vetores de bits totalmente observados, correspondendo a padrões que queremos memorizar [^9]. Então, no tempo de teste, apresentamos um padrão parcial à rede [^9]. Gostaríamos de estimar as variáveis faltantes; isso é chamado de **completação de padrão** [^9]. Isso pode ser pensado como recuperar um exemplo da memória com base em uma parte do exemplo em si, daí o termo "memória associativa" [^9].

Como a inferência exata é intratável neste modelo, é padrão usar um algoritmo de descida de coordenadas conhecido como **modos condicionais iterativos (ICM)**, que apenas define cada nó para seu estado mais provável (energia mais baixa), dados todos os seus vizinhos [^9]. O condicional completo pode ser mostrado como:

$$p(y_s = 1|y_{-s}, \theta) = sigm(\sum_t w_{st}y_t + b_s) \quad (19.21)$$

Escolher o estado mais provável equivale a usar a regra $y^*_s = 1$ se $\sum_t w_{st}y_t > b_s$ e usar $y^*_s = 0$ caso contrário [^9].

Como a inferência é determinística, também é possível interpretar este modelo como uma **rede neural recorrente** [^9].

Uma **máquina de Boltzmann** generaliza o modelo de Hopfield/Ising, incluindo alguns nós ocultos, o que torna o modelo representacionalmente mais poderoso [^9]. A inferência em tais modelos frequentemente usa amostragem de Gibbs, que é uma versão estocástica de ICM (ver Seção 24.2 para detalhes) [^9].

#### Modelo de Potts
É fácil generalizar o modelo de Ising para múltiplos estados discretos, $y_t \in \{1, 2, ..., K\}$ [^11]. É comum usar uma função de potencial da seguinte forma:

$$psi_{st}(y_s, y_t) = \begin{pmatrix} e^0 & 0 & 0 \\\\ 0 & e^J & 0 \\\\ 0 & 0 & e^J \end{pmatrix} \quad (19.22)$$

Isto é chamado o **modelo de Potts** [^11]. Se $J > 0$, então os nós vizinhos são encorajados a ter a mesma etiqueta [^11]. O modelo de Potts pode ser usado como um prior para **segmentação de imagem**, pois diz que pixels vizinhos são propensos a ter a mesma etiqueta discreta e, portanto, pertencem ao mesmo segmento [^11]. Podemos combinar este prior com um termo de verossimilhança da seguinte forma:

$$p(y, x|\theta) = p(y|J) \prod_t p(x_t|y_t, \theta) = \frac{1}{Z(J)} \prod_{s \sim t} \psi(y_s, y_t; J) \prod_t p(x_t|y_t, \theta) \quad (19.23)$$

onde $p(x_t|y_t = k, \theta)$ é a probabilidade de observar o pixel $x_t$ dado que o segmento correspondente pertence à classe *k* [^11]. O modelo observacional pode ser modelado usando uma gaussiana ou uma densidade não paramétrica [^11]. O modelo gráfico correspondente é uma mistura de arestas direcionadas e não direcionadas, como mostrado na Figura 19.9 [^11]. A treliça 2D não direcionada representa o prior $p(y)$; além disso, há uma aresta direcionada de cada $y_t$ para seu $x_t$ correspondente, representando a evidência local [^11]. Tecnicamente falando, esta combinação de um grafo não direcionado e direcionado é chamada de **chain graph** [^11].

#### Gaussian MRFs

Um GGM não direcionado, também chamado de **Gaussian MRF**, é um MRF pairwise da seguinte forma:

$$p(y|\theta) \propto \prod_{s \sim t} \psi_{st}(y_s, y_t) \prod_t \psi_t(y_t) \quad (19.24)$$

$$psi_{st}(y_s, y_t) = exp(-\frac{1}{2}y_s \Lambda_{st} y_t) \quad (19.25)$$

$$psi_t(y_t) = exp(-\frac{1}{2} \Lambda_{tt} y_t^2 + \eta_t y_t) \quad (19.26)$$

A distribuição conjunta pode ser escrita como:

$$p(y|\theta) \propto exp[\eta^T y - \frac{1}{2} y^T \Lambda y] \quad (19.27)$$

Reconhecemos isso como uma gaussiana multivariada escrita na **forma de informação** onde $\Lambda = \Sigma^{-1}$ e $\eta = \Lambda \mu$ [^12].

Se $\Lambda_{st} = 0$, então não há termo pairwise conectando *s* e *t*, então pelo teorema de fatorização (Teorema 2.2.1), concluímos que:

$$y_s \perp y_t | y_{\setminus \{s,t\}} \iff \Lambda_{st} = 0 \quad (19.28)$$

As entradas zero em $\Lambda$ são chamadas de **zeros estruturais**, pois representam as arestas ausentes no grafo [^12]. Assim, GGMs não direcionados correspondem a matrizes de precisão esparsas [^12].

### Conditional Random Fields (CRFs)

Um **Conditional Random Field (CRF)** (Lafferty et al. 2001), às vezes um *discriminative random field* (Kumar e Hebert 2003), é apenas uma versão de um MRF onde todos os potenciais de clique são condicionados em características de entrada:

$$p(y|x, w) = \frac{1}{Z(x, w)} \prod_c \psi_c(y_c|x, w) \quad (19.63)$$

Um CRF pode ser pensado como uma extensão de saída estruturada da regressão logística [^12]. Geralmente assumimos uma representação log-linear dos potenciais:

$$psi_c(y_c|x, w) = exp(w^T \phi(x, y_c)) \quad (19.64)$$

onde $\phi(x, y_c)$ é um vetor de características derivado das entradas globais *x* e do conjunto local de rótulos $y_c$ [^12].

A vantagem de um CRF sobre um MRF é análoga à vantagem de um classificador discriminativo sobre um classificador generativo (ver Seção 8.6), ou seja, não precisamos "desperdiçar recursos" modelando coisas que sempre observamos [^12]. Em vez disso, podemos focar nossa atenção em modelar o que nos importa, ou seja, a distribuição de rótulos dados os dados [^12].

#### CRF de Cadeia, MEMMs e o Problema do Label-Bias
O tipo mais amplamente usado de CRF usa um grafo de estrutura de cadeia para modelar a correlação entre rótulos vizinhos [^24]. Tais modelos são úteis para uma variedade de tarefas de rotulagem de sequência (ver Seção 19.6.2) [^24].

Uma maneira óbvia de fazer uma versão discriminativa de um HMM é "inverter as setas" de $Y_t$ para $x_t$, como na Figura 19.14(b) [^25]. Isso define um modelo discriminativo direcionado da forma

$$p(y|x, w) = \prod_t P(y_t|y_{t-1}, x, w) \quad (19.66)$$

onde $x = (x_{1:T}, x_g)$, $x_g$ são características globais, e $x_t$ são características específicas ao nó *t* [^25]. Isso é chamado de **maximum entropy Markov model** ou **MEMM** (McCallum et al. 2000; Kakade et al. 2002) [^25].

Um MEMM é simplesmente uma cadeia de Markov na qual as probabilidades de transição de estado são condicionadas nas características de entrada [^25]. Isso parece a generalização natural da regressão logística para a configuração de saída estruturada, mas sofre de um problema sutil conhecido (bastante obscuramente) como o **problema do label bias** (Lafferty et al. 2001) [^25]. O problema é que características locais no tempo *t* não influenciam estados anteriores ao tempo *t* [^25]. Isso segue ao examinar o DAG, que mostra que $x_t$ é d-separado de $y_{t-1}$ (e todos os pontos de tempo anteriores) pela v-estrutura em $y_t$, que é um filho oculto, bloqueando assim o fluxo de informação [^25].

O problema do label bias em MEMMs ocorre porque modelos direcionados são **localmente normalizados**, significando que cada CPD soma para 1 [^26]. Por contraste, MRFs e CRFs são **globalmente normalizados**, o que significa que fatores locais não precisam somar para 1, uma vez que a função de partição Z, que soma sobre todas as configurações conjuntas, irá garantir que o modelo defina uma distribuição válida [^26]. No entanto, esta solução tem um preço: não obtemos uma distribuição de probabilidade válida sobre *y* até que tenhamos visto a frase inteira, pois somente então podemos normalizar sobre todas as configurações [^26]. Consequentemente, CRFs não são tão úteis quanto DGMs (sejam discriminativos ou generativos) para inferência online ou em tempo real [^26]. Além disso, o fato de que Z depende de todos os nós, e, portanto, de todos os seus parâmetros, torna os CRFs muito mais lentos para treinar do que os DGMs, como veremos na Seção 19.6.3 [^26].

### Conclusão
Este capítulo apresentou uma visão abrangente dos Undirected Graphical Models (Markov Random Fields), incluindo suas propriedades de independência condicional, métodos de parametrização e vários exemplos práticos [^1, 5, 8]. Também exploramos extensões discriminativas como os Conditional Random Fields e como eles se comparam aos modelos generativos [^24]. Finalmente, introduzimos as Structural SVMs, que oferecem uma abordagem alternativa para o aprendizado de modelos estruturados, com foco na maximização da margem [^33].

### Referências
[^1]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.
[^5]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.
[^6]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.
[^7]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.
[^8]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.
[^9]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.
[^11]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.
[^12]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.
[^24]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.
[^25]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.
[^26]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.
[^33]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.
<!-- END -->