## Structural Support Vector Machines (SSVMs)

### Introdução

Este capítulo se concentra em **Structural Support Vector Machines (SSVMs)**, métodos para treinar classificadores de saída estruturada que aproveitam a existência de *solvers* MAP rápidos [^1]. SSVMs minimizam a perda esperada *a posteriori* no conjunto de treinamento, considerando a função de perda durante a estimativa de parâmetros [^1]. O objetivo do SSVM pode ser visto como a otimização de um limite superior no objetivo Bayesiano [^1]; ele concentra o esforço em ajustar os parâmetros que afetam o limite de decisão, tornando-o computacionalmente eficiente [^1].

### Conceitos Fundamentais

SSVMs são métodos para treinar classificadores com saída estruturada, onde a saída não é apenas uma única classe, mas sim uma estrutura complexa, como uma sequência, uma árvore ou um grafo. A eficiência dos SSVMs reside na capacidade de utilizar *solvers* MAP (Maximum A Posteriori) rápidos para inferir a saída estruturada mais provável, dado um conjunto de parâmetros [^1].

A formulação dos SSVMs envolve a minimização da perda esperada *a posteriori* no conjunto de treinamento [^1]. Formalmente, o objetivo é encontrar os parâmetros $w$ que minimizam a seguinte expressão:

$$\
\min_w \frac{1}{N} \sum_{i=1}^{N} \sum_{y} L(y_i, y) p(y | x_i, w)
$$

onde:

*   $N$ é o número de exemplos de treinamento.
*   $x_i$ é a entrada do i-ésimo exemplo.
*   $y_i$ é a saída estruturada verdadeira para o i-ésimo exemplo.
*   $y$ é uma possível saída estruturada.
*   $L(y_i, y)$ é a função de perda que quantifica a diferença entre a saída verdadeira $y_i$ e a saída predita $y$.
*   $p(y | x_i, w)$ é a distribuição de probabilidade da saída $y$ dado a entrada $x_i$ e os parâmetros $w$.

Os SSVMs abordam a minimização da perda esperada *a posteriori* otimizando um limite superior no objetivo Bayesiano [^1]. Isso é feito ao concentrar o esforço computacional no ajuste dos parâmetros que influenciam o limite de decisão, em vez de modelar toda a distribuição *a posteriori*.

Uma visão não probabilística dos SSVMs é abordada em [^1].
Nesta abordagem, o objetivo é encontrar os parâmetros $w$ que maximizem a margem entre a saída correta $y_i$ e todas as outras possíveis saídas $y$, sujeito a restrições que garantam que a margem seja maior ou igual a um valor predefinido. Este valor predefinido pode ser definido pela função de perda $L(y_i,y)$.

A formulação do problema de otimização é a seguinte:

$$\
\begin{aligned}
& \min_{w, \xi} \frac{1}{2} ||w||^2 + C \sum_{i=1}^N \xi_i \\\\
& \text{sujeito a } \forall i, \forall y \neq y_i: w^T \phi(x_i, y_i) - w^T \phi(x_i, y) \geq \Delta(y_i, y) - \xi_i, \xi_i \geq 0
\end{aligned}
$$

Onde:

*   $\phi(x_i, y)$ representa as *features* conjuntas da entrada $x_i$ e da saída $y$.
*   $\Delta(y_i, y)$ é uma função de perda que penaliza a predição de uma saída $y$ diferente da saída correta $y_i$.
*   $C$ é um parâmetro de regularização que controla o *trade-off* entre a maximização da margem e a minimização dos erros de classificação.
*   $\xi_i$ são variáveis de *slack* que permitem que algumas restrições sejam violadas, penalizando a violação no objetivo.

### Conclusão

Structural SVMs oferecem uma abordagem poderosa para problemas de aprendizado de saída estruturada, aproveitando a eficiência dos *solvers* MAP e otimizando um limite superior no objetivo Bayesiano [^1]. Essa estratégia permite que os SSVMs se concentrem no ajuste dos parâmetros que afetam o limite de decisão, resultando em um aprendizado eficiente e eficaz [^1]. Apesar de não serem convexos, podem ser resolvidos pelo método CCCP [^1].

### Referências

[^1]: Capítulo 19 do livro.
<!-- END -->