## Chain-Structured CRFs, MEMMs e o Problema do Label Bias

### Introdução
Este capítulo explora a relação entre Maximum Entropy Markov Models (MEMMs) e Conditional Random Fields (CRFs), focando no problema do *label bias* que aflige os MEMMs e como os CRFs o superam [^24]. Compreender essa distinção é crucial para selecionar o modelo apropriado para tarefas de modelagem sequencial, particularmente quando as dependências de longo alcance e o contexto global são importantes [^24].

### Conceitos Fundamentais

#### Maximum Entropy Markov Models (MEMMs)
Um **MEMM** é uma cadeia de Markov onde as probabilidades de transição de estado são condicionadas às *features* de entrada [^24]. No entanto, os MEMMs sofrem do problema do **label bias** [^24]. Esse problema surge porque as *features* locais no tempo *t* não influenciam os estados anteriores ao tempo *t* [^24]. Em outras palavras, a decisão sobre o estado no tempo *t* é baseada apenas nas observações locais e não leva em consideração a informação global da sequência [^24].

O problema do *label bias* em MEMMs ocorre porque os modelos direcionados são **localmente normalizados**, o que significa que cada Conditional Probability Distribution (CPD) soma 1 [^24]. Isso implica que, em estados com poucas transições possíveis, cada transição terá uma probabilidade relativamente alta, enquanto estados com muitas transições possíveis terão probabilidades menores, independentemente da influência das *features* [^24].

#### Conditional Random Fields (CRFs)

Em contraste com os MEMMs, os **CRFs** são modelos não direcionados que são **globalmente normalizados** [^24]. Isso significa que os fatores locais não precisam somar 1, pois a **função de partição** *Z*, que soma todas as configurações conjuntas, garante que o modelo defina uma distribuição válida [^24]. A função de partição é dada por:

$$ Z(0) = \sum_x \prod_{c \in C} \psi_c(y_c|0_c) $$

onde *C* é o conjunto de todos os cliques (maximais) de *G* [^6].

Em um CRF, o problema do *label bias* não existe, pois *y<sub>t</sub>* não bloqueia a informação de *x<sub>t</sub>* de alcançar outros nós *y<sub>t</sub>* [^24]. A normalização global permite que o modelo considere todas as dependências entre os estados e as observações, capturando o contexto global da sequência [^24].

#### A Vantagem dos CRFs
A principal vantagem de um CRF sobre um MEMM é que o CRF é capaz de considerar dependências de longo alcance e contexto global ao fazer previsões [^24]. Isso ocorre porque o CRF é globalmente normalizado, o que significa que cada estado é influenciado por todas as outras variáveis no modelo [^24]. Em contraste, um MEMM é localmente normalizado, o que significa que cada estado é influenciado apenas por seus vizinhos imediatos [^24].

### Conclusão

Enquanto MEMMs e CRFs são utilizados para modelagem sequencial, a normalização global dos CRFs supera as limitações do *label bias* presentes nos MEMMs [^24]. Essa capacidade de considerar o contexto global torna os CRFs mais adequados para tarefas onde as dependências de longo alcance são importantes, como em muitas aplicações de processamento de linguagem natural e visão computacional [^24].

### Referências
[^24]: Lafferty, J., McCallum, A., & Pereira, F. C. N. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. *Proceedings of the 18th International Conference on Machine Learning*, 282–289.
[^6]: Koller, D., & Friedman, N. (2009). *Probabilistic graphical models: principles and techniques*. MIT press.
<!-- END -->