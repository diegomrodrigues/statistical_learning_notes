## Conditional Random Fields (CRFs)

### Introdução
Este capítulo aprofunda o conceito de **Conditional Random Fields (CRFs)**, um tipo de modelo gráfico não direcionado (UGM) ou Markov Random Field (MRF) que se destaca na previsão de saídas estruturadas. CRFs, como mencionado na introdução aos modelos gráficos não direcionados [^1], oferecem uma abordagem discriminativa para modelar a densidade condicional $p(y|x)$, onde $x$ representa as características de entrada e $y$ representa as variáveis de saída estruturadas. A capacidade de condicionar potenciais de clique em características de entrada torna os CRFs particularmente adequados para tarefas onde a relação entre as características de entrada e a estrutura de saída é complexa e onde a modelagem explícita da distribuição conjunta de entradas e saídas (como em modelos gerativos) é desnecessária ou impraticável. Este capítulo explorará a formulação, a representação, o treinamento e as aplicações de CRFs, com foco em sua representação log-linear e os desafios associados ao treinamento de modelos com potenciais complexos e estruturas gráficas.

### Conceitos Fundamentais
Um **Conditional Random Field (CRF)** pode ser definido como uma versão de um MRF onde todos os potenciais de clique são condicionados em características de entrada, expressos como:
$$ p(y|x, w) = \frac{1}{Z(x, w)} \prod_{c} \psi_c(y_c|x, w) $$
onde:
- $y$ representa as variáveis de saída estruturadas.
- $x$ representa as características de entrada.
- $w$ representa os parâmetros do modelo.
- $c$ indexa os cliques no grafo.
- $\psi_c(y_c|x, w)$ é o potencial de clique para o clique $c$, condicionado nas características de entrada $x$ e parametrizado por $w$.
- $Z(x, w)$ é a função de partição, que garante que a distribuição seja normalizada.

A principal diferença entre um MRF e um CRF é que, em um MRF, os potenciais de clique são definidos sobre as variáveis de saída $y$ sem condicionamento explícito nas características de entrada $x$, enquanto em um CRF, os potenciais de clique são condicionados em $x$. Essa distinção permite que os CRFs modelem diretamente a dependência entre as saídas e as entradas, tornando-os modelos discriminativos.

A representação log-linear é uma escolha comum para os potenciais de clique em CRFs:
$$ \psi_c(y_c|x, w) = \exp(w^T \phi(x, y_c)) $$
onde $\phi(x, y_c)$ é um vetor de características derivado das entradas globais $x$ e do conjunto local de rótulos $y_c$. Essa representação oferece várias vantagens, incluindo flexibilidade na definição de características e facilidade de cálculo do gradiente para treinamento.

**Treinamento de CRFs:**
O treinamento de um CRF envolve estimar os parâmetros $w$ que maximizam a probabilidade condicional dos dados de treinamento. Dada uma amostra de treinamento $\{(x_i, y_i)\}_{i=1}^N$, o objetivo é maximizar o log-likelihood:
$$ \ell(w) = \sum_{i=1}^N \log p(y_i|x_i, w) = \sum_{i=1}^N \left[ \sum_c w^T \phi(x_i, y_{ic}) - \log Z(x_i, w) \right] $$
onde $y_{ic}$ denota a atribuição de rótulos para as variáveis no clique $c$ para a amostra $i$.

O gradiente do log-likelihood com relação aos parâmetros $w$ é dado por:
$$ \frac{\partial \ell}{\partial w} = \sum_{i=1}^N \left[ \sum_c \phi(x_i, y_{ic}) - \mathbb{E}_{p(y|x_i, w)} \left[ \sum_c \phi(x_i, y_c) \right] \right] $$
onde $\mathbb{E}_{p(y|x_i, w)}$ denota o valor esperado das características sob a distribuição condicional $p(y|x_i, w)$. O primeiro termo no gradiente é a soma das características observadas nos dados de treinamento, e o segundo termo é o valor esperado das características sob o modelo atual.

O treinamento de CRFs apresenta desafios significativos, principalmente devido à necessidade de calcular a função de partição $Z(x, w)$ e o valor esperado das características sob o modelo atual. Para grafos com estrutura complexa, o cálculo exato dessas quantidades é intratável, exigindo o uso de métodos de inferência aproximados, como:
- **Pseudo-likelihood:** Aproxima a verossimilhança conjunta pelo produto das verossimilhanças condicionais completas.
- **Stochastic maximum likelihood (SML):** Utiliza Monte Carlo Markov Chain (MCMC) para aproximar o valor esperado das características sob o modelo atual.
- **Iterative proportional fitting (IPF):** Um algoritmo iterativo que atualiza os potenciais de clique para corresponder às estatísticas empíricas.

### Conclusão
Os Conditional Random Fields (CRFs) oferecem uma abordagem poderosa e flexível para a previsão de saídas estruturadas, combinando a capacidade de modelar dependências complexas entre variáveis de saída com a capacidade de condicionar essas dependências em características de entrada. Embora o treinamento de CRFs apresente desafios computacionais, o desenvolvimento de métodos de inferência aproximados e técnicas de otimização eficiente tornou os CRFs uma ferramenta amplamente utilizada em uma variedade de aplicações, incluindo processamento de linguagem natural, visão computacional e bioinformática.
### Referências
[^1]: Capítulo 19. Undirected graphical models (Markov random fields)
[^677]: Seção 19.5. Learning
[^678]: Seção 19.5.3. Approximate methods for computing the MLEs of MRFs
<!-- END -->