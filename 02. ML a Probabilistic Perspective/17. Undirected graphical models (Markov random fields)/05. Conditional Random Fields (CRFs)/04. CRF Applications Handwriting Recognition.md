## Aplicações de Conditional Random Fields (CRFs) em Reconhecimento de Padrões

### Introdução
Os **Conditional Random Fields (CRFs)** são modelos probabilísticos não direcionados que definem uma distribuição de probabilidade condicional sobre variáveis de saída dado um conjunto de variáveis de entrada [^684]. Diferentemente dos Modelos de Markov (MRFs), os CRFs são modelos discriminativos, o que significa que eles modelam diretamente a probabilidade condicional $p(y|x)$, onde $y$ representa as variáveis de saída e $x$ representa as variáveis de entrada [^684]. Esta característica permite que os CRFs sejam mais flexíveis na incorporação de *features* (características) do domínio do problema, sem a necessidade de modelar a distribuição conjunta de entradas e saídas [^684]. Este capítulo explora as aplicações de CRFs em reconhecimento de padrões, com foco em reconhecimento de escrita manual, *noun phrase chunking* e reconhecimento de entidades nomeadas.

### Reconhecimento de Escrita Manual
Uma aplicação proeminente de CRFs é no **reconhecimento de escrita manual**, onde a ambiguidade local de caracteres individuais pode ser resolvida utilizando o contexto fornecido pelos caracteres vizinhos [^686]. A ideia central é que, embora um caractere isolado possa ser difícil de identificar, a sequência de caracteres como um todo oferece informações contextuais que auxiliam na identificação correta [^686].

#### Classificação de Sequências de Dígitos Manuscritos
Especificamente, os CRFs são aplicados para classificar *strings* de dígitos manuscritos [^686]. Neste contexto, o potencial de nó, $\Psi_t(y_t|x_t)$, é frequentemente definido como um classificador discriminativo probabilístico, como uma rede neural ou uma *Relevance Vector Machine* (RVM) [^686]. Este classificador estima a probabilidade do rótulo $y_t$ para o dígito $x_t$ na posição $t$.

Para modelar as dependências entre dígitos adjacentes, potenciais de aresta, $\Psi_{st}(y_s, y_t)$, são introduzidos para capturar as transições mais prováveis entre pares de dígitos [^686]. Por exemplo, a probabilidade de um "3" seguir um "2" pode ser maior do que a probabilidade de um "9" seguir um "2". Desta forma, o CRF é capaz de usar o contexto para refinar a classificação de cada dígito.

O modelo CRF para reconhecimento de dígitos manuscritos pode ser formalizado como:

$$
p(y|x, w) = \frac{1}{Z(x, w)} \prod_{t=1}^{T} \Psi_t(y_t|x_t) \prod_{t=1}^{T-1} \Psi_{t,t+1}(y_t, y_{t+1})
$$

onde:
*   $y$ representa a sequência de rótulos (dígitos classificados).
*   $x$ representa a sequência de imagens de dígitos manuscritos.
*   $w$ representa os parâmetros do modelo.
*   $T$ é o comprimento da sequência.
*   $\Psi_t(y_t|x_t)$ é o potencial do nó, que representa a probabilidade de um dígito individual ser classificado como $y_t$ dado a imagem $x_t$.
*   $\Psi_{t,t+1}(y_t, y_{t+1})$ é o potencial da aresta, que representa a probabilidade de transição entre os dígitos $y_t$ e $y_{t+1}$.
*   $Z(x, w)$ é a função de partição, que garante que a distribuição de probabilidade condicional seja normalizada.

### Noun Phrase Chunking
Outra aplicação relevante é o **noun phrase chunking**, que envolve a segmentação de uma sentença em *noun phrases* (NPs) distintos [^687]. Esta tarefa é um exemplo de *shallow parsing*, onde o objetivo é identificar a estrutura básica de uma frase sem realizar uma análise sintática completa [^687].

#### BIO Notation
No *noun phrase chunking*, cada palavra na sentença é etiquetada usando a notação BIO, onde:
*   B indica o início de um novo NP.
*   I indica que a palavra está dentro de um NP.
*   O indica que a palavra está fora de um NP [^687].

Por exemplo, na frase "British Airways rose after announcing its withdrawal from the UAI deal", os NPs são "British Airways", "its withdrawal" e "the UAI deal" [^687].

#### CRF para Noun Phrase Chunking
Um CRF pode ser usado para modelar a probabilidade condicional da sequência de etiquetas NP (NP1:T) e etiquetas POS (POS1:T) dada a sequência de palavras (words1:T), ou seja, $p(NP_{1:T}, POS_{1:T}|words_{1:T})$ [^687]. Ao modelar as dependências entre etiquetas adjacentes, o CRF pode impor restrições, como o fato de que B deve preceder I [^687].

Os *features* usados neste modelo são frequentemente projetados manualmente e podem incluir informações sobre se uma palavra começa com uma letra maiúscula, se é seguida por um ponto final e se é um substantivo [^687]. Tipicamente, existem entre 1.000 e 10.000 *features* por nó [^687].

### Reconhecimento de Entidades Nomeadas
O **reconhecimento de entidades nomeadas** (NER) é uma tarefa relacionada ao *noun phrase chunking* que envolve a identificação e classificação de *phrases* que se referem a pessoas, locais, organizações, etc. [^688].

#### CRF para NER
Uma abordagem comum para NER é usar um CRF com uma estrutura de cadeia, mas com um espaço de estados expandido para incluir etiquetas como B-Per (início de nome de pessoa), I-Per (dentro de nome de pessoa), B-Loc (início de nome de local), I-Loc (dentro de nome de local) e Other (nenhuma entidade) [^688].

Para melhorar o desempenho do NER, correlações de longo alcance entre palavras podem ser consideradas, por exemplo, adicionando um *link* entre todas as ocorrências da mesma palavra e forçando a palavra a ter a mesma etiqueta em cada ocorrência [^688]. Este modelo é conhecido como *skip-chain CRF* [^688].

### Conclusão
Os CRFs são modelos poderosos para tarefas de reconhecimento de padrões que envolvem dados sequenciais ou estruturados. Sua capacidade de modelar diretamente a probabilidade condicional das variáveis de saída dadas as variáveis de entrada, juntamente com sua flexibilidade na incorporação de *features* do domínio do problema, os torna uma escolha popular para uma ampla gama de aplicações, incluindo reconhecimento de escrita manual, *noun phrase chunking* e reconhecimento de entidades nomeadas. No entanto, é importante notar que os CRFs requerem dados de treinamento rotulados e podem ser mais lentos para treinar do que os modelos generativos [^684].

<!-- END -->