## Parameterização de MRFs: O Teorema de Hammersley-Clifford

### Introdução
Este capítulo explora a parameterização de Markov Random Fields (MRFs), focando no teorema de Hammersley-Clifford, que estabelece uma conexão fundamental entre as propriedades de independência condicional de uma distribuição e sua representação em termos de potenciais de clique. Em contraste com os Directed Graphical Models (DGMs), onde a parameterização é naturalmente derivada da regra da cadeia [^665], os MRFs requerem uma abordagem diferente devido à ausência de uma ordenação topológica inerente.

### Conceitos Fundamentais
Em MRFs, a distribuição conjunta é representada através de **funções de potencial** ou **fatores**, associados a cada clique maximal no grafo [^665]. Uma *clique* é um subconjunto de nós onde cada par de nós é conectado por uma aresta. Um *clique maximal* é um clique que não pode ser estendido adicionando outro nó ao subconjunto sem quebrar a propriedade de clique [^666].

Formalmente, seja $G = (V, E)$ um grafo não-direcionado, onde $V$ é o conjunto de nós e $E$ é o conjunto de arestas. Seja $C$ o conjunto de cliques maximais em $G$. A distribuição conjunta sobre as variáveis aleatórias $y = (y_1, ..., y_n)$, onde cada $y_i$ corresponde a um nó em $V$, é dada por:

$$ p(y|\theta) = \frac{1}{Z(\theta)} \prod_{c \in C} \psi_c(y_c|\theta_c) $$

onde:
*   $\psi_c(y_c|\theta_c)$ é a função de potencial associada ao clique $c$, que depende das variáveis $y_c$ pertencentes ao clique e dos parâmetros $\theta_c$.
*   $Z(\theta)$ é a função de partição, responsável por normalizar o produto dos potenciais de clique, garantindo que a distribuição conjunta some a 1 [^665]:

$$ Z(\theta) = \sum_{y} \prod_{c \in C} \psi_c(y_c|\theta_c) $$

A função de partição é uma soma sobre todas as possíveis configurações de $y$, o que pode tornar seu cálculo computacionalmente desafiador, especialmente para grafos grandes [^665].

O **Teorema de Hammersley-Clifford** [^666] estabelece a equivalência entre a representação de uma distribuição por um MRF e suas propriedades de independência condicional. Formalmente:

**Teorema 19.3.1 (Hammersley-Clifford):** Uma distribuição positiva $p(y) > 0$ satisfaz as propriedades de independência condicional de um grafo não-direcionado $G$ se e somente se $p$ pode ser representada como um produto de fatores, um por clique maximal, ou seja,

$$ p(y|\theta) = \frac{1}{Z(\theta)} \prod_{c \in C} \psi_c(y_c|\theta_c) $$

O teorema garante que qualquer distribuição positiva cujas propriedades de independência condicional podem ser representadas por um UGM pode ser representada desta forma [^665].

**Observação:** A restrição de positividade ($p(y) > 0$) é crucial. Distribuições que violam essa condição podem apresentar independências não refletidas no grafo [^666].

**Exemplo:** Considere um MRF com três variáveis, $y_1, y_2, y_3$, e cliques maximais $\{y_1, y_2\}$ e $\{y_2, y_3\}$. A distribuição conjunta pode ser escrita como [^666]:

$$ p(y|\theta) = \frac{1}{Z(\theta)} \psi_{12}(y_1, y_2) \psi_{23}(y_2, y_3) $$

onde $\psi_{12}$ e $\psi_{23}$ são as funções de potencial associadas aos cliques $\{y_1, y_2\}$ e $\{y_2, y_3\}$, respectivamente. A função de partição é dada por [^666]:

$$ Z = \sum_{y} \psi_{12}(y_1, y_2) \psi_{23}(y_2, y_3) $$

### Conclusão
O teorema de Hammersley-Clifford fornece uma base teórica sólida para a parameterização de MRFs, conectando as propriedades de independência condicional à representação da distribuição conjunta. A escolha das funções de potencial e a computação da função de partição são aspectos cruciais na modelagem com MRFs, com implicações significativas na complexidade computacional e na capacidade de representar diferentes tipos de dependências [^665].

### Referências
[^665]: Koller, D., & Friedman, N. (2009). *Probabilistic graphical models: principles and techniques*. MIT press.
[^666]: Lafferty, J., McCallum, A., & Pereira, F. C. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. *In Proceedings of the 18th international conference on machine learning* (pp. 282-289).

<!-- END -->