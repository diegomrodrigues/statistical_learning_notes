## Representação de Funções Potenciais em MRFs

### Introdução
No contexto de **Parameterization of MRFs** [^5], a representação das **funções potenciais** é um passo crucial para definir a distribuição conjunta sobre as variáveis do modelo. As funções potenciais, também conhecidas como *fatores*, capturam as dependências entre as variáveis em cada clique do grafo [^5]. Este capítulo explora diferentes formas de representar essas funções potenciais, com ênfase nos modelos log-lineares ou de máxima entropia.

### Conceitos Fundamentais
As funções potenciais $\psi_c(y_c)$ podem ser representadas de diversas maneiras [^5]:
1.  **Tabelas de Números Não Negativos:** Similar ao que se faz com as Conditional Probability Tables (CPTs) em DGMs, as funções potenciais podem ser representadas como tabelas contendo números não negativos [^6]. Esses números refletem a compatibilidade relativa entre diferentes atribuições das variáveis no clique $c$. No entanto, as funções potenciais não são probabilidades, mas sim medidas de compatibilidade [^7].

2.  **Funções Lineares dos Parâmetros:** Uma abordagem mais geral e flexível é definir os log potenciais como funções lineares dos parâmetros [^7]. Essa abordagem leva aos modelos de máxima entropia ou log-lineares.

    *   **Modelos Log-Lineares:** Nesses modelos, o logaritmo da função potencial é expresso como uma função linear dos parâmetros $\theta_c$ e um vetor de características $\phi_c(y_c)$ derivado dos valores das variáveis $y_c$ [^7]:
        $$         \log \psi_c(y_c) = \phi_c(y_c)^T \theta_c         $$
        O vetor de características $\phi_c(y_c)$ codifica informações relevantes sobre a configuração das variáveis no clique $c$. Os parâmetros $\theta_c$ representam os pesos associados a essas características [^7].

        > Essa representação permite capturar dependências complexas entre as variáveis, sem a necessidade de especificar manualmente todos os valores da tabela de potencial.

        A probabilidade conjunta resultante tem a forma:
        $$         \log p(y|\theta) = \sum_c \phi_c(y_c)^T \theta_c - \log Z(\theta)         $$
        onde $Z(\theta)$ é a função de partição, que garante que a distribuição de probabilidade some para 1 [^7].

        Um exemplo prático é modelar a ortografia em inglês. Em vez de usar uma tabela de potencial com $26^3$ parâmetros para trigramas de letras, podemos definir funções indicadoras para trigramas específicos (como "ing" ou "qu-") e combinar esses recursos linearmente [^7]:
        $$         \psi(y_{t-1}, y_t, y_{t+1}) = \exp \left( \sum_k \theta_k \phi_k(y_{t-1}, y_t, y_{t+1}) \right)         $$
        onde $\phi_k$ é uma função binária que indica a presença do trigrama $k$ e $\theta_k$ é o peso correspondente [^7]. Isso permite definir a probabilidade de uma palavra de qualquer tamanho usando:
        $$         p(y|\theta) \propto \exp \left( \sum_t \sum_k \theta_k \phi_k(y_{t-1}, y_t, y_{t+1}) \right)         $$
        Essa abordagem é mais parcimoniosa e permite generalizar para trigramas não observados nos dados de treinamento [^7].

### Conclusão
A escolha da representação da função potencial impacta diretamente a capacidade do modelo de capturar dependências complexas e a eficiência computacional do processo de aprendizado [^7]. A representação log-linear, com vetores de características e parâmetros ajustáveis, oferece uma alternativa flexível e poderosa para modelar distribuições de probabilidade complexas em MRFs. Essa abordagem é amplamente utilizada em diversas aplicações, como modelagem de linguagem, visão computacional e bioinformática [^8].

### Referências
[^5]: 19.3 Parameterization of MRFs (p. 665)
[^6]: 19.3.1 The Hammersley-Clifford theorem (p. 665)
[^7]: 19.3.2 Representing potential functions (p. 667)

<!-- END -->