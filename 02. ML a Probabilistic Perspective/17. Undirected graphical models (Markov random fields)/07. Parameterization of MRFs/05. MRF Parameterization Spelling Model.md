## Parameterização de MRFs para Modelagem de Ortografia

### Introdução
Este capítulo explora a parameterização de Markov Random Fields (MRFs) e, especificamente, como esses modelos podem ser aplicados para modelagem de ortografia [^1]. A parameterização de MRFs é uma alternativa à utilização de Directed Graphical Models (DGMs) [^5], oferecendo uma abordagem mais natural para domínios onde a direção das arestas é arbitrária [^1]. Embora as propriedades de independência condicional de UGMs sejam mais simples e naturais do que as de DGMs, representar a distribuição conjunta para um UGM é menos direto [^5].

### Conceitos Fundamentais

#### Parameterização de MRFs
Em vez de associar Conditional Probability Distributions (CPDs) a cada nó, como é feito em DGMs, em MRFs, associamos **funções de potencial** ou **fatores** a cada clique maximal no grafo [^5]. A função de potencial para um clique $c$ é denotada por $\psi_c(y_c|\theta_c)$, onde $y_c$ representa as variáveis no clique $c$ e $\theta_c$ são os parâmetros associados [^5]. Uma função de potencial deve ser não-negativa, e a distribuição conjunta é proporcional ao produto dos potenciais de clique [^5]:

$$
p(y|\theta) = \frac{1}{Z(\theta)} \prod_{c \in C} \psi_c(y_c|\theta_c)
$$

onde $C$ é o conjunto de todos os cliques maximais no grafo, e $Z(\theta)$ é a função de partição, garantindo que a distribuição se normalize para 1 [^6]:

$$
Z(\theta) = \sum_y \prod_{c \in C} \psi_c(y_c|\theta_c)
$$

#### Modelos Pairwise e a Relação com a Ortografia
Uma forma comum de MRF é o **pairwise MRF**, onde restringimos a parameterização às arestas do grafo [^6]. Isso simplifica o modelo, embora possa reduzir sua generalidade [^6]. No contexto da modelagem de ortografia, podemos associar um *vetor de características* de comprimento $K^2$ a cada aresta, transformando-o em uma função de potencial $K \times K$ [^7], onde $K$ é o número de estados possíveis para cada variável (por exemplo, o número de letras no alfabeto) [^7].

#### Modelagem de Trigramas para Ortografia em Inglês
Para modelar a ortografia em inglês, podemos usar **funções indicadoras** para trigramas de letras "especiais" [^7]. Definimos o potencial em cada trigrama como uma função exponencial dessas características, amarrando os parâmetros entre as localizações para definir a probabilidade de uma palavra de qualquer comprimento [^7].

Especificamente, seja $y_t$ a letra na posição $t$ de uma palavra. Definimos a função potencial para um trigrama como [^7]:

$$
\psi(y_{t-1}, y_t, y_{t+1}) = \exp \left( \sum_k \theta_k \phi_k(y_{t-1}, y_t, y_{t+1}) \right)
$$

onde $\phi_k(y_{t-1}, y_t, y_{t+1})$ é uma função indicadora binária que indica a presença de um trigrama específico (por exemplo, "ing", "qu-") [^7], e $\theta_k$ é o parâmetro correspondente associado a essa característica [^7].

A probabilidade de uma palavra $y$ de qualquer comprimento é então dada por [^7]:

$$
p(y|\theta) \propto \exp \left( \sum_t \sum_k \theta_k \phi_k(y_{t-1}, y_t, y_{t+1}) \right)
$$

Essa abordagem permite que o modelo capture dependências de longo alcance entre as letras em uma palavra, mesmo que a dependência direta seja apenas entre trigramas adjacentes [^7]. A **amarração dos parâmetros** ($\theta_k$) entre diferentes posições na palavra garante que o modelo possa generalizar para palavras de diferentes comprimentos [^7].

#### Aprendizagem das Funções de Características

A escolha das funções de características $\phi_k$ é crucial para o desempenho do modelo [^7]. Em muitas aplicações, essas características são criadas manualmente para refletir o conhecimento do domínio [^7]. No entanto, também é possível aprender essas características a partir dos dados [^7]. Uma abordagem é usar **feature induction**, começando com um conjunto base de características e, em seguida, criar continuamente novas combinações de características, adicionando greedy aquelas que melhoram o desempenho do modelo [^20].

### Conclusão
A parameterização de MRFs oferece uma abordagem flexível e poderosa para modelar a ortografia em inglês [^7]. Ao usar funções indicadoras para trigramas de letras "especiais" e amarrar os parâmetros entre as localizações, podemos definir a probabilidade de uma palavra de qualquer comprimento [^7]. A escolha das funções de características é crucial para o desempenho do modelo, e técnicas como feature induction podem ser usadas para aprender essas características a partir dos dados [^7]. Embora a modelagem de ortografia possa parecer um problema simples, a escolha das funções de potencial e a complexidade da inferência podem levar a modelos sofisticados e eficazes [^7].

### Referências
[^1]: Page 1, Chapter 19. Undirected graphical models (Markov random fields)
[^5]: Page 5, Chapter 19. Undirected graphical models (Markov random fields)
[^6]: Page 6, Chapter 19. Undirected graphical models (Markov random fields)
[^7]: Page 7, Chapter 19. Undirected graphical models (Markov random fields)
[^20]: Page 20, Chapter 19. Undirected graphical models (Markov random fields)
<!-- END -->