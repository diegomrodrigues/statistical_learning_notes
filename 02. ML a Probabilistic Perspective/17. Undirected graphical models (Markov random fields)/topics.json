{
  "topics": [
    {
      "topic": "Undirected Graphical Models (Markov Random Fields)",
      "sub_topics": [
        "Undirected Graphical Models (UGMs), also known as Markov Random Fields (MRFs) or Markov Networks, are graphical models that do not require the specification of edge orientations, making them suitable for problems where the relationships between variables are symmetric, such as in image analysis and spatial statistics. UGMs capture conditional dependence relationships between variables through the structure of the graph, where the absence of an edge between two nodes indicates conditional independence given the remaining variables. They offer an alternative to Directed Graphical Models (DGMs) when edge orientation is not naturally defined, offering symmetry suitable for spatial or relational data.",
        "The main advantage of UGMs over DGMs is their symmetry, which makes them more natural for certain domains, such as spatial or relational data. Additionally, discriminative UGMs, such as Conditional Random Fields (CRFs), which define conditional densities of the form p(y|x), generally perform better than discriminative DGMs. The disadvantages of UGMs compared to DGMs include the lower interpretability and modularity of the parameters, as well as the higher computational cost in parameter estimation.",
        "UGMs define conditional independence (CI) relationships through graph separation: for sets of nodes A, B, and C, XA is independent of XB conditional on XC if C separates A from B in the graph G. This means that, upon removing all nodes in C, there are no paths connecting any node in A to any node in B. This property is fundamental for inference and learning in UGMs and is known as the global Markov property.",
        "The global Markov property for UGMs states that if there are no paths connecting nodes in A to nodes in B after removing C, then A and B are conditionally independent given C. The Markov blanket of a node t is the set of nodes that makes it conditionally independent of all other nodes in the graph. In a UGM, a node's Markov blanket consists of its immediate neighbors, known as the undirected local Markov property. Formally, the Markov blanket of a node t, denoted as mb(t), includes all nodes that render t conditionally independent of all other nodes in the graph, formally defined as t \u2aeb V \\ cl(t) | mb(t), where cl(t) is the closure of node t, comprising mb(t) \u222a {t}.",
        "Two additional Markov properties are relevant: the pairwise Markov property, which states that two nodes are conditionally independent given the rest if there is no direct edge between them, and the implication that global Markov implies local Markov, which implies pairwise Markov. The pairwise Markov Property states that two nodes are conditionally independent given the rest if there is no direct edge between them, represented as s \u2aeb t | V \\ {s, t} \u21d4 Gst = 0. Assuming a positive density p(x) > 0 for all x, the pairwise property implies global, making all these Markov properties equivalent, allowing the use of pairwise CI statements to construct a graph from which global CI statements can be extracted. Under the condition of a positive density function (p(x) > 0), pairwise Markov property also implies global Markov property, making all three equivalent. This result is useful because pairwise CI is usually easier to empirically assess.",
        "Moralization is a process used to convert a DGM to a UGM by adding edges between 'unmarried parents' (nodes that have a common child) and removing the direction of the edges. This process ensures that the resulting UGM captures the relevant conditional independence relationships of the original DGM, avoiding incorrect declarations of conditional independence. However, moralization can lead to the loss of some conditional independence information present in the original DGM, making the moralized UGM unsuitable for determining the CI properties of the original DGM. To determine if A \u2aeb B|C in a DGM, first form the ancestral graph of the DAG G with respect to U = A \u222a B \u222a C, remove all nodes that are not in U or are ancestors of U, and then moralize this ancestral graph.",
        "Both DGMs and UGMs are capable of representing different sets of distributions perfectly. DGMs can model cause-and-effect relationships more naturally, while UGMs are more suitable for representing symmetric dependencies. Some distributions can be perfectly represented by both types of models; in these cases, the corresponding graph must be chordal. The conditional independence properties in UGMs are monotonic, while in DGMs they can be non-monotonic. A graph G is a perfect map of a distribution p if I(G) = I(p), meaning the graph can represent all and only the CI properties of the distribution. DGMs and UGMs are perfect maps for different sets of distributions, making neither more powerful than the other as a representation language. Distributions that can be perfectly modeled by either a DGM or a UGM are called decomposable or chordal, implying that if we collapse together all the variables in each maximal clique, to make \u201cmega-variables\u201d, the resulting graph will be a tree.",
        "The Hammersley-Clifford theorem states that a positive distribution p(y) > 0 satisfies the CI properties of an undirected graph G if and only if p can be represented as a product of factors, one per maximal clique in the graph. This theorem provides a theoretical basis for the parameterization of MRFs using potential functions defined over maximal cliques, i.e., p(y|\u03b8) = (1/Z(\u03b8)) \u220f \u03c8c(yc|\u03b8c), where C is the set of all maximal cliques of G, and Z(\u03b8) is the partition function. UGMs use potential functions or factors associated with each maximal clique in the graph, rather than conditional probability distributions (CPDs) associated with each node as in DGMs, due to the absence of topological ordering.",
        "The Gibbs distribution is a model closely related to UGMs, with the form p(y|\u03b8) = (1/Z(\u03b8)) * exp(-\u2211c Ec(yc|\u03b8c)), where Ec(yc) > 0 is the energy associated with the variables in clique c, and high probability states correspond to low energy configurations. Potential functions can be represented as tables of non-negative numbers or, more generally, as a log-linear function of the parameters, log \u03c8c(yc) = \u03c6c(yc)T\u03b8c, which is also known as a maximum entropy model."
      ]
    },
    {
      "topic": "Ising Model",
      "sub_topics": [
        "The Ising Model is an example of an MRF that originated from statistical physics, used originally to model the behavior of magnets. It represents the spin of an atom (ys) as a value in {-1, +1}, indicating spin down or spin up. In ferromagnets, neighboring spins tend to align in the same direction, while in antiferromagnets, they prefer to be different. The Ising model captures these behaviors. The model is constructed by creating a graph (typically a 2D or 3D grid) and connecting neighboring variables. The interaction between neighboring spins is defined by pairwise clique potentials.",
        "The strength of this interaction is determined by a parameter Wst, representing the coupling strength between nodes s and t. If Wst > 0, neighboring spins are likely to be in the same state (ferromagnet); if Wst < 0, they tend to be in different states (antiferromagnet). When all the weights are positive (J > 0), the model represents a ferromagnet, and if the weights are sufficiently strong, the corresponding probability distribution will have two modes, corresponding to the all +1 state and the all -1 state, known as the ground states of the system. When the weights are negative (J < 0), the model represents an antiferromagnet, resulting in a frustrated system where not all constraints can be satisfied simultaneously, leading to multiple modes in the probability distribution.",
        "The Ising model has an analogy to Gaussian graphical models. Assuming yt \u2208 {-1, +1}, the unnormalized log probability of an Ising model can be written as log p(y) = \u2212(1/2) \u2211 ys Wst yt, where the minimization of energy corresponds to high-probability states if neighbors agree. Hopfield networks are fully connected Ising models with a symmetric weight matrix, applied as associative memories, where weights and bias terms are learned from data, and pattern completion is achieved through iterative conditional modes (ICM)."
      ]
    },
    {
      "topic": "Potts Model",
      "sub_topics": [
        "The Potts Model is a generalization of the Ising model to multiple discrete states, where yt \u2208 {1, 2, ..., K}. A potential function is commonly used with the following form: \u03c8st(ys, yt) = exp(J) if ys = yt and 0 otherwise. In the Potts model, if J > 0, neighboring nodes are encouraged to have the same label. This model exhibits a phase transition: for J > 1.44, large clusters occur; for J < 1.44, many small clusters occur; and at the critical value of K = 1.44, there is a mixture of small and large clusters.",
        "The Potts model serves as a prior for image segmentation, assuming that neighboring pixels are likely to have the same discrete label and therefore belong to the same segment. This prior can be combined with a likelihood term to form a complete model. To combine this prior with a likelihood term, we can use an observation model where the probability of observing a pixel given that the corresponding segment belongs to class k is modeled using a Gaussian or non-parametric density. The corresponding graphical model is a mixture of undirected and directed edges, where the undirected 2D grid represents the prior p(y), and the directed edges represent the local evidence. The combination of the Potts prior with a likelihood term results in a graphical model that mixes directed and undirected edges. The undirected 2D grid represents the prior p(y), and the directed edges from each yt to its corresponding xt represent the local evidence."
      ]
    },
    {
      "topic": "Gaussian MRFs",
      "sub_topics": [
        "A Gaussian Undirected Model (GGM), also called a Gaussian MRF, is a pairwise MRF of the following form: p(y|\u03b8) \u221d \u220f \u03c8st(ys, yt) \u220f \u03c8t(yt), where \u03c8st(ys, yt) = exp(\u2212(1/2)ysAstyt) and \u03c8t(yt) = exp(\u2212(1/2)Atty\u00b2 + \u03b7tyt). In a Gaussian MRF, if Ast = 0, then there is no pairwise term connecting s and t, and by the factorization theorem, ys \u2aeb yt | y\u2212{s,t}. Zero entries in the precision matrix A (structural zeros) represent absent edges in the graph, corresponding to sparse precision matrices.",
        "The zero entries in A are called structural zeros, as they represent the missing edges in the graph. Thus, undirected GGMs correspond to sparse precision matrices, a fact that is exploited to efficiently learn the structure of the graph. Directed GGMs correspond to sparse regression matrices and therefore sparse Cholesky factorizations of covariance matrices, whereas undirected GGMs correspond to sparse precision matrices."
      ]
    },
    {
      "topic": "Conditional Random Fields (CRFs)",
      "sub_topics": [
        "A Conditional Random Field (CRF) is a version of an MRF where all the clique potentials are conditioned on input features, given by: p(y|x, w) = (1/Z(x, w)) \u220f \u03c8c(yc|x, w). CRFs are discriminative models that condition clique potentials on input features, making them suitable for structured output prediction tasks; the clique potentials take the form p(y|x, w) = (1/Z(x, w)) * \u220f\u03c8c(yc|x, w). A CRF can be considered a structured output extension of logistic regression. CRFs are often used with a log-linear representation of the potentials: \u03c8c(yc|x, w) = exp(wT\u03c6(x, yc)), where \u03c6(x, yc) is a feature vector derived from the global inputs x and the local set of labels yc. CRFs are typically represented using a 'log-linear' representation of the potentials.",
        "The advantage of a CRF over an MRF is analogous to the advantage of a discriminative classifier over a generative classifier: we don't need to model things that we always observe. Instead, we can focus our attention on modeling what we care about, namely the distribution of labels given the data. Another important advantage of CRFs is that we can make the potentials (or factors) of the model dependent on the data. For example, in image processing applications, we can turn off label smoothing between two neighboring nodes s and t if there is an observed discontinuity in image intensity between pixels s and t. Similarly, in natural language processing problems, we can make latent labels dependent on global properties of the sentence, such as the language in which it is written.",
        "The Maximum Entropy Markov Model (MEMM) is a Markov chain in which the state transition probabilities are conditioned on the input features, but it suffers from the label bias problem. This problem occurs because the local features at time t do not influence the states prior to time t. The label bias problem in MEMMs occurs because directed models are locally normalized, meaning that each CPD sums to 1. In contrast, MRFs and CRFs are globally normalized, which means that the local factors do not need to sum to 1, since the partition function Z, which sums over all joint configurations, will ensure that the model defines a valid distribution. In a CRF, the label bias problem does not exist, as yt does not block information from xt from reaching other yt nodes.",
        "CRFs have been applied to many interesting problems, including handwriting recognition, noun phrase chunking, and named entity recognition. In handwriting recognition, the key observation is that locally a letter may be ambiguous, but depending on the (unknown) labels of the neighbors, it is possible to use context to reduce the error rate. A natural application of CRFs is to classify hand-written digit strings, where the node potential is often taken to be a probabilistic discriminative classifier, such as a neural network or RVM.",
        "Training CRFs involves modifying the gradient-based optimization of MRFs for the CRF case in a straightforward manner. In particular, the scaled log-likelihood becomes l(w) = (1/N)\u2211i log p(yi|xi, w) = (1/N)\u2211i \u2211c wT\u03c6c(yi, xi) \u2212 log Z(w, xi), and the gradient becomes \u2202l/\u2202wc = (1/N)\u2211i [\u03c6c(yi, xi) \u2212 E[\u03c6c(y, xi)]]. Note that we now have to perform inference for each training case within each gradient step, which is O(N) times slower than the MRF case. However, this solution comes at a price: we do not get a valid probability distribution over y until we have seen the entire sentence, since only then can we normalize over all configurations."
      ]
    },
    {
      "topic": "Structural SVMs",
      "sub_topics": [
        "Structural Support Vector Machines (SSVMs) are methods for training structured output classifiers that leverage the existence of fast MAP solvers. They can be seen as minimizing the posterior expected loss on the training set. SSVMs are trained by leveraging fast MAP solvers; they minimize the posterior expected loss on the training set, considering the loss function during parameter estimation. The SSVM objective can be seen as optimizing an upper bound on the Bayesian objective; it focuses effort on fitting parameters that affect the decision boundary, making it computationally efficient.",
        "SSVMs are trained by minimizing an objective function that includes a regularization term and a loss term. The objective function can be constrained to ensure that the correct prediction has a higher score than all other predictions by at least a margin. The SSVM objective can be seen as optimizing an upper bound on the Bayesian objective. The resulting objective is the same as used in the SSVM approach and can be reduced to the standard binary SVM objective in the special case of binary classification. SSVMs, viewed non-probabilistically, aim to find a prediction function that obtains zero loss on the training set while maximizing the margin, leading to constraints that can be relaxed using slack variables.",
        "The training of SSVMs can be done using cutting plane methods, which iteratively add constraints to the objective function until a solution is found. The cutting plane method involves starting with an initial estimate of the weights and no constraints, and in each iteration, finding the most violated constraint and adding it to the working set. The cutting plane method is an efficient algorithm for fitting SSVMs that can handle general loss functions and is based on the cutting plane method from convex optimization; the key to the efficiency of this method is that only polynomially many constraints need to be added, and as soon as they are, the exponential number of other constraints are guaranteed to also be satisfied to within a tolerance of \u03b5. Cutting plane methods can be used for efficient fitting SSVMs, handling general loss functions and relying on loss-augmented decoding to find the most violated constraint at each iteration.",
        "Loss-augmented decoding is the ability to find the most violated constraint, i.e., to compute argmax L(yi, y) \u2212 wT \u03b4i(y) = argmax L(yi, y) + wT \u03a6(xi, y). Loss-augmented decoding is a key component of the cutting plane method, where the most violated constraint is found by maximizing a combination of the loss function and the model's score.",
        "Another approach to training SSVMs is to use stochastic subgradient methods, which update the weights iteratively based on the gradient of a single training example. The structured perceptron algorithm and Pegasos are examples of stochastic subgradient methods that can be used to train SSVMs. Online algorithms, such as the structured perceptron algorithm and stochastic subgradient descent, can be used for fitting SSVMs when dealing with large datasets.",
        "In many applications of interest, we have latent or hidden variables. To handle this, we can extend our model as follows, to get a latent CRF: p(y, h|x, w) = (1/Z(x, w)) exp(wT \u03a6(x, y, h)). These models can be trained using a variant of the convex-concave procedure (CCCP). Latent structural SVMs extend the model to include latent or hidden variables, which requires the use of the CCCP or concave-convex procedure for optimization due to the non-convexity of the objective."
      ]
    },
    {
      "topic": "Parameterization of MRFs",
      "sub_topics": [
        "In MRFs, as there is no topological ordering associated with an undirected graph, we cannot use the chain rule to represent p(y). Instead, we associate potential functions or factors with each maximal clique in the graph. The potential function for a clique c is denoted by \u03c8c(yc), and can be any non-negative function of its arguments. The joint distribution is then defined as proportional to the product of the clique potentials.",
        "The Hammersley-Clifford theorem guarantees that any positive distribution whose conditional independence properties can be represented by a UGM can be represented in this way. The partition function Z is used to normalize the product of the clique potentials, ensuring that the joint distribution sums to 1.",
        "The Gibbs distribution provides a connection between UGMs and statistical physics, where the probability of a state is proportional to exp(-E(y)), with E(y) being the energy associated with that state; high probability states correspond to low energy configurations.",
        "Potential functions can be represented as tables of non-negative numbers or as linear functions of the parameters. A common approach is to define the log potentials as a linear function of the parameters, where the potential is expressed as log\u03c8c(yc) = \u03c6c(yc)T\u03b8c, where \u03c6c(yc) is a feature vector derived from the values of the variables yc. This leads to maximum entropy or log-linear models. A more general approach defines log potentials as a linear function of parameters, log \u03c8c(yc) = \u03c6c(yc)T\u03b8c, where \u03c6c(yc) is a feature vector derived from the values of variables yc, also known as a maximum entropy or log-linear model.",
        "For a pairwise MRF, we can associate a feature vector of length K\u00b2 to each edge and convert this into a K \u00d7 K potential function. To model spelling in English, we can use indicator functions for trigrams of 'special' letters and define the potential on each trigram as an exponential function of these features, tying the parameters across locations to define the probability of a word of any length."
      ]
    },
    {
      "topic": "Examples of MRFs",
      "sub_topics": [
        "The Ising model is an example of an MRF that originated from statistical physics, used to model the behavior of magnets, where each atom has a 'spin' that can be up or down. The model creates a graph in the form of a 2D or 3D grid, connecting neighboring variables and defining a pairwise 'clique' potential based on the coupling strength between nodes. If all the weights are positive, neighboring 'spins' are likely to be in the same state, modeling ferromagnets and forming an 'associative Markov network'. If the weights are negative, the 'spins' will try to be different, modeling antiferromagnets and resulting in a 'frustrated system'.",
        "Hopfield networks are fully connected Ising models with a symmetric weight matrix, used as an 'associative memory' or 'content addressable memory'. They are trained on fully observed bit vectors, and the estimation of missing variables is called 'pattern completion'. The Boltzmann machine generalizes the Hopfield model by including hidden nodes, which make the model representationally more powerful.",
        "The Potts Model generalizes the Ising model to multiple discrete states, yt \u2208 {1, 2, ..., K}, with a potential function \u03c8st(ys, yt) = exp(J) if ys = yt, 0 otherwise; it is used as a prior for image segmentation, encouraging neighboring pixels to have the same label.",
        "Gaussian MRFs, also called Gaussian Graphical Models (GGMs), are pairwise MRFs of the form p(y|\u03b8) \u221d \u220f \u03c8st(ys, yt) \u220f \u03c8t(yt), with \u03c8st(ys, yt) = exp(-0.5 ys Ast yt) and \u03c8t(yt) = exp(-0.5 Atty\u00b2 + \u03b7t yt); zero entries in the precision matrix A (structural zeros) represent absent edges in the graph, corresponding to sparse precision matrices.",
        "The Ising model has an analogy to Gaussian graphical models, and its unnormalized log probability can be written in terms of a weight matrix W representing coupling strengths between nodes."
      ]
    },
    {
      "topic": "Learning in MRFs",
      "sub_topics": [
        "Maximum Entropy models can be trained using gradient methods on the scaled log-likelihood l(\u03b8) = (1/N) \u2211 log p(y_i|\u03b8) = (1/N) \u2211 (\u2211 \u03b8c \u03a6c(y_i) \u2212 log Z(\u03b8)), where c indexes the cliques; the gradient of the log likelihood is \u2202l/\u2202\u03b8c = (1/N) \u2211 (\u03a6c(y_i) \u2212 E[\u03a6c(y)]), with the first term being the clamped term and the second being the unclamped or contrastive term. At the optimum, the empirical distribution of the features will match the model's predictions; this is called moment matching, motivating optimization algorithms.",
        "Training MRFs involves maximizing the log-likelihood function, which is convex for exponential families; gradient-based optimizers are used to find the unique global maximum, with derivatives involving clamped and unclamped terms. The gradient of the log-likelihood can be expressed as the difference between the expected feature vector under the empirical distribution and the model's expectation; at the optimum, these expectations match, known as moment matching.",
        "For partially observed maxent models, the gradient involves clamping the visible nodes to observed values and marginalizing over hidden variables, which can be approximated using generalized EM with gradient methods in the M step. Training partially observed maxent models requires averaging over hidden variables in the gradient computation; generalized EM algorithms can be used, employing gradient methods in the M step.",
        "Approximate methods for MLEs of MRFs are necessary when inference is intractable; pseudo-likelihood maximizes the product of full conditionals, while stochastic maximum likelihood uses Monte Carlo sampling to approximate model expectations. Approximate methods for computing MLEs of MRFs are necessary when inference is intractable, motivating computationally faster alternatives such as pseudo-likelihood, which maximizes the product of full conditionals.",
        "Stochastic Maximum Likelihood (SML) can be used to approximate the model expectations using Monte Carlo sampling and combine this with stochastic gradient descent. Stochastic Maximum Likelihood (SML) approximates model expectations using Monte Carlo sampling combined with stochastic gradient descent, starting the MCMC chain at its previous value for efficiency.",
        "Pseudo-likelihood maximizes the product of the full conditionals, defined as CPL(\u03b8) = \u220f p(y_d|y_{\u2212d}); in Gaussian MRFs, PL is equivalent to ML, but this is not true in general.",
        "Iterative Proportional Fitting (IPF) is a fixed-point algorithm for enforcing moment matching constraints, guaranteed to converge to the global optimum; however, it may require many iterations and is computationally expensive. Iterative Proportional Fitting (IPF) is a fixed-point algorithm for enforcing moment matching constraints, guaranteed to converge to the global optimum, with speed depending on the form of the model and the use of efficient marginal computation methods."
      ]
    }
  ]
}