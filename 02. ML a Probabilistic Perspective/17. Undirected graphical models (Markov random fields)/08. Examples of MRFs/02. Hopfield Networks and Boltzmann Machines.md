## Hopfield Networks: Associative Memory in Markov Random Fields

### Introdução
Este capítulo explora as **Hopfield networks** como um exemplo específico de **Markov Random Fields (MRFs)**, focando em seu uso como *memória associativa* e sua generalização através da **Boltzmann machine** [^1]. As Hopfield networks são modelos de Ising totalmente conectados com uma matriz de pesos simétrica [^1]. Elas representam um caso particular de MRFs que podem ser expressos convenientemente como UGMs [^1].

### Conceitos Fundamentais

Uma **Hopfield network** é um modelo de Ising totalmente conectado com uma matriz de pesos simétrica, $W = W^T$ [^1]. Os nós representam spins de átomos que podem estar em um estado para cima (+1) ou para baixo (-1), ou seja, $y_s \in \{-1, +1\}$ [^1]. A energia do sistema é definida pelas conexões entre os nós e um possível campo externo [^1].

A aplicação principal das Hopfield networks é como **memória associativa** ou *content addressable memory* [^1]. A ideia é treinar a rede em um conjunto de vetores de bits totalmente observados, que correspondem aos padrões que queremos memorizar [^1]. No tempo de teste, apresentamos um padrão parcial à rede e gostaríamos de estimar as variáveis faltantes; isto é chamado de **pattern completion** [^1].

A energia da rede é definida como [^1]:
$$
logp(y) = -\sum_{s \sim t} y_s W_{st} y_t = -\frac{1}{2} y^T W y
$$
onde $W_{st}$ é a força de acoplamento entre os nós $s$ e $t$. Se dois nós não estão conectados no grafo, definimos $W_{st} = 0$ [^1]. Assumimos que a matriz de pesos $W$ é simétrica, então $W_{st} = W_{ts}$ [^1]. Frequentemente, assumimos que todas as arestas têm a mesma força, então $W_{st} = J$ (assumindo $W_{st} \ne 0$) [^1].

Se todos os pesos são positivos, $J > 0$, então os spins vizinhos provavelmente estarão no mesmo estado; isto pode ser usado para modelar ferromagnetos e é um exemplo de uma **rede de Markov associativa** [^1]. Se os pesos são suficientemente fortes, a distribuição de probabilidade correspondente terá dois modos, correspondendo ao estado de todos +1 e todos -1 [^1]. Estes são chamados os **ground states** do sistema [^1].

Se todos os pesos são negativos, $J < 0$, então os spins querem ser diferentes de seus vizinhos; isto pode ser usado para modelar um antiferromagneto e resulta em um **frustrated system**, no qual nem todas as restrições podem ser satisfeitas ao mesmo tempo [^1]. A distribuição de probabilidade correspondente terá múltiplos modos [^1].

A inferência exata é intratável neste modelo, então é padrão usar um algoritmo de descida de coordenadas conhecido como **iterative conditional modes (ICM)**, que simplesmente define cada nó para seu estado mais provável (menor energia), dado todos os seus vizinhos [^1]. A condicional completa pode ser mostrada como [^1]:
$$
p(y_s = 1 | y_{-s}, \theta) = \text{sigm} (\sum_t W_{st} y_t + b_s)
$$
Escolher o estado mais provável equivale a usar a regra $y_s = 1$ se $\sum_t W_{st} y_t > b_s$ e $y_s = 0$ caso contrário [^1].

Como a inferência é determinística, também é possível interpretar este modelo como uma **recurrent neural network** [^1]. Uma **Boltzmann machine** generaliza o modelo Hopfield/Ising incluindo alguns nós ocultos, o que torna o modelo representacionalmente mais poderoso [^1]. A inferência em tais modelos frequentemente usa Gibbs sampling, que é uma versão estocástica do ICM [^1].

### Conclusão
As Hopfield networks oferecem um exemplo fundamental de como os MRFs podem ser aplicados para criar memórias associativas [^1]. Suas propriedades, como a convergência para estados de baixa energia e a capacidade de completar padrões, demonstram o poder dos modelos gráficos não direcionados na modelagem de sistemas complexos [^1]. A generalização para Boltzmann machines introduz a capacidade de modelar dependências mais complexas através de nós ocultos, expandindo ainda mais o potencial das MRFs [^1].

### Referências
[^1]: Seção 19.4.2 do texto fornecido.
<!-- END -->