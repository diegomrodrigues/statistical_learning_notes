## Treinamento de MRFs e Otimização da Log-Verossimilhança

### Introdução
Este capítulo explora o processo de treinamento de Markov Random Fields (MRFs), com foco na maximização da função de log-verossimilhança. O treinamento de MRFs envolve encontrar os parâmetros do modelo que melhor se ajustam aos dados observados. A função de log-verossimilhança, que mede o quão bem o modelo explica os dados, desempenha um papel central nesse processo. Dada a importância de MRFs em diversos domínios, como modelagem de imagens e análise espacial [^1], compreender os métodos de treinamento associados é crucial.

### Conceitos Fundamentais

#### Função de Log-Verossimilhança e Convexidade

O treinamento de MRFs frequentemente envolve a maximização da função de log-verossimilhança [^referência da introdução]. Para famílias exponenciais, essa função é convexa, o que significa que existe um único máximo global [^referência da introdução]. Essa propriedade é fundamental, pois garante que os otimizadores baseados em gradiente convergirão para a solução ótima. A função de log-verossimilhança para um MRF em forma log-linear é dada por [^19.37]:

$$
p(y|\theta) = \frac{1}{Z(\theta)} \exp \left( \sum_c \theta_c^T \phi_c(y) \right)
$$

onde $y$ representa a configuração das variáveis, $\theta$ são os parâmetros do modelo, $\phi_c(y)$ são as funções de características para o clique $c$, e $Z(\theta)$ é a função de partição.

#### Otimização Baseada em Gradiente

Otimizadores baseados em gradiente são comumente empregados para encontrar o máximo global da função de log-verossimilhança [^referência da introdução]. Esses otimizadores iterativamente ajustam os parâmetros do modelo na direção do gradiente, que indica a direção de maior aumento na log-verossimilhança. O gradiente da função de log-verossimilhança envolve termos *clamped* e *unclamped* [^referência da introdução]. O termo *clamped* refere-se à expectativa das funções de características sob a distribuição empírica dos dados, enquanto o termo *unclamped* refere-se à expectativa sob a distribuição do modelo.

A derivada da log-verossimilhança em relação aos pesos de um clique específico, $c$, é dada por [^19.39]:

$$
\frac{\partial l}{\partial \theta_c} = \frac{1}{N} \sum_i \phi_c(y_i) - \frac{\partial}{\partial \theta_c} \log Z(\theta)
$$

onde $l$ é a log-verossimilhança, $N$ é o número de amostras, e $y_i$ é a $i$-ésima amostra.

A derivada do log da função de partição é a expectativa da função de característica sob o modelo [^19.40]:
$$
\frac{\partial \log Z(\theta)}{\partial \theta_c} = \mathbb{E}_{\theta} [\phi_c(y)] = \sum_y \phi_c(y) p(y|\theta)
$$

Portanto, o gradiente da log-verossimilhança pode ser expresso como [^19.41]:

$$
\frac{\partial l}{\partial \theta_c} = \frac{1}{N} \sum_i \phi_c(y_i) - \mathbb{E}_{\theta} [\phi_c(y)]
$$

O primeiro termo é a expectativa das características sob a distribuição empírica, enquanto o segundo termo é a expectativa sob o modelo.

#### Correspondência de Momentos (Moment Matching)

No ótimo, o gradiente da função de log-verossimilhança é zero, o que implica que as expectativas das funções de características sob a distribuição empírica e a distribuição do modelo coincidem [^referência da introdução]. Esse princípio é conhecido como *moment matching*. Em outras palavras, o modelo aprende a reproduzir os momentos estatísticos dos dados observados.

No ótimo, o gradiente será zero [^19.43]:

$$
\mathbb{E}_{p_{emp}} [\phi_c(y)] = \mathbb{E}_{p(y|\theta)} [\phi_c(y)]
$$

Essa observação motiva diferentes algoritmos de otimização, como discutido na Seção 19.5.7.

#### Treinamento com Dados Parcialmente Observados

Quando os dados estão incompletos ou há variáveis ocultas, o treinamento torna-se mais desafiador. Nesses casos, a função de log-verossimilhança envolve a marginalização sobre as variáveis ausentes ou ocultas [^19.44]:

$$
p(y, h|\theta) = \frac{1}{Z(\theta)} \exp \left( \sum_c \theta_c^T \phi_c(h, y) \right)
$$

onde $h$ representa as variáveis ocultas.

O gradiente da log-verossimilhança para dados parcialmente observados é dado por [^19.48]:

$$
\frac{\partial l}{\partial \theta_c} = \frac{1}{N} \sum_i \left\{ \mathbb{E} [\phi_c(h, y_i)|\theta] - \mathbb{E} [\phi_c(h, y)|\theta] \right\}
$$

O primeiro conjunto de expectativas é computado "fixando" os nós visíveis em seus valores observados, enquanto o segundo conjunto é computado deixando os nós visíveis livres. Em ambos os casos, marginalizamos sobre $h_i$ [^19.48].

Um método alternativo é usar EM generalizado, onde usamos métodos de gradiente no passo M. Veja (Koller e Friedman 2009, p956) para detalhes [^19.48].

#### Métodos Aproximados para Computar MLEs

Quando se ajusta um UGM, geralmente não há solução de forma fechada para a estimativa ML ou MAP dos parâmetros, então precisamos usar otimizadores baseados em gradiente. Esse gradiente requer inferência. Em modelos onde a inferência é intratável, o aprendizado também se torna intratável. Isso motivou várias alternativas computacionalmente mais rápidas para a estimativa ML/MAP, que listamos na Tabela 19.1 [^19.48].

#### Pseudo-verossimilhança

Uma alternativa para MLE é maximizar a pseudo-verossimilhança [^19.5.4] (Besag 1975), definida como segue:

$$
\ell_{CPL}(\theta) \triangleq \sum_{y} p_{emp}(y) \sum_{d=1}^D \log p(y_d | y_{-d}) = \frac{1}{N} \sum_{i=1}^N \sum_{d=1}^D \log p(y_{id} | y_{i, -d}, \theta)
$$

Ou seja, otimizamos o produto das condicionais completas, também conhecido como a *verossimilhança composta* [^19.49] (Lindsay 1988).

### Conclusão

O treinamento de MRFs é um processo fundamental para a construção de modelos probabilísticos eficazes. A maximização da função de log-verossimilhança, frequentemente realizada por meio de otimizadores baseados em gradiente, garante que o modelo aprenda a capturar os padrões estatísticos nos dados observados. O princípio de *moment matching* fornece uma interpretação valiosa desse processo, revelando que o modelo aprende a reproduzir os momentos dos dados. Embora o treinamento possa se tornar mais complexo com dados incompletos ou variáveis ocultas, os princípios subjacentes e as técnicas de otimização permanecem essenciais.

### Referências
[^1]: Capítulo 19 do livro texto.
[^19.37]: Koller, D., & Friedman, N. (2009). *Probabilistic graphical models: principles and techniques*. MIT press.
[^19.39]: Koller, D., & Friedman, N. (2009). *Probabilistic graphical models: principles and techniques*. MIT press.
[^19.40]: Koller, D., & Friedman, N. (2009). *Probabilistic graphical models: principles and techniques*. MIT press.
[^19.41]: Koller, D., & Friedman, N. (2009). *Probabilistic graphical models: principles and techniques*. MIT press.
[^19.43]: Koller, D., & Friedman, N. (2009). *Probabilistic graphical models: principles and techniques*. MIT press.
[^19.44]: Koller, D., & Friedman, N. (2009). *Probabilistic graphical models: principles and techniques*. MIT press.
[^19.48]: Koller, D., & Friedman, N. (2009). *Probabilistic graphical models: principles and techniques*. MIT press.
[^19.49]: Lindsay, B. G. (1988). Composite likelihood methods. *Contemporary Mathematics*, *80*, 221-239.
[^19.5.4]: Besag, J. (1975). Statistical analysis of non-lattice data. *The Statistician*, 179-195.

<!-- END -->