## Ising Model and Gaussian Graphical Models: An Analogy
### Introdução
Este capítulo explora a conexão entre o **Ising Model** e os **Gaussian Graphical Models (GGMs)**, com ênfase nas propriedades e aplicações do Ising Model, notadamente em **redes de Hopfield**. O Ising Model, originário da física estatística, fornece uma estrutura rica para modelar interações entre variáveis discretas, enquanto os GGMs oferecem uma abordagem complementar usando distribuições Gaussianas para representar dependências condicionais. A analogia entre esses modelos permite uma melhor compreensão de suas propriedades e aplicações, particularmente em problemas de memória associativa [^668].

### Conceitos Fundamentais

#### Ising Model
O Ising Model é um exemplo de **Markov Random Field (MRF)** que surgiu da física estatística [^668]. Originalmente, ele foi usado para modelar o comportamento de materiais magnéticos. No modelo, cada átomo tem um *spin* que pode estar em um de dois estados: para cima (+1) ou para baixo (-1). Formalmente, seja $y_t \in \{-1, +1\}$ representando o spin de um átomo, que pode estar para cima ou para baixo [^668]. Em alguns materiais, chamados ferromagnetos, os spins vizinhos tendem a se alinhar na mesma direção, enquanto em outros, chamados antiferromagnetos, eles tendem a se diferenciar [^668].

Podemos modelar esse sistema como um MRF. Criamos um grafo na forma de uma grade 2D ou 3D e conectamos variáveis vizinhas [^668]. Definimos então o seguinte potencial de clique *pairwise*:

$$ \Psi_{st}(y_s, y_t) = \begin{pmatrix} e^{w_{st}} & e^{-w_{st}} \\ e^{-w_{st}} & e^{w_{st}} \end{pmatrix} $$

Aqui, $w_{st}$ é a força de acoplamento entre os nós *s* e *t*. Se dois nós não estão conectados no grafo, definimos $w_{st} = 0$. Assumimos que a matriz de peso *W* é simétrica, de modo que $w_{st} = w_{ts}$ [^668]. Frequentemente, assumimos que todas as arestas têm a mesma força, de modo que $w_{st} = J$ (assumindo $w_{st} \neq 0$) [^668].

Se todos os pesos são positivos, $J > 0$, então os spins vizinhos provavelmente estarão no mesmo estado; isso pode ser usado para modelar ferromagnetos e é um exemplo de uma **rede de Markov associativa**. Se os pesos forem suficientemente fortes, a distribuição de probabilidade correspondente terá dois modos, correspondendo ao estado de todos os +1 e ao estado de todos os -1. Estes são chamados os **estados fundamentais** do sistema [^668].

Se todos os pesos são negativos, $J < 0$, então os spins querem ser diferentes de seus vizinhos; isso pode ser usado para modelar um antiferromagneto e resulta em um sistema frustrado, no qual nem todas as restrições podem ser satisfeitas ao mesmo tempo. A distribuição de probabilidade correspondente terá múltiplos modos. Curiosamente, computar a função de partição *Z(J)* pode ser feito em tempo polinomial para redes de Markov associativas, mas é NP-difícil em geral (Cipra 2000) [^668].

#### Analogia com Gaussian Graphical Models
Existe uma analogia interessante entre os Ising Models e os Gaussian Graphical Models [^668]. Primeiro, assumindo $y_t \in \{-1,+1\}$, podemos escrever a probabilidade logarítmica não normalizada de um Ising Model como:

$$ \log p(y) = -\frac{1}{2} \sum_{s \sim t} y_s W_{st} y_t $$

onde a minimização da energia corresponde a estados de alta probabilidade se os vizinhos concordarem [^668].

#### Redes de Hopfield
**Redes de Hopfield** são Ising Models totalmente conectados com uma matriz de peso simétrica, $W = W^T$ [^669]. Esses pesos, mais os termos de *bias* *b*, podem ser aprendidos a partir de dados de treinamento usando (aproximação) da máxima verossimilhança, como descrito na Seção 19.5 [^669]. A principal aplicação das Redes de Hopfield é como uma **memória associativa** ou **memória endereçável por conteúdo** [^669].

A ideia é a seguinte: suponha que treinamos em um conjunto de vetores de bits totalmente observados, correspondendo a padrões que queremos memorizar. Então, no tempo de teste, apresentamos um padrão parcial para a rede [^669]. Gostaríamos de estimar as variáveis ausentes; isso é chamado **completamento de padrão**. Veja a Figura 19.7 para um exemplo. Isso pode ser pensado como recuperar um exemplo da memória com base em uma parte do próprio exemplo, daí o termo "memória associativa" [^669].

Como a inferência exata é intratável neste modelo, é padrão usar um algoritmo de descida de coordenadas conhecido como **modos condicionais iterativos (ICM)**, que apenas define cada nó para seu estado mais provável (menor energia), dados todos os seus vizinhos. A condicional completa pode ser mostrada como sendo:

$$ p(y_s = 1 | y_{-s}, \theta) = \text{sigm} \left( \sum_t w_{st} y_t + b_s \right) $$

Escolher o estado mais provável equivale a usar a regra $y_s = 1$ se $\sum_t w_{st} y_t > b_s$ e usar $y_s = 0$ caso contrário [^669]. Como a inferência é determinística, também é possível interpretar este modelo como uma **rede neural recorrente** [^669]. Uma **Boltzmann Machine** generaliza o modelo de Hopfield/Ising incluindo alguns nós ocultos, o que torna o modelo representacionalmente mais poderoso [^669]. A inferência em tais modelos frequentemente usa Gibbs sampling, que é uma versão estocástica do ICM (veja a Seção 24.2 para detalhes) [^669].

### Conclusão
A analogia entre o Ising Model e os Gaussian Graphical Models oferece uma perspectiva valiosa sobre as propriedades e aplicações desses modelos. O Ising Model, particularmente em redes de Hopfield, demonstra a capacidade de modelar interações complexas entre variáveis discretas e realizar tarefas de memória associativa. As redes de Hopfield, sendo modelos de Ising totalmente conectados, exemplificam como a estrutura do modelo e os algoritmos de inferência podem ser adaptados para resolver problemas específicos [^669]. A introdução de nós ocultos em máquinas de Boltzmann expande ainda mais a capacidade representacional do modelo, destacando a flexibilidade e o potencial do Ising Model em uma variedade de aplicações [^669].

### Referências
[^668]: Chapter 19. Undirected graphical models (Markov random fields)
[^669]: Chapter 19. Undirected graphical models (Markov random fields)

<!-- END -->