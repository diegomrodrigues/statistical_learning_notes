{
  "topics": [
    {
      "topic": "Variational Inference",
      "sub_topics": [
        "Variational inference is a deterministic approximate inference technique used to estimate intractable posterior distributions by selecting a tractable family of distributions, q(x), and finding the member of that family that is closest to the true posterior, p*(x) = p(x|D), effectively transforming inference into an optimization problem. The core idea involves iteratively refining q(x) to closely resemble p*(x), where D is the observed data, by minimizing a cost function such as the Kullback-Leibler (KL) divergence, which measures the dissimilarity between two probability distributions. This often involves relaxing constraints or approximating the objective function to balance accuracy and computational speed, offering a trade-off between accuracy and speed, providing faster estimation akin to MAP estimation while retaining statistical benefits from the Bayesian approach.",
        "The Kullback-Leibler (KL) divergence is used as a cost function to measure the similarity between the approximation q(x) and the true posterior p*(x), with two common approaches being minimizing KL(p*||q) (forward KL) and KL(q||p*) (reverse KL), each leading to different approximation characteristics. Minimizing KL divergence KL(p*||q) is computationally challenging because it requires expectations with respect to the intractable true posterior p*(x); a more tractable alternative is to minimize the reverse KL divergence KL(q||p*), which involves expectations with respect to the tractable approximation q(x).",
        "The objective function J(q) is defined as KL(q||p), where p is an unnormalized distribution such that p(x) = p(x, D) = p*(x)Z, with Z being the intractable normalization constant p(D). Minimizing J(q) forces q to become close to p*, providing an upper bound on the negative log likelihood (NLL). Alternative interpretations of the variational objective include viewing it as minimizing the variational free energy (Helmholtz free energy) in statistical physics, which is the expected energy minus the entropy of the system, or as minimizing the expected NLL plus a penalty term that measures how far the approximate posterior is from the exact prior, connecting it to information theory and the bits-back argument.",
        "Minimizing KL(q||p) (reverse KL or I-projection) and KL(p||q) (forward KL or M-projection) lead to different behaviors; reverse KL is zero forcing, underestimating the support of p, while forward KL is zero avoiding, overestimating the support of p, which is critical in scenarios like multimodal distributions. Alpha divergence provides a family of divergence measures indexed by a parameter \u03b1, where KL(p||q) corresponds to \u03b1 \u2192 1 and KL(q||p) corresponds to \u03b1 \u2192 -1, offering a spectrum of behaviors between the two KL divergences; the Hellinger distance is a symmetric divergence measure related to the alpha divergence when \u03b1 = 0.",
        "The mean field method, a popular form of variational inference, assumes that the posterior is a fully factorized approximation, q(x) = \u220fqi(xi), transforming the optimization problem into iteratively updating each marginal distribution qi while holding the others fixed, using a coordinate descent method, where log qj(xj) = E\u2072qj[log p(x)] + const. Variational Bayes (VB) extends variational inference to infer the parameters themselves; a fully factorized (i.e., mean field) approximation, p(\u03b8|D) \u2248 \u220fk q(\u03b8k), results in a method known as variational Bayes."
      ]
    },
    {
      "topic": "Structured Mean Field",
      "sub_topics": [
        "Structured mean field methods extend the basic mean field approach by exploiting tractable substructure in the problem, grouping sets of variables together and updating them simultaneously to handle dependencies more efficiently. This approach addresses the strong independence assumption of the basic mean field method. In structured mean field, variables are grouped into 'mega-variables,' and the derivation process from basic mean field is repeated, which can be tractable overall as long as efficient inference can be performed within each group.",
        "An example of structured mean field is applied to a factorial Hidden Markov Model (HMM), where multiple chains are a priori independent but become coupled in the posterior due to having an observed common child. In the factorial HMM, the structured mean field approach involves approximating the posterior as a product of chains, where each chain is tractably updated individually using the forwards-backwards algorithm, resulting in an update cost of O(TMK^2I), where I is the number of mean field iterations. This is more efficient than the junction tree algorithm, which takes O(TMK^(M+1)) time.",
        "In structured mean field for factorial HMM, the parameters (\u03b2tmk) act as approximate local evidence, averaging out the effects of the other chains, contrasting with the exact local evidence that couples all the chains together. Mean field methods often use a coordinate descent approach to optimize the parameters of each marginal distribution, where at each step the algorithm updates one distribution while holding the others fixed, iterating until convergence."
      ]
    },
    {
      "topic": "Variational Bayes",
      "sub_topics": [
        "Variational Bayes (VB) is a method used when model parameters are unknown, applying a fully factorized approximation p(\u03b8|D) \u2248 \u220fk q(\u03b8k) to infer the parameters themselves, and is also known as ensemble learning, resulting in a method where we compute a posterior distribution over the parameters instead of point estimates. The variational posterior is determined by the form of the likelihood and prior, and the optimal form is obtained by looking at the complete data log joint, ignoring terms that do not involve z, and taking expectations of what's left over with respect to all the hidden variables except for z.",
        "Variational Bayes EM (VBEM) is used for latent variable models, approximating both latent variables and parameters, p(\u03b8, z1:N|D) \u2248 q(\u03b8) \u220fi qi(zi), combining variational Bayes with the Expectation-Maximization (EM) algorithm to model uncertainty in both latent variables and parameters. In VBEM, the algorithm alternates between a variational E step, updating q(zi|D), and a variational M step, updating q(\u03b8|D); standard EM can be recovered from VBEM by approximating the parameter posterior using a delta function.",
        "In VBEM, the variational E step involves averaging over the parameters instead of plugging in a MAP estimate, while the variational M step updates the hyper-parameters using expected sufficient statistics, enabling computation of a lower bound on the marginal likelihood for model selection. A key advantage of VBEM over regular EM is that by marginalizing out the parameters, one can compute a lower bound on the marginal likelihood, which can be used for model selection, and treats parameters as 'first class citizens', removing the artificial distinction between parameters and latent variables.",
        "In VB for a univariate Gaussian, we apply VB to infer the posterior over the parameters \u03bc and \u03bb (precision) using a conjugate prior and an approximate factored posterior, q(\u03bc, \u03bb) = q\u03bc(\u03bc)q\u03bb(\u03bb), and derive update equations for the distributions q\u03bc(\u03bc) and q\u03bb(\u03bb). The lower bound L(q) in VB is maximized, providing a lower bound on the log marginal likelihood, and is useful for assessing convergence, correctness, and Bayesian model selection."
      ]
    },
    {
      "topic": "Variational Message Passing and VIBES",
      "sub_topics": [
        "Variational Message Passing (VMP) is a method similar to Gibbs sampling, where each node\u2019s full conditional is computed, and neighbors are averaged out, using a general-purpose set of update equations applicable to any Directed Graphical Model (DGM) with exponential family CPDs and conjugate priors. VIBES (Variational Inference for Bayesian Networks with Exponential familyS) is an open-source program that implements VMP, serving as a Variational Bayes analog to BUGS, a popular generic program for Gibbs sampling.",
        "VMP/mean field is best-suited for inference where one or more hidden nodes are continuous (e.g., Bayesian learning), while more accurate approximate inference algorithms are used for models where all hidden nodes are discrete."
      ]
    },
    {
      "topic": "Local Variational Bounds",
      "sub_topics": [
        "Local variational bounds involve replacing a specific term in the joint distribution with a simpler function to simplify computation of the posterior, contrasting with mean field inference, which is a global approximation. Local variational approximation replaces a specific term in the joint distribution with a simpler function to simplify computation of the posterior, unlike mean field, which is a global approximation.",
        "One application of local variational bounds is in variational logistic regression, where the likelihood function is not conjugate to the Gaussian prior, and 'Gaussian-like' lower bounds are computed to the likelihood, giving rise to approximate Gaussian posteriors. Local variational bounds are used in applications such as variational logistic regression to approximate the parameter posterior for multiclass logistic regression under a Gaussian prior by monotonically optimizing a lower bound on the likelihood of the data.",
        "Bohning's quadratic bound provides a way to derive a 'Gaussian-like' lower bound on the log-sum-exp (lse) term, which is used in the softmax likelihood, by considering a Taylor series expansion of the lse function and replacing the Hessian matrix with a matrix that satisfies certain conditions. Bohning\u2019s quadratic bound provides a Gaussian-like lower bound on the log-sum-exp function, enabling the derivation of a \u201cGaussianized\u201d version of the observation model for multinomial logistic regression.",
        "Jensen's inequality and the multivariate delta method are alternative approaches to compute bounds and approximations to the log-sum-exp function; the multivariate delta method approximates moments of a function using a Taylor series expansion. In some cases, an upper bound is needed instead of a lower bound, such as in deriving a mean field algorithm for sigmoid belief nets, where an upper bound on the sigmoid function is derived to make the update tractable and result in a monotonically convergent inference procedure."
      ]
    },
    {
      "topic": "Alternative Interpretations of the Variational Objective",
      "sub_topics": [
        "The variational objective function, denoted as J(q), can be expressed as the expected energy minus the entropy of the system, offering a connection to statistical physics where J(q) corresponds to the variational free energy or Helmholtz free energy. From an information theory perspective, the variational objective can be interpreted using the bits-back argument, providing insights into how the approximate posterior captures information about the true posterior.",
        "The variational objective can be decomposed into the expected Negative Log-Likelihood (NLL) plus a Kullback-Leibler (KL) divergence term, which penalizes the approximate posterior for deviating from the exact prior, balancing model fit and complexity.",
        "Forward KL divergence, also known as M-projection or moment projection, minimizes KL(p||q) and tends to overestimate the support of the true distribution p, ensuring that q(x) > 0 where p(x) > 0. Reverse KL divergence, also known as I-projection or information projection, minimizes KL(q||p) and tends to underestimate the support of the true distribution p, effectively zero-forcing q to be zero where p is zero.",
        "Alpha divergence provides a family of divergence measures indexed by a parameter \u03b1, where KL(p||q) corresponds to the limit as \u03b1 approaches 1, and KL(q||p) corresponds to the limit as \u03b1 approaches -1, offering a flexible framework for controlling the behavior of the approximation."
      ]
    }
  ]
}