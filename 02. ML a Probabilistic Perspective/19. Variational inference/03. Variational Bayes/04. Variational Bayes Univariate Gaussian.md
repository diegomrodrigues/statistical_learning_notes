## Variational Bayes para uma Gaussiana Univariada

### Introdução
Este capítulo aprofunda o conceito de Variational Bayes (VB), aplicando-o a um modelo específico: uma Gaussiana univariada. O objetivo é inferir a distribuição posterior sobre os parâmetros μ (média) e λ (precisão) de uma Gaussiana, utilizando uma *prior* conjugada e uma posterior aproximada fatorada. A maximização do *lower bound* L(q) em VB fornece um limite inferior para o log da verossimilhança marginal, útil para avaliar a convergência, correção e seleção de modelos Bayesianos [^1]. Este capítulo se baseia nos conceitos de inferência variacional introduzidos anteriormente [^1, 2, 3, 4, 5, 6], explorando uma aplicação prática e detalhada.

### Conceitos Fundamentais

#### Formulação do Problema
Considere o problema de inferir os parâmetros μ e λ de uma distribuição Gaussiana univariada, dados alguns dados $D = \\{x_1, ..., x_N\\}$. A precisão λ é definida como o inverso da variância, ou seja, $\lambda = 1/\sigma^2$ [^42]. Para conveniência, utilizamos uma *prior* conjugada da forma [^21.65]:

$$p(\mu, \lambda) = \mathcal{N}(\mu|\mu_0, (\kappa_0\lambda)^{-1})Ga(\lambda|a_0, b_0)$$

onde $\mathcal{N}$ denota a distribuição Normal (Gaussiana) e $Ga$ denota a distribuição Gamma, e $\mu_0$, $\kappa_0$, $a_0$ e $b_0$ são hiperparâmetros da *prior* [^21.65].

#### Aproximação Fatorada
Em VB, aproximamos a distribuição posterior conjunta $p(\mu, \lambda|D)$ por uma distribuição fatorada $q(\mu, \lambda)$ [^1]:

$$q(\mu, \lambda) = q_\mu(\mu)q_\lambda(\lambda)$$

Esta aproximação simplifica o problema de inferência, permitindo-nos otimizar $q_\mu(\mu)$ e $q_\lambda(\lambda)$ iterativamente [^21.66].

#### Derivação das Equações de Update
O objetivo do VB é maximizar o *lower bound* L(q), que é uma aproximação do log da verossimilhança marginal. Para uma Gaussiana univariada, as equações de *update* para $q_\mu(\mu)$ e $q_\lambda(\lambda)$ são derivadas maximizando $L(q)$ em relação a cada uma das distribuições, mantendo a outra fixa [^21.69, 21.72].

##### Update de $q_\mu(\mu)$
A forma ótima para $q_\mu(\mu)$ é obtida integrando sobre λ [^21.69]:

$$log \\ q_\mu(\mu) = \mathbb{E}_{q_\lambda} [log \\ p(D|\mu, \lambda) + log \\ p(\mu|\lambda)] + const$$

Após completar o quadrado, podemos mostrar que $q_\mu(\mu)$ é uma Gaussiana [^21.70]:

$$q_\mu(\mu) = \mathcal{N}(\mu|\mu_N, \kappa_N^{-1})$$

onde

$$\mu_N = \frac{\kappa_0 \mu_0 + N\bar{x}}{\kappa_0 + N}$$
$$\kappa_N = (\kappa_0 + N)\mathbb{E}_{q_\lambda}[\lambda]$$

Note que $\mathbb{E}_{q_\lambda}[\lambda]$ ainda não é conhecido neste ponto, pois depende da forma de $q_\lambda(\lambda)$ [^21.71].

##### Update de $q_\lambda(\lambda)$
Similarmente, a forma ótima para $q_\lambda(\lambda)$ é obtida integrando sobre μ [^21.72]:

$$log \\ q_\lambda(\lambda) = \mathbb{E}_{q_\mu} [log \\ p(D|\mu, \lambda) + log \\ p(\mu|\lambda) + log \\ p(\lambda)] + const$$

Após simplificação, podemos mostrar que $q_\lambda(\lambda)$ é uma distribuição Gamma [^21.73]:

$$q_\lambda(\lambda) = Ga(\lambda|a_N, b_N)$$

onde

$$a_N = a_0 + \frac{N+1}{2}$$
$$b_N = b_0 + \frac{1}{2}\mathbb{E}_{q_\mu} [\kappa_0 (\mu - \mu_0)^2 + \sum_{i=1}^{N} (x_i - \mu)^2]$$

#### Computando as Expectativas
Para implementar as equações de *update*, precisamos computar as expectativas $\mathbb{E}_{q_\lambda}[\lambda]$ e $\mathbb{E}_{q_\mu} [\kappa_0 (\mu - \mu_0)^2 + \sum_{i=1}^{N} (x_i - \mu)^2]$. Usando as propriedades das distribuições Normal e Gamma, temos [^21.76, 21.77, 21.78]:

$$\mathbb{E}_{q_\mu}[\mu] = \mu_N$$
$$\mathbb{E}_{q_\mu}[\mu^2] = \frac{1}{\kappa_N} + \mu_N^2$$
$$\mathbb{E}_{q_\lambda}[\lambda] = \frac{a_N}{b_N}$$

Substituindo estas expressões nas equações de *update*, obtemos um algoritmo iterativo onde atualizamos $a_N$, $b_N$, $\mu_N$ e $\kappa_N$ até a convergência [^21.79, 21.80, 21.81, 21.82].

#### Lower Bound
O *lower bound* $L(q)$ é uma medida da qualidade da aproximação variacional [^1]. Ele pode ser expresso como [^21.83, 21.84, 21.85]:

$$L(q) = \int \int q(\mu, \lambda) log \frac{p(D, \mu, \lambda)}{q(\mu, \lambda)} d\mu d\lambda$$

Expandindo esta expressão, obtemos [^21.86, 21.87]:

$$L(q) = \mathbb{E} [log \\ p(D|\mu, \lambda)] + \mathbb{E} [log \\ p(\mu|\lambda)] + \mathbb{E} [log \\ p(\lambda)] - \mathbb{E} [log \\ q(\mu)] - \mathbb{E} [log \\ q(\lambda)]$$

onde as expectativas são calculadas em relação a $q(\mu, \lambda)$. Computar o *lower bound* é útil para verificar a convergência do algoritmo e para seleção de modelos [^21.83].

### Conclusão
Este capítulo demonstrou a aplicação do Variational Bayes para inferir os parâmetros de uma Gaussiana univariada. Através da aproximação fatorada e da maximização do *lower bound*, derivamos equações de *update* iterativas para as distribuições $q_\mu(\mu)$ e $q_\lambda(\lambda)$. A computação do *lower bound* permite avaliar a qualidade da aproximação variacional e verificar a convergência do algoritmo [^1]. Este exemplo ilustra a metodologia geral do VB, que pode ser aplicada a uma variedade de modelos estatísticos [^1].

### Referências
[^1]: Capítulo 21: Variational Inference
[^2]: Seção 21.5.1: Example: VB for a univariate Gaussian
[^3]: Seção 21.5.1.1: Target distribution
[^4]: Seção 21.5.1.2: Updating qμ(μ)
[^5]: Seção 21.5.1.3: Updating qλ(λ)
[^6]: Seção 21.5.1.4: Computing the expectations
[^7]: Seção 21.5.1.6: Lower bound
<!-- END -->