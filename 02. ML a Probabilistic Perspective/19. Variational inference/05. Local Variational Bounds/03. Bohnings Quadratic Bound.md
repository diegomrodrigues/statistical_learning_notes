## Bohning's Quadratic Bound for Log-Sum-Exp in Local Variational Bounds

### Introdução
Este capítulo explora o uso do **Bohning's quadratic bound** como uma técnica para aproximar a função **log-sum-exp (lse)**, que é fundamental na **softmax likelihood** [^758]. A função lse surge em diversos modelos estatísticos, especialmente em regressão logística multinomial, e sua aproximação precisa é crucial para inferência variacional eficiente. O contexto geral deste capítulo se insere dentro do tema mais amplo de **local variational bounds**, onde termos específicos na distribuição conjunta são substituídos por funções mais simples para facilitar os cálculos [^756].

### Conceitos Fundamentais

O principal desafio ao lidar com a **softmax likelihood** é a presença da função **log-sum-exp (lse)**, definida como [^757]:
$$lse(\eta) = \log\left(1 + \sum_{m=1}^{M} e^{\eta_m}\right)$$

Essa função, embora fundamental, não é conjugada com priors Gaussianos, o que dificulta a obtenção de soluções analíticas para a inferência variacional. Para contornar essa dificuldade, o **Bohning's quadratic bound** oferece uma maneira de derivar um limite inferior "Gaussian-like" para a função lse [^758].

A ideia central é expandir a função lse em uma série de Taylor em torno de um ponto $\psi_i \in \mathbb{R}^M$:

$$lse(\eta_i) \approx lse(\psi_i) + (\eta_i - \psi_i)^T g(\psi_i) + \frac{1}{2} (\eta_i - \psi_i)^T H(\psi_i) (\eta_i - \psi_i)$$

onde $g$ e $H$ são o gradiente e o Hessiano de $lse$, respectivamente [^758]:

$$g(\psi_i) = \exp[\psi_i - lse(\psi_i)] = S(\psi_i)$$
$$H(\psi_i) = diag(g(\psi_i)) - g(\psi_i)g(\psi_i)^T$$

O gradiente $g(\psi_i)$ representa o **softmax function** [^759]. O Hessiano $H(\psi_i)$ é então substituído por uma matriz $A_i$ que satisfaz a condição $A_i \preceq H(\psi_i)$. **Bohning (1992)** mostrou que essa condição pode ser satisfeita usando a matriz [^758]:

$$A_i = \frac{1}{2} \left[I_M - \frac{1}{M+1} \mathbb{1}_M \mathbb{1}_M^T\right]$$

onde $\mathbb{1}_M$ é um vetor de uns de tamanho $M$ [^758]. Essa escolha resulta em um limite inferior quadrático para a função lse:

$$lse(\eta_i) \leq \frac{1}{2} \eta_i^T A_i \eta_i - b_i^T \eta_i + c_i$$

onde os termos $b_i$ e $c_i$ são definidos como [^758]:

$$b_i = A_i \psi_i - g(\psi_i)$$
$$c_i = \frac{1}{2} \psi_i^T A_i \psi_i - g(\psi_i)^T \psi_i + lse(\psi_i)$$

Aqui, $\psi_i \in \mathbb{R}^M$ é um vetor de **parâmetros variacionais** que precisam ser otimizados [^758].

**Gaussianização da Observation Model**

Com este limite quadrático, é possível obter um limite inferior para a softmax likelihood. Considerando $y_i$ como a classe observada para a entrada $x_i$ e $w$ como os pesos do modelo, temos [^759]:

$$log p(y_i = c | x_i, w) \geq y_i^T X_i w - \frac{1}{2} w^T X_i^T A_i X_i w - b_i^T X_i w - c_i$$

Para simplificar a notação, define-se a **pseudo-measurement** $\tilde{y}_i$ como [^759]:

$$tilde{y}_i = A_i^{-1} (b_i + y_i)$$

Com isso, obtém-se uma versão "Gaussianizada" do modelo de observação:

$$p(y_i | x_i, w) \geq f(x_i, \psi_i) N(\tilde{y}_i | X_i w, A_i^{-1})$$

onde $f(x_i, \psi_i)$ é uma função que não depende de $w$ [^759]. Isso permite calcular a posteriori $q(w) = N(m_N, V_N)$ usando as regras de Bayes para Gaussianas, facilitando a inferência variacional [^759].

**Aplicação à Regressão Logística Multinomial**

Na regressão logística multinomial, o objetivo da inferência variacional é maximizar o limite inferior $L(q)$ [^759]:

$$L(q) = -KL(q(w) || p(w|D)) + E_q \left[\sum_{i=1}^N \log p(y_i | x_i, w)\right]$$

Substituindo a softmax likelihood pelo limite quadrático de Bohning, o objetivo se torna [^759]:

$$L(q) \geq -KL(q(w) || p(w|D)) + E_q \left[\sum_{i=1}^N y_i^T \eta_i - lse(\eta_i)\right]$$

$$L(q) \geq -KL(q(w) || p(w|D)) + \sum_{i=1}^N y_i^T E_q[\eta_i] - \sum_{i=1}^N E_q[lse(\eta_i)]$$

O primeiro termo é a divergência KL entre duas Gaussianas, que possui uma forma analítica [^759]:

$$-KL(N(m_0, V_0) || N(m_N, V_N)) = \frac{1}{2} \left[tr(V_N V_0^{-1}) - \log|V_N V_0^{-1}| + (m_N - m_0)^T V_0^{-1} (m_N - m_0) - DM\right]$$

onde $DM$ é a dimensionalidade da Gaussiana [^759]. O segundo termo é simplesmente [^759]:

$$sum_{i=1}^N y_i^T E_q[\eta_i] = \sum_{i=1}^N y_i^T X_i m_N$$

O termo final pode ser limitado superiormente tomando as expectativas do limite quadrático [^759]:

$$sum_{i=1}^N E_q[lse(\eta_i)] \geq \sum_{i=1}^N \left\{\frac{1}{2} tr(A_i V_i) + \frac{1}{2} m_i^T A_i m_i + b_i^T m_i - c_i\right\}$$

onde $V_i = X_i V_N X_i^T$ [^759]. Combinando todos os termos, obtém-se o limite inferior $L_{QJ}(q)$ [^759]:

$$L_{QJ}(q) \geq \frac{1}{2} \left[tr(V_N V_0^{-1}) - \log|V_N V_0^{-1}| + (m_N - m_0)^T V_0^{-1} (m_N - m_0)\right] - \frac{1}{2} DM + \sum_{i=1}^N y_i^T X_i m_N - \sum_{i=1}^N \left\{\frac{1}{2} tr(A_i V_i) + \frac{1}{2} m_i^T A_i m_i + b_i^T m_i - c_i\right\}$$

Este limite inferior combina a desigualdade de Jensen com o limite quadrático do termo lse [^79]. Para otimizar este limite, utiliza-se coordinate ascent, atualizando iterativamente os parâmetros variacionais da posteriori $V_N$ e $m_N$, e os parâmetros de likelihood variacional $\psi_i$ [^759].

### Conclusão

O Bohning's quadratic bound oferece uma abordagem eficaz para aproximar a função log-sum-exp, permitindo a derivação de limites inferiores Gaussian-like para a softmax likelihood. Essa técnica é particularmente útil em modelos como a regressão logística multinomial, onde a função lse impede a obtenção de soluções analíticas para a inferência variacional. Ao combinar o Bohning's bound com a inferência variacional, é possível obter aproximações eficientes e precisas para a posteriori dos parâmetros do modelo.

### Referências
[^757]: 21.8. Local variational bounds *
[^758]: 21.8.2 Bohning's quadratic bound to the log-sum-exp function
[^759]: 21.8.2.1 Applying Bohning's bound to multinomial logistic regression
<!-- END -->