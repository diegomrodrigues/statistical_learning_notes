## Tópicos Avançados em Local Variational Bounds: Desigualdade de Jensen e Método Delta Multivariado

### Introdução
Este capítulo aprofunda-se em métodos avançados para calcular limites e aproximações para a função *log-sum-exp*, um componente crucial em diversas áreas do aprendizado de máquina, como inferência variacional [^1, ^2]. Exploramos a **desigualdade de Jensen** e o **método delta multivariado** como abordagens alternativas para este problema [^33]. Além disso, discutimos situações onde um limite superior é necessário em vez de um limite inferior, como na derivação de um algoritmo de *mean field* para redes de crenças sigmoides [^33].

### Conceitos Fundamentais

#### Desigualdade de Jensen
A desigualdade de Jensen é uma ferramenta fundamental para obter limites para funções convexas ou côncavas. Em sua forma geral, afirma que para uma função convexa $\phi$ e uma variável aleatória $X$, temos:

$$\
\phi(E[X]) \le E[\phi(X)]
$$\

De forma análoga, se $\phi$ é côncava, a desigualdade se inverte. No contexto da função *log-sum-exp*, a desigualdade de Jensen pode ser utilizada para derivar limites inferiores [^33]. A função *log-sum-exp* é definida como:

$$\
\lse(\eta) = \log \left( 1 + \sum_{m=1}^M e^{\eta_m} \right)
$$\

Aplicar a desigualdade de Jensen diretamente à função *log-sum-exp* pode levar a limites que são úteis em certos contextos, como aproximar a função para torná-la conjugada a uma *Gaussian prior* [^33].

#### Método Delta Multivariado
O método delta multivariado é uma técnica para aproximar momentos de uma função usando uma expansão em série de Taylor [^33]. Dada uma função $f(w)$ de interesse, podemos aproximá-la por uma expansão de Taylor de segunda ordem em torno de um ponto $m$:

$$\
f(w) \approx f(m) + (w - m)^T g + \frac{1}{2} (w - m)^T H (w - m)
$$\

onde $g$ e $H$ são o gradiente e o Hessiano de $f$ avaliados em $m$, respectivamente. Se assumirmos que $w$ segue uma distribuição normal $q(w) = \mathcal{N}(w|m, V)$, podemos aproximar o valor esperado de $f(w)$ como:

$$\
E_q[f(w)] \approx f(m) + \frac{1}{2} tr[HV]
$$\

Este método é particularmente útil quando a função $f(w)$ é complexa e o cálculo direto de seus momentos é intratável. No contexto da função *log-sum-exp*, o método delta multivariado pode ser usado para aproximar os momentos desta função, permitindo a derivação de aproximações para a distribuição posterior [^33].

#### Limites Superiores vs. Limites Inferiores
Em algumas situações, é necessário um limite superior em vez de um limite inferior. Um exemplo notável é a derivação de um algoritmo de *mean field* para redes de crenças sigmoides. Nestes casos, um limite superior na função sigmoide é usado para tornar a atualização tratável e garantir um procedimento de inferência monotonicamente convergente [^33]. Saul et al. (1996) demonstraram como derivar um limite superior para a função sigmoide para tornar as atualizações tratáveis, resultando em um procedimento de inferência monotonicamente convergente [^33].

### Conclusão
Este capítulo explorou métodos avançados para lidar com a função *log-sum-exp*, um componente fundamental em muitos modelos estatísticos. A desigualdade de Jensen e o método delta multivariado oferecem abordagens alternativas para aproximar e limitar esta função, cada um com suas próprias vantagens e desvantagens. Além disso, discutimos a importância de limites superiores em certos contextos, como na derivação de algoritmos de *mean field* para redes de crenças sigmoides. A escolha do método apropriado depende das características específicas do problema em questão e das propriedades desejadas da aproximação resultante.

### Referências
[^33]: (Saul et al. 1996) Jensen's inequality and the multivariate delta method are alternative approaches to compute bounds and approximations to the log-sum-exp function; the multivariate delta method approximates moments of a function using a Taylor series expansion. In some cases, an upper bound is needed instead of a lower bound, such as in deriving a mean field algorithm for sigmoid belief nets, where an upper bound on the sigmoid function is derived to make the update tractable and result in a monotonically convergent inference procedure.

<!-- END -->