## Variational Inference: A Deterministic Approximate Inference Technique

### Introdução
O presente capítulo mergulha no domínio da **Variational Inference (VI)**, uma técnica de inferência aproximada determinística utilizada para estimar distribuições posteriores *intratáveis* [^1]. Em contraste com métodos de inferência exatos, que podem se tornar computacionalmente proibitivos, especialmente em modelos complexos, a VI oferece uma abordagem alternativa, transformando o problema de inferência em um problema de otimização [^1]. A ideia central é aproximar a posterior verdadeira, $p^*(x) = p(x|D)$, por uma distribuição $q(x)$ de uma família tratável, minimizando uma função de custo, como a divergência de Kullback-Leibler (KL) [^1]. A VI busca um equilíbrio entre precisão e velocidade computacional, oferecendo estimativas mais rápidas, comparáveis à estimativa MAP, mas retendo benefícios estatísticos da abordagem Bayesiana [^1]. Neste capítulo, exploraremos os conceitos fundamentais, as diferentes interpretações e as nuances da VI.

### Conceitos Fundamentais
A Variational Inference (VI) é uma técnica de inferência aproximada utilizada quando a distribuição posterior verdadeira, $p^*(x)$, é intratável [^1]. A intratabilidade pode surgir devido à complexidade do modelo, ao grande número de variáveis ou à forma funcional da posterior. A VI contorna esse problema aproximando $p^*(x)$ por uma distribuição $q(x)$ escolhida de uma família tratável [^1].

**Família Tratável**: A escolha da família de distribuições $q(x)$ é crucial. Famílias comuns incluem Gaussianas multivariadas [^2] e distribuições fatoradas [^2]. A tratabilidade significa que as expectativas em relação a $q(x)$ podem ser computadas analiticamente ou aproximadas eficientemente [^2].

**Divergência de Kullback-Leibler (KL)**: A divergência de KL, denotada por $KL(p||q)$, quantifica a dissimilaridade entre duas distribuições de probabilidade [^1]. Na VI, o objetivo é encontrar a distribuição $q(x)$ que minimize a divergência de KL entre $q(x)$ e a posterior verdadeira $p^*(x)$ [^1]. Existem duas formas de divergência de KL:

1.  **KL Direta (Forward KL)**: $KL(p^*||q) = \sum_x p^*(x) \log \frac{p^*(x)}{q(x)}$ [^2]
2.  **KL Reversa (Reverse KL)**: $KL(q||p^*) = \sum_x q(x) \log \frac{q(x)}{p^*(x)}$ [^2]

Minimizar a KL direta *tenta cobrir* a distribuição verdadeira, enquanto minimizar a KL reversa *tende a se concentrar* em um dos modos da distribuição verdadeira [^3].

**Otimização da Divergência de KL Reversa**: A KL reversa é geralmente preferida na VI porque permite computar expectativas em relação a $q(x)$, que é tratável [^2]. No entanto, mesmo com a KL reversa, calcular $p^*(x)$ diretamente pode ser difícil devido à constante de normalização intratável $Z = p(D)$ [^2]. Para contornar isso, define-se uma nova função objetivo $J(q)$ que não requer a normalização de $p^*(x)$ [^2]:

$$J(q) = KL(q||p) = \sum_x q(x) \log \frac{q(x)}{p(x)}$$

onde $p(x) = p(x, D)$ é a distribuição não normalizada [^2]. Expandindo a equação, obtemos:

$$J(q) = KL(q||p^*) - \log Z$$

Minimizar $J(q)$ força $q$ a se aproximar de $p^*$, uma vez que $Z$ é constante [^2]. Além disso, $J(q)$ é um limite superior para o NLL (negative log-likelihood) [^2]:

$$J(q) = KL(q||p^*) - \log Z \geq - \log Z = -\log p(D)$$

Alternativamente, pode-se maximizar o *functional de energia* $L(q)$, que é um limite inferior para o log-likelihood dos dados [^2]:

$$L(q) \stackrel{\triangle}{=} -J(q) = -KL(q||p^*) + \log Z \leq \log Z = \log p(D)$$

Este limite é *apertado* quando $q = p^*$, indicando a relação entre a VI e o algoritmo EM (Expectation-Maximization) [^2].

**Interpretações Alternativas do Objetivo Variacional**: O objetivo variacional pode ser interpretado de diferentes maneiras, fornecendo insights adicionais [^3]:

1.  **Energia Esperada e Entropia**: $J(q) = Eq[log q(x)] + Eq[-log p(x)] = -H(q) + Eq[E(x)]$, onde $E(x) = -\log p(x)$ é a energia [^3]. Em física estatística, $J(q)$ é chamado de *energia livre variacional* ou *energia livre de Helmholtz* [^3].
2.  **NLL Esperado e Penalidade**: $J(q) = Eq[-log p(D|x)] + KL(q(x)||p(x))$, que é o NLL esperado mais um termo de penalidade que mede a distância entre a posterior aproximada e a prior exata [^3].

**Forward vs. Reverse KL**: A escolha entre minimizar a KL direta ou reversa leva a comportamentos diferentes [^3]. A KL reversa (I-projection) é *zero forcing* para $q$, o que significa que $q(x) = 0$ sempre que $p(x) = 0$ [^3]. Isso tende a subestimar o suporte de $p$ [^3]. A KL direta (M-projection) é *zero avoiding* para $q$, o que significa que $q(x) > 0$ sempre que $p(x) > 0$ [^3]. Isso tende a superestimar o suporte de $p$ [^3]. Quando a distribuição verdadeira é multimodal, a KL direta pode levar a resultados ruins se $q$ for unimodal, pois o modo resultante estará em uma região de baixa densidade [^3]. Nesses casos, a KL reversa é mais tratável e estatisticamente sensível [^3].

### Conclusão

A Variational Inference (VI) oferece uma abordagem flexível e computacionalmente eficiente para aproximar distribuições posteriores intratáveis [^1]. Ao transformar a inferência em um problema de otimização, a VI permite estimar parâmetros e realizar inferências em modelos complexos, mantendo os benefícios de uma abordagem Bayesiana [^1]. A escolha da família de distribuições aproximadas, a função objetivo e a direção da divergência de KL são considerações importantes que influenciam a precisão e a eficiência da VI [^2, 3].

### Referências
[^1]: Capítulo 21, Variational inference, Introdução
[^2]: Capítulo 21, Variational inference, Variational inference
[^3]: Capítulo 21, Variational inference, Alternative interpretations of the variational objective
<!-- END -->