## Divergência de Kullback-Leibler na Inferência Variacional

### Introdução
A inferência variacional é uma técnica poderosa para aproximar distribuições posteriores complexas, transformando o problema de inferência em um problema de otimização [^1]. O objetivo principal é encontrar uma distribuição $q(x)$ de uma família tratável que seja a mais próxima possível da distribuição posterior verdadeira, mas intratável, $p^*(x) = p(x|D)$, onde $x$ representa as variáveis latentes e $D$ os dados observados [^1]. A similaridade entre $q(x)$ e $p^*(x)$ é tipicamente medida pela divergência de Kullback-Leibler (KL), e diferentes abordagens para minimizar essa divergência levam a diferentes propriedades da aproximação [^1].

### Conceitos Fundamentais
A **Divergência de Kullback-Leibler (KL)** é uma medida de dissimilaridade entre duas distribuições de probabilidade. No contexto da inferência variacional, ela é usada como função de custo para medir a similaridade entre a aproximação $q(x)$ e a posterior verdadeira $p^*(x)$ [^1]. Existem duas abordagens comuns para minimizar a divergência KL, cada uma levando a diferentes características de aproximação:

1.  **KL Forward (KL(p*||q))**: Essa abordagem minimiza a divergência de KL da posterior verdadeira $p^*(x)$ para a aproximação $q(x)$ [^1]. Matematicamente, é expressa como:
    $$KL(p^*||q) = \sum_x p^*(x) \log \frac{p^*(x)}{q(x)} \qquad (21.1)$$
    A minimização da KL forward é computacionalmente desafiadora porque requer o cálculo de expectativas em relação à posterior verdadeira $p^*(x)$, que é intratável por definição [^1]. Essa abordagem tende a ser *zero avoiding* para $q$, o que significa que $q(x) > 0$ sempre que $p^*(x) > 0$. Em outras palavras, $q$ tende a superestimar o suporte de $p^*$.

2.  **KL Reverse (KL(q||p*))**: Essa abordagem minimiza a divergência de KL da aproximação $q(x)$ para a posterior verdadeira $p^*(x)$ [^1]. Matematicamente, é expressa como:
    $$KL(q||p^*) = \sum_x q(x) \log \frac{q(x)}{p^*(x)} \qquad (21.2)$$
    Uma alternativa mais tratável é minimizar a divergência KL reversa $KL(q||p^*)$, que envolve expectativas em relação à aproximação tratável $q(x)$ [^1]. A minimização da KL reversa é *zero forcing* para $q$, o que significa que $q(x) = 0$ sempre que $p^*(x) = 0$. Em outras palavras, $q$ tende a subestimar o suporte de $p^*$.

#### Vantagens e Desvantagens
A principal vantagem da divergência KL reversa é a sua tratabilidade computacional. No entanto, a equação (21.2) ainda não é tratável na prática, pois avaliar $p^*(x) = p(x|D)$ pontualmente requer avaliar a constante de normalização intratável $Z = p(D)$ [^2]. Para contornar esse problema, define-se uma nova função objetivo $J(q)$:
$$J(q) = KL(q||p) \qquad (21.3)$$
onde $p(x) = p(x, D) = p^*(x)Z$ é a distribuição não normalizada [^2]. Expandindo a definição de KL, obtemos:
$$J(q) = \sum_x q(x) \log \frac{q(x)}{p(x)} = \sum_x q(x) \log \frac{q(x)}{Zp^*(x)} = \sum_x q(x) \log \frac{q(x)}{p^*(x)} - \log Z = KL(q||p^*) - \log Z \qquad (21.4, 21.5, 21.6, 21.7)$$
Como $Z$ é uma constante, minimizar $J(q)$ força $q$ a se aproximar de $p^*$. Além disso, como a divergência KL é sempre não negativa, $J(q)$ é um limite superior para o logaritmo negativo da verossimilhança (NLL):
$$J(q) = KL(q||p^*) - \log Z \geq - \log Z = - \log p(D) \qquad (21.8)$$
Alternativamente, podemos tentar maximizar a seguinte quantidade (referida como o *funcional de energia*):
$$L(q) \hat{=} -J(q) = -KL(q||p^*) + \log Z \leq \log Z = \log p(D) \qquad (21.9)$$
Este limite inferior é apertado quando $q = p^*$, mostrando que a inferência variacional está intimamente relacionada ao algoritmo EM [^2].

#### Interpretações alternativas do objetivo variacional
Existem várias maneiras equivalentes de escrever o objetivo variacional, proporcionando diferentes perspectivas [^3]. Uma formulação é:
$$J(q) = E_q[\log q(x)] + E_q[-\log p(x)] = -H(q) + E_q[E(x)] \qquad (21.10)$$
onde $H(q)$ é a entropia de $q$ e $E(x) = -\log p(x)$ é a energia. Em física estatística, $J(q)$ é chamada de *energia livre variacional* ou *energia livre de Helmholtz*.

Outra formulação é:
$$J(q) = E_q[\log q(x) - \log p(x)p(D|x)] = E_q[\log q(x) - \log p(x) - \log p(D|x)] = E_q[-\log p(D|x)] + KL(q(x)||p(x)) \qquad (21.11, 21.12, 21.13)$$
Esta é a NLL esperada, mais um termo de penalidade que mede o quão distante a posterior aproximada está da prior exata.

### Forward vs Reverse KL
A escolha entre minimizar a divergência KL forward ou reverse tem implicações significativas na qualidade da aproximação [^3].

*   **KL Reverse (I-projection)**: Minimizar $KL(q||p)$ é conhecido como *I-projection* ou *information projection*. Essa abordagem é *zero forcing*, ou seja, se $p(x) = 0$, então $q(x) = 0$. Isso significa que $q$ tipicamente subestima o suporte de $p$ [^3].

*   **KL Forward (M-projection)**: Minimizar $KL(p||q)$ é conhecido como *M-projection* ou *moment projection*. Essa abordagem é *zero avoiding*, ou seja, se $q(x) = 0$, então $p(x) = 0$. Isso significa que $q$ tipicamente superestima o suporte de $p$ [^3].

Quando a distribuição verdadeira é multimodal e $q$ é restrita a ser unimodal, usar a KL forward pode levar a resultados ruins, pois o modo/média resultante de $q$ pode ficar em uma região de baixa densidade entre os picos [^3]. Nesses contextos, a KL reversa não é apenas mais tratável computacionalmente, mas também mais sensata estatisticamente [^3].

### Conclusão
A escolha entre a divergência KL forward e reverse na inferência variacional depende das características da distribuição posterior verdadeira e das restrições impostas à distribuição aproximada. A KL reversa é geralmente preferível devido à sua tratabilidade computacional e comportamento mais estável em distribuições multimodais, embora possa subestimar o suporte da posterior verdadeira. A compreensão dessas compensações é fundamental para aplicar a inferência variacional de forma eficaz.

### Referências
[^1]: Variational inference - 21 Variational inference - 21.1 Introduction.
[^2]: Variational inference - 21 Variational inference - 21.2 Variational inference.
[^3]: Variational inference - 21 Variational inference - 21.2.2 Forward or reverse KL? *.
<!-- END -->