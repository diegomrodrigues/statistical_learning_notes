## Método do Campo Médio em Inferência Variacional

### Introdução
A inferência variacional é uma técnica poderosa para aproximar distribuições posteriores complexas, especialmente em modelos gráficos onde a inferência exata é intratável [^2]. Uma das abordagens mais populares dentro da inferência variacional é o **método do campo médio** (mean field method) [^5]. Este capítulo explora em detalhes o método do campo médio, suas aplicações e extensões.

### Conceitos Fundamentais
#### A Aproximação do Campo Médio
O método do campo médio simplifica o problema de inferência ao assumir que a distribuição posterior pode ser aproximada por uma distribuição completamente fatorada [^5]. Formalmente, se $x = (x_1, x_2, ..., x_D)$ representa o conjunto de variáveis latentes, a distribuição aproximada $q(x)$ é expressa como:
$$q(x) = \prod_{i=1}^{D} q_i(x_i)$$
onde $q_i(x_i)$ é a distribuição marginal para a variável $x_i$ [^5]. Esta suposição de fatoração transforma o problema de inferência em um problema de otimização, onde cada distribuição marginal $q_i$ é atualizada iterativamente enquanto as outras são mantidas fixas [^5].

#### Otimização por Descida de Coordenadas
A otimização das distribuições marginais $q_i$ é tipicamente realizada utilizando um método de descida de coordenadas [^5]. Em cada passo, a distribuição $q_j(x_j)$ é atualizada de acordo com a seguinte equação:
$$ \log q_j(x_j) = \mathbb{E}_{-q_j}[\log p(x)] + \text{const} $$
onde $\mathbb{E}_{-q_j}[\log p(x)]$ representa a expectativa do logaritmo da distribuição conjunta $p(x)$ em relação a todas as variáveis *exceto* $x_j$ [^5, 6]. O termo "const" é uma constante que garante que $q_j(x_j)$ seja uma distribuição de probabilidade válida. A expectativa é calculada como:
$$mathbb{E}_{-q_j}[f(x)] = \sum_{x_1} \cdots \sum_{x_{j-1}} \sum_{x_{j+1}} \cdots \sum_{x_D} q(x_1) \cdots q(x_{j-1}) q(x_{j+1}) \cdots q(x_D) f(x)$$
No caso contínuo, as somas são substituídas por integrais [^6].

É importante notar que, ao atualizar $q_j$, só precisamos considerar os fatores que compartilham variáveis com $x_j$, ou seja, os termos no *Markov blanket* de $x_j$ [^6]. Os outros termos são absorvidos na constante.

#### Derivação das Equações de Atualização
O objetivo da inferência variacional é minimizar a divergência de Kullback-Leibler (KL) entre a distribuição aproximada $q(x)$ e a distribuição posterior verdadeira $p(x|D)$ [^1, 2]. No entanto, minimizar $KL(q||p^*)$ diretamente é intratável, então maximizamos o limite inferior da evidência (Evidence Lower Bound - ELBO) [^2]:
$$mathcal{L}(q) = -KL(q||p) = \sum_x q(x) \log \frac{p(x)}{q(x)} \leq \log p(D)$$
Para derivar as equações de atualização, maximizamos $\mathcal{L}(q)$ em relação a cada $q_j(x_j)$ individualmente, mantendo as outras distribuições fixas [^5]. Reescrevendo $\mathcal{L}(q)$ isolando os termos que dependem de $q_j$:
$$mathcal{L}(q_j) = \sum_{x_j} q_j(x_j) \log f_j(x_j) - \sum_{x_j} q_j(x_j) \log q_j(x_j) + \text{const}$$
onde $f_j(x_j) = \mathbb{E}_{-q_j}[\log p(x)]$ [^7]. Maximizar $\mathcal{L}(q_j)$ é equivalente a minimizar a divergência KL entre $q_j(x_j)$ e $f_j(x_j)$, o que leva à solução:
$$q_j(x_j) = \frac{1}{Z_j} \exp(\mathbb{E}_{-q_j}[\log p(x)])$$
onde $Z_j$ é uma constante de normalização [^7].

#### Variational Bayes
Variational Bayes (VB) estende a inferência variacional para inferir os próprios parâmetros do modelo [^5]. Em vez de fixar os parâmetros em valores pontuais, VB trata os parâmetros como variáveis aleatórias e infere uma distribuição posterior sobre eles.  Uma aproximação de campo médio completamente fatorada, $p(\theta|D) \approx \prod_k q(\theta_k)$, resulta em um método conhecido como Variational Bayes [^5].

### Conclusão
O método do campo médio é uma ferramenta fundamental na inferência variacional, oferecendo uma maneira tratável de aproximar distribuições posteriores complexas [^5]. Ao assumir uma estrutura fatorada e otimizar iterativamente as distribuições marginais, o método do campo médio permite realizar inferência aproximada em uma ampla gama de modelos gráficos. Variational Bayes estende este método para inferir também os parâmetros do modelo, tornando-o uma técnica poderosa para inferência Bayesiana aproximada [^5].

### Referências
[^2]: Chapter 21. Variational inference. Page 731.
[^5]: Chapter 21. Variational inference. Page 735.
[^6]: Chapter 21. Variational inference. Page 736.
[^7]: Chapter 21. Variational inference. Page 737.
<!-- END -->