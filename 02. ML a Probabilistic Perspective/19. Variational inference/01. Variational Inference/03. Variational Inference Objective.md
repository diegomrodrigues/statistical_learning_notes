## Otimização da Função Objetivo em Inferência Variacional

### Introdução
Como vimos anteriormente, a inferência variacional busca aproximar uma distribuição posterior intratável $p^*(x)$ por uma distribuição $q(x)$ de uma família tratável [^1]. Este capítulo se aprofunda na função objetivo utilizada para otimizar $q(x)$ e suas interpretações, baseando-se nos conceitos de divergência KL e energia livre variacional.

### Conceitos Fundamentais
A função objetivo $J(q)$ é definida como a divergência de Kullback-Leibler (KL) entre $q$ e uma distribuição não normalizada $p$ [^1]:
$$J(q) = KL(q||p)$$
onde $p(x) = p(x, D) = p^*(x)Z$, com $Z = p(D)$ sendo a constante de normalização intratável [^1].

**Minimização da Divergência KL:**
Minimizar $J(q)$ força $q$ a se aproximar de $p^*$, provendo um limite superior para a negative log-likelihood (NLL) [^1]. Matematicamente [^1]:
$$J(q) = KL(q||p^*) - \log Z \ge - \log Z = -\log p(D)$$
Uma vez que a divergência KL é sempre não negativa, $J(q)$ é um limite superior para a NLL [^2].

**Derivação da Função Objetivo:**
A partir da definição de divergência KL [^2]:
$$J(q) = \sum_x q(x) \log \frac{q(x)}{p(x)}$$
Substituindo $p(x) = \frac{p(x, D)}{Z}$ [^2]:
$$J(q) = \sum_x q(x) \log \frac{q(x)}{Zp^*(x)} = \sum_x q(x) \log \frac{q(x)}{p^*(x)} - \log Z$$
$$J(q) = KL(q||p^*) - \log Z$$
Como $Z$ é constante, minimizar $J(q)$ equivale a minimizar $KL(q||p^*)$ [^2].

**Interpretações Alternativas:**
1.  ***Energia Livre Variacional (Helmholtz Free Energy):*** $J(q)$ pode ser vista como a energia livre variacional em física estatística [^3]:
    $$J(q) = E_q[\log q(x)] + E_q[-\log p(x)] = -H(q) + E_q[E(x)]$$
    onde $E(x) = -\log p(x)$ é a energia do sistema e $H(q)$ é a entropia de $q$ [^3]. Minimizar a energia livre variacional é equivalente a encontrar um equilíbrio entre minimizar a energia esperada e maximizar a entropia do sistema.
2.  ***NLL Esperada Mais Penalidade:*** $J(q)$ também pode ser expressa como a NLL esperada mais um termo de penalidade que mede a distância entre a posterior aproximada e a prior exata [^3]:
    $$J(q) = E_q[\log q(x) - \log p(x)p(D|x)]$$
    $$J(q) = E_q[\log q(x) - \log p(x) - \log p(D|x)]$$
    $$J(q) = E_q[-\log p(D|x)] + KL(q(x)||p(x))$$
    Este ponto de vista conecta a inferência variacional à teoria da informação e ao argumento *bits-back* [^3].

**Maximizar o Limite Inferior:**
Alternativamente, podemos maximizar o limite inferior da log-likelihood dos dados [^2]:
$$mathcal{L}(q) = -J(q) = -KL(q||p^*) + \log Z \le \log Z = \log p(D)$$
Este limite inferior é referido como *energy functional* [^2]. A inferência variacional busca tornar $q$ o mais próximo possível de $p^*$, tornando o limite inferior o mais *tight* possível [^2].

### Conclusão
A função objetivo $J(q)$ em inferência variacional desempenha um papel central na aproximação da distribuição posterior intratável. Sua minimização, ou equivalentemente, a maximização do limite inferior da log-likelihood, permite encontrar uma distribuição aproximada $q(x)$ que equilibra a proximidade à posterior verdadeira e a tratabilidade computacional. As interpretações alternativas da função objetivo fornecem insights valiosos sobre os princípios subjacentes da inferência variacional.

### Referências
[^1]: Variational inference. (n.d.).
[^2]: Variational inference. (n.d.).
[^3]: Variational inference. (n.d.).
<!-- END -->