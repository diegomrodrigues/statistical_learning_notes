## Divergências de Kullback-Leibler e Divergência Alpha em Inferência Variacional

### Introdução
Em Inferência Variacional (VI), o objetivo é aproximar uma distribuição posterior *intratável* $p^*(x)$ por uma distribuição *tratável* $q(x)$ [^732]. A escolha da divergência utilizada para medir a similaridade entre essas distribuições tem um impacto significativo no comportamento da aproximação. Este capítulo explora as divergências de Kullback-Leibler (KL) em suas duas formas (forward e reverse), a divergência Alpha, e a distância de Hellinger, destacando suas propriedades e implicações na inferência variacional.

### Conceitos Fundamentais

#### Divergências de Kullback-Leibler (KL)
A divergência de Kullback-Leibler, ou KL divergence, é uma medida de quão diferente uma distribuição de probabilidade é de uma segunda distribuição de probabilidade de referência. No contexto de inferência variacional, temos duas opções principais para minimizar a divergência KL entre a distribuição verdadeira (mas intratável) $p^*(x)$ e a distribuição aproximada $q(x)$:

1.  **Reverse KL (I-projection ou Information Projection):** Minimizar $KL(q||p^*)$ [^733].
    $$KL(q||p^*) = \sum_x q(x) \ln \frac{q(x)}{p^*(x)}$$
    Esta abordagem é também conhecida como *I-projection* ou *information projection* [^733]. A reverse KL é *zero forcing* para $q$ [^733]. Isso significa que se $p^*(x) = 0$, devemos garantir que $q(x) = 0$ [^733]. Consequentemente, $q$ tipicamente *subestima* o suporte de $p^*$ [^733].

2.  **Forward KL (M-projection ou Moment Projection):** Minimizar $KL(p^*||q)$ [^733].
    $$KL(p^*||q) = \sum_x p^*(x) \ln \frac{p^*(x)}{q(x)}$$
    Esta abordagem é também conhecida como *M-projection* ou *moment projection* [^733]. A forward KL é *zero avoiding* para $q$ [^733]. Isso significa que se $p^*(x) > 0$, devemos garantir que $q(x) > 0$ [^733]. Consequentemente, $q$ tipicamente *superestima* o suporte de $p^*$ [^733].

A escolha entre minimizar a forward KL ou a reverse KL leva a comportamentos distintos [^733]. A reverse KL tende a se concentrar em apenas um dos modos da distribuição verdadeira, enquanto a forward KL tenta cobrir todos os modos, mesmo que isso signifique colocar massa de probabilidade em regiões de baixa densidade [^734].

Em contextos onde a distribuição verdadeira é multimodal e a distribuição aproximada é unimodal, minimizar a forward KL pode ser problemático, pois o modo resultante pode estar em uma região de baixa densidade entre os picos [^733]. Nesses casos, a reverse KL não é apenas mais tratável computacionalmente, mas também mais sensata estatisticamente [^733].

#### Divergência Alpha
A divergência Alpha ($D_\alpha(p||q)$) fornece uma família de medidas de divergência indexadas por um parâmetro $\alpha \in \mathbb{R}$ [^735]:

$$D_\alpha(p||q) \triangleq \frac{4}{1 - \alpha^2} \left( 1 - \int p(x)^{\frac{1 + \alpha}{2}} q(x)^{\frac{1 - \alpha}{2}} dx \right)$$

A forward KL corresponde ao limite $\alpha \rightarrow 1$, enquanto a reverse KL corresponde ao limite $\alpha \rightarrow -1$ [^735]. A divergência Alpha oferece, assim, um espectro de comportamentos entre as duas KL divergências [^735].

#### Distância de Hellinger
Quando $\alpha = 0$, obtemos uma medida de divergência simétrica que é linearmente relacionada à distância de Hellinger ($D_H(p||q)$) [^735]:

$$D_H(p||q) \triangleq \int \left( \sqrt{p(x)} - \sqrt{q(x)} \right)^2 dx$$

A raiz quadrada da distância de Hellinger, $\sqrt{D_H(p||q)}$, é uma métrica válida, ou seja, é simétrica, não negativa e satisfaz a desigualdade triangular [^735].

### Conclusão
A escolha da divergência em inferência variacional é crucial e depende da natureza da distribuição posterior que se deseja aproximar. A reverse KL é útil quando se deseja um suporte mais conservador, enquanto a forward KL é apropriada quando se busca cobrir todos os modos da distribuição, mesmo que isso signifique inflar o suporte. A divergência Alpha oferece uma flexibilidade adicional, permitindo ajustar o comportamento da aproximação. A distância de Hellinger, sendo uma métrica, pode ser útil em contextos onde a simetria é desejável.

### Referências
[^732]: Bishop, C. M. (2006b). *Pattern Recognition and Machine Learning*. Springer.
[^733]: Bishop, C. M. (2006b). *Pattern Recognition and Machine Learning*. Springer.
[^734]: Bishop, C. M. (2006b). *Pattern Recognition and Machine Learning*. Springer.
[^735]: Bishop, C. M. (2006b). *Pattern Recognition and Machine Learning*. Springer.
<!-- END -->