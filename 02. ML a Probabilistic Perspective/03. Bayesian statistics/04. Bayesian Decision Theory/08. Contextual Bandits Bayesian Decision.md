## Contextual Bandits and Bayesian Decision Theory

### Introdução
Este capítulo explora a aplicação da teoria da decisão Bayesiana em problemas de **contextual bandits**. Os contextual bandits representam uma classe de problemas de decisão sequencial onde um agente deve escolher uma ação de um conjunto de opções, cada uma com uma função de recompensa desconhecida [^36]. A teoria da decisão Bayesiana oferece um arcabouço para equilibrar *exploration* (exploração) e *exploitation* (explotação) nesses cenários, utilizando técnicas como UCB (Upper Confidence Bound) e Thompson sampling [^36]. Este capítulo se baseia nos conceitos de estatística Bayesiana previamente introduzidos [^1], como a sumarização de distribuições posteriores [^5], a estimativa MAP [^1] e os intervalos de credibilidade [^4], para fornecer uma análise aprofundada dos contextual bandits.

### Conceitos Fundamentais

**Contextual Bandits**: Em um problema de contextual bandit, um agente observa um contexto $x$ e deve escolher uma ação $a$ de um conjunto de ações $A$. Após escolher uma ação, o agente recebe uma recompensa $r(x, a)$, que é uma amostra de uma distribuição de recompensa desconhecida $p(r|x, a)$. O objetivo do agente é maximizar a recompensa acumulada ao longo do tempo.

**Bayesian Approach**: A abordagem Bayesiana para contextual bandits envolve manter uma distribuição posterior sobre as funções de recompensa para cada ação, $p(r_k|D)$, onde $r_k$ é a recompensa da ação $k$ e $D$ é o histórico de observações (contextos, ações e recompensas) [^36]. Essa distribuição posterior é atualizada a cada interação, incorporando novas informações sobre as recompensas das ações.

**Exploração vs. Explotação**: O desafio central nos contextual bandits é equilibrar a exploração de ações menos conhecidas, para aprender sobre suas recompensas, com a explotação de ações que se acredita terem altas recompensas, para maximizar o ganho imediato [^36]. A teoria da decisão Bayesiana fornece ferramentas para lidar com esse tradeoff.

**UCB (Upper Confidence Bound)**: A estratégia UCB seleciona a ação que maximiza o limite superior da recompensa esperada, dado pela fórmula [^37]:

$$
k^* = \underset{k=1}{\text{argmax}} \ \mu_k + \lambda \sigma_k
$$

Onde $\mu_k = E[r_k|D]$ é a recompensa esperada da ação $k$, $\sigma_k^2 = \text{var}[r_k|D]$ é a variância da recompensa da ação $k$, e $\lambda$ é um parâmetro de ajuste que controla a trade-off entre exploração e explotação [^37]. A intuição é que devemos escolher ações que acreditamos serem boas (µk é grande) e/ou ações sobre as quais estamos incertos (σk é grande) [^37].

**Thompson Sampling**: Thompson sampling é uma estratégia probabilística que seleciona a ação com uma probabilidade proporcional à sua probabilidade de ser a ação ótima [^37]. Em cada etapa, uma amostra é extraída da distribuição posterior de cada ação, e a ação com a maior amostra é selecionada. Formalmente [^37]:

$$
p_k = \int I(E[r|a, x, \theta] \geq \max_{a'} E[r|a', x, \theta]) p(\theta|D) d\theta
$$

Podemos aproximar isso desenhando uma única amostra da posterior, $\theta^t \sim p(\theta|D)$, e então escolhendo [^37]:

$$
k^* = \text{argmax}_k E[r|x, k, \theta^t]
$$

**Contextualização**: Em contextual bandits, cada ação e o agente possuem um vetor de características associado, $x$ [^36]. Este contexto permite que o agente generalize o aprendizado entre diferentes situações, adaptando suas escolhas de ação com base nas características observadas. Por exemplo, os “braços” podem representar anúncios ou notícias que queremos mostrar ao usuário, e as características podem representar propriedades desses anúncios ou artigos, bem como propriedades do usuário, como dados demográficos [^36].

**Modelos Lineares**: Assumindo um modelo linear para a recompensa, $r_k = \theta_k^T x$, podemos manter uma distribuição sobre os parâmetros de cada braço, $p(\theta_k|D)$ [^36]. As formas de computar $p(\theta_k|D)$ a partir de modelos de regressão linear e logística serão discutidas em capítulos posteriores [^37].

### Conclusão
A teoria da decisão Bayesiana oferece um arcabouço poderoso para resolver problemas de contextual bandits, equilibrando a exploração de ações desconhecidas com a explotação de ações promissoras. Técnicas como UCB e Thompson sampling fornecem estratégias eficazes para selecionar ações, enquanto a contextualização permite que o agente generalize o aprendizado entre diferentes situações. A aplicação desses conceitos é crucial em diversas áreas, como recomendação de conteúdo, publicidade online e otimização de tratamento médico. Ao utilizar modelos Bayesianos, é possível incorporar incerteza nas estimativas de recompensa, o que leva a decisões mais robustas e adaptáveis ao longo do tempo.
### Referências
[^1]: Seção 5.1, "Introduction"
[^2]: Seção 5.2, "Summarizing posterior distributions"
[^3]: Seção 5.2.1, "MAP estimation"
[^4]: Seção 5.2.2, "Credible intervals"
[^5]: Seção 5.2.2.1, "Highest posterior density regions"
[^36]: Seção 5.7.3.1, "Contextual bandits"
[^37]: Seção 5.7.3.2, "Utility theory"
<!-- END -->