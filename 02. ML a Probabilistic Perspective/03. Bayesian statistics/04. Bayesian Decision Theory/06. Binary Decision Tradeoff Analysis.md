## Trade-off entre Falsos Positivos e Falsos Negativos em Problemas de Decisão Binária

### Introdução
Em continuidade ao estudo da **Teoria da Decisão Bayesiana** [^5], este capítulo aprofunda a análise de problemas de decisão binária, nos quais a escolha recai entre duas alternativas. Como veremos, a tomada de decisão nesses cenários envolve um *trade-off* crucial entre **falsos positivos** e **falsos negativos** [^1]. Este *trade-off* é intrinsecamente ligado aos custos associados a cada tipo de erro, representados na **matriz de perda**. Exploraremos como as **curvas ROC** (Receiver Operating Characteristic) e as **curvas de precisão-revocação** (precision-recall) fornecem ferramentas poderosas para analisar esse *trade-off* e selecionar um limiar (threshold) apropriado para a decisão [^1]. Este capítulo se baseia nos conceitos fundamentais apresentados anteriormente, incluindo a **função de perda** [^2], que quantifica as consequências de uma decisão incorreta e a **inferência Bayesiana** [^5].

### Conceitos Fundamentais

#### A Matriz de Perda e o Trade-off
Em problemas de decisão binária, nosso objetivo é classificar uma observação em uma de duas classes possíveis. No entanto, essa classificação nem sempre é perfeita, e dois tipos de erros podem ocorrer [^1]:

*   **Falso Positivo (FP):** Classificar uma observação como pertencente à classe positiva quando, na verdade, pertence à classe negativa. Também conhecido como *falso alarme* [^32].
*   **Falso Negativo (FN):** Classificar uma observação como pertencente à classe negativa quando, na verdade, pertence à classe positiva. Também conhecido como *detecção perdida* [^32].

A importância relativa desses erros é capturada pela **matriz de perda** $L(y, a)$, onde $y$ representa a classe verdadeira e $a$ a ação (decisão) tomada [^2]. Em um problema de classificação binária, a matriz de perda assume a seguinte forma:

|                  | Predito Positivo (a=1) | Predito Negativo (a=0) |
| :--------------- | :---------------------- | :---------------------- |
| Realmente Positivo (y=1) | 0                       | LFN                       |
| Realmente Negativo (y=0) | LFP                       | 0                       |

Onde:

*   $L(1, 1) = L(0, 0) = 0$: Não há perda quando a decisão está correta [^29].
*   $L(1, 0) = L_{FN}$: Perda associada a um falso negativo.
*   $L(0, 1) = L_{FP}$: Perda associada a um falso positivo.

A **decisão ótima** depende da minimização da **perda esperada** [^29], que é calculada levando em consideração as probabilidades *a posteriori* das classes e os custos associados a cada tipo de erro [^29]:

$$\
\rho(a|x) = E_{p(y|x)}[L(y, a)] = \sum_{y} L(y, a) p(y|x)
$$

O *trade-off* surge porque, ao ajustarmos o limiar de decisão, podemos influenciar as taxas de falsos positivos e falsos negativos. Diminuir o limiar pode aumentar a taxa de verdadeiros positivos, mas também aumentar a taxa de falsos positivos, e vice-versa [^1].

#### Curvas ROC e Análise do Trade-off
A **curva ROC** (Receiver Operating Characteristic) é uma ferramenta gráfica que ilustra o desempenho de um classificador binário à medida que o limiar de decisão é variado [^33]. Ela plota a **taxa de verdadeiros positivos** (TPR) contra a **taxa de falsos positivos** (FPR) [^33]:

*   **TPR (Taxa de Verdadeiros Positivos):** Também conhecida como sensibilidade ou revocação, é a proporção de observações positivas corretamente classificadas [^33].
    $$\
    TPR = \frac{TP}{N_+}
    $$
*   **FPR (Taxa de Falsos Positivos):** Também conhecida como taxa de falso alarme, é a proporção de observações negativas incorretamente classificadas como positivas [^33].
    $$\
    FPR = \frac{FP}{N_-}
    $$

Uma curva ROC ideal se aproxima do canto superior esquerdo do gráfico, indicando alta TPR e baixa FPR em todos os limiares [^33]. A **área sob a curva ROC** (AUC) é uma métrica que resume o desempenho geral do classificador [^33]. Quanto maior a AUC (próximo de 1), melhor o classificador [^33]. Um classificador aleatório terá uma AUC de 0.5, correspondente à linha diagonal no gráfico ROC [^33].

#### Curvas de Precisão-Revocação
As **curvas de precisão-revocação** (precision-recall) são alternativas às curvas ROC, especialmente úteis em conjuntos de dados desbalanceados, onde uma classe é muito mais frequente que a outra [^34]. Elas plotam a **precisão** contra a **revocação** [^34]:

*   **Precisão:** A proporção de observações classificadas como positivas que são realmente positivas [^34].
    $$\
    Precision = \frac{TP}{\tilde{N_+}}
    $$
*   **Revocação:** A proporção de observações positivas que são corretamente classificadas (igual à TPR) [^34].

Uma curva de precisão-revocação ideal se aproxima do canto superior direito do gráfico, indicando alta precisão e alta revocação em todos os limiares [^34].

#### Escolha do Limiar de Decisão
A seleção do limiar de decisão apropriado depende dos custos relativos dos falsos positivos e falsos negativos [^1]. Se os falsos negativos forem muito mais custosos que os falsos positivos, um limiar mais baixo pode ser preferível, e vice-versa [^1]. A análise das curvas ROC e de precisão-revocação permite visualizar o impacto da variação do limiar nas taxas de erro e, assim, auxiliar na escolha do limiar que melhor equilibra o *trade-off* de acordo com os requisitos específicos do problema [^1].

### Conclusão
A tomada de decisões em problemas binários exige uma compreensão clara do *trade-off* entre falsos positivos e falsos negativos. A matriz de perda quantifica os custos relativos desses erros, enquanto as curvas ROC e de precisão-revocação fornecem ferramentas visuais para analisar o impacto da variação do limiar de decisão no desempenho do classificador. Ao combinar essas ferramentas com o conhecimento do contexto do problema, podemos selecionar um limiar que minimize a perda esperada e maximize a utilidade da decisão.

### Referências
[^1]: Texto fornecido
[^2]: Seção 5.7
[^3]: Seção 5.7
[^4]: Seção 5.7
[^5]: Capítulo 5
<!-- END -->