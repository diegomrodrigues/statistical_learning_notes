## Empirical Bayes com o Modelo Beta-Binomial

### Introdução
Este capítulo explora a aplicação do método **Empirical Bayes (EB)**, também conhecido como *type-II maximum likelihood* [^44], utilizando o modelo **beta-binomial** como exemplo ilustrativo. O Empirical Bayes é uma abordagem que estima os hiperparâmetros de uma distribuição *a priori* maximizando a verossimilhança marginal [^79]. Essa técnica permite obter uma estimativa *a posteriori* que representa uma média ponderada entre a estimativa de máxima verossimilhança local (MLE) e a média *a priori*, onde os pesos são determinados pelos hiperparâmetros estimados [^44]. Este método é particularmente útil em modelos hierárquicos Bayesianos, como será demonstrado.

### Conceitos Fundamentais

No contexto do Empirical Bayes, o objetivo é estimar os hiperparâmetros $\eta$ da distribuição *a priori* $p(\theta|\eta)$ maximizando a verossimilhança marginal $p(D|\eta)$ [^79]. Matematicamente, isso é expresso como:

$$hat{\eta} = \underset{\eta}{\operatorname{argmax}} \\, p(D|\eta) = \underset{\eta}{\operatorname{argmax}} \left[ \int p(D|\theta)p(\theta|\eta) d\theta \right]$$

onde $D$ representa os dados observados e $\theta$ é o parâmetro do modelo [^79]. A integral interna representa a verossimilhança marginal, também conhecida como *evidência* [^79].

#### Modelo Beta-Binomial
O modelo beta-binomial é frequentemente utilizado para modelar dados binários agrupados, onde a probabilidade de sucesso varia entre os grupos. A distribuição beta é utilizada como *a priori* para a probabilidade de sucesso $\theta$, e a distribuição binomial modela o número de sucessos em cada grupo.

Seja $x_i$ o número de sucessos observados em $N_i$ tentativas para o grupo $i$. Assumimos que $x_i \sim \text{Bin}(N_i, \theta_i)$ e $\theta_i \sim \text{Beta}(a, b)$, onde $a$ e $b$ são os hiperparâmetros da distribuição beta [^81]. A distribuição *a priori* para $\theta$ é dada por:

$$p(\theta|a,b) = \frac{1}{B(a,b)} \theta^{a-1} (1-\theta)^{b-1}$$

onde $B(a, b)$ é a função beta, que garante que a distribuição seja normalizada [^81].

A verossimilhança marginal para o modelo beta-binomial é dada por [^81]:

$$p(D|a,b) = \prod_{i=1}^{D} \int \text{Bin}(x_i|N_i, \theta_i) \text{Beta}(\theta_i|a,b) d\theta_i = \prod_{i=1}^{D} \frac{B(a + x_i, b + N_i - x_i)}{B(a,b)}$$

onde $D$ é o número de grupos [^81].

Para estimar os hiperparâmetros $a$ e $b$ usando Empirical Bayes, maximizamos a verossimilhança marginal em relação a $a$ e $b$:

$$hat{a}, \hat{b} = \underset{a,b}{\operatorname{argmax}} \\, p(D|a,b) = \underset{a,b}{\operatorname{argmax}} \\, \prod_{i=1}^{D} \frac{B(a + x_i, b + N_i - x_i)}{B(a,b)}$$

A maximização desta função pode ser realizada utilizando métodos numéricos, como o algoritmo EM [^81] (várias formas de maximizar isso wrt a e b são discutidas em (Minka 2000e)).

Após estimar os hiperparâmetros $\hat{a}$ e $\hat{b}$, podemos calcular a distribuição *a posteriori* para cada $\theta_i$:

$$p(\theta_i|x_i, \hat{a}, \hat{b}) = \text{Beta}(\theta_i|\hat{a} + x_i, \hat{b} + N_i - x_i)$$

A média *a posteriori* de $\theta_i$ é dada por:

$$E[\theta_i|x_i, \hat{a}, \hat{b}] = \frac{\hat{a} + x_i}{\hat{a} + \hat{b} + N_i}$$

Esta média *a posteriori* pode ser vista como uma média ponderada entre a estimativa de máxima verossimilhança local (MLE) e a média *a priori*:

$$E[\theta_i|x_i, \hat{a}, \hat{b}] = B_i \bar{\theta} + (1 - B_i) \frac{x_i}{N_i}$$

onde $\bar{\theta} = \frac{\hat{a}}{\hat{a} + \hat{b}}$ é a média *a priori*, e $B_i = \frac{\hat{a}+\hat{b}}{\hat{a} + \hat{b} + N_i}$ controla o grau de *shrinkage* em direção à média *a priori*. Se $N_i$ é grande, $B_i$ será pequeno e a média *a posteriori* será próxima da MLE local. Se $N_i$ é pequeno, $B_i$ será grande e a média *a posteriori* será puxada em direção à média *a priori* [^81].

#### Vantagens do Empirical Bayes
O Empirical Bayes oferece uma abordagem pragmática para a estimação de parâmetros em modelos hierárquicos, permitindo que os dados informem a escolha da distribuição *a priori* [^44]. Isso pode levar a estimativas mais precisas, especialmente quando os dados são escassos [^81]. No entanto, é importante notar que o Empirical Bayes viola o princípio Bayesiano de que a *a priori* deve ser escolhida independentemente dos dados [^44]. Apesar disso, pode ser visto como uma aproximação computacionalmente eficiente da inferência Bayesiana hierárquica [^44].

### Conclusão
Neste capítulo, exploramos o uso do modelo beta-binomial para ilustrar o método Empirical Bayes. Vimos como a estimação dos hiperparâmetros da *a priori* beta através da maximização da verossimilhança marginal leva a uma média *a posteriori* que é uma média ponderada entre a MLE local e a média *a priori* [^44]. Essa abordagem é particularmente útil em situações onde os dados são escassos, permitindo que os dados informem a escolha da *a priori* e levando a estimativas mais precisas [^81].

### Referências
[^44]: Capítulo 5, Bayesian statistics, página 157, 173
[^79]: Capítulo 5, Bayesian statistics, página 157, 172, 173
[^81]: Capítulo 5, Bayesian statistics, página 160, 173, 174
<!-- END -->