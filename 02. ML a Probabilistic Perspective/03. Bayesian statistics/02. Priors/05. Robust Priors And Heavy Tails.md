## Robust Priors: Mitigating Undue Influence in Bayesian Inference

### Introdução
Em estatística Bayesiana, a escolha da **prior** desempenha um papel fundamental na formulação do modelo e na obtenção da distribuição *posterior*. A *prior* representa nosso conhecimento ou crenças *a priori* sobre os parâmetros do modelo, e a *posterior* combina essa informação *a priori* com os dados observados para fornecer uma estimativa atualizada dos parâmetros [^5]. No entanto, a influência da *prior* sobre a *posterior* pode ser problemática, especialmente quando a *prior* é muito informativa ou quando os dados são escassos. Para mitigar essa influência indevida, priors **robustas**, como as priors de Cauchy, são frequentemente empregadas [^20].

### Conceitos Fundamentais
**Priors robustas** são caracterizadas por **caudas pesadas** [^20]. Isso significa que elas atribuem uma probabilidade relativamente alta a valores extremos dos parâmetros, permitindo que os dados dominem a *posterior* e evitando que a *posterior* seja excessivamente influenciada pela média *a priori* [^20]. Em outras palavras, priors robustas são menos sensíveis a *outliers* nos dados e produzem inferências mais estáveis e confiáveis.

Um exemplo comum de prior robusta é a **prior de Cauchy**, também conhecida como distribuição de Cauchy ou distribuição de Lorentz [^20]. A densidade de probabilidade da distribuição de Cauchy é dada por:

$$ p(\theta) = \frac{1}{\pi \gamma \left[1 + \left(\frac{\theta - \theta_0}{\gamma}\right)^2\right]} $$

onde $\theta_0$ é o parâmetro de localização (mediana) e $\gamma$ é o parâmetro de escala. A prior de Cauchy tem caudas mais pesadas do que a prior Gaussiana, o que a torna menos sensível a *outliers*.

Para ilustrar o efeito das priors robustas, considere o exemplo apresentado na página 168 [^20]. Suponha que observamos $x \sim N(\theta, 1)$ e queremos estimar $\theta$. A *MLE* (estimativa de máxima verossimilhança) é, obviamente, $\theta = 5$, o que parece razoável [^20]. A média *posterior* sob uma *prior* uniforme também é $\theta = 5$ [^20]. No entanto, suponha que sabemos que a mediana *a priori* é 0 e que os quantis *a priori* estão em -1 e 1, de modo que $p(\theta \le -1) = p(-1 < \theta < 0) = p(0 < \theta < 1) = p(1 < \theta) = 0.25$ [^20]. Suponha também que a *prior* seja suave e unimodal [^20].

Pode-se mostrar facilmente que uma *prior* Gaussiana da forma $N(0, 0, 2.19^2)$ satisfaz essas restrições *a priori* [^20]. Mas, nesse caso, a média *posterior* é dada por 3.43, o que não parece muito satisfatório [^20]. Agora, suponha que usamos uma *prior* de Cauchy $T(0|0, 1, 1)$ [^20]. Isso também satisfaz as restrições *a priori* do nosso exemplo [^20]. Mas desta vez encontramos (usando integração numérica: ver `robustPriorDemo` para o código) que a média *posterior* é de cerca de 4.6, o que parece muito mais razoável [^20].

### Conclusão

As priors robustas, como a prior de Cauchy, oferecem uma abordagem valiosa para mitigar a influência indevida da prior em inferências Bayesianas [^20]. Suas caudas pesadas permitem que os dados dominem a posterior, levando a estimativas mais estáveis e confiáveis, especialmente em presença de *outliers*. Ao escolher uma prior robusta, é importante considerar as características específicas do problema em questão e selecionar uma prior que equilibre o conhecimento *a priori* com a capacidade de aprendizado a partir dos dados.

### Referências
[^5]: Página 1, Capítulo 5
[^20]: Página 168, Capítulo 5
<!-- END -->