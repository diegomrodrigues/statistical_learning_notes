## Hierarchical Bayes: Learning Priors from Data

### Introdução
Expandindo sobre o conceito de priors em Bayesian statistics, este capítulo aborda o Hierarchical Bayes, uma técnica que permite a aprendizagem de priors diretamente dos dados [^1]. Como vimos anteriormente, a escolha de um prior influencia significativamente a inferência Bayesiana. O Hierarchical Bayes oferece uma abordagem para mitigar essa dependência, colocando priors em *hiperparâmetros* dos priors, permitindo que os dados guiem o aprendizado dos parâmetros do prior e promovendo o *borrowing statistical strength* entre parâmetros relacionados [^1]. Este capítulo detalha os fundamentos, aplicações e vantagens do Hierarchical Bayes.

### Conceitos Fundamentais
O Hierarchical Bayes, também conhecido como modelo multinível, introduz uma estrutura hierárquica de priors. Em vez de fixar os priors, eles são governados por *hiperparâmetros*, que por sua vez podem ter seus próprios priors, criando uma hierarquia [^1].

**Estrutura Hierárquica:**
1. **Dados:** $D$
2. **Parâmetros:** $\theta$, que modelam os dados $p(D|\theta)$
3. **Prior:** $p(\theta|\eta)$, onde $\eta$ são os *hiperparâmetros*.
4. **Hiperprior:** $p(\eta)$, um prior sobre os hiperparâmetros.

A inferência Bayesiana envolve calcular a distribuição posterior conjunta:
$$
p(\theta, \eta | D) \propto p(D | \theta) p(\theta | \eta) p(\eta)
$$

**Borrowing Statistical Strength:**
Uma das principais vantagens do Hierarchical Bayes é a capacidade de *borrow statistical strength*. Isso é particularmente útil quando se tem poucos dados para certos parâmetros, mas dados abundantes para outros parâmetros relacionados [^1]. O modelo compartilha informações entre os parâmetros através do prior hierárquico, resultando em estimativas mais robustas.

**Exemplo: Modelando taxas de câncer relacionadas [^5]**
Considere o problema de prever taxas de câncer em várias cidades. Suponha que medimos o número de pessoas em várias cidades, $N_i$, e o número de pessoas que morreram de câncer nessas cidades, $x_i$ [^5]. Assumimos que $x_i \sim Bin(N_i, \theta_i)$, e queremos estimar as taxas de câncer $\theta_i$ [^5].

Uma abordagem seria estimar cada $\theta_i$ separadamente, mas isso sofre do problema de *sparse data* (subestimação da taxa devido a pequenos $N_i$) [^5]. Outra abordagem é assumir que todos os $\theta_i$ são iguais, chamado de *parameter tying* [^5]. No entanto, assumir que todas as cidades têm a mesma taxa é uma suposição forte.

Uma abordagem de compromisso é assumir que as $\theta_i$ são similares, mas que pode haver variações específicas da cidade [^5]. Isso pode ser modelado assumindo que as $\theta_i$ são sorteadas de uma distribuição comum, digamos $\theta_i \sim Beta(a, b)$ [^5]. A distribuição conjunta completa pode ser escrita como:

$$
p(D, \theta, \eta | N) = p(\eta) \prod_{i=1}^{N} Bin(x_i | N_i, \theta_i) Beta(\theta_i | \eta)
$$

onde $\eta = (a, b)$ [^5]. É crucial inferir $\eta = (a, b)$ dos dados; se simplesmente definirmos para um constante, os $\theta_i$ serão condicionalmente independentes, e não haverá fluxo de informação entre eles [^5]. Ao tratar $\eta$ como uma variável desconhecida (oculta), permitimos que as cidades com poucos dados *borrow statistical strength* daquelas com mais dados [^5].

**Empirical Bayes:**
No contexto do exemplo das taxas de câncer, podemos integrar analiticamente os $\theta_i$ e escrever a verossimilhança marginal diretamente, como mostrado na seção 5.6.1 [^5]:
$$
p(D | a, b) = \prod_i \frac{B(a + x_i, b + N_i - x_i)}{B(a, b)}
$$
Várias maneiras de maximizar isso em relação a *a* e *b* são discutidas em (Minka 2000e) [^5]. Tendo estimado *a* e *b*, podemos ligar os hiperparâmetros para calcular o posterior $p(\theta_i | a, b, D)$ da maneira usual, usando a análise conjugada [^5]. O resultado líquido é que a média posterior de cada $\theta_i$ é uma média ponderada de seu MLE local e dos meios anteriores, o que depende de $\eta = (a, b)$ [^5]; mas como *n* é estimado com base em todos os dados, cada $\theta_i$ é influenciado por todos os dados [^5].

**Cálculo da Evidência Marginal:**
O cálculo da evidência marginal, $p(D|\eta)$, é fundamental no Hierarchical Bayes. Este termo representa a probabilidade dos dados dado os hiperparâmetros, integrando sobre todos os valores possíveis dos parâmetros $\theta$ [^5]. A equação 5.37 [^5] fornece uma aproximação para $p(D|m)$ que envolve otimizar em vez de integrar:
$$
p(D\m) \approx \int p(D\w)p(w|a,m)dw
$$
onde
$$
â = argmax \int p(D\w)p(w/a,m)dw
$$

**Priors Não Informativos e Jeffreys Priors:**
Quando não temos fortes crenças sobre o que $\theta$ deve ser, é comum usar um prior não informativo [^4]. O problema de projetar priors não informativos é, na verdade, um pouco complicado [^4]. Como um exemplo da dificuldade, considere um parâmetro de Bernoulli, $\theta \in [0,1]$ [^4]. Pode-se pensar que o prior mais não informativo seria a distribuição uniforme, $Beta(1,1)$ [^4]. Mas a média posterior neste caso é $E[\theta|D] = \frac{N_1+1}{N_1+N_0+2}$, enquanto o MLE é $\frac{N_1}{N_1+N_0}$ [^4]. Harold Jeffreys projetou uma técnica de propósito geral para criar priors não informativos [^4]. O resultado é conhecido como Jeffreys prior [^4].

### Conclusão
O Hierarchical Bayes oferece uma estrutura poderosa para incorporar conhecimento prévio enquanto permite que os dados influenciem a inferência [^1, 5]. Ao modelar a incerteza nos priors, podemos obter estimativas mais robustas e precisas, especialmente em situações com dados limitados. O conceito de *borrowing statistical strength* [^1] é fundamental para entender a eficácia do Hierarchical Bayes em modelar fenômenos relacionados. A flexibilidade do Hierarchical Bayes o torna uma ferramenta valiosa em várias aplicações estatísticas, desde modelagem de taxas de câncer [^5] até sistemas de recomendação e aprendizado de máquina [^5].

### Referências
[^1]: Página 1, "Hierarchical Bayes involves placing a prior on the hyper-parameters of the prior, enabling data-driven learning of prior parameters and borrowing statistical strength across related parameters."
[^4]: Página 165, seção 5.4.1
[^5]: Página 171, seção 5.5.1

<!-- END -->