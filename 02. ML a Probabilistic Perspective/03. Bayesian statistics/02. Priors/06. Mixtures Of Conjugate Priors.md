## Mixtures of Conjugate Priors

### Introdução
Em estatística Bayesiana, a escolha da *prior* é um passo crucial que influencia a inferência *posterior*. Enquanto *priors* não informativas buscam minimizar a influência das crenças prévias, *priors* informativas permitem incorporar conhecimento prévio ao modelo. No entanto, *priors* informativas podem ser difíceis de especificar e computacionalmente intensivas. Uma abordagem que combina os benefícios da conveniência computacional e da flexibilidade é o uso de **misturas de *priors* conjugadas** [^20]. Este capítulo explora em detalhes as propriedades e aplicações dessas misturas.

### Conceitos Fundamentais

**Priors Conjugadas** são aquelas que, quando combinadas com uma dada função de verossimilhança, resultam em uma distribuição *posterior* que pertence à mesma família da *prior*. Essa propriedade simplifica significativamente os cálculos *posteriores*, pois a forma funcional da *posterior* é conhecida a priori.

**Misturas de Priors** são combinações lineares de duas ou mais distribuições *priors*. Formalmente, uma mistura de *priors* pode ser expressa como:
$$\np(\theta) = \sum_{k=1}^{K} \pi_k p_k(\theta)\n$$
onde $\pi_k$ são os pesos de mistura, satisfazendo $\sum_{k=1}^{K} \pi_k = 1$, e $p_k(\theta)$ são as densidades das *priors* componentes. A flexibilidade das misturas de *priors* permite aproximar uma ampla gama de crenças prévias [^20].

A principal vantagem das misturas de *priors* conjugadas reside no fato de que a *posterior* resultante também é uma mistura de distribuições conjugadas. Para demonstrar isso, considere uma verossimilhança $p(D|\theta)$ e uma *prior* que é uma mistura de *priors* conjugadas:
$$\np(\theta) = \sum_{k=1}^{K} \pi_k p_k(\theta)\n$$
A *posterior* é então dada por:
$$\np(\theta|D) \propto p(D|\theta) p(\theta) = p(D|\theta) \sum_{k=1}^{K} \pi_k p_k(\theta) = \sum_{k=1}^{K} \pi_k p(D|\theta) p_k(\theta)\n$$
Como $p_k(\theta)$ são *priors* conjugadas, $p(D|\theta) p_k(\theta) \propto p_k(\theta|D)$, que também é uma distribuição conjugada. Portanto,
$$\np(\theta|D) \propto \sum_{k=1}^{K} \pi_k p_k(\theta|D)\n$$
Essa equação mostra que a *posterior* é uma mistura de distribuições conjugadas, onde os pesos de mistura são atualizados com base nos dados. Os pesos *posteriores* da mistura são dados por [^21]:
$$\np(Z = k|D) = \frac{p(Z = k)p(D|Z = k)}{\sum_{k'} p(Z = k')p(D|Z = k')}\n$$
onde $p(Z=k)$ são os pesos de mistura *prior*, e $p(D|Z=k)$ é a verossimilhança marginal para o componente *k* da mistura, que pode ser calculada analiticamente devido à conjugação.

**Exemplo:** Considere o modelo Beta-Binomial, onde a verossimilhança é binomial e a *prior* é uma mistura de distribuições Beta [^21]:
$$\np(\theta) = 0.5 Beta(\theta|a_1, b_1) + 0.5 Beta(\theta|a_2, b_2)\n$$
A *posterior* resultante, após observar $N_1$ sucessos e $N_0$ falhas, é:
$$\np(\theta|D) = p(Z = 1|D)Beta(\theta|a_1 + N_1, b_1 + N_0) + p(Z = 2|D)Beta(\theta|a_2 + N_1, b_2 + N_0)\n$$
onde $p(Z = k|D)$ são os pesos *posteriores* da mistura, calculados usando a Equação (5.70) do texto fonte.

### Conclusão

As misturas de *priors* conjugadas oferecem uma abordagem poderosa e flexível para a modelagem Bayesiana. Elas combinam a conveniência computacional das *priors* conjugadas com a capacidade de aproximar uma ampla gama de crenças prévias. Ao usar misturas de *priors* conjugadas, é possível obter inferências *posteriores* analiticamente tratáveis, mantendo a flexibilidade para incorporar conhecimento prévio ao modelo. Este compromisso entre eficiência computacional e capacidade de codificar conhecimento prévio torna as misturas de *priors* conjugadas uma ferramenta valiosa na análise Bayesiana [^20].

### Referências
[^20]: Mixtures of conjugate priors combine the benefits of computational convenience and flexibility, allowing approximation of any prior and simplifying posterior computations. They are also conjugate. Conjugate priors simplify computation, while mixtures can approximate a wide range of prior beliefs. This approach offers a good compromise between computational efficiency and the ability to encode prior knowledge.
[^21]: p(Z = k|D) = \frac{p(Z = k)p(D|Z = k)}{\sum_{k'} p(Z = k')p(D|Z = k')}
<!-- END -->