## Jeffreys Priors: A General Approach to Non-Informative Priors

### Introdução
Dentro do contexto da estatística Bayesiana, a escolha de uma **prior** (distribuição *a priori*) é fundamental para inferência. Quando se deseja minimizar a influência das crenças subjetivas e *deixar os dados falarem por si mesmos*, o uso de **priors não-informativas** torna-se relevante [^17]. No entanto, a construção de priors não-informativas é uma tarefa sutil e complexa. Uma abordagem geral para construir priors não-informativas é a utilização dos **Jeffreys priors** [^16]. Este capítulo detalha a construção e as propriedades dos Jeffreys priors, incluindo sua invariância sob reparametrização.

### Conceitos Fundamentais
**Jeffreys priors** fornecem uma técnica de propósito geral para a criação de priors não-informativas, baseada na **informação de Fisher** e invariante à reparametrização [^16].

A principal observação é que se $p(\phi)$ é não-informativa, então qualquer reparametrização da prior, como $\theta = h(\phi)$ para alguma função $h$, também deve ser não-informativa [^16]. No entanto, pela fórmula de mudança de variáveis,
$$
p_\theta(\theta) = p_\phi(\phi) \left| \frac{d\phi}{d\theta} \right|
$$
assim, a prior irá, em geral, mudar [^16]. Para evitar isso, podemos escolher
$$
p_\phi(\phi) \propto \sqrt{I(\phi)}
$$
onde $I(\phi)$ é a **informação de Fisher**:
$$
I(\phi) \triangleq - \mathbb{E} \left[ \frac{d^2 \log p(x|\phi)}{d\phi^2} \right] = \mathbb{E} \left[ \left( \frac{d \log p(x|\phi)}{d\phi} \right)^2 \right]
$$
Esta é uma medida da curvatura da log-verossimilhança negativa esperada e, portanto, uma medida da estabilidade do estimador de máxima verossimilhança (MLE) [^16].

Usando a regra da cadeia, temos
$$
\frac{d \log p(x|\theta)}{d\theta} = \frac{d \log p(x|\phi)}{d\phi} \frac{d\phi}{d\theta}
$$
Elevando ao quadrado e tomando as expectativas sobre $x$, temos
$$
I(\theta) = \mathbb{E} \left[ \left( \frac{d \log p(x|\theta)}{d\theta} \right)^2 \right] = I(\phi) \left( \frac{d\phi}{d\theta} \right)^2
$$
Portanto, encontramos que a prior transformada é
$$
p_\theta(\theta) = p_\phi(\phi) \left| \frac{d\phi}{d\theta} \right| \propto \sqrt{I(\phi)} \left| \frac{d\phi}{d\theta} \right| = \sqrt{I(\theta)}
$$
Assim, $p_\theta(\theta)$ e $p_\phi(\phi)$ são os mesmos [^16].

#### Jeffreys Prior para Bernoulli e Multinoulli
Suponha que $X \sim \text{Ber}(\theta)$. A log-verossimilhança para uma única amostra é
$$
\log p(X|\theta) = X \log \theta + (1 - X) \log (1 - \theta)
$$
A **função score** é apenas o gradiente da log-verossimilhança:
$$
s(\theta) \triangleq \frac{d}{d\theta} \log p(X|\theta) = \frac{X}{\theta} - \frac{1-X}{1-\theta}
$$
A **informação observada** é a segunda derivada da log-verossimilhança:
$$
J(\theta) = \frac{d^2}{d\theta^2} \log p(X|\theta) = -s'(\theta|X) = -\frac{X}{\theta^2} - \frac{1-X}{(1-\theta)^2}
$$
A informação de Fisher é a informação esperada:
$$
I(\theta) = \mathbb{E}[J(\theta|X)|X \sim \theta] = \frac{\theta}{\theta^2} + \frac{1-\theta}{(1-\theta)^2} = \frac{1}{\theta(1-\theta)}
$$
Portanto, o Jeffreys prior é
$$
p(\theta) \propto \theta^{-\frac{1}{2}}(1-\theta)^{-\frac{1}{2}} = \frac{1}{\sqrt{\theta(1-\theta)}} \propto \text{Beta}(\frac{1}{2}, \frac{1}{2})
$$
Agora, considere uma variável aleatória multinoulli com $K$ estados. Pode-se mostrar que o Jeffreys prior é dado por
$$
p(\theta) \propto \text{Dir}(\frac{1}{2},...,\frac{1}{2})
$$
Observe que isso é diferente das escolhas mais óbvias de $\text{Dir}(\frac{1}{K},...,\frac{1}{K})$ ou $\text{Dir}(1,...,1)$ [^19].

#### Jeffreys Prior para Parâmetros de Localização e Escala
Pode-se mostrar que o Jeffreys prior para um parâmetro de localização, como a média Gaussiana, é $p(\mu) \propto 1$ [^19]. Isso é um exemplo de uma **prior invariante translacional**, que satisfaz a propriedade de que a massa de probabilidade atribuída a qualquer intervalo $[A, B]$ é a mesma que a atribuída a qualquer outro intervalo deslocado da mesma largura, como $[A - c, B - c]$ [^19].

Similarmente, pode-se mostrar que o Jeffreys prior para um parâmetro de escala, como a variância Gaussiana, é $p(\sigma^2) \propto 1/\sigma^2$ [^19]. Isso é um exemplo de um prior invariante de escala, que satisfaz a propriedade de que a massa de probabilidade atribuída a qualquer intervalo $[A, B]$ é a mesma que a atribuída a qualquer outro intervalo $[A/c, B/c]$ que é escalado em tamanho por algum fator constante $c > 0$ [^19].

### Conclusão

Os Jeffreys priors oferecem uma abordagem sistemática para a construção de priors não-informativas em problemas de inferência Bayesiana [^16]. Ao basear a prior na informação de Fisher, eles garantem a invariância sob reparametrização, uma propriedade desejável para evitar a influência arbitrária da escolha da parametrização. Embora os Jeffreys priors sejam frequentemente usados como priors padrão não-informativas, é importante notar que eles nem sempre são apropriados para todos os problemas, e outras abordagens, como priors robustas ou priors de mistura conjugada, podem ser mais adequadas em certos casos [^20, 21].

### Referências
[^16]: Bayesian statistics, Page 166, Jeffreys priors *.
[^17]: Bayesian statistics, Page 165, Uninformative priors.
[^18]: Bayesian statistics, Page 166, which is a mixture of two equal point masses at 0 and 1 (see (Zhu and Lu 2004)). This is also called the Haldane prior.
[^19]: Bayesian statistics, Page 167, so we find the transformed prior is.
[^20]: Bayesian statistics, Page 168, Robust priors.
[^21]: Bayesian statistics, Page 169, Mixtures of conjugate priors.

<!-- END -->