## Sumarização da Distribuição Posterior em Estatística Bayesiana

### Introdução
Em estatística Bayesiana, a **distribuição posterior** $p(\theta|D)$ é fundamental para resumir o conhecimento sobre variáveis desconhecidas $\theta$, dado um conjunto de dados observados $D$ [^1]. Esta distribuição condensa toda a informação disponível sobre os parâmetros, combinando o conhecimento *a priori* com a evidência fornecida pelos dados [^1]. Este capítulo explora em detalhe como a distribuição posterior é utilizada para inferência estatística, contrastando com as abordagens frequentistas e abordando métodos para sua sumarização.

### Conceitos Fundamentais
A **distribuição posterior** é calculada utilizando o teorema de Bayes:
$$
p(\theta|D) = \frac{p(D|\theta)p(\theta)}{p(D)}
$$
onde:
- $p(\theta|D)$ é a distribuição posterior dos parâmetros $\theta$ dados os dados $D$ [^1].
- $p(D|\theta)$ é a *verossimilhança* dos dados $D$ dado os parâmetros $\theta$.
- $p(\theta)$ é a distribuição *a priori* dos parâmetros $\theta$.
- $p(D)$ é a evidência ou *marginal likelihood*, calculada como $$p(D) = \int p(D|\theta)p(\theta) d\theta$$.

A distribuição posterior oferece uma representação intuitiva e visualizável do que se sabe sobre as quantidades desconhecidas, em comparação com a distribuição conjunta completa [^1].

#### Sumarização da Distribuição Posterior
A distribuição posterior $p(\theta|D)$ resume tudo o que sabemos sobre as quantidades desconhecidas $\theta$ [^1].  Em vez de trabalhar diretamente com a distribuição posterior completa, é comum derivar **estatísticas sumárias** que são mais fáceis de entender e visualizar [^1].

##### Estimativas Pontuais
Uma maneira de sumarizar a distribuição posterior é através de **estimativas pontuais**, como a média, mediana ou moda posterior [^1].  A escolha entre esses métodos pode ser informada pela **teoria da decisão**, que envolve a especificação de uma função de perda $L(\theta, \hat{\theta})$, onde $\theta$ é o valor verdadeiro e $\hat{\theta}$ é a estimativa [^1, 2].

1.  **Moda Posterior (MAP - Maximum A Posteriori):** A moda posterior, também conhecida como a estimativa MAP, é o valor de $\theta$ que maximiza a distribuição posterior [^1]:
    $$
    \hat{\theta}_{MAP} = \arg \max_{\theta} p(\theta|D)
    $$
    A estimativa MAP é popular devido à sua interpretação como um problema de otimização, para o qual existem algoritmos eficientes [^1]. Além disso, a estimativa MAP pode ser interpretada em termos não Bayesianos, considerando o logaritmo do *a priori* como um regularizador [^1]. No entanto, a estimativa MAP tem várias desvantagens [^1, 2]:
    *   Não fornece uma medida de **incerteza** [^2].
    *   Pode levar a **overfitting** se a incerteza nos parâmetros não for modelada adequadamente [^2].
    *   Pode ser um ponto **atípico** da distribuição [^2].
    *   Não é **invariante** à reparametrização [^3].
2.  **Média Posterior:** Para quantidades com valores contínuos, a média posterior é frequentemente preferida, especialmente quando se utiliza uma função de perda de erro quadrático [^3]:
    $$
    \hat{\theta}_{Mean} = \mathbb{E}[\theta|D] = \int \theta p(\theta|D) d\theta
    $$
3.  **Mediana Posterior:** A mediana posterior é mais robusta a outliers e é a estimativa ótima quando se utiliza uma função de perda mais robusta, como $L(\theta, \hat{\theta}) = |\theta - \hat{\theta}|$ [^3].

##### Intervalos Credíveis
Além das estimativas pontuais, é importante quantificar a incerteza associada às estimativas. Uma medida padrão de confiança é o **intervalo credível** [^4]. Um intervalo credível de 100(1 - α)% é uma região *C* = (l, u) que contém 1 - α da massa de probabilidade posterior [^4]:
$$
P(l < \theta \le u | D) = 1 - \alpha
$$
onde *l* e *u* representam os limites inferior e superior, respectivamente [^4].

Existem vários tipos de intervalos credíveis:
*   **Intervalos Centrais:** São construídos de forma que haja α/2 de massa em cada cauda da distribuição posterior [^4].
*   **Regiões de Maior Densidade Posterior (HPD - Highest Posterior Density):**  As regiões HPD são definidas como o conjunto de pontos mais prováveis que, no total, constituem 100(1 - α)% da massa de probabilidade [^5]. Formalmente, encontramos um limiar $p^*$ tal que:
    $$
    1 - \alpha = \int_{\{\theta: p(\theta|D) > p^*\}} p(\theta|D) d\theta
    $$
    e então definimos a região HPD como [^5]:
    $$
    C_{\alpha}(D) = \{\theta: p(\theta|D) \ge p^*\}
    $$
    Ao contrário dos intervalos centrais, as regiões HPD garantem que todos os pontos dentro do intervalo têm uma densidade de probabilidade maior do que qualquer ponto fora do intervalo [^5].

    **Diferenças entre Intervalos Credíveis e Intervalos de Confiança Frequentistas:** É crucial notar que intervalos credíveis Bayesianos e intervalos de confiança frequentistas são conceitos distintos [^5]. Intervalos credíveis fornecem uma probabilidade direta de que o parâmetro esteja dentro do intervalo, enquanto intervalos de confiança fornecem uma probabilidade sobre o desempenho do procedimento de amostragem em muitos conjuntos de dados hipotéticos [^5].

### Conclusão
A distribuição posterior é uma ferramenta central na estatística Bayesiana, permitindo a incorporação de conhecimento *a priori* e a quantificação da incerteza nas estimativas [^1].  Estatísticas sumárias, como estimativas pontuais e intervalos credíveis, fornecem maneiras práticas de resumir e interpretar a informação contida na distribuição posterior [^1].  A escolha da estatística sumária apropriada depende do contexto específico e dos objetivos da análise [^1, 2, 3, 4, 5].

### Referências
[^1]: Página 1 do contexto.
[^2]: Página 2 do contexto.
[^3]: Página 3 do contexto.
[^4]: Página 4 do contexto.
[^5]: Página 5 do contexto.
<!-- END -->