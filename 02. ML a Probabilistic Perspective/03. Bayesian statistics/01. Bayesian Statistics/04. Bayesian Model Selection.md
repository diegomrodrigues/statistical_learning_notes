## Bayesian Model Selection

### Introdução
Este capítulo aborda a **seleção de modelos Bayesianos**, um tópico crucial na estatística Bayesiana, que lida com o desafio de escolher o melhor modelo de um conjunto de modelos com diferentes níveis de complexidade [^1]. A seleção de modelos é uma questão fundamental em estatística, pois a escolha de um modelo inadequado pode levar a conclusões errôneas e previsões imprecisas. Como mencionado na introdução do capítulo [^1], o uso da distribuição posterior está no cerne da estatística Bayesiana. Assim, a seleção do modelo é feita utilizando a probabilidade posterior sobre os modelos.

### Conceitos Fundamentais

A **seleção de modelos Bayesianos** visa encontrar o modelo que melhor equilibra a capacidade de ajuste aos dados com a complexidade do modelo, evitando *overfitting* [^1]. Existem duas abordagens principais para a seleção de modelos:

1.  **Validação cruzada (Cross-validation):** Estima o erro de generalização de cada modelo candidato. No entanto, essa abordagem pode ser computacionalmente intensiva, pois requer o ajuste de cada modelo *K* vezes, onde *K* é o número de *folds* na validação cruzada [^1].
2.  **Cálculo da posterior sobre os modelos:** Computa a probabilidade posterior de cada modelo dado os dados, *p(m|D)*, usando a **verossimilhança marginal** (marginal likelihood), também conhecida como *integrated likelihood* ou *evidence* para o modelo *m* [^1]. A verossimilhança marginal é definida como a integral da verossimilhança do modelo ponderada pela *prior* dos parâmetros:

    $$     p(D|m) = \int p(D|\theta)p(\theta|m)d\theta     $$

    onde:

    *   *p(D|m)* é a verossimilhança marginal do modelo *m*
    *   *p(D|θ)* é a verossimilhança dos dados dado os parâmetros *θ*
    *   *p(θ|m)* é a distribuição *prior* dos parâmetros *θ* para o modelo *m*

A abordagem Bayesiana incorpora naturalmente o **princípio da navalha de Occam** [^1], que favorece modelos mais simples que explicam adequadamente os dados. Isso ocorre porque a verossimilhança marginal integra os parâmetros, em vez de maximizá-los. Modelos complexos, capazes de prever muitos resultados possíveis, precisam diluir sua massa de probabilidade, resultando em menor probabilidade para qualquer conjunto de dados específico em comparação com modelos mais simples [^1].
A probabilidade posterior de um modelo *m* dado os dados *D* é calculada usando o teorema de Bayes:

$$ p(m|D) = \frac{p(D|m)p(m)}{\sum_{m' \in M} p(D|m')p(m')} $$

onde:

*   *p(m|D)* é a probabilidade *a posteriori* do modelo *m* dado os dados *D*
*   *p(D|m)* é a verossimilhança marginal do modelo *m*
*   *p(m)* é a probabilidade *a priori* do modelo *m*
*   A soma no denominador é sobre todos os modelos possíveis no conjunto *M*.

Sob uma *prior* uniforme sobre os modelos, *p(m)* ∝ 1, a seleção do modelo se resume a escolher o modelo que maximiza a verossimilhança marginal [^1]:

$$ \hat{m} = \underset{m}{\text{argmax}} \ p(D|m) $$

A **navalha de Occam Bayesiana** surge do fato de que modelos mais complexos devem espalhar sua probabilidade sobre um espaço maior de possíveis conjuntos de dados, resultando em uma verossimilhança marginal menor para qualquer conjunto de dados específico [^1].

O **Bayesian Information Criterion (BIC)** [^1] é uma aproximação da verossimilhança marginal logarítmica que penaliza a complexidade do modelo e evita o *overfitting*. Ele tem a forma de uma verossimilhança logarítmica penalizada, onde o termo de penalidade depende da complexidade do modelo. O BIC é definido como:

$$ BIC = \log p(D|\hat{\theta}) - \frac{dof(\theta)}{2} \log N $$

onde:

*   *p(D|\hat{\theta})* é a verossimilhança dos dados dado a estimativa de máxima verossimilhança (MLE) dos parâmetros
*   *dof(θ)* é o número de graus de liberdade (parâmetros) no modelo
*   *N* é o número de observações

Alternativamente, o BIC pode ser expresso como um custo a ser minimizado:

$$ BIC_{cost} = -2 \log p(D|\hat{\theta}) + dof(\theta) \log N $$
No contexto da regressão linear, o BIC cost pode ser expresso como:
$$ BIC_{cost} = N \log(\hat{\sigma}^2) + D \log(N) $$
onde $\hat{\sigma}^2$ é a variância estimada dos resíduos e $D$ é o número de variáveis no modelo [^1].

### Conclusão

A seleção de modelos Bayesianos oferece uma estrutura coerente para escolher entre modelos de diferentes complexidades, equilibrando a capacidade de ajuste aos dados com a necessidade de evitar *overfitting*. Ao computar a probabilidade *a posteriori* dos modelos utilizando a verossimilhança marginal, a abordagem Bayesiana incorpora naturalmente o princípio da navalha de Occam. O BIC fornece uma aproximação útil da verossimilhança marginal logarítmica, permitindo a seleção eficiente de modelos em diversas aplicações.

### Referências
[^1]: Página 155-156, Bishop, Christopher. *Pattern Recognition and Machine Learning*. Springer, 2006.
<!-- END -->