## Inferência de Hiperparâmetros em Modelos Hierárquicos Bayesianos
### Introdução
Este capítulo aprofunda a inferência de hiperparâmetros em modelos hierárquicos Bayesianos, com foco na aplicação ao modelo Beta-Binomial. Em modelos hierárquicos, a distribuição a *priori* dos parâmetros é governada por hiperparâmetros, denotados aqui por $\eta$. A capacidade de inferir esses hiperparâmetros a partir dos dados é crucial para permitir que os dados informem a estrutura do modelo, em vez de depender de escolhas arbitrárias do usuário [^23]. Este capítulo se baseia nos conceitos apresentados anteriormente sobre modelos Bayesianos, estimativa MAP e distribuições *a priori* conjugadas [^1].

### Modelo Beta-Binomial Hierárquico
Considere um cenário onde observamos dados binários $D = \\{x_i\\}_{i=1}^N$, onde $x_i$ representa o número de sucessos em $N_i$ tentativas. Podemos modelar isso usando uma distribuição binomial, onde a probabilidade de sucesso $\theta_i$ varia entre as observações. Em vez de tratar cada $\theta_i$ como independente, impomos uma estrutura hierárquica, assumindo que eles são amostrados de uma distribuição Beta comum, com hiperparâmetros $\eta = (a, b)$. A distribuição conjunta completa pode ser escrita como [^23]:
$$p(D, \theta, \eta|N) = p(\eta) \prod_{i=1}^N Bin(x_i|N_i, \theta_i) Beta(\theta_i|\eta)$$
onde $Bin(x_i|N_i, \theta_i)$ é a distribuição binomial e $Beta(\theta_i|\eta)$ é a distribuição Beta. O objetivo principal é inferir $\eta = (a, b)$ a partir dos dados [^23].

#### Inferência de Hiperparâmetros
A inferência de $\eta$ pode ser abordada de diferentes maneiras. Uma abordagem comum é a **Bayesiana empírica** (EB), também conhecida como **maximização de verossimilhança do tipo II** [^173]. Nesta abordagem, aproximamos a distribuição posterior dos hiperparâmetros com uma estimativa pontual, $\hat{\eta} = \text{argmax}_{\eta} p(\eta|D)$ [^173]. Assumindo um *a priori* uniforme para $\eta$, isso se reduz a maximizar a verossimilhança marginal:

$$\hat{\eta} = \text{argmax}_{\eta} p(D|\eta) = \text{argmax}_{\eta} \int p(D|\theta)p(\theta|\eta) d\theta$$

Para o modelo Beta-Binomial, podemos integrar analiticamente os $\theta_i$\'s e escrever a verossimilhança marginal diretamente [^173]:

$$p(D|a, b) = \prod_{i=1}^N \frac{B(a + x_i, b + N_i - x_i)}{B(a, b)}$$

onde $B(\cdot, \cdot)$ é a função Beta [^173]. A estimativa de $\hat{\eta} = (\hat{a}, \hat{b})$ pode ser obtida maximizando esta verossimilhança usando métodos numéricos [^173]. Várias maneiras de maximizar isso em relação a $a$ e $b$ são discutidas em (Minka 2000e) [^173].

##### Exemplo: Taxas de Câncer
Como exemplo, considere o problema de prever as taxas de câncer em várias cidades [^171]. Suponha que medimos o número de pessoas em várias cidades, $N_i$, e o número de pessoas que morreram de câncer nessas cidades, $x_i$. Assumimos que $x_i \sim Bin(N_i, \theta_i)$, e queremos estimar as taxas de câncer $\theta_i$ [^171]. Em vez de estimar cada $\theta_i$ separadamente, assumimos que eles são extraídos de uma distribuição comum, digamos $\theta_i \sim Beta(a, b)$ [^171]. É crucial que inferirmos $\eta = (a, b)$ dos dados; se apenas o fixarmos em uma constante, os $\theta_i$ serão condicionalmente independentes e não haverá fluxo de informações entre eles [^171]. Ao tratar $\eta$ como uma variável desconhecida (oculta), permitimos que as cidades com poucos dados "peguem emprestado força estatística" daquelas ricas em dados [^171].

#### Alternativas Bayesianas Completas
Embora a EB seja computacionalmente eficiente, ela viola o princípio de que o *a priori* deve ser escolhido independentemente dos dados [^173]. Uma abordagem Bayesiana completa envolveria a especificação de um *a priori* em $\eta$, $p(\eta)$, e o cálculo da distribuição posterior conjunta [^173]:
$$p(\eta, \theta|D) \propto p(D|\theta)p(\theta|\eta)p(\eta)$$
A inferência completa exigiria a computação de integrais complexas, que geralmente são tratadas usando métodos de Monte Carlo [^3].

### Critérios de Seleção de Modelos
Ao escolher entre diferentes modelos hierárquicos, é necessário quantificar o *trade-off* entre a complexidade do modelo e o ajuste aos dados. Uma ferramenta comum é a verossimilhança marginal $p(D|m)$ [^156]. Além disso, critérios como o Critério de Informação Bayesiano (BIC) podem ser usados para aproximar a verossimilhança marginal [^161].

#### Navalha de Occam Bayesiana
A navalha de Occam Bayesiana é um princípio que afirma que se deve escolher o modelo mais simples que explica adequadamente os dados [^156]. Isso é alcançado pela verossimilhança marginal, que penaliza automaticamente modelos mais complexos [^156].

### Conclusão
A inferência de hiperparâmetros em modelos hierárquicos Bayesianos é um passo crítico para permitir que os dados informem a estrutura do modelo. As abordagens da Bayesiana empírica fornecem uma aproximação computacionalmente eficiente, enquanto as abordagens Bayesianas completas oferecem inferência mais consistente, mas exigem técnicas computacionais mais intensivas. A seleção do modelo pode ser informada pela verossimilhança marginal e por critérios relacionados, como o BIC, que incorpora o princípio da navalha de Occam Bayesiana.

### Referências
[^1]: 5 Bayesian statistics
[^23]: 5.5 Hierarchical Bayes
[^156]: 5.3 Bayesian model selection
[^161]: 5.3.2.4 BIC approximation to log marginal likelihood
[^171]: 5.5.1 Example: modeling related cancer rates
[^173]: 5.6 Empirical Bayes
<!-- END -->