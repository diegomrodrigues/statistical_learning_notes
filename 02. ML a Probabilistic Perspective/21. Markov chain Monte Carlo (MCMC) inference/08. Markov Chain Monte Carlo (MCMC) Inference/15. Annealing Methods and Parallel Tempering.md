## Annealing Methods in Markov Chain Monte Carlo (MCMC) Inference

### Introdução
Em continuidade aos métodos de Monte Carlo introduzidos no Capítulo 23, este capítulo explora técnicas avançadas para amostrar de distribuições complexas de alta dimensionalidade usando Markov Chain Monte Carlo (MCMC) [^1]. Especificamente, focaremos em **annealing methods**, como *simulated annealing*, *annealed importance sampling* e *parallel tempering*, que são projetadas para lidar com distribuições multimodais, onde os métodos MCMC tradicionais podem ter dificuldades em explorar o espaço de estados de forma eficiente [^22].

### Conceitos Fundamentais

#### Simulated Annealing
O **simulated annealing** é um algoritmo estocástico que busca o **ótimo global** de uma função *black-box* $f(x)$ [^24]. Inspirado pela termodinâmica, o simulated annealing utiliza um parâmetro de **temperatura computacional** $T$ para suavizar a distribuição de probabilidade e escapar de ótimos locais [^24]. A probabilidade de estar em um estado particular $x$ é dada pela **distribuição de Boltzmann**:

$$ p(x) \propto \exp(-f(x)/T) $$

À medida que a temperatura $T$ se aproxima de zero (o sistema é resfriado), o sistema passa mais tempo no estado de **mínima energia** (estado mais provável) [^24]. O algoritmo procede da seguinte forma [^24]:
1.  Propor um novo estado $x'$ com base em uma distribuição de proposta $q(x'|x_k)$
2.  Calcular $\alpha = \exp((f(x_k) - f(x'))/T)$ [^24]
3.  Aceitar o novo estado com probabilidade $\min(1, \alpha)$, caso contrário, permanecer no estado atual [^24]
4.  Reduzir gradualmente a temperatura $T$ de acordo com um **cooling schedule** [^24]

O **cooling schedule** determina a taxa na qual a temperatura é reduzida ao longo do tempo [^24]. Uma taxa de resfriamento muito rápida pode levar o algoritmo a ficar preso em um máximo local, enquanto uma taxa de resfriamento muito lenta pode ser computacionalmente ineficiente [^24]. Um **cooling schedule exponencial** comum é dado por $T_k = T_0 C^k$, onde $T_0$ é a temperatura inicial e $C$ é a taxa de resfriamento (tipicamente em torno de 0.8) [^24].

#### Annealed Importance Sampling
O **annealed importance sampling (AIS)** combina os princípios do *simulated annealing* e *importance sampling* para desenhar amostras independentes de distribuições difíceis [^24]. O AIS constrói uma sequência de distribuições intermediárias que conectam uma distribuição inicial fácil de amostrar $p_n(x) \propto f_n(x)$ a uma distribuição alvo $p_0(x) \propto f_0(x)$ [^24]. A sequência de distribuições intermediárias é definida como:

$$ f_j(x) = f_0(x)^{\beta_j} f_n(x)^{1-\beta_j} $$

onde $1 = \beta_0 > \beta_1 > ... > \beta_n = 0$ [^24]. Uma série de cadeias de Markov $T_i(x,x')$ que deixam cada $p_j$ invariante são usadas para transicionar entre as distribuições [^24]. Amostras são geradas da distribuição alvo da seguinte forma [^24]:

1.  Amostrar uma sequência $z = (z_{n-1}, ..., z_0)$ da seguinte forma: amostrar $z_{n-1} \sim p_n$; amostrar $z_{n-2} \sim T_{n-1}(z_{n-1}, .)$, ..., amostrar $z_0 \sim T_1(z_1, .)$
2.  Definir $x = z_0$ e atribuir um peso:

$$ w = \frac{f_{n-1}(z_{n-1})}{f_n(z_{n-1})} \frac{f_{n-2}(z_{n-2})}{f_{n-1}(z_{n-2})} ... \frac{f_1(z_1)}{f_2(z_1)} \frac{f_0(z_0)}{f_1(z_0)} $$
O AIS é uma forma de *importance sampling* em um espaço de estados estendido, onde o peso $w$ corrige a diferença entre a distribuição proposta e a distribuição alvo [^24].

#### Parallel Tempering
O **parallel tempering**, também conhecido como *replica exchange MCMC*, executa múltiplas cadeias MCMC em paralelo a diferentes temperaturas [^24]. Cadeias de alta temperatura podem fazer movimentos de longa distância no espaço de estados e influenciar as cadeias de baixa temperatura, permitindo assim uma exploração mais eficiente de distribuições multimodais [^24]. Periodicamente, duas cadeias em temperaturas adjacentes trocam suas configurações com uma probabilidade que depende das temperaturas e das energias dos estados [^24]. A probabilidade de troca é projetada para satisfazer o *detailed balance*, garantindo que a distribuição estacionária do sistema combinado seja a distribuição alvo correta [^24].

### Conclusão

Os métodos de annealing como *simulated annealing*, *annealed importance sampling* e *parallel tempering* fornecem ferramentas poderosas para amostrar de distribuições complexas e multimodais que são frequentemente encontradas em problemas de inferência estatística e aprendizado de máquina [^24]. Ao introduzir um parâmetro de temperatura computacional e usar uma variedade de técnicas de annealing, esses métodos podem superar as limitações dos métodos MCMC tradicionais e explorar o espaço de estados de forma mais eficiente [^24]. O *annealed importance sampling* combina simulated annealing e importance sampling para desenhar amostras independentes de distribuições difíceis [^24]. O *parallel tempering* executa múltiplas cadeias em diferentes temperaturas, permitindo que cadeias de alta temperatura façam movimentos de longa distância e influenciem cadeias de baixa temperatura [^24].

### Referências
[^1]: Chapter 24. Markov chain Monte Carlo (MCMC) inference
[^22]: 24.4.3 Practical convergence diagnostics
[^24]: 24.6 Annealing methods
<!-- END -->