## 24.2.4 Collapsed Gibbs Sampling: Enhancing Efficiency in GMM Parameter Inference

### Introdução
Em continuidade à discussão sobre Gibbs sampling e suas aplicações, este capítulo aprofunda-se na técnica de **collapsed Gibbs sampling**, uma variação que visa melhorar a eficiência computacional da inferência Bayesiana [^2]. Especificamente, exploraremos como essa abordagem pode ser aplicada na inferência de parâmetros em modelos de mistura de Gaussianas (GMM), um problema fundamental em diversas áreas, incluindo machine learning e análise de dados [^2.3]. O collapsed Gibbs sampling se destaca por integrar analiticamente algumas variáveis desconhecidas, como os parâmetros do modelo, reduzindo a dimensionalidade do espaço amostral e a variância das estimativas [^2.4].

### Conceitos Fundamentais

O Gibbs sampling, conforme apresentado na seção 24.2, é um algoritmo MCMC que amostra cada variável condicionalmente às demais [^2]. No contexto de um GMM, isso envolve amostrar as variáveis de atribuição de cluster ($z_i$) e os parâmetros do modelo (médias $\mu_k$, covariâncias $\Sigma_k$ e pesos de mistura $\pi_k$) iterativamente, com base nas distribuições condicionais completas [^2.3]. No entanto, essa abordagem pode ser computacionalmente custosa, especialmente em modelos complexos ou com grandes conjuntos de dados.

O **collapsed Gibbs sampling** busca mitigar essa complexidade ao integrar analiticamente algumas variáveis, removendo-as do processo de amostragem [^2.4]. No contexto do GMM, isso geralmente envolve integrar os parâmetros do modelo ($\mu_k$, $\Sigma_k$ e $\pi_k$), amostrando apenas as variáveis de atribuição de cluster ($z_i$). Essa integração é possível quando se utilizam priors conjugados, o que permite obter distribuições posteriores analiticamente tratáveis.

A principal vantagem dessa abordagem é a **redução da dimensionalidade** do espaço amostral, o que pode levar a uma convergência mais rápida e estimativas mais precisas [^2.4]. Além disso, o collapsed Gibbs sampling frequentemente resulta em uma **redução da variância** das estimativas, um fenômeno conhecido como **Rao-Blackwellização** [^2.4].

**Teorema 24.2.1 (Rao-Blackwell):** Sejam $z$ e $\theta$ variáveis aleatórias dependentes, e $f(z, \theta)$ uma função escalar. Então:
$$var_{z,\theta} [f(z, \theta)] \geq var_z [E_{\theta} [f(z, \theta) | z]]$$
Este teorema garante que a variância da estimativa criada pela integração analítica de $\theta$ sempre será menor (ou, no máximo, igual) à variância de uma estimativa de Monte Carlo direta [^2.4]. No collapsed Gibbs, amostramos $z$ com $\theta$ integrado; o teorema de Rao-Blackwell ainda se aplica nesse caso [^2.4].

Para ilustrar o processo, consideremos um GMM com priors conjugados completos. Nesse caso, podemos integrar analiticamente os parâmetros do modelo ($\mu_k$, $\Sigma_k$ e $\pi$), amostrando apenas os indicadores $z$ [^2.4.1]. Após integrar $\pi$, todos os nós $z_i$ tornam-se inter-dependentes. Da mesma forma, após integrar $\theta_k$, todos os nós $x_i$ tornam-se inter-dependentes [^2.4.1]. No entanto, podemos computar facilmente as condicionais completas da seguinte forma:
$$p(z_i = k | z_{-i}, x, \alpha, \beta) \propto p(z_i = k | z_{-i}, \alpha) p(x | z_i = k, z_{-i}, \beta)$$
$$p(z_i = k | z_{-i}, \alpha) p(x_i | x_{-i}, z_i = k, z_{-i}, \beta)$$
$$p(z_i = k | z_{-i}, \alpha) p(x_i | x_{-i}, z_i = k, z_{-i}, \beta)$$
onde $\beta = (m_0, V_0, S_0, v_0)$ são os hiperparâmetros para as densidades condicionais de classe. O primeiro termo pode ser obtido integrando $\pi$. Suponha que usamos um prior simétrico da forma $\pi \sim Dir(\alpha)$, onde $\alpha_k = \alpha / K$ [^2.4.1]. Da Equação 5.26, temos:
$$p(z_1, ..., z_N | \alpha) = \frac{\Gamma(\alpha)}{\Gamma(N+\alpha)} \prod_{k=1}^K \frac{\Gamma(N_k + \alpha/K)}{\Gamma(\alpha/K)}$$

### Conclusão

O collapsed Gibbs sampling oferece uma alternativa eficiente ao Gibbs sampling tradicional para a inferência de parâmetros em GMMs. Ao integrar analiticamente os parâmetros do modelo, essa técnica reduz a dimensionalidade do espaço amostral, acelera a convergência e diminui a variância das estimativas. Embora a derivação das distribuições condicionais completas possa ser mais complexa, os benefícios computacionais e estatísticos frequentemente superam essa dificuldade. A aplicação bem-sucedida do collapsed Gibbs sampling requer a escolha de priors conjugados, que garantem a tratabilidade analítica das integrações.

### Referências
[^2]: Chapter 24. Markov chain Monte Carlo (MCMC) inference
[^2.3]: 24.2.3 Example: Gibbs sampling for inferring the parameters of a GMM
[^2.4]: 24.2.4 Collapsed Gibbs sampling *
[^2.4.1]: 24.2.4.1 Example: collapsed Gibbs for fitting a GMM
<!-- END -->