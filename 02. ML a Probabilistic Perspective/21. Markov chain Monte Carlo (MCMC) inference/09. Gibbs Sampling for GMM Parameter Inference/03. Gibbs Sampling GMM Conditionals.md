## Condicionais Completos para Indicadores Discretos no Gibbs Sampling para GMM

### Introdução
No contexto da inferência de parâmetros para um *Gaussian Mixture Model* (GMM) utilizando *Gibbs sampling*, a derivação dos condicionais completos é um passo crucial. Este capítulo detalha a forma dos condicionais completos para os indicadores discretos, $p(z_i = k | x_i, \mu, \Sigma, \pi)$, que são proporcionais ao produto do peso de mistura $\pi_k$ e da densidade Gaussiana $N(x_i | \mu_k, \Sigma_k)$ [^840]. Esta relação indica a probabilidade do ponto de dado $x_i$ pertencer ao cluster $k$. Em continuidade ao que foi apresentado anteriormente sobre *Gibbs sampling* [^838], este capítulo aprofunda a aplicação específica desse método para modelos de mistura Gaussianos, detalhando a derivação e a interpretação dos condicionais completos.

### Conceitos Fundamentais

O *Gibbs sampling* é um algoritmo MCMC (Markov chain Monte Carlo) popular que amostra cada variável condicionalmente aos valores das outras [^838]. No caso de um GMM, o objetivo é inferir os parâmetros do modelo ($\mu, \Sigma, \pi$) e as atribuições de cluster ($z$) dados os dados observados ($x$).

**Derivação do Condicional Completo**

O condicional completo para a variável indicadora discreta $z_i$, que indica a qual cluster o ponto de dado $x_i$ pertence, é dado por [^840]:

$$p(z_i = k | x_i, \mu, \Sigma, \pi) \propto \pi_k N(x_i | \mu_k, \Sigma_k)$$

onde:
*   $z_i = k$ indica que o ponto de dado $x_i$ é atribuído ao cluster $k$.
*   $x_i$ é o *i*-ésimo ponto de dado.
*   $\mu_k$ é o vetor de médias do cluster *k*.
*   $\Sigma_k$ é a matriz de covariância do cluster *k*.
*   $\pi_k$ é o peso de mistura do cluster *k*, representando a probabilidade a priori de um ponto de dado pertencer ao cluster *k*.
*   $N(x_i | \mu_k, \Sigma_k)$ é a densidade Gaussiana multivariada avaliada em $x_i$, com média $\mu_k$ e covariância $\Sigma_k$, dada por:

$$N(x_i | \mu_k, \Sigma_k) = \frac{1}{(2\pi)^{D/2} |\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2} (x_i - \mu_k)^T \Sigma_k^{-1} (x_i - \mu_k)\right)$$

onde *D* é a dimensão dos dados.

**Interpretação**

O condicional completo $p(z_i = k | x_i, \mu, \Sigma, \pi)$ representa a probabilidade posterior de que o ponto de dado $x_i$ pertença ao cluster $k$, dadas as observações e os parâmetros atuais do modelo. A intuição por trás desta fórmula é que a probabilidade de $x_i$ pertencer a um cluster é proporcional a dois fatores:

1.  O peso de mistura $\pi_k$, que reflete a probabilidade a priori de pertencer ao cluster *k*.
2.  A verossimilhança $N(x_i | \mu_k, \Sigma_k)$, que mede a compatibilidade de $x_i$ com a distribuição Gaussiana do cluster *k*.

Portanto, o *Gibbs sampling* para GMMs envolve iterativamente amostrar cada $z_i$ de acordo com esta distribuição condicional, e então amostrar os parâmetros $\mu$, $\Sigma$ e $\pi$ de seus respectivos condicionais completos, que dependem dos valores de $z$ [^840].

**Normalização**

Na prática, para amostrar de $p(z_i = k | x_i, \mu, \Sigma, \pi)$, é necessário normalizar a distribuição. Como $z_i$ é uma variável categórica que pode assumir valores de 1 a *K* (onde *K* é o número de clusters), a distribuição normalizada é dada por:

$$p(z_i = k | x_i, \mu, \Sigma, \pi) = \frac{\pi_k N(x_i | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j N(x_i | \mu_j, \Sigma_j)}$$

Essa normalização garante que as probabilidades sobre todos os clusters somem 1, permitindo a amostragem correta.

### Conclusão

A derivação e implementação correta dos condicionais completos para os indicadores discretos são fundamentais para a aplicação bem-sucedida do *Gibbs sampling* em GMMs. Este capítulo detalhou a forma desses condicionais, sua interpretação e a necessidade de normalização para a amostragem. Em continuidade ao que foi apresentado sobre *Gibbs sampling* e *GMMs*, este capítulo fornece um componente essencial para a inferência Bayesiana em modelos de mistura Gaussianos, permitindo a estimação dos parâmetros do modelo e a atribuição de clusters de forma iterativa e eficiente.

### Referências
[^838]: Capítulo 24. Markov chain Monte Carlo (MCMC) inference, Seções 24.1 e 24.2
[^840]: Capítulo 24. Markov chain Monte Carlo (MCMC) inference, Seção 24.2.3

<!-- END -->