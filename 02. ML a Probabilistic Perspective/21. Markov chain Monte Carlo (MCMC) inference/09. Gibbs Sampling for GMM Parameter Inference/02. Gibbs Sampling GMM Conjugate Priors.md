## Conjugate Priors and Gibbs Sampling for GMM Parameter Inference

### Introdução
O algoritmo de Gibbs sampling é uma técnica poderosa para inferência Bayesiana, especialmente em modelos complexos como as Gaussian Mixture Models (GMMs). Uma das principais vantagens do Gibbs sampling é a sua facilidade de implementação, que é significativamente simplificada quando priors conjugados são utilizados. Este capítulo explora como priors conjugados e semi-conjugados facilitam a derivação e implementação do Gibbs sampling para a inferência de parâmetros em GMMs [^24.2.3]. O uso de priors conjugados permite atualizações diretas para os indicadores discretos, pesos de mistura, médias e covariâncias dos componentes Gaussianos [^24.2.3].

### Conceitos Fundamentais
Em uma GMM, o objetivo é modelar a distribuição de probabilidade de um conjunto de dados como uma soma ponderada de distribuições Gaussianas. Para realizar a inferência Bayesiana dos parâmetros da GMM usando Gibbs sampling, precisamos definir uma distribuição conjunta completa (full joint distribution) [^24.2.3]. Uma forma de definir essa distribuição é através de um prior semi-conjugado, que é expresso como um produto de distribuições condicionais [^24.2.3]:

$$ p(x, z, \mu, \Sigma, \pi) = p(x|z, \mu, \Sigma)p(z|\pi)p(\pi) \prod_{k=1}^{K} p(\mu_k)p(\Sigma_k) $$

onde:
*   $x$ representa os dados observados.
*   $z$ representa as atribuições de cluster para cada ponto de dados.
*   $\mu$ representa as médias dos componentes Gaussianos.
*   $\Sigma$ representa as matrizes de covariância dos componentes Gaussianos.
*   $\pi$ representa os pesos de mistura dos componentes Gaussianos.
*   $K$ é o número de componentes Gaussianos.

A utilização de priors conjugados simplifica enormemente as distribuições condicionais completas (**full conditionals**), que são necessárias para o Gibbs sampling [^24.2]. Para cada parâmetro, precisamos amostrar da sua distribuição condicional completa, dado os outros parâmetros e os dados. As distribuições condicionais completas para os parâmetros da GMM com priors conjugados são:

1.  **Atribuições de Cluster ($z_i$)**: A probabilidade de um ponto de dados $x_i$ pertencer ao cluster $k$ é proporcional ao produto do peso de mistura do cluster $k$ e a probabilidade de $x_i$ sob a distribuição Gaussiana do cluster $k$ [^24.10]:
    $$     p(z_i = k|x_i, \mu, \Sigma, \pi) \propto \pi_k \mathcal{N}(x_i|\mu_k, \Sigma_k)     $$
    onde $\mathcal{N}(x_i|\mu_k, \Sigma_k)$ é a função de densidade de probabilidade Gaussiana com média $\mu_k$ e covariância $\Sigma_k$.

2.  **Pesos de Mistura ($\pi$)**: Dado as atribuições de cluster, os pesos de mistura seguem uma distribuição de Dirichlet [^24.11]:
    $$     p(\pi|z) = \text{Dir}(\{\alpha_k + \sum_{i=1}^{N} \mathbb{I}(z_i = k)\}_{k=1}^{K})     $$
    onde $\alpha_k$ é o parâmetro do prior de Dirichlet para o componente $k$ e $\mathbb{I}(z_i = k)$ é uma função indicadora que é 1 se $z_i = k$ e 0 caso contrário.

3.  **Médias dos Componentes ($\mu_k$)**: Dado as atribuições de cluster e as covariâncias, as médias dos componentes seguem uma distribuição Gaussiana [^24.12]:
    $$     p(\mu_k|\Sigma_k, z, x) = \mathcal{N}(\mu_k|m_k, V_k)     $$
    com
    $$     V_k = (\mathbf{V}_0^{-1} + N_k \Sigma_k^{-1})^{-1}     $$
    e
    $$     m_k = V_k (\Sigma_k^{-1} \sum_{i: z_i = k} x_i + \mathbf{V}_0^{-1} \mathbf{m}_0)     $$
    onde $N_k$ é o número de pontos atribuídos ao cluster $k$, $\mathbf{m}_0$ e $\mathbf{V}_0$ são os parâmetros do prior Gaussiano para as médias.

4.  **Covariâncias dos Componentes ($\Sigma_k$)**: Dado as atribuições de cluster e as médias, as covariâncias dos componentes seguem uma distribuição Inverse Wishart [^24.17]:
    $$     p(\Sigma_k|\mu_k, z, x) = \text{IW}(\Sigma_k|S_k, \nu_k)     $$
    com
    $$     S_k = \mathbf{S}_0 + \sum_{i: z_i = k} (x_i - \mu_k)(x_i - \mu_k)^T     $$
    e
    $$     \nu_k = \nu_0 + N_k     $$
    onde $\mathbf{S}_0$ e $\nu_0$ são os parâmetros do prior Inverse Wishart para as covariâncias.

Essas distribuições condicionais completas, derivadas a partir de priors conjugados, permitem que o Gibbs sampling proceda de forma iterativa, amostrando cada parâmetro condicionado nos valores atuais dos outros parâmetros [^24.2].

### Conclusão
A utilização de priors conjugados no contexto do Gibbs sampling para GMMs simplifica significativamente o processo de inferência Bayesiana [^24.2.3]. As distribuições condicionais completas resultantes têm formas analíticas conhecidas, o que facilita a implementação do algoritmo e reduz a complexidade computacional [^24.2]. Este capítulo demonstrou como priors conjugados para os pesos de mistura, médias e covariâncias levam a distribuições condicionais completas que são amostráveis de forma eficiente, permitindo a inferência precisa dos parâmetros da GMM através do Gibbs sampling [^24.2.3].

### Referências
[^24.2]: Chapter 24. Markov chain Monte Carlo (MCMC) inference
[^24.2.3]: Chapter 24. Markov chain Monte Carlo (MCMC) inference, section 24.2.3
[^24.10]: Chapter 24. Markov chain Monte Carlo (MCMC) inference, equation 24.10
[^24.11]: Chapter 24. Markov chain Monte Carlo (MCMC) inference, equation 24.11
[^24.12]: Chapter 24. Markov chain Monte Carlo (MCMC) inference, equation 24.12
[^24.17]: Chapter 24. Markov chain Monte Carlo (MCMC) inference, equation 24.17
<!-- END -->