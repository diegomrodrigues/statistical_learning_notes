## Gibbs Sampling para Inferência de Parâmetros em Modelos de Mistura Gaussiana (GMM)

### Introdução
O presente capítulo explora a aplicação do **Gibbs sampling** na inferência de parâmetros para **Modelos de Mistura Gaussiana (GMM)**. Conforme introduzido anteriormente [^2], o Gibbs sampling é um método popular de **Markov chain Monte Carlo (MCMC)**, notável por sua facilidade de implementação e aplicabilidade a uma ampla gama de modelos. Este capítulo detalha como o Gibbs sampling pode ser utilizado para estimar os parâmetros de um GMM, iterativamente amostrando das distribuições condicionais completas dos componentes, dadas as observações e os valores correntes dos outros parâmetros [^4].

### Conceitos Fundamentais
Um **Modelo de Mistura Gaussiana (GMM)** é um modelo probabilístico que assume que todos os pontos de dados são gerados a partir de uma mistura de um número finito de distribuições Gaussianas com parâmetros desconhecidos [^4]. A utilização de Gibbs sampling para inferência de parâmetros em GMMs envolve a estimação iterativa da média (**μ**), covariância (**Σ**) e pesos de mistura (**π**) de cada componente Gaussiana, dada a amostra observada (**x**) e os valores correntes dos outros parâmetros.

O processo iterativo do Gibbs sampling para GMMs pode ser descrito nos seguintes passos:

1.  **Inicialização:** Inicializar os parâmetros do modelo (μ, Σ, π).
2.  **Iteração:** Para cada iteração *s*:
    *   **Amostragem dos Indicadores de Componente (z):** Para cada ponto de dado *i*, amostrar o indicador de componente *zi* da sua distribuição condicional completa, dada a observação *xi* e os valores correntes dos parâmetros (μ, Σ, π) [^4]. A probabilidade de um ponto de dado *xi* pertencer ao componente *k* é proporcional ao produto do peso de mistura *πk* e a densidade Gaussiana *N(xi|μk, Σk)* [^4]:

        $$p(z_i = k | x_i, \mu, \Sigma, \pi) \propto \pi_k N(x_i | \mu_k, \Sigma_k)$$
    *   **Amostragem dos Pesos de Mistura (π):** Amostrar os pesos de mistura *π* da sua distribuição condicional completa, dada a amostra de indicadores de componente *z* [^4]. Se utilizarmos um prior de Dirichlet para os pesos de mistura, a distribuição condicional completa também será uma distribuição de Dirichlet:

        $$p(\pi | z) = Dir(\\{\alpha_k + \sum_{i=1}^N \mathbb{I}(z_i = k)\\}_{k=1}^K)$$

        onde *αk* é o parâmetro do prior de Dirichlet para o componente *k* e *N* é o número total de pontos de dados [^4].
    *   **Amostragem das Médias dos Componentes (μ):** Para cada componente *k*, amostrar a média *μk* da sua distribuição condicional completa, dada a amostra de pontos de dados atribuídos ao componente *k* e a matriz de covariância *Σk* [^4]. Se utilizarmos um prior Gaussiano para as médias, a distribuição condicional completa também será uma distribuição Gaussiana:

        $$p(\mu_k | \Sigma_k, z, x) = N(\mu_k | m_k, V_k)$$

        onde $V_k = (\Sigma_k^{-1} N_k + V_0^{-1})^{-1}$ e $m_k = V_k (\Sigma_k^{-1} \sum_{i: z_i = k} x_i + V_0^{-1} m_0)$, com *m0* e *V0* sendo os parâmetros do prior Gaussiano para as médias e *Nk* sendo o número de pontos de dados atribuídos ao componente *k* [^4].
    *   **Amostragem das Matrizes de Covariância (Σ):** Para cada componente *k*, amostrar a matriz de covariância *Σk* da sua distribuição condicional completa, dada a amostra de pontos de dados atribuídos ao componente *k* e a média *μk* [^4]. Se utilizarmos um prior Inverse Wishart para as matrizes de covariância, a distribuição condicional completa também será uma distribuição Inverse Wishart:

        $$p(\Sigma_k | \mu_k, z, x) = IW(\Sigma_k | S_k, \nu_k)$$

        onde $\nu_k = \nu_0 + N_k$ e $S_k = S_0 + \sum_{i: z_i = k} (x_i - \mu_k)(x_i - \mu_k)^T$, com *ν0* e *S0* sendo os parâmetros do prior Inverse Wishart para as matrizes de covariância [^4].
3.  **Convergência:** Repetir o passo 2 por um número suficiente de iterações até que a cadeia de Markov convirja para a distribuição estacionária [^2]. A convergência pode ser avaliada utilizando várias métricas, como a estabilização dos parâmetros ou a redução do potencial de escala estimado (EPSR) [^24.4.3.1].

Após a convergência, as amostras obtidas representam uma amostra da distribuição posterior dos parâmetros do modelo, permitindo a inferência sobre os valores dos parâmetros e a realização de previsões [^2].

### Label Switching
Uma dificuldade inerente ao uso de Gibbs sampling para modelos de mistura é o problema de **label switching** [^24.2.3.1]. Como os componentes da mistura são intercambiáveis, as etiquetas dos componentes podem mudar ao longo das iterações do Gibbs sampling, levando a uma má interpretação dos resultados. Para mitigar este problema, várias técnicas podem ser aplicadas, como a imposição de restrições nos parâmetros para garantir a identificabilidade ou o pós-processamento das amostras para alinhar as etiquetas [^24.2.3.1].

### Collapsed Gibbs Sampling
Uma otimização do Gibbs sampling para GMMs é o **collapsed Gibbs sampling** [^24.2.4], onde os parâmetros do modelo (μ, Σ, π) são integrados analiticamente, e apenas os indicadores de componente *z* são amostrados [^24.2.4.1]. Esta abordagem reduz a dimensionalidade do espaço amostral, levando a uma convergência mais rápida e amostras com menor variância [^24.2.4.1]. O collapsed Gibbs sampling para GMMs envolve a amostragem iterativa dos indicadores de componente *zi* da sua distribuição condicional completa, dada a amostra de pontos de dados *x* e os valores correntes dos outros indicadores de componente *z-i* [^24.2.4.1].

### Conclusão
O Gibbs sampling oferece uma abordagem flexível e eficaz para a inferência de parâmetros em GMMs [^2]. Ao amostrar iterativamente das distribuições condicionais completas, o Gibbs sampling permite a estimação dos parâmetros do modelo e a realização de previsões. Apesar dos desafios, como o problema de label switching, o Gibbs sampling continua a ser uma ferramenta valiosa para a análise de dados e modelagem estatística [^24.2.3.1].

### Referências
[^2]: Chapter 24. Markov chain Monte Carlo (MCMC) inference
[^4]: 24.2.3 Example: Gibbs sampling for inferring the parameters of a GMM
[^24.2.3.1]: 24.2.3.1 Label switching
[^24.2.4]: 24.2.4 Collapsed Gibbs sampling *
[^24.2.4.1]: 24.2.4.1 Example: collapsed Gibbs for fitting a GMM
[^24.4.3.1]: 24.4.3.1 Estimated potential scale reduction (EPSR)
<!-- END -->