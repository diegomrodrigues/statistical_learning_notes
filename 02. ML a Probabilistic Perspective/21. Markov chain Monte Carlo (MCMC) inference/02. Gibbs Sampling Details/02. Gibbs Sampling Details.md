## Detalhes da Amostragem de Gibbs: Condicional Completa e Markov Blanket

### Introdução
A amostragem de Gibbs, um dos algoritmos MCMC mais populares [^2], é uma técnica iterativa onde cada variável é amostrada condicionalmente aos valores das outras variáveis no modelo. Este capítulo explora em detalhes o conceito da condicional completa e sua relação com a estrutura gráfica do modelo, crucial para a implementação eficiente do Gibbs sampling.

### Conceitos Fundamentais

A peça central do Gibbs sampling é a **condicional completa** ( *full conditional* ) [^2] para uma variável *i*, denotada por $p(x_i | x_{-i})$. Aqui, $x_i$ representa a variável *i*, e $x_{-i}$ denota todas as outras variáveis no modelo, ou seja, $x_{-i} = \\{x_1, ..., x_{i-1}, x_{i+1}, ..., x_D\\}$ se tivermos *D* variáveis. A condicional completa define a distribuição de probabilidade de $x_i$ dado o estado atual de todas as outras variáveis [^2].

Em geral, $x_i$ pode depender apenas de um subconjunto das outras variáveis [^2]. Essa dependência é capturada pelo conceito de **Markov blanket**. O *Markov blanket* de uma variável *i* em um modelo gráfico inclui seus vizinhos no grafo [^2]. Formalmente, o Markov blanket de $x_i$ é o conjunto de nós que a tornam condicionalmente independente do resto do grafo. Assim, para amostrar $x_i$, só precisamos conhecer os valores de seus vizinhos [^2]. Isso simplifica significativamente o processo de amostragem, especialmente em modelos com muitas variáveis.

**Em resumo:**

*   **Condicional Completa:** $p(x_i | x_{-i})$ é a distribuição de $x_i$ dado o estado atual de todas as outras variáveis.
*   **Markov Blanket:** Define o conjunto mínimo de variáveis necessárias para amostrar $x_i$, ou seja, seus vizinhos no modelo gráfico.

Expandindo a ideia, em modelos *pairwise* MRF/CRF, o Gibbs sampling envolve amostrar da distribuição condicional $p(X_t | X_{-t}, \theta)$ [^2], onde *t* representa um nó específico e $X_{-t}$ representa todos os outros nós. Essa distribuição é proporcional ao produto dos potenciais *pairwise* $\psi_{st}(x_s, x_t)$ sobre todos os vizinhos *s* ∈ nbr(*t*) [^2], refletindo a compatibilidade entre variáveis vizinhas no modelo.

$$\np(X_t | X_{-t}, \theta) \propto \prod_{s \in nbr(t)} \psi_{st}(x_s, x_t)\n$$

**Exemplo:** Considere um modelo de Ising *pairwise* [^2] com potenciais de aresta dados por $\psi_{st}(x_s, x_t) = \exp(J x_s x_t)$, onde $x_t \in \\{-1, +1\\}$ e *J* é a força do acoplamento. A distribuição condicional completa para um nó *t* é dada por [^2]:

$$\np(x_t = +1 | x_{-t}, \theta) = \frac{\prod_{s \in nbr(t)} \psi_{st}(x_s, +1)}{\prod_{s \in nbr(t)} \psi_{st}(x_s, +1) + \prod_{s \in nbr(t)} \psi_{st}(x_s, -1)}\n$$

Substituindo o potencial de aresta, temos [^2]:

$$\np(x_t = +1 | x_{-t}, \theta) = \frac{\exp[J \sum_{s \in nbr(t)} x_s]}{\exp[J \sum_{s \in nbr(t)} x_s] + \exp[-J \sum_{s \in nbr(t)} x_s]}\n$$

Se definirmos $n_t = \sum_{s \in nbr(t)} x_s$ [^2], a expressão simplifica para:

$$\np(x_t = +1 | x_{-t}, \theta) = \text{sigm}(2J n_t)\n$$

onde $\text{sigm}(u) = 1/(1 + e^{-u})$ é a função sigmoide [^2].

### Conclusão
A amostragem de Gibbs, facilitada pelo conceito de condicional completa e Markov blanket, oferece um método iterativo para amostrar de distribuições complexas. Ao explorar a estrutura de dependência no modelo gráfico, o Gibbs sampling permite uma amostragem eficiente, focando apenas nos vizinhos relevantes de cada variável. A implementação em modelos *pairwise* MRF/CRF destaca a importância dos potenciais *pairwise* na determinação da compatibilidade entre variáveis vizinhas, guiando o processo de amostragem.

### Referências
[^2]: Chapter 24. Markov chain Monte Carlo (MCMC) inference.
<!-- END -->