## MCMC: Implementation and Applications

### Introdução
Markov Chain Monte Carlo (MCMC) é uma técnica poderosa para amostragem de distribuições de probabilidade complexas, especialmente útil em estatística Bayesiana e, cada vez mais, em *machine learning* [^2]. Este capítulo explora as nuances de implementação e as diversas aplicações de MCMC, com foco nas vantagens e desvantagens em relação a métodos alternativos como a inferência variacional. MCMC se destaca pela sua relativa facilidade de implementação e aplicabilidade a uma gama mais ampla de modelos, incluindo aqueles com tamanho ou estrutura variáveis, ou aqueles que carecem de *priors* conjugados convenientes [^2].

### Conceitos Fundamentais
A ideia central por trás do MCMC é construir uma **cadeia de Markov** no espaço de estados $\mathcal{X}$, cuja distribuição estacionária é a densidade alvo $p^*(x)$ de interesse [^24]. Essa densidade pode representar uma *prior* ou uma *posterior*. O algoritmo executa um passeio aleatório no espaço de estados, de tal forma que a fração de tempo que passamos em cada estado $x$ é proporcional a $p^*(x)$ [^24]. Ao desenhar amostras (correlacionadas) $x_0, x_1, x_2, ...$ da cadeia, podemos realizar a integração de Monte Carlo em relação a $p^*$.

#### Vantagens e Desvantagens
Comparado com a inferência variacional, MCMC oferece certas vantagens [^2]. A implementação tende a ser mais fácil, e o método é aplicável a uma gama mais ampla de modelos, como aqueles com tamanho ou estrutura variáveis, ou que carecem de *priors* conjugados convenientes [^2]. Em modelos ou conjuntos de dados muito grandes, MCMC pode ser mais rápido devido à passagem de mensagens esparsas [^2].

Por outro lado, a inferência variacional é geralmente mais rápida para problemas de pequeno a médio porte e fornece um limite inferior para a *log-likelihood* [^2].

#### Algoritmos MCMC Populares
1.  **Amostragem de Gibbs (Gibbs Sampling):** Um dos algoritmos MCMC mais populares, a amostragem de Gibbs amostra cada variável por vez, condicionada aos valores de todas as outras variáveis na distribuição [^24]. Dada uma amostra conjunta $x^s$ de todas as variáveis, geramos uma nova amostra $x^{s+1}$ amostrando cada componente por vez, com base nos valores mais recentes das outras variáveis. Por exemplo, com $D = 3$ variáveis:
    *   $x_1^{s+1} \sim p(x_1 | x_2^s, x_3^s)$
    *   $x_2^{s+1} \sim p(x_2 | x_1^{s+1}, x_3^s)$
    *   $x_3^{s+1} \sim p(x_3 | x_1^{s+1}, x_2^{s+1})$
    A expressão $p(x_i | x_{-i})$ é chamada de **condicional completa** para a variável $i$ [^24].
2.  **Algoritmo de Metropolis-Hastings (MH):** Em cada etapa, o algoritmo MH propõe mover do estado atual $x$ para um novo estado $x'$ com probabilidade $q(x'|x)$, onde $q$ é chamada de **distribuição de proposta** (*proposal distribution*) ou *kernel* [^24]. O usuário pode usar qualquer tipo de proposta que desejar, sujeito a algumas condições. Uma proposta comumente usada é uma distribuição gaussiana simétrica centrada no estado atual, $q(x'|x) = \mathcal{N}(x'|x, \Sigma)$ [^24]. Isso é chamado de **algoritmo de Metropolis de passeio aleatório** (*random walk Metropolis algorithm*). Depois de propor um movimento para $x'$, decidimos se aceitamos essa proposta ou não, de acordo com uma fórmula que garante que a fração de tempo gasto em cada estado $x$ seja proporcional a $p^*(x)$ [^24].

#### Distribuições de Proposta (Proposal distributions)
Para uma dada distribuição alvo $p^*$, uma distribuição de proposta $q$ é válida ou admissível se ela der uma probabilidade não nula de mover para os estados que têm probabilidade não nula no alvo [^24]. Formalmente:
$$ supp(p^*) \subseteq \bigcup_x supp(q(x|\cdot)) $$
Por exemplo, uma proposta de passeio aleatório gaussiano tem densidade de probabilidade não nula em todo o espaço de estados e, portanto, é uma proposta válida para qualquer espaço de estados contínuo [^24].

#### Taxa de mistura de cadeias de Markov
A quantidade de tempo que leva para uma cadeia de Markov convergir para a distribuição estacionária e esquecer seu estado inicial é chamada de **tempo de mistura** [^24]. Mais formalmente, dizemos que o tempo de mistura do estado $x_0$ é o tempo mínimo tal que, para qualquer constante $\epsilon > 0$, temos que
$$ \tau_\epsilon(x_0) \triangleq min\{t : ||\delta_{x_0}(x)T^t - p^*||_1 \leq \epsilon\} $$
onde $\delta_{x_0}(x)$ é uma distribuição com toda a sua massa no estado $x_0$, $T$ é a matriz de transição da cadeia (que depende do alvo $p^*$ e da proposta $q$), e $\delta_{x_0}(x)T^t$ é a distribuição após $t$ passos. O tempo de mistura da cadeia é definido como
$$ \tau \triangleq \max_{x_0} \tau_\epsilon(x_0) $$
O tempo de mistura é determinado pelo *eigengap* $\gamma = \lambda_1 - \lambda_2$, que é a diferença do primeiro e segundo autovalores da matriz de transição.

### Conclusão
MCMC é uma ferramenta versátil e poderosa para inferência Bayesiana e *machine learning*, com uma ampla gama de aplicações [^2]. Embora possa ser computacionalmente intensivo e exigir considerações cuidadosas sobre o projeto do algoritmo e o diagnóstico de convergência, sua flexibilidade e aplicabilidade o tornam uma escolha valiosa para muitos problemas complexos [^24]. A escolha entre MCMC e métodos alternativos como a inferência variacional depende das características específicas do problema em questão, incluindo o tamanho do modelo, a complexidade da distribuição alvo e os recursos computacionais disponíveis [^2].
<!-- END -->