## Markov Chain Monte Carlo (MCMC): Fundamentos e Aplicações
### Introdução
O método de **Markov Chain Monte Carlo (MCMC)** é uma técnica amplamente utilizada para amostrar de distribuições de alta dimensionalidade. Reconhecido como um dos algoritmos mais importantes do século XX, o MCMC é essencial em diversas aplicações de *machine learning* [^1]. Este capítulo explora os fundamentos do MCMC, com foco em **Gibbs Sampling**, uma das implementações mais populares e intuitivas.

O MCMC supera as limitações dos métodos de Monte Carlo simples, como *rejection sampling* e *importance sampling*, que não se escalam bem para espaços de alta dimensão [^1]. A ideia central do MCMC é construir uma **cadeia de Markov** no espaço de estados cuja distribuição estacionária corresponde à densidade alvo de interesse, seja ela uma *prior* ou uma *posterior* [^1]. Ao realizar um *random walk* nesse espaço, o algoritmo garante que a fração de tempo gasta em cada estado seja proporcional à densidade alvo, permitindo a aproximação de distribuições complexas através da amostragem de valores correlacionados [^1].

### Conceitos Fundamentais
O algoritmo MCMC possui uma história interessante, tendo sido descoberto por físicos que trabalhavam na bomba atômica em Los Alamos durante a Segunda Guerra Mundial [^1]. Sua primeira publicação foi em um jornal de química (*Metropolis et al. 1953*) [^1]. Uma extensão foi publicada na literatura estatística (*Hastings 1970*), mas passou despercebida [^1]. Um caso especial, a **Gibbs Sampling**, foi inventada independentemente em 1984 no contexto de modelos de Ising e publicada em (*Geman and Geman 1984*) [^1]. Foi somente em (*Gelfand and Smith 1990*) que o algoritmo se tornou amplamente conhecido na comunidade estatística [^1].

**Gibbs Sampling:**
Uma das implementações mais populares do MCMC é a **Gibbs sampling**[^2]. Em física, este método é conhecido como *Glauber dynamics* ou *heat bath method* [^2]. Gibbs sampling é o análogo MCMC da descida coordenada [^2].

A ideia básica por trás da Gibbs sampling é amostrar cada variável por vez, condicionado nos valores de todas as outras variáveis na distribuição [^2]. Ou seja, dado uma amostra conjunta $x^s$ de todas as variáveis, geramos uma nova amostra $x^{s+1}$ amostrando cada componente por vez, baseado nos valores mais recentes das outras variáveis [^2]. Por exemplo, se temos $D = 3$ variáveis, usamos:
*   $x_1^{s+1} \sim p(x_1 | x_2^s, x_3^s)$
*   $x_2^{s+1} \sim p(x_2 | x_1^{s+1}, x_3^s)$
*   $x_3^{s+1} \sim p(x_3 | x_1^{s+1}, x_2^{s+1})$

Isto generaliza-se prontamente para $D$ variáveis [^2]. Se $x_i$ é uma variável visível, nós não a amostramos, uma vez que seu valor já é conhecido [^2].

A expressão $p(x_i | x_{-i})$ é chamada de **full conditional** para a variável $i$ [^2]. Em geral, $x_i$ pode depender somente de algumas das outras variáveis. Se representarmos $p(x)$ como um modelo gráfico, podemos inferir as dependências olhando para o *Markov blanket* de $i$, que são seus vizinhos no grafo [^2]. Assim, para amostrar $x_i$, precisamos apenas conhecer os valores dos vizinhos de $i$ [^2]. Neste sentido, Gibbs sampling é um algoritmo distribuído [^2]. Contudo, não é um algoritmo paralelo, uma vez que as amostras devem ser geradas sequencialmente [^2].

Por razões que serão explicadas posteriormente na seção 24.4.1, é necessário descartar algumas das amostras iniciais até que a cadeia de Markov tenha *burned in*, ou entrado em sua distribuição estacionária [^2]. A estimativa de quando o *burn-in* ocorreu será discutida na seção 24.4.1 [^2].

#### Exemplo: Gibbs sampling para o modelo de Ising
Na seção 21.3.2, aplicamos *mean field* para um modelo de Ising [^2]. Aqui aplicamos Gibbs sampling. Gibbs sampling em *pairwise* MRF/CRF toma a forma:
$$\np(x_t|x_{-t}, \theta) \propto \prod_{s \in nbr(t)} \psi_{st}(x_s, x_t)$$\
No caso de um modelo de Ising com potenciais de aresta $\psi(x_s, x_t) = exp(Jx_sx_t)$, onde $x_t \in \{-1, +1\}$, o *full conditional* se torna:
$$\np(x_t = +1|x_{-t}, \theta) = \frac{\prod_{s\in nbr(t)} \psi_{st}(x_t = +1, x_s)}{\prod_{s\in nbr(t)} \psi(x_t = +1, x_s) + \prod_{s\in nbr(t)} \psi(x_t = -1, x_s)}$$\
$$\n= \frac{exp[J\sum_{s \in nbr(t)} x_s]}{exp[J\sum_{s \in nbr(t)} x_s] + exp[-J\sum_{s \in nbr(t)} x_s]}$$\
$$\n= \frac{exp[J\eta_t]}{exp[J\eta_t] + exp[-J\eta_t]} = sigm(2J\eta_t)$$\
onde $J$ é a força de acoplamento, $\eta_t \equiv \sum_{s \in nbr(t)} x_s$ e $sigm(u) = 1/(1+e^{-u})$ é a função sigmóide [^3]. É fácil ver que $\eta_t = x_+(a_t - d_t)$, onde $a_t$ é o número de vizinhos que concordam com (têm o mesmo sinal que) t, e $d_t$ é o número de vizinhos que discordam [^3]. Se este número é igual, as "forças" em $x_t$ se cancelam, então o *full conditional* é uniforme [^3].

Podemos combinar uma *Ising prior* com um termo de evidência local $\psi_t$ [^3]. Por exemplo, com um modelo de observação Gaussiano, temos $\psi_t(x_t) = N(y_t|x_t, \sigma^2)$ [^3]. O *full conditional* se torna:
$$\np(x_t = +1|x_{-t}, y, \theta) = \frac{exp[J\eta_t]\psi_t(+1)}{exp[J\eta_t]\psi_t(+1) + exp[-J\eta_t]\psi_t(-1)}$$\
$$\n= sigm\left(2J\eta_t - log \frac{\psi_t(+1)}{\psi_t(-1)}\right)$$\
Agora a probabilidade de $x_t$ entrar em cada estado é determinada tanto pela compatibilidade com seus vizinhos (a *Ising prior*) quanto pela compatibilidade com os dados (o termo de verossimilhança local) [^3].

### Conclusão
O MCMC, e especificamente a Gibbs sampling, oferece uma abordagem flexível e poderosa para a amostragem de distribuições complexas em espaços de alta dimensão [^1, 2]. Apesar dos desafios relacionados à convergência e à escolha de parâmetros adequados, o MCMC continua sendo uma ferramenta fundamental na estatística bayesiana e no *machine learning* [^1, 2].

### Referências
[^1]: Chapter 24. Markov chain Monte Carlo (MCMC) inference, pages 837-847
[^2]: Chapter 24. Markov chain Monte Carlo (MCMC) inference, Gibbs sampling, pages 838-839
[^3]: Chapter 24. Markov chain Monte Carlo (MCMC) inference, Gibbs sampling, pages 839
<!-- END -->