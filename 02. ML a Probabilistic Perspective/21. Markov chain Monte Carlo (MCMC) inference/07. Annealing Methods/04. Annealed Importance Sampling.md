## Annealed Importance Sampling

### Introdução
Este capítulo aprofunda o conceito de *Annealed Importance Sampling* (AIS), uma técnica avançada para amostragem de distribuições complexas. AIS combina elementos de *Simulated Annealing* e *Importance Sampling* para gerar amostras independentes de distribuições difíceis [^1]. Ao contrário dos métodos Monte Carlo via Cadeias de Markov (MCMC), que produzem amostras correlacionadas, o AIS visa obter amostras independentes, o que pode ser vantajoso em certas aplicações.

### Conceitos Fundamentais
O AIS constrói uma sequência de distribuições intermediárias que evoluem gradualmente de uma distribuição mais simples para a distribuição alvo [^1]. A ideia central é facilitar a transição entre distribuições complexas, permitindo uma exploração mais eficiente do espaço amostral.

Formalmente, o AIS funciona da seguinte forma:
1.  **Sequência de Distribuições Intermediárias:** Define-se uma sequência de distribuições de probabilidade $p_0(x), p_1(x), ..., p_n(x)$, onde $p_0(x)$ é uma distribuição fácil de amostrar e $p_n(x)$ é a distribuição alvo. Uma forma comum de construir essa sequência é através de uma interpolação entre duas distribuições, utilizando um parâmetro de temperatura inversa $\beta_j$:

    $$f(x) = f_0(x)^{\beta_j} f_n(x)^{1-\beta_j} \quad [24.102]$$

    onde $1 = \beta_0 > \beta_1 > ... > \beta_n = 0$ [^1]. Note que esta abordagem é diferente do *Simulated Annealing* tradicional, onde $f_j(x) = f_0(x)^{\beta_j}$ [^1].

2.  **Cadeias de Markov:** Para cada distribuição intermediária $p_j(x)$, utiliza-se uma cadeia de Markov $T_j(x, x')$ que preserva a distribuição $p_j(x)$ como sua distribuição estacionária [^1]. Isso significa que, se $x \sim p_j(x)$, então $x' \sim T_j(x, \cdot)$ também seguirá a distribuição $p_j(x)$.

3.  **Amostragem Annealed:** O processo de amostragem começa amostrando um ponto $z_{n-1}$ da distribuição inicial $p_n(x)$ [^1]. Em seguida, amostra-se sequencialmente $z_{n-2}$ da cadeia de Markov $T_{n-1}(z_{n-1}, \cdot)$, e assim por diante, até obter $z_0$ da cadeia de Markov $T_1(z_1, \cdot)$ [^1]. Finalmente, define-se $x = z_0$ como a amostra obtida.

4.  **Ponderação por Importance Sampling:** Cada amostra $x$ é associada a um peso $w$ que corrige o viés introduzido pelas distribuições intermediárias [^1]. O peso é calculado como:

    $$w = \frac{f_{n-1}(z_{n-1})}{f_n(z_{n-1})} \frac{f_{n-2}(z_{n-2})}{f_{n-1}(z_{n-2})} ... \frac{f_1(z_1)}{f_2(z_1)} \frac{f_0(z_0)}{f_1(z_0)} \quad [24.103]$$

    Este peso garante que as amostras geradas pelo AIS representem a distribuição alvo $p_0(x)$.

O processo de AIS pode ser interpretado como *importance sampling* em um espaço de estados estendido $z = (z_0, z_1, ..., z_{n-1})$ [^1]. A distribuição conjunta nesse espaço é dada por:

$$p(z) \propto f(z) = f_0(z_0) T_1(z_0, z_1) T_2(z_1, z_2) ... T_{n-1}(z_{n-2}, z_{n-1}) \quad [24.104]$$

onde $T_j$ é o reverso de $T_j$:\n
$$T_j(z, z') = T_j(z', z) \frac{p_j(z')}{p_j(z)} = T_j(z', z) \frac{f_j(z')}{f_j(z)} \quad [24.105]$$

A distribuição proposta pelo algoritmo é:

$$q(z) \propto g(z) = f_n(z_{n-1}) T_{n-1}(z_{n-1}, z_{n-2}) ... T_2(z_2, z_1) T_1(z_1, z_0) \quad [24.106]$$

Os pesos de *importance sampling* são então dados por $w = \frac{f(z)}{g(z)}$, que corresponde à Equação 24.103 [^1].

### Conclusão
O *Annealed Importance Sampling* oferece uma abordagem alternativa aos métodos MCMC, com a vantagem de gerar amostras independentes [^1]. A construção da sequência de distribuições intermediárias e a escolha das cadeias de Markov são cruciais para o desempenho do AIS. Embora conceitualmente mais complexo do que o *Importance Sampling* tradicional, o AIS pode ser mais eficiente para distribuições multimodais, pois facilita a transição entre diferentes modos [^1].

<!-- END -->