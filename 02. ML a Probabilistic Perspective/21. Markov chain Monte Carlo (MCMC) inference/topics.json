{
  "topics": [
    {
      "topic": "Markov Chain Monte Carlo (MCMC)",
      "sub_topics": [
        "Markov Chain Monte Carlo (MCMC) is a widely used method for sampling from high-dimensional distributions, recognized as one of the top algorithms of the 20th century and essential in various machine learning applications. It constructs a Markov chain on the state space whose stationary distribution matches the target density of interest, allowing for Monte Carlo integration by drawing correlated samples from the chain. The algorithm performs a random walk on the state space, ensuring the fraction of time spent in each state is proportional to the target density, facilitating the approximation of complex distributions.",
        "MCMC has become widely popular in Bayesian statistics and increasingly popular in machine learning due to its ease of implementation and applicability to a broader range of models, including those with changing size or structure depending on variable values, or those lacking nice conjugate priors, offering flexibility compared to variational methods. MCMC can be faster than variational methods when applied to very large models or datasets due to passing sparse messages. Advantages of MCMC over variational inference include easier implementation and applicability to models with changing size or structure, while variational inference is generally faster for small to medium problems and provides a lower bound on the log likelihood.",
        "Gibbs sampling is a popular MCMC algorithm where each variable is sampled in turn, conditioned on the current values of all other variables in the distribution, generating a new sample by sampling each component based on the most recent values of others. It generalizes readily to D variables and infers dependencies through the Markov blanket in graphical models, making it a distributed algorithm suitable for graphical models. The full conditional for variable i, p(xi|x_{-i}), is a key component in Gibbs sampling, where x_i represents all variables except xi; dependencies can be inferred by examining i's Markov blanket in a graphical model, simplifying the sampling process. However, it is not a parallel algorithm, as samples must be generated sequentially.",
        "Collapsed Gibbs sampling analytically integrates out some unknown quantities, resulting in a collapsed Gibbs sampler that is more efficient due to sampling in a lower-dimensional space, achieved through Rao-Blackwellisation, which guarantees a lower variance estimate. Parameters do not participate in the Markov chain, and conditionally independent samples are drawn from p(\\u03b8|z, D). The Rao-Blackwell theorem guarantees that the variance of the estimate created by analytically integrating out \\u03b8 will always be lower than the variance of a direct Monte Carlo estimate, making collapsed Gibbs sampling a variance reduction technique.",
        "Metropolis-Hastings (MH) algorithm is a more general algorithm than Gibbs sampling, involving proposing a move from the current state to a new state with probability q(x'|x) and accepting this proposal based on a formula that ensures the fraction of time spent in each state is proportional to p*(x), making it flexible and applicable to models where Gibbs sampling is not suitable. In MH, the proposal distribution q(x'|x) can be any distribution subject to certain conditions. The Hastings correction compensates for asymmetry in the proposal distribution to ensure detailed balance is maintained. A significant advantage of MH is that it only requires knowing the target density up to a normalization constant, as the normalization constant cancels out in the acceptance ratio, allowing sampling from p* even if the normalization constant is unknown. Gibbs sampling is a special case of MH, where the proposal distribution is the full conditional, and the acceptance rate is always 100%."
      ]
    },
    {
      "topic": "Gibbs Sampling Details",
      "sub_topics": [
        "Gibbs sampling is an MCMC technique where each variable is sampled conditioned on the current values of all other variables; in physics, it is referred to as Glauber dynamics or the heat bath method. The algorithm iteratively samples each variable from its full conditional distribution given the current values of all other variables, allowing the Markov chain to gradually converge to the joint distribution. It is an MCMC analog of coordinate descent.",
        "The full conditional for a variable 'i' is the conditional distribution p(xi|x_{-i}), where 'i' may only depend on some other variables, determined by its Markov blanket (neighbors) in a graphical model. In graphical models, dependencies can be inferred by examining i's Markov blanket, which includes its neighbors in the graph, allowing sampling of xi based only on the values of its neighbors. In pairwise MRF/CRF models, Gibbs sampling involves sampling from the conditional distribution p(Xt|X\\u2212t, \\u03b8), which is proportional to the product of pairwise potentials \\u03c8st(xs, xt) over all neighbors s \\u2208 nbr(t), reflecting the compatibility between neighboring variables in the model.",
        "Gibbs sampling can be applied to models such as the Ising model, where the probability of a variable entering each state is determined by compatibility with its neighbors and the data, illustrating its utility in spatial modeling and image processing tasks."
      ]
    },
    {
      "topic": "Metropolis-Hastings Algorithm Details",
      "sub_topics": [
        "The Metropolis-Hastings (MH) algorithm is a general MCMC method where, at each step, a move from the current state x to a new state x' is proposed with probability q(x'|x), where q is the proposal distribution or kernel. The Metropolis-Hastings algorithm uses a proposal distribution to generate new states and an acceptance formula to decide whether to accept the proposed state, ensuring the algorithm samples from the desired distribution even if the proposal is asymmetric. It allows the user to define any proposal distribution, making it a flexible method, with a common choice being a symmetric Gaussian distribution centered on the current state, known as a random walk Metropolis algorithm.",
        "The acceptance probability in MH is designed to ensure that the fraction of time spent in each state x is proportional to p*(x), and for asymmetric proposals, a Hastings correction is applied to compensate for the proposal distribution favoring certain states. The acceptance probability in MH is given by r = min(1, \\u03b1), where \\u03b1 = [p*(x')q(x|x')] / [p*(x)q(x'|x)], and the Hastings correction compensates for asymmetry in the proposal distribution to ensure detailed balance is maintained.",
        "Gibbs sampling is a special case of the Metropolis-Hastings algorithm, where the proposal distribution is the full conditional, resulting in an acceptance rate of 100%. It is equivalent to using a sequence of proposals of the form q(x'|x) = p(xi|x_{-i})I(x_{-i} = x_{-i}), where xi is sampled from its full conditional, resulting in an acceptance rate of 100%."
      ]
    },
    {
      "topic": "Proposal Distributions in MCMC",
      "sub_topics": [
        "A proposal distribution q is valid if it provides a non-zero probability of moving to states with non-zero probability in the target distribution, ensuring the algorithm can explore the relevant regions of the state space. A proposal distribution q is considered valid or admissible for a target distribution p* if it assigns a non-zero probability of moving to states that have non-zero probability in the target distribution, ensuring the entire support of the target distribution can be reached.",
        "In practice, it is important for the proposal to spread its probability mass appropriately, with a Gaussian random walk proposal being a valid choice for continuous state spaces. A Gaussian random walk proposal, due to its non-zero probability density across the entire state space, serves as a valid proposal for any continuous state space; however, the practical efficacy depends on appropriately spreading its probability mass. However, its variance must be tuned correctly; too low, and the chain only explores one mode; too high, and most moves are rejected, resulting in a sticky chain, highlighting the importance of balancing exploration and acceptance.",
        "The Hessian matrix at a local mode can be used to define the covariance of a Gaussian proposal distribution, allowing the proposal to adapt to the local curvature and length scales of each dimension. In the context of Gaussian proposals, Roberts and Rosenthal (2001) demonstrated that, for a Gaussian posterior, an asymptotically optimal scaling factor of s\\u00b2 = 2.38\\u00b2/D (where D is the dimensionality of w) results in an acceptance rate of approximately 0.234.",
        "One can also use mixture proposals, which combine several base proposals into a convex combination, ensuring overall validity as long as each individual proposal is valid, allowing the algorithm to adapt to different regions of the state space. When faced with uncertainty about the appropriate proposal distribution, one can employ a mixture proposal, constructed as a convex combination of base proposals, provided that each base proposal is individually valid.",
        "Data-driven MCMC uses proposals that depend on both the previous hidden state and the visible data, training a discriminative classifier to predict p(x|f(D)), where f(D) are features extracted from the visible data, improving the efficiency of the proposals."
      ]
    },
    {
      "topic": "Speed and Accuracy of MCMC",
      "sub_topics": [
        "In MCMC, the 'burn-in' phase is a critical initial period where samples are discarded to allow the chain to 'forget' its starting point and converge to the stationary distribution, ensuring subsequent samples are representative of the target distribution. The burn-in phase refers to the initial period of MCMC where the chain has not yet reached its stationary distribution, and samples collected during this phase are typically discarded to ensure they come from the target distribution. A key challenge in MCMC is ensuring that the Markov chain has 'burned in,' meaning it has reached its stationary distribution, so initial samples must be discarded to avoid bias.",
        "The mixing time of a Markov chain is the time it takes to converge to the stationary distribution and forget its initial state, with a lower mixing time indicating faster convergence and more efficient sampling. The mixing time is the time it takes for a Markov chain to converge to the stationary distribution, determined by the eigengap \\u03b3 = \\u03bb1 - \\u03bb2, and is influenced by the geometry of the state space, with low conductance chains having high mixing times. Estimating the mixing time of a chain is generally difficult, but alternative approaches involve examining the geometry of the state space; for instance, chains with low conductance (i.e., chains with narrow bottlenecks between regions of high probability) tend to have high mixing times.",
        "Practical convergence diagnostics for MCMC involve running multiple chains from overdispersed starting points and examining trace plots to assess mixing, along with quantitative measures like the estimated potential scale reduction (EPSR) to ensure reliable estimates. Practical convergence diagnostics are used to assess whether the chain has converged, such as running multiple chains from overdispersed starting points and plotting the samples of variables to check for overlap and consistency. Practical convergence diagnostics, while not definitively proving convergence, can indicate non-convergence; one common approach is to run multiple chains from overdispersed starting points and examine trace plots to see if they converge to the same distribution.",
        "The estimated potential scale reduction (EPSR) compares the variance within each chain to the variance across chains, helping to determine if the chains have ranged over the full posterior and converged to the same distribution. The Estimated Potential Scale Reduction (EPSR) is a quantitative measure used to assess MCMC convergence by comparing the variance within each chain to the variance across chains; values close to 1 suggest convergence, while higher values indicate potential issues. The estimated potential scale reduction (EPSR) compares the variance of a quantity within each chain to its variance across chains, with R \\u2248 1 indicating convergence and is based on within-sequence variance (W) and between-sequence variance (B).",
        "The accuracy of MCMC samples is affected by autocorrelation, which reduces the information content; thinning can mitigate this by keeping every n'th sample, and the effective sample size (ESS) quantifies the information content of correlated samples. MCMC samples are autocorrelated, reducing their information content, which can be quantified by the effective sample size (ESS), reflecting the number of independent samples to achieve the same accuracy as the autocorrelated samples. The samples produced by MCMC are auto-correlated, reducing their information content relative to independent samples; the autocorrelation function (ACF) measures this correlation, and thinning can be used to reduce it."
      ]
    },
    {
      "topic": "Auxiliary Variable MCMC",
      "sub_topics": [
        "Auxiliary variable MCMC improves sampling efficiency by introducing dummy variables that reduce correlation between original variables, requiring that the extended model is easier to sample from and that integrating out the auxiliary variables recovers the original distribution. Auxiliary variable MCMC improves sampling efficiency by introducing dummy variables to reduce correlations between original variables, requiring that the marginal distribution of the original variables remains unchanged and that the joint distribution with auxiliary variables is easier to sample from. Auxiliary variable MCMC improves sampling efficiency by introducing dummy variables z, requiring that \\u03a3 p(x, z) = p(x) and that p(x, z) is easier to sample from than p(x), allowing sampling in an enlarged model and discarding sampled z values.",
        "In auxiliary variable sampling for logistic regression, a latent variable interpretation is used, where a latent variable is introduced, and sampling is performed by alternating between sampling the model parameters and the latent variables.",
        "Slice sampling enhances the ability to make large moves in univariate distributions by adding an auxiliary variable u, defining a joint distribution, and sampling from slices, where the full conditionals have a specific form related to the original distribution. Slice sampling enhances the ability to make large moves in a univariate distribution by adding an auxiliary variable, defining a joint distribution, and sampling by alternating between sampling uniformly under the curve and sampling from the slice.",
        "The Swendsen-Wang algorithm, an auxiliary variable MCMC sampler, enhances mixing in Ising models by introducing bond variables on edges, which, combined with Gibbs sampling, enables larger moves through the state space. The Swendsen-Wang algorithm is an auxiliary variable MCMC sampler for Ising models that introduces auxiliary binary variables called bond variables, leading to faster mixing compared to Gibbs sampling, especially for attractive models with J > 0. The Swendsen-Wang algorithm, designed for Ising models, introduces auxiliary binary variables (bond variables) to facilitate faster mixing by allowing the algorithm to disconnect edges between nodes with different assignments and to update entire connected components simultaneously, addressing the slow mixing issues of standard Gibbs sampling in certain scenarios.",
        "Hybrid/Hamiltonian MCMC is used in continuous state spaces where the gradient of the unnormalized log-posterior can be computed, and it treats the parameters as a particle in space, introducing auxiliary variables representing momentum and updating both using specific rules. Hybrid/Hamiltonian MCMC is a method for continuous state spaces that introduces auxiliary variables representing the 'momentum' of the parameters, updating the parameter/momentum pair according to certain rules based on the gradient of the (unnormalized) log-posterior."
      ]
    },
    {
      "topic": "Annealing Methods",
      "sub_topics": [
        "Annealing methods are used to handle multimodal distributions by analogy to the way metals are heated and cooled; they involve using a computational temperature parameter to smooth out the distribution and gradually cooling it to recover the original distribution.",
        "Simulated annealing is a stochastic algorithm for finding the global optimum of a black-box function, inspired by statistical physics, where the Boltzmann distribution guides the search and the temperature parameter controls exploration versus exploitation. Simulated annealing is a stochastic algorithm that attempts to find the global optimum of a black-box function by sampling from a Boltzmann distribution, where the temperature is gradually decreased to concentrate the probability mass around the minimum energy state. Simulated annealing is a stochastic algorithm for finding the global optimum of a black-box function f(x) by generating samples from a Boltzmann distribution p(x) \\u221d exp(-f(x)/T) and gradually cooling the temperature T to 'track' the largest peak.",
        "In simulated annealing, the cooling schedule, which dictates how the temperature changes over time, is critical for finding the global optimum; exponential cooling is common, but the best schedule is difficult to determine.",
        "Annealed importance sampling combines ideas from simulated annealing and importance sampling to draw independent samples from difficult distributions by constructing a sequence of intermediate distributions that move slowly from an easier distribution to the target distribution. Annealed importance sampling combines simulated annealing and importance sampling to draw independent samples from difficult distributions by constructing a sequence of intermediate distributions that move slowly from an easier distribution to the target distribution. Annealed importance sampling combines simulated annealing and importance sampling to draw independent samples from difficult distributions by constructing a sequence of intermediate distributions and using Markov chains to transition between them.",
        "Parallel tempering combines MCMC and annealing by running multiple chains in parallel at different temperatures, allowing chains at higher temperatures to make long-distance moves and influence the lower temperature chains. Parallel tempering combines MCMC and annealing by running multiple chains in parallel at different temperatures, allowing a high temperature chain to make long distance moves and influence lower temperature chains."
      ]
    },
    {
      "topic": "Markov Chain Monte Carlo (MCMC) Inference",
      "sub_topics": [
        "MCMC constructs a Markov chain on the state space X with a stationary distribution that matches the target density p*(x), enabling Monte Carlo integration by drawing correlated samples from the chain. MCMC constructs a Markov chain on the state space X, aiming for a stationary distribution that matches the target density p*(x) of interest, which could be a prior or posterior distribution, using a random walk approach where the time spent in each state x is proportional to p*(x). MCMC constructs a Markov chain on the state space whose stationary distribution is the target density of interest, enabling Monte Carlo integration by drawing correlated samples from the chain, where the fraction of time spent in each state is proportional to the target density.",
        "The MCMC algorithm, originating from physics, has gained popularity in Bayesian statistics and machine learning, offering an alternative to variational inference, especially for complex models or large datasets. MCMC methods are used to sample from high-dimensional distributions, addressing the limitations of simpler Monte Carlo methods like rejection sampling and importance sampling in such spaces. MCMC's widespread adoption is due to its applicability to a broad range of models, including those with varying size or structure depending on variable values or models lacking conjugate priors, offering an alternative to variational methods, especially for large datasets.",
        "Advantages of MCMC over variational inference include easier implementation and applicability to models with changing size or structure, or models without conjugate priors; it can be faster for very large datasets. The advantages of MCMC over variational inference include easier implementation and applicability to a broader range of models, while variational inference is generally faster for small to medium problems and provides a lower bound on the log likelihood. Advantages of MCMC compared to variational inference include easier implementation and applicability to a broader range of models, including those with changing size/structure or without conjugate priors; MCMC can also be faster than variational methods for very large datasets due to passing sparse messages.",
        "A key challenge in MCMC is ensuring that the Markov chain has 'burned in,' meaning it has reached its stationary distribution, so initial samples must be discarded to avoid bias. A critical aspect of MCMC is ensuring that the Markov chain has 'burned in' or reached its stationary distribution, necessitating the discarding of initial samples; the determination of when burn-in occurs is discussed later. A key challenge in MCMC is ensuring that the Markov chain has 'burned in' or reached its stationary distribution, requiring the discarding of initial samples; estimating when burn-in has occurred is crucial to ensure samples are drawn from the target distribution.",
        "Gibbs sampling, a popular MCMC algorithm, samples each variable in turn, conditioned on the current values of all other variables, making it an MCMC analog of coordinate descent. Gibbs sampling is a popular MCMC algorithm that samples each variable in turn, conditioned on the values of all other variables in the distribution, generalizing readily to D variables and inferring dependencies from the Markov blanket in graphical models.",
        "The full conditional p(xi|x_i) represents the conditional distribution of variable i given all other variables, and in graphical models, dependencies can be inferred from i's Markov blanket.",
        "Gibbs sampling is a distributed algorithm, but not parallel, as samples must be generated sequentially, relying on the most recent values of neighboring variables. In the context of GMMs, Gibbs sampling involves iteratively sampling the discrete indicators, mixing weights, means and covariances, using conjugate priors to simplify the full conditionals.",
        "A fundamental weakness of Gibbs sampling for mixture models is label switching, where the parameters and indicator functions are unidentifiable due to the arbitrary permutation of hidden labels; this can be addressed by focusing on uniquely identifiable questions or post-processing the samples.",
        "Collapsed Gibbs sampling improves efficiency by analytically integrating out some unknown quantities, resulting in sampling from a lower-dimensional space and reducing variance, as guaranteed by the Rao-Blackwell theorem. Collapsed Gibbs sampling analytically integrates out some unknown quantities, resulting in a lower-dimensional space and more efficient sampling, with parameters not participating in the Markov chain and conditionally independent samples drawn from p(\\u03b8|z, D).",
        "The Metropolis-Hastings (MH) algorithm is a more general MCMC method that proposes moves to new states based on a proposal distribution q(x'|x) and accepts or rejects the move to ensure the correct stationary distribution. The MH algorithm requires a Hastings correction when the proposal distribution is asymmetric, compensating for the fact that the proposal distribution itself might favor certain states. Gibbs sampling is a special case of MH, where the proposal distribution is the full conditional, and the acceptance rate is always 100%.",
        "Proposal distributions in MH must be valid, giving non-zero probability of moving to states with non-zero probability in the target distribution, and should be tuned to ensure efficient exploration of the state space. Gaussian proposals are common in continuous state spaces, with the Hessian at a local mode informing the covariance structure to model local curvature and length scales.",
        "Adaptive MCMC adjusts the proposal distribution during the algorithm's execution to improve efficiency, but must be done carefully to avoid violating the Markov property. It is necessary to start MCMC in an initial state that has non-zero probability; deterministic constraints may make finding such a legal configuration difficult. The MH algorithm defines a Markov chain with a specific transition matrix, and detailed balance ensures that the stationary distribution is the target distribution. Reversible jump MCMC (RJMCMC) is used to compare models with different numbers of parameters by augmenting the lower-dimensional space with extra random variables to achieve a common measure.",
        "The mixing time of a Markov chain is the time it takes to converge to the stationary distribution and is related to the eigengap of the transition matrix and the conductance of the chain. Practical convergence diagnostics include running multiple chains from overdispersed starting points and examining trace plots for overlap, as well as using the estimated potential scale reduction (EPSR) to compare within-chain and between-chain variances.",
        "MCMC produces autocorrelated samples, reducing their information content; thinning can reduce autocorrelation, and effective sample size (ESS) quantifies the information content of a set of samples. Auxiliary variable MCMC improves sampling efficiency by introducing dummy variables to reduce correlation between original variables, sampling in the enlarged model, and then discarding the auxiliary variables. The Swendsen-Wang algorithm is an auxiliary variable MCMC sampler for Ising models that mixes much faster than Gibbs sampling by introducing bond variables and finding connected components. Hybrid/Hamiltonian MCMC uses the gradient of the log-posterior to create auxiliary variables representing the momentum of a particle in space, updating both position and momentum to efficiently explore continuous state spaces.",
        "Annealing methods, like simulated annealing, use a computational temperature parameter to smooth out multimodal distributions, gradually cooling to recover the original distribution and find the global optimum. Annealed importance sampling combines simulated annealing and importance sampling to draw independent samples from difficult distributions by constructing a sequence of intermediate distributions and using Markov chains to transition between them. Parallel tempering runs multiple chains at different temperatures, allowing high-temperature chains to make long-distance moves and influence lower-temperature chains.",
        "Approximating the marginal likelihood involves estimating an often intractable integral; methods like the candidate method and harmonic mean estimate have limitations. Annealed importance sampling can be used to evaluate a ratio of partition functions and estimate the marginal likelihood, provided the prior has a known normalization constant."
      ]
    },
    {
      "topic": "Gibbs Sampling for GMM Parameter Inference",
      "sub_topics": [
        "Gibbs sampling can be used to estimate the parameters of a Gaussian Mixture Model (GMM) by iteratively sampling from the full conditional distributions of the component means, covariances, and mixing weights, given the observed data and the current values of the other parameters.",
        "Using conjugate priors simplifies the derivation of the Gibbs sampling algorithm, allowing straightforward updates for the discrete indicators, mixing weights, means, and covariances of the Gaussian mixture components. A semi-conjugate prior can be used to define the full joint distribution, expressed as a product of conditional distributions for data points given cluster assignments, cluster assignments given mixing weights, mixing weights, component means, and component covariances.",
        "The full conditionals for the discrete indicators, p(zi = k | xi, \\u03bc, \\u03a3, \\u03c0), are proportional to the product of the mixing weight \\u03c0k and the Gaussian density N(xi | \\u03bck, \\u03a3k), indicating the probability of data point xi belonging to cluster k.",
        "The problem of label switching arises in Gibbs sampling for mixture models because the parameters and indicator functions are unidentifiable, leading to challenges in computing posterior means and requiring careful consideration of how to address this issue.",
        "Collapsed Gibbs sampling can improve efficiency by analytically integrating out some unknown quantities (e.g., model parameters) and only sampling the remaining variables, leading to a lower-dimensional space and reduced variance in the estimates (Rao-Blackwellisation)."
      ]
    },
    {
      "topic": "Approximating the Marginal Likelihood",
      "sub_topics": [
        "The marginal likelihood p(D|M) is a key quantity for Bayesian model selection, but it is often intractable to compute; approximations using Monte Carlo methods are commonly employed.",
        "The candidate method approximates the marginal likelihood by evaluating the joint distribution and prior at a specific point and dividing by the posterior at that point, but it relies on the assumption that the posterior has been marginalized over all modes, which is often not the case.",
        "The harmonic mean estimate approximates the marginal likelihood using the harmonic mean of the likelihood of the data under each sample drawn from the posterior; however, this method is known to perform poorly in practice because it depends only on samples drawn from the posterior, which is often insensitive to the prior.",
        "Annealed importance sampling can be used to evaluate a ratio of partition functions, which can then be used to approximate the marginal likelihood, provided the prior has a known normalization constant, making it a preferred method for evaluating difficult partition functions."
      ]
    }
  ]
}