## Expectation Propagation: A Variational Perspective

### Introdução
Este capítulo explora a **Expectation Propagation (EP)** como um método de *belief propagation* que generaliza o algoritmo *assumed density filtering (ADF)* e pode ser enquadrado como um problema de inferência variacional [^787]. Em contraste com a inferência variacional *mean field*, que aproxima a posterior por um produto de distribuições marginais [^767], a EP aproxima as mensagens trocadas em um grafo de fatores, oferecendo uma abordagem iterativa para a inferência aproximada. LBP, ou *loopy belief propagation*, é um caso especial de EP [^787].

### Conceitos Fundamentais
**Expectation Propagation (EP)** é uma técnica de inferência aproximada que se encaixa no paradigma de *belief propagation* [^787]. Diferentemente do *mean field*, que assume uma fatoração completa da distribuição posterior [^767], a EP opera aproximando as mensagens trocadas entre os nós em um grafo de fatores.

A ideia central da EP é particionar os parâmetros e as estatísticas suficientes em termos tratáveis ($\theta$) e intratáveis ($\theta_i$), aproximando o conjunto convexo $M$ com um conjunto convexo maior $L$ [^787, 788]. Matematicamente, o problema pode ser expresso da seguinte forma [^788]:
$$\np(\mathbf{x}|\mathbf{\theta},\mathbf{\tilde{\theta}}) \propto f_0(\mathbf{x}) \exp(\mathbf{\theta}^T \phi(\mathbf{x})) \prod_{i=1}^{d_I} \exp(\tilde{\theta}_i^T \tilde{\phi}_i(\mathbf{x}))\n$$
onde $\mathbf{x}$ representa as variáveis latentes, $\phi(\mathbf{x})$ e $\tilde{\phi}_i(\mathbf{x})$ são as estatísticas suficientes, e $f_0(\mathbf{x})$ é um termo tratável.

A EP itera sobre os fatores intratáveis, aproximando cada fator individualmente enquanto mantém uma aproximação global da distribuição posterior. O algoritmo envolve os seguintes passos [^790]:

1.  **Escolher um fator** $f_i$ para refinar.
2.  **Remover o fator** $f_i$ da aproximação da posterior, dividindo:
    $$\
    q_{-i}(\mathbf{x}) = \frac{q(\mathbf{x})}{\tilde{f}_i(\mathbf{x})}\
    $$
3.  **Calcular a nova posterior** $q^{\text{new}}(\mathbf{x})$ minimizando a divergência de Kullback-Leibler (KL):
    $$\
    \min_{q^{\text{new}}} KL(f_i(\mathbf{x}) q_{-i}(\mathbf{x}) || q^{\text{new}}(\mathbf{x}))\
    $$
4.  **Calcular o novo fator** (mensagem) que foi implicitamente usado:
    $$\
    \tilde{f}_i(\mathbf{x}) = Z_i \frac{q^{\text{new}}(\mathbf{x})}{q_{-i}(\mathbf{x})}\
    $$
Um aspecto crucial da EP é a utilização do *moment matching* para atualizar os parâmetros das aproximações. Isso envolve garantir que os momentos da aproximação global correspondam aos momentos da distribuição "correta" [^790].

**Expectation Propagation e Loopy Belief Propagation (LBP)**:
LBP pode ser vista como um caso especial de EP, onde a distribuição base contém as marginais dos nós e os termos intratáveis correspondem aos potenciais das arestas, resultando na aproximação de Bethe para a entropia [^787]. Em outras palavras, LBP é uma aproximação de EP onde os fatores são definidos pelas arestas do grafo [^792].

**TrueSkill**:
No contexto de *ranking* de jogadores, o TrueSkill utiliza EP para estimar e atualizar os níveis de habilidade dos jogadores com base nos resultados dos jogos [^787]. Ele usa uma representação de grafo de fatores e aproximações Gaussianas para as distribuições de habilidade, permitindo um *ranking* preciso em sistemas de jogos online [^787].

### Conclusão
A Expectation Propagation oferece uma abordagem flexível e poderosa para inferência aproximada em modelos gráficos. Ao aproximar as mensagens e particionar os termos em tratáveis e intratáveis, a EP permite uma inferência eficiente, especialmente em modelos complexos onde a inferência exata é inviável. Sua conexão com LBP e aplicações práticas, como o TrueSkill, demonstram sua relevância e utilidade em diversos domínios.

### Referências
[^767]: Murphy, K. P. (2012). *Machine learning: a probabilistic perspective*. MIT press.
[^787]: Minka, T. P. (2001c). Expectation propagation for approximate Bayesian inference. *arXiv preprint cs/0103001*.
[^788]: Wainwright, M. J., & Jordan, M. I. (2008b). Graphical models, exponential families, and variational inference. *Foundations and Trends in Machine Learning*, *1*(1-2), 1-305.
[^790]: Bishop, C. M. (2006b). Pattern recognition and machine learning. *Information Science and Statistics. Springer*.
[^792]: Minka, T. P. (2001d). A family of algorithms for approximate Bayesian inference. *Doctoral dissertation, Massachusetts Institute of Technology*.
<!-- END -->