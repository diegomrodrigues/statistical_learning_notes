## 22.3.4 Mean Field Inference as a Variational Optimization Problem

### Introdução
Em continuidade ao capítulo anterior, que discutiu a inferência de campo médio (Mean Field - MF), este capítulo visa reinterpretar a inferência de campo médio dentro de uma estrutura mais abstrata, a da inferência variacional [^1]. Esta reformulação facilitará a comparação com outros métodos aproximados que serão discutidos posteriormente [^2].

### Conceitos Fundamentais
A inferência de campo médio, como vimos anteriormente [^1], aproxima a distribuição posterior por um produto de distribuições marginais. Agora, vamos considerar como isso pode ser visto como um problema de otimização variacional.

Primeiramente, seja $F$ um subgrafo de arestas do grafo original $G$, e seja $I(F) \subseteq I$ o subconjunto de estatísticas suficientes associadas aos cliques de $F$ [^2]. Seja $\Omega$ o conjunto de parâmetros canônicos para o modelo completo, e defina o espaço de parâmetros canônicos para o submodelo como segue [^2]:

$$Omega(F) \equiv \{\theta \in \Omega : \theta_\alpha = 0 \quad \forall \alpha \in I \setminus I(F) \} \quad (22.25)$$

Em outras palavras, exigimos que os parâmetros naturais associados às estatísticas suficientes $\alpha$ fora da classe escolhida sejam zero [^2]. Por exemplo, no caso de uma aproximação totalmente fatorada, $F_0$, removemos todas as arestas do gráfico, resultando em [^2]:

$$Omega(F_0) \equiv \{\theta \in \Omega : \theta_{st} = 0 \quad \forall (s, t) \in \mathcal{E} \} \quad (22.26)$$

No caso do campo médio estruturado (Seção 21.4 de [^1]), definimos $\theta_{st} = 0$ para as arestas que não estão no subgrafo tratável [^2].

Em seguida, definimos o espaço de parâmetros médios do modelo restrito como segue [^2]:

$$M_F(G) \equiv \{\mu \in \mathbb{R}^d : \mu = \mathbb{E}_\theta [\phi(x)] \text{ para algum } \theta \in \Omega(F) \} \quad (22.27)$$

Isto é chamado de **inner approximation** para o **marginal polytope**, uma vez que $M_F(G) \subseteq M(G)$ [^2]. Veja a Figura 22.7(b) de [^1] para um esboço. Note que $M_F(G)$ é um **non-convex polytope**, o que resulta em múltiplos ótimos locais [^2]. Em contraste, algumas das aproximações que consideraremos mais tarde serão convexas [^2].

Definimos a **entropia** da nossa aproximação $H(\mu(F))$ como a entropia da distribuição $\mu$ definida no submodelo $F$ [^2]. Então, definimos o problema de otimização funcional da energia de campo médio como segue [^2]:

$$max_{\mu \in M_F(G)} \theta^T \mu + H(\mu) \leq \log Z(\theta) \quad (22.28)$$

No caso da aproximação de campo médio totalmente fatorada para UGMs (Undirected Graphical Models) pairwise, podemos escrever este objetivo como segue [^2]:

$$max_{\mu_s \in \mathcal{P}} \sum_{s \in \mathcal{V}} \sum_{x_s} \theta_s(x_s) \mu_s(x_s) + \sum_{(s, t) \in \mathcal{E}} \sum_{x_s, x_t} \theta_{st}(x_s, x_t) \mu_s(x_s) \mu_t(x_t) + \sum_{s \in \mathcal{V}} H(\mu_s) \quad (22.29)$$

onde $\mu_s \in \mathcal{P}$, e $\mathcal{P}$ é o simplex de probabilidade sobre $\mathcal{X}$ [^2].

O campo médio envolve um objetivo côncavo sendo maximizado sobre um conjunto não convexo [^2]. É tipicamente otimizado usando **coordinate ascent**, uma vez que é fácil otimizar uma função escalar côncava sobre $\mathcal{P}$ para cada $\mu_s$ [^2]. Por exemplo, para um UGM pairwise, obtemos [^2]:

$$mu_s(x_s) \propto \exp(\theta_s(x_s)) \exp \left( \sum_{t \in \text{nbr}(s)} \sum_{x_t} \mu_t(x_t) \theta_{st}(x_s, x_t) \right) \quad (22.30)$$

### Conclusão
Esta seção demonstrou como a inferência de campo médio pode ser reinterpretada como um problema de otimização variacional envolvendo uma aproximação interna para o marginal polytope. O funcional de energia de campo médio é otimizado sobre um conjunto não convexo, tipicamente usando coordinate ascent. Essa formulação fornece uma base para comparar o MF com outros métodos aproximados de inferência que serão discutidos posteriormente [^2].

### Referências
[^1]: Chapter 21.
[^2]: Chapter 22.
<!-- END -->