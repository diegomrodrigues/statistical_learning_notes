## Técnicas para Melhorar a Convergência do LBP em Inferência Variacional

### Introdução
Em inferência variacional, o algoritmo Loopy Belief Propagation (LBP) é uma ferramenta simples e eficiente para a aproximação da inferência em modelos gráficos discretos ou Gaussianos [^1]. No entanto, o LBP não tem garantia de convergência e pode até convergir para resultados incorretos [^1]. Este capítulo explora técnicas para melhorar a convergência do LBP, incluindo amortecimento (damping), programação de mensagens (message scheduling) e algoritmos de _double loop_.

### Conceitos Fundamentais

#### Amortecimento (Damping)
O amortecimento é uma técnica que visa reduzir a oscilação das mensagens no LBP [^1]. Em vez de enviar a mensagem $M_{s \rightarrow t}(x_t)$ diretamente, envia-se uma mensagem amortecida, que é uma combinação da nova e da mensagem anterior [^1]:
$$ M_{s \rightarrow t}^k(x_t) = \lambda M_{s \rightarrow t}^k(x_t) + (1 - \lambda) M_{s \rightarrow t}^{k-1}(x_t) $$
onde $0 \le \lambda \le 1$ é o fator de amortecimento [^1]. Se $\lambda = 1$, o esquema se reduz ao LBP padrão. Valores de $\lambda$ em torno de 0.5 são comumente utilizados [^1]. O amortecimento resulta em convergência mais frequente em comparação com a atualização não amortecida, como ilustrado na Figura 22.5 [^1].

#### Programação de Mensagens (Message Scheduling)
A programação de mensagens refere-se à ordem em que as mensagens são atualizadas no LBP [^1]. A abordagem padrão é a atualização síncrona, onde todos os nós absorvem e enviam mensagens em paralelo [^1]:
$$ m^{k+1} = (f_1(m^k), ..., f_E(m^k)) $$
onde $E$ é o número de arestas, e $f_{st}(m)$ é a função que computa a mensagem para a aresta $s \rightarrow t$ dadas todas as mensagens antigas [^1]. Isso é análogo ao método de Jacobi para resolver sistemas lineares [^1].

Uma alternativa é a atualização assíncrona, inspirada no método de Gauss-Seidel, onde as mensagens são atualizadas sequencialmente [^1]:
$$ m_i^{k+1} = f_i(\{m_j^{k+1}: j < i\}, \{m_j^k: j > i\}) $$
onde a mensagem para a aresta $i$ é computada usando novas mensagens (iteração $k+1$) de arestas anteriores na ordem, e mensagens antigas (iteração $k$) de arestas posteriores na ordem [^1]. A escolha da ordem em que as mensagens são atualizadas levanta a questão de qual ordem usar. Uma ideia simples é usar uma ordem fixa ou aleatória [^1]. A Figura 22.5 mostra que a atualização assíncrona (amortecida) resulta em convergência mais frequente do que a atualização síncrona [^1].

Uma abordagem mais inteligente é usar uma ordenação adaptativa, como na _residual belief propagation_ [^1]. A intuição é focar os esforços computacionais nas variáveis mais incertas. A _residual belief propagation_ agenda as mensagens a serem enviadas de acordo com a norma da diferença de seu valor anterior [^1]:
$$ r(s, t, k) = || \log m_{s \rightarrow t}^k - \log m_{s \rightarrow t}^{k-1} || = \max_i | \log m_{s \rightarrow t}^k(i) - \log m_{s \rightarrow t}^{k-1}(i) | $$
As mensagens são armazenadas em uma fila de prioridade e sempre enviadas aquelas com o maior resíduo [^1]. Quando uma mensagem é enviada de $s$ para $t$, todas as outras mensagens que dependem de $m_{s \rightarrow t}$ (i.e., mensagens da forma $m_{t \rightarrow u}$ onde $u \in \text{nbr}(t) \setminus s$) precisam ser recalculadas; seu resíduo é recalculado e elas são adicionadas de volta à fila [^1].

Uma variação da _residual BP_ utiliza um limite superior no resíduo de uma mensagem em vez do resíduo real [^1]. Isso significa que as mensagens só são computadas se forem ser enviadas, e não apenas para avaliar o resíduo [^1].

#### Algoritmos de _Double Loop_
Os algoritmos de _double loop_ são métodos que garantem a convergência para um mínimo local do objetivo do LBP [^1]. No entanto, esses métodos são lentos e não melhoram significativamente a precisão [^1].

### Conclusão

Melhorar a convergência do LBP é crucial para sua aplicação eficaz em inferência variacional. Técnicas como amortecimento, programação de mensagens (especialmente a assíncrona e a _residual belief propagation_) são importantes para aumentar a probabilidade e a velocidade de convergência. Embora os algoritmos de _double loop_ ofereçam garantias teóricas de convergência, sua complexidade computacional e ganhos limitados em precisão restringem seu uso prático.

### Referências
[^1]: Murphy, Kevin P. *Machine Learning: A Probabilistic Perspective*. MIT Press, 2012.

<!-- END -->