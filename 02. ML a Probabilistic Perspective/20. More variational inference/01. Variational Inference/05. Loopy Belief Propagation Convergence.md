## Convergência e Estabilidade em Loopy Belief Propagation

### Introdução
Em Variational Inference, o algoritmo **Loopy Belief Propagation (LBP)** é uma ferramenta aproximada para inferência em modelos gráficos, especialmente em modelos com loops [^2]. No entanto, a convergência do LBP não é garantida e, mesmo quando converge, a precisão dos resultados pode ser questionável [^5]. Este capítulo explora os fatores que influenciam a convergência do LBP e as técnicas para melhorá-la, baseando-se nas informações fornecidas.

### Conceitos Fundamentais

#### Condições de Convergência e Exatidão
O LBP, quando aplicado a grafos com loops, pode não fornecer resultados corretos e pode até não convergir [^1]. Judea Pearl, o inventor do Belief Propagation para árvores, já alertava sobre essa questão em 1988 [^1]. A presença de loops faz com que a rede não seja mais *singly connected*, e a propagação local pode levar a mensagens circulando indefinidamente, impedindo a convergência para um equilíbrio estável [^1].

#### Técnicas para Melhorar a Convergência
1.  **Damping:** Uma técnica simples para reduzir a chance de oscilação é o *damping* [^7]. Em vez de enviar a mensagem original $M_{st}(x_t)$, enviamos uma mensagem amortecida da forma:

    $$\
    M'_{st}(x_t) = \lambda M_{st}(x_t) + (1 - \lambda)M_{s \rightarrow t}^{k-1}(x_t)\
    $$

    onde $0 \leq \lambda \leq 1$ é o fator de amortecimento [^7]. Usar um valor como $\lambda \approx 0.5$ é uma prática comum [^7].

2.  **Atualizações Assíncronas:** A abordagem padrão para implementar o LBP é realizar *synchronous updates*, onde todos os nós absorvem e enviam mensagens em paralelo [^7]. No entanto, *asynchronous updates* podem levar a uma convergência mais rápida [^8]. Podemos aplicar a mesma ideia do método de Gauss-Seidel, utilizando atualizações da forma:

    $$\
    m_{i}^{k+1} = f_i(\{m_{j}^{k+1}: j < i\}, \{m_{j}^{k}: j > i\})\
    $$

    onde a mensagem para a aresta $i$ é computada usando as novas mensagens (iteração $k+1$) de arestas anteriores na ordenação e as mensagens antigas (iteração $k$) de arestas posteriores [^8].

3. **Residual Belief Propagation:** Uma técnica que agenda o envio de mensagens com base na norma da diferença de seus valores anteriores [^8]. O residual da nova mensagem $m_{st}$ na iteração $k$ é definido como:

    $$\
    r(s, t, k) = || \log m_{st}^{k} - \log m_{st}^{k-1} || = \max_i |\log m_{st}^k(i) - \log m_{st}^{k-1}(i)|\
    $$

    Mensagens com maior residual são enviadas primeiro, e as mensagens que dependem delas são recalculadas e adicionadas de volta à fila [^8].

#### Computation Tree
A análise de convergência do LBP pode ser feita através da ferramenta *computation tree*, que visualiza as mensagens passadas durante o algoritmo [^6]. Após $T$ iterações de LBP, o resultado é equivalente à computação exata em uma *computation tree* de altura $T+1$ [^6]. A convergência ocorre quando as conexões nas arestas são suficientemente fracas, de modo que a influência das folhas na raiz diminui ao longo do tempo [^6].

### Conclusão

Embora o LBP seja uma ferramenta poderosa, sua convergência e exatidão dependem da estrutura do grafo e dos parâmetros do modelo. Técnicas como *damping* e *asynchronous updates* podem melhorar a convergência, enquanto a *computation tree* oferece insights sobre o progresso do algoritmo. Ao entender esses fatores e aplicar as técnicas apropriadas, podemos aumentar a probabilidade de obter resultados precisos com o LBP.

### Referências
[^1]: Page 767, 22.2.1 A brief history
[^2]: Page 767, 22.2 Loopy belief propagation: algorithmic issues
[^5]: Page 771, 22.2.4 Convergence
[^6]: Page 772, 22.2.4.1 When will LBP converge?
[^7]: Page 773, 22.2.4.2 Making LBP converge
[^8]: Page 774, 22.2.4.3 Increasing the convergence rate: message scheduling
<!-- END -->