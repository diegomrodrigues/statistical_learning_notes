## Loopy Belief Propagation: Algorithmic Issues and Variational Inference Perspective

### Introdução
O presente capítulo dedica-se a explorar o algoritmo de **Loopy Belief Propagation (LBP)**, uma técnica de inferência aproximada utilizada em modelos gráficos discretos ou Gaussianos. Como vimos anteriormente, a inferência variacional busca aproximar distribuições complexas por outras mais tratáveis. LBP, apesar de aplicado a grafos com loops onde não há garantia de convergência ou resultados corretos, frequentemente demonstra bom desempenho na prática [^1]. Exploraremos suas características algorítmicas, métodos para melhorar sua convergência e sua interpretação no contexto da inferência variacional.

### Conceitos Fundamentais

**Loopy Belief Propagation (LBP)** é uma versão do algoritmo de *Belief Propagation (BP)* aplicado a grafos que contêm loops [^1]. Em grafos sem loops (árvores), BP garante resultados corretos e converge. No entanto, em grafos com loops, essa garantia desaparece. Apesar disso, LBP é amplamente utilizado devido à sua simplicidade e eficiência, frequentemente superando o *mean field* [^1].

#### Algoritmo LBP para Modelos Pairwise

O algoritmo LBP pode ser aplicado a modelos gráficos não direcionados com fatores *pairwise*. A ideia básica é iterativamente aplicar as equações de passagem de mensagens até a convergência [^2]. O Algoritmo 22.1 [^2] descreve o pseudocódigo para um *pairwise* MRF (Markov Random Field):

**Algoritmo 22.1: Loopy Belief Propagation para um Pairwise MRF**

1.  *Input*: Potenciais de nós $\psi_s(x_s)$, potenciais de arestas $\psi_{st}(x_s, x_t)$;\n2.  *Initialize*: Mensagens $m_{s \rightarrow t}(x_t) = 1$ para todas as arestas $s \rightarrow t$;\n3.  *Initialize*: Crenças $bel_s(x_s) = 1$ para todos os nós $s$;\n4.  *Repeat*:\n5.  *Send message on each edge*:\n    $$m_{s \rightarrow t}(x_t) = \sum_{x_s} \left[ \psi_s(x_s) \psi_{st}(x_s, x_t) \prod_{u \in nbr(s) \setminus t} m_{u \rightarrow s}(x_s) \right]$$\n6.  *Update belief of each node*:\n    $$bel_s(x_s) \propto \psi_s(x_s) \prod_{t \in nbr(s)} m_{t \rightarrow s}(x_s)$$\n7.  *Until*: Crenças não mudam significativamente;\n8.  *Return*: Crenças marginais $bel_s(x_s)$.

#### LBP em Factor Graphs

Para lidar com modelos que possuem potenciais de *clique* de ordem superior (incluindo modelos direcionados onde alguns nós têm mais de um pai), é útil usar uma representação conhecida como *factor graph* [^3].

Um **factor graph** é um grafo bipartido não direcionado com dois tipos de nós: nós redondos representam variáveis, nós quadrados representam fatores, e há uma aresta de cada variável para cada fator que a menciona [^3].

Nesse contexto, o BP envolve dois tipos de mensagens: de variáveis para fatores e de fatores para variáveis [^5]:

$$m_{x \rightarrow f}(x) = \prod_{h \in nbr(x) \setminus \\{f\\}} m_{h \rightarrow x}(x) \qquad (22.4)$$\n
$$m_{f \rightarrow x}(x) = \sum_{y} f(x, y) \prod_{y \in nbr(f) \setminus \\{x\\}} m_{y \rightarrow f}(y) \qquad (22.5)$$\n

As crenças finais são computadas como o produto das mensagens de entrada [^5]:

$$bel(x) \propto \prod_{f \in nbr(x)} m_{f \rightarrow x}(x) \qquad (22.6)$$\n

#### Convergência e Técnicas de Melhoria

LBP não converge em todos os casos, e mesmo quando converge, pode convergir para respostas incorretas [^5]. A convergência de LBP pode ser influenciada por diversos fatores, e várias técnicas foram desenvolvidas para aumentar sua probabilidade e taxa de convergência.

*   **Damping:** Uma técnica simples para reduzir a chance de oscilação é o *damping*. Em vez de enviar a mensagem $m_{s \rightarrow t}(x_t)$, enviamos uma mensagem amortecida da forma [^7]:\n    $$M_{s \rightarrow t}(x_s) = \lambda m_{s \rightarrow t}(x_s) + (1 - \lambda) M_{s \rightarrow t}^{k-1}(x_s) \qquad (22.7)$$\n    onde $0 \le \lambda \le 1$ é o fator de *damping*.

*   **Message Scheduling:** A abordagem padrão para implementar LBP é realizar atualizações síncronas, onde todos os nós absorvem mensagens em paralelo e, em seguida, enviam mensagens em paralelo. No entanto, atualizações assíncronas (análogas ao método de Gauss-Seidel) podem convergir mais rapidamente [^8]. Uma técnica mais sofisticada é a *residual belief propagation*, onde as mensagens são agendadas para serem enviadas de acordo com a norma da diferença de seu valor anterior [^8].

#### Exatidão do LBP

Para um grafo com um único loop, pode-se mostrar que a versão *max-product* de LBP encontrará a estimativa MAP correta, se convergir [^8]. Em grafos mais gerais, pode-se limitar o erro nas marginais aproximadas computadas por LBP [^8].

#### LBP como Otimização Variacional

O algoritmo LBP pode ser compreendido sob a ótica da inferência variacional [^10]. A ideia básica é minimizar a divergência de Kullback-Leibler (KL) entre uma distribuição aproximada $q$ e a distribuição posterior exata (não normalizada) $\tilde{p}$ [^10]. No entanto, ao invés de exigir que $q$ seja uma distribuição conjunta globalmente válida, exigimos apenas que seja localmente consistente [^10].

*   **Marginal Polytope:** O espaço de vetores $\mu$ admissíveis é chamado de *marginal polytope*, denotado por $M(G)$ [^11]. Este é definido como o conjunto de todos os parâmetros médios para o modelo dado que podem ser gerados a partir de uma distribuição de probabilidade válida [^11].

*   **Outer Approximation:** Como o conjunto $M(G)$ é exponencialmente grande, é comum relaxar as restrições e considerar um vetor $\tau$ que satisfaz apenas as restrições de consistência local [^14]:\n    $$\sum_{x_s} \tau_s(x_s) = 1 \qquad (22.31)$$\n    $$\sum_{x_t} \tau_{st}(x_s, x_t) = \tau_s(x_s) \qquad (22.32)$$\n    O conjunto de tais $\tau$ é denotado por $L(G)$, que é um *outer approximation* convexo de $M(G)$ [^14]. Os termos $\tau_s, \tau_{st} \in L(G)$ são chamados de *pseudo-marginais*, pois podem não corresponder às marginais de qualquer distribuição de probabilidade válida [^14].

*   **Entropy Approximation:** A aproximação de Bethe da entropia é dada por [^15]:\n    $$H_{Bethe}(\tau) = \sum_{s \in V} H_s(\tau_s) - \sum_{(s,t) \in E} I_{st}(\tau_{st}) \qquad (22.39)$$\n    onde $H_s$ é a entropia marginal do nó $s$ e $I_{st}$ é a informação mútua entre os nós $s$ e $t$ [^15].

*   **Bethe Free Energy:** A Bethe free energy é definida como [^15]:\n    $$F_{Bethe}(\tau) = - \left[ \theta^T \tau + H_{Bethe}(\tau) \right] \qquad (22.40)$$\n    O objetivo do LBP é minimizar a Bethe free energy sujeita às restrições de consistência local [^16].

### Conclusão
O Loopy Belief Propagation (LBP) é um algoritmo de inferência aproximada amplamente utilizado, especialmente em cenários onde a inferência exata é intratável. Ao aplicá-lo em grafos com loops, abrimos mão das garantias de convergência e correção presentes em grafos sem loops. No entanto, como vimos, LBP frequentemente oferece um desempenho satisfatório na prática, e sua convergência pode ser aprimorada através de técnicas como *damping* e *message scheduling*. A interpretação de LBP como um problema de otimização variacional nos permite compreender melhor seu comportamento e limitações, além de fornecer insights para o desenvolvimento de algoritmos de inferência aproximada mais avançados.

### Referências
[^1]: 22.2 Loopy belief propagation: algorithmic issues\n[^2]: 22.2.2 LBP on pairwise models\n[^3]: 22.2.3 LBP on a factor graph\n[^5]: 22.2.4 Convergence\n[^7]: 22.2.4.2 Making LBP converge\n[^8]: 22.2.5 Accuracy of LBP\n[^10]: 22.1 Introduction\n[^11]: 22.3.2 The marginal polytope\n[^14]: 22.3.5.1 An outer approximation to the marginal polytope\n[^15]: 22.3.5.2 The entropy approximation\n[^16]: 22.3.5.3 The LBP objective
<!-- END -->