## Capítulo 22: Inferência Variacional Avançada e Propagação de Crenças

### Introdução

Este capítulo expande os conceitos de inferência variacional introduzidos anteriormente, focando em métodos que relaxam a necessidade de fatorização completa da distribuição aproximada. Exploraremos a inferência variacional que busca minimizar a divergência de Kullback-Leibler $KL(q||p)$ entre uma distribuição aproximada $q$ e a posterior não normalizada $p$, com ênfase na consistência local entre nós adjacentes [^1]. Ao contrário da inferência de campo médio, este tipo de inferência permite dependências mais complexas entre variáveis, possibilitando uma melhor aproximação da verdadeira distribuição posterior [^1]. Além disso, investigaremos a *loopy belief propagation (LBP)* e sua relação com a inferência variacional.

### Conceitos Fundamentais

**Inferência Variacional com Consistência Local:** A inferência variacional tradicional, como o *Mean Field*, aproxima a distribuição posterior por um produto de distribuições marginais [^1]. No entanto, a inferência variacional que discutiremos neste capítulo permite que a distribuição $q$ não seja completamente fatorizada nem globalmente válida como uma distribuição conjunta [^1]. Em vez disso, a exigência principal é que $q$ seja **localmente consistente**, o que significa que a distribuição conjunta de dois nós adjacentes deve concordar com as marginais correspondentes [^1].

A ideia básica é minimizar a função objetivo:
$$J(q) = KL(q || \tilde{p})$$
onde $\tilde{p}$ é a posterior exata, mas não normalizada [^1]. A diferença crucial é que não requeremos mais que $q$ seja fatorizada ou uma distribuição conjunta globalmente válida [^1].

**Loopy Belief Propagation (LBP):** LBP é um algoritmo de inferência aproximada para modelos gráficos discretos (ou Gaussianos) [^1]. A ideia central é aplicar o algoritmo de *belief propagation* (BP) da Seção 20.2 ao grafo, mesmo que este contenha loops (isto é, não seja uma árvore) [^1]. Apesar de simples e eficiente, e frequentemente apresentar bom desempenho na prática, superando o *Mean Field* [^1], o LBP não tem garantia de convergência nem de correção dos resultados em grafos com loops [^1].

**Algoritmo LBP para MRF Pairwise:** O algoritmo LBP pode ser aplicado a um modelo gráfico não direcionado com fatores *pairwise* [^2]. O método consiste em aplicar continuamente as Equações 20.11 e 20.10 até a convergência [^2]. O Algoritmo 22.1 fornece o pseudocódigo para LBP em um MRF *pairwise* [^2]:

```
Algorithm 22.1: Loopy belief propagation for a pairwise MRF
1 Input: node potentials Vs(xs), edge potentials Vst(xs, xt);
2 Initialize messages ms→t(xt) = 1 for all edges s → t;
3 Initialize beliefs bels(xs) = 1 for all nodes s;
4 repeat
5    Send message on each edge
6    mst(xt) = ∑xs (Vs(xs)Vst(xs, xt) Пu∈nbrs\t Mus(xs));
7    Update belief of each node bels(x) x ψs(xs) Пtenbr. Mt→s(xs);\
8 until beliefs don't change significantly;
9 Return marginal beliefs bels(xs);
```
onde:
*   $V_s(x_s)$ representa os potenciais dos nós.
*   $V_{st}(x_s, x_t)$ representa os potenciais das arestas.
*   $m_{s \rightarrow t}(x_t)$ representa a mensagem do nó $s$ para o nó $t$.
*   $bel_s(x_s)$ representa a crença no nó $s$.

**LBP em um Grafo Fator:** Para lidar com modelos com potenciais de cliques de ordem superior (que incluem modelos direcionados onde alguns nós têm mais de um pai), é útil usar uma representação conhecida como *grafo fator* [^3]. Um *grafo fator* é um grafo bipartido não direcionado com dois tipos de nós: nós redondos representam variáveis, nós quadrados representam fatores, e há uma aresta de cada variável para cada fator que a menciona [^3].

A versão de BP que envia mensagens em um *grafo fator* é proposta em (Kschischang et al. 2001) [^5]. Especificamente, temos dois tipos de mensagens: variáveis para fatores e fatores para variáveis [^5]:
$$m_{x \rightarrow f}(x) = \prod_{h \in nbr(x) \setminus \{f\}} m_{h \rightarrow x}(x) \quad (22.4)$$
$$m_{f \rightarrow x}(x) = \sum_{y} f(x,y) \prod_{y \in nbr(f) \setminus \{x\}} m_{y \rightarrow f}(y) \quad (22.5)$$
onde $nbr(x)$ são todos os fatores conectados à variável $x$, e $nbr(f)$ são todas as variáveis conectadas ao fator $f$ [^5].

**Convergência do LBP:** LBP nem sempre converge e, mesmo quando converge, pode convergir para as respostas erradas [^5]. Existem diversas técnicas para melhorar a convergência do LBP, incluindo o uso de *amortecimento* (*damping*) [^7]. Em vez de enviar a mensagem $m_{ts}$, enviamos uma mensagem amortecida da forma:
$$M_{ts}(x_s) = \lambda m_{ts}(x_s) + (1 - \lambda) M_{ts}^{k-1}(x_s) \quad (22.7)$$
onde $0 \leq \lambda \leq 1$ é o fator de amortecimento [^7].

**LBP como um Problema Variacional:** LBP pode ser interpretado como um problema de inferência variacional [^13]. A chave para entender essa conexão reside na aproximação da entropia e na relaxação das restrições sobre as marginais [^15].

### Conclusão

Este capítulo apresentou uma visão mais profunda da inferência variacional, explorando métodos que vão além da fatorização completa e permitem dependências mais complexas entre as variáveis. A introdução do conceito de *loopy belief propagation* e sua análise no contexto da inferência variacional fornecem ferramentas adicionais para aproximar a distribuição posterior em modelos gráficos complexos. A discussão sobre convergência e as técnicas para melhorá-la são cruciais para a aplicação prática desses métodos.

### Referências

[^1]: Capítulo 22 - More variational inference, Introduction
[^2]: Capítulo 22 - More variational inference, Loopy belief propagation: algorithmic issues
[^3]: Capítulo 22 - More variational inference, LBP on a factor graph
[^5]: Capítulo 22 - More variational inference, BP on a factor graph
[^7]: Capítulo 22 - More variational inference, Making LBP converge
[^13]: Capítulo 22 - More variational inference, Mean field as a variational optimization problem
[^15]: Capítulo 22 - More variational inference, The entropy approximation
<!-- END -->