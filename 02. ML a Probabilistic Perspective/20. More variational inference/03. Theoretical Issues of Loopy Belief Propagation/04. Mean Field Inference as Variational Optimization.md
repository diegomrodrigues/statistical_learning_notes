## 22.3.4 Mean Field Inference como um Problema de Otimização Variacional

### Introdução
Este capítulo explora uma reinterpretação da inferência de **Mean Field (MF)** como um problema de otimização variacional, contrastando-a com outras abordagens aproximadas. A motivação é comparar o MF com outros métodos aproximados que serão discutidos posteriormente [^779].

### Conceitos Fundamentais
A inferência de Mean Field, discutida no Capítulo 21, aproxima a posterior por um produto de distribuições marginais [^768]. Agora, reinterpreta-se a inferência de MF em um framework mais abstrato.

Primeiramente, define-se *F* como um subgrafo de arestas do grafo original *G*, e *I(F) ⊆ I* como o subconjunto de estatísticas suficientes associadas aos cliques de *F* [^779]. Seja *Ω* o conjunto de parâmetros canônicos para o modelo completo. Define-se o espaço de parâmetros canônicos para o submodelo como:

$$\
\Omega(F) \equiv \{\theta \in \Omega: \theta_{\alpha} = 0 \quad \forall \alpha \in I \setminus I(F)\}
$$

Em outras palavras, requer-se que os parâmetros naturais associados às estatísticas suficientes *α* fora da classe escolhida sejam zero [^779]. Por exemplo, no caso de uma aproximação totalmente fatorada, *F₀*, remove-se todas as arestas do grafo, resultando em:

$$\
\Omega(F_0) \equiv \{\theta \in \Omega: \theta_{st} = 0 \quad \forall (s, t) \in E\}
$$

No caso do Mean Field estruturado (Seção 21.4), define-se *θ<sub>st</sub> = 0* para arestas que não estão no subgrafo tratável [^779].

Em seguida, define-se o espaço de parâmetros médios do modelo restrito como:

$$\
M_F(G) \equiv \{\mu \in \mathbb{R}^d: \mu = \mathbb{E}_{\theta}[\phi(x)] \text{ para algum } \theta \in \Omega(F)\}
$$

Isto é chamado de uma **aproximação interna** para o politopo marginal, uma vez que *M<sub>F</sub>(G) ⊆ M(G)* [^779]. Veja a Figura 22.7(b) [^778] para um esboço. Note que *M<sub>F</sub>(G)* é um politopo não convexo, o que resulta em múltiplos ótimos locais. Em contraste, algumas das aproximações consideradas posteriormente serão convexas [^779].

Define-se a entropia da aproximação *H(μ(F))* como a entropia da distribuição *μ* definida no submodelo *F*. Então, define-se o problema de otimização funcional de energia de Mean Field como se segue:

$$\
\max_{\mu \in M_F(G)} \theta^T \mu + H(\mu) \leq \log Z(\theta)
$$

No caso da aproximação de Mean Field totalmente fatorada para UGMs de pares, pode-se escrever este objetivo como se segue [^779]:

$$\
\max_{\mu \in \mathcal{P}} \sum_{s \in V} \sum_{x_s} \theta_s(x_s) \mu_s(x_s) + \sum_{(s,t) \in E} \sum_{x_s, x_t} \theta_{st}(x_s, x_t) \mu_s(x_s) \mu_t(x_t) + \sum_{s \in V} H(\mu_s)
$$

onde *μ<sub>s</sub> ∈ P*, e *P* é o simplex de probabilidade sobre *X* [^779].

O Mean Field envolve um objetivo côncavo sendo maximizado sobre um conjunto não convexo [^779]. Ele é tipicamente otimizado usando ascensão de coordenadas, uma vez que é fácil otimizar uma função côncava escalar sobre *P* para cada *μ<sub>s</sub>*. Por exemplo, para um UGM de pares, obtém-se:

$$\
\mu_s(x_s) \propto \exp(\theta_s(x_s)) \exp \left( \sum_{t \in \text{nbr}(s)} \sum_{x_t} \mu_t(x_t) \theta_{st}(x_s, x_t) \right)
$$

### Conclusão

A reinterpretação da inferência de Mean Field como um problema de otimização variacional fornece uma base para comparar o MF com outras aproximações, como as discutidas na seção 22.3.5 [^779]. A formulação destaca a natureza aproximada do MF, onde parâmetros fora de um subgrafo são forçados a zero, resultando em uma aproximação interna do politopo marginal [^779].

### Referências
[^768]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. Cambridge, MA: MIT press, 2012.
[^778]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. Cambridge, MA: MIT press, 2012.
[^779]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. Cambridge, MA: MIT press, 2012.
<!-- END -->