## Modelos Gráficos Não Direcionados na Forma da Família Exponencial e Estimativa de Parâmetros

### Introdução
Este capítulo aprofunda a representação de **Modelos Gráficos Não Direcionados (UGMs)** na forma da **família exponencial**, bem como a estimativa dos **parâmetros médios** (*mean parameters*). Como introduzido anteriormente, UGMs podem ser representados na forma da família exponencial [^10]:

$$p(x|\theta, G) = \frac{1}{Z(\theta)} \exp\left(\sum_{s \in V} \theta_s(x_s) + \sum_{(s, t) \in E} \theta_{st}(x_s, x_t)\right)$$

onde $\theta$ representa os parâmetros dos nós e arestas, e o objetivo é estimar os parâmetros médios $\mu$, que são os valores esperados das estatísticas suficientes [^10]. Este capítulo expande este conceito, detalhando aspectos teóricos e algorítmicos relevantes para a aplicação de métodos de inferência aproximada, como o *Loopy Belief Propagation* (LBP).

### Conceitos Fundamentais

#### Representação na Família Exponencial
A representação de UGMs na forma da família exponencial [^10] facilita a análise teórica e o desenvolvimento de algoritmos de inferência. A função de partição $Z(\theta)$ garante que a distribuição seja normalizada [^10]. A forma exponencial destaca a relação entre os parâmetros $\theta$ e as estatísticas suficientes, que são as funções $s(x_s)$ e $s(x_s, x_t)$ associadas aos nós e arestas, respectivamente [^10].  A Equação 22.12 [^10] representa a forma geral da distribuição, enquanto a Equação 22.13 [^10] apresenta a forma simplificada utilizando a função de energia $E(x)$.

#### Estatísticas Suficientes e Parâmetros Médios
As estatísticas suficientes $\phi(x)$ são funções dos dados que, juntamente com os parâmetros $\theta$, capturam toda a informação relevante sobre a distribuição [^10]. Os parâmetros médios $\mu$ são os valores esperados dessas estatísticas, conforme definido na Equação 22.15 [^10]:

$$\mu = E[\phi(x)] = (\{p(x_s = j)\}_s, \{p(x_s = j, x_t = k)\}_{s \neq t})$$

Eles representam as marginais dos nós e arestas [^10]. A representação "overcomplete" [^10] (Equação 22.12) ignora as restrições de soma-a-um, o que pode ser conveniente em certos casos. Uma representação minimal [^10], como a utilizada para o modelo de Ising (Equação 22.16), pode ser mais eficiente em termos de parâmetros.

#### Polítopo Marginal
O espaço de todos os vetores $\mu$ admissíveis é denominado *marginal polytope* [^10], denotado por $M(G)$ [^10]. Este polítopo é definido como o conjunto de todos os parâmetros médios que podem ser gerados a partir de uma distribuição de probabilidade válida [^10] (Equação 22.18). O *marginal polytope* desempenha um papel crucial nos algoritmos de inferência aproximada [^10].  A Equação 22.20 define *marginal polytope* como o *convex hull* do *feature set* [^10].

#### Inferência Exata como Otimização Variacional
A inferência exata pode ser formulada como um problema de otimização variacional, onde o objetivo é encontrar a distribuição $q$ que maximiza o funcional de energia $L(q)$ [^10] (Equação 22.22). A maximização do funcional de energia [^10] (Equação 22.23) sobre o *marginal polytope* $M(G)$ é equivalente a encontrar a distribuição exata. No entanto, a complexidade exponencial do *marginal polytope* torna a inferência exata intratável para modelos grandes [^10].

#### LBP em Modelos Pairwise
O algoritmo LBP [^2] (Algorithm 22.1) é aplicado a modelos pairwise com fatores de nós e arestas [^2]. As mensagens são inicializadas e iterativamente atualizadas [^2] (Equação 22.4, 22.5). A crença de cada nó é atualizada com base nas mensagens recebidas [^2] (Equação 22.6).

#### LBP em Factor Graphs
LBP pode ser aplicado a *factor graphs* [^3], que são representações bipartite que unificam modelos direcionados e não direcionados [^3]. As mensagens são passadas entre variáveis e fatores [^5] (Equação 22.4, 22.5).

#### Convergência de LBP
A convergência de LBP [^5] não é garantida e, mesmo quando converge, pode levar a resultados incorretos [^5]. A análise da convergência é complexa, mas o conceito de *computation tree* [^6] ajuda a visualizar as mensagens passadas [^6]. Técnicas como *damping* [^7] (Equação 22.7) e *message scheduling* [^7] (Equação 22.8, 22.9, 22.10) podem melhorar a convergência [^7].

#### LBP como um Problema de Otimização Variacional
O LBP pode ser visto como uma aproximação variacional onde se busca maximizar uma função objetivo sobre um conjunto restrito [^13]. A restrição é relaxada para um *outer bound* do *marginal polytope*, o *local consistency polytope* $L(G)$ [^14]. A função objetivo é aproximada pela *Bethe free energy* [^15] (Equação 22.40) que é baseada na *Bethe entropy approximation* [^15] (Equação 22.39).

### Conclusão
Este capítulo detalhou a representação de UGMs na forma da família exponencial e explorou a relação entre os parâmetros, as estatísticas suficientes e os *marginal polytopes* [^10]. A formulação da inferência exata como um problema de otimização variacional foi apresentada, juntamente com as aproximações utilizadas pelo LBP, incluindo a relaxação do *marginal polytope* e a aproximação da entropia [^10]. As condições de convergência e as técnicas para melhorar a convergência do LBP foram discutidas. Os conceitos apresentados neste capítulo fornecem uma base teórica sólida para a compreensão e aplicação do LBP e outros algoritmos de inferência aproximada em UGMs.

### Referências
[^2]: Murphy, Kevin P. *Probabilistic Machine Learning and Artificial Intelligence*. MIT Press, 2022, Chapter 22, Section 22.2.
[^3]: Murphy, Kevin P. *Probabilistic Machine Learning and Artificial Intelligence*. MIT Press, 2022, Chapter 22, Section 22.2.3.1.
[^5]: Murphy, Kevin P. *Probabilistic Machine Learning and Artificial Intelligence*. MIT Press, 2022, Chapter 22, Section 22.2.4.
[^6]: Murphy, Kevin P. *Probabilistic Machine Learning and Artificial Intelligence*. MIT Press, 2022, Chapter 22, Section 22.2.4.1.
[^7]: Murphy, Kevin P. *Probabilistic Machine Learning and Artificial Intelligence*. MIT Press, 2022, Chapter 22, Section 22.2.4.2.
[^10]: Murphy, Kevin P. *Probabilistic Machine Learning and Artificial Intelligence*. MIT Press, 2022, Chapter 22, Section 22.3.1.
[^13]: Murphy, Kevin P. *Probabilistic Machine Learning and Artificial Intelligence*. MIT Press, 2022, Chapter 22, Section 22.3.4.
[^14]: Murphy, Kevin P. *Probabilistic Machine Learning and Artificial Intelligence*. MIT Press, 2022, Chapter 22, Section 22.3.5.1.
[^15]: Murphy, Kevin P. *Probabilistic Machine Learning and Artificial Intelligence*. MIT Press, 2022, Chapter 22, Section 22.3.5.2.
<!-- END -->