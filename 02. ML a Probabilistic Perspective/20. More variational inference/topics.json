{
  "topics": [
    {
      "topic": "Variational Inference",
      "sub_topics": [
        "Variational inference aims to approximate the posterior distribution by minimizing the Kullback-Leibler divergence KL(q||p) between an approximate distribution q and the unnormalized posterior p, without requiring q to be fully factorized or globally consistent, focusing instead on local consistency between adjacent nodes. Unlike mean field inference, variational inference allows for more complex dependencies between variables, enabling a better approximation of the true posterior distribution. Variational Bayes (VB) and VB-EM are specific applications of mean field inference, useful for Bayesian inference of statistical model parameters, such as the mean and variance of a Gaussian or regression weights in a GLM, where the posterior is approximated by a product of marginal distributions.",
        "Loopy belief propagation (LBP) is an approximate inference algorithm applicable to discrete or Gaussian graphical models, which applies the belief propagation algorithm to graphs with loops, despite the lack of guarantee for correct results or convergence, but often performs well in practice. Its convergence can be improved using techniques like damping or asynchronous updates. LBP can be viewed as a variational inference problem by considering an outer approximation to the marginal polytope, where the Bethe approximation is used to estimate the entropy, and the optimization involves pseudo-marginals that satisfy local consistency constraints. The analysis of LBP involves variational inference to understand its behavior.",
        "Factor graphs are bipartite graphs used to represent probabilistic models, unifying directed and undirected models, where round nodes represent variables, square nodes represent factors, and edges connect variables to factors that mention them, simplifying message passing algorithms. LBP can be applied to factor graphs by defining messages passed between variables and factors, allowing for computation of beliefs as a product of incoming messages.",
        "Belief propagation (BP) on a factor graph involves passing messages between variables and factors, where messages from variables to factors are the product of incoming messages, and messages from factors to variables are computed by summing over the product of the factor and incoming messages, allowing for the computation of final beliefs as a product of incoming messages at convergence.",
        "Convergence in loopy belief propagation is not guaranteed, and even when it occurs, it may not be accurate; damping techniques and asynchronous updates can improve convergence, while the computation tree provides insights into the algorithm's progress and potential convergence. The computation tree visualizes the messages passed during LBP, where T iterations of LBP are equivalent to exact computation on a computation tree of height T+1. Convergence occurs when the strengths of connections on the edges are sufficiently weak, such that the influence of leaves on the root diminishes over time.",
        "Techniques to improve LBP convergence include damping, which involves sending a damped message that is a combination of the new and previous messages, and message scheduling, which involves performing synchronous or asynchronous updates to compute new messages in parallel or sequentially. Double loop algorithms guarantee convergence to a local minimum of the LBP objective, though they are often slow and do not significantly improve accuracy. The standard synchronous update approach in LBP, where all nodes absorb and send messages in parallel, can be improved by asynchronous updates, inspired by the Gauss-Seidel method, and adaptive ordering, such as residual belief propagation, which prioritizes messages based on the norm of the difference from their previous value.",
        "Speedup tricks for LBP include fast message computation for large state spaces using techniques like FFT or distance transform, and multi-scale methods that construct a coarse-to-fine grid and compute messages at a coarse level to initialize messages at a finer level. Computational cascades prune out improbable states based on a computationally cheap filtering step, creating a hierarchy of models that trade off speed and accuracy, and can be used with exact inference methods like chain-structured CRFs.",
        "The marginal polytope M(G) represents the space of allowable mean parameters for a given undirected graphical model (UGM), defined as the set of mean parameters that can be generated from a valid probability distribution. Exact inference can be framed as a variational optimization problem, maximizing a linear combination of parameters and entropy over the marginal polytope.",
        "Mean field inference can be re-interpreted as a variational optimization problem involving an inner approximation to the marginal polytope, where the mean field energy functional is optimized over a non-convex set, typically using coordinate ascent.",
        "Generalized Belief Propagation (GBP) improves accuracy by clustering nodes into tighter loops, resulting in a hyper-graph and message passing algorithm, but at increased computational cost and complexity. Convex Belief Propagation involves working with a set of tractable submodels and is a concave objective being maximized over a convex set, and hence has a unique maximum. Tree-reweighted belief propagation (TRBP) is a specific case where F is all spanning trees of a graph. To compute the upper bound, obtained by averaging over all trees, note that the terms for single nodes will just be Hs, since node s appears in every tree, and the sum of probabilities equals 1.",
        "Expectation propagation (EP) is a form of belief propagation where messages are approximated, generalizing the assumed density filtering (ADF) algorithm, and can be viewed as a variational inference problem by partitioning parameters and sufficient statistics into tractable and intractable terms and approximating the convex set M with a larger convex set L. LBP is a special case of EP where the base distribution contains node marginals and intractable terms correspond to edge potentials, resulting in the Bethe approximation to the entropy. In the context of ranking players, TrueSkill leverages Expectation Propagation (EP) to estimate and update player skill levels based on game outcomes, using a factor graph representation and Gaussian approximations for skill distributions, enabling accurate ranking in online gaming systems."
      ]
    },
    {
      "topic": "MAP State Estimation",
      "sub_topics": [
        "MAP state estimation involves finding the most probable configuration of variables in a discrete-state graphical model, which can be approached using linear programming relaxation by rewriting the objective in terms of variational parameters and relaxing the constraint set to a convex outer bound, or using max-product belief propagation by considering the zero temperature limit. Graphcuts provide another method for finding MAP state estimates by using max flow/min cut algorithms, particularly for binary nodes and restricted potentials.",
        "For binary MRFs with submodular potentials, graphcuts can be applied to find the globally optimal MAP estimate by constructing a graph with specific edge weights based on the submodularity condition, ensuring that the minimum cut corresponds to the minimum energy configuration.",
        "For nonbinary metric MRFs, the alpha expansion algorithm can be used to approximate the MAP state estimation by iteratively picking a label \u03b1 and solving a binary subproblem where each variable can choose to remain in its current state or to become state \u03b1, facilitating the optimization over multiple states."
      ]
    },
    {
      "topic": "Theoretical Issues of Loopy Belief Propagation",
      "sub_topics": [
        "Undirected Graphical Models (UGMs) can be represented in exponential family form as p(x|\u03b8, G) = (1/Z(\u03b8)) exp(\u03a3 \u03b8s(xs) + \u03a3 \u03b8st(xs, xt)), where \u03b8 represents node and edge parameters, and the goal is to estimate the mean parameters \u03bc, which are the expected values of the sufficient statistics.",
        "The marginal polytope M(G) represents the space of allowable mean parameter vectors \u03bc for a given UGM, defined as the set of all mean parameters that can be generated from a valid probability distribution, and can be expressed as a convex hull of feature vectors.",
        "Exact inference can be formulated as a variational optimization problem: max \u03b8^T \u03bc + H(\u03bc) where \u03bc \u2208 M(G), but the marginal polytope M(G) has exponentially many facets, making it difficult to optimize.",
        "Mean field inference can be re-interpreted as a variational optimization problem by considering an edge subgraph F of the original graph G and defining the canonical parameter space \u03a9(F) where parameters outside of F are set to zero, leading to an inner approximation MF(G) of the marginal polytope.",
        "Loopy Belief Propagation (LBP) can be viewed as a variational inference problem by using an outer approximation L(G) to the marginal polytope, which only enforces local consistency constraints, and approximating the entropy using the Bethe approximation, resulting in the Bethe variational problem (BVP).",
        "The Bethe free energy FBethe(\u03c4) is defined as \u03b8^T \u03c4 + HBethe(\u03c4), where HBethe(\u03c4) is the Bethe approximation to the entropy, and LBP can be shown to find a stationary point of this objective by relating the update equations to Lagrange multipliers that enforce normalization and marginalization constraints."
      ]
    }
  ]
}