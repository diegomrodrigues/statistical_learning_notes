## Directed Graphical Models (Bayes Nets)

### Introdução
Este capítulo explora os **Directed Graphical Models (DGMs)**, também conhecidos como **Bayesian networks** ou **belief networks**. DGMs representam distribuições conjuntas codificando suposições de independência condicional (CI) usando grafos acíclicos direcionados (DAGs) [^1]. Os nós representam variáveis aleatórias, e a ausência de arestas denota suposições de CI, fornecendo uma estrutura visual e intuitiva para relações probabilísticas e facilitando a inferência probabilística e o raciocínio sob incerteza [^1].

### Conceitos Fundamentais

#### Representação e a Chain Rule
DGMs facilitam a representação compacta de distribuições conjuntas complexas. A *chain rule* da probabilidade [^1] permite representar qualquer distribuição conjunta, mas essa representação pode se tornar excessivamente complicada [^1]:

$$
p(x_{1:V}) = p(x_1)p(x_2|x_1)p(x_3|x_{1:2}) \dots p(x_V|x_{1:V-1})
$$

onde $V$ é o número de variáveis [^1]. A complexidade aumenta exponencialmente com o número de variáveis, tornando a representação e o aprendizado impraticáveis [^1].

#### Independência Condicional
A chave para representar eficientemente distribuições conjuntas é fazer suposições sobre **independência condicional (CI)** [^1].  Duas variáveis $X$ e $Y$ são condicionalmente independentes dado $Z$, denotado como $X \perp Y | Z$, se e somente se a distribuição conjunta condicional pode ser escrita como um produto de distribuições marginais condicionais [^1]:

$$
X \perp Y | Z \Leftrightarrow p(X, Y | Z) = p(X | Z) p(Y | Z)
$$

A suposição de **Markov** é um exemplo de CI, onde o futuro é independente do passado dado o presente [^1]: $x_{t+1} \perp x_{1:t-1} | x_t$. Usando essa suposição e a *chain rule*, a distribuição conjunta pode ser escrita como [^1]:

$$
p(x_{1:V}) = p(x_1) \prod_{t=2}^{V} p(x_t | x_{t-1})
$$

Isso leva ao conceito de uma **cadeia de Markov** de primeira ordem [^1].

#### Grafos e Terminologia
Um **grafo** $G = (V, E)$ consiste em um conjunto de **nós** ou **vértices** $V = \{1, \dots, V\}$ e um conjunto de **arestas** $E = \{(s, t) : s, t \in V\}$ [^1]. O grafo pode ser representado por sua **matriz de adjacência** $G(s, t)$, onde $G(s, t) = 1$ se $(s, t) \in E$, indicando uma aresta de $s$ para $t$ [^1]. Se $G(s, t) = 1$ se e somente se $G(t, s) = 1$, o grafo é **não direcionado**, caso contrário, é **direcionado** [^1]. Assume-se geralmente que $G(s, s) = 0$, indicando a ausência de auto-loops [^1].

Termos adicionais importantes incluem [^1]:
*   **Parent (Pai):** $pa(s) = \{t : G(t, s) = 1\}$
*   **Child (Filho):** $ch(s) = \{t : G(s, t) = 1\}$
*   **Family (Família):** $fam(s) = \{s\} \cup pa(s)$
*   **Root (Raiz):** Nó sem pais.
*   **Leaf (Folha):** Nó sem filhos.
*   **Ancestors (Ancestrais):** $anc(t) = \{s : s \to t\}$
*   **Descendants (Descendentes):** $desc(s) = \{t : s \rightsquigarrow t\}$
*   **Neighbors (Vizinhos):** $nbr(s) = \{t : G(s, t) = 1 \lor G(t, s) = 1\}$
*   **Degree (Grau):** Número de vizinhos. Para grafos direcionados, considera-se *in-degree* (número de pais) e *out-degree* (número de filhos).
*   **Cycle or Loop (Ciclo ou Laço):** Sequência de nós $s_1 \to s_2 \to \dots \to s_n \to s_1$.
*   **Path or Trail (Caminho ou Trilha):** Sequência de arestas direcionadas de $s$ para $t$.
*   **Tree (Árvore):** Grafo não direcionado sem ciclos. Uma árvore direcionada é um DAG sem ciclos direcionados.
*   **Forest (Floresta):** Conjunto de árvores.
*   **Subgraph (Subgrafo):** Grafo induzido por um subconjunto de nós e suas arestas correspondentes.
*   **Clique:** Conjunto de nós onde todos são vizinhos entre si.

#### Directed Acyclic Graphs (DAGs)
Um **Directed Acyclic Graph (DAG)** é um grafo direcionado sem ciclos direcionados [^1].  Em um DAG, os nós podem ser ordenados de forma que os pais venham antes dos filhos; isso é conhecido como **ordenação topológica** ou **ordenação total** [^1]. Dada uma ordenação topológica, a **ordered Markov property** afirma que um nó depende apenas de seus pais imediatos, e não de todos os seus predecessores [^1]:

$$
X_s \perp X_{pred(s) \setminus pa(s)} | X_{pa(s)}
$$

onde $pa(s)$ são os pais do nó $s$ e $pred(s)$ são os predecessores de $s$ na ordenação [^1]. Esta propriedade é uma generalização natural da propriedade de Markov de primeira ordem de cadeias para DAGs gerais [^1].

#### Conditional Probability Tables (CPTs) e Conditional Probability Distributions (CPDs)
Em DGMs, a relação entre um nó e seus pais é definida por uma **Conditional Probability Table (CPT)** ou uma **Conditional Probability Distribution (CPD)** [^1]. Se cada nó tem $K$ estados e no máximo $F$ pais, o número de parâmetros no modelo é $O(VK^F)$, que é muito menor do que $O(K^V)$ necessário para um modelo sem suposições de CI [^1].

#### Exemplos de Modelos Gráficos Direcionados

1.  **Naive Bayes Classifiers:** Assume que os recursos são condicionalmente independentes dada a classe [^1] (Figura 10.2(a)). A distribuição conjunta é dada por [^1]:

$$
p(y, \mathbf{x}) = p(y) \prod_{j=1}^D p(x_j | y)
$$

2.  **Tree-Augmented Naive Bayes (TAN):** Captura a correlação entre os recursos usando uma árvore [^1] (Figura 10.2(b)).
3.  **Markov Models:** Uma cadeia de Markov de primeira ordem pode ser representada como um DAG [^1] (Figura 10.3(a)). Cadeias de Markov de ordem superior podem ser criadas adicionando dependências de estados anteriores [^1] (Figura 10.3(b)).
4.  **Hidden Markov Models (HMMs):** Modelam um processo oculto subjacente que é uma cadeia de Markov de primeira ordem, onde os dados são observações ruidosas desse processo [^1] (Figura 10.4).
5.  **Alarm Networks and Quick Medical Reference (QMR) Networks:** Usados no diagnóstico médico para modelar as relações entre variáveis medidas em uma unidade de terapia intensiva (UTI) [^1] (Figura 10.5).
6.  **Genetic Linkage Analysis:** A análise de ligação genética utiliza DGMs para representar a relação entre pais e filhos em gráficos de pedigree [^1] (Figura 10.6).
7.  **Directed Gaussian Graphical Models:** Modelos onde todas as variáveis são reais e as CPDs são Gaussianas lineares [^1].

### Conclusão
DGMs fornecem uma poderosa ferramenta para representar e raciocinar sobre distribuições de probabilidade complexas, explorando suposições de independência condicional. A capacidade de visualizar relações probabilísticas através de DAGs e realizar inferência probabilística faz dos DGMs uma ferramenta fundamental em muitos campos, incluindo aprendizado de máquina, inteligência artificial e bioinformática. Os exemplos discutidos ilustram a versatilidade dos DGMs na modelagem de vários tipos de dados e dependências.

### Referências
[^1]: Capítulo 10 do texto fornecido.
<!-- END -->