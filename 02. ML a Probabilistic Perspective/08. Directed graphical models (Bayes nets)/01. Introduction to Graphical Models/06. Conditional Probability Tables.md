## Tabelas de Probabilidade Condicional e Distribuições Condicionais Mais Parcimoniosas em Modelos Gráficos Direcionados

### Introdução
Em modelos gráficos direcionados (DGMs), as relações probabilísticas entre variáveis são representadas por meio de **tabelas de probabilidade condicional (CPTs)** ou **distribuições de probabilidade condicional (CPDs)** [^308]. No entanto, o número de parâmetros em CPTs cresce exponencialmente com o número de pais, o que dificulta o aprendizado com dados limitados [^308]. Este capítulo explora a substituição de CPTs por CPDs mais parcimoniosas, como a regressão logística multinomial, que reduz o número de parâmetros e cria um modelo de densidade compacto adequado para avaliar a probabilidade de um vetor totalmente observado [^308].

### Conceitos Fundamentais

#### Tabelas de Probabilidade Condicional (CPTs)
As CPTs são usadas para representar as probabilidades condicionais entre variáveis em DGMs [^308]. Considere um DGM onde cada variável pode assumir *K* estados. A representação de $p(x_1)$ requer $O(K)$ números, representando uma distribuição discreta [^307]. A representação de $p(x_2|x_1)$ requer $O(K^2)$ números, formando uma matriz estocástica *T* onde $p(x_2 = j|x_1 = i) = T_{ij}$ [^307]. De forma similar, $p(x_3|x_1, x_2)$ requer uma tabela 3D com $O(K^3)$ números [^308]. Em geral, para um modelo com *V* variáveis, o número de parâmetros em CPTs é $O(K^V)$ [^308]. O problema com essa representação é que o número de parâmetros cresce exponencialmente com o número de pais, tornando o aprendizado difícil com dados limitados [^308].

#### Distribuições de Probabilidade Condicional (CPDs) Mais Parcimoniosas
Uma solução para o problema do crescimento exponencial de parâmetros é substituir cada CPT por uma CPD mais parcimoniosa [^308]. Um exemplo é a regressão logística multinomial, onde a probabilidade condicional é modelada como:
$$np(x_t = k|x_{1:t-1}) = S(W_k x_{1:t-1})$$
onde *S* é a função softmax e $W_k$ é uma matriz de pesos específica para o estado *k* [^308].

Com essa abordagem, o número total de parâmetros é reduzido para $O(K^2V^2)$, tornando o modelo uma densidade compacta [^308]. Isso é adequado quando o objetivo é avaliar a probabilidade de um vetor totalmente observado $x_{1:V}$, por exemplo, para definir uma densidade condicional de classe $p(x|y = c)$ em um classificador generativo [^308].

#### Vantagens e Desvantagens
A utilização de CPDs mais parcimoniosas, como a regressão logística multinomial, oferece a vantagem de reduzir significativamente o número de parâmetros em comparação com as CPTs tradicionais [^308]. Isso torna o aprendizado mais viável, especialmente em situações com dados limitados [^308]. No entanto, essa abordagem pode não ser adequada para outros tipos de tarefas de predição, uma vez que cada variável depende de todas as variáveis anteriores, limitando a flexibilidade do modelo [^308].

#### Independência Condicional
A **independência condicional (CI)** é um conceito chave para representar grandes distribuições conjuntas de forma eficiente [^308]. Duas variáveis *X* e *Y* são condicionalmente independentes dado *Z*, denotado como $X \perp Y | Z$, se e somente se a distribuição conjunta condicional pode ser escrita como um produto de distribuições marginais condicionais:
$$nX \perp Y | Z \Leftrightarrow p(X, Y|Z) = p(X|Z)p(Y|Z)$$
Essa propriedade permite simplificar a representação de DGMs, reduzindo o número de parâmetros necessários [^308].

#### A Propriedade de Markov
Como exemplo de aplicação da independência condicional, considere a **propriedade de Markov** de primeira ordem, que afirma que "o futuro é independente do passado dado o presente" [^308]. Matematicamente:
$$nx_{t+1} \perp x_{1:t-1} | x_t$$
Usando essa propriedade e a regra da cadeia, a distribuição conjunta pode ser escrita como:
$$np(x_{1:V}) = p(x_1) \prod_{t=1}^{V} p(x_t|x_{t-1})$$
Essa representação é conhecida como **cadeia de Markov** [^308].

#### Modelos Gráficos
Os **modelos gráficos (GMs)** são uma forma de representar uma distribuição conjunta fazendo suposições de independência condicional [^308]. Em um GM, os nós representam variáveis aleatórias e a ausência de arestas representa suposições de CI [^308]. Existem diferentes tipos de GMs, dependendo se o grafo é direcionado, não direcionado ou uma combinação de ambos [^308].

### Conclusão

A escolha entre CPTs e CPDs mais parcimoniosas em DGMs depende do equilíbrio entre a capacidade de representar relações complexas e a necessidade de evitar o sobreajuste com dados limitados [^308]. A regressão logística multinomial oferece uma alternativa compacta às CPTs, mas pode não ser adequada para todas as tarefas de predição [^308]. A independência condicional e a propriedade de Markov são conceitos fundamentais para simplificar a representação de distribuições conjuntas em DGMs [^308]. Os modelos gráficos fornecem uma estrutura para representar suposições de independência condicional e facilitar o raciocínio probabilístico [^308].

### Referências
[^307]: Capítulo 10, Directed graphical models (Bayes nets), página 307.
[^308]: Capítulo 10, Directed graphical models (Bayes nets), página 308.

<!-- END -->