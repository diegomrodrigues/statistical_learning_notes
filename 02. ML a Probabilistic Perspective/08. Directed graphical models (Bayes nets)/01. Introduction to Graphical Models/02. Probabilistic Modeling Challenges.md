## Probabilistic Modeling in Graphical Models

### Introdução
A modelagem probabilística é fundamental em machine learning, abordando desafios centrais como a representação compacta de distribuições conjuntas, inferência eficiente de variáveis e aprendizado de parâmetros de distribuição com dados limitados [^1]. As redes Bayesianas, ou modelos gráficos direcionados (DGMs), oferecem uma estrutura para representar e manipular essas distribuições de forma eficiente. Este capítulo explora como a modelagem probabilística, especialmente através de DGMs, lida com esses desafios, focando na exploração de *conditional independence* (CI) [^1].

### Conceitos Fundamentais

A representação compacta de distribuições conjuntas $p(x|\theta)$ é crucial quando se lida com múltiplas variáveis correlacionadas [^1]. A regra da cadeia (chain rule) da probabilidade permite escrever a distribuição conjunta como um produto de condicionais [^2]:

$$ p(x_{1:V}) = p(x_1)p(x_2|x_1)p(x_3|x_{1:2})...p(x_V|x_{1:V-1}) \quad (10.1) $$

onde $V$ é o número de variáveis. No entanto, essa representação pode se tornar complexa à medida que o número de variáveis aumenta, necessitando de um grande número de parâmetros para representar as distribuições condicionais $p(x_t|x_{1:t-1})$ [^2]. Por exemplo, com $V$ variáveis e $K$ estados cada, a representação direta requer $O(K^V)$ parâmetros [^2].

Uma solução é substituir cada tabela de probabilidade condicional (CPT) por uma distribuição de probabilidade condicional mais parcimoniosa (CPD), como a regressão logística multinomial [^2]:

$$ p(x_t = k|x_{1:t-1}) = S(W_t x_{1:t-1})_k $$

onde $S$ é a função softmax. Isso reduz o número de parâmetros para $O(K^2V^2)$, mas ainda pode ser inadequado para tarefas de previsão onde cada variável depende de todas as anteriores [^2].

A chave para representar eficientemente grandes distribuições conjuntas reside em fazer **assunções de independência condicional (CI)** [^2]. Duas variáveis $X$ e $Y$ são condicionalmente independentes dado $Z$, denotado como $X \perp Y | Z$, se e somente se a distribuição conjunta condicional pode ser escrita como um produto de marginais condicionais [^2]:

$$ X \perp Y | Z \Leftrightarrow p(X, Y | Z) = p(X | Z)p(Y | Z) \quad (10.2) $$

Se assumirmos que $x_{t+1} \perp x_{1:t-1} | x_t$, ou seja, "o futuro é independente do passado dado o presente", temos a **assunção de Markov** de primeira ordem [^2]. Usando essa assunção e a regra da cadeia, a distribuição conjunta pode ser escrita como [^2]:

$$ p(x_{1:V}) = p(x_1) \prod_{t=2}^{V} p(x_t | x_{t-1}) \quad (10.3) $$

Isso define uma **cadeia de Markov** de primeira ordem, caracterizada por uma distribuição inicial $p(x_1 = i)$ e uma matriz de transição de estados $p(x_t = j | x_{t-1} = i)$ [^2].

Os **modelos gráficos (GMs)** representam distribuições conjuntas fazendo assunções de CI [^2]. Os nós do grafo representam variáveis aleatórias, e a ausência de arestas indica assunções de CI [^2]. Existem diferentes tipos de GMs, dependendo se o grafo é direcionado, não direcionado ou uma combinação de ambos [^2]. Este capítulo se concentra em **gráficos direcionados**, também conhecidos como **redes Bayesianas** [^4].

Um **modelo gráfico direcionado (DGM)** é um GM cujo grafo é um grafo acíclico direcionado (DAG) [^4]. Uma propriedade fundamental de DAGs é que os nós podem ser ordenados de forma que os pais venham antes dos filhos, chamada de **ordenação topológica** [^4]. Dada essa ordenação, definimos a **propriedade de Markov ordenada** como a assunção de que um nó depende apenas de seus pais imediatos, e não de todos os seus predecessores [^4]:

$$ X_s \perp X_{pred(s)} \setminus X_{pa(s)} | X_{pa(s)} \quad (10.4) $$

onde $pa(s)$ são os pais do nó $s$ e $pred(s)$ são os predecessores de $s$ na ordenação [^4]. Isso generaliza a propriedade de Markov de primeira ordem para cadeias a DAGs gerais [^4].

O DAG na Figura 10.1(a) [^3] codifica a seguinte distribuição conjunta [^5]:
$$ p(x_{1:5}) = p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2,x_3)p(x_5|x_3) \quad (10.6) $$

Em geral, para um DGM [^5]:

$$ p(x_{1:V} | G) = \prod_{t=1}^{V} p(x_t | x_{pa(t)}) \quad (10.7) $$

onde cada termo $p(x_t | x_{pa(t)})$ é uma CPD. Se cada nó tiver $O(F)$ pais e $K$ estados, o número de parâmetros no modelo é $O(VK^F)$, que é muito menor do que os $O(K^V)$ necessários sem as assunções de CI [^5].

**Exemplos de DGMs:**
*   **Classificadores Naive Bayes:** Assumem que as features são condicionalmente independentes dado o rótulo da classe [^5].
*   **Modelos de Markov e Hidden Markov Models:** Modelam sequências de eventos, com ou sem variáveis latentes [^6].
*   **Redes de diagnóstico médico:** Modelam relações entre doenças e sintomas [^7].
*   **Análise de ligação genética:** Analisam a herança de traços genéticos [^9].

Para que a inferência seja possível, são utilizadas técnicas como a d-separação e algoritmos como o *Bayes Ball* para verificar a independência condicional entre os nós [^18].

### Conclusão
A modelagem probabilística, facilitada pelos modelos gráficos direcionados (redes Bayesianas), oferece uma abordagem poderosa para representar, inferir e aprender distribuições de probabilidade complexas. Ao explorar as independências condicionais, os DGMs permitem uma representação compacta e eficiente, superando as limitações das abordagens tradicionais que requerem um número exponencial de parâmetros. A capacidade de realizar inferência probabilística em DGMs abre caminho para aplicações em diversas áreas, desde o diagnóstico médico até a análise genética e a tomada de decisões.
<!-- END -->