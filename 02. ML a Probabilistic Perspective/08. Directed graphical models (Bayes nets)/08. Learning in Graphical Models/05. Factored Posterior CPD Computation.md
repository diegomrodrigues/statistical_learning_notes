## Learning with Complete Data in Directed Graphical Models: Exploiting Factorization

### Introdução
Este capítulo explora o aprendizado em modelos gráficos direcionados (DGMs) quando os dados estão completos, ou seja, quando não há dados faltantes ou variáveis latentes. Em particular, focaremos em como uma **prior fatorada** combinada com uma **verossimilhança fatorada** implica uma **posterior fatorada** [^16]. Essa propriedade é fundamental para simplificar o processo de aprendizado, permitindo que as distribuições posteriores para cada distribuição de probabilidade condicional (CPD) sejam computadas independentemente.

### Conceitos Fundamentais

Em um DGM com dados completos, a verossimilhança pode ser expressa como um produto de termos, um para cada CPD no gráfico [^16]. Formalmente, a verossimilhança é dada por:

$$
p(\mathcal{D}|\theta) = \prod_{i=1}^{N} p(x_i|\theta) = \prod_{i=1}^{N} \prod_{t=1}^{V} p(x_{it}|x_{i,pa(t)}, \theta_t) = \prod_{t=1}^{V} p(\mathcal{D}_t|\theta_t)
$$

Onde:
*   $\mathcal{D}$ representa o conjunto de dados completo.
*   $\theta$ representa os parâmetros do modelo.
*   $N$ é o número de instâncias de dados.
*   $V$ é o número de nós no gráfico.
*   $x_{it}$ é o valor do nó *t* na instância de dados *i*.
*   $pa(t)$ denota os pais do nó *t*.
*   $\theta_t$ representa os parâmetros associados à CPD do nó *t*.
*   $\mathcal{D}_t$ representa os dados associados ao nó *t* e seus pais.

A equação acima demonstra que a verossimilhança se **decompõe** de acordo com a estrutura do gráfico [^16]. Isso significa que podemos calcular a verossimilhança total multiplicando as verossimilhanças de cada "família" (nó e seus pais) no gráfico.

Agora, suponha que a *prior* sobre os parâmetros também seja fatorada [^16]:

$$
p(\theta) = \prod_{t=1}^{V} p(\theta_t)
$$

Nesse caso, a *posterior* também se fatorará:

$$
p(\theta|\mathcal{D}) \propto p(\mathcal{D}|\theta)p(\theta) = \prod_{t=1}^{V} p(\mathcal{D}_t|\theta_t)p(\theta_t)
$$

Essa equação demonstra que a *posterior* de cada CPD pode ser computada independentemente [^16]. Isso simplifica significativamente o processo de aprendizado, pois podemos otimizar os parâmetros de cada CPD separadamente.

Para ilustrar, considere o caso em que todas as CPDs são tabulares [^16]. Podemos representar a CPD do nó *t* como $p(x_t|x_{pa(t)} = c) \sim Cat(\theta_{tc})$, onde $\theta_{tck} = p(x_t = k|x_{pa(t)} = c)$, para $k = 1:K_t$ e $c = 1:C_t$. Aqui, $K_t$ é o número de estados para o nó *t*, e $C_t$ é o número de combinações de pais. Se assumirmos uma *prior* de Dirichlet separada para cada linha da CPT, ou seja, $\theta_{tc} \sim Dir(\alpha_{tc})$, então podemos calcular a *posterior* simplesmente adicionando pseudo-contagens às contagens empíricas [^16]: $\theta_{tc}|\mathcal{D} \sim Dir(N_{tc} + \alpha_{tc})$, onde $N_{tck}$ é o número de vezes que o nó *t* está no estado *k* enquanto seus pais estão no estado *c*.

### Conclusão

A propriedade de fatoração da posterior em DGMs com dados completos é uma ferramenta poderosa para simplificar o aprendizado. Ao assumir uma prior fatorada, podemos decompor o problema de aprendizado em subproblemas independentes, um para cada CPD. Isso reduz significativamente a complexidade computacional e permite que os parâmetros do modelo sejam estimados de forma eficiente. Essa abordagem é particularmente útil em modelos com um grande número de variáveis e parâmetros. No entanto, essa propriedade não se mantém quando há dados faltantes ou variáveis latentes [^17], o que torna o aprendizado muito mais desafiador.

### Referências
[^16]: Vide seção 10.4.2 do texto fornecido.
[^17]: Vide seção 10.4.3 do texto fornecido.
<!-- END -->