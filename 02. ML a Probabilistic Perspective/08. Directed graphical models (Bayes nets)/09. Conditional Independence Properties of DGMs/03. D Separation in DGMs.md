## d-Separation em Modelos Gráficos Direcionados

### Introdução
Este capítulo explora as propriedades de independência condicional em modelos gráficos direcionados (DGMs), também conhecidos como redes Bayesianas [^3, ^4]. Uma ferramenta crucial para determinar tais independências é o conceito de *d-separação*. Este capítulo detalha o critério de d-separação e o algoritmo de Bayes Ball, fornecendo uma base teórica sólida para sua aplicação em DGMs.

### Conceitos Fundamentais
A espinha dorsal de qualquer modelo gráfico reside em seu conjunto de premissas de independência condicional (CI) [^2, ^4]. Formalmente, denotamos $X_A \perp_G X_B | X_C$ se o conjunto de nós A é independente do conjunto de nós B dado o conjunto C no grafo G. Aqui, adotamos a semântica que será definida a seguir. Denotamos $I(G)$ como o conjunto de todas as declarações de CI codificadas pelo grafo [^2, ^4].

Um grafo G é considerado um *I-map* (mapa de independência) para uma distribuição de probabilidade *p*, ou *p* é *Markov* em relação a *G*, se $I(G) \subseteq I(p)$, onde $I(p)$ representa o conjunto de todas as declarações de CI válidas para *p* [^2, ^4]. Em outras palavras, um grafo é um I-map se ele não fizer nenhuma afirmação de CI que não seja verdadeira na distribuição [^2, ^4]. O grafo totalmente conectado é trivialmente um I-map para qualquer distribuição [^2, ^4]. Um grafo G é um *I-map minimal* para *p* se G é um I-map de p e não existe $G' \subset G$ que também seja um I-map de p [^2, ^4].

A d-separação é um critério para determinar a independência condicional em DGMs [^2, ^18]. Formalmente, um *caminho não direcionado* P é *d-separado* por um conjunto de nós E (contendo a evidência) se pelo menos uma das seguintes condições for verdadeira [^18]:

1.  P contém uma *cadeia* $s \rightarrow m \rightarrow t$ ou $s \leftarrow m \leftarrow t$, onde $m \in E$ [^18].
2.  P contém um *garfo* (fork) ou *tenda* $s \leftarrow m \rightarrow t$, onde $m \in E$ [^18].
3.  P contém um *colisor* ou *v-estrutura* $s \rightarrow m \leftarrow t$, onde $m \notin E$ e nenhum descendente de *m* está em *E* [^18].

Um conjunto de nós A é *d-separado* de um conjunto de nós B dado um terceiro conjunto observado E se cada caminho não direcionado de cada nó $a \in A$ para cada nó $b \in B$ é d-separado por E [^18]. Definimos as propriedades de independência condicional de um DAG da seguinte forma [^18]:

$$X_A \perp_G X_B | X_E \Leftrightarrow \text{A é d-separado de B dado E}$$

O **algoritmo de Bayes Ball** [^18], (Shachter 1998), oferece uma maneira intuitiva de verificar se A é d-separado de B dado E. A ideia central é "sombrear" todos os nós em E, indicando que eles são observados [^18]. Em seguida, "bolas" são colocadas em cada nó em A e são permitidas "quicar" de acordo com um conjunto de regras [^18]. Se alguma das bolas atingir qualquer nó em B, então A não é d-separado de B dado E [^18]. As três regras principais são ilustradas na Figura 10.9 [^18].

As regras do Bayes Ball podem ser justificadas considerando estruturas de cadeia, garfo e v-estruturas [^18]. Para uma estrutura de cadeia $X \rightarrow Y \rightarrow Z$, a distribuição conjunta é $p(x, y, z) = p(x)p(y|x)p(z|y)$ [^18]. Quando condicionamos em *y*, temos [^18]:

$$p(x, z|y) = \frac{p(x, y, z)}{p(y)} = \frac{p(x)p(y|x)p(z|y)}{p(y)} = p(x|y)p(z|y)$$

e, portanto, $x \perp z | y$ [^18]. Observar o nó do meio da cadeia a divide em duas, como em uma cadeia de Markov [^18].

Para uma estrutura de garfo $X \leftarrow Y \rightarrow Z$, a distribuição conjunta é $p(x, y, z) = p(y)p(x|y)p(z|y)$ [^18]. Quando condicionamos em *y*, temos [^18]:

$$p(x, z|y) = \frac{p(x, y, z)}{p(y)} = \frac{p(y)p(x|y)p(z|y)}{p(y)} = p(x|y)p(z|y)$$

e, portanto, $x \perp z | y$ [^18]. A observação de um nó raiz separa seus filhos [^18].

Finalmente, considere uma v-estrutura $X \rightarrow Y \leftarrow Z$. A distribuição conjunta é $p(x, y, z) = p(x)p(z)p(y|x, z)$ [^18]. Sem condicionamento, $p(x, z) = p(x)p(z)$, então $x \perp z$ [^18]. No entanto, quando condicionamos em *y*, $x \not\perp z | y$ [^18]. Este efeito é conhecido como *explicação para longe* (explaining away), *raciocínio inter-causal* ou *paradoxo de Berkson* [^18].

O algoritmo de Bayes Ball também necessita de "condições de contorno" [^18], como ilustrado na Figura 10.10 [^18]. Se $Y'$ é uma cópia sem ruído de Y, então observar $Y'$ é equivalente a observar Y [^18], forçando os pais X e Z a competirem para explicar $Y'$ [^18].

### Conclusão
O critério de d-separação e o algoritmo de Bayes Ball fornecem ferramentas poderosas para analisar propriedades de independência condicional em DGMs [^18]. Ao determinar quais variáveis são independentes dadas outras, podemos simplificar a inferência e o aprendizado em modelos gráficos [^18]. As propriedades de independência condicional desempenham um papel fundamental no projeto de algoritmos eficientes e na compreensão das relações entre variáveis em sistemas complexos [^2, ^4].

### Referências
[^2]: Capítulo 10. Directed graphical models (Bayes nets).
[^3]: Seção 10.1 Introdução
[^4]: Seção 10.1.2 Conditional independence
[^18]: Seção 10.5 Conditional independence properties of DGMs
<!-- END -->