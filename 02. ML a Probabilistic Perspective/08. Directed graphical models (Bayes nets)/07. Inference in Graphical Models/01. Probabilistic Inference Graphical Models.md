## Probabilistic Inference in Graphical Models

### Introdução

A inferência probabilística é um componente central no uso de modelos gráficos [^319]. Como mencionado anteriormente, modelos gráficos oferecem uma maneira compacta de definir distribuições de probabilidade conjuntas [^319]. O objetivo principal da inferência é estimar quantidades desconhecidas a partir de quantidades conhecidas, utilizando essa distribuição conjunta [^319]. Este capítulo explora em detalhes o processo de inferência, focando na computação da distribuição *a posteriori* de variáveis ocultas, dadas as variáveis visíveis e os parâmetros do modelo [^319].

### Conceitos Fundamentais

A tarefa fundamental na inferência probabilística é calcular a distribuição *a posteriori* das variáveis ocultas (não observadas) dado o conhecimento das variáveis visíveis (observadas) e dos parâmetros do modelo [^319]. Matematicamente, isso é expresso como:

$$ p(x_h | x_v, \theta) = \frac{p(x_h, x_v | \theta)}{p(x_v | \theta)} $$

onde:

*   $x_h$ representa as variáveis ocultas
*   $x_v$ representa as variáveis visíveis
*   $\theta$ representa os parâmetros do modelo
*   $p(x_h | x_v, \theta)$ é a distribuição *a posteriori* das variáveis ocultas dado as variáveis visíveis e os parâmetros
*   $p(x_h, x_v | \theta)$ é a distribuição conjunta das variáveis ocultas e visíveis, dado os parâmetros
*   $p(x_v | \theta)$ é a probabilidade marginal das variáveis visíveis, dado os parâmetros, também conhecida como *likelihood* dos dados ou *probabilidade da evidência* [^319].

O cálculo de $p(x_v | \theta)$ envolve marginalizar a distribuição conjunta sobre todas as possíveis configurações das variáveis ocultas:

$$ p(x_v | \theta) = \sum_{x_h} p(x_h, x_v | \theta) $$

Essencialmente, estamos *condicionando* nos dados ao fixar as variáveis visíveis aos seus valores observados, e então normalizando para transformar $p(x_h, x_v)$ em $p(x_h | x_v)$ [^319]. A constante de normalização, $p(x_v)$, é a *likelihood* dos dados [^319].

Em alguns casos, estamos interessados apenas em um subconjunto das variáveis ocultas, chamadas *query variables* ($x_q$), e o restante são chamadas de *nuisance variables* ($x_n$) [^320]. Podemos obter a distribuição *a posteriori* das variáveis de interesse marginalizando a distribuição conjunta sobre as *nuisance variables*:

$$ p(x_q | x_v, \theta) = \sum_{x_n} p(x_q, x_n | x_v, \theta) $$

Para modelos gaussianos multivariados, todas estas operações podem ser eficientemente realizadas em tempo $O(V^3)$, onde $V$ é o número de variáveis [^320]. No entanto, para variáveis discretas com $K$ estados cada, o cálculo direto pode levar um tempo $O(K^V)$ [^320]. A complexidade computacional da inferência exata em modelos gráficos é um desafio significativo, especialmente em modelos grandes [^320].

A estrutura do modelo gráfico codifica as independências condicionais (CI) entre as variáveis, permitindo que a distribuição conjunta seja fatorada em um produto de distribuições condicionais locais [^308, 309]. Essa fatoração é crucial para tornar a inferência mais tratável computacionalmente [^308].

Por exemplo, em um modelo gráfico direcionado (DGM), a distribuição conjunta pode ser escrita como [^308]:

$$ p(x_{1:V} | G) = \prod_{t=1}^{V} p(x_t | x_{pa(t)}) $$

onde $pa(t)$ representa os pais do nó $t$ no grafo $G$.

Se cada nó tem $O(F)$ pais e $K$ estados, o número de parâmetros é $O(VK^F)$, que é muito menor do que o $O(K^V)$ necessário em um modelo sem suposições de Independência Condicional [^311].

### Conclusão

A inferência probabilística em modelos gráficos é uma ferramenta poderosa para estimar variáveis desconhecidas com base em observações [^319]. A chave para uma inferência eficiente reside na exploração da estrutura de dependência codificada no modelo gráfico, que permite fatorar a distribuição conjunta e reduzir a complexidade computacional [^320]. Embora a inferência exata possa ser intratável para modelos complexos, diversas técnicas aproximadas são empregadas para obter soluções viáveis [^320].

### Referências

[^308]: Capítulo 10, Directed graphical models (Bayes nets), página 308.
[^309]: Capítulo 10, Directed graphical models (Bayes nets), páginas 309.
[^311]: Capítulo 10, Directed graphical models (Bayes nets), página 311.
[^319]: Capítulo 10, Directed graphical models (Bayes nets), página 319.
[^320]: Capítulo 10, Directed graphical models (Bayes nets), página 320.
<!-- END -->