## Normalização e Probabilidade da Evidência em Modelos Gráficos
### Introdução
Em modelos gráficos, a inferência probabilística desempenha um papel crucial na estimação de quantidades desconhecidas com base em informações conhecidas. Dentro deste contexto, um conceito fundamental é o da normalização, que está intimamente ligado à verossimilhança dos dados observados, também conhecida como probabilidade da evidência [^13]. Este capítulo irá explorar em detalhe este conceito, a sua importância na inferência e como ele se manifesta dentro da estrutura dos modelos gráficos direcionados (DGMs).

### Conceitos Fundamentais
Em essência, a inferência probabilística nos permite calcular a distribuição *a posteriori* de variáveis não observadas, condicionada nos valores das variáveis observadas [^13]. Este processo envolve o conceito de **condicionamento**, que consiste em fixar (ou "clamp") as variáveis visíveis (observadas) aos seus valores concretos, e subsequentemente, **normalizar** a distribuição resultante [^13]. A normalização assegura que a distribuição *a posteriori* seja uma distribuição de probabilidade válida, ou seja, que a soma das probabilidades sobre todos os possíveis estados das variáveis desconhecidas seja igual a 1.

Matematicamente, este processo pode ser expresso da seguinte forma:
$$ p(x_h|x_v, \theta) = \frac{p(x_h, x_v|\theta)}{p(x_v|\theta)} $$
onde:
*   $x_h$ representa as variáveis ocultas (não observadas)
*   $x_v$ representa as variáveis visíveis (observadas)
*   $\theta$ representa os parâmetros do modelo
*   $p(x_h|x_v, \theta)$ é a distribuição *a posteriori* das variáveis ocultas dado os valores observados das variáveis visíveis e os parâmetros do modelo
*   $p(x_h, x_v|\theta)$ é a distribuição conjunta das variáveis ocultas e visíveis dado os parâmetros do modelo
*   $p(x_v|\theta)$ é a **constante de normalização**, também conhecida como **verossimilhança dos dados** ou **probabilidade da evidência** [^13]

A constante de normalização $p(x_v|\theta)$ é, portanto, a probabilidade dos dados observados, dado os parâmetros do modelo. Ela desempenha um papel fundamental na inferência, pois garante que a distribuição *a posteriori* seja uma distribuição de probabilidade válida [^13].

**A importância da normalização**
A normalização é crucial por várias razões:
1.  **Garantia de uma Distribuição de Probabilidade Válida:** Assegura que a distribuição *a posteriori* seja uma distribuição de probabilidade adequada, permitindo interpretações probabilísticas significativas.
2.  **Comparação de Modelos:** A probabilidade da evidência pode ser usada para comparar diferentes modelos, favorecendo aqueles que melhor explicam os dados observados.
3.  **Inferência Precisa:** Uma normalização correta é essencial para obter estimativas precisas das variáveis ocultas, condicionadas aos dados observados.

### Conclusão
Em resumo, a normalização em modelos gráficos é um passo essencial no processo de inferência probabilística. A constante de normalização, ou probabilidade da evidência, não só garante que a distribuição *a posteriori* seja uma distribuição de probabilidade válida, mas também fornece uma medida da verossimilhança dos dados, que pode ser usada para comparar diferentes modelos. Compreender o papel da normalização é fundamental para aplicar modelos gráficos de forma eficaz e obter insights significativos a partir dos dados.

### Referências
[^13]: Capítulo 10, Directed graphical models (Bayes nets), página 319.
<!-- END -->