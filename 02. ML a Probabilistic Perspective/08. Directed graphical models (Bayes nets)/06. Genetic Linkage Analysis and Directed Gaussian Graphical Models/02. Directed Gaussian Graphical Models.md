## Directed Gaussian Graphical Models

### Introdução
Este capítulo aprofunda o estudo dos **modelos gráficos direcionados gaussianos (GGMs)**, explorando a sua estrutura, propriedades e aplicações. Os GGMs direcionados utilizam variáveis de valor real com distribuições de probabilidade condicionais gaussianas lineares (CPDs) [^317]. Este tópico se encaixa na discussão mais ampla de modelos gráficos, onde a representação da independência condicional é fundamental para modelar distribuições conjuntas complexas [^308]. Ao contrário de outros modelos gráficos que podem lidar com dados discretos ou mistos, os GGMs direcionados são especificamente projetados para dados contínuos, modelados através de distribuições gaussianas [^308].

### Conceitos Fundamentais

**Definição e Estrutura**
Em um GGM direcionado, cada variável $x_t$ é modelada condicionalmente aos seus pais $x_{pa(t)}$ através de uma CPD gaussiana linear [^317]:
$$ p(x_t|x_{pa(t)}) = \mathcal{N}(x_t|\mu_t + w^T x_{pa(t)}, \sigma_t^2) $$
onde:
- $x_t$ é a variável de interesse.
- $x_{pa(t)}$ é o conjunto de pais de $x_t$ no grafo direcionado.
- $\mu_t$ é o termo constante (ou intercepto) da distribuição condicional.
- $w$ é o vetor de pesos que modula a influência dos pais sobre $x_t$.
- $\sigma_t^2$ é a variância da distribuição condicional.

O produto de todas as CPDs resulta em uma distribuição gaussiana conjunta multivariada [^317]:
$$ p(x) = \mathcal{N}(x|\mu, \Sigma) $$
Esta distribuição é conhecida como uma *Gaussian Bayes net* [^317].

**Derivação da Média e Covariância**
A média $\mu$ e a matriz de covariância $\Sigma$ da distribuição gaussiana conjunta podem ser derivadas dos parâmetros das CPDs [^317]. Para facilitar essa derivação, as CPDs são reescritas em forma matricial-vetorial [^317]. Reescrevendo as CPDs na forma:

$$ x_t = \mu_t + \sum_{s \in pa(t)} w_{ts}(x_s - \mu_s) + \sigma_t z_t $$

onde $z_t \sim \mathcal{N}(0, 1)$ e $\sigma_t$ é o desvio padrão condicional de $x_t$ dados seus pais [^317]. Em notação matricial, isso se torna:

$$ (x - \mu) = W(x - \mu) + Sz $$

onde $W$ é uma matriz cujos elementos $W_{ts}$ representam os pesos das arestas direcionadas do grafo, $S = \text{diag}(\sigma_1, ..., \sigma_V)$ é uma matriz diagonal contendo os desvios padrão condicionais, e $z$ é um vetor de variáveis gaussianas padrão independentes.

A partir dessa representação, podemos derivar a relação entre $x$, $\mu$ e $z$:

$$ (I - W)(x - \mu) = Sz $$

$$ x - \mu = (I - W)^{-1}Sz = Uz $$

onde $U = (I - W)^{-1}$. Agora podemos calcular a matriz de covariância $\Sigma$:

$$ \Sigma = \text{cov}[x - \mu] = \text{cov}[Uz] = U \text{cov}[z] U^T = USU^T $$

Como $S$ é uma matriz diagonal contendo os desvios padrão, $\text{cov}[z] = S^2$, e portanto:

$$ \Sigma = USU^T = US^2U^T $$

**Propriedades e Inferência**
A matriz $W$ é triangular inferior devido à ordenação topológica dos nós no grafo, o que simplifica a inversão de $(I - W)$ [^319]. Isso permite calcular eficientemente $U$ e, consequentemente, $\Sigma$ [^319].

A inferência em GGMs direcionados gaussianos, como calcular $p(x_q|x_v, \theta)$ onde $x_q$ são as variáveis de consulta, $x_v$ são as variáveis visíveis e $\theta$ são os parâmetros do modelo, pode ser realizada usando as propriedades da distribuição gaussiana multivariada [^320]. A inferência envolve marginalizar as variáveis de ruído, o que é facilitado pela estrutura gaussiana e pela forma matricial das CPDs [^320].

**Independência Condicional**
Os GGMs direcionados gaussianos também incorporam noções de independência condicional, que são cruciais para simplificar a estrutura do modelo e reduzir o número de parâmetros [^308]. A *d-separação* é um conceito chave para determinar a independência condicional em GGMs direcionados [^324].

### Conclusão

Os modelos gráficos direcionados gaussianos oferecem uma estrutura poderosa para modelar distribuições conjuntas de variáveis contínuas com dependências lineares [^317]. A capacidade de derivar a média e a covariância da distribuição conjunta a partir dos parâmetros das CPDs, juntamente com a estrutura gráfica que impõe independência condicional, torna os GGMs direcionados uma ferramenta valiosa em muitas aplicações [^317].

### Referências
[^308]: Capítulo 10, Directed graphical models (Bayes nets), pg 308
[^317]: Capítulo 10, Directed graphical models (Bayes nets), pg 317
[^319]: Capítulo 10, Directed graphical models (Bayes nets), pg 319
[^320]: Capítulo 10, Directed graphical models (Bayes nets), pg 320
[^324]: Capítulo 10, Directed graphical models (Bayes nets), pg 324

<!-- END -->