## Directed Gaussian Graphical Models

### Introdução
Este capítulo explora os **Directed Gaussian Graphical Models (GGMs)**, também conhecidos como *Gaussian Bayes Nets*, um tipo específico de modelo gráfico direcionado que utiliza variáveis de valor real e distribuições condicionais Gaussianas lineares (CPDs). Os GGMs direcionados são uma ferramenta poderosa para modelar e inferir sobre distribuições conjuntas Gaussianas, permitindo computações eficientes de distribuições condicionais e inferência [^1]. Este capítulo se baseia nos conceitos de modelos gráficos direcionados (DGMs) introduzidos anteriormente [^1], explorando as propriedades e aplicações dos GGMs direcionados.

### Conceitos Fundamentais
Em um **Directed Gaussian Graphical Model (GGM)**, todas as variáveis são de valor real e as distribuições de probabilidade condicionais (CPDs) têm a seguinte forma [^1]:
$$ p(x_t|x_{pa(t)}) = \mathcal{N}(x_t|\mu_t + w_{t}^\top x_{pa(t)}, \sigma_t^2) $$
onde $x_t$ representa a variável no nó *t*, $x_{pa(t)}$ representa o conjunto de variáveis nos pais do nó *t*, $\mu_t$ é o termo médio local, $w_t$ é o vetor de pesos associado aos pais de *t*, e $\sigma_t^2$ é a variância condicional. Esta forma funcional para as CPDs é conhecida como **linear Gaussian CPD** [^1].

A combinação de todas essas CPDs resulta em uma distribuição conjunta Gaussiana da forma $p(x) = \mathcal{N}(x|\mu, \Sigma)$ [^1]. A eficiência computacional para inferência e cálculo de distribuições condicionais surge da propriedade de que a distribuição conjunta é Gaussiana.

Para derivar os parâmetros $\mu$ e $\Sigma$ da distribuição conjunta a partir dos parâmetros das CPDs, podemos reescrever as CPDs na seguinte forma [^1]:

$$ x_t = \mu_t + \sum_{s \in pa(t)} w_{ts}(x_s - \mu_s) + \sigma_t z_t $$

onde $z_t \sim \mathcal{N}(0, 1)$, $\sigma_t$ é o desvio padrão condicional de $x_t$ dado seus pais, $w_{ts}$ é a força da aresta de *s* para *t*, e $\mu_t$ é a média local [^1].

É fácil ver que a média global é apenas a concatenação das médias locais, $\mu = (\mu_1, ..., \mu_V)^\top$ [^1].

Para derivar a matriz de covariância global $\Sigma$, podemos reescrever a equação acima na forma matricial-vetorial [^1]:

$$ (x - \mu) = W(x - \mu) + Sz $$

onde *S* é uma matriz diagonal contendo os desvios padrão condicionais [^1]. Rearranjando os termos, obtemos [^1]:

$$ e = (I - W)(x - \mu) $$

onde *e* é um vetor de termos de ruído, $e \sim Sz$ [^1]. Como *W* é triangular inferior (devido à ordenação topológica dos nós no DAG), $(I - W)$ também é triangular inferior com 1s na diagonal. Portanto, $(I - W)$ é sempre invertível [^1].

Resolvendo para $(x - \mu)$, obtemos [^1]:

$$ x - \mu = (I - W)^{-1}e = Ue = USz $$

onde $U = (I - W)^{-1}$ [^1]. A matriz de covariância $\Sigma$ é então dada por [^1]:

$$ \Sigma = cov[x] = cov[x - \mu] = cov[USz] = US cov[z] S U^\top = U S^2 U^\top $$
Esta expressão mostra que a matriz de covariância global $\Sigma$ pode ser expressa em termos dos parâmetros locais $\mu_t$, $w_{ts}$ e $\sigma_t$ das CPDs lineares Gaussianas [^1].

### Conclusão
Os Directed Gaussian Graphical Models oferecem uma maneira eficiente e flexível de modelar distribuições conjuntas Gaussianas [^1]. Ao explorar a estrutura gráfica e as propriedades das CPDs lineares Gaussianas, podemos realizar inferências e cálculos de probabilidade de forma eficiente. Os GGMs direcionados encontram aplicações em diversas áreas, incluindo modelagem de dados contínuos, análise de regressão e reconhecimento de padrões. O uso de representações gráficas e a capacidade de realizar inferências eficientes tornam os GGMs direcionados uma ferramenta valiosa para análise de dados e tomada de decisões.

### Referências
[^1]: Chapter 10. Directed graphical models (Bayes nets). *[Refer to all relevant excerpts from the provided document]*

<!-- END -->