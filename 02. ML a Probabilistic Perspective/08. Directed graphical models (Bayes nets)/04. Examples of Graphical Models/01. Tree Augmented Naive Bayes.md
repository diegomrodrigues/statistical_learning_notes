## Naive Bayes e Modelos Gráficos: Tree-Augmented Naive Bayes

### Introdução
Em continuidade à representação de modelos probabilísticos através de **Directed Graphical Models (DGMs)** ou *Bayes Nets* [^3, ^4], este capítulo se aprofunda em uma variação específica do classificador Naive Bayes, o **Tree-Augmented Naive Bayes (TAN)**. O classificador Naive Bayes, como previamente introduzido [^5], assume independência condicional entre as features dado o label da classe. Essa simplificação, embora útil, pode ser excessivamente restritiva em muitos cenários práticos. O modelo TAN surge como uma extensão que busca mitigar essa limitação, introduzindo dependências entre as features através de uma estrutura de árvore.

### Conceitos Fundamentais

O classificador **Naive Bayes** é um modelo generativo que utiliza o teorema de Bayes para calcular a probabilidade a posteriori de uma classe, dado um conjunto de features. A premissa central do Naive Bayes é que, dado o valor da variável classe $y$, as features $x_j$ são condicionalmente independentes entre si [^5]. Matematicamente, essa premissa se traduz na seguinte decomposição da distribuição conjunta [^5]:

$$ p(y, x) = p(y) \prod_{j=1}^{D} p(x_j|y) $$

onde $D$ é o número de features. Essa formulação simplifica drasticamente o aprendizado do modelo, pois requer apenas a estimativa das distribuições marginais $p(y)$ e $p(x_j|y)$, em vez da distribuição conjunta completa $p(y, x)$.

Apesar de sua simplicidade e eficiência, a premissa de independência condicional do Naive Bayes é frequentemente violada em aplicações reais. Features podem estar correlacionadas, e ignorar essas correlações pode levar a uma perda de acurácia.

O **Tree-Augmented Naive Bayes (TAN)** [^5] é uma extensão do Naive Bayes que busca capturar dependências entre as features, mantendo a eficiência computacional. A ideia central é relaxar a premissa de independência condicional, permitindo que cada feature dependa, no máximo, de outra feature além da variável classe. Essa dependência é modelada através de uma estrutura de árvore, onde cada feature $x_j$ tem um pai, que pode ser a variável classe $y$ ou outra feature $x_k$.

A distribuição conjunta no TAN é dada por:

$$ p(y, x) = p(y) \prod_{j=1}^{D} p(x_j|y, x_{pa(j)}) $$

onde $x_{pa(j)}$ representa o pai da feature $x_j$ na árvore, e $pa(j)$ é o índice do pai de $x_j$. Se o pai de $x_j$ for a variável classe $y$, então $p(x_j|y, x_{pa(j)})$ se reduz a $p(x_j|y)$.

A estrutura da árvore é aprendida a partir dos dados, buscando maximizar a dependência mútua entre as features. Um algoritmo comum para aprender a estrutura da árvore é o **Chow-Liu algorithm** [^5], que utiliza a informação mútua para construir uma árvore de máxima ponderação.

O **Chow-Liu algorithm** [^5] consiste nos seguintes passos:

1.  Calcular a informação mútua $I(x_j; x_k|y)$ entre cada par de features $x_j$ e $x_k$, condicional à variável classe $y$. A informação mútua quantifica a dependência estatística entre duas variáveis.
2.  Construir um grafo completo onde cada nó representa uma feature e o peso de cada aresta $(j, k)$ é dado por $I(x_j; x_k|y)$.
3.  Encontrar a árvore de máxima ponderação (maximum spanning tree) nesse grafo. Essa árvore representa as dependências mais fortes entre as features, dado o label da classe.

A escolha de uma estrutura de árvore, em vez de um grafo mais geral, é motivada por considerações de eficiência computacional. Árvores permitem inferência e aprendizado eficientes, enquanto grafos mais complexos podem levar a problemas de intratabilidade. Além disso, árvores evitam ciclos, o que simplifica a análise e interpretação do modelo.

### Conclusão
O modelo TAN oferece uma alternativa interessante ao Naive Bayes, permitindo capturar dependências entre features sem comprometer excessivamente a eficiência computacional. A estrutura de árvore impõe restrições sobre as dependências, mas permite modelar correlações relevantes em muitos cenários práticos. O aprendizado da estrutura da árvore pode ser realizado eficientemente através do Chow-Liu algorithm, que utiliza a informação mútua para identificar as dependências mais importantes. O TAN representa um compromisso entre a simplicidade do Naive Bayes e a flexibilidade de modelos gráficos mais complexos, tornando-se uma ferramenta útil em diversas aplicações de classificação.

### Referências
[^3]: Capítulo 10, *Directed graphical models (Bayes nets)*
[^4]: Seção 10.1, *Introduction*
[^5]: Seção 10.2.1, *Naive Bayes classifiers*
<!-- END -->