{
  "topics": [
    {
      "topic": "Exact Inference in Graphical Models",
      "sub_topics": [
        "Exact inference in graphical models aims to compute posterior marginals, posterior modes, and posterior samples, generalizing algorithms like the forwards-backwards algorithm and Kalman smoother to arbitrary graph structures, applicable to both directed and undirected models. In chain-structured graphical models, where 'x' represents hidden variables and 'v' represents visible variables, exact inference algorithms compute posterior marginals p(xt|v, \u03b8).",
        "Belief Propagation (BP), also known as the sum-product algorithm, generalizes the forwards-backwards algorithm from chains to trees by initially assuming a pairwise Markov Random Field (MRF) or Conditional Random Field (CRF) model and implementing message passing protocols between nodes to compute belief states. Two versions exist: belief updating, which uses division and is analogous to the Kalman smoother, and sum-product, which multiplies all-but-one of the messages and is analogous to the backwards algorithm.",
        "The serial protocol in Belief Propagation involves picking an arbitrary node as the root, orienting all edges away from it, and then sending messages up from the leaves to the root (collect evidence phase) and back down from the root (distribute evidence phase), analogous to forwards-backwards on chains. The bottom-up belief state, denoted as bel\u0302(xt), conditions the belief only on evidence at or below node t in the graph, computed by multiplying incoming messages from children and local evidence, then normalizing. The message m\u0302s\u2192t(xt) represents what child s thinks parent t should know about the evidence in its subtree, computed recursively using edge potentials and bottom-up beliefs. The upward pass culminates in the local belief state at the root, while the downward pass computes messages from the parent node, incorporating information from the rest of the graph.",
        "The parallel version of Belief Propagation updates all nodes simultaneously, exchanging messages until convergence, resembling a systolic array. Convergence occurs after D(G) steps, where D(G) is the diameter of the graph.",
        "Gaussian Belief Propagation applies when p(x|v) is jointly Gaussian, representing it as a Gaussian pairwise MRF with node and edge potentials defined using exponential functions involving precision matrices and bias vectors. In Gaussian BP, messages and marginals are Gaussian, and computations involve multiplying Gaussian factors and marginalizing out variables from joint Gaussian factors, using properties of Gaussian distributions. Node and edge potentials are defined as exponentials of quadratic forms, and belief states are represented as Gaussians with mean \u03bct and precision \u039bt.",
        "The variable elimination (VE) algorithm computes marginal probabilities p(xq|xv) for any kind of graph by systematically eliminating variables through pushing sums inside products, also known as bucket elimination or peeling algorithm. It converts directed graphical models (DGMs) to undirected graphical models (UGMs) through moralization and defining potentials for each CPD. The key idea behind VE is to perform summations over variables as early as possible in the computation to reduce the size of intermediate factors. VE computes temporary factors by multiplying potentials in the scope of an operator, marginalizing out variables to create new factors, and repeating until the desired marginal is obtained.  The computational complexity of VE is exponential in the size of the largest factor created during elimination, influenced by the elimination order; the goal is to find an ordering that minimizes the induced width or treewidth of the graph. VE converts directed graphical models to undirected form by defining potentials for each CPD, moralizing the graph to ensure CI properties match, and then enumerating variable assignments to compute marginal probabilities.",
        "The generalized distributive law provides an abstract view of VE as computing an expression involving summation and products of factors, which can be extended to other tasks such as MAP estimation by replacing summation with maximization. VE can be applied to any commutative semi-ring, which is a set K, together with two binary operations called \u201c+\u201d and \u201c\u00d7\u201d.",
        "The junction tree algorithm (JTA) generalizes belief propagation from trees to arbitrary graphs by first running the variable elimination algorithm symbolically to create a chordal graph, then arranging the cliques into a junction tree that satisfies the running intersection property. Message passing on a junction tree involves initializing potentials, sending messages between cliques, and updating belief states, ensuring exact inference by partitioning evidence and satisfying the running intersection property.  Applying the jtree algorithm to a chain structured graph, such as an HMM, involves cliques representing edges and separators representing nodes, with potentials initialized based on conditional probabilities and local evidence. Generalizations of JTA include computing MAP estimates, N-most probable configurations, posterior samples, solving constraint satisfaction problems, and solving logical reasoning problems, all exploiting graph decomposition. The Hugin algorithm, a belief updating form of JTA, initializes potentials, sends messages between cliques, and updates potentials to ensure junction tree calibration, where each edge partitions evidence into distinct groups."
      ]
    },
    {
      "topic": "Computational Intractability of Exact Inference",
      "sub_topics": [
        "Exact inference, including variable elimination (VE) and the junction tree algorithm (JTA), is computationally intractable in the worst case, with time exponential in treewidth; exact inference is NP-hard, with the proof being a reduction from the satisfiability problem. This intractability is demonstrated by the NP-hardness of inference in general discrete GMs, requiring the use of approximate inference methods for many practical problems.",
        "Many popular probabilistic models support efficient exact inference, since they are based on chains, trees or low treewidth graphs. However, for graphs with high treewidth, exact inference in graphical models can be computationally intractable (NP-hard).",
        "Approximate inference methods are used when exact inference is intractable, but they often lack guarantees on accuracy or running time, making them heuristics; many probabilistic models support efficient exact inference due to their structure. A summary of methods that can be used for inference in graphical models indicates whether all the hidden variables must be discrete (\u201cD\u201d) or if all the factors must be linear-Gaussian (\u201cL-G\u201d)."
      ]
    }
  ]
}