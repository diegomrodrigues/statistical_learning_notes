## Belief Propagation for Trees

### Introdução
Este capítulo expande o conceito do algoritmo forwards-backwards, apresentado na Seção 17.4.3, para inferência exata em modelos gráficos com estrutura de árvore [^1]. O algoritmo resultante, conhecido como **Belief Propagation (BP)**, ou algoritmo sum-product, é uma ferramenta fundamental para realizar inferência em modelos gráficos tanto direcionados quanto não direcionados [^1]. O objetivo é generalizar os algoritmos de inferência exata para grafos arbitrários, apresentando o BP como uma extensão natural do algoritmo forwards-backwards de cadeias para árvores [^1].

### Conceitos Fundamentais
O Belief Propagation (BP), também conhecido como algoritmo sum-product, generaliza o algoritmo forwards-backwards de cadeias para árvores [^1]. Inicialmente, assume-se um modelo pairwise Markov Random Field (MRF) ou Conditional Random Field (CRF) [^1]. O BP implementa protocolos de *message passing* entre os nós para computar os *belief states* [^1]. Existem duas versões principais do BP:

1.  **Belief Updating:** Esta versão utiliza divisão e é análoga ao Kalman smoother [^1].
2.  **Sum-Product:** Esta versão multiplica todas as mensagens, exceto uma, e é análoga ao algoritmo backwards [^1].

Para implementar o BP em árvores não direcionadas, um nó arbitrário é escolhido como a raiz (*root*), *r* [^1]. Todas as arestas são então orientadas para longe de *r*, criando uma noção de pai e filho [^1]. O algoritmo opera em duas fases:

1.  **Collect Evidence Phase:** As mensagens são enviadas das folhas para a raiz [^1].
2.  **Distribute Evidence Phase:** As mensagens são enviadas de volta da raiz para as folhas [^1].

Este processo é análogo ao algoritmo forwards-backwards em cadeias [^1].

**Protocolo Serial:**
Inicialmente, o modelo é assumido como um pairwise MRF (ou CRF) [^1]:
$$\
p(\mathbf{x}|\mathbf{v}) = \frac{1}{Z(\mathbf{v})} \prod_{s \in \mathcal{V}} \psi_s(x_s) \prod_{(s,t) \in \mathcal{E}} \psi_{s,t}(x_s, x_t)
$$
onde $\psi_s$ é a evidência local para o nó *s*, e $\psi_{s,t}$ é o potencial para a aresta *s* - *t* [^1].

Para calcular o *belief state* no nó *t*, condicionamos inicialmente a crença apenas na evidência que está em ou abaixo de *t* no grafo [^2], ou seja, calculamos $bel_t^-(x_t) \propto p(x_t|v_t^-)$ [^2]. Isso é chamado de *"bottom-up belief state"*. Por indução, assume-se que as "mensagens" dos dois filhos de *t* foram computadas, resumindo o que eles pensam que *t* deve saber sobre a evidência em suas subárvores [^2]. Essas mensagens são $m_{s \rightarrow t}(x_t) = p(x_t|v_{st}^-)$, onde $v_{st}^-$ é toda a evidência no lado *downstream* da aresta *s* - *t* [^2]. De forma similar, $m_{u \rightarrow t}(x_t)$ é computado [^2]. O *bottom-up belief state* em *t* é então calculado como:
$$\
bel_t^-(x_t) = \frac{1}{Z_t} \psi_t(x_t) \prod_{c \in ch(t)} m_{c \rightarrow t}(x_t)
$$
onde $\psi_t(x_t) \propto p(x_t|v_t)$ é a evidência local para o nó *t*, e $Z_t$ é a constante de normalização local [^2]. Em outras palavras, multiplicamos todas as mensagens de entrada de nossos filhos, bem como a mensagem de entrada de nossa evidência local e, em seguida, normalizamos [^2].

Para computar as mensagens em si, considere calcular $m_{s \rightarrow t}(x_t)$, onde *s* é um dos filhos de *t* [^2]. Por recursão, assume-se que $bel_s^-(x_s) = p(x_s|v_s^-)$ foi computado [^2]. A mensagem é então calculada como:
$$\
m_{s \rightarrow t}(x_t) = \sum_{x_s} \psi_{s,t}(x_s, x_t) bel_s^-(x_s)
$$
Essencialmente, convertemos as crenças sobre $x_s$ em crenças sobre $x_t$ usando o potencial de aresta $\psi_{s,t}$ [^2]. Este processo continua até alcançarmos a raiz [^2]. Uma vez na raiz, "vimos" toda a evidência na árvore, então podemos calcular nosso *belief state* local na raiz usando [^2]:
$$\
bel(x_r) = p(x_r|v) \propto \psi_r(x_r) \prod_{c \in ch(r)} m_{c \rightarrow r}(x_r)
$$
Isso completa o final da passagem ascendente, que é análoga à passagem *forwards* em um HMM [^2]. Como um "efeito colateral", podemos calcular a probabilidade da evidência coletando as constantes de normalização [^2]:
$$\
p(v) = \prod_t Z_t
$$
Agora podemos passar mensagens para baixo a partir da raiz [^3]. Por exemplo, considere o nó *s*, com pai *t* [^3]. Para calcular o *belief state* para *s*, precisamos combinar o *belief* *bottom-up* para *s* juntamente com uma mensagem *top-down* de *t*, que resume todas as informações no resto do grafo, $m_{t \rightarrow s}(x_s) \propto p(x_t|v_t^+)$, onde $v_t^+$ é toda a evidência no lado *upstream* (raiz) da aresta *s* - *t* [^3]. Temos então:
$$\
bel_s(x_s) = p(x_s|v) \propto bel_s^-(x_s) \prod_{t \in pa(s)} m_{t \rightarrow s}(x_s)
$$

Para calcular essas mensagens descendentes, considere a mensagem de *t* para *s* [^3]. Suponha que o pai de *t* seja *r*, e os filhos de *t* sejam *s* e *u* [^3]. Queremos incluir em $m_{t \rightarrow s}$ todas as informações que *t* recebeu, exceto as informações que *s* enviou [^3]:
$$\
m_{t \rightarrow s}(x_s) = p(x_s|v_t^+) = \sum_{x_t} \psi_{s,t}(x_s, x_t) \frac{bel_t^-(x_t)}{m_{s \rightarrow t}(x_t)}
$$
Em vez de dividir a mensagem enviada para *t*, podemos inserir a equação de $bel_t^-$ para obter [^3]:
$$\
m_{t \rightarrow s}(x_s) = \sum_{x_t} \psi_{s,t}(x_s, x_t) \psi_t(x_t) \prod_{c \in ch(t), c \neq s} m_{c \rightarrow t}(x_t) \prod_{p \in pa(t)} m_{p \rightarrow t}(x_t)
$$
Em outras palavras, multiplicamos todas as mensagens que chegam a *t* de todos os nós, exceto o destinatário *s*, combinamos e, em seguida, passamos pelo potencial de aresta $\psi_{s,t}$ [^3]. No caso de uma cadeia, *t* tem apenas um filho *s* e um pai *p*, então o acima simplifica para [^3]:
$$\
m_{t \rightarrow s}(x_s) = \sum_{x_t} \psi_{s,t}(x_s, x_t) \psi_t(x_t) m_{p \rightarrow t}(x_t)
$$

A versão do BP em que usamos a divisão é chamada de **belief updating**, e a versão em que multiplicamos todas as mensagens, exceto uma, é chamada de **sum-product** [^3]. A versão *belief updating* é análoga à forma como formulamos o Kalman smoother na Seção 18.3.2: as mensagens *top-down* dependem das mensagens *bottom-up* [^3]. Isso significa que elas podem ser interpretadas como probabilidades posteriores condicionais [^3]. A versão *sum-product* é análoga à forma como formulamos o algoritmo *backwards* na Seção 17.4.3: as mensagens *top-down* são completamente independentes das mensagens *bottom-up*, o que significa que elas só podem ser interpretadas como verossimilhanças condicionais [^3].

**Protocolo Paralelo:**
Até agora, apresentamos uma versão serial do algoritmo, na qual enviamos mensagens para cima até a raiz e de volta [^3]. Esta é a abordagem ideal para uma árvore e é uma extensão natural de *forwards-backwards* em cadeias [^3]. No entanto, como um prelúdio para lidar com grafos gerais com loops, agora consideramos uma versão paralela do BP [^3]. Isso dá resultados equivalentes à versão serial, mas é menos eficiente quando implementado em uma máquina serial [^3].

A ideia básica é que todos os nós recebam mensagens de seus vizinhos em paralelo, atualizem seus *belief states* e, finalmente, enviem novas mensagens de volta para seus vizinhos [^4]. Este processo se repete até a convergência [^4]. Esse tipo de arquitetura de computação é chamado de *systolic array*, devido à sua semelhança com um coração batendo [^4].

Mais precisamente, inicializamos todas as mensagens para o vetor de todos os 1s [^4]. Então, em paralelo, cada nó absorve as mensagens de todos os seus vizinhos usando [^4]:
$$\
bel_s(x_s) \propto \psi_s(x_s) \prod_{t \in nbr(s)} m_{t \rightarrow s}(x_s)
$$

Então, em paralelo, cada nó envia mensagens para cada um de seus vizinhos [^4]:
$$\
m_{s \rightarrow t}(x_t) = \sum_{x_s} \psi_s(x_s) \psi_{s,t}(x_s, x_t) \prod_{u \in nbr(s) \setminus t} m_{u \rightarrow s}(x_s)
$$

A mensagem $m_{s \rightarrow t}$ é computada multiplicando todas as mensagens de entrada, exceto a enviada pelo destinatário, e então passando pelo potencial $\psi_{s,t}$ [^4]. Na iteração *T* do algoritmo, $bel_s(x_s)$ representa a crença posterior de $x_s$ condicionada à evidência que está a *T* passos de distância no grafo [^4]. Após *D(G)* passos, onde *D(G)* é o diâmetro do grafo (a maior distância entre quaisquer dois pares de nós), cada nó obteve informações de todos os outros nós [^4]. Seu *belief state* local é então a marginal posterior correta [^4]. Como o diâmetro de uma árvore é no máximo |V| – 1, o algoritmo converge em um número linear de passos [^4].

Podemos realmente derivar a versão *up-down* do algoritmo impondo a condição de que um nó só pode enviar uma mensagem depois de receber mensagens de todos os seus outros vizinhos [^4]. Isso significa que devemos começar com os nós folha, que têm apenas um vizinho [^4]. As mensagens então se propagam até a raiz e de volta [^4]. Também podemos atualizar os nós em uma ordem aleatória [^4]. O único requisito é que cada nó seja atualizado *D(G)* vezes [^4]. Isso é tempo suficiente para que as informações se espalhem por toda a árvore [^4].

Algoritmos paralelos e distribuídos semelhantes para resolver sistemas lineares de equações são discutidos em (Bertsekas 1997) [^4]. Em particular, o algoritmo de Gauss-Seidel é análogo à versão serial *up-down* do BP, e o algoritmo de Jacobi é análogo à versão paralela do BP [^4].

**Gaussian BP:**
Agora considere o caso em que $p(x|v)$ é conjuntamente Gaussiano, de modo que pode ser representado como um MRF pairwise Gaussiano, como na Seção 19.4.4 [^4]. Apresentamos agora o algoritmo de propagação de crenças para esta classe de modelos, seguindo a apresentação de (Bickson 2009) (ver também (Malioutov et al. 2006)) [^4]. Assumiremos os seguintes potenciais de nó e aresta [^4]:
$$\
\psi_t(x_t) = \exp\left(-\frac{1}{2} A_{tt} x_t^2 + b_t x_t\right)
$$
$$\
\psi_{s,t}(x_s, x_t) = \exp\left(-\frac{1}{2} x_s A_{st} x_t\right)
$$
de modo que o modelo geral tem a forma [^4]:
$$\
p(\mathbf{x}|\mathbf{v}) \propto \exp\left(-\frac{1}{2} \mathbf{x}^\top \mathbf{A} \mathbf{x} + \mathbf{b}^\top \mathbf{x}\right)
$$
Esta é a forma de informação do MVN (ver Exercício 9.2), onde A é a matriz de precisão [^5]. Observe que, ao completar o quadrado, a evidência local pode ser reescrita como uma Gaussiana [^5]:
$$\
\psi_t(x_t) \propto \mathcal{N}(b_t/A_{tt}, A_{tt}^{-1}) \triangleq \mathcal{N}(m_t, \ell_t^{-1})
$$
Abaixo, descrevemos como usar o BP para computar as marginais dos nós posteriores [^5]:
$$\
p(x_t|v) = \mathcal{N}(\mu_t, \lambda_t^{-1})
$$
Se o grafo for uma árvore, o método é exato [^5]. Se o grafo for *loopy*, as médias posteriores ainda podem ser exatas, mas as variâncias posteriores são frequentemente muito pequenas (Weiss e Freeman 1999) [^5].
Embora a matriz de precisão A seja frequentemente esparsa, o cálculo da média posterior requer a inversão dela, uma vez que $\mu = \mathbf{A}^{-1} \mathbf{b}$ [^5]. O BP fornece uma maneira de explorar a estrutura do grafo para realizar essa computação em tempo O(D) em vez de O(D³) [^5]. Isso está relacionado a vários métodos de álgebra linear, conforme discutido em (Bickson 2009) [^5].
Como o modelo é conjuntamente Gaussiano, todas as marginais e todas as mensagens serão Gaussianas [^5]. As principais operações de que precisamos são multiplicar dois fatores Gaussianos e marginalizar uma variável de um fator Gaussiano conjunto [^5].
Para multiplicação, podemos usar o fato de que o produto de duas Gaussianas é Gaussiano [^5]:
$$\
\mathcal{N}(x|\mu_1, \lambda_1^{-1}) \times \mathcal{N}(x|\mu_2, \lambda_2^{-1}) = C \mathcal{N}(x|\mu, \lambda^{-1})
$$
$$\
\lambda = \lambda_1 + \lambda_2
$$
$$\
\mu = \lambda^{-1}(\mu_1 \lambda_1 + \mu_2 \lambda_2)
$$
onde
$$\
C = \sqrt{\frac{\lambda}{\lambda_1 \lambda_2}} \exp\left(\frac{1}{2} \left[\lambda_1 \mu_1^2 (\lambda^{-1} \lambda_1 - 1) + \lambda_2 \mu_2^2 (\lambda^{-1} \lambda_2 - 1) + 2 \lambda^{-1} \mu_1 \lambda_1 \mu_2 \lambda_2 \right]\right)
$$
Para a marginalização, temos o seguinte resultado [^5]:
$$\
\int \exp(-ax^2 + bx) dx = \sqrt{\pi/a} \exp(b^2/4a)
$$

Para computar as próprias mensagens, usamos a Equação 20.11, que é dada por [^6]:
$$\
m_{s \rightarrow t}(x_t) = \int_{x_s} \psi_{s,t}(x_s, x_t) \psi_s(x_s) \prod_{u \in nbr(s) \setminus t} m_{u \rightarrow s}(x_s) dx_s = \int_{x_s} \psi_{s,t}(x_s, x_t) f_{s \setminus t}(x_s) dx_s
$$
onde $f_{s \setminus t}(x_s)$ é o produto da evidência local e todas as mensagens de entrada, excluindo a mensagem de *t* [^6]:
$$\
f_{s \setminus t}(x_s) = \psi_s(x_s) \prod_{u \in nbr(s) \setminus t} m_{u \rightarrow s}(x_s) = \mathcal{N}(x_s|\mu_{s\setminus t}, \lambda_{s \setminus t}^{-1})
$$
$$\
\lambda_{s \setminus t} \triangleq \ell_s + \sum_{u \in nbr(s) \setminus t} \Lambda_{us}
$$
$$\
\mu_{s \setminus t} \triangleq \lambda_{s \setminus t}^{-1} \left(\ell_s m_s + \sum_{u \in nbr(s) \setminus t} \Lambda_{us} \mu_{us} \right)
$$
Retornando à Equação 20.26, temos [^6]:
$$\
m_{s \rightarrow t}(x_t) = \int_{x_s} \exp(-x_s A_{st} x_t) \exp(-\lambda_{s \setminus t}/2 (x_s - \mu_{s \setminus t})^2) dx_s = \int_{x_s} \exp ((-A_{s \setminus t} x_s^2/2) + (\lambda_{s \setminus t} \mu_{s \setminus t} - A_{st} x_t) x_s) dx_s + const
$$
$$\
\propto \exp \left( (\lambda_{s \setminus t} \mu_{s \setminus t} - A_{st} x_t)^2/(2 \lambda_{s \setminus t}) \right) \propto \mathcal{N}(\mu_{st}, \Lambda_{st}^{-1})
$$
$$\
\Lambda_{st} = A_{st}^2/\lambda_{s \setminus t}
$$
$$\
\mu_{st} = A_{st} \mu_{s \setminus t}/\Lambda_{st}
$$

Pode-se generalizar essas equações para o caso em que cada nó é um vetor e as mensagens se tornam pequenos MVNs em vez de Gaussianas escalares (Alag e Agogino 1996) [^6]. Se aplicarmos o algoritmo resultante a um sistema dinâmico linear, recuperamos o algoritmo de suavização de Kalman da Seção 18.3.2 [^6].

Para realizar a passagem de mensagens em modelos com potenciais não Gaussianos, pode-se usar métodos de amostragem para aproximar as integrais relevantes [^6]. Isso é chamado de BP não paramétrico (Sudderth et al. 2003; Isard 2003; Sudderth et al. 2010) [^6].

### Conclusão
O Belief Propagation oferece uma abordagem eficaz para a inferência exata em modelos gráficos com estrutura de árvore [^1]. Suas variantes, como belief updating e sum-product, fornecem ferramentas flexíveis para diferentes tipos de modelos e aplicações [^1]. A compreensão detalhada dos protocolos serial e paralelo, bem como a adaptação para modelos Gaussianos, capacita os acadêmicos a aplicar e estender esse algoritmo em uma variedade de contextos [^3, 4].

### Referências
[^1]: Chapter 20. Exact inference for graphical models, 20.1 Introduction, 20.2 Belief propagation for trees
[^2]: Chapter 20. Exact inference for graphical models, 20.2.1 Serial protocol
[^3]: Chapter 20. Exact inference for graphical models, 20.2.2 Parallel protocol
[^4]: Chapter 20. Exact inference for graphical models, 20.2.3 Gaussian BP *
[^5]: Chapter 20. Exact inference for graphical models, 20.2 Belief propagation for trees
[^6]: Chapter 20. Exact inference for graphical models, 20.2 Belief propagation for trees
<!-- END -->