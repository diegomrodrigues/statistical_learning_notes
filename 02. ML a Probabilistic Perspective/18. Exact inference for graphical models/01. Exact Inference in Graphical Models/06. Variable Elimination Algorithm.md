## Variable Elimination: Um Algoritmo para Inferência Exata

### Introdução
Como vimos anteriormente, a inferência exata em modelos gráficos é um problema fundamental. O algoritmo de *belief propagation* (BP) [^1] oferece uma solução eficiente para árvores. No entanto, para grafos arbitrários, precisamos de abordagens mais gerais. Esta seção explora o algoritmo de **Variable Elimination (VE)** [^7], uma técnica poderosa para computar probabilidades marginais em qualquer tipo de grafo. O VE generaliza o algoritmo *forwards-backwards* [^1] e o BP, fornecendo uma estrutura unificada para inferência exata.

### Conceitos Fundamentais

O algoritmo VE, também conhecido como *bucket elimination* ou *peeling algorithm* [^7], calcula probabilidades marginais $p(x_q|x_v)$ para qualquer tipo de grafo [^7]. Ele alcança isso eliminando sistematicamente as variáveis, "empurrando" somas para dentro de produtos [^7].

**Transformação de DGMs para UGMs:** O VE opera convertendo modelos gráficos direcionados (DGMs) em modelos gráficos não direcionados (UGMs) [^7]. Esta conversão envolve duas etapas principais:

1.  **Moralização:** Adicionar arestas entre todos os pais de um nó [^8].
2.  **Definição de Potenciais:** Definir potenciais para cada CPD [^7].

Após a conversão, o algoritmo procede com a eliminação das variáveis.

**Mecânica do Algoritmo:** O VE funciona multiplicando potenciais dentro do escopo de um operador, marginalizando variáveis para criar novos fatores e repetindo até que a marginal desejada seja obtida [^7]. A chave do VE é realizar as somas sobre as variáveis o mais cedo possível na computação para reduzir o tamanho dos fatores intermediários [^7].

**Exemplo Ilustrativo:** Considere o modelo gráfico direcionado (DGM) na Figura 20.3(a) [^8]. A distribuição conjunta correspondente é dada por [^8]:

$$P(C, D, I, G, S, L, J, H) = P(C)P(D|C)P(I)P(G|I, D)P(S|I)P(L|G)P(J|L, S)P(H|G, J)$$

Para aplicar o VE, convertemos este DGM para um UGM definindo um potencial para cada CPD [^8]:

$$p(C, D, I, G, S, L, J, H) = \psi_C(C)\psi_{D}(D,C)\psi_I(I)\psi_{G}(G, I, D)\psi_S(S, I)\psi_L(L, G)\psi_J(J, L, S)\psi_H(H, G, J)$$

Suponha que desejamos calcular $p(J = 1)$ [^9]. Poderíamos enumerar todas as atribuições possíveis para as variáveis (exceto J) e somar as probabilidades conjuntas [^9], mas isso levaria um tempo $O(2^7)$ [^9]. Em vez disso, podemos usar VE para empurrar as somas para dentro dos produtos [^9]:

$$p(J) = \sum_{L,S,G,H,I,D,C} \psi_C(C)\psi_{D}(D,C)\psi_I(I)\psi_{G}(G, I, D)\psi_S(S, I)\psi_L(L, G)\psi_J(J, L, S)\psi_H(H, G, J)$$

$$ = \sum_{L,S} \psi_J(J, L, S) \sum_{G} \psi_L(L, G) \sum_{H} \psi_H(H, G, J) \sum_{I} \psi_S(S, I)\psi_I(I) \sum_{D} \psi_G(G, I, D) \sum_{C} \psi_C(C)\psi_{D}(D,C)$$

**Complexidade Computacional e Eliminação de Variáveis:** A complexidade computacional do VE é exponencial no tamanho do maior fator criado durante a eliminação [^7]. A ordem em que as variáveis são eliminadas influencia significativamente o tamanho desses fatores [^7]. O objetivo é encontrar uma ordem que minimize a *largura induzida* ou a *treewidth* do grafo [^7].

**Largura Induzida e Treewidth:** A *largura induzida* de um grafo, dada uma ordem de eliminação, é o tamanho do maior fator criado durante a eliminação menos 1 [^13]. A *treewidth* é a largura induzida mínima sobre todas as possíveis ordens de eliminação [^13]. Encontrar uma ordem de eliminação que minimize a treewidth é NP-hard [^13]. Na prática, são usadas técnicas de busca *greedy* para encontrar ordens razoáveis [^13].

**Preenchimento de Arestas:** Quando eliminamos uma variável $X_t$, conectamos todos os nós que compartilham um fator com $X_t$ [^12]. As arestas criadas por este processo são chamadas de *fill-in edges* [^13].

**Exemplo de Eliminação de Variáveis:** Para ilustrar o processo de eliminação, vamos considerar o DGM na Figura 20.3(a) [^8] e eliminar as variáveis na ordem C, D, I, H, G, S, L [^9].
*   Primeiro, multiplicamos todos os termos no escopo do operador $\sum_C$ para criar o fator temporário: $\tau_1(C, D) = \psi_C(C)\psi_D(D,C)$ [^9].
*   Em seguida, marginalizamos C para obter o novo fator: $\tau_1(D) = \sum_C \tau_1(C, D)$ [^9].
*   Continuamos multiplicando todos os termos no escopo do operador $\sum_D$ e marginalizando para criar: $\tau_2(G, I) = \sum_D \psi_G(G, I, D) \tau_1(D)$ [^9].
*   E assim por diante [^9].

**Generalização da Lei Distributiva:** Abstractamente, o VE pode ser visto como computando a seguinte expressão [^11]:

$$p(x_q|x_v) \propto \sum_x \prod_c \psi_c(x_c)$$

onde as variáveis visíveis $x_v$ são fixadas, e não somadas [^11]. O VE usa programação dinâmica não serial, armazenando em cache resultados intermediários para evitar computação redundante [^11].

### Conclusão

O algoritmo de Variable Elimination oferece uma abordagem sistemática para a inferência exata em modelos gráficos [^7]. Embora sua complexidade computacional seja exponencial na treewidth do grafo [^13], o VE fornece uma estrutura fundamental para entender e realizar inferência em uma ampla gama de modelos [^7]. Técnicas como a escolha cuidadosa da ordem de eliminação e a exploração da estrutura do grafo podem ajudar a mitigar os desafios computacionais associados ao VE [^7].

### Referências
[^1]: Section 17.4.3, 18.3.2, 20.2.1, 20.2
[^7]: Section 20.3
[^8]: Figure 20.3(a), Section 20.3
[^9]: Section 20.3
[^11]: Section 20.3.1
[^12]: Section 20.3
[^13]: Section 20.3

<!-- END -->