## Inferência Exata em Cadeias Estruturadas

### Introdução
Este capítulo expande os conceitos de inferência exata em modelos gráficos, generalizando algoritmos como o *forwards-backwards* e o *Kalman smoother* para estruturas de grafos arbitrárias [^1]. O foco inicial é em modelos gráficos com estrutura de cadeia, nos quais o objetivo é computar as marginais posteriores $p(x_t|v, \theta)$, onde $x$ representa variáveis ocultas e $v$ variáveis visíveis [^1]. O presente tópico detalhará como o algoritmo *forwards-backwards* pode ser generalizado para árvores, culminando no algoritmo de *belief propagation* (BP), também conhecido como algoritmo *sum-product* [^1].

### Belief Propagation para Árvores
O algoritmo *forwards-backwards*, discutido na Seção 17.4.3 [^1], é uma ferramenta fundamental para inferência exata em modelos gráficos com estrutura de cadeia. O objetivo desta seção é generalizar este algoritmo para árvores, resultando no algoritmo de *belief propagation* (BP) [^1].

**Serial Protocol**
Inicialmente, assume-se que o modelo é um MRF (Markov Random Field) ou CRF (Conditional Random Field) *pairwise*, dado por [^1]:

$$np(\mathbf{x}|\mathbf{v}) = \frac{1}{Z(\mathbf{v})} \prod_{s \in \mathcal{V}} \psi_s(x_s) \prod_{(s,t) \in \mathcal{E}} \psi_{s,t}(x_s, x_t)$$

onde $\psi_s$ é a evidência local para o nó $s$, e $\psi_{s,t}$ é o potencial para a aresta $s-t$ [^1]. Assume-se também que $\mathbf{x}$ representa as variáveis ocultas e $\mathbf{v}$ as variáveis visíveis [^1].

Para implementar o BP em árvores não direcionadas, um nó arbitrário é escolhido como raiz, $r$, e todas as arestas são orientadas para longe de $r$ [^1]. Isso estabelece uma noção de pai e filho. O algoritmo então procede em duas fases: a fase de *collect evidence*, onde as mensagens são enviadas das folhas para a raiz, e a fase de *distribute evidence*, onde as mensagens são enviadas da raiz para as folhas [^1]. Essas fases são análogas às passagens *forwards* e *backwards* em cadeias [^1].

Para calcular o estado de crença (belief state) no nó $t$, inicialmente condiciona-se a crença apenas nas evidências abaixo de $t$ na árvore, ou seja, calcula-se $bel_t^-(x_t) \propto p(x_t|v_t^-)$ [^2]. Este é chamado de "estado de crença bottom-up". Por indução, assume-se que as "mensagens" dos filhos de $t$ já foram computadas, resumindo o que eles "pensam" que $t$ deve saber sobre a evidência em suas subárvores [^2]:

$$nm_{s \rightarrow t}(x_t) = p(x_t|v_{s \rightarrow t}^-)$$

onde $v_{s \rightarrow t}^-$ é toda a evidência no lado downstream da aresta $s-t$ [^2]. Similarmente, assume-se que $m_{u \rightarrow t}(x_t)$ também foi computado [^2]. O estado de crença bottom-up em $t$ pode então ser calculado como [^2]:

$$nbel_t^-(x_t) = \frac{1}{Z_t} \psi_t(x_t) \prod_{c \in ch(t)} m_{c \rightarrow t}(x_t)$$

onde $\psi_t(x_t) \propto p(x_t|v_t)$ é a evidência local para o nó $t$, e $Z_t$ é a constante de normalização local [^2]. Em palavras, multiplicam-se todas as mensagens vindas dos filhos de $t$, bem como a mensagem da evidência local, e então normaliza-se [^2].

Para computar as próprias mensagens, considera-se $m_{s \rightarrow t}(x_t)$, onde $s$ é um dos filhos de $t$ [^2]. Assume-se, por recursão, que $bel_s^-(x_s) = p(x_s|v_s^-)$ foi computado [^2]. Então, a mensagem pode ser calculada como [^2]:

$$nm_{s \rightarrow t}(x_t) = \sum_{x_s} \psi_{s,t}(x_s, x_t) bel_s^-(x_s)$$

Essencialmente, converte-se crenças sobre $x_s$ em crenças sobre $x_t$ usando o potencial da aresta $\psi_{s,t}$ [^2]. Esse processo continua subindo na árvore até alcançar a raiz. Na raiz, "vimos" toda a evidência na árvore, então podemos computar o estado de crença local na raiz usando [^2]:

$$nbel(x_r) = p(x_r|v) = p(x_r|v_r) \propto \psi_r(x_r) \prod_{c \in ch(r)} m_{c \rightarrow r}(x_r)$$

Isso completa o passe ascendente (upwards pass), análogo ao passe *forwards* em um HMM [^2]. Como um "efeito colateral", a probabilidade da evidência pode ser computada coletando as constantes de normalização [^2]:

$$np(v) = \prod_t Z_t$$

Agora, as mensagens podem ser passadas para baixo a partir da raiz [^3]. Para calcular o estado de crença para $s$, com pai $t$, é necessário combinar a crença bottom-up para $s$ com uma mensagem top-down de $t$, que resume toda a informação no resto do grafo [^3]:

$$nm_{t \rightarrow s}(x_s) \propto p(x_t|v_t^+)$$

onde $v_t^+$ é toda a evidência no lado upstream (raiz) da aresta $s-t$ [^3]. Então, [^3]:

$$nbel_s(x_s) \propto p(x_s|v) \propto bel_s^-(x_s) \prod_{t \in pa(s)} m_{t \rightarrow s}(x_s)$$

Para computar as mensagens descendentes (downward messages), considera-se a mensagem de $t$ para $s$. Suponha que o pai de $t$ seja $r$, e os filhos de $t$ sejam $s$ e $u$. Queremos incluir em $m_{t \rightarrow s}$ toda a informação que $t$ recebeu, exceto pela informação que $s$ enviou [^3]:

$$nm_{t \rightarrow s}(x_s) \propto p(x_s|v_t) = \sum_{x_t} \psi_{s,t}(x_s, x_t) \frac{bel_t(x_t)}{m_{s \rightarrow t}(x_t)}$$

Em vez de dividir a mensagem enviada para cima para $t$, podemos inserir a equação de $bel_t$ para obter [^3]:

$$nm_{t \rightarrow s}(x_s) = \sum_{x_t} \psi_{s,t}(x_s, x_t) \psi_t(x_t) \prod_{c \in ch(t), c \neq s} m_{c \rightarrow t}(x_t) \prod_{p \in pa(t)} m_{p \rightarrow t}(x_t)$$

Em outras palavras, multiplicam-se todas as mensagens que chegam em $t$ de todos os nós, exceto o destinatário $s$, combina-se tudo e então passa-se pelo potencial da aresta $\psi_{s,t}$ [^3]. No caso de uma cadeia, $t$ tem apenas um filho $s$ e um pai $p$, então o acima simplifica para [^3]:

$$nm_{t \rightarrow s}(x_s) = \sum_{x_t} \psi_{s,t}(x_s, x_t) \psi_t(x_t) m_{p \rightarrow t}(x_t)$$

A versão do BP na qual usamos divisão é chamada *belief updating*, e a versão na qual multiplicamos todas as mensagens, exceto uma, é chamada *sum-product* [^3]. A versão *belief updating* é análoga a como formulamos o *Kalman smoother* na Seção 18.3.2: as mensagens top-down dependem das mensagens bottom-up [^3]. Isso significa que elas podem ser interpretadas como probabilidades posteriores condicionais [^3]. A versão *sum-product* é análoga a como formulamos o algoritmo *backwards* na Seção 17.4.3: as mensagens top-down são completamente independentes das mensagens bottom-up [^3], o que significa que elas só podem ser interpretadas como *conditional likelihoods* [^3].

### Conclusão
O algoritmo de *belief propagation* (BP) generaliza o algoritmo *forwards-backwards* para árvores, permitindo a inferência exata em modelos gráficos com essa estrutura. A implementação serial do BP envolve a passagem de mensagens das folhas para a raiz e, em seguida, da raiz para as folhas, garantindo a computação das marginais posteriores em cada nó [^3]. As versões *belief updating* e *sum-product* oferecem perspectivas complementares sobre a propagação de crenças, com a primeira mantendo dependências entre as mensagens top-down e bottom-up, e a última tratando-as como independentes [^3].

### Referências
[^1]: Seção 20.1 do texto fornecido.
[^2]: Seção 20.2 do texto fornecido.
[^3]: Seção 20.2.1 do texto fornecido.
<!-- END -->