## M-Step in EM Algorithm for PCA: Updating the Factor Loading Matrix

### Introdução
O algoritmo EM (Expectation-Maximization) é uma técnica iterativa para encontrar estimativas de máxima verossimilhança de parâmetros em modelos probabilísticos que dependem de variáveis latentes não observadas. No contexto de PCA (Principal Component Analysis), o algoritmo EM é usado para estimar a matriz de carregamento de fatores **W**. Este capítulo se concentrará especificamente no passo M (Maximização) do algoritmo EM para PCA, detalhando como a matriz **W** é atualizada.

### Conceitos Fundamentais
O passo M no algoritmo EM para PCA envolve a atualização da matriz de carregamento de fatores **W**. De acordo com o contexto fornecido [^1], a atualização é realizada usando a seguinte equação:

$$ W = X \tilde{Z}^T (\tilde{Z} \tilde{Z}^T)^{-1} $$

onde:
- **W** é a matriz de carregamento de fatores (*factor loading matrix*).
- **X** representa os dados observados.
- $\tilde{Z}$ representa os valores esperados das variáveis latentes.

A equação acima é análoga à regressão linear de múltiplas saídas (*multi-output linear regression*), onde as entradas observadas são substituídas pelos valores esperados das variáveis latentes [^1].

**Analogia com Regressão Linear de Múltiplas Saídas**
Para entender melhor o passo M, é útil traçar um paralelo com a regressão linear de múltiplas saídas. Em uma regressão linear padrão, tentamos modelar a relação entre uma variável dependente e uma ou mais variáveis independentes. Na regressão de múltiplas saídas, temos múltiplas variáveis dependentes que tentamos modelar simultaneamente.

A equação para regressão linear de múltiplas saídas é dada por:

$$ Y = XB + \epsilon $$

onde:
- **Y** é uma matriz de variáveis dependentes.
- **X** é uma matriz de variáveis independentes.
- **B** é uma matriz de coeficientes.
- $\epsilon$ é o termo de erro.

A solução para **B** que minimiza o erro quadrático médio é:

$$ B = (X^T X)^{-1} X^T Y $$

No passo M do EM para PCA, estamos efetivamente realizando uma regressão linear onde **X** é a matriz de dados observados, $\tilde{Z}$ são os valores esperados das variáveis latentes, e **W** é a matriz de coeficientes que desejamos estimar. A diferença crucial é que, em vez de usar as variáveis latentes diretamente, usamos seus valores esperados, que são calculados no passo E (Expectation).

**Derivação Detalhada do Passo M**
A derivação do passo M pode ser vista como um problema de otimização onde buscamos encontrar a matriz **W** que maximiza a verossimilhança dos dados observados, dados os valores esperados das variáveis latentes. A função de verossimilhança pode ser expressa como:

$$ L(W) = p(X | \tilde{Z}, W) $$

Sob a suposição de que os dados são Gaussianos, a função de verossimilhança pode ser escrita como:

$$ L(W) \propto \exp \left\{ -\frac{1}{2} \sum_{i=1}^{N} (x_i - W \tilde{z}_i)^T \Psi^{-1} (x_i - W \tilde{z}_i) \right\} $$

onde:
- $x_i$ é o i-ésimo vetor de dados.
- $\tilde{z}_i$ é o valor esperado do i-ésimo vetor de variável latente.
- $\Psi$ é a matriz de covariância do ruído.

Para maximizar a verossimilhança, minimizamos a soma dos quadrados dos resíduos:

$$ \mathcal{J}(W) = \sum_{i=1}^{N} (x_i - W \tilde{z}_i)^T \Psi^{-1} (x_i - W \tilde{z}_i) $$

Tomando a derivada de $\mathcal{J}(W)$ em relação a **W** e igualando a zero, obtemos:

$$ \frac{\partial \mathcal{J}(W)}{\partial W} = -2 \sum_{i=1}^{N} \Psi^{-1} (x_i - W \tilde{z}_i) \tilde{z}_i^T = 0 $$

Resolvendo para **W**, temos:

$$ W = \left( \sum_{i=1}^{N} x_i \tilde{z}_i^T \right) \left( \sum_{i=1}^{N} \tilde{z}_i \tilde{z}_i^T \right)^{-1} $$

Na forma matricial, isso se torna:

$$ W = X \tilde{Z}^T (\tilde{Z} \tilde{Z}^T)^{-1} $$

que é a equação fornecida no contexto [^1].

**Considerações Práticas**
Na prática, o cálculo da inversa da matriz $(\tilde{Z} \tilde{Z}^T)$ pode ser computacionalmente caro, especialmente para grandes conjuntos de dados. Além disso, se a matriz $(\tilde{Z} \tilde{Z}^T)$ for singular ou quase singular, a inversão direta pode levar a instabilidades numéricas. Para mitigar esses problemas, técnicas de regularização ou métodos iterativos podem ser empregados.

### Conclusão
O passo M no algoritmo EM para PCA é crucial para atualizar a matriz de carregamento de fatores **W**, que define a relação entre os dados observados e as variáveis latentes. Este passo é análogo à regressão linear de múltiplas saídas, onde os valores esperados das variáveis latentes são usados para estimar **W**. A derivação detalhada e as considerações práticas fornecem uma compreensão profunda do passo M e sua importância no algoritmo EM para PCA.

### Referências
[^1]: Page 381, "The M step involves updating the factor loading matrix W using W = X Z̃ᵀ (Z̃ Z̃ᵀ)⁻¹, which is similar to multi-output linear regression where the observed inputs are replaced by the expected values of the latent variables. The M step is given by W = X Z̃ᵀ (Z̃ Z̃ᵀ)⁻¹."
<!-- END -->