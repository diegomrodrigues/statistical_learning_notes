## Convergência do Algoritmo EM para PCA

### Introdução
O algoritmo EM (Expectation-Maximization) é uma técnica iterativa para encontrar estimativas de máxima verossimilhança de parâmetros em modelos estatísticos onde o modelo depende de variáveis latentes não observadas. No contexto da análise de componentes principais (PCA), o algoritmo EM oferece uma abordagem alternativa para encontrar os componentes principais de um conjunto de dados, em vez dos métodos tradicionais baseados em autovetores da matriz de covariância [^386]. Este capítulo explora a convergência do algoritmo EM aplicado ao PCA, demonstrando que ele converge para uma solução globalmente ótima.

### Conceitos Fundamentais

No contexto do PCA, o algoritmo EM busca encontrar uma matriz de pesos **W** que mapeia as variáveis latentes **z** para as observações **x**. A formulação probabilística do PCA, também conhecida como PPCA (Probabilistic Principal Component Analysis), assume que as observações **x** são geradas a partir de variáveis latentes gaussianas **z** através de uma transformação linear dada por **W**, acrescida de ruído gaussiano isotrópico [^12.2].

O algoritmo EM itera entre duas etapas:
1.  **Etapa E (Expectation):** Calcula a distribuição posterior das variáveis latentes **z** dado as observações **x** e a estimativa atual dos parâmetros. No caso do PPCA, a distribuição posterior é gaussiana, e pode ser calculada analiticamente [^12.2].
2.  **Etapa M (Maximization):** Atualiza as estimativas dos parâmetros (neste caso, a matriz de pesos **W**) maximizando a verossimilhança esperada, dada a distribuição posterior das variáveis latentes calculada na etapa E [^386].

Um resultado fundamental é que o algoritmo EM, quando aplicado ao PPCA, converge para uma solução onde o espaço linear abrangido pelas colunas de **W** coincide com o espaço linear definido pelos primeiros *L* autovetores da matriz de covariância amostral [^396]. Este resultado garante que o algoritmo EM encontra uma solução que corresponde aos componentes principais que capturam a maior variância dos dados.

**Teorema da Convergência Global:** O único ponto fixo estável do algoritmo EM para PCA é a solução globalmente ótima, onde **W** abrange o mesmo subespaço linear definido pelos primeiros *L* autovetores [^396].

**Demonstração:**
A demonstração deste teorema envolve mostrar que, a cada iteração do algoritmo EM, a verossimilhança dos dados aumenta monotonicamente, e que o único ponto onde a verossimilhança não pode ser mais aumentada corresponde à solução globalmente ótima.

A etapa E do algoritmo EM para PCA é dada por [^396]:
$$\
\tilde{Z} = (W^T W)^{-1} W^T X
$$
onde $\tilde{Z}$ é uma matriz $L \times N$ que armazena as médias posteriores (representações de baixa dimensão) ao longo de suas colunas e $X$ é a matriz de dados original, com as observações ao longo de suas colunas.

A etapa M é dada por [^396]:
$$\
W = X \tilde{Z}^T (\tilde{Z} \tilde{Z}^T)^{-1}
$$

Este passo é semelhante à regressão linear, onde substituímos as entradas observadas pelos valores esperados das variáveis latentes [^396].

Para provar a convergência, é fundamental demonstrar que a cada iteração, a verossimilhança marginal $p(X|W,\sigma^2)$ aumenta ou permanece constante. Além disso, é crucial mostrar que o único ponto fixo estável é aquele em que as colunas de $W$ abrangem o mesmo espaço que os primeiros $L$ autovetores da matriz de covariância amostral.

**Considerações Adicionais:**

*   **Inicialização:** A escolha da inicialização da matriz de pesos **W** pode influenciar a velocidade de convergência do algoritmo EM, mas não afeta a solução final para a qual ele converge.
*   **Ortogonalização:** Embora o algoritmo EM convirja para a solução globalmente ótima, a matriz **W** resultante não é necessariamente ortogonal [^396]. Se for desejável uma matriz ortogonal, pode-se aplicar um processo de ortogonalização a **W** após a convergência.
*   **Outras Abordagens:** Existem outras abordagens para modificar o EM para obter a base principal diretamente (Ahn e Oh 2003), mas a forma apresentada aqui garante a convergência para uma solução ótima [^396].

### Conclusão

O algoritmo EM oferece uma alternativa válida para realizar PCA, especialmente em cenários onde há dados faltantes [^387]. O teorema da convergência global garante que o algoritmo EM converge para uma solução que corresponde aos componentes principais que capturam a maior variância dos dados. A interpretação física do algoritmo EM para PCA, em termos de um sistema de molas e hastes rígidas, fornece uma intuição valiosa sobre o processo de otimização [^396]. Embora a matriz de pesos **W** resultante não seja necessariamente ortogonal, ela pode ser ortogonalizada posteriormente, se necessário. A convergência do algoritmo EM para a solução globalmente ótima, combinada com sua capacidade de lidar com dados faltantes, torna-o uma ferramenta valiosa no campo da análise de dados e redução de dimensionalidade.
### Referências
[^386]: Capítulo 12, Latent linear models, página 386.
[^396]: Capítulo 12, Latent linear models, página 396.

<!-- END -->