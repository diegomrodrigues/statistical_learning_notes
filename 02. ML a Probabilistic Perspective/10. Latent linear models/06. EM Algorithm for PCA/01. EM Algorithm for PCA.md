## EM Algorithm for PCA in the Zero Noise Limit

### Introdução
O algoritmo EM (Expectation-Maximization) é uma técnica iterativa poderosa para encontrar estimativas de máxima verossimilhança de parâmetros em modelos probabilísticos que envolvem variáveis latentes. No contexto da Análise de Componentes Principais (PCA), o algoritmo EM oferece uma abordagem alternativa para ajustar um modelo PCA, especialmente útil em situações onde os métodos tradicionais baseados em autovetores podem ser desafiadores. Uma dessas situações é o limite de ruído zero, onde a variância do ruído se aproxima de zero ($σ^2 = 0$). Este capítulo explora o algoritmo EM para PCA, com foco em seu comportamento e aplicabilidade no limite de ruído zero.

### Conceitos Fundamentais

O algoritmo EM para PCA se baseia na formulação probabilística de PCA, também conhecida como PPCA (Probabilistic Principal Component Analysis) [^15]. Em PPCA, assume-se que os dados observados **x** são gerados a partir de variáveis latentes **z** através de uma transformação linear, acrescida de ruído Gaussiano isotrópico [^1]. Especificamente, o modelo PPCA é definido por:

$$\
p(\mathbf{z}) = \mathcal{N}(\mathbf{z}|\mathbf{0}, \mathbf{I})
$$
$$\
p(\mathbf{x}|\mathbf{z}, \mathbf{W}, \sigma^2) = \mathcal{N}(\mathbf{x}|\mathbf{W}\mathbf{z} + \mathbf{\mu}, \sigma^2\mathbf{I})
$$

onde **W** é a matriz de pesos (ou *factor loading matrix* [^1]), $\mathbf{\mu}$ é a média dos dados, e $\sigma^2$ é a variância do ruído. O objetivo é estimar os parâmetros **W** e $\sigma^2$ a partir dos dados observados.

O algoritmo EM itera entre duas etapas [^6]:

1.  **Etapa E (Expectation):** Calcula a distribuição *a posteriori* das variáveis latentes dado os dados observados e as estimativas atuais dos parâmetros.
2.  **Etapa M (Maximization):** Atualiza as estimativas dos parâmetros maximizando a função de verossimilhança esperada, utilizando a distribuição *a posteriori* calculada na etapa E.

No contexto de PCA, a etapa E envolve calcular a média *a posteriori* (ou *latent scores* [^2]) das variáveis latentes:

$$\
\mathbb{E}[\mathbf{z}|\mathbf{x}, \mathbf{W}, \sigma^2] = (\mathbf{W}^T\mathbf{W} + \sigma^2\mathbf{I})^{-1}\mathbf{W}^T(\mathbf{x} - \mathbf{\mu})
$$

e a covariância *a posteriori*:

$$\
\text{cov}[\mathbf{z}|\mathbf{x}, \mathbf{W}, \sigma^2] = \sigma^2(\mathbf{W}^T\mathbf{W} + \sigma^2\mathbf{I})^{-1}
$$

A etapa M envolve atualizar as estimativas de **W** e $\sigma^2$ [^7]:

$$\
\mathbf{W} = \left( \sum_i \mathbf{x}_i \mathbb{E}[\mathbf{z}_i]^T \right) \left( \sum_i \mathbb{E}[\mathbf{z}_i \mathbf{z}_i^T] \right)^{-1}
$$

$$\
\sigma^2 = \frac{1}{D} \sum_i \mathbb{E} \left[ ||\mathbf{x}_i - \mathbf{W}\mathbf{z}_i||^2 \right]
$$

O ponto crucial é que o algoritmo EM **continua a funcionar mesmo no limite de ruído zero, $\sigma^2 = 0$** [^16]. Neste limite, as equações para a média *a posteriori* e a atualização de **W** se simplificam. Especificamente, a média *a posteriori* torna-se:

$$\
\mathbb{E}[\mathbf{z}|\mathbf{x}, \mathbf{W}] = (\mathbf{W}^T\mathbf{W})^{-1}\mathbf{W}^T(\mathbf{x} - \mathbf{\mu})
$$

e a atualização de **W** mantém sua forma, mas com as médias *a posteriori* calculadas usando a equação acima.

No entanto, é importante notar que, para garantir a convergência para uma solução única e interpretabilidade, pode ser necessário impor restrições adicionais, como ortogonalidade em **W** [^4]. Sem essas restrições, a solução pode não ser unicamente identificável devido à ambiguidade rotacional [^4].

#### Vantagens do EM para PCA
O uso do algoritmo EM para PCA oferece diversas vantagens em comparação com abordagens tradicionais baseadas em autovetores [^17]:

*   **Robustez a dados faltantes:** O EM pode ser facilmente adaptado para lidar com dados faltantes, uma situação comum em muitas aplicações do mundo real [^7].
*   **Implementação *online*:** O EM pode ser implementado de forma *online*, permitindo a atualização incremental da estimativa de **W** à medida que novos dados se tornam disponíveis [^18].
*   **Extensões para modelos mais complexos:** O EM fornece uma estrutura flexível que pode ser estendida para ajustar modelos mais complexos, como misturas de modelos PCA (Mixture of Factor Analyzers) [^6, 18].

### Conclusão
O algoritmo EM para PCA oferece uma abordagem iterativa robusta para ajustar modelos PCA, mesmo no limite de ruído zero. Sua capacidade de lidar com dados faltantes, implementação *online* e extensibilidade o tornam uma ferramenta valiosa em diversas aplicações. Compreender o comportamento do EM para PCA no limite de ruído zero é crucial para garantir a convergência para soluções significativas e para a correta interpretação dos resultados.

### Referências
[^1]: Page 381: "p(zi) = N(Ζίμο, Σο)", "p(xizi, 0) = N(Wz; + μ, Ψ)", "where W is a D × L matrix, known as the factor loading matrix"
[^2]: Page 382: "The mi are sometimes called the latent scores, or latent factors."
[^3]: Page 382: " Σ(Σ1+WT-1W)-1"
[^4]: Page 384: "Forcing W to be orthonormal Perhaps the cleanest solution to the identifiability problem is to force W to be orthonormal, and to order the columns by decreasing variance of the corresponding latent factors."
[^5]: Page 387: "Consider the FA model where we constrain Ψ = σ²I, and W to be orthonormal."
[^6]: Page 386: "EM for factor analysis models"
[^7]: Page 387: "(Wc, μc), ž = (z, 1), Also, define"
[^8]: Page 387: "Σricxibic Eric Cic"
[^9]: Page 387: "1Ndiag∑iTic(xi−Wcbic)(xi−Wcbic)T"
[^10]: Page 395: "Probabilistic PCA"
[^11]: Page 395: "One can show the following remarkable result."
[^12]: Page 395: "log p(X|W, σ²) = N2[−D ln σ² − 1σ²∑i=1N||xi||² + 1σ4∑i=1N||Wxi||²]"
[^13]: Page 395: "W = V(Λ – σ²I)½R"
[^14]: Page 395: "σ2 = 1D−L∑j=L+1Dλj"
[^15]: Page 395: "Theorem 12.2.2 ((Tipping and Bishop 1999)). Consider a factor analysis model in which Ψ = σ²I and W is orthogonal."
[^16]: Page 396: "However the algorithm continues to work in the zero noise limit, σ² = 0, as shown by (Roweis 1997)."
[^17]: Page 397: "Apart from this pleasing intuitive interpretation, EM for PCA has the following advantages over eigenvector methods:"
[^18]: Page 398: "EM can be implemented in an online fashion, i.e., we can update our estimate of W as the data streams in."
[^19]: Page 398: "EM can handle missing data in a simple way (see Section 12.1.6)."

<!-- END -->