## Scree Plot e Erro Residual em PCA

### Introdução
A escolha do número de dimensões latentes (*L*) em Principal Component Analysis (**PCA**) é um passo crucial para equilibrar a capacidade de representação do modelo e a complexidade, evitando *overfitting* [^384, 399]. Como vimos anteriormente, o **PCA** busca encontrar um conjunto ortogonal de vetores base lineares $w_j \in \mathbb{R}^D$ e as *scores* correspondentes $z_i \in \mathbb{R}^L$ que minimizam o erro médio de reconstrução [^387]. Este capítulo explora métodos para avaliar o erro de reconstrução e como o *scree plot* pode auxiliar na seleção de *L*.

### Erro Residual e Autovalores Descartados
Em **PCA**, a reconstrução dos dados $x_i$ usando *L* componentes principais é dada por $\hat{x}_i = W z_i + \mu$, onde $W$ é a matriz de *loadings* fatoriais e $\mu$ é a média dos dados [^399]. O erro residual resultante do uso de apenas *L* termos em **PCA** é quantificado pela soma dos autovalores descartados [^400]:

$$
E(D_{train}, L) = \sum_{j=L+1}^{D} \lambda_j
$$

onde $\lambda_j$ são os autovalores da matriz de covariância empírica $\Sigma = \frac{1}{N} \sum_{i=1}^{N} x_i x_i^T$ [^388, 390].

**Lemma 1:** *O erro residual $E(D_{train}, L)$ representa a variância total dos dados que não é explicada pelas primeiras *L* componentes principais.*

*Prova:* Os autovalores $\lambda_j$ representam a variância dos dados ao longo da direção do autovetor correspondente $v_j$. Ao descartar os autovalores $\lambda_{L+1}, \dots, \lambda_D$, estamos descartando a variância correspondente nessas direções. A soma desses autovalores, portanto, quantifica a variância total não explicada pelas primeiras *L* componentes. $\blacksquare$

### Scree Plot
Uma alternativa para plotar o erro residual diretamente é plotar os autovalores retidos em ordem decrescente, o que é conhecido como *scree plot* [^400]. No *scree plot*, o eixo *x* representa o número da componente principal, e o eixo *y* representa o valor do autovalor correspondente.

**Observação:** *O *scree plot* visualiza a importância de cada componente principal na explicação da variância total dos dados.*

A forma do *scree plot* pode fornecer *insights* sobre o número apropriado de componentes a serem retidos. Tipicamente, o gráfico exibe uma queda acentuada nos primeiros autovalores, seguida por uma diminuição gradual. O ponto de inflexão, onde a curva se achata, sugere um limite para o número de componentes a serem retidos. Componentes além desse ponto contribuem pouco para explicar a variância dos dados e podem representar ruído.

**Exemplo:** Se os primeiros três autovalores forem significativamente maiores que os demais, pode ser razoável reter apenas as três primeiras componentes principais [^390].

### Fração da Variância Explicada
Uma quantidade relacionada ao erro residual é a *fração da variância explicada* (**FVE**), definida como [^400]:

$$
F(D_{train}, L) = \frac{\sum_{j=1}^{L} \lambda_j}{\sum_{j=1}^{L_{max}} \lambda_j}
$$

onde $L_{max}$ é o número total de autovalores (igual à dimensão dos dados, *D*). A **FVE** quantifica a proporção da variância total dos dados que é explicada pelas primeiras *L* componentes principais. Um valor alto de **FVE** indica que as primeiras *L* componentes capturam a maior parte da informação nos dados.

**Corolário 1:** *A soma do erro residual normalizado e da fração da variância explicada é igual a 1.*

*Prova:* Dividindo a equação do erro residual pela soma de todos os autovalores, obtemos o erro residual normalizado:

$$
\frac{E(D_{train}, L)}{\sum_{j=1}^{D} \lambda_j} = \frac{\sum_{j=L+1}^{D} \lambda_j}{\sum_{j=1}^{D} \lambda_j}
$$

Somando a **FVE** e o erro residual normalizado:

$$
F(D_{train}, L) + \frac{E(D_{train}, L)}{\sum_{j=1}^{D} \lambda_j} = \frac{\sum_{j=1}^{L} \lambda_j}{\sum_{j=1}^{D} \lambda_j} + \frac{\sum_{j=L+1}^{D} \lambda_j}{\sum_{j=1}^{D} \lambda_j} = \frac{\sum_{j=1}^{D} \lambda_j}{\sum_{j=1}^{D} \lambda_j} = 1
$$

$\blacksquare$

### Considerações Práticas
Ao utilizar o *scree plot* e a **FVE** para escolher o número de componentes principais, é importante considerar o contexto do problema e o objetivo da análise. Em algumas aplicações, pode ser suficiente reter apenas as componentes que explicam uma porcentagem significativa da variância (por exemplo, 80% ou 90%). Em outras aplicações, pode ser necessário reter mais componentes para preservar informações importantes, mesmo que a contribuição de cada componente individual seja pequena.

Além disso, é importante avaliar o desempenho do modelo **PCA** em um conjunto de teste independente para evitar *overfitting* [^384]. O erro de reconstrução no conjunto de teste pode ser usado como um critério para selecionar o número ideal de componentes.

### Conclusão
O *scree plot* e a análise do erro residual são ferramentas valiosas para auxiliar na escolha do número de dimensões latentes em **PCA**. Ao visualizar os autovalores e quantificar a variância explicada, é possível tomar decisões informadas sobre a complexidade do modelo e evitar *overfitting*. No entanto, é importante lembrar que a escolha final deve ser baseada em uma combinação de critérios teóricos e empíricos, levando em consideração o contexto do problema e o desempenho do modelo em dados não vistos.

### Referências
[^384]: Bishop, C. M. (2006b). *Pattern recognition and machine learning*. Springer.
[^387]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning*. Springer.
[^388]: Nabney, I. T. (2001). *Netlab: Algorithms for pattern recognition*. Springer.
[^390]: Shalizi, C. R. (2009). *Advanced data analysis from an elementary point of view*. Cambridge University Press.
[^399]: Hinton, G. E., Dayan, P., Frey, B. J., & Neal, R. M. (1997). The “wake-sleep” algorithm for unsupervised neural networks. *Science*, *276*(5318), 1521-1524.
[^400]: Jolliffe, I. T., & Cadima, J. (2016). Principal component analysis: a review and recent developments. *Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences*, *374*(2066), 20150202.
<!-- END -->