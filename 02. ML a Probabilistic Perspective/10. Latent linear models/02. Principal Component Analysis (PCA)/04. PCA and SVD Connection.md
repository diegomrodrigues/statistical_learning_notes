## Singular Value Decomposition (SVD) and its Connection to PCA

### Introdução
Este capítulo explora a relação fundamental entre a Análise de Componentes Principais (**PCA**) e a Decomposição em Valores Singulares (**SVD**). Enquanto a **PCA** é tradicionalmente definida em termos dos *eigenvectors* da matriz de covariância, a **SVD** oferece uma abordagem alternativa e computacionalmente eficiente para encontrar os componentes principais [^392]. A **SVD** generaliza o conceito de *eigenvectors* para matrizes não quadradas, tornando-a uma ferramenta poderosa para diversas aplicações em análise de dados e redução de dimensionalidade.

### Conceitos Fundamentais
A **SVD** é uma técnica de fatoração de matrizes que decompõe qualquer matriz real $X$ de dimensão $N \times D$ na forma [^392]:
$$ X = USV^T $$
onde:
*   $U$ é uma matriz ortonormal de dimensão $N \times N$ (i.e., $U^TU = I_N$).
*   $V$ é uma matriz ortonormal de dimensão $D \times D$ (i.e., $V^TV = V V^T = I_D$).
*   $S$ é uma matriz diagonal de dimensão $N \times D$ contendo os valores singulares $\sigma_i \geq 0$ na diagonal principal. O número de valores singulares não negativos é igual a $r = min(N, D)$, com o restante das entradas da matriz preenchidas com zeros [^392].

As colunas de $U$ são chamadas de *vetores singulares à esquerda*, e as colunas de $V$ são chamadas de *vetores singulares à direita* [^392].

**Conexão com Eigenvectors**

A ligação crucial entre **SVD** e **PCA** reside na relação entre os *eigenvectors* e os *vetores singulares*. Para uma matriz real arbitrária $X$, se $X = USV^T$, então:
*   Os *eigenvectors* de $X^TX$ são iguais a $V$, os *vetores singulares à direita* de $X$ [^392].
*   Os *eigenvalues* de $X^TX$ são iguais aos valores singulares ao quadrado, contidos na diagonal de $D = S^2$ [^392].

Formalmente [^392]:
$$(X^TX)V = VD$$
onde $D$ é uma matriz diagonal contendo os *eigenvalues* de $X^TX$. De forma similar:
$$U = evec(XX^T), V = evec(X^TX), S^2 = eval(XX^T) = eval(X^TX)$$
onde $evec$ denota os *eigenvectors* e $eval$ denota os *eigenvalues* [^392].

**SVD Truncada**

Em muitas aplicações, especialmente em redução de dimensionalidade, é útil usar uma **SVD** truncada. Dada a **SVD** de $X$, podemos aproximar $X$ por uma matriz de rank $L$ [^392]:
$$X \approx U_{:,1:L} S_{1:L, 1:L} V_{:,1:L}^T$$
Esta aproximação captura a maior parte da variância nos dados usando apenas os primeiros $L$ componentes principais. O erro de aproximação é dado por [^395]:
$$||X - X_L||_F \approx \sigma_{L+1}$$
onde $||.||_F$ denota a norma de Frobenius e $\sigma_{L+1}$ é o $(L+1)$-ésimo valor singular [^395].

A **SVD** truncada oferece a melhor aproximação de rank $L$ para uma matriz, no sentido de minimizar a norma de Frobenius do erro [^395]. O número total de parâmetros necessários para representar uma matriz $N \times D$ usando uma aproximação de rank $L$ é [^393]:
$$NL + LD + L = L(N + D + 1)$$

**PCA via SVD**

A conexão entre PCA e SVD é ainda mais profunda. Considerando que $X = USV^T$ seja a SVD truncada de $X$, e definindo $W = V$ e $Z = XW$, temos [^395]:
$$Z = USV^TV = US$$
A reconstrução ideal é dada por $\hat{X} = Z\hat{W}^T$. Assim, encontramos [^395]:
$$hat{X} = USV^T$$
Este resultado demonstra que PCA pode ser implementada usando SVD, fornecendo uma maneira computacionalmente eficiente de calcular os componentes principais [^395]. Além disso, a SVD oferece a melhor aproximação de baixa dimensão para os dados [^395].

### Conclusão
A **SVD** é uma ferramenta poderosa e versátil que oferece uma alternativa computacionalmente eficiente à **PCA** tradicional. Ao explorar a relação entre *eigenvectors* e *vetores singulares*, podemos entender como a **SVD** pode ser usada para realizar a **PCA** e obter aproximações de rank reduzido de dados. A capacidade de truncar a **SVD** e reter apenas os componentes mais importantes torna-a inestimável para tarefas de redução de dimensionalidade e remoção de ruído.

### Referências
[^392]: Seção 12.2.3
[^393]: Seção 12.2.3
[^395]: Seção 12.2.4
<!-- END -->