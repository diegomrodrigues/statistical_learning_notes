## Classical PCA: Minimizing Reconstruction Error

### Introdução
O presente capítulo aprofunda-se na análise de **Componentes Principais (PCA)**, uma técnica fundamental de redução de dimensionalidade. PCA busca representar dados em um espaço de menor dimensão, minimizando o erro de reconstrução [^1]. Este capítulo explora a formulação clássica do PCA, sua relação com a decomposição em valores singulares (SVD) e o algoritmo EM para PCA.

### Conceitos Fundamentais

**PCA Clássico: Formulação e Teorema**
O PCA clássico procura um conjunto ortogonal de *L* vetores de base linear $\hat{w} \in \mathbb{R}^D$ e *scores* correspondentes $z_i \in \mathbb{R}^L$ que minimizam o erro médio de reconstrução entre os pontos de dados originais $x_i$ e suas projeções no subespaço gerado pelos vetores de base [^1]. Matematicamente, o objetivo é minimizar a função de custo:

$$J(W, Z) = \frac{1}{N} \sum_{i=1}^{N} ||x_i - Wz_i||^2 \quad \text{[^1]}$$

onde *N* é o número de pontos de dados.

O teorema fundamental que define a síntese do PCA clássico é enunciado como se segue [^1]:

**Teorema 12.2.1:** Dado um conjunto de dados, o objetivo é encontrar um conjunto ortogonal de *L* vetores de base linear $w_j \in \mathbb{R}^D$ e os *scores* correspondentes $z_i \in \mathbb{R}^L$ que minimizem o erro médio de reconstrução [^1].

**Prova:**
A prova detalhada do Teorema 12.2.1 envolve a minimização da função de custo $J(w_1, z_1)$ em relação a $z_{i1}$, resultando em $z_{i1} = w_1^T x_i$ [^1]. Substituindo este resultado de volta na função de custo, obtém-se:

$$J(w_1) = \frac{1}{N} \sum_{i=1}^{N} x_i^T x_i - \frac{1}{N} \sum_{i=1}^{N} z_{i1}^2$$

Para minimizar $J(w_1)$, é necessário maximizar a variância dos dados projetados, o que leva à solução onde $w_1$ é o autovetor correspondente ao maior autovalor da matriz de covariância Σ [^1].

Para encontrar o próximo vetor $w_2$, impõe-se a condição de ortogonalidade $w_1^T w_2 = 0$ e repete-se o processo de minimização, garantindo que cada vetor subsequente capture a máxima variância restante nos dados [^1].

$\blacksquare$

**Padronização dos Dados**
É crucial padronizar os dados antes de aplicar o PCA [^1]. Isso ocorre porque o PCA é sensível à escala das variáveis; se uma variável tiver uma variância alta apenas devido à sua escala, ela pode dominar os componentes principais. A padronização garante que todas as variáveis contribuam igualmente para a análise, ou, equivalentemente, trabalhar com matrizes de correlação em vez de matrizes de covariância [^1].

**Decomposição em Valores Singulares (SVD)**
O PCA está intimamente relacionado com a Decomposição em Valores Singulares (SVD) [^1]. A SVD de uma matriz *X* (N × D) é dada por:

$$X = USV^T$$

onde *U* é uma matriz ortogonal N × N, *V* é uma matriz ortogonal D × D, e *S* é uma matriz diagonal N × D contendo os valores singulares de *X* [^1].

Os vetores de base do PCA correspondem aos vetores singulares à direita (colunas de *V*) da SVD de *X*, e os valores singulares estão relacionados aos autovalores da matriz de covariância Σ [^1].

**Algoritmo EM para PCA**
Embora o PCA possa ser resolvido usando métodos de autovetores ou SVD, o algoritmo EM (Expectation-Maximization) fornece uma alternativa [^1]. O algoritmo EM para PCA itera entre duas etapas [^1]:

1.  **Etapa E (Expectation):** Estimar os *scores* latentes *Z* dado os dados observados *X* e os vetores de base atuais *W* [^1]:

    $$Z = (W^T W)^{-1} W^T X$$

2.  **Etapa M (Maximization):** Estimar os vetores de base *W* dado os *scores* latentes estimados *Z* [^1]:

    $$W = XZ^T (ZZ^T)^{-1}$$

O algoritmo EM converge para a solução PCA, onde *W* abrange o mesmo subespaço linear que os vetores de base obtidos por métodos de autovetores ou SVD [^1].

### Conclusão

Este capítulo apresentou uma análise aprofundada do PCA clássico, demonstrando sua formulação matemática, sua relação com a SVD e o algoritmo EM para PCA. Além disso, foi apresentada a importância da padronização dos dados antes da aplicação do PCA e a conexão entre PCA e FA, onde o PCA pode ser visto como um caso especial de FA com restrições específicas [^1]. O PCA é uma ferramenta poderosa para redução de dimensionalidade e análise de dados, com aplicações em diversas áreas, desde visão computacional até bioinformática.

### Referências
[^1]: Trechos extraídos das páginas 381-391 do texto fornecido.
<!-- END -->