## Solução Ótima em PCA: Autovetores, Autovalores e Projeção Ortogonal

### Introdução
Este capítulo explora a solução ótima para a Análise de Componentes Principais (PCA), detalhando como essa solução é alcançada através da utilização de autovetores e autovalores da matriz de covariância empírica. A PCA, como visto na seção 12.2 [^19], é uma técnica para reduzir a dimensionalidade de um conjunto de dados, mantendo a maior parte da variância original. A solução ótima envolve a projeção ortogonal dos dados em um subespaço de menor dimensão, definido pelos autovetores correspondentes aos maiores autovalores. A figura 12.5(a) [^15] ilustra um exemplo de PCA em um espaço bidimensional sendo reduzido para uma dimensão.

### Conceitos Fundamentais
A solução para o problema de PCA é baseada na decomposição da matriz de covariância empírica $\Sigma$. O objetivo é encontrar uma matriz $W$ que minimize o erro de reconstrução dos dados originais. De acordo com o contexto, a solução ótima é obtida definindo $W$ como:

$$W = V_L$$

onde $V_L$ contém os $L$ autovetores correspondentes aos maiores autovalores da matriz de covariância empírica $\Sigma$ [^15]. Os autovetores representam as **componentes principais**, que são as direções no espaço de dados original que capturam a maior variância. Os autovalores, por sua vez, representam a quantidade de variância explicada por cada componente principal.

A matriz de covariância empírica $\Sigma$ é definida como:

$$\Sigma = \frac{1}{N} \sum_{i=1}^{N} x_i x_i^T$$

onde $x_i$ são os dados centrados (assumindo média zero para simplificação notacional) e $N$ é o número de amostras [^15].

A **codificação de baixa dimensionalidade** dos dados é dada por:

$$z_i = W^T x_i$$

Esta equação representa uma projeção ortogonal dos dados $x_i$ no espaço coluna gerado pelos autovetores em $W$ [^15]. Essa projeção transforma os dados do espaço original de dimensão $D$ para um espaço de dimensão $L$, onde $L < D$, preservando o máximo possível da variância.

**Prova da otimalidade:**

O objetivo da PCA é minimizar o erro de reconstrução, sujeito à restrição de que os vetores de base sejam ortonormais. O erro de reconstrução pode ser expresso como:

$$J(W, Z) = \frac{1}{N} \sum_{i=1}^{N} ||x_i - Wz_i||^2$$

onde $Z$ é a matriz de *scores* ou codificações de baixa dimensionalidade.

Para encontrar a solução ótima, podemos usar o método dos multiplicadores de Lagrange para incorporar a restrição de ortonormalidade. O Lagrangiano é:

$$L(W, Z, \Lambda) = J(W, Z) + \sum_{j=1}^{L} \lambda_j (w_j^T w_j - 1) + \sum_{j \neq k} \lambda_{jk} (w_j^T w_k)$$

onde $\lambda_j$ e $\lambda_{jk}$ são os multiplicadores de Lagrange.

Derivando em relação a $z_i$ e igualando a zero, obtemos:

$$\frac{\partial L}{\partial z_i} = -2W^T(x_i - Wz_i) = 0$$
$$W^T x_i = W^T W z_i$$

Como $W^T W = I$ (devido à ortonormalidade de $W$), temos:

$$z_i = W^T x_i$$

Derivando o Lagrangiano em relação a $W$ e igualando a zero, obtemos:

$$\frac{\partial L}{\partial W} = \frac{1}{N} \sum_{i=1}^{N} -2(x_i - Wz_i)z_i^T + 2W\Lambda = 0$$
$$\frac{1}{N} \sum_{i=1}^{N} x_i z_i^T = W(\frac{1}{N} \sum_{i=1}^{N} z_i z_i^T) - W\Lambda = 0$$

Substituindo $z_i = W^T x_i$, temos:

$$\Sigma W = W\Lambda$$

onde $\Sigma = \frac{1}{N} \sum_{i=1}^{N} x_i x_i^T$ é a matriz de covariância empírica.

Esta equação mostra que as colunas de $W$ são os autovetores de $\Sigma$, e os elementos diagonais de $\Lambda$ são os autovalores correspondentes. Para minimizar o erro de reconstrução, escolhemos os $L$ autovetores correspondentes aos maiores autovalores.  $\blacksquare$

### Conclusão
Em resumo, a solução ótima para PCA envolve a decomposição da matriz de covariância empírica em seus autovetores e autovalores. Os autovetores correspondentes aos maiores autovalores são selecionados para formar a matriz $W$, que é usada para projetar os dados originais em um espaço de menor dimensão. Essa projeção ortogonal preserva a maior parte da variância original, permitindo uma representação mais compacta e eficiente dos dados. Esta técnica é fundamental para diversas aplicações, como visualização de dados (Figura 12.2 [^3]), compressão e extração de características. A seção 12.2.3 [^12] explora a relação entre PCA e a decomposição em valores singulares (SVD), fornecendo uma perspectiva adicional sobre a implementação e interpretação da PCA.

### Referências
[^15]: Página 8, Parágrafo contendo: "The optimal solution is obtained by setting W = VL, where VL contains the L eigenvectors with largest eigenvalues of the empirical covariance matrix, ∑ = 1xx..."
[^19]: Página 7, Início da seção 12.2 Principal components analysis (PCA)
[^3]: Página 3, Figure 12.2 2D projection of 2004 cars data based on factor analysis
[^12]: Página 12, Início da seção 12.2.3 Singular value decomposition (SVD)
<!-- END -->