## Independent Component Analysis (ICA)

### Introdução
O presente capítulo explora a técnica de **Independent Component Analysis (ICA)**, uma ferramenta poderosa para decompor sinais multivariados em subcomponentes aditivos estatisticamente independentes [^407]. ICA encontra aplicações em diversas áreas, incluindo o famoso *cocktail party problem*, onde o objetivo é separar sinais de fala misturados [^407]. Em contraste com Factor Analysis (FA) e Principal Component Analysis (PCA), que se baseiam em distribuições Gaussianas, ICA relaxa essa suposição, permitindo a identificação única dos componentes [^408].

### Conceitos Fundamentais
**ICA** é uma técnica que visa separar um sinal multivariado em subcomponentes aditivos, assumindo a independência estatística mútua dos sinais de origem não-Gaussianos [^407]. O objetivo é encontrar uma representação onde os componentes sejam estatisticamente independentes, ou seja, não forneçam informações uns sobre os outros [^407].

**O Problema do Cocktail Party:** ICA é frequentemente utilizada para resolver o *cocktail party problem*, onde múltiplos sinais de fala são misturados e o objetivo é separá-los em seus componentes individuais [^407]. Nesse cenário, cada microfone atua como um sensor que captura uma combinação linear dos diferentes sinais de fala, e ICA é usada para desconvolver os sinais misturados [^407]. ICA aborda o *cocktail party problem* deconvolvendo sinais misturados em suas partes constituintes, assumindo que os sinais observados são combinações lineares de sinais de origem independentes [^407].

**Formalização do ICA:** Seja $x_t \in \mathbb{R}^D$ o sinal observado nos sensores no "tempo" $t$, e $z_t \in \mathbb{R}^L$ o vetor de sinais de origem [^407]. Assumimos que:

$x_t = Wz_t + \epsilon_t$ [^407]

onde $W$ é uma matriz de mistura $D \times L$ e $\epsilon_t \sim N(0, \Psi)$ representa o ruído [^408]. Nesta seção, tratamos cada ponto no tempo como uma observação independente, ou seja, não modelamos a correlação temporal [^408]. O objetivo é inferir os sinais de origem $p(z_t|x_t, \theta)$ [^408].

**Diferenças em relação ao PCA/FA:** Até aqui, o modelo é idêntico à análise fatorial (ou PCA se não houver ruído, exceto que geralmente não requeremos ortogonalidade de $W$) [^408]. No entanto, usaremos uma prior diferente para $p(z_t)$ [^408]. Em PCA, assumimos que cada fonte é independente e tem uma distribuição Gaussiana:

$p(z_t) = \prod_{j=1}^L N(z_{tj}|0,1)$ [^408]

Agora relaxaremos essa suposição Gaussiana e permitiremos que as distribuições da fonte sejam não-Gaussianas [^408].

**Não-Gaussianidade:** A razão pela qual a distribuição Gaussiana é proibida como um prior de origem em ICA é que ela não permite a recuperação única das fontes [^409]. Isso ocorre porque a verossimilhança de PCA é invariante a qualquer transformação ortogonal das fontes $z_t$ e da matriz de mistura $W$ [^409]. PCA pode recuperar o melhor subespaço linear no qual os sinais estão, mas não pode recuperar exclusivamente os próprios sinais [^409].

**Densidades de origem:**  Existem vários tipos de distribuições não-Gaussianas [^413]:
*   **Distribuições Super-Gaussianas:** São distribuições que têm um grande pico na média e, portanto (para garantir a variância unitária), têm caudas pesadas [^413]. Formalmente, dizemos que uma distribuição é super-Gaussiana ou leptocúrtica ("lepto" vindo do grego para "fino") se $kurt(z) > 0$, onde $kurt(z)$ é a curtose da distribuição, definida por
$$kurt(z) \triangleq \frac{\mu_4}{\sigma^4} - 3$$ [^413]
*   **Distribuições Sub-Gaussianas:** Uma distribuição sub-Gaussiana ou platicúrtica ("platy" vindo do grego para "largo") tem curtose negativa [^413].

### Conclusão
ICA oferece uma abordagem poderosa para separar sinais em componentes independentes, especialmente útil em aplicações como o *cocktail party problem*. Ao contrário do PCA e da análise fatorial, ICA não exige distribuições Gaussianas, permitindo a recuperação única de fontes não-Gaussianas. Os métodos para estimar a matriz de mistura $W$ e as distribuições de origem $p_j$ incluem estimativa de máxima verossimilhança e maximização da não-Gaussianidade [^410, 415]. O algoritmo FastICA oferece uma abordagem eficiente para estimar os componentes independentes, enquanto o uso de EM com misturas Gaussianas permite uma modelagem flexível das densidades de origem [^411, 414].

### Referências
[^407]: Chapter 12. Latent linear models
[^408]: Chapter 12. Latent linear models
[^409]: Chapter 12. Latent linear models
[^410]: Chapter 12. Latent linear models
[^411]: Chapter 12. Latent linear models
[^413]: Chapter 12. Latent linear models
[^414]: Chapter 12. Latent linear models
[^415]: Chapter 12. Latent linear models
<!-- END -->