{
  "topics": [
    {
      "topic": "Factor Analysis",
      "sub_topics": [
        "Factor Analysis (FA) is a latent variable model used for dimensionality reduction and identifying underlying relationships in data. It represents observed data using a smaller number of unobserved latent variables, or factors, allowing for a more flexible representation of data correlations than mixture models by using real-valued latent variables. The model assumes that observed data is generated from latent factors through a linear transformation, represented as p(x|z, \u03b8) = N(Wz + \u03bc, \u03a8), where W is the factor loading matrix, \u03bc is the mean, and \u03a8 is the covariance matrix. The factor loading matrix W captures the relationship between the latent factors and the observed variables, while \u03a8 represents the residual variance not explained by the factors. In factor analysis, the covariance matrix \u03a8 is often constrained to be diagonal, forcing the latent factors to explain the correlations in the data.",
        "Factor analysis can be interpreted as a low-rank parameterization of a multivariate normal (MVN) distribution, allowing for a joint density model on x using a small number of parameters. The induced marginal distribution p(x|\u03b8) is a Gaussian, given by p(x|\u03b8) = N(x|W\u03bc\u2080 + \u03bc, \u03a8 + W\u03a3\u2080W\u1d40), where the parameters are the factor loading matrix W, the mean \u03bc, and the covariance matrices \u03a8 and \u03a3. Factor analysis approximates the covariance matrix C of the visible vector x using a low-rank decomposition: C \u2248 WW\u1d40 + \u03a8. This decomposition captures the main sources of variation in the data using a smaller number of latent factors, effectively reducing the dimensionality of the data.",
        "Inference of latent factors involves computing the posterior distribution over the latent factors given the observed data, p(z|x, \u03b8) = N(z|m, \u03a3). This posterior distribution is used to estimate the values of the latent factors, which can reveal interesting information about the underlying structure of the data. The latent scores, m, represent the expected values of the latent factors given the observed data and can be used for visualization and interpretation. Using Bayes' rule for Gaussians, this posterior is also a Gaussian distribution with mean m\u2093 and covariance \u03a3\u2093, where \u03a3\u2093 = (\u03a3\u2080\u207b\u00b9 + W\u1d40\u03a8\u207b\u00b9W)\u207b\u00b9 and m\u2093 = \u03a3\u2093(W\u1d40\u03a8\u207b\u00b9(x - \u03bc) + \u03a3\u2080\u207b\u00b9\u03bc\u2080). Setting \u03bc\u2080 = 0 and \u03a3\u2080 = I simplifies the model without loss of generality, as these can be absorbed into the weight matrix W.",
        "Factor analysis models are unidentifiable because multiplying the factor loading matrix W by an orthogonal rotation matrix R (where RRT = I) does not change the likelihood of the model. This means that the latent factors are not uniquely determined, and there are infinitely many equivalent solutions. To ensure a unique solution, constraints are often imposed on W, such as forcing it to be orthonormal or lower triangular. Variations of factor analysis include imposing sparsity-promoting priors on the weights to encourage interpretable solutions, choosing informative rotation matrices to modify W, and using non-Gaussian priors for the latent factors to uniquely identify W, as in independent component analysis (ICA)."
      ]
    },
    {
      "topic": "Principal Component Analysis (PCA)",
      "sub_topics": [
        "Principal Component Analysis (PCA) is a dimensionality reduction technique that finds an orthogonal set of L linear basis vectors wj \u2208 \u211d\u1d30 that best represent the data in a lower-dimensional space, minimizing the average reconstruction error. PCA seeks to project the data onto a new coordinate system where the principal components capture the directions of maximum variance. Classical PCA seeks an orthogonal set of L linear basis vectors w\u0302 \u2208 \u211d\u1d30 and corresponding scores z\u1d62 \u2208 \u211d\u1d38 that minimize the average reconstruction error between the original data points x\u1d62 and their projections onto the subspace spanned by the basis vectors, expressed as J(W, Z) = (1/N) \u03a3\u1d62 ||x\u1d62 - Wz\u1d62||\u00b2. PCA can be misleading if the variance is high merely because of the measurement scale, so it is standard practice to standardize the data first, or equivalently, to work with correlation matrices instead of covariance matrices.",
        "The optimal solution to PCA is obtained by setting W equal to VL, where VL contains the L eigenvectors with the largest eigenvalues of the empirical covariance matrix \u03a3. The eigenvectors represent the principal components, and the eigenvalues represent the amount of variance explained by each component. The optimal low-dimensional encoding of the data is given by zi = WTxi, which is an orthogonal projection of the data onto the column space spanned by the eigenvectors. This projection transforms the data from the original D-dimensional space to an L-dimensional space, where L < D, while preserving as much of the variance as possible.",
        "PCA can be interpreted as finding the directions of maximal variance in the data, which is known as the analysis view of PCA. The variance of the projected data is given by (1/N) \u03a3\u1d62 (WTxi)\u00b2 = WT \u03a3 W, where \u03a3 is the empirical covariance matrix. Maximizing this variance subject to the constraint that W is orthonormal leads to the eigenvectors of \u03a3 as the optimal solution. The principal directions in PCA are the ones along which the data shows maximal variance, and the variance of the projected data can be written as (1/N) \u03a3\u1d62 (w\u1d40 x\u1d62)\u00b2 = w\u1d40 \u03a3 w, where \u03a3 is the empirical covariance matrix.",
        "The connection between PCA and Singular Value Decomposition (SVD) is that the eigenvectors of the covariance matrix are equal to the right singular vectors of the data matrix X. This means that PCA can be performed using SVD, which is a computationally efficient algorithm for finding the principal components. Any real N \u00d7 D matrix X can be decomposed as X = USV\u1d40, where U and V are orthonormal matrices and S is a diagonal matrix of singular values. The connection between eigenvectors and singular vectors is that the eigenvectors of X\u1d40X are equal to V, the right singular vectors of X, and the eigenvalues of X\u1d40X are equal to the squared singular values. A truncated SVD can be used to produce a rank L approximation of the data matrix, which is useful for dimensionality reduction and noise reduction.",
        "Probabilistic PCA (PPCA) is a variant of PCA that assumes a factor analysis model with a specific noise structure, where the covariance matrix is the sum of a low-rank matrix and a diagonal matrix, and the principal components are the eigenvectors of the sample covariance matrix. In PPCA, the covariance matrix \u03a8 is set to \u03c3\u00b2I, and the factor loading matrix W is constrained to be orthonormal. As \u03c3\u00b2 approaches 0, PPCA reduces to classical (non-probabilistic) PCA. The maxima of the log-likelihood in PPCA are given by W = V(\u039b - \u03c3\u00b2I)R, where V is the matrix of eigenvectors of S, \u039b is the corresponding diagonal matrix of eigenvalues, and R is an arbitrary orthogonal matrix, and \u03c3\u00b2 is the average variance associated with the discarded dimensions.",
        "The EM algorithm can be used to fit a PCA model, which has advantages such as being able to handle missing data and being implementable in an online fashion, where the estimate of the principal components can be updated as the data streams in. The EM algorithm iteratively estimates the latent variables (E step) and the model parameters (M step) until convergence. The E step in EM for PCA involves computing the posterior means (low-dimensional representations) of the latent variables, given by Z\u0303 = (WTW)\u207b\u00b9WTX, which is an orthogonal projection of the data, and the M step is given by W = X Z\u0303\u1d40 (Z\u0303 Z\u0303\u1d40)\u207b\u00b9, which is similar to multi-output linear regression. The EM algorithm converges to a solution where W spans the same linear subspace as that defined by the first L eigenvectors, providing a globally optimal solution for PCA.",
        "Model selection in PCA involves choosing the appropriate number of latent dimensions L. Since PCA is not a probabilistic model, methods based on marginal likelihood are not directly applicable. Instead, one can use the reconstruction error or the fraction of variance explained as a proxy for the likelihood to guide model selection. The residual error from only using L terms in PCA is given by the sum of the discarded eigenvalues: E(Dtrain, L) = \u03a3\u2c7c=L+1\u1d30 \u03bb\u2c7c, and an alternative to plotting the error is to plot the retained eigenvalues in decreasing order, called a scree plot."
      ]
    },
    {
      "topic": "Independent Component Analysis (ICA)",
      "sub_topics": [
        "Independent Component Analysis (ICA) is a technique used to separate a multivariate signal into additive subcomponents assuming the mutual statistical independence of the non-Gaussian source signals. The goal is to find a representation where the components are statistically independent, meaning that they do not provide information about each other. ICA is often used to solve the cocktail party problem, where multiple speech signals are mixed together and the goal is to separate them into their individual components. In this scenario, each microphone acts as a sensor that captures a linear combination of the different speech signals, and ICA is used to deconvolve the mixed signals. ICA addresses the cocktail party problem by deconvolving mixed signals into their constituent parts, assuming the observed signals are linear combinations of independent source signals.",
        "The ICA model assumes that the observed signal xt is a linear combination of the source signals zt, given by the equation xt = Wzt + et, where W is the mixing matrix and et is the noise. The goal is to estimate the mixing matrix W and the source signals zt from the observed data xt. In ICA, the goal is to infer the source signals, p(zt|xt, \u03b8), and a different prior for p(zt) is used, where each source is independent and has a non-Gaussian distribution, defined as p(zt) = \u220fj=1\u1d38 pj(ztj). For simplicity, it is often assumed that the noise level is zero.",
        "Unlike PCA, ICA does not assume that the source signals are uncorrelated, but rather that they are statistically independent. This means that the joint probability distribution of the source signals can be factored into the product of their marginal distributions, i.e., p(zt) = \u220fj pj(ztj). ICA requires that the source distributions be non-Gaussian, because the Gaussian distribution is invariant to orthogonal transformations, which means that it is not possible to uniquely recover the sources if they are Gaussian. The non-Gaussianity of the source distributions is what allows ICA to separate the mixed signals. The reason the Gaussian distribution is disallowed as a source prior in ICA is that it does not permit unique recovery of the sources, as the PCA likelihood is invariant to any orthogonal transformation of the sources zt and mixing matrix W.",
        "Various ways of estimating the source densities in ICA, such as using super-Gaussian distributions (e.g., Laplace distribution) or sub-Gaussian distributions (e.g., uniform distribution). The choice of source distribution depends on the characteristics of the data and the application. Super-Gaussian distributions, such as the Laplace distribution, are distributions which have a big spike at the mean and heavy tails, and are often used as signal priors in ICA. An alternative to assuming a particular form for the source distributions is to use a flexible non-parametric density estimator, such as a mixture of (uni-variate) Gaussians. This approach allows ICA to adapt to the data and estimate the source distributions without making strong assumptions about their shape.",
        "Maximum likelihood estimation in ICA involves finding a square mixing matrix that is orthogonal, which reduces the number of parameters to estimate and simplifies the math and the algorithms. The log-likelihood can be written as (1/T) log p(D|V) = log |det(V)| + (1/T) \u03a3\u0302c \u03a3\u2099 log p\u0302c(v\u0302c\u1d40x\u2099), where V = W\u207b\u00b9 are the recognition weights. In ICA, we constrain the variance of the source distributions to be 1, and the resulting model is known as independent component analysis or ICA.",
        "The FastICA algorithm is a popular method for estimating the mixing matrix W in ICA. It is an approximate Newton method that iteratively updates the weight vectors until convergence. The algorithm is based on maximizing the non-Gaussianity of the source signals, which is measured by the negentropy. The FastICA algorithm involves iteratively updating the vector v using v* \u2190 E[x g(v\u1d40 x)] - E[g'(v\u1d40 x)] v and projecting back onto the constraint surface using vnew \u2190 v*/||v*||. Alternative estimation principles for ICA include maximizing non-Gaussianity, which involves finding a matrix V such that the distribution z = Vx is as far from Gaussian as possible, and minimizing mutual information, which involves finding independent components by minimizing the multi-information I(z) = \u03a3H(z\u0302c) - H(z). The infomax principle provides another perspective on ICA, framing it as maximizing the information flow through a neural network. By maximizing the mutual information between the input and output of a nonlinear transformation, infomax aims to extract independent components that capture the most relevant information in the data."
      ]
    },
    {
      "topic": "Unidentifiability in Factor Analysis",
      "sub_topics": [
        "Factor analysis models are unidentifiable because multiplying the factor loading matrix (W) by an arbitrary orthogonal rotation matrix (R) does not change the likelihood, meaning there are multiple equivalent parameterizations that fit the data equally well. To ensure a unique solution in FA, constraints such as removing L(L-1)/2 degrees of freedom are necessary, where L is the number of latent factors, to eliminate rotational ambiguity and identify a specific, interpretable factor structure.",
        "Forcing the factor loading matrix (W) to be orthonormal provides a clean solution to the identifiability problem by ensuring that the latent factors are uncorrelated and have unit variance, which is commonly adopted by PCA.",
        "Another method to achieve identifiability is to force W to be lower triangular, which ensures that each observed variable is only influenced by a subset of the latent factors, creating a hierarchical structure that simplifies interpretation and parameter estimation. The lower triangular approach involves ensuring the first visible feature is only generated by the first latent factor, the second by the first two, and so on, with the constraint wjj > 0 for j = 1 : L.",
        "Sparsity-promoting priors on the weights encourage many elements of the factor loading matrix (W) to be zero, simplifying the model and improving interpretability by selecting a subset of latent factors that strongly influence each observed variable.",
        "Choosing an informative rotation matrix involves applying heuristic methods like varimax to transform W, aiming to maximize the variance of the squared loadings and thereby enhance the interpretability of the latent factors.",
        "Using non-Gaussian priors for the latent factors, such as in Independent Component Analysis (ICA), can enable unique identification of W and the latent factors, as non-Gaussianity provides additional information to separate independent sources."
      ]
    },
    {
      "topic": "Probabilistic PCA",
      "sub_topics": [
        "Probabilistic PCA (PPCA) is a factor analysis model with an isotropic error covariance matrix (\u03a8 = \u03c3\u00b2I) and an orthogonal factor loading matrix (W), linking it to classical PCA. As the noise variance \u03c3\u00b2 approaches zero, PPCA converges to classical PCA, with the factor loading matrix W approaching the matrix of eigenvectors V.",
        "The observed data log likelihood in PPCA can be expressed in terms of the covariance matrix C = WWT + \u03c3\u00b2I and the empirical covariance matrix S, enabling parameter estimation via maximum likelihood. The log likelihood of the observed data in PPCA is given by log p(X|W, \u03c3\u00b2) = -(N/2) [log |C| + tr(C\u207b\u00b9S)], where C = WWT + \u03c3\u00b2I and S = (1/N)X\u1d40X.",
        "The maxima of the log-likelihood in PPCA are given by W = V(\u039b - \u03c3\u00b2I)R, where V is the matrix of eigenvectors of S, \u039b is the corresponding diagonal matrix of eigenvalues, and R is an arbitrary orthogonal matrix, and \u03c3\u00b2 is the average variance associated with the discarded dimensions. In PPCA, the MLE (Maximum Likelihood Estimate) of W is given by \u0174 = V(\u039b - \u03c3\u00b2I)\u00b9/\u00b2, where V is the matrix of the first L eigenvectors of S, and \u039b is the corresponding diagonal matrix of eigenvalues. The MLE of the noise variance is given by \u03c3\u00b2 = (1/(D-L)) \u03a3\u2c7c=L+1 \ud835\udf06\u2c7c, which is the average variance associated with the discarded dimensions.",
        "The posterior over the latent factors in PPCA is given by a Gaussian distribution with mean F\u207b\u00b9W\u1d40xi and covariance \u03c3\u00b2F\u207b\u00b9, where F = W\u1d40W + \u03c3\u00b2I, allowing for probabilistic inference of latent variables."
      ]
    },
    {
      "topic": "EM Algorithm for PCA",
      "sub_topics": [
        "The EM algorithm provides an iterative method to fit a PCA model, even in the zero noise limit, \u03c3\u00b2 = 0. The EM algorithm for PCA relies on the probabilistic formulation of PCA and continues to work in the zero noise limit, \u03c3\u00b2 = 0, by iteratively updating the posterior means (low-dimensional representations) and the weight matrix.",
        "The E step in EM for PCA involves computing the posterior means (low-dimensional representations) of the latent variables, given by Z\u0303 = (WTW)\u207b\u00b9WTX, which is an orthogonal projection of the data. In PPCA, the E step involves computing Z\u0303 = (W\u1d40W)\u207b\u00b9W\u1d40X, which is an orthogonal projection of the data.",
        "The M step involves updating the factor loading matrix W using W = X Z\u0303\u1d40 (Z\u0303 Z\u0303\u1d40)\u207b\u00b9, which is similar to multi-output linear regression where the observed inputs are replaced by the expected values of the latent variables. The M step is given by W = X Z\u0303\u1d40 (Z\u0303 Z\u0303\u1d40)\u207b\u00b9.",
        "The EM algorithm converges to a solution where W spans the same linear subspace as that defined by the first L eigenvectors, providing a globally optimal solution for PCA. In the EM algorithm for PCA, the only stable fixed point is the globally optimal solution, where W spans the same linear subspace as that defined by the first L eigenvectors.",
        "This algorithm has a simple physical analogy in the case D = 2 and L = 1, where some points in R2 are attached by springs to a rigid rod, and the E and M steps iteratively adjust the attachment points and rod orientation to minimize the spring energy."
      ]
    },
    {
      "topic": "Choosing the Number of Latent Dimensions",
      "sub_topics": [
        "Model selection for FA/PPCA involves computing L* = argmaxL p(L|D), but evaluating the marginal likelihood for latent variable models (LVMs) is quite difficult, and simple approximations such as BIC or variational lower bounds can be used.",
        "Since PCA is not a probabilistic model, an obvious proxy for the likelihood is the reconstruction error, defined as E(D, L) = (1/|D|) \u03a3\u1d62 \u2208 D ||x\u1d62 - x\u0302\u1d62||\u00b2, where x\u0302\u1d62 = W zi + \u03bc and zi = WT (x\u1d62 - \u03bc).",
        "The residual error from only using L terms in PCA is given by the sum of the discarded eigenvalues: E(Dtrain, L) = \u03a3\u2c7c=L+1D \u03bb\u2c7c, and an alternative to plotting the error is to plot the retained eigenvalues in decreasing order, called a scree plot.",
        "The profile likelihood method is used to automate the detection of a regime change in the plots by partitioning the values into two groups, depending on whether k \u2264 L or k > L, and measuring the quality of L using a simple change-point model, where \u03bbk \u223c N(\u03bc\u2081, \u03c3\u00b2) if k \u2264 L and \u03bbk \u223c N(\u03bc\u2082, \u03c3\u00b2) if k > L."
      ]
    }
  ]
}