## Factor Analysis como Parametrização de Baixo Rank de uma MVN

### Introdução
O modelo de **Factor Analysis (FA)** pode ser interpretado como uma forma de especificar um modelo de densidade conjunta em **x** utilizando um número reduzido de parâmetros [^1]. Especificamente, o FA permite uma parametrização de baixo rank da distribuição normal multivariada (MVN), o que possibilita a modelagem da densidade conjunta de **x** de forma eficiente [^1]. Este capítulo detalha como o FA induz uma distribuição marginal Gaussiana e como essa formulação se relaciona com a aproximação da matriz de covariância.

### Conceitos Fundamentais

#### Distribuição Marginal Induzida
No Factor Analysis, a distribuição marginal induzida *p(x|θ)* é uma Gaussiana, dada por [^1]:
$$
p(x|\theta) = \mathcal{N}(x|W\mu_0 + \mu, \Psi + W\Sigma_0W^T)
$$
onde:
*   **W** é a *matriz de loadings fatoriais* (factor loading matrix), de dimensão *D x L*, que relaciona as variáveis latentes **z** com as observações **x** [^1, 2].
*   **μ** é o vetor de médias [^1].
*   **Ψ** é a matriz de covariância, geralmente diagonal, de dimensão *D x D* [^1, 2].
*   **Σ₀** é a matriz de covariância da variável latente *zᵢ* [^1]. Mais especificamente, *p(zᵢ) = N(zᵢ|μ₀, Σ₀)* [^1].

Essa formulação permite que o modelo FA capture as principais fontes de variação nos dados utilizando um número menor de fatores latentes, reduzindo efetivamente a dimensionalidade dos dados [^1].

#### Aproximação da Matriz de Covariância
O Factor Analysis aproxima a matriz de covariância **C** do vetor visível **x** utilizando uma decomposição de baixo rank [^1]:
$$
C \approx WW^T + \Psi
$$
Essa decomposição captura as principais fontes de variação nos dados usando um número menor de fatores latentes, reduzindo efetivamente a dimensionalidade dos dados [^1].

Para garantir a interpretabilidade, a matriz **Ψ** é geralmente considerada diagonal [^2]. Isso força as variáveis latentes **zᵢ** a explicar a correlação entre as variáveis observadas, em vez de "incorporar" essa correlação na covariância das observações [^2].

#### Parametrização e Flexibilidade
O modelo FA utiliza *O(LD)* parâmetros [^2]. Isso oferece um compromisso flexível entre uma Gaussiana de covariância completa, que usa *O(D²)* parâmetros, e uma covariância diagonal, que usa *O(D)* parâmetros [^2].

É importante notar que, se **Ψ** não fosse restrita a ser diagonal, poderíamos trivialmente defini-la como uma matriz de covariância completa e definir **W = 0**, caso em que os fatores latentes não seriam necessários [^2].

#### Relação com PPCA
O caso especial em que **Ψ = σ²I** é chamado de *probabilistic principal component analysis (PPCA)* [^2].

#### Identificabilidade

Similarmente aos modelos de mistura, o FA também sofre de problemas de não-identificabilidade [^3]. Se **R** é uma matriz de rotação ortogonal arbitrária (ou seja, **RRT = I**), então definir **W = WR** resulta na mesma função de verossimilhança [^3]:

$$
Cov[x] = WE[zz^T]W^T + E[\epsilon \epsilon^T] = WRR^TW^T + \Psi = WW^T + \Psi
$$

Geometricamente, multiplicar **W** por uma matriz ortogonal é como rotacionar **z** antes de gerar **x** [^3]. No entanto, como **z** é amostrado de uma Gaussiana isotrópica, isso não altera a verossimilhança [^3]. Consequentemente, não podemos identificar unicamente **W**, e, portanto, não podemos identificar unicamente os fatores latentes [^3].

Para garantir uma solução única, precisamos remover *L(L – 1)/2* graus de liberdade, pois esse é o número de matrizes ortonormais de tamanho *L x L* [^3]. No total, o modelo FA tem *D + LD – L(L-1)/2* parâmetros livres (excluindo a média), onde o primeiro termo surge de **Ψ** [^3]. Obviamente, exigimos que isso seja menor ou igual a *D(D + 1)/2*, que é o número de parâmetros em uma matriz de covariância não restrita (mas simétrica) [^3]. Isso nos dá um limite superior em *L*, como segue [^3]:

$$
L_{max} = \lfloor D + 0.5(1 - \sqrt{1 + 8D}) \rfloor
$$

Por exemplo, *D = 6* implica *L ≤ 3* [^3]. Mas geralmente nunca escolhemos esse limite superior, pois isso resultaria em overfitting (veja a discussão na Seção 12.3 sobre como escolher *L*) [^3].

### Conclusão
O Factor Analysis oferece uma abordagem flexível e eficiente para modelar a estrutura de covariância de dados multivariados através de uma parametrização de baixo rank. A capacidade de aproximar a matriz de covariância com um número reduzido de fatores latentes torna o FA uma ferramenta valiosa para redução de dimensionalidade e análise exploratória de dados. A compreensão das propriedades da distribuição marginal induzida e das considerações de identificabilidade é crucial para a aplicação correta e interpretação dos resultados do modelo FA.

### Referências
[^1]: Page 381 "FA can be thought of as a way of specifying a joint density model on x using a small number of parameters. To see this, note that from Equation 4.126, the induced marginal distribution p(x0) is a Gaussian: ..."
[^2]: Page 381 "where W is a D × L matrix, known as the factor loading matrix, and Ψ is a D × D covariance matrix. We take Ý to be diagonal, since the whole point of the model is to "force" zi to explain the correlation, rather than "baking it in” to the observation\'s covariance. This overall model is called factor analysis or FA. The special case in which Ψ = σ²I is called probabilistic principal components analysis or PPCA."
[^3]: Page 383 "Just like with mixture models, FA is also unidentifiable. To see this, suppose R is an arbitrary orthogonal rotation matrix, satisfying RRT = I. Let us define W = WR; then the likelihood ..."
<!-- END -->