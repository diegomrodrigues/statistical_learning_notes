## Análise Fatorial: Modelo, Identificabilidade e Inferência

### Introdução
A Análise Fatorial (FA) é um modelo de variável latente empregado para a redução de dimensionalidade e identificação de relações subjacentes em dados [^1]. Diferentemente dos modelos de mistura, que utilizam variáveis latentes discretas para gerar observações, a FA emprega variáveis latentes contínuas, oferecendo uma representação mais flexível das correlações nos dados [^1]. Este capítulo explora em detalhes o modelo de FA, suas propriedades, a questão da identificabilidade e os métodos de inferência associados.

### Conceitos Fundamentais

O modelo de FA assume que os dados observados são gerados a partir de fatores latentes através de uma transformação linear [^1]. Matematicamente, essa relação é expressa como:

$$
p(x|z, \theta) = N(Wz + \mu, \Psi)
$$

onde:
*   $x$ representa os dados observados.
*   $z$ representa os fatores latentes.
*   $W$ é a **matriz de carregamento fatorial**, que captura a relação entre os fatores latentes e as variáveis observadas [^1]. É uma matriz de dimensão $D \times L$, onde $D$ é a dimensão dos dados observados e $L$ é a dimensão do espaço latente [^1].
*   $\mu$ é o vetor de médias [^1].
*   $\Psi$ é a **matriz de covariância** [^1]. Frequentemente, em FA, $\Psi$ é restringida a ser diagonal, forçando os fatores latentes a explicar as correlações nos dados [^1]. Essa restrição é crucial para a interpretabilidade do modelo [^1].

A **distribuição marginal induzida** $p(x|\theta)$ é uma Gaussiana, dada por [^1]:

$$
p(x|\theta) = \int N(x|Wz + \mu, \Psi)N(z|\mu_0, \Sigma_0) dz = N(x|W\mu_0 + \mu, \Psi + W\Sigma_0W^T)
$$

Tipicamente, assume-se uma **prior Gaussiana** para os fatores latentes $z$, ou seja, $p(z) = N(z|\mu_0, \Sigma_0)$ [^1]. Usualmente, $\mu_0 = 0$ e $\Sigma_0 = I$ são utilizados sem perda de generalidade [^1].

A FA pode ser vista como uma forma de especificar um modelo de densidade conjunta em $x$ usando um pequeno número de parâmetros [^1]. Uma análise da matriz de covariância revela que:

$$
C \approx cov[x] = WW^T + \Psi
$$

onde $C$ é a **matriz de covariância** dos dados observados [^1]. Isso significa que a FA aproxima a matriz de covariância do vetor visível usando uma decomposição de baixo rank [^1].

O modelo de FA utiliza $O(LD)$ parâmetros, oferecendo um compromisso flexível entre uma Gaussiana de covariância completa, que usa $O(D^2)$ parâmetros, e uma covariância diagonal, que usa $O(D)$ parâmetros [^1].

**Inferência dos fatores latentes:**

A inferência dos fatores latentes é um aspecto crucial da FA, pois permite descobrir informações interessantes sobre os dados [^1]. Para isso, é necessário calcular a distribuição *a posteriori* dos fatores latentes, $p(z|x, \theta)$. Usando a regra de Bayes para Gaussianas, obtém-se [^2]:

$$
p(z|x, \theta) = N(z|m, \Sigma)
$$

onde:

$$
\Sigma = (\Sigma_0^{-1} + W^T\Psi^{-1}W)^{-1}
$$

$$
m = \Sigma(W^T\Psi^{-1}(x - \mu) + \Sigma_0^{-1}\mu_0)
$$

No modelo de FA, $\Sigma$ é independente de $i$, podendo ser denotada simplesmente por $\Sigma$ [^2]. O cálculo de $\Sigma$ leva tempo $O(L^3 + L^2D)$, enquanto o cálculo de $m = E[z|x, \theta]$ leva tempo $O(L^2 + LD)$ [^2]. Os $m_i$ são chamados de *latent scores* ou *latent factors* [^2].

### Unidentifiability

Assim como os modelos de mistura, a FA também sofre de **unidentifiability** [^3]. Se $R$ é uma matriz de rotação ortogonal (i.e., $RR^T = I$), então definindo $W' = WR$, a função de verossimilhança permanece a mesma [^3]:

$$
cov[x] = WE[zz^T]W^T + \Psi = WRR^TW^T + \Psi = WW^T + \Psi
$$

Geometricamente, multiplicar $W$ por uma matriz ortogonal é como rotacionar $z$ antes de gerar $x$ [^4]. Como $z$ é extraído de uma Gaussiana isotrópica, isso não altera a verossimilhança [^4]. Consequentemente, não é possível identificar unicamente $W$, e, portanto, os fatores latentes [^4].

Para garantir uma solução única, é necessário remover $L(L-1)/2$ graus de liberdade, que é o número de matrizes ortonormais de tamanho $L \times L$ [^4]. Em total, o modelo de FA tem $D + LD - L(L-1)/2$ parâmetros livres (excluindo a média), onde o primeiro termo vem de $\Psi$ [^4]. Obviamente, requer-se que isso seja menor ou igual a $D(D+1)/2$, que é o número de parâmetros em uma matriz de covariância não restrita (mas simétrica) [^4]. Isso dá um limite superior em $L$, como segue [^4]:

$$
L_{max} = \lfloor D + 0.5(1 - \sqrt{1 + 8D}) \rfloor
$$

Mesmo se $L < L_{max}$, ainda não é possível identificar unicamente os parâmetros, pois a ambiguidade rotacional ainda existe [^4]. A não-identificabilidade não afeta o desempenho preditivo do modelo, mas afeta a matriz de carregamento, e portanto, a interpretação dos fatores latentes [^4].

**Soluções para a unidentifiability:**
*   **Forçar $W$ a ser ortonormal:** Ordenar as colunas por variância decrescente [^4]. É a abordagem adotada por PCA. O resultado não é necessariamente mais interpretável, mas é único [^4].
*   **Forçar $W$ a ser triangular inferior:** Garantir que a primeira característica visível seja gerada apenas pelo primeiro fator latente, a segunda característica visível seja gerada apenas pelos dois primeiros fatores latentes, e assim por diante [^4]. Por exemplo, se $L = 3$ e $D = 4$, a matriz de carregamento fatorial correspondente é dada por [^4]:

$$
W = \begin{pmatrix}
w_{11} & 0 & 0 \\
w_{21} & w_{22} & 0 \\
w_{31} & w_{32} & w_{33} \\
w_{41} & w_{42} & w_{43}
\end{pmatrix}
$$

Também requer-se que $w_{jj} > 0$ para $j = 1:L$ [^4]. O número total de parâmetros nesta matriz restrita é $D + DL - L(L-1)/2$, que é igual ao número de parâmetros unicamente identificáveis [^4]. A desvantagem é que as primeiras $L$ variáveis visíveis, conhecidas como *founder variables*, afetam a interpretação dos fatores latentes, e portanto, devem ser escolhidas cuidadosamente [^5].

Outras abordagens incluem [^5]:
*   **Sparsity promoting priors on the weights:** Incentivar que as entradas em $W$ sejam zero, usando regularização $l_1$, ARD ou priors spike-and-slab.
*   **Choosing an informative rotation matrix:** Métodos heurísticos para encontrar matrizes de rotação $R$ que podem ser usadas para modificar $W$ (e, portanto, os fatores latentes) de modo a aumentar a interpretabilidade, normalmente incentivando que sejam (aproximadamente) esparsos. Um método popular é conhecido como *varimax*.
*   **Use of non-Gaussian priors for the latent factors:** Substituir $p(z_i)$ por uma distribuição não-Gaussiana pode permitir identificar unicamente $W$. Esta técnica é conhecida como ICA.

### Conclusão
A Análise Fatorial oferece uma abordagem poderosa para modelar e entender a estrutura subjacente em dados multivariados. Ao representar os dados observados em termos de um conjunto menor de fatores latentes, a FA facilita a redução da dimensionalidade e a identificação de relações significativas. No entanto, é crucial estar ciente da questão da unidentifiability e empregar técnicas adequadas para mitigar seus efeitos, garantindo a interpretabilidade e a validade das conclusões obtidas a partir do modelo.
<!-- END -->