## Sparsity-Promoting Priors in Factor Analysis

### Introdução
No contexto da análise fatorial (FA), a interpretabilidade dos fatores latentes é fundamental para descobrir a estrutura subjacente nos dados [^3]. Conforme mencionado anteriormente, a análise fatorial é inerentemente não identificável devido à ambiguidade rotacional [^3]. Para mitigar essa não identificabilidade e melhorar a interpretabilidade, podemos empregar **priors que promovem a esparsidade** nas matrizes de pesos. Este capítulo explorará essa abordagem em detalhes.

### Conceitos Fundamentais
A ideia central é que a maioria das variáveis observadas é influenciada apenas por um subconjunto dos fatores latentes. Ao impor esparsidade na matriz de carregamento de fatores (**W**), podemos identificar quais fatores latentes influenciam fortemente cada variável observada, simplificando o modelo e tornando-o mais interpretável [^5]. Existem várias maneiras de implementar priors que promovem a esparsidade:

1.  ***l₁ regularization***: Adicionar um termo de regularização $l_1$ à função objetivo incentiva que muitos elementos de **W** sejam exatamente zero. Isso é matematicamente expresso como:
    $$     \text{minimize} \quad \mathcal{L}(W) + \lambda \sum_{i,j} |W_{ij}|     $$
    onde $\mathcal{L}(W)$ é a função de perda, e $\lambda$ é o parâmetro de regularização que controla a força da penalidade $l_1$ [^5]. Esta técnica é discutida em (Zou et al. 2006) [^5].

2.  ***Automatic Relevance Determination (ARD)***: ARD usa priors Gaussianos hierárquicos onde cada elemento $W_{ij}$ tem sua própria variância que é também uma variável aleatória. Essas variâncias são controladas por hiperparâmetros, que são aprendidos a partir dos dados. Se um hiperparâmetro se torna pequeno, a variância correspondente de $W_{ij}$ também se torna pequena, efetivamente "desligando" a conexão entre o fator latente *j* e a variável observada *i* [^5].  Este método é discutido em (Bishop 1999; Archambeau and Bach 2008) [^5].

3.  ***Spike-and-Slab Priors***: Este método utiliza uma mistura de duas distribuições: um "spike" em zero (representado por uma função delta de Dirac ou uma Gaussiana com variância muito pequena) e um "slab" (uma distribuição mais ampla, como uma Gaussiana). Isso força os elementos de **W** a serem exatamente zero (se amostrados do "spike") ou a terem um valor diferente de zero (se amostrados do "slab") [^5]. Esta técnica é discutida em (Rattray et al. 2009) [^5].

É importante notar que, embora esses priors promovam a esparsidade, eles não garantem necessariamente uma estimativa MAP única [^5]. No entanto, eles incentivam soluções interpretáveis, o que é crucial para a descoberta de estrutura nos dados [^5].

### Conclusão
A imposição de priors que promovem a esparsidade em **W** é uma técnica valiosa para lidar com a não identificabilidade e melhorar a interpretabilidade em análise fatorial. Ao selecionar um subconjunto de fatores latentes que influenciam fortemente cada variável observada, esses priors simplificam o modelo e facilitam a descoberta de estruturas subjacentes nos dados. A escolha do prior específico (regularização $l_1$, ARD ou priors spike-and-slab) depende das características do conjunto de dados e dos objetivos da análise [^5].

### Referências
[^3]: "Just like with mixture models, FA is also unidentifiable. To see this, suppose R is an arbitrary orthogonal rotation matrix, satisfying RRT = I. Let us define W = WR; then the likelihood"
[^5]: "Sparsity promoting priors on the weights Instead of pre-specifying which entries in W are zero, we can encourage the entries to be zero, using l₁ regularization (Zou et al. 2006), ARD (Bishop 1999; Archambeau and Bach 2008), or spike-and-slab priors (Rattray et al. 2009). This is called sparse factor analysis. This does not necessarily ensure a unique MAP estimate, but it does encourage interpretable solutions. See Section 13.8."
<!-- END -->