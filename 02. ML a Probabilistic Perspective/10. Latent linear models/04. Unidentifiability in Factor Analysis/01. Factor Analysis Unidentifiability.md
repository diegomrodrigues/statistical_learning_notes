## Unidentifiability and Solutions in Factor Analysis

### Introdução
O modelo de **Factor Analysis (FA)**, como muitos outros modelos estatísticos, enfrenta o problema da **unidentifiability**, onde múltiplas parametrizações podem ajustar os dados igualmente bem [^3]. Em particular, a **matriz de loadings fatoriais** (*factor loading matrix*) $W$ é *unidentifiable* devido à possibilidade de rotações ortogonais arbitrárias [^3]. Este capítulo explora em profundidade a natureza dessa *unidentifiability* e as abordagens comuns para resolvê-la, garantindo uma estrutura fatorial única e interpretável.

### Conceitos Fundamentais
Em Factor Analysis, o modelo assume que as observações $x_i \in \mathbb{R}^D$ são geradas por variáveis latentes $z_i \in \mathbb{R}^L$, onde $L < D$ [^1]. A relação entre as variáveis latentes e as observações é dada por:

$$ p(x_i|z_i, \theta) = \mathcal{N}(x_i | Wz_i + \mu, \Psi) \quad [^1] $$

onde $W$ é a *factor loading matrix* de dimensão $D \times L$, $\mu$ é o vetor de médias, e $\Psi$ é a matriz de covariância, tipicamente assumida como diagonal [^1]. A distribuição marginal de $x_i$ é então:

$$ p(x_i|\theta) = \mathcal{N}(x_i | W\mu_0 + \mu, \Psi + W\Sigma_0W^T) \quad [^1] $$

Um problema fundamental surge porque, se $R$ é uma **matriz ortogonal arbitrária** ($R R^T = I$), então podemos definir $\tilde{W} = WR$ sem alterar a função de verossimilhança (*likelihood function*) [^3]:

$$ \text{cov}[x] = W E[zz^T] W^T + \Psi = W R R^T W^T + \Psi = W W^T + \Psi \quad [^4] $$

Geometricamente, multiplicar $W$ por uma matriz ortogonal $R$ equivale a rotacionar $z$ antes de gerar $x$ [^4]. No entanto, como $z$ é tipicamente extraído de uma Gaussiana isotrópica, essa rotação não afeta a *likelihood* [^4]. Isso significa que existem infinitas matrizes $W$ equivalentes que podem gerar os mesmos dados, tornando o modelo *unidentifiable* [^3].

Para garantir uma solução única, é necessário **remover** $L(L-1)/2$ **graus de liberdade**, que correspondem ao número de matrizes ortonormais de tamanho $L \times L$ [^4]. O modelo FA tem $D + DL - L(L-1)/2$ parâmetros livres (excluindo a média), onde o primeiro termo vem de $\Psi$ [^4]. Para evitar *overfitting*, esse número deve ser menor ou igual a $D(D+1)/2$, que é o número de parâmetros em uma matriz de covariância irrestrita (mas simétrica) [^4]. Isso leva a um limite superior para $L$:

$$ L_{max} = \lfloor D + 0.5(1 - \sqrt{1 + 8D}) \rfloor \quad [^4] $$

Mesmo definindo $L < L_{max}$, a *unidentifiability* rotacional ainda persiste [^4]. Essa *unidentifiability* não afeta o desempenho preditivo do modelo, mas dificulta a interpretação da *factor loading matrix* [^4].

#### Soluções para a Unidentifiability
Várias abordagens são utilizadas para resolver o problema da *unidentifiability* em FA:

1.  **Forçar W a ser ortonormal:** Uma das soluções mais diretas é forçar $W$ a ser ortonormal e ordenar as colunas pela variância decrescente dos fatores latentes correspondentes [^4]. Essa abordagem é adotada pela Análise de Componentes Principais (PCA), que será discutida na seção 12.2 [^4]. Embora essa abordagem forneça uma solução única, ela nem sempre resulta em uma estrutura mais interpretável [^4].
2.  **Forçar W a ser triangular inferior:** Outra abordagem comum, especialmente na comunidade Bayesiana, é garantir que a matriz $W$ seja triangular inferior [^4]. Por exemplo, se $L = 3$ e $D = 4$, a matriz $W$ teria a seguinte forma:

    $$     W = \begin{pmatrix}\     w_{11} & 0 & 0 \\\\\     w_{21} & w_{22} & 0 \\\\\     w_{31} & w_{32} & w_{33} \\\\\     w_{41} & w_{42} & w_{43}\     \end{pmatrix} \quad [^4]     $$

    Além disso, requeremos que $w_{jj} > 0$ para $j = 1:L$ [^4]. O número total de parâmetros nessa matriz restrita é $D + DL - L(L-1)/2$, que é igual ao número de parâmetros identificáveis de forma única [^4]. A desvantagem dessa abordagem é que as primeiras $L$ variáveis visíveis são tratadas de forma diferente das demais, o que pode ser inadequado em algumas aplicações [^4].
3.  **Sparsity promoting priors on the weights:** Em vez de pré-especificar quais entradas em $W$ são zero, podemos incentivar as entradas a serem zero, usando regularização $l_1$ [^5], ARD [^5], ou priors *spike-and-slab* [^5]. Isso é chamado de *sparse factor analysis* [^5]. Isso não garante necessariamente uma estimativa MAP única, mas incentiva soluções interpretáveis [^5].
4.  **Choosing an informative rotation matrix:** Existem vários métodos heurísticos que tentam encontrar matrizes de rotação $R$ que podem ser usadas para modificar $W$ (e, portanto, os fatores latentes) para tentar aumentar a interpretabilidade, normalmente incentivando-os a serem (aproximadamente) esparsos [^5]. Um método popular é conhecido como *varimax* [^5].
5.  **Use of non-Gaussian priors for the latent factors:** Substituir $p(z_i)$ por uma distribuição não-Gaussiana pode, às vezes, permitir identificar exclusivamente $W$ [^5]. Essa técnica é conhecida como Análise de Componentes Independentes (ICA), conforme discutido na Seção 12.6 [^5].

### Conclusão
A *unidentifiability* é um desafio inerente ao modelo de Factor Analysis, mas pode ser abordada através da imposição de restrições ou da utilização de priors informativos [^4]. As soluções discutidas neste capítulo, como a ortonormalização, a triangularização inferior e o uso de priors esparsos, visam garantir uma solução única e, em muitos casos, melhorar a interpretabilidade dos fatores latentes [^4, 5]. A escolha da abordagem mais adequada depende do contexto específico da aplicação e dos objetivos da análise [^4, 5].

### Referências
[^1]: Capítulo 12, Seção 12.1
[^2]: Capítulo 12, Seção 12.1.1
[^3]: Capítulo 12, Seção 12.1.3
[^4]: Capítulo 12, Seção 12.1.3
[^5]: Capítulo 12, Seção 12.1.3
[^6]: Capítulo 12, Seção 12.2

<!-- END -->