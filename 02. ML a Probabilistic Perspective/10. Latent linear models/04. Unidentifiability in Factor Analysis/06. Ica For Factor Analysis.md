## Utilização de Priors Não-Gaussianos para Identificação Única em Análise Fatorial

### Introdução
Como vimos anteriormente [^3], a Análise Fatorial (FA) e outros modelos lineares latentes sofrem de *unidentifiability*. Especificamente, a função de verossimilhança permanece inalterada sob rotações ortogonais da matriz de carregamentos fatoriais **W**. Este capítulo explora como a utilização de priors não-gaussianos para os fatores latentes pode mitigar essa questão, permitindo a identificação única de **W** e dos fatores latentes.

### Conceitos Fundamentais
A Análise de Componentes Independentes (ICA) surge como uma solução para o problema de unidentifiability em modelos lineares latentes, como a Análise Fatorial [^3, ^27]. A chave para a ICA é relaxar a suposição de que os fatores latentes seguem uma distribuição Gaussiana [^28]. Ao invés disso, a ICA emprega priors não-gaussianos, explorando informações adicionais para separar as fontes independentes [^5].

**Unidentifiability em FA:**
Em FA, a matriz de covariância das observações pode ser decomposta como:
$$C \approx \mathbf{W}\mathbf{W}^T + \mathbf{\Psi}$$
Onde **W** é a matriz de carregamentos fatoriais e **Ψ** é uma matriz de covariância diagonal [^1, ^2].  A dificuldade reside no fato de que, dada uma matriz de rotação ortogonal **R** (tal que $\mathbf{R}\mathbf{R}^T = \mathbf{I}$), podemos definir $\mathbf{W}' = \mathbf{W}\mathbf{R}$ e obter a mesma função de verossimilhança:
$$\mathbf{W}'\mathbf{W}'^T = \mathbf{W}\mathbf{R}(\mathbf{W}\mathbf{R})^T = \mathbf{W}\mathbf{R}\mathbf{R}^T\mathbf{W}^T = \mathbf{W}\mathbf{W}^T$$
Isso significa que uma rotação dos fatores latentes não altera a adequação do modelo aos dados observados, tornando a identificação única de **W** impossível [^3].

**ICA e Priors Não-Gaussianos:**
A ICA supera essa limitação ao impor priors não-gaussianos sobre os fatores latentes $z_i$ [^5]. A ideia central é que se os fatores latentes são independentes e não-gaussianos, então a transformação linear que os mistura (representada por **W**) pode ser desfeita de forma única [^5, ^29].

**Formalização do Modelo ICA:**
O modelo ICA assume que as observações $\mathbf{x}_t$ são uma combinação linear de fontes latentes independentes $\mathbf{z}_t$, acrescidas de ruído $\mathbf{\epsilon}_t$ [^27]:
$$\mathbf{x}_t = \mathbf{W}\mathbf{z}_t + \mathbf{\epsilon}_t$$
Onde **W** é a matriz de mistura (mixing matrix). A chave para a ICA é o prior sobre $\mathbf{z}_t$, que é assumido como não-gaussiano e independente:
$$p(\mathbf{z}_t) = \prod_{j=1}^L p_j(z_{tj})$$
Onde $p_j(z_{tj})$ é a distribuição não-gaussiana para o j-ésimo fator latente [^29].

**Intuição por trás da Identificação Única:**
A utilização de priors não-gaussianos fornece informações adicionais que quebram a simetria rotacional presente em FA [^29]. Distribuições não-gaussianas possuem características estatísticas (como curtose ou assimetria) que são alteradas por rotações [^33]. Ao buscar uma matriz de desmistura (unmixing matrix) $\mathbf{V} = \mathbf{W}^{-1}$ que maximize a independência e a não-gaussianidade dos componentes estimados $\mathbf{z}_t = \mathbf{V}\mathbf{x}_t$, a ICA consegue identificar de forma única as fontes latentes originais [^30].

**Exemplos de Priors Não-Gaussianos:**

*   **Distribuições Super-Gaussianas (Leptocúrticas):**  Essas distribuições possuem um pico acentuado na média e caudas pesadas [^33]. A distribuição de Laplace é um exemplo.
*   **Distribuições Sub-Gaussianas (Platicúrticas):**  Essas distribuições são mais achatadas do que a Gaussiana [^33]. A distribuição uniforme é um exemplo.
*   **Distribuições Assimétricas (Skewed):**  Essas distribuições não são simétricas em relação à média [^33]. A distribuição gama é um exemplo.

**Algoritmos para ICA:**

*   **Maximum Likelihood Estimation (MLE):**  Este método busca maximizar a verossimilhança dos dados observados, dado o modelo ICA [^30]. Isso envolve estimar a matriz de desmistura **V** e os parâmetros das distribuições não-gaussianas [^30].
*   **FastICA:**  Um algoritmo eficiente que utiliza uma aproximação do método de Newton para encontrar a matriz de desmistura [^31].
*   **Infomax:**  Este método busca maximizar o fluxo de informação através de uma rede neural, o que leva à separação das fontes independentes [^36].
*   **Expectation-Maximization (EM):**  O algoritmo EM pode ser utilizado para estimar os parâmetros do modelo ICA, especialmente quando as distribuições das fontes são modeladas como misturas de gaussianas [^34].

### Conclusão
A Análise de Componentes Independentes (ICA) oferece uma abordagem poderosa para superar a unidentifiability inerente à Análise Fatorial (FA) e outros modelos lineares latentes [^5]. Ao empregar priors não-gaussianos sobre os fatores latentes, a ICA explora informações estatísticas adicionais para identificar de forma única as fontes independentes que compõem os dados observados [^29]. Vimos que diferentes tipos de distribuições não-gaussianas, como as super-gaussianas e sub-gaussianas, podem ser utilizadas como priors, e que diversos algoritmos, incluindo MLE, FastICA, Infomax e EM, podem ser empregados para estimar os parâmetros do modelo ICA [^30, ^31, ^36, ^34]. A escolha do prior e do algoritmo depende das características específicas dos dados e dos objetivos da análise.

### Referências
[^1]: 12.1 Factor analysis
[^2]: 12.1.1 FA is a low rank parameterization of an MVN
[^3]: 12.1.3 Unidentifiability
[^4]: 12.1.4 Mixtures of factor analysers
[^5]: 12.1.4 Use of non-Gaussian priors for the latent factors
[^27]: 12.6 Independent Component Analysis (ICA)
[^28]: 12.6 Independent Component Analysis (ICA)
[^29]: 12.6 Independent Component Analysis (ICA)
[^30]: 12.6 Independent Component Analysis (ICA)
[^31]: 12.6 Independent Component Analysis (ICA)
[^33]: 12.6 Independent Component Analysis (ICA)
[^34]: 12.6 Independent Component Analysis (ICA)
[^36]: 12.6 Independent Component Analysis (ICA)
<!-- END -->