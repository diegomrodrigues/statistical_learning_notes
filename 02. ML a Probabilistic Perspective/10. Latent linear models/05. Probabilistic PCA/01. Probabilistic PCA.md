## Probabilistic PCA and its Relationship to Classical PCA

### Introdução
Este capítulo explora a conexão entre **Probabilistic Principal Component Analysis (PPCA)** e o **Principal Component Analysis (PCA) clássico**. O PPCA pode ser visto como um modelo de análise fatorial com restrições específicas na matriz de covariância de erro e na matriz de carregamento de fatores [^1]. Compreender a relação entre PPCA e PCA clássico é crucial para apreciar as nuances e vantagens do PPCA, especialmente em cenários onde a incerteza e o ruído são fatores importantes.

### Conceitos Fundamentais
O modelo de **análise fatorial** geral, como apresentado anteriormente [^1], assume que as observações $x_i \in \mathbb{R}^D$ são geradas a partir de variáveis latentes $z_i \in \mathbb{R}^L$ através da seguinte relação:

$$ p(x_i|z_i, \theta) = \mathcal{N}(x_i | Wz_i + \mu, \Psi) $$

onde $W$ é a **matriz de carregamento de fatores** de dimensão $D \times L$, $\mu$ é o vetor de média e $\Psi$ é a **matriz de covariância** $D \times D$. No contexto do PPCA, duas restrições significativas são impostas [^1, 7]:

1.  A matriz de covariância de erro $\Psi$ é isotrópica, ou seja, $\Psi = \sigma^2I$, onde $\sigma^2$ é a variância do ruído e $I$ é a matriz identidade.
2.  A matriz de carregamento de fatores $W$ é ortogonal.

Com essas restrições, o PPCA pode ser visto como uma versão probabilística do PCA clássico [^1, 7]. O **Teorema 12.2.2** [^15] afirma que, sob essas condições, a log-verossimilhança dos dados observados é dada por:

$$ \log p(X|W, \sigma^2) = -\frac{N}{2} \left[ \log |C| + \text{tr}(C^{-1}S) \right] $$

onde $C = WW^T + \sigma^2I$ e $S = \frac{1}{N} \sum_{i=1}^{N} x_i x_i^T = (1/N)X^TX$ é a matriz de covariância empírica (assumindo dados centrados). Os máximos da log-verossimilhança são obtidos por:

$$ W = V(\Lambda - \sigma^2 I)^{\frac{1}{2}} R $$

onde $R$ é uma matriz ortogonal arbitrária $L \times L$, $V$ é a matriz $D \times L$ cujas colunas são os primeiros $L$ autovetores de $S$, e $\Lambda$ é a matriz diagonal correspondente de autovalores [^15]. Sem perda de generalidade, podemos definir $R = I$ [^15]. Além disso, a estimativa de máxima verossimilhança (MLE) da variância do ruído é dada por:

$$ \sigma^2 = \frac{1}{D - L} \sum_{j=L+1}^{D} \lambda_j $$

que é a variância média associada às dimensões descartadas [^15].

**Convergência para PCA Clássico:**
A relação crucial entre PPCA e PCA clássico surge quando a variância do ruído $\sigma^2$ tende a zero [^7]. Neste limite, a matriz de carregamento de fatores $W$ no PPCA converge para a matriz $V$ de autovetores no PCA clássico [^7]. Formalmente, quando $\sigma^2 \rightarrow 0$, temos $\hat{W} \rightarrow V$.

**Interpretação Geométrica:**
A Figura 12.5 [^8] ilustra essa conexão geometricamente. No PCA clássico, os pontos de dados são projetados ortogonalmente sobre uma linha (no caso de $L = 1$ e $D = 2$). No PPCA, a projeção não é mais ortogonal e as reconstruções são encolhidas em direção à média dos dados. No entanto, à medida que $\sigma^2$ se aproxima de zero, essa projeção do PPCA se aproxima da projeção ortogonal do PCA clássico.

**Algoritmo EM para PCA:**
O texto menciona que o algoritmo EM pode ser usado para ajustar modelos PCA [^16].

*   **Etapa E:** $\bar{Z} = (W^T W)^{-1} W^T X$
*   **Etapa M:** $W = X \bar{Z}^T (\bar{Z} \bar{Z}^T)^{-1}$

O texto também menciona que a única solução de ponto fixo estável do algoritmo EM é a solução globalmente ótima [^16]. Isso significa que o algoritmo EM converge para uma solução onde $W$ abrange o mesmo subespaço linear definido pelos primeiros $L$ autovetores.

### Conclusão
Em resumo, o PPCA fornece uma estrutura probabilística para o PCA, incorporando explicitamente uma variância de ruído $\sigma^2$. A principal ligação ao PCA clássico é que, à medida que a variância do ruído se aproxima de zero, o PPCA converge para o PCA clássico, com a matriz de carregamento de fatores $W$ se aproximando da matriz de autovetores $V$. Essa conexão permite que o PPCA seja visto como uma generalização do PCA clássico que lida com o ruído de forma mais robusta.

### Referências
[^1]: Página 381, "Latent linear models"
[^7]: Página 387, "Principal components analysis (PCA)"
[^8]: Página 388, "Principal components analysis (PCA)"
[^15]: Página 395, "Probabilistic PCA"
[^16]: Página 396, "EM algorithm for PCA"
<!-- END -->