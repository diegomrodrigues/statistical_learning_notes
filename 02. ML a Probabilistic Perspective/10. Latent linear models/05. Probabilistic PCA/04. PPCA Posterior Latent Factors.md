## Inferência Bayesiana em PPCA: A Distribuição Posterior dos Fatores Latentes

### Introdução
Este capítulo aprofunda a inferência Bayesiana no contexto de Probabilistic Principal Component Analysis (PPCA). Em particular, focaremos na derivação e interpretação da distribuição *a posteriori* dos fatores latentes, um passo crucial para revelar informações interessantes sobre os dados através da identificação de estruturas subjacentes [^382]. Construindo sobre o conceito de Factor Analysis (FA) e sua especialização PPCA [^381], exploraremos como a regra de Bayes para Gaussianas nos permite computar a distribuição *a posteriori* dos fatores latentes $z$ dado uma observação $x$ e os parâmetros do modelo $\theta$ [^382].

### Conceitos Fundamentais

Em PPCA, assumimos que os dados observados $x_i \in \mathbb{R}^D$ são gerados a partir de fatores latentes $z_i \in \mathbb{R}^L$ através de uma transformação linear com ruído Gaussiano. O modelo generativo é definido por [^381]:

$$\np(z_i) = \mathcal{N}(z_i | \mu_0, \Sigma_0)$$

$$\np(x_i | z_i, \theta) = \mathcal{N}(x_i | Wz_i + \mu, \Psi)$$

onde $W$ é a *factor loading matrix* de dimensão $D \times L$, $\mu$ é a média, e $\Psi$ é a matriz de covariância, que em PPCA é restrita a $\Psi = \sigma^2I$, sendo $I$ a matriz identidade [^381]. O objetivo da inferência Bayesiana é encontrar a distribuição *a posteriori* $p(z_i | x_i, \theta)$, que representa nossa crença sobre os fatores latentes dado os dados observados [^382]. Utilizando a regra de Bayes para Gaussianas [^382], temos:

$$\np(z_i | x_i, \theta) = \mathcal{N}(z_i | m_i, \Sigma_i)$$

com [^382]:

$$\Sigma_i = (\Sigma_0^{-1} + W^T \Psi^{-1} W)^{-1}$$

$$\nm_i = \Sigma_i (W^T \Psi^{-1} (x_i - \mu) + \Sigma_0^{-1} \mu_0)$$

Em PPCA, com as simplificações $\mu_0 = 0$ e $\Sigma_0 = I$ (sem perda de generalidade [^382]) e $\Psi = \sigma^2I$, as equações se reduzem a:

$$\Sigma = (I + W^T (\sigma^2 I)^{-1} W)^{-1} = (I + \frac{1}{\sigma^2} W^T W)^{-1}$$

$$\nm = \Sigma (\frac{1}{\sigma^2} W^T (x - \mu))$$

Definindo $F = W^T W + \sigma^2I$, podemos expressar a covariância *a posteriori* como $\sigma^2F^{-1}$. A média *a posteriori* torna-se $F^{-1}W^T x_i$, assumindo $\mu = 0$ [^395]. Portanto, a distribuição *a posteriori* dos fatores latentes em PPCA é dada por [^395]:

$$\np(z_i | x_i, \theta) = \mathcal{N}(z_i | F^{-1}W^T x_i, \sigma^2 F^{-1})$$

onde $F = W^T W + \sigma^2I$. Esta distribuição Gaussiana permite a inferência probabilística das variáveis latentes [^395].

**Lemma 1:** A matriz $F = W^T W + \sigma^2I$ é sempre invertível se $\sigma^2 > 0$.

*Prova:*
A matriz $W^T W$ é sempre semi-definida positiva. Adicionar $\sigma^2I$ com $\sigma^2 > 0$ garante que todos os autovalores de $F$ sejam estritamente positivos. Portanto, $F$ é definida positiva e, consequentemente, invertível. $\blacksquare$

**Corolário 1:** A matriz de covariância *a posteriori* $\sigma^2F^{-1}$ é sempre bem definida.

### Conclusão

Neste capítulo, detalhamos a derivação da distribuição *a posteriori* dos fatores latentes em PPCA. Esta distribuição, uma Gaussiana com média $F^{-1}W^T x_i$ e covariância $\sigma^2F^{-1}$, possibilita a inferência probabilística dos fatores latentes, crucial para diversas aplicações de modelagem de dados [^382]. A análise da invertibilidade de $F$ garante a validade da covariância *a posteriori*. Nos próximos capítulos, exploraremos como essa inferência *a posteriori* pode ser utilizada em conjunto com o algoritmo EM para estimar os parâmetros do modelo PPCA e como esses fatores latentes podem ser interpretados para descobrir estruturas subjacentes nos dados [^386].
<!-- END -->