## Log-Likelihood e Estimação de Parâmetros em PPCA

### Introdução
Este capítulo detalha a formulação do log-likelihood para dados observados no contexto de Probabilistic Principal Component Analysis (PPCA) e como essa formulação permite a estimação de parâmetros através do método de máxima verossimilhança [^1]. O foco está em derivar e explicar a função log-likelihood e sua relação com a matriz de covariância e a matriz de covariância empírica.

### Conceitos Fundamentais
Em PPCA, o objetivo é modelar dados de alta dimensão utilizando um número menor de variáveis latentes. Assume-se que os dados observados, $X$, são gerados a partir de variáveis latentes $z_i$ através de uma transformação linear, adicionando-se ruído isotrópico [^1]:
$$p(x|z_i, \theta) = \mathcal{N}(Wz_i + \mu, \sigma^2I)$$
onde $W$ é a **matriz de carregamento fatorial** (*factor loading matrix*) de dimensão $D \times L$, com $D$ sendo a dimensionalidade dos dados observados e $L$ a dimensionalidade do espaço latente ($L < D$) [^1]. O termo $\sigma^2I$ representa a **matriz de covariância do ruído**, onde $\sigma^2$ é a variância do ruído e $I$ é a matriz identidade de dimensão $D \times D$ [^1].

A distribuição marginal de $x$ pode ser obtida integrando sobre as variáveis latentes $z_i$, assumindo que $p(z_i) = \mathcal{N}(z_i | \mu_0, \Sigma_0)$ [^1]:
$$p(x|\theta) = \int p(x|z_i, \theta) p(z_i) dz_i = \mathcal{N}(x | W\mu_0 + \mu, \Psi + W\Sigma_0W^T)$$
No caso específico do PPCA, $\Psi = \sigma^2I$ [^1]. Simplificando ainda mais, frequentemente assume-se $\mu_0 = 0$ e $\Sigma_0 = I$, o que leva a [^2]:
$$p(x|\theta) = \mathcal{N}(x | \mu, \sigma^2I + WW^T)$$

A **matriz de covariância** $C$ dos dados observados é, portanto, dada por [^1]:
$$C = WW^T + \sigma^2I$$
e a **matriz de covariância empírica** $S$ é definida como [^1]:
$$S = \frac{1}{N}X^TX$$
onde $X$ é a matriz de dados $N \times D$, com $N$ sendo o número de observações.

O **log-likelihood** dos dados observados $X$ dado os parâmetros $W$ e $\sigma^2$ é dado por [^1]:
$$log \ p(X|W, \sigma^2) = -\frac{N}{2} [log \ |C| + tr(C^{-1}S)]$$
onde $tr(\cdot)$ denota o traço da matriz.

Para estimar os parâmetros $W$ e $\sigma^2$ via **máxima verossimilhança**, maximizamos o log-likelihood em relação a esses parâmetros [^1]. Isso envolve encontrar os valores de $W$ e $\sigma^2$ que melhor se ajustam aos dados, de acordo com o modelo PPCA.

### Derivação e Explicação Detalhada
A derivação do log-likelihood envolve alguns passos importantes. Primeiramente, reconhecemos que a distribuição marginal de $x$ é uma Gaussiana multivariada com média $\mu$ e covariância $C$ [^1, 2]. A função de densidade de probabilidade para uma única observação $x_i$ é dada por:
$$p(x_i | W, \sigma^2) = \frac{1}{(2\pi)^{D/2} |C|^{1/2}} exp\left(-\frac{1}{2} (x_i - \mu)^T C^{-1} (x_i - \mu)\right)$$
Assumindo que as observações são independentes e identicamente distribuídas (i.i.d.), o likelihood dos dados observados $X$ é o produto das densidades de probabilidade individuais:
$$p(X | W, \sigma^2) = \prod_{i=1}^{N} p(x_i | W, \sigma^2)$$
Tomando o logaritmo do likelihood, obtemos o log-likelihood:
$$log \ p(X | W, \sigma^2) = \sum_{i=1}^{N} log \ p(x_i | W, \sigma^2)$$
Substituindo a expressão para a densidade Gaussiana e simplificando, chegamos a [^1]:
$$log \ p(X | W, \sigma^2) = -\frac{N}{2} [log \ |C| + tr(C^{-1}S)]$$
Essa expressão é central para a estimação dos parâmetros no modelo PPCA. A maximização desta função em relação a $W$ e $\sigma^2$ nos fornece as estimativas de máxima verossimilhança para esses parâmetros [^1].

### Conclusão
A formulação do log-likelihood no PPCA permite uma abordagem baseada em máxima verossimilhança para a estimação dos parâmetros do modelo. Ao expressar o log-likelihood em termos da matriz de covariância $C$ e da matriz de covariância empírica $S$, podemos otimizar os parâmetros $W$ e $\sigma^2$ para melhor ajustar o modelo aos dados observados. Este processo é fundamental para a aplicação do PPCA em diversas áreas, incluindo redução de dimensionalidade, modelagem de dados e inferência estatística. O capítulo seguinte abordará algoritmos específicos para a otimização desta função log-likelihood, como o algoritmo EM [^7].

### Referências
[^1]: Página 381-382
[^2]: Página 382
[^7]: Página 386
<!-- END -->