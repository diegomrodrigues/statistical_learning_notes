## Ordinary Least Squares (OLS) Solution in Linear Regression

### Introdução
Dentro do contexto de **Linear Regression**, a solução de **Ordinary Least Squares (OLS)**, derivada da **normal equation**, oferece uma abordagem fechada para estimar os parâmetros em modelos de regressão linear [^1]. Este capítulo explora em detalhes a derivação, propriedades e interpretações da solução OLS, fornecendo uma compreensão profunda para o leitor com conhecimento avançado em matemática, modelos estatísticos e otimização.

### Conceitos Fundamentais
Em **Linear Regression**, o objetivo é modelar a relação entre uma variável dependente *y* e uma ou mais variáveis independentes *x* [^1]. A forma geral do modelo é dada por:

$p(y|\mathbf{x}, \theta) = \mathcal{N}(y|\mathbf{w}^T\mathbf{x}, \sigma^2)$ [^1]

onde $\mathbf{w}$ é o vetor de pesos e $\sigma^2$ é a variância do ruído [^1]. A solução OLS procura encontrar os valores de $\mathbf{w}$ que minimizam a soma dos quadrados das diferenças entre os valores observados e os valores preditos [^1].

A **normal equation** é dada por:

$$X^TX\mathbf{w} = X^T\mathbf{y}$$ [^1]

onde $X$ é a matriz de design, cujas linhas contêm os vetores de entrada $\mathbf{x}_i$, e $\mathbf{y}$ é o vetor de valores observados [^1].

Para derivar a solução OLS, começamos definindo a função de custo como a soma dos quadrados dos resíduos (RSS):

$$RSS(\mathbf{w}) = \sum_{i=1}^{N}(y_i - \mathbf{w}^T\mathbf{x}_i)^2$$ [^2]

Em notação matricial, o RSS pode ser escrito como:

$$RSS(\mathbf{w}) = (\mathbf{y} - X\mathbf{w})^T(\mathbf{y} - X\mathbf{w})$$

Para minimizar o RSS, tomamos a derivada em relação a $\mathbf{w}$ e a igualamos a zero [^3]:

$$frac{\partial RSS(\mathbf{w})}{\partial \mathbf{w}} = -2X^T(\mathbf{y} - X\mathbf{w}) = 0$$

Resolvendo para $\mathbf{w}$, obtemos a solução OLS:

$$X^TX\mathbf{w} = X^T\mathbf{y}$$

$$hat{\mathbf{w}}_{OLS} = (X^TX)^{-1}X^T\mathbf{y}$$ [^1]

Esta solução oferece um método direto e eficiente para a estimativa de parâmetros em regressão linear [^1]. A matriz $(X^TX)^{-1}X^T$ é também conhecida como a **pseudo-inversa** de $X$ [^1].

É importante notar que a existência da solução OLS depende da invertibilidade da matriz $X^TX$. Se $X^TX$ não for invertível, o que pode ocorrer se as colunas de $X$ forem linearmente dependentes, então a solução OLS não é única [^4]. Nesses casos, técnicas de regularização, como a **ridge regression**, podem ser usadas para obter uma solução estável [^4].

**Interpretação Geométrica:**

A solução OLS tem uma interpretação geométrica elegante [^4]. As colunas da matriz $X$ definem um subespaço linear de dimensionalidade *D* embutido em um espaço de *N* dimensões [^4]. O objetivo é encontrar um vetor $\hat{\mathbf{y}}$ neste subespaço que esteja o mais próximo possível de $\mathbf{y}$ [^4]. A solução OLS corresponde à projeção ortogonal de $\mathbf{y}$ no espaço coluna de *X* [^5].

Para minimizar a norma do resíduo, $\mathbf{y} - \hat{\mathbf{y}}$, queremos que o vetor resíduo seja ortogonal a cada coluna de *X* [^5]. Isso leva à condição:

$$X^T(\mathbf{y} - \hat{\mathbf{y}}) = 0$$

Substituindo $\hat{\mathbf{y}} = X\mathbf{w}$, obtemos a mesma **normal equation**:

$$X^T(\mathbf{y} - X\mathbf{w}) = 0$$

$$hat{\mathbf{w}} = (X^TX)^{-1}X^T\mathbf{y}$$

A matriz de projeção $P = X(X^TX)^{-1}X^T$ é conhecida como a **hat matrix** [^5], pois "coloca o chapéu" em $\mathbf{y}$ [^5].

### Conclusão
A solução OLS, derivada da **normal equation**, é um dos pilares da regressão linear, oferecendo uma abordagem direta e eficiente para a estimativa de parâmetros [^1]. Sua interpretação geométrica fornece uma compreensão intuitiva do processo de minimização dos resíduos [^4]. Embora a solução OLS seja amplamente utilizada, é essencial estar ciente de suas limitações, como a necessidade de invertibilidade de $X^TX$ e sua sensibilidade a outliers [^7]. Técnicas de regularização e abordagens robustas podem ser empregadas para mitigar essas limitações [^7].

### Referências
[^1]: Capítulo 7, Linear regression, página 1
[^2]: Capítulo 7, Linear regression, página 2, equação 7.9
[^3]: Capítulo 7, Linear regression, página 3, seção 7.3.1
[^4]: Capítulo 7, Linear regression, página 4, seção 7.3.2
[^5]: Capítulo 7, Linear regression, página 5
[^7]: Capítulo 7, Linear regression, página 7, seção 7.4

<!-- END -->