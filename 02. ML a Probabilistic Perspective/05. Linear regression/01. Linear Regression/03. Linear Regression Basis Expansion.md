## Regressão Linear com Expansão de Funções de Base

### Introdução
A regressão linear é uma ferramenta fundamental na análise de dados e machine learning [^1]. Como vimos anteriormente, a regressão linear modela a relação entre uma variável dependente e uma ou mais variáveis independentes, assumindo uma relação linear entre elas. No entanto, muitas vezes as relações no mundo real são não lineares. Para lidar com essas situações, a regressão linear pode ser estendida através da **expansão de funções de base** [^1]. Este capítulo explora essa extensão, detalhando como modelar relações não lineares mantendo a estrutura linear nos parâmetros.

### Conceitos Fundamentais

A **expansão de funções de base** é uma técnica que permite modelar relações não lineares usando modelos lineares [^1]. A ideia central é transformar as variáveis de entrada (features) através de funções não lineares antes de aplicar o modelo linear. Matematicamente, substituímos as variáveis de entrada $x$ por uma função não linear $\phi(x)$ [^1].

O modelo resultante é dado por:

$$\np(y|x, \theta) = N(y|w^T\phi(x), \sigma^2)\n$$

onde:
*   $y$ é a variável dependente.
*   $x$ é a variável independente.
*   $\theta$ representa os parâmetros do modelo.
*   $w$ é o vetor de pesos (parâmetros) do modelo.
*   $\phi(x)$ é a função de base não linear.
*   $\sigma^2$ é a variância do ruído.
*   $N(\cdot)$ denota a distribuição normal (gaussiana).

É crucial notar que o modelo permanece linear nos parâmetros $w$, mesmo que $\phi(x)$ seja não linear [^1]. Isso permite utilizar as técnicas de regressão linear para estimar os parâmetros, enquanto capturamos padrões complexos nos dados.

Um exemplo comum de funções de base são as **funções de base polinomiais** [^1]:

$$\phi(x) = [1, x, x^2, ..., x^d]\n$$

onde $d$ é o grau do polinômio. Aumentar o valor de $d$ permite modelar funções cada vez mais complexas [^1]. Por exemplo, se $d=2$, a função de base seria $\phi(x) = [1, x, x^2]$, e o modelo se tornaria uma regressão linear com termos quadráticos.
Considere modelar a temperatura em função da localização [^1]. Um modelo linear simples seria:

$$\nE[y|x] = w_0 + w_1x_1 + w_2x_2\n$$

onde $x_1$ e $x_2$ representam as coordenadas da localização. Para capturar relações não lineares, podemos adicionar termos quadráticos:

$$\nE[y|x] = w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_2^2\n$$

Neste caso, $\phi(x) = [1, x_1, x_2, x_1^2, x_2^2]$.

**Maximum Likelihood Estimation (MLE)**

Para estimar os parâmetros do modelo, podemos usar a técnica de **Maximum Likelihood Estimation (MLE)** [^1]. O objetivo é encontrar os valores dos parâmetros $w$ que maximizam a função de verossimilhança (likelihood) dos dados observados. A função de verossimilhança é dada por:

$$\nl(\theta) = \arg \max_\theta log p(D|\theta)\n$$

onde $D$ representa os dados observados.
Na regressão linear com ruído gaussiano, maximizar a verossimilhança é equivalente a minimizar a soma dos quadrados dos resíduos (RSS) [^1]:

$$\nRSS(w) = \sum_{i=1}^N (y_i - w^T\phi(x_i))^2\n$$

A solução para $w$ que minimiza o RSS é dada pela **equação normal** [^1]:

$$\nw_{OLS} = (X^TX)^{-1}X^Ty\n$$

onde $X$ é a matriz de design, cujas linhas são $\phi(x_i)^T$.

**Regularização**

Um problema comum com a expansão de funções de base é o **overfitting** [^1]. Quando o grau do polinômio ($d$) é muito alto, o modelo pode se ajustar excessivamente aos dados de treinamento, resultando em um desempenho ruim em dados não vistos. Para mitigar o overfitting, podemos usar técnicas de regularização, como a **regressão de Ridge** [^1]. A regressão de Ridge adiciona um termo de penalidade à função de custo, que penaliza valores grandes para os parâmetros $w$:

$$\nJ(w) = \frac{1}{N} \sum_{i=1}^N (y_i - (w_0 + w^T\phi(x_i)))^2 + \lambda ||w||^2\n$$

onde $\lambda$ é o parâmetro de regularização, que controla a força da penalidade.

### Conclusão
A expansão de funções de base é uma técnica poderosa para estender a regressão linear e modelar relações não lineares [^1]. Ao transformar as variáveis de entrada através de funções não lineares, podemos capturar padrões complexos nos dados, mantendo a estrutura linear nos parâmetros. No entanto, é importante estar ciente do problema de overfitting e utilizar técnicas de regularização para evitar que o modelo se ajuste excessivamente aos dados de treinamento [^1].

### Referências
[^1]:  Linear regression.
<!-- END -->