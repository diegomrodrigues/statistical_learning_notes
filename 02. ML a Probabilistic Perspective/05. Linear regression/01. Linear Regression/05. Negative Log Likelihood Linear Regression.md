## Negative Log-Likelihood in Linear Regression

### Introdução

Em estatística e aprendizado de máquina, a **estimação de máxima verossimilhança (MLE)** é um método popular para estimar os parâmetros de um modelo estatístico [^1]. No entanto, em vez de maximizar diretamente a função de verossimilhança, podemos equivalentemente *minimizar o negativo do logaritmo da função de verossimilhança (NLL)*. Essa abordagem é particularmente útil porque muitos algoritmos de otimização são projetados para encontrar os mínimos de funções [^2]. Este capítulo explora o conceito de NLL no contexto da regressão linear, detalhando sua formulação matemática e relevância prática.

### Conceitos Fundamentais

O negativo do logaritmo da verossimilhança (NLL) é definido como [^2]:

$$
NLL(\theta) = -\sum_{i=1}^{N} \log p(y_i|x_i, \theta)
$$

onde:
- $\theta$ representa os parâmetros do modelo.
- $y_i$ é o i-ésimo valor observado da variável dependente.
- $x_i$ é o i-ésimo valor da variável independente.
- $p(y_i|x_i, \theta)$ é a função de densidade de probabilidade da variável dependente, dado $x_i$ e os parâmetros $\theta$.
- $N$ é o número total de observações.

Minimizar o NLL é equivalente a maximizar a função de log-verossimilhança, pois o sinal negativo inverte o problema de otimização [^2]. A conveniência do NLL reside no fato de que muitos algoritmos de otimização são projetados para encontrar os mínimos de funções.

#### NLL para Regressão Linear

Para o caso específico da regressão linear, onde assumimos que os dados seguem uma distribuição gaussiana, o NLL pode ser expresso como [^2]:

$$
NLL(\theta) = \frac{1}{2\sigma^2} RSS(w) + \frac{N}{2} \log(2\pi\sigma^2)
$$

onde:
- $RSS(w) = \sum_{i=1}^{N} (y_i - w^Tx_i)^2$ é a soma dos quadrados dos resíduos (residual sum of squares) [^2].
- $w$ é o vetor de pesos do modelo de regressão linear.
- $\sigma^2$ é a variância dos erros.

O termo $RSS(w)$ mede a discrepância entre os valores observados e os valores previstos pelo modelo. Minimizar o NLL, neste caso, implica em encontrar os pesos $w$ que minimizam o $RSS(w)$, ponderado pela variância $\sigma^2$ e um termo constante relacionado ao número de observações e à variância [^2].

#### Derivação da MLE e NLL

Na regressão linear, o objetivo é encontrar os parâmetros $\theta$ que melhor se ajustam aos dados [^1]. Uma forma comum de estimar esses parâmetros é calcular a **MLE**, definida como [^1]:

$$
\hat{\theta} = \arg \max_{\theta} \log p(D|\theta)
$$

onde $D$ representa os dados observados. Em vez de maximizar a log-verossimilhança, podemos equivalentemente *minimizar o negativo do log-verossimilhança* [^2]:

$$
NLL(\theta) = -\sum_{i=1}^{N} \log p(y_i|x_i, \theta)
$$

Assumindo que os exemplos de treinamento são independentes e identicamente distribuídos (iid), podemos escrever a função de log-verossimilhança como [^2]:

$$
l(\theta) = \log p(D|\theta) = \sum_{i=1}^{N} \log p(y_i|x_i, \theta)
$$

Inserindo a definição da gaussiana na equação acima, encontramos que a função de log-verossimilhança é dada por [^2]:

$$
l(\theta) = \sum_{i=1}^{N} \log \left[ \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{1}{2\sigma^2} (y_i - w^Tx_i)^2 \right) \right]
$$

Simplificando, obtemos [^2]:

$$
l(\theta) = -\frac{1}{2\sigma^2} RSS(w) - \frac{N}{2} \log(2\pi\sigma^2)
$$

onde $RSS(w)$ é a soma dos quadrados dos resíduos, definida como [^2]:

$$
RSS(w) = \sum_{i=1}^{N} (y_i - w^Tx_i)^2
$$

O $RSS$ também é conhecido como a soma dos erros quadrados (SSE), e $SSE/N$ é chamado de erro quadrático médio (MSE) [^2].

#### Derivação da MLE

Para derivar a MLE, primeiro reescrevemos o objetivo em uma forma que é mais adequada para diferenciação [^3]:

$$
NLL(w) = \frac{1}{2} (y - Xw)^T (y - Xw) = \frac{1}{2} (y^Ty - w^TX^Ty - y^TXw + w^TX^TXw)
$$

$$
NLL(w) = \frac{1}{2} (y^Ty - 2w^TX^Ty + w^TX^TXw)
$$

O gradiente desta função é dado por [^4]:

$$
g(w) = \nabla NLL(w) = X^TXw - X^Ty
$$

Igualando a zero, obtemos [^4]:

$$
X^TXw = X^Ty
$$

Esta é conhecida como a **equação normal**. A solução correspondente $\hat{w}$ para este sistema linear de equações é chamada de solução dos mínimos quadrados ordinários (OLS), que é dada por [^4]:

$$
\hat{w}_{OLS} = (X^TX)^{-1}X^Ty
$$ $\blacksquare$

### Conclusão

O negativo do logaritmo da verossimilhança (NLL) oferece uma alternativa conveniente para a estimação de máxima verossimilhança (MLE) em regressão linear. Ao minimizar o NLL, podemos encontrar os parâmetros do modelo que melhor se ajustam aos dados, utilizando algoritmos de otimização projetados para encontrar mínimos de funções. A formulação do NLL para regressão linear, expressa em termos do RSS, fornece uma estrutura clara para entender como os parâmetros do modelo influenciam o ajuste aos dados. A minimização do NLL leva à solução dos mínimos quadrados ordinários (OLS), amplamente utilizada em análise estatística e aprendizado de máquina.

### Referências
[^1]: Section 7.3 Maximum likelihood estimation (least squares)
[^2]: Section 7.6 Negative log likelihood (NLL)
[^3]: Section 7.3.1 Derivation of the MLE
[^4]: Section 7.3.2 Geometric interpretation
<!-- END -->