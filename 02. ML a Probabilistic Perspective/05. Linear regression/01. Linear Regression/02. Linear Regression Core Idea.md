## Mínimos Quadrados e Projeção Geométrica em Regressão Linear

### Introdução
A regressão linear é uma ferramenta fundamental em estatística e aprendizado de máquina, utilizada para modelar a relação entre uma variável dependente e uma ou mais variáveis independentes [^1]. O objetivo central é encontrar a linha (ou hiperplano em dimensões superiores) que melhor se ajusta aos dados, minimizando a diferença entre os valores previstos e os valores reais da variável dependente [^1]. Este capítulo explora a fundo o método dos mínimos quadrados (least squares) e sua interpretação geométrica, fornecendo uma base sólida para a compreensão e aplicação da regressão linear.

### Conceitos Fundamentais
O método dos **mínimos quadrados** é uma técnica para estimar os parâmetros de um modelo estatístico, minimizando a soma dos quadrados dos resíduos (RSS), que representam a diferença entre os valores observados e os valores previstos pelo modelo [^1]. Matematicamente, o objetivo é minimizar a seguinte função [^7]:
$$ RSS(w) = \sum_{i=1}^{N} (y_i - w^T x_i)^2 $$
onde $y_i$ são os valores observados, $x_i$ são as variáveis independentes, e $w$ são os parâmetros a serem estimados. Essa função também pode ser expressa em notação matricial como [^7]:
$$ RSS(w) = (y - Xw)^T(y - Xw) $$
onde $y$ é o vetor de valores observados e $X$ é a matriz de variáveis independentes.

A **solução de mínimos quadrados** para $w$, denotada como $\hat{w}$, é obtida derivando a função RSS em relação a $w$ e igualando a zero [^7]:
$$ \frac{\partial RSS(w)}{\partial w} = -2X^T(y - Xw) = 0 $$
Resolvendo para $w$, obtemos a **equação normal**:
$$ X^TXw = X^Ty $$
A solução para a equação normal é dada por [^4]:
$$ \hat{w}_{OLS} = (X^TX)^{-1}X^Ty $$
Esta solução é conhecida como a solução de **mínimos quadrados ordinários** (OLS).

A **interpretação geométrica** da regressão linear oferece uma perspectiva valiosa sobre o processo de ajuste do modelo [^4]. Assumindo que o número de exemplos ($N$) é maior que o número de features ($D$), as colunas da matriz $X$ definem um subespaço linear de dimensão $D$ em um espaço de $N$ dimensões [^4]. O vetor $y$ também reside neste espaço $N$-dimensional. O objetivo da regressão linear é encontrar um vetor $\hat{y}$ no subespaço definido por $X$ que esteja o mais próximo possível de $y$ [^4]. Isso equivale a encontrar a projeção ortogonal de $y$ no subespaço de $X$ [^4].

Matematicamente, buscamos o vetor $\hat{y}$ que minimize a distância euclidiana entre $y$ e $\hat{y}$ [^4]:
$$ \underset{\hat{y} \in span(X)}{\text{argmin}} ||y - \hat{y}||_2 $$
Como $\hat{y}$ está no espaço gerado por $X$, ele pode ser expresso como uma combinação linear das colunas de $X$ [^4]:
$$ \hat{y} = w_1x_1 + ... + w_Dx_D = Xw $$
Para minimizar a norma do resíduo, $y - \hat{y}$, o vetor resíduo deve ser ortogonal a cada coluna de $X$. Ou seja, $x_j^T(y - \hat{y}) = 0$ para todo $j$ [^5]. Isso leva à seguinte equação:
$$ X^T(y - Xw) = 0 $$
Resolvendo para $w$, obtemos a mesma solução de mínimos quadrados:
$$ \hat{w} = (X^TX)^{-1}X^Ty $$
A **projeção ortogonal** de $y$ no espaço coluna de $X$ é então dada por [^5]:
$$ \hat{y} = X\hat{w} = X(X^TX)^{-1}X^Ty $$
A matriz $P = X(X^TX)^{-1}X^T$ é conhecida como a **matriz de projeção** ou **matriz hat**, pois "coloca o chapéu" em $y$ [^5].

### Conclusão
O método dos mínimos quadrados é uma técnica poderosa e amplamente utilizada para estimar os parâmetros em modelos de regressão linear [^1]. Sua interpretação geométrica revela que o objetivo é encontrar a projeção ortogonal do vetor da variável dependente no subespaço gerado pelas variáveis independentes [^4]. Essa compreensão geométrica fornece *insights* valiosos sobre o processo de ajuste do modelo e suas propriedades. O entendimento profundo desses conceitos é fundamental para a aplicação eficaz da regressão linear em diversas áreas do conhecimento.

### Referências
[^1]: Capítulo 7, Linear Regression, Introdução.
[^2]: Capítulo 7, Linear Regression, Model specification.
[^3]: Capítulo 7, Linear Regression, Maximum likelihood estimation (least squares).
[^4]: Capítulo 7, Linear Regression, Geometric interpretation.
[^5]: Capítulo 7, Linear Regression, Convexity.
[^6]: Capítulo 7, Linear Regression, Robust linear regression.
[^7]: Capítulo 7, Linear Regression, Derivation of the MLE.
<!-- END -->