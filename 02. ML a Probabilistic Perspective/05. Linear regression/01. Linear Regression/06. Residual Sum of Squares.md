## Residual Sum of Squares em Regressão Linear

### Introdução
Em regressão linear, o objetivo principal é encontrar a relação linear que melhor se ajusta aos dados observados. Uma métrica fundamental para avaliar a qualidade desse ajuste é o **Residual Sum of Squares (RSS)**, também conhecido como **Sum of Squared Errors (SSE)**. Este capítulo explora o RSS em detalhes, definindo-o formalmente, demonstrando sua importância na estimação de parâmetros via Maximum Likelihood Estimation (MLE) e discutindo sua interpretação geométrica.

### Conceitos Fundamentais

O RSS quantifica a discrepância entre os valores observados e os valores previstos pelo modelo de regressão linear [^1]. Formalmente, o RSS é definido como:

$$ RSS(w) = \sum_{i=1}^{N} (y_i - w^T x_i)^2 $$

onde:
- $N$ é o número de observações no conjunto de dados.
- $y_i$ é o valor observado da variável dependente para a i-ésima observação.
- $x_i$ é o vetor de características (variáveis independentes) para a i-ésima observação.
- $w$ é o vetor de pesos (coeficientes) do modelo de regressão linear.
- $w^T x_i$ é o valor previsto da variável dependente para a i-ésima observação.

O RSS representa a soma dos quadrados das diferenças (resíduos) entre os valores observados e os valores previstos. Minimizar o RSS é um objetivo comum ao ajustar o modelo aos dados [^1].

**Maximum Likelihood Estimation (MLE) e Least Squares**

Uma maneira comum de estimar os parâmetros de um modelo estatístico é computar o MLE [^1]. No contexto da regressão linear, assumindo que os erros são independentes e identicamente distribuídos (iid) seguindo uma distribuição normal com média zero e variância constante $\sigma^2$, a função de log-verossimilhança (log-likelihood) pode ser escrita como [^2]:

$$ l(\theta) = \sum_{i=1}^{N} \log p(y_i|x_i, \theta) $$

onde $\theta$ representa os parâmetros do modelo (incluindo $w$ e $\sigma^2$). Em vez de maximizar a função de log-verossimilhança, podemos equivalentemente minimizar a *negative log-likelihood (NLL)* [^2]:

$$ NLL(\theta) = -\sum_{i=1}^{N} \log p(y_i|x_i, \theta) $$

Ao aplicar o método de MLE ao cenário de regressão linear, inserindo a definição da distribuição Gaussiana, encontramos que a log-verossimilhança é dada por [^2]:

$$ l(\theta) =  -\frac{1}{2\sigma^2} RSS(w) - \frac{N}{2} \log(2\pi\sigma^2) $$

Maximizar essa função de log-verossimilhança é equivalente a minimizar o RSS [^1]. Portanto, o estimador de máxima verossimilhança para $w$ é aquele que minimiza o RSS [^3]. Este método é conhecido como **least squares** [^3].

**Derivação do MLE**

Para derivar o MLE, reescrevemos a NLL em uma forma mais conveniente para diferenciação [^3]:

$$ NLL(w) = \frac{1}{2} (y - Xw)^T (y - Xw) = \frac{1}{2} (y^T y - w^T (X^T y) - (X^T y)^T w + w^T (X^T X) w) $$

onde $X$ é a matriz de design (design matrix) cujas linhas são os vetores $x_i^T$.  Tomando o gradiente da NLL em relação a $w$ e igualando a zero, obtemos [^4]:

$$ \nabla_w NLL(w) = X^T X w - X^T y = 0 $$

Resolvendo para $w$, obtemos a solução de **ordinary least squares (OLS)** [^4]:

$$ \hat{w}_{OLS} = (X^T X)^{-1} X^T y $$

Esta é a solução que minimiza o RSS e, portanto, fornece o melhor ajuste linear aos dados, sob as suposições de normalidade e independência dos erros.

**Interpretação Geométrica**

A solução de least squares tem uma interpretação geométrica elegante [^4]. As colunas de $X$ definem um subespaço linear de dimensionalidade $D$ (o número de características) embutido em um espaço de $N$ dimensões (o número de observações).  Procuramos um vetor $\hat{y}$ nesse subespaço que esteja o mais próximo possível de $y$, ou seja, queremos encontrar:

$$ \underset{\hat{y} \in span(\{x_1, ..., x_D\})}{\operatorname{argmin}} ||y - \hat{y}||_2 $$

Como $\hat{y}$ está no espaço gerado pelas colunas de $X$, existe um vetor de pesos $w$ tal que $\hat{y} = Xw$ [^4]. Para minimizar a norma do resíduo, $y - \hat{y}$, o vetor resíduo deve ser ortogonal a cada coluna de $X$ [^5]. Isso leva à equação:

$$ X^T (y - Xw) = 0 $$

que é equivalente à equação normal e à solução OLS derivada anteriormente. A projeção ortogonal de $y$ no espaço das colunas de $X$ é dada por [^5]:

$$ \hat{y} = X(X^T X)^{-1} X^T y $$

A matriz $P = X(X^T X)^{-1} X^T$ é conhecida como a **hat matrix**, pois "coloca o chapéu" em $y$ [^5].

### Conclusão

O Residual Sum of Squares (RSS) é uma métrica fundamental na regressão linear que quantifica a qualidade do ajuste do modelo aos dados. Minimizar o RSS é equivalente a maximizar a verossimilhança sob a suposição de erros Gaussianos, levando à solução de ordinary least squares (OLS). A interpretação geométrica do RSS fornece insights valiosos sobre como a solução OLS projeta os dados observados no subespaço definido pelas características. O entendimento profundo do RSS é crucial para a construção e avaliação de modelos de regressão linear eficazes.

### Referências
[^1]: Section 7, Linear Regression, page 1
[^2]: Section 7.3, Maximum likelihood estimation (least squares), page 1
[^3]: Section 7.3, Maximum likelihood estimation (least squares), page 3
[^4]: Section 7.3.2, Geometric interpretation, page 4
[^5]: Section 7.3.3, Convexity, page 5
<!-- END -->