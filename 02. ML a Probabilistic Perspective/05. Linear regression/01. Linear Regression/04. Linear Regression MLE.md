## Maximum Likelihood Estimation in Linear Regression

### Introdução
A **estimação de máxima verossimilhança (MLE)** é um método fundamental para estimar os parâmetros de um modelo estatístico [^7]. No contexto da regressão linear, a MLE busca os valores dos parâmetros que maximizam a probabilidade de observar os dados fornecidos [^7]. Este capítulo explora em detalhes a aplicação da MLE em regressão linear, com ênfase na sua relação com a minimização da soma dos quadrados dos resíduos (RSS) quando o ruído é gaussiano [^7].

### Conceitos Fundamentais
A regressão linear é um modelo da forma $p(y|\mathbf{x}, \theta) = \mathcal{N}(y|\mathbf{w}^T\mathbf{x}, \sigma^2)$ [^1], onde $y$ é a variável dependente, $\mathbf{x}$ é o vetor de variáveis independentes, $\mathbf{w}$ é o vetor de pesos (parâmetros) e $\sigma^2$ é a variância do ruído gaussiano [^1]. A função de verossimilhança é dada por $p(D|\theta)$, onde $D$ representa os dados observados e $\theta$ representa os parâmetros do modelo [^1]. O objetivo da MLE é encontrar os parâmetros $\theta$ que maximizam a função de verossimilhança, ou equivalentemente, maximizar o logaritmo da verossimilhança:
$$\hat{\theta} \triangleq \underset{\theta}{\arg \max} \log p(D|\theta) \qquad (7.4)$$ [^1].

Assumindo que os exemplos de treinamento são independentes e identicamente distribuídos (iid), o log-likelihood pode ser escrito como:
$$l(\theta) \triangleq \log p(D|\theta) = \sum_{i=1}^N \log p(y_i|\mathbf{x}_i, \theta) \qquad (7.5)$$ [^2].

Minimizar o *negative log-likelihood* (NLL) é equivalente a maximizar o log-likelihood [^2]:
$$NLL(\theta) \triangleq -\sum_{i=1}^N \log p(y_i|\mathbf{x}_i, \theta) \qquad (7.6)$$ [^2].

Aplicando a MLE ao cenário de regressão linear e inserindo a definição da gaussiana, obtemos o log-likelihood [^2]:
$$l(\theta) = \sum_{i=1}^N \log \left[ \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{1}{2\sigma^2} (y_i - \mathbf{w}^T\mathbf{x}_i)^2 \right) \right] \qquad (7.7)$$ [^2].

Simplificando a equação (7.7), chegamos a:
$$l(\theta) = -\frac{1}{2\sigma^2} RSS(\mathbf{w}) - \frac{N}{2} \log(2\pi\sigma^2) \qquad (7.8)$$ [^2].

Onde $RSS(\mathbf{w})$ é a **soma dos quadrados dos resíduos**:
$$RSS(\mathbf{w}) \triangleq \sum_{i=1}^N (y_i - \mathbf{w}^T\mathbf{x}_i)^2 \qquad (7.9)$$ [^2].

A RSS também é conhecida como *sum of squared errors* (SSE), e $SSE/N$ é chamado de *mean squared error* (MSE) [^2].

A minimização da RSS corresponde à MLE para regressão linear com ruído gaussiano [^7]. Isso porque, ao maximizar o log-likelihood na equação (7.8), o termo $-\frac{N}{2} \log(2\pi\sigma^2)$ é constante em relação a $\mathbf{w}$, e o termo $-\frac{1}{2\sigma^2} RSS(\mathbf{w})$ domina a otimização. Portanto, maximizar o log-likelihood é equivalente a minimizar a RSS [^2].

Para encontrar o valor de $\mathbf{w}$ que minimiza a RSS, derivamos a NLL em relação a $\mathbf{w}$ e igualamos a zero. Primeiramente, reescrevemos o objetivo em uma forma mais adequada para a diferenciação [^3]:

$$NLL(\mathbf{w}) = \frac{1}{2} (\mathbf{y} - X\mathbf{w})^T (\mathbf{y} - X\mathbf{w}) = \frac{1}{2} (\mathbf{y}^T\mathbf{y} - \mathbf{w}^T X^T \mathbf{y} - \mathbf{y}^T X \mathbf{w} + \mathbf{w}^T X^T X \mathbf{w}) = \frac{1}{2} (\mathbf{w}^T (X^T X) \mathbf{w} - 2\mathbf{w}^T (X^T \mathbf{y})) \qquad (7.11)$$ [^3].

Onde $X$ é a matriz de design, com cada linha contendo um vetor de características $\mathbf{x}_i$ [^3].
Definimos:
$$X^T X = \sum_{i=1}^N \mathbf{x}_i \mathbf{x}_i^T \qquad (7.12)$$ [^4]
como a **matriz de soma de quadrados** e
$$X^T \mathbf{y} = \sum_{i=1}^N \mathbf{x}_i y_i \qquad (7.13)$$ [^4].

Usando os resultados da Equação 4.10, vemos que o gradiente é dado por:
$$g(\mathbf{w}) = [X^T X \mathbf{w} - X^T \mathbf{y}] = \sum_{i=1}^N \mathbf{x}_i (\mathbf{w}^T \mathbf{x}_i - y_i) \qquad (7.14)$$ [^4].
Igualando a zero, obtemos:
$$X^T X \mathbf{w} = X^T \mathbf{y} \qquad (7.15)$$ [^4].

Esta é conhecida como a **equação normal**. A solução correspondente $\hat{\mathbf{w}}$ para este sistema linear de equações é chamada de **solução dos mínimos quadrados ordinários (OLS)**, que é dada por:
$$\hat{\mathbf{w}}_{OLS} = (X^T X)^{-1} X^T \mathbf{y} \qquad (7.16)$$ [^4].

### Conclusão
A MLE fornece uma estrutura teórica para estimar os parâmetros em modelos de regressão linear. Sob a suposição de ruído gaussiano, a MLE se reduz à minimização da soma dos quadrados dos resíduos (RSS), um problema de otimização bem definido que pode ser resolvido analiticamente através da equação normal [^2]. A solução OLS resultante fornece uma estimativa dos parâmetros do modelo que maximiza a probabilidade dos dados observados [^4]. Este método é amplamente utilizado em estatística e aprendizado de máquina devido à sua simplicidade e eficácia [^1].

### Referências
[^1]: Página 1, Seção 7.3
[^2]: Página 2, Seção 7.3
[^3]: Página 3, Seção 7.3.1
[^4]: Página 4, Seção 7.3.2
[^7]: Página inicial, texto fornecido
<!-- END -->