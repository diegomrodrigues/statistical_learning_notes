## Linear Regression: A Foundational Algorithm

### Introdução
A regressão linear é um algoritmo fundamental de aprendizado supervisionado usado para modelar a relação entre uma variável dependente e uma ou mais variáveis independentes, ajustando uma equação linear aos dados observados [^1]. Ela é frequentemente descrita como o "cavalo de batalha" da estatística e do aprendizado de máquina devido à sua simplicidade e interpretabilidade, prevendo uma variável alvo contínua com base em uma combinação linear de *features* de entrada [^1]. Este capítulo explora em profundidade os conceitos, a especificação do modelo, a estimativa de máxima verossimilhança e outras extensões da regressão linear, conforme apresentado no Capítulo 7 [^1].

### Conceitos Fundamentais

#### Especificação do Modelo
A regressão linear assume que a relação entre as variáveis dependentes e independentes pode ser modelada por uma função linear [^1]. Matematicamente, isso é expresso como:

$$
p(y|x, \theta) = \mathcal{N}(y|w^Tx, \sigma^2)
$$

onde $y$ é a variável dependente, $x$ é o vetor de variáveis independentes, $w$ é o vetor de pesos (parâmetros) e $\sigma^2$ é a variância do ruído [^2]. A notação $\mathcal{N}$ denota uma distribuição Gaussiana [^2].

É importante notar que, ao substituir $x$ por uma função não linear dos inputs, $\phi(x)$, a regressão linear pode ser estendida para modelar relações não lineares [^2]:

$$
p(y|x, \theta) = \mathcal{N}(y|w^T\phi(x), \sigma^2)
$$

Isso é conhecido como *basis function expansion* [^2]. Um exemplo simples são as funções de base polinomial, onde $\phi(x) = [1, x, x^2, ..., x^d]$ [^2]. A Figura 1.18 (não incluída aqui) ilustra o efeito de alterar $d$: aumentar o grau $d$ nos permite criar funções cada vez mais complexas [^2].

#### Estimativa de Máxima Verossimilhança (Least Squares)
Uma maneira comum de estimar os parâmetros de um modelo estatístico é calcular a Estimativa de Máxima Verossimilhança (MLE), definida como [^2]:

$$
\hat{\theta} \triangleq \arg \max_{\theta} \log p(\mathcal{D}|\theta)
$$

onde $\mathcal{D}$ representa os dados [^2]. Assumindo que os exemplos de treinamento são independentes e identicamente distribuídos (iid), a função de *log-likelihood* pode ser escrita como [^2]:

$$
l(\theta) \triangleq \log p(\mathcal{D}|\theta) = \sum_{i=1}^{N} \log p(y_i|x_i, \theta)
$$

Em vez de maximizar a *log-likelihood*, podemos equivalentemente minimizar a *negative log-likelihood* (NLL) [^2]:

$$
NLL(\theta) \triangleq - \sum_{i=1}^{N} \log p(y_i|x_i, \theta)
$$

A formulação NLL é mais conveniente, pois muitos pacotes de software de otimização são projetados para encontrar os mínimos das funções, em vez dos máximos [^2]. Aplicando o método MLE à regressão linear, inserindo a definição da Gaussiana, encontramos que a *log-likelihood* é dada por [^2]:

$$
l(\theta) = \sum_{i=1}^{N} \log \left[ \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{1}{2\sigma^2} (y_i - w^Tx_i)^2 \right) \right] = -\frac{1}{2\sigma^2} RSS(w) - \frac{N}{2} \log(2\pi\sigma^2)
$$

onde RSS significa *residual sum of squares* [^2], definido por [^2]:

$$
RSS(w) \triangleq \sum_{i=1}^{N} (y_i - w^Tx_i)^2
$$

O RSS também é chamado de *sum of squared errors* (SSE), e SSE/N é chamado de *mean squared error* (MSE) [^2]. Também pode ser escrito como o quadrado da norma $l_2$ do vetor de erros residuais [^2]. A Figura 7.2(a) (não incluída aqui) ilustra este método [^3].

#### Derivação da MLE
Para derivar a MLE, primeiro reescrevemos o objetivo em uma forma mais adequada à diferenciação [^3]:

$$
NLL(w) = \frac{1}{2} (y - Xw)^T (y - Xw) = \frac{1}{2} (y^Ty - w^TX^Ty - y^TXw + w^TX^TXw) = \frac{1}{2} (w^TX^TXw - 2w^TX^Ty + y^Ty)
$$

onde
$$
X^TX = \sum_{i=1}^{N} x_ix_i^T = \begin{bmatrix} \sum_{i=1}^{N} x_{i,1}^2 & \sum_{i=1}^{N} x_{i,1}x_{i,2} & \cdots & \sum_{i=1}^{N} x_{i,1}x_{i,D} \\ \sum_{i=1}^{N} x_{i,2}x_{i,1} & \sum_{i=1}^{N} x_{i,2}^2 & \cdots & \sum_{i=1}^{N} x_{i,2}x_{i,D} \\ \vdots & \vdots & \ddots & \vdots \\ \sum_{i=1}^{N} x_{i,D}x_{i,1} & \sum_{i=1}^{N} x_{i,D}x_{i,2} & \cdots & \sum_{i=1}^{N} x_{i,D}^2 \end{bmatrix}
$$

é a *sum of squares matrix* e

$$
X^Ty = \sum_{i=1}^{N} x_iy_i
$$

Usando os resultados da Equação 4.10 (não incluída aqui), vemos que o gradiente disso é dado por [^4]:

$$
g(w) = [X^TXw - X^Ty] = \sum_{i=1}^{N} x_i(w^Tx_i - y_i)
$$

Igualando a zero, obtemos [^4]:

$$
X^TXw = X^Ty
$$

Isso é conhecido como a *normal equation* [^4]. A solução correspondente $\hat{w}$ para este sistema linear de equações é chamada de solução *ordinary least squares* (OLS), que é dada por [^4]:

$$
\hat{w}_{OLS} = (X^TX)^{-1}X^Ty
$$

#### Interpretação Geométrica
Esta equação tem uma interpretação geométrica elegante [^4]. Assumimos que $N > D$, então temos mais exemplos do que *features* [^4]. As colunas de $X$ definem um subespaço linear de dimensionalidade $D$ que é *embedded* em $N$ dimensões [^4]. Seja a j-ésima coluna $x_j$, que é um vetor em $\mathbb{R}^N$ [^4]. (Isso não deve ser confundido com $x_i \in \mathbb{R}^D$, que representa o i-ésimo caso de dados.) Da mesma forma, $y$ é um vetor em $\mathbb{R}^N$ [^4].

Buscamos um vetor $\hat{y} \in \mathbb{R}^N$ que esteja neste subespaço linear e o mais próximo possível de $y$, ou seja, queremos encontrar [^4]:

$$
\arg \min_{\hat{y} \in \text{span}(\{x_1, ..., x_D\})} ||y - \hat{y}||_2
$$

Como $\hat{y} \in \text{span}(X)$, existe algum vetor de peso $w$ tal que [^4]:

$$
\hat{y} = w_1x_1 + ... + w_Dx_D = Xw
$$

Para minimizar a norma do resíduo, $y - \hat{y}$, queremos que o vetor residual seja ortogonal a cada coluna de $X$, então $x_j^T(y - \hat{y}) = 0$ para $j = 1:D$ [^5]. Portanto [^5]:

$$
x_j^T(y - \hat{y}) = 0 \Rightarrow X^T(y - Xw) = 0 \Rightarrow w = (X^TX)^{-1}X^Ty
$$

Portanto, nosso valor projetado de $y$ é dado por [^5]:

$$
\hat{y} = X\hat{w} = X(X^TX)^{-1}X^Ty
$$

Isso corresponde a uma projeção ortogonal de $y$ no espaço das colunas de $X$ [^5]. A matriz de projeção $P = X(X^TX)^{-1}X^T$ é chamada de *hat matrix*, pois "coloca o chapéu em y" [^5].

#### Convexidade
Ao discutir os *least squares*, notamos que o NLL tinha uma forma de tigela com um mínimo único [^5]. O termo técnico para funções como essa é *convexo* [^5]. As funções convexas desempenham um papel muito importante no aprendizado de máquina [^5].

### Conclusão
A regressão linear é um algoritmo poderoso e fundamental no campo do aprendizado de máquina [^1]. Sua simplicidade e interpretabilidade o tornam uma ferramenta valiosa para modelar relações entre variáveis e fazer previsões [^1]. Ao entender os conceitos fundamentais, a especificação do modelo e os métodos de estimativa, podemos efetivamente aplicar a regressão linear a uma ampla gama de problemas [^1]. Além disso, as extensões da regressão linear, como a expansão da função de base e os métodos de regularização, aprimoram sua capacidade de modelar relacionamentos complexos e evitar o *overfitting* [^2].

### Referências
[^1]: Capítulo 7, Introdução
[^2]: Capítulo 7, Model specification
[^3]: Capítulo 7, Maximum likelihood estimation (least squares)
[^4]: Capítulo 7, Maximum likelihood estimation (least squares)
[^5]: Capítulo 7, Maximum likelihood estimation (least squares)
<!-- END -->