## Effective Degrees of Freedom in Ridge Regression

### Introdução
O presente capítulo se dedica a explorar o conceito de **graus de liberdade efetivos** (*effective degrees of freedom*) no contexto da **regressão Ridge**. A regressão Ridge, introduzida na seção 7.5 [^225], é uma técnica de regularização que visa mitigar o *overfitting* ao adicionar uma penalidade à magnitude dos coeficientes do modelo. O conceito de graus de liberdade efetivos fornece uma maneira de quantificar a complexidade do modelo resultante, permitindo uma melhor compreensão do *trade-off* entre *bias* e variância.

### Conceitos Fundamentais

Na regressão Ridge, o estimador dos coeficientes ŵ é obtido pela minimização da seguinte função objetivo [^226]:
$$
J(w) = \frac{1}{N} \sum_{i=1}^{N} (y_i - (w_0 + w^T x_i))^2 + \lambda ||w||^2
$$
onde $\lambda \geq 0$ é o parâmetro de regularização, $||w||^2 = w^T w$ é o quadrado da norma $l_2$ dos coeficientes (excluindo o termo de *offset* $w_0$), e $N$ é o número de amostras. A solução para este problema de otimização é dada por [^226]:
$$
\hat{w}_{ridge} = (X^T X + \lambda I_D)^{-1} X^T y
$$
onde $X$ é a matriz de *design*, $y$ é o vetor de respostas, e $I_D$ é a matriz identidade de dimensão $D$, sendo $D$ o número de *features*.

Para analisar a complexidade do modelo, define-se os **graus de liberdade efetivos** como [^229]:
$$
dof(\lambda) = \sum_{j=1}^{D} \frac{\sigma_j^2}{\sigma_j^2 + \lambda}
$$
onde $\sigma_j$ são os valores singulares da matriz $X$. Esta métrica quantifica o número de parâmetros efetivamente utilizados pelo modelo. Quando $\lambda = 0$ (sem regularização), $dof(\lambda) = D$, indicando que todos os $D$ parâmetros são utilizados. À medida que $\lambda$ aumenta, $dof(\lambda)$ diminui, refletindo a simplificação do modelo e a redução da sua complexidade [^225].

A intuição por trás dessa definição pode ser compreendida ao considerar a decomposição em valores singulares (SVD) da matriz $X$, dada por $X = USV^T$ [^228]. As predições do modelo Ridge no conjunto de treinamento podem ser expressas como [^228]:
$$
\hat{y} = X \hat{w}_{ridge} = U \Sigma U^T y = \sum_{j=1}^{D} u_j u_j^T y
$$
onde $\Sigma_{jj} = \frac{\sigma_j^2}{\sigma_j^2 + \lambda}$. Se $\sigma_j^2$ é pequeno comparado a $\lambda$, então a direção $u_j$ terá pouco efeito na predição [^229].

**Relação com a Regularização:**
A regressão Ridge, ao adicionar a penalidade $\lambda ||w||^2$, efetivamente "encolhe" os coeficientes do modelo em direção a zero. Isso tem o efeito de reduzir a variância do modelo, tornando-o menos sensível a flutuações nos dados de treinamento. No entanto, essa redução na variância geralmente vem acompanhada de um aumento no *bias*, pois o modelo se torna menos capaz de capturar a verdadeira relação subjacente nos dados [^226].

**Comportamento Assintótico:**
Quando $\lambda = 0$, $dof(\lambda) = D$, indicando que o modelo utiliza todos os graus de liberdade disponíveis, o que pode levar ao *overfitting*. À medida que $\lambda \rightarrow \infty$, $dof(\lambda) \rightarrow 0$, indicando que o modelo se torna extremamente simples, tendendo a uma função constante. Este cenário extremo leva ao *underfitting*, onde o modelo é incapaz de capturar a complexidade dos dados [^229].

### Conclusão

O conceito de graus de liberdade efetivos é uma ferramenta valiosa para entender o impacto da regularização na complexidade do modelo na regressão Ridge. Ao quantificar o número de parâmetros efetivamente utilizados, ele auxilia na escolha de um valor apropriado para o parâmetro de regularização $\lambda$, permitindo um melhor equilíbrio entre *bias* e variância. A análise dos graus de liberdade efetivos complementa a compreensão teórica da regressão Ridge e fornece *insights* práticos para a construção de modelos mais robustos e generalizáveis.

### Referências
[^225]: Seção 7.5 do texto original.
[^226]: Seção 7.5 do texto original.
[^228]: Seção 7.5.3 do texto original.
[^229]: Seção 7.5 do texto original.
<!-- END -->