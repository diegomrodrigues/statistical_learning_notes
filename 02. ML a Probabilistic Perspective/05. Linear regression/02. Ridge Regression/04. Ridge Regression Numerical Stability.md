## Estabilidade Numérica em Ridge Regression
### Introdução
A **Ridge Regression**, também conhecida como *penalized least squares* [^226], é uma técnica utilizada para mitigar o *overfitting* em modelos de regressão linear [^225]. Uma das vantagens da Ridge Regression, além de sua capacidade de melhorar a generalização do modelo, é a sua **estabilidade numérica** [^227]. Este capítulo explora em detalhes como a Ridge Regression alcança essa estabilidade e as implicações práticas dessa característica.

### Conceitos Fundamentais
A **estabilidade numérica** em Ridge Regression surge da adição de um termo de penalização $l_2$ à função de custo do modelo [^226]. Matematicamente, o objetivo da Ridge Regression é minimizar a seguinte expressão [^226]:
$$ J(w) = \frac{1}{N} \sum_{i=1}^{N} (y_i - (w_0 + w^T x_i))^2 + \lambda ||w||^2 $$
onde:
- $N$ é o número de amostras.
- $y_i$ é o valor da variável dependente para a amostra $i$.
- $x_i$ é o vetor de características para a amostra $i$.
- $w$ é o vetor de pesos (coeficientes) a ser otimizado.
- $w_0$ é o termo de *offset*.
- $\lambda$ é o parâmetro de regularização, que controla a força da penalização.
- $||w||^2 = w^T w$ é o quadrado da norma $l_2$ do vetor de pesos.

A solução para o vetor de pesos $w$ que minimiza $J(w)$ é dada por [^226]:
$$ \hat{w}_{ridge} = (\lambda I_D + X^T X)^{-1} X^T y $$
onde:
- $X$ é a matriz de características.
- $y$ é o vetor de variáveis dependentes.
- $I_D$ é a matriz identidade de dimensão $D$, onde $D$ é o número de características.

A chave para a estabilidade numérica reside no termo $\lambda I_D$ adicionado à matriz $X^T X$. A matriz $X^T X$ é a **matriz de covariância das características**, e a adição de $\lambda I_D$ garante que a matriz $(\lambda I_D + X^T X)$ seja **melhor condicionada** [^227].

**Definição:** Uma matriz é *bem condicionada* se o seu número de condição (a razão entre o maior e o menor valor singular) é pequeno. Matrizes mal condicionadas são difíceis de inverter numericamente, pois pequenos erros nos dados podem levar a grandes erros na solução.

A adição de $\lambda I_D$ tem o efeito de aumentar os valores próprios da matriz $X^T X$, o que reduz o seu número de condição e a torna mais fácil de inverter numericamente [^227].

**Técnica da Decomposição de Cholesky:**
Para tornar a computação ainda mais estável, a Ridge Regression pode ser implementada usando a **decomposição de Cholesky** [^227]. Essa técnica envolve aumentar os dados originais com "dados virtuais" provenientes do *prior* [^227].

Primeiro, assume-se que os pesos $w$ seguem uma distribuição normal com média zero e matriz de precisão $\Lambda$ [^227], ou seja, $p(w) = N(0, \Lambda^{-1})$ [^227]. No caso da Ridge Regression, $\Lambda = (1/\tau^2)I$ [^227].

Em seguida, os dados originais são aumentados da seguinte forma [^227]:
$$ \tilde{X} = \begin{pmatrix} X \\\\ \sqrt{\Lambda} \end{pmatrix}, \quad \tilde{y} = \begin{pmatrix} y \\\\ 0_{Dx1} \end{pmatrix} $$
onde $\sqrt{\Lambda}$ é a decomposição de Cholesky de $\Lambda$.

Com essa formulação, a minimização da *negative log-likelihood (NLL)* penalizada nos dados expandidos é equivalente à Ridge Regression [^227]. Isso pode ser demonstrado da seguinte forma [^227]:
$$ f(w) = (\tilde{y} - \tilde{X}w)^T (\tilde{y} - \tilde{X}w) = (y - Xw)^T (y - Xw) + w^T \Lambda w $$
Essa abordagem tem a vantagem de evitar a inversão direta da matriz $(\lambda I_D + X^T X)$, o que resulta em uma computação mais robusta e estável [^227].

**Singular Value Decomposition (SVD)**
Em casos onde o número de features $D$ é muito maior que o número de exemplos $N$, pode ser mais eficiente realizar uma Singular Value Decomposition (SVD) primeiro [^228]. Se $X = USV^T$ é a SVD de $X$, então a solução para Ridge Regression pode ser reescrita como [^228]:

$$ \hat{w}_{ridge} = V(S^2 + \lambda I)^{-1} S U^T y $$

Essa abordagem reduz a complexidade computacional e também pode melhorar a estabilidade numérica [^228].

### Conclusão
A Ridge Regression não apenas melhora a generalização de modelos de regressão linear, mas também oferece vantagens significativas em termos de **estabilidade numérica** [^227]. A adição de um termo de regularização $l_2$ à função de custo garante que a matriz a ser invertida seja melhor condicionada, e técnicas como a decomposição de Cholesky e SVD podem ser utilizadas para tornar a computação ainda mais robusta [^227, 228].

### Referências
[^226]: Capítulo 7, Linear regression
[^227]: Seção 7.5.2, Numerically stable computation
[^228]: Seção 7.5.3, Connection with PCA
<!-- END -->