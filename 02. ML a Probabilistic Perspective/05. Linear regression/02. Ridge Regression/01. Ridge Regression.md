## Ridge Regression: Regularization and MAP Estimation

### Introdução
Este capítulo explora a **Ridge Regression**, uma técnica de regularização essencial no contexto da regressão linear [^225]. Regularização é crucial para evitar o *overfitting*, um problema comum em modelos complexos que se ajustam excessivamente aos dados de treinamento, resultando em baixa capacidade de generalização [^225]. A Ridge Regression alcança isso adicionando um termo de penalidade à função de custo, que penaliza a magnitude dos coeficientes do modelo [^225]. Este capítulo detalha a formulação matemática da Ridge Regression e sua interpretação como um caso especial da **Maximum a Posteriori (MAP) estimation** com um prior Gaussiano [^225].

### Conceitos Fundamentais
A Ridge Regression é uma técnica utilizada para prevenir o *overfitting* em regressão linear, adicionando um termo de penalidade à função de custo [^225]. A função de custo a ser minimizada na Ridge Regression é dada por:

$$
J(w) = \frac{1}{N} \sum_{i=1}^{N} (y_i - (w_0 + w^T x_i))^2 + \lambda ||w||^2
$$

onde:
- $N$ é o número de amostras no conjunto de treinamento.
- $y_i$ é o valor alvo para a i-ésima amostra.
- $x_i$ é o vetor de características para a i-ésima amostra.
- $w$ é o vetor de coeficientes (pesos) a ser otimizado.
- $w_0$ é o termo de *offset* (bias).
- $\lambda$ é o parâmetro de regularização, que controla a força da penalidade.
- $||w||^2 = w^T w$ é o quadrado da norma L2 do vetor de coeficientes, também conhecido como *squared two-norm* [^226].

O termo $\lambda ||w||^2$ é a penalidade de regularização, que força os coeficientes a serem menores, evitando assim que o modelo se ajuste excessivamente aos ruídos nos dados de treinamento [^226]. O parâmetro $\lambda$ controla a intensidade da regularização: quanto maior o valor de $\lambda$, maior a penalidade sobre os coeficientes, resultando em um modelo mais simples [^226].

**Interpretação Bayesiana (MAP Estimation)**

A Ridge Regression pode ser interpretada como um caso especial da **Maximum a Posteriori (MAP) estimation** com um prior Gaussiano sobre os pesos [^225]. Na MAP estimation, buscamos o valor dos parâmetros que maximiza a probabilidade *a posteriori*, dada pelos dados observados:

$$
w_{MAP} = \arg \max_w p(w|D)
$$

Usando o teorema de Bayes, podemos escrever:

$$
p(w|D) = \frac{p(D|w)p(w)}{p(D)}
$$

onde:
- $p(D|w)$ é a *likelihood* dos dados, assumindo uma distribuição Gaussiana para os erros [^225].
- $p(w)$ é o *prior* sobre os pesos, que na Ridge Regression é uma distribuição Gaussiana centrada em zero:

$$
p(w) = \prod_j N(w_j|0, \tau^2)
$$

onde $\tau^2$ controla a variância dos pesos. Isso implica que os pesos tendem a ser pequenos, o que resulta em uma curva mais suave [^225]. O *prior* Gaussiano penaliza valores grandes para os coeficientes, o que é equivalente a adicionar a penalidade L2 na função de custo da Ridge Regression [^226].

Tomando o logaritmo da probabilidade *a posteriori* e maximizando-o, obtemos a mesma solução que minimizamos a função de custo da Ridge Regression. A relação entre o parâmetro de regularização $\lambda$ e a variância do *prior* $\tau^2$ é dada por $\lambda = \sigma^2/\tau^2$, onde $\sigma^2$ é a variância do ruído nos dados [^226].

**Solução da Ridge Regression**

A solução para a Ridge Regression pode ser obtida analiticamente [^226]. Minimizar a função de custo $J(w)$ em relação a $w$ leva a:

$$
\hat{w}_{ridge} = (X^T X + \lambda I_D)^{-1} X^T y
$$

onde:
- $X$ é a matriz de design, com cada linha representando uma amostra de entrada.
- $I_D$ é a matriz identidade de dimensão $D$, onde $D$ é o número de características [^226].

Essa solução mostra que a Ridge Regression adiciona uma matriz diagonal $\lambda I_D$ à matriz $X^T X$, o que torna a matriz resultante mais bem condicionada e mais fácil de inverter [^227]. Isso é particularmente útil quando $X^T X$ é quase singular, o que pode ocorrer quando há multicolinearidade entre as características [^227].

**Estabilidade Numérica**

A Ridge Regression não só melhora a capacidade de generalização, mas também a estabilidade numérica [^227]. A solução da Ridge Regression é mais estável numericamente do que a solução de mínimos quadrados ordinários (OLS), especialmente quando a matriz $X^T X$ está mal condicionada [^227].

**Decomposição QR e SVD**
Para melhorar ainda mais a estabilidade numérica e a eficiência computacional, a solução da Ridge Regression pode ser calculada usando a decomposição QR ou a decomposição em valores singulares (SVD) [^228].

**Decomposição QR:**
Seja $X = QR$ a decomposição QR de $X$, onde $Q$ é uma matriz ortogonal e $R$ é uma matriz triangular superior [^228]. Então, a solução da Ridge Regression pode ser escrita como:

$$
\hat{w}_{ridge} = R^{-1} Q^T y
$$

Como $R$ é triangular superior, a inversão é computacionalmente eficiente e numericamente estável [^228].

**Decomposição SVD:**
Seja $X = USV^T$ a decomposição SVD de $X$, onde $U$ e $V$ são matrizes ortogonais e $S$ é uma matriz diagonal contendo os valores singulares de $X$ [^228]. Então, a solução da Ridge Regression pode ser escrita como:

$$
\hat{w}_{ridge} = V(S^2 + \lambda I)^{-1} S U^T y
$$

Essa abordagem é particularmente útil quando o número de características $D$ é muito maior do que o número de amostras $N$ [^228].

**Conexão com PCA**

Existe uma conexão interessante entre a Ridge Regression e a Análise de Componentes Principais (PCA) [^228]. A PCA é uma técnica de redução de dimensionalidade que identifica as direções de maior variância nos dados [^228]. A Ridge Regression tende a encolher os coeficientes associados às direções de menor variância, que são mais propensas a conter ruído [^229]. Isso pode ser visto na solução da Ridge Regression usando a decomposição SVD, onde os valores singulares menores são mais fortemente penalizados [^229].

### Conclusão
A Ridge Regression é uma técnica poderosa para prevenir o *overfitting* em regressão linear, fornecendo uma solução mais estável e generalizável [^225]. Sua interpretação como MAP estimation com um prior Gaussiano oferece uma perspectiva bayesiana que ajuda a entender seu comportamento [^225]. As técnicas de decomposição QR e SVD permitem calcular a solução de forma eficiente e numericamente estável [^228]. A conexão com a PCA fornece *insights* adicionais sobre como a Ridge Regression lida com a variância nos dados [^228]. Em resumo, a Ridge Regression é uma ferramenta fundamental no *toolkit* de qualquer cientista de dados ou engenheiro de *machine learning*.

### Referências
[^225]: Capítulo 7, Linear Regression
[^226]: Seção 7.5, Ridge Regression
[^227]: Seção 7.5.2, Numerically stable computation *
[^228]: Seção 7.5.3, Connection with PCA *
[^229]: Figura 7.9, Geometry of ridge regression

<!-- END -->