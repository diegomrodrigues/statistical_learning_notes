## Ridge Regression Estimator: Derivação e Propriedades
### Introdução
Este capítulo explora em profundidade o estimador de **Ridge Regression**, uma técnica fundamental para lidar com o *overfitting* em modelos de regressão linear. Ridge Regression introduz uma penalidade *L2* na função de perda, resultando em estimativas de coeficientes menores e modelos mais generalizáveis. Analisaremos a derivação do estimador, suas propriedades de estabilidade numérica e conexões com outros métodos de regularização [^225].

### Conceitos Fundamentais

O estimador de Ridge Regression, denotado por $\hat{w}_{ridge}$, é dado pela seguinte fórmula [^1]:
$$\hat{w}_{ridge} = (\lambda I + X^T X)^{-1} X^T y$$
onde:
*   $X$ é a matriz de características
*   $y$ é o vetor de variáveis dependentes
*   $\lambda$ é o parâmetro de regularização (não negativo)
*   $I$ é a matriz identidade

A derivação deste estimador surge da minimização da função de perda *L2*-penalizada [^1]:
$$J(w) = \frac{1}{N} \sum_{i=1}^{N} (y_i - (w_0 + w^T x_i))^2 + \lambda ||w||^2$$
onde $||w||^2 = w^T w$ é o quadrado da norma *L2* dos coeficientes.

**Derivação:**

Para minimizar $J(w)$, calculamos o gradiente em relação a $w$ e igualamos a zero. Primeiro, reescrevemos $J(w)$ sem o termo constante $w_0$ por simplicidade, assumindo que os dados foram centrados [^226]:
$$J(w) = \frac{1}{N} \sum_{i=1}^{N} (y_i - w^T x_i)^2 + \lambda ||w||^2$$
Expandindo e utilizando notação matricial:
$$J(w) = \frac{1}{N} (y - Xw)^T (y - Xw) + \lambda w^T w$$
O gradiente de $J(w)$ é:
$$\nabla J(w) = \frac{2}{N} (X^T Xw - X^T y) + 2\lambda w$$
Igualando o gradiente a zero:
$$\frac{2}{N} (X^T Xw - X^T y) + 2\lambda w = 0$$
$$X^T Xw - X^T y + N\lambda w = 0$$
$$(X^T X + N\lambda I)w = X^T y$$
$$w = (X^T X + N\lambda I)^{-1} X^T y$$
Note que, na prática, o fator $N$ é frequentemente absorvido em $\lambda$, resultando na forma mais comum do estimador [^1]:
$$\hat{w}_{ridge} = (\lambda I + X^T X)^{-1} X^T y$$
$\blacksquare$

**Efeito da Regularização:**

A adição de $\lambda I$ a $X^T X$ tem dois efeitos importantes [^1]:

1.  *Redução da Complexidade do Modelo*: O termo de penalidade $\lambda ||w||^2$ força os coeficientes a serem menores, *shrinkando* as estimativas em direção a zero. Isso reduz a variância do modelo, tornando-o menos sensível aos dados de treinamento e melhorando sua capacidade de generalização.
2.  *Melhora da Estabilidade Numérica*: A matriz $X^T X$ pode ser mal condicionada, especialmente quando há multicolinearidade entre as características. A adição de $\lambda I$ melhora o condicionamento da matriz, tornando-a mais fácil de inverter e fornecendo uma solução mais estável. Uma matriz bem condicionada significa que pequenos ruídos nos dados não levarão a grandes mudanças na solução [^1].

**Conexão com MAP Estimation:**

O estimador de Ridge Regression pode ser interpretado como uma estimativa de *Maximum a Posteriori* (MAP) sob um prior Gaussiano com média zero [^225]:
$$p(w) = \prod_{j} N(w_j | 0, \tau^2)$$
onde $\frac{1}{\tau^2}$ controla a força do prior. O problema de estimativa MAP correspondente é [^225]:
$$\underset{w}{\text{argmax}} \sum_{i=1}^{N} \log N(y_i | w_0 + w^T x_i, \sigma^2) + \sum_{j=1}^{D} \log N(w_j | 0, \tau^2)$$
A solução deste problema é precisamente o estimador de Ridge Regression, com $\lambda = \frac{\sigma^2}{\tau^2}$.

**Estabilidade Numérica:**

Ridge Regression não apenas melhora o desempenho estatístico, mas também a estabilidade numérica [^227]. A matriz $(\lambda I + X^T X)$ é mais bem condicionada do que $X^T X$, tornando-a mais fácil de inverter numericamente. No entanto, a inversão direta de matrizes deve ser evitada sempre que possível.

Um truque útil para ajustar modelos de Ridge Regression de forma numericamente robusta envolve aumentar os dados originais com dados "virtuais" provenientes do prior. Definimos [^227]:
$$\tilde{X} = \begin{pmatrix} X \\\\ \Lambda^{1/2} \end{pmatrix}, \quad \tilde{y} = \begin{pmatrix} y \\\\ 0_{D \times 1} \end{pmatrix}$$
onde $\Lambda$ é a matriz de precisão do prior. No caso de Ridge Regression, $\Lambda = (1/\tau^2)I$. Pode-se mostrar que minimizar o erro quadrático médio nos dados aumentados é equivalente a minimizar a função de perda *L2*-penalizada nos dados originais [^227].

**Complexidade e Escolha de $\lambda$:**

A escolha do parâmetro $\lambda$ é crucial. Um $\lambda$ muito grande leva a um modelo muito simples (underfitting), enquanto um $\lambda$ muito pequeno leva a um modelo complexo (overfitting). Métodos comuns para escolher $\lambda$ incluem validação cruzada [^226] e otimização da *marginal likelihood* (evidence procedure) [^238]. A validação cruzada envolve dividir os dados em conjuntos de treinamento e validação, treinar o modelo com diferentes valores de $\lambda$ e escolher o valor que minimiza o erro no conjunto de validação [^226]. A *evidence procedure* envolve maximizar a probabilidade marginal dos dados em relação a $\lambda$ [^238].

**Conexão com PCA:**

Existe uma conexão interessante entre Ridge Regression e *Principal Component Analysis* (PCA) [^228]. Se $X = USV^T$ é a *Singular Value Decomposition* (SVD) de $X$, então o estimador de Ridge Regression pode ser escrito como:
$$\hat{w}_{ridge} = V(S^2 + \lambda I)^{-1} S U^T y$$
As predições de Ridge Regression no conjunto de treinamento são dadas por [^228]:
$$\hat{y} = X \hat{w}_{ridge} = U S V^T V (S^2 + \lambda I)^{-1} S U^T y = U S (S^2 + \lambda I)^{-1} S U^T y$$
Se $\sigma_j^2$ é pequeno comparado a $\lambda$, então a direção $u_j$ terá pouco efeito na predição. Isso significa que Ridge Regression encolhe as direções com menor variância nos dados, que são as direções mais propensas a serem afetadas pelo ruído [^229].

### Conclusão
Ridge Regression é uma técnica poderosa para regularizar modelos de regressão linear, reduzindo a complexidade do modelo e melhorando a estabilidade numérica. Ao introduzir uma penalidade *L2* na função de perda, Ridge Regression *shrinka* os coeficientes em direção a zero, levando a uma melhor generalização. A escolha apropriada do parâmetro de regularização $\lambda$ é crucial para o desempenho do modelo, e métodos como validação cruzada e otimização da *marginal likelihood* podem ser usados para selecionar um valor adequado. A conexão com PCA fornece uma visão adicional sobre como Ridge Regression funciona, mostrando que ele encolhe as direções com menor variância nos dados [^229].

### Referências
[^1]: Capítulo sobre Ridge Regression.
[^225]: Seção 7.5 do capítulo.
[^226]: Seção 7.5, Figura 7.8.
[^227]: Seção 7.5.2 do capítulo.
[^228]: Seção 7.5.3 do capítulo.
[^229]: Seção 7.5.3 do capítulo.
[^238]: Seção 7.6.4 do capítulo.
<!-- END -->