## Robustez em Regressão Linear: Tratamento de Outliers

### Introdução
Em regressão linear, a **robustez** refere-se à capacidade do modelo de manter a precisão e a estabilidade na presença de *outliers* ou ruído nos dados [^1]. A regressão linear robusta aborda a sensibilidade dos mínimos quadrados ordinários a *outliers* usando funções de perda menos sensíveis a valores extremos [^1].

### Conceitos Fundamentais
A regressão linear, como vimos em [^1], é um modelo da forma:
$$\
p(y|x, \theta) = \mathcal{N}(y|w^Tx, \sigma^2)
$$
onde $y$ é a variável dependente, $x$ é a variável independente, $w$ são os pesos e $\sigma^2$ é a variância do ruído.  A estimação de máxima verossimilhança (MLE), ou mínimos quadrados, busca minimizar a soma dos erros quadrados residuais (RSS), definida como [^2]:
$$\
RSS(w) = \sum_{i=1}^{N} (y_i - w^Tx_i)^2
$$
O método de mínimos quadrados é sensível a *outliers* porque penaliza desvios quadraticamente, de modo que pontos distantes da linha de regressão têm um impacto desproporcional no ajuste [^7].

Para alcançar robustez, uma abordagem é substituir a distribuição Gaussiana por uma distribuição com **caudas pesadas**. Uma possibilidade é usar a distribuição de Laplace, introduzida na Seção 2.4.3, resultando na seguinte verossimilhança [^7]:
$$\
p(y|x, w, b) = \text{Lap}(y|w^Tx, b) \propto \exp\left(-\frac{1}{b} |y - w^Tx|\right)
$$
A robustez surge do uso de $|y - w^Tx|$ em vez de $(y - w^Tx)^2$ [^7].  Assumindo $b$ fixo e definindo o resíduo como $r_i = y_i - w^Tx_i$, a função de perda (NLL) tem a forma [^7]:
$$\
l(w) = \sum_i |r_i(w)|
$$
Esta função de perda corresponde à norma $L_1$.

Outra alternativa é minimizar a função de perda de **Huber** (Huber, 1964), definida como [^8]:
$$\
L_H(r, \delta) = \begin{cases}
r^2/2 & \text{se } |r| \le \delta \\
\delta |r| - \delta^2/2 & \text{se } |r| > \delta
\end{cases}
$$
A função de perda de Huber se comporta como $L_2$ para erros pequenos (menores que $\delta$) e como $L_1$ para erros grandes (maiores que $\delta$) [^8]. A vantagem desta função de perda é que ela é diferenciável em todos os pontos [^8].

A minimização da NLL com a distribuição de Laplace resulta em um problema de otimização não-linear, que pode ser convertido em um problema de programação linear (LP) usando o truque da variável dividida. Primeiro, definimos [^8]:
$$\
r_i = r_i^+ - r_i^-
$$
e impomos as restrições $r_i^+ \ge 0$ e $r_i^- \ge 0$ [^8]. O problema de otimização torna-se [^8]:
$$\
\min_{w, r^+, r^-} \sum_i (r_i^+ + r_i^-) \quad \text{s.t.} \quad r_i^+ \ge 0, r_i^- \ge 0, w^Tx_i + r_i^+ - r_i^- = y_i
$$
Este é um problema de programação linear com $D + 2N$ incógnitas e $3N$ restrições, que pode ser resolvido usando solvers LP [^8].

### Conclusão
A regressão linear robusta oferece alternativas valiosas ao método de mínimos quadrados quando os dados contêm *outliers*. Ao utilizar funções de perda menos sensíveis a valores extremos, como a norma $L_1$ (distribuição de Laplace) ou a função de perda de Huber, é possível obter modelos mais estáveis e precisos. A escolha da função de perda e do método de otimização dependerá das características específicas dos dados e dos requisitos de desempenho computacional.

### Referências
[^1]: Seção "Robustness in linear regression" na introdução.
[^2]: Equação 7.9
[^7]: Seção 7.4
[^8]: Seção 7.4, Huber loss function

<!-- END -->