{
  "topics": [
    {
      "topic": "Linear Regression",
      "sub_topics": [
        "Linear regression is a foundational supervised machine learning algorithm used for modeling the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. Often described as the 'work horse' of statistics and machine learning due to its simplicity and interpretability, it predicts a continuous target variable based on a linear combination of input features.",
        "The core idea is to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the difference between the predicted and actual values of the dependent variable, typically using the least squares method. This involves projecting the target vector onto the linear subspace spanned by the input features, where the optimal solution corresponds to the point in the subspace that is closest to the target vector in terms of Euclidean distance.",
        "Linear regression can be extended to model non-linear relationships through basis function expansion, where the input features are transformed using non-linear functions (e.g., polynomials, kernels) before applying the linear model. By replacing the input variables \\\\(x\\\\) with a non-linear function \\\\(\\\\phi(x)\\\\), the model becomes \\\\( p(y|x, \\\\theta) = N(y|w^T\\\\phi(x), \\\\sigma^2) \\\\), maintaining linearity in the parameters \\\\(w\\\\) while capturing complex data patterns. Polynomial basis functions, such as \\\\(\\\\phi(x) = [1, x, x^2, ..., x^d]\\\\), are a common example, where increasing \\\\(d\\\\) allows for modeling increasingly complex functions.",
        "Maximum likelihood estimation (MLE) is a common method for estimating the parameters of a linear regression model by finding the parameter values that maximize the likelihood of observing the given data. This is achieved by maximizing the likelihood function \\\\( p(D|\\\\theta) \\\\), or equivalently, maximizing the log-likelihood \\\\(\\\\arg \\\\max_{\\\\theta} \\\\log p(D|\\\\theta) \\\\). In linear regression with Gaussian noise, this is equivalent to minimizing the residual sum of squares (RSS).",
        "The negative log-likelihood (NLL) provides an alternative to MLE by minimizing the negative logarithm of the likelihood function, expressed as \\\\( NLL(\\\\theta) = -\\\\sum_{i=1}^{N} \\\\log p(y_i|x_i, \\\\theta) \\\\). Minimizing the NLL is equivalent to maximizing the log-likelihood and is particularly useful because many optimization algorithms are designed to minimize functions. For linear regression, the NLL can be expressed as \\\\(NLL(\\\\theta) = \\\\frac{1}{2\\\\sigma^2} RSS(w) + \\\\frac{N}{2} \\\\log(2\\\\pi\\\\sigma^2)\\\\), where \\\\(RSS(w) = \\\\sum_{i=1}^{N} (y_i - w^Tx_i)^2\\\\).",
        "The residual sum of squares (RSS), defined as \\\\( RSS(w) = \\\\sum_{i=1}^{N} (y_i - w^T x_i)^2 \\\\), also known as the sum of squared errors (SSE), quantifies the difference between the observed and predicted values. Minimizing RSS is a common objective in fitting the model to the data.",
        "Ordinary Least Squares (OLS) solution, derived from the normal equation, provides a closed-form solution for linear regression parameters by minimizing the sum of squared differences between observed and predicted values. The normal equation, \\\\(X^TXw = X^Ty\\\\), leads to the ordinary least squares (OLS) solution \\\\(\\\\hat{w}_{OLS} = (X^TX)^{-1}X^Ty\\\\), offering an efficient and direct method for parameter estimation.",
        "Convexity ensures that the negative log-likelihood (NLL) has a unique minimum, making it easier to find the global optimum. A function \\\\(f(\\\\theta)\\\\) is convex if for any \\\\(\\\\theta, \\\\theta' \\\\in S\\\\), and for any \\\\(\\\\lambda \\\\in [0, 1]\\\\), \\\\(f(\\\\lambda\\\\theta + (1 - \\\\lambda)\\\\theta\") \\\\leq \\\\lambda f(\\\\theta) + (1 - \\\\lambda)f(\\\\theta\")."
      ]
    },
    {
      "topic": "Ridge Regression",
      "sub_topics": [
        "Ridge regression is a regularization technique used to prevent overfitting in linear regression by adding a penalty term proportional to the squared magnitude of the coefficients, shrinking the coefficients towards zero and reducing the model's complexity. It can be interpreted as a maximum a posteriori (MAP) estimation with a zero-mean Gaussian prior on the weights, \\\\( p(w) = \\\\prod_j N(w_j|0, \\\\tau^2) \\\\), which encourages smaller parameter values and results in a smoother curve.",
        "L2 regularization, also known as weight decay, is the specific type of regularization used in ridge regression, where the penalty term is the sum of the squares of the coefficients, encouraging smaller coefficient values and preventing individual features from dominating the model. The objective function becomes \\\\(J(w) = \\\\frac{1}{N} \\\\sum_{i=1}^{N} (y_i - (w_0 + w^T x_i))^2 + \\\\lambda ||w||_2^2\\\\), where \\\\(\\\\lambda\\\\) controls the strength of the regularization.",
        "The ridge regression estimator, \\\\(\\\\hat{w}_{ridge} = (\\\\lambda I + X^T X)^{-1} X^T y\\\\), is derived by minimizing the \\\\(L_2\\\\)-penalized loss function and provides a closed-form solution that shrinks the coefficient estimates towards zero, reducing model complexity and improving generalization. The addition of \\\\( \\\\lambda I \\\\) to \\\\( X^T X \\\\) improves the conditioning of the matrix and provides a more stable solution.",
        "Ridge regression improves numerical stability by adding a small constant to the diagonal of the feature covariance matrix, making the matrix better-conditioned and easier to invert, which is crucial for solving the linear regression problem. Numerically stable computation in ridge regression can be achieved using the Cholesky decomposition to augment the original data with 'virtual data' from the prior, resulting in an equivalent penalized negative log-likelihood (NLL) on the expanded data.",
        "The effective degrees of freedom, defined as \\\\(dof(\\\\lambda) = \\\\sum_{j=1}^{D} \\\\frac{\\\\sigma_j^2}{\\\\sigma_j^2 + \\\\lambda}\\\\), quantify the model's complexity in ridge regression, decreasing as \\\\(\\\\lambda\\\\) increases, indicating a simpler model with reduced variance but potentially increased bias.",
        "The connection between ridge regression and principal component analysis (PCA) reveals that ridge regression shrinks the directions with high posterior variance, corresponding to small singular values of the input data matrix, thereby reducing the impact of noisy or irrelevant features on the prediction."
      ]
    },
    {
      "topic": "Bayesian Linear Regression",
      "sub_topics": [
        "Bayesian linear regression computes a full posterior distribution over the model parameters (weights and noise variance), quantifying the uncertainty in the estimates rather than providing only a single point estimate. It allows for a more complete characterization of uncertainty compared to point estimates and improved predictive performance, especially when data is limited.",
        "The posterior distribution in Bayesian linear regression, given by \\\\(p(w|X, y, \\\\sigma^2) = N(w|w_N, V_N)\\\\), is a Gaussian distribution with mean \\\\(w_N\\\\) and covariance matrix \\\\(V_N\\\\), where \\\\(w_N = V_N (V_0^{-1} w_0 + \\\\frac{1}{\\\\sigma^2} X^T y)\\\\) and \\\\(V_N = (V_0^{-1} + \\\\frac{1}{\\\\sigma^2} X^T X)^{-1}\\\\), combining the prior and likelihood information. This is obtained by combining a Gaussian likelihood with a conjugate Gaussian prior, resulting in a posterior that is also Gaussian, initially simplified by assuming the noise variance \\\\( \\\\sigma^2 \\\\) is known.",
        "The posterior predictive distribution, given by \\\\(p(y|x, D, \\\\sigma^2) = N(y|x^T w_N, \\\\sigma^2(x))\\\\), is a Gaussian distribution that quantifies the uncertainty in predictions, with variance \\\\(\\\\sigma^2(x) = \\\\sigma^2 + x^T V_N x\\\\) depending on both the observation noise and the parameter uncertainty. In general, \\\\( p(y|x, D, \\\\sigma^2) = \\\\int N(y|x^T w, \\\\sigma^2) N(w|w_N, V_N) dw \\\\) is a Gaussian with a variance that depends on both the observation noise and the uncertainty in the parameters, providing a more realistic estimate of prediction uncertainty.",
        "Bayesian inference when \\\\( \\\\sigma^2 \\\\) is unknown involves using a conjugate prior that combines a Gaussian for the weights and an inverse Gamma for the variance, denoted as \\\\( p(w, \\\\sigma^2) = NIG(w, \\\\sigma^2 | w_0, V_0, a_0, b_0) \\\\), which allows for analytical computation of the posterior distribution.",
        "Empirical Bayes (EB) methods, also known as evidence procedures, are used to select hyperparameters by maximizing the marginal likelihood, providing an alternative to cross-validation for model selection. The marginal likelihood is defined as \\\\( p(D|m) = \\\\int p(D|w, m) p(w|m, \\\\eta) p(\\\\eta|m) dw d\\\\eta \\\\), where \\\\( \\\\eta \\\\) represents the hyperparameters."
      ]
    },
    {
      "topic": "Robust Linear Regression",
      "sub_topics": [
        "Robustness in linear regression refers to the model's ability to maintain accuracy and stability in the presence of outliers or noise in the data. Robust linear regression addresses the sensitivity of ordinary least squares to outliers by using loss functions less sensitive to extreme values.",
        "Heavy-tailed distributions, such as the Laplace distribution, are used in robust linear regression to assign higher likelihood to outliers, reducing their impact on the model's parameter estimates and improving overall fit compared to standard Gaussian assumptions. The Laplace distribution models the likelihood as \\\\( p(y|x, w, b) \\\\propto \\\\exp(-\\\\frac{1}{b} |y - w^T x|) \\\\), where \\\\(b\\\\) is a scale parameter.",
        "The Huber loss function provides a compromise between squared error loss (L2) for small residuals and absolute error loss (L1) for large residuals, offering robustness to outliers while maintaining differentiability for efficient optimization. It combines the properties of \\\\( l_2 \\\\) and \\\\( l_1 \\\\) loss functions, defined piecewise as \\\\( L_H(r, \\\\delta) = \\\\begin{cases} r^2/2 & \\\\text{if } |r| \\\\leq \\\\delta \\\\\\\\ \\\\delta |r| - \\\\delta^2/2 & \\\\text{if } |r| > \\\\delta \\\\end{cases} \\\\), where \\\\( r \\\\) is the residual and \\\\( \\\\delta \\\\) is a threshold parameter.",
        "Linear programming can be used to optimize the non-linear objective function in robust linear regression by employing a split variable trick, which involves defining \\\\( r_i = r_i^+ - r_i^- \\\\) and imposing constraints \\\\( r_i^+ \\\\geq 0 \\\\) and \\\\( r_i^- \\\\geq 0 \\\\), transforming the problem into a linear objective function subject to linear constraints."
      ]
    }
  ]
}