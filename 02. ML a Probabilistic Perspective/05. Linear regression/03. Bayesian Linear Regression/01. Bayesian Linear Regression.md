## Bayesian Linear Regression: Posterior Distribution and Uncertainty Quantification

### Introdução
Este capítulo aprofunda o conceito de **Bayesian linear regression**, focando especificamente no cálculo da distribuição posterior completa sobre os parâmetros do modelo, ou seja, os *pesos* (weights) e a *variância do ruído* (noise variance) [^1]. Em vez de fornecer apenas uma estimativa pontual única (single point estimate) dos parâmetros, a abordagem Bayesiana quantifica a incerteza nas estimativas, oferecendo uma caracterização mais completa da incerteza e melhorando o desempenho preditivo, especialmente quando os dados são limitados [^1]. Este capítulo se baseará em conceitos fundamentais de regressão linear [^1], explorando a inferência Bayesiana no contexto deste modelo estatístico amplamente utilizado.

### Conceitos Fundamentais

#### Distribuição Posterior Completa
Em Bayesian linear regression, o objetivo é derivar a distribuição posterior $p(\theta | D)$, onde $\theta$ representa os parâmetros do modelo (weights $w$ e variância do ruído $\sigma^2$) e $D$ representa os dados observados. A distribuição posterior combina a *likelihood* dos dados $p(D | \theta)$ com o *prior* sobre os parâmetros $p(\theta)$ via o teorema de Bayes:

$$p(\theta | D) = \frac{p(D | \theta) p(\theta)}{p(D)}$$

onde $p(D) = \int p(D | \theta) p(\theta) d\theta$ é a evidência (evidence) ou probabilidade marginal dos dados.

#### Likelihood e Prior
Em Bayesian linear regression, assume-se geralmente um modelo de *likelihood* Gaussiano [^16]:

$$p(y|X, w, \sigma^2) = N(y|Xw, \sigma^2I_N)$$

onde $y$ é o vetor de observações, $X$ é a matriz de design, $w$ é o vetor de pesos, $\sigma^2$ é a variância do ruído, e $I_N$ é a matriz identidade de tamanho $N$ (número de observações).

Para o *prior* sobre os pesos $w$, é comum utilizar um prior Gaussiano conjugado [^16]:

$$p(w) = N(w|w_0, V_0)$$

onde $w_0$ é a média do prior e $V_0$ é a matriz de covariância do prior. A escolha de um prior conjugado simplifica o cálculo da distribuição posterior, garantindo que a posterior também seja Gaussiana.

#### Cálculo da Distribuição Posterior
Com a escolha de um *prior* Gaussiano conjugado e um modelo de *likelihood* Gaussiano, a distribuição posterior para os pesos $w$ dado os dados $D$ e a variância do ruído $\sigma^2$ também é Gaussiana [^16]:

$$p(w|X, y, \sigma^2) = N(w|w_N, V_N)$$

onde a média posterior $w_N$ e a covariância posterior $V_N$ são dadas por [^16]:

$$V_N = (\sigma^{-2}X^TX + V_0^{-1})^{-1}$$
$$w_N = V_N(\sigma^{-2}X^Ty + V_0^{-1}w_0)$$

Essas equações mostram como a distribuição posterior é atualizada com base nos dados observados e no prior. A covariância posterior $V_N$ representa a incerteza restante nos pesos após observar os dados, enquanto a média posterior $w_N$ é uma combinação ponderada da média do prior $w_0$ e da solução de mínimos quadrados (least squares).

#### Variância do Ruído Desconhecida
Quando a variância do ruído $\sigma^2$ é desconhecida, é necessário especificar um *prior* para $\sigma^2$ e calcular a distribuição posterior conjunta $p(w, \sigma^2 | D)$. Um prior conjugado comum para $\sigma^2$ é a distribuição Inverse Gamma (IG) [^18]:

$$p(\sigma^2) = IG(\sigma^2|a_0, b_0)$$

onde $a_0$ e $b_0$ são os hiperparâmetros do prior. Com este prior, a distribuição posterior conjunta $p(w, \sigma^2 | D)$ é uma distribuição Normal-Inverse-Gamma (NIG) [^18]:

$$p(w, \sigma^2 | D) = NIG(w, \sigma^2|w_N, V_N, a_N, b_N)$$

onde os parâmetros posteriores são atualizados com base nos dados e nos priors.

#### Inferência Preditiva
Uma das vantagens da Bayesian linear regression é a capacidade de fazer inferência preditiva, quantificando a incerteza nas previsões. A distribuição preditiva posterior para uma nova entrada $x_*$ é dada por [^17]:

$$p(y_*|x_*, D) = \int p(y_*|x_*, w, \sigma^2) p(w, \sigma^2|D) dw d\sigma^2$$

Esta integral geralmente não tem uma forma fechada e pode ser aproximada usando métodos de Monte Carlo, como *Markov Chain Monte Carlo* (MCMC).

### Conclusão
Bayesian linear regression fornece uma abordagem completa para modelagem de regressão, quantificando a incerteza nas estimativas dos parâmetros e nas previsões. Ao calcular a distribuição posterior completa sobre os parâmetros do modelo, a abordagem Bayesiana oferece uma caracterização mais rica da incerteza do que as estimativas pontuais. Esta caracterização da incerteza é particularmente útil quando os dados são limitados ou quando a incerteza nas previsões é crítica. A escolha de *priors* conjugados simplifica o cálculo da distribuição posterior, permitindo a aplicação eficiente de Bayesian linear regression em uma ampla gama de problemas.

### Referências
[^1]: Linear regression is the “work horse" of statistics and (supervised) machine learning. When augmented with kernels or other forms of basis function expansion, it can model also non-linear relationships. And when the Gaussian output is replaced with a Bernoulli or multinoulli distribution, it can be used for classification, as we will see below. So it pays to study this model in detail.
[^16]: In linear regression, the likelihood is given by $p(y|X, w, \mu, \sigma^2) = N(y|\mu + Xw, \sigma^2I_N)$. The conjugate prior to the above Gaussian likelihood is also a Gaussian, which we will denote by $p(w) = N(w|wo, Vo)$.
[^17]: In machine learning, we often care more about predictions than about interpreting the parameters. Using Equation 4.126, we can easily show that the posterior predictive distribution at a test point x is also Gaussian.
[^18]: By analogy to Section 4.6.3, one can show that the natural conjugate prior has the following form: $p(w, \sigma^2) = NIG(w, \sigma^2| wo, Vo, ao, bo)$.

<!-- END -->