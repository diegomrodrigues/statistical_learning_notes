## Inferência Bayesiana com Variância Desconhecida em Regressão Linear
### Introdução
Em regressão linear Bayesiana, a inferência sobre os parâmetros do modelo torna-se mais complexa quando a variância do ruído, $\sigma^2$, é desconhecida. Este capítulo detalha o uso de um *prior conjugado* que combina uma distribuição Gaussiana para os pesos e uma distribuição *inversa Gamma* para a variância, permitindo o cálculo analítico da distribuição *a posteriori* [^1].

### Conceitos Fundamentais
Quando $\sigma^2$ é desconhecido, a inferência Bayesiana envolve a utilização de um *prior conjugado* que combina uma Gaussiana para os pesos, $w$, e uma *inversa Gamma* para a variância, $\sigma^2$ [^1]. Este *prior* é denotado como $p(w, \sigma^2) = NIG(w, \sigma^2 | w_0, V_0, a_0, b_0)$, onde:

*   $w_0$ é a média *a priori* dos pesos.
*   $V_0$ é a matriz de covariância *a priori* dos pesos.
*   $a_0$ e $b_0$ são os parâmetros da distribuição *inversa Gamma* para a variância.

A distribuição *Normal-Inversa-Gamma* (NIG) é definida como o produto de uma distribuição normal condicional para $w$ dado $\sigma^2$ e uma distribuição *inversa Gamma* para $\sigma^2$ [^1]:

$$\np(w, \sigma^2) = p(w | \sigma^2)p(\sigma^2) = \mathcal{N}(w | w_0, \sigma^2V_0) \mathcal{IG}(\sigma^2 | a_0, b_0)$$\n
A utilização deste *prior conjugado* permite obter uma distribuição *a posteriori* que também é uma *Normal-Inversa-Gamma*, o que facilita os cálculos analíticos. A distribuição *a posteriori* é dada por [^1]:

$$\np(w, \sigma^2 | \mathcal{D}) = NIG(w, \sigma^2 | w_N, V_N, a_N, b_N)$$\n
onde os parâmetros *a posteriori* são calculados como [^19]:
$$\begin{aligned}\nV_N &= (V_0^{-1} + X^T X)^{-1} \\\\nw_N &= V_N (V_0^{-1} w_0 + X^T y) \\\\na_N &= a_0 + \frac{N}{2} \\\\nb_N &= b_0 + \frac{1}{2} (w_0^T V_0^{-1} w_0 + y^T y - w_N^T V_N^{-1} w_N)\n\end{aligned}$$\n
Aqui, $X$ é a matriz de desenho, $y$ é o vetor de respostas e $N$ é o número de observações.

#### Conjugate Prior
A escolha de um *prior conjugado* é crucial para manter a tratabilidade analítica na inferência Bayesiana. No caso da regressão linear com variância desconhecida, a distribuição *Normal-Inversa-Gamma* serve como um *prior conjugado* natural devido à sua forma funcional que se alinha com a estrutura da função de verossimilhança Gaussiana [^1]. Esta conjugação garante que a distribuição *a posteriori* pertença à mesma família de distribuições que o *prior*, simplificando assim os cálculos e permitindo a obtenção de soluções analíticas.

#### Derivação da Distribuição Posterior
A derivação da distribuição *a posteriori* envolve a aplicação do Teorema de Bayes, que combina a função de verossimilhança com o *prior conjugado* [^1]. A função de verossimilhança para a regressão linear é dada por:
$$\np(y | X, w, \sigma^2) = \mathcal{N}(y | Xw, \sigma^2 I)$$\n
Ao multiplicar a função de verossimilhança pelo *prior* *Normal-Inversa-Gamma* e completar os quadrados, obtemos a distribuição *a posteriori* também na forma *Normal-Inversa-Gamma*. Os parâmetros da *a posteriori* são atualizados com base nos dados observados, refletindo o aprendizado a partir das evidências [^19].

#### Distribuições Marginais
A partir da distribuição conjunta *a posteriori* $p(w, \sigma^2 | \mathcal{D})$, podemos obter as distribuições marginais para $w$ e $\sigma^2$ integrando em relação à outra variável [^20]. A distribuição marginal para $w$ é uma distribuição *t-Student multivariada*, enquanto a distribuição marginal para $\sigma^2$ é uma distribuição *inversa Gamma*. Estas distribuições marginais fornecem informações valiosas sobre a incerteza associada a cada parâmetro individualmente.

### Conclusão
A inferência Bayesiana em regressão linear com variância desconhecida é elegantemente tratada através do uso de um *prior conjugado* *Normal-Inversa-Gamma*. Esta abordagem permite a obtenção analítica da distribuição *a posteriori* e das distribuições marginais, facilitando a análise e interpretação dos resultados. A escolha do *prior conjugado* simplifica os cálculos e fornece uma estrutura coerente para a incorporação de conhecimento *a priori* no processo de inferência. <!-- END -->

### Referências
[^1]: Página 1, Capítulo 7, "Linear regression"
[^19]: Página 19, Capítulo 7, "Bayesian linear regression"
[^20]: Página 20, Capítulo 7, "Bayesian linear regression"
<!-- END -->