## Backfitting in Generalized Additive Models

### Introdução
Este capítulo aprofunda o conceito de **backfitting** no contexto de **Generalized Additive Models (GAMs)** [^552]. O backfitting é um algoritmo iterativo usado para ajustar os componentes aditivos de um GAM, e compreender seus detalhes é crucial para a aplicação eficaz desses modelos [^552]. Este capítulo explorará a identificabilidade do modelo, a implementação do algoritmo e suas propriedades de convergência [^552].

### Conceitos Fundamentais

Em um GAM, o modelo é expresso como uma soma de funções univariadas, mais uma constante [^552]:
$$
f(x) = a + f_1(x_1) + \dots + f_D(x_D)
$$
onde $f_j$ são funções suaves de cada variável preditora $x_j$, e $a$ é uma constante [^552]. O objetivo é estimar essas funções $f_j$ a partir dos dados.

**Identificabilidade:** Uma questão fundamental é que a constante *a* não é univocamente identificável [^552]. Isso significa que podemos adicionar uma constante a uma função $f_j$ e subtraí-la de *a* sem alterar o valor previsto do modelo. Para resolver este problema, impomos a seguinte restrição [^552]:
$$
\sum_{i=1}^N f_j(x_{ij}) = 0 \quad \text{para todo} \ j
$$
Essa restrição garante que a soma dos valores de cada função $f_j$ sobre os dados seja zero [^552]. Com esta restrição, o estimador de máxima verossimilhança (MLE) para *a* é simplesmente a média das respostas [^552]:
$$
a = \frac{1}{N} \sum_{i=1}^N y_i
$$

**Algoritmo de Backfitting:** Para ajustar o restante do modelo, primeiro centramos as respostas subtraindo *a* [^552]. Em seguida, atualizamos iterativamente cada função $f_j$ usando um *scatterplot smoother* [^552]. O vetor alvo para cada atualização é o vetor de resíduos obtido omitindo o termo $f_j$ [^552]:
$$
f_j := \text{smoother}(\{y_i - \sum_{k \neq j} f_k(x_{ik})\}_{i=1}^N)
$$
Este processo é repetido para cada função $f_j$ em cada iteração até que a convergência seja alcançada [^552].

Após cada atualização, garantimos que a restrição $\sum_{i=1}^N f_j(x_{ij}) = 0$ seja satisfeita, subtraindo a média da função atualizada $f_j$ [^552]:
$$
f_j := f_j - \frac{1}{N} \sum_{i=1}^N f_j(x_{ij})
$$

O algoritmo completo é conhecido como **backfitting algorithm** [^552]. Se a matriz de design X tem rank de coluna completo, então o objetivo acima é convexo (já que cada *smoothing spline* é um operador linear), então este procedimento tem a garantia de convergir para o ótimo global [^552].

**Extensões para GLMs:** No caso de um Modelo Linear Generalizado (GLM), o método precisa ser modificado [^552]. A ideia básica é substituir o passo de mínimos quadrados ponderados do IRLS (Iterative Reweighted Least Squares) por um algoritmo de backfitting ponderado [^552]. Na regressão logística, por exemplo, cada resposta tem um peso $s_i = \mu_i(1 - \mu_i)$ associado, onde $\mu_i = \text{sigm}(a + \sum_{j=1}^D f_j(x_{ij}))$ [^552].

### Conclusão
O algoritmo de backfitting fornece uma abordagem iterativa para ajustar GAMs, permitindo a modelagem flexível de relações não lineares entre preditores e a resposta [^552]. Ao lidar com a identificabilidade do modelo através de restrições e adaptar o algoritmo para GLMs, o backfitting se torna uma ferramenta poderosa no arsenal de modelagem estatística [^552].

### Referências
[^552]: Capítulo 16, página 552
<!-- END -->