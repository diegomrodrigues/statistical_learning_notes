## Multivariate Adaptive Regression Splines (MARS)

### Introdução
Em continuidade ao estudo dos **Generalized Additive Models (GAMs)** [^10], este capítulo explora uma extensão poderosa que permite a modelagem de efeitos de interação entre as variáveis preditoras: as **Multivariate Adaptive Regression Splines (MARS)** [^11]. Como vimos anteriormente, os GAMs permitem modelar relações não lineares entre preditores e a resposta, mas, em sua forma básica, não capturam interações. As MARS superam essa limitação através de uma decomposição ANOVA [^11], que será detalhada a seguir.

### Conceitos Fundamentais

As MARS estendem os GAMs ao permitir efeitos de interação, criando uma decomposição ANOVA da forma [^11]:

$$
f(x) = \beta_0 + \sum_j f_j(x_j) + \sum_{j,k} f_{jk}(x_j, x_k) + \sum_{j,k,l} f_{jkl}(x_j, x_k, x_l) + ...
$$

onde cada termo $f_j(x_j)$ representa o efeito principal da variável $x_j$, $f_{jk}(x_j, x_k)$ o efeito de interação entre $x_j$ e $x_k$, e assim por diante [^11, 20].

A seleção dos termos a serem incluídos no modelo é realizada através de uma **busca gulosa** (greedy search), que decide iterativamente quais variáveis adicionar [^11]. Esse processo constrói o modelo de forma adaptativa, explorando o espaço de possíveis interações.

Para representar as funções de regressão multidimensionais, MARS utiliza uma **base de produto tensorial de *regression splines*** [^11]. Em outras palavras, a função $f(x)$ é expressa como uma combinação linear de funções base que são produtos de *splines* univariadas. No caso de um *input* bidimensional, por exemplo, poderíamos ter [^11]:

$$
f(x_1, x_2) = \beta_0 + \sum_m \beta_{1m} (x_1 - t_{1m})_+ + \sum_m \beta_{2m} (t_{2m} - x_2)_+ + \sum_m \beta_{12m} (x_1 - t_{1m})_+ (t_{2m} - x_2)_+
$$

onde $(x-t)_+ = max(0, x-t)$ é a função *hinge* ou *rectified linear unit (ReLU)* [^11].

Para construir essa função, inicia-se com um conjunto de funções base candidatas da forma [^11]:

$$
C = \{(x_j - t)_+, (t - x_j)_+ : t \in \{x_{1j}, ..., x_{Nj}\}, j = 1, ..., D\}
$$

onde $t$ são os nós (*knots*) e  $\{x_{1j}, ..., x_{Nj}\}$ representam os valores observados da variável $x_j$ [^11]. As funções $(x_j - t)_+$ e $(t - x_j)_+$ formam um **par reflexivo** (*reflecting pair*) [^11], representando *splines* lineares com inclinação para cima em ambas as direções.

O algoritmo MARS começa com um modelo simples, contendo apenas o termo constante $\beta_0$ [^11]. Em seguida, iterativamente, busca o par de funções base e o termo existente no modelo que, quando adicionados, resultam na maior redução do erro residual. Por exemplo, poderíamos começar com [^11]:

$$
f(x) = 25 - 4(x_1 - 5)_+ + 20(5 - x_1)_+
$$

obtido multiplicando $h_0(x) = 1$ com um par reflexivo envolvendo $x_1$ com *knot* $t=5$ [^11].

Após a construção de um modelo grande, o algoritmo realiza uma **poda recursiva para trás** (*pruning*) [^11], removendo os termos que menos contribuem para a precisão do modelo, a fim de evitar *overfitting*. A escolha do melhor subconjunto de termos é geralmente baseada em validação cruzada.

É importante notar a forte relação entre MARS e **Classification and Regression Trees (CART)** [^11]. Se substituirmos as funções base lineares por funções degrau, o algoritmo MARS se torna essencialmente equivalente ao algoritmo de construção de árvores CART.

### Conclusão

As MARS representam uma extensão flexível e poderosa dos GAMs, permitindo a modelagem de interações complexas entre variáveis preditoras [^11]. Através de uma busca gulosa e da utilização de *regression splines*, as MARS constroem modelos adaptativos que podem capturar relações não lineares e interações de alta ordem. Embora a interpretação dos modelos MARS possa ser mais desafiadora do que a dos GAMs simples, a capacidade de modelar interações complexas torna as MARS uma ferramenta valiosa em diversas aplicações.

### Referências
[^11]: Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). *The elements of statistical learning: data mining, inference, and prediction*. New York, NY: Springer.
[^10]: (Referência ao contexto de Generalized Additive Models, caso disponível em tópicos anteriores)
[^20]: (Referência à equação da decomposição ANOVA)
<!-- END -->