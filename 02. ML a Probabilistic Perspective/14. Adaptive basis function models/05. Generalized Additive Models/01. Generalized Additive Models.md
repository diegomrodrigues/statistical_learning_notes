## Generalized Additive Models (GAM)

### Introdução
Este capítulo explora os **Generalized Additive Models (GAMs)**, uma extensão dos modelos lineares generalizados (GLMs) que permitem modelar relações não lineares entre a variável resposta e as variáveis preditoras [^552]. Ao contrário dos modelos lineares, onde a relação entre as variáveis é assumida como linear, os GAMs utilizam funções *smoother* para capturar padrões mais complexos nos dados [^552]. Desta forma, os GAMs oferecem uma flexibilidade maior na modelagem de dados, mantendo a interpretabilidade que é característica dos GLMs.

### Conceitos Fundamentais
Um GAM é definido pela seguinte equação [^552]:
$$
f(x) = \alpha + f_1(x_1) + ... + f_D(x_D)
$$
onde:
*   $f(x)$ é a função que modela a relação entre as variáveis preditoras e a variável resposta.
*   $\alpha$ é o intercepto global.
*   $f_j(x_j)$ são funções *smoother* que modelam a relação não linear entre a variável preditora $x_j$ e a variável resposta. Cada $f_j$ pode ser modelada por um *scatterplot smoother*.
*   $x_j$ representa a j-ésima variável preditora.
*   $D$ é o número total de variáveis preditoras.

Assim como nos GLMs, a função $f(x)$ pode ser mapeada para $p(y|x)$ usando uma função de ligação (link function) [^552].

No contexto de regressão, o objetivo é minimizar a seguinte função de custo [^552]:
$$
J(\alpha, f_1,..., f_D) = \sum_{i=1}^{N} (y_i - \alpha - \sum_{j=1}^{D} f_j(x_{ij}))^2 + \sum_{j=1}^{D} \lambda_j \int (f'_j(t_j))^2 dt_j
$$
onde:
*   $y_i$ é o valor observado da variável resposta para a i-ésima observação.
*   $x_{ij}$ é o valor da j-ésima variável preditora para a i-ésima observação.
*   $\lambda_j$ é o parâmetro de regularização que controla a suavidade da função $f_j$. Quanto maior o valor de $\lambda_j$, mais suave será a função $f_j$. O termo $\lambda_j \int (f'_j(t_j))^2 dt_j$ penaliza a variação excessiva da função $f_j$, promovendo a suavidade.

#### Backfitting
O **backfitting** é um algoritmo iterativo utilizado para ajustar os GAMs [^552]. O algoritmo funciona da seguinte forma:

1.  Inicialize as funções $f_j(x_j)$ com valores arbitrários (por exemplo, zero).
2.  Estime o intercepto $\alpha$ como a média dos valores da variável resposta: $\alpha = \frac{1}{N} \sum_{i=1}^{N} y_i$ [^552].
3.  Itere sobre as funções $f_j(x_j)$ da seguinte forma:
    *   Para cada função $f_j(x_j)$, calcule os resíduos parciais subtraindo as outras funções do valor da variável resposta: $y_i - \sum_{k \neq j} f_k(x_{ik})$ [^552].
    *   Ajuste a função $f_j(x_j)$ aos resíduos parciais usando um *scatterplot smoother* [^552]:
        $$
        f_j := smoother(\{y_i - \sum_{k \neq j} f_k(x_{ik})\}_{i=1}^N)
        $$
    *   Garanta que a saída da função $f_j(x_j)$ tenha média zero:
        $$
        f_j := f_j - \frac{1}{N} \sum_{i=1}^{N} f_j(x_{ij})
        $$
        Isto garante a identificabilidade do modelo [^552].

4.  Repita o passo 3 até que as funções $f_j(x_j)$ convirjam [^552].

Se a matriz de dados $X$ tiver rank completo, o objetivo acima é convexo (já que cada *smoothing spline* é um operador linear), então este procedimento tem garantia de convergência para o ótimo global [^552].

#### Eficiência Computacional
Cada chamada ao *smoother* leva um tempo de $O(N)$, então o custo total é $O(NDT)$, onde $T$ é o número de iterações [^553]. Se tivermos entradas de alta dimensão, o ajuste de um GAM se torna caro [^553]. Uma abordagem é combiná-lo com uma penalidade de esparsidade, veja por exemplo, a abordagem SpAM (*sparse additive model*) [^553]. Alternativamente, podemos usar uma abordagem *greedy*, como *boosting* [^553].

#### Multivariate Adaptive Regression Splines (MARS)
Podemos estender os GAMs permitindo efeitos de interação [^553]. Em geral, podemos criar uma decomposição ANOVA:
$$
f(x) = \beta_0 + \sum_{j=1}^D f_j(x_j) + \sum_{j,k} f_{jk}(x_j, x_k) + \sum_{j,k,l} f_{jkl}(x_j, x_k, x_l) + ...
$$
Obviamente, não podemos permitir muitas interações de ordem superior, pois haverá muitos parâmetros para ajustar [^553].

É comum usar a busca *greedy* para decidir quais variáveis adicionar [^553]. As *multivariate adaptive regression splines* ou algoritmo MARS são um exemplo disso [^553]. Ele ajusta modelos da forma na Equação 16.20, onde usa uma base de produto tensorial de *regression splines* para representar as funções de regressão multidimensionais [^553]. Por exemplo, para entrada 2d, podemos usar
$$
f(x_1,x_2) = \beta_0 + \sum_m \beta_{1m}(x_1 - t_{1m})_+ + \sum_m \beta_{2m}(t_{2m} - x_2)_+ + \sum_m \beta_{12m}(x_1-t_{1m})_+(t_{2m} -x_2)_+
$$

Para criar tal função, começamos com um conjunto de funções de base candidatas da forma
$$
C = \{(x_j - t)_+, (t - x_j)_+ : t \in \{X_{1j},...,X_{Nj}\}, j = 1, ..., D\}
$$
Estas são *ld linear splines* onde os nós estão em todos os valores observados para aquela variável [^553]. Consideramos *splines* inclinadas para cima em ambas as direções; isso é chamado de *par refletor* [^553]. Seja $M$ o conjunto atual de funções de base. Inicializamos usando $M = \{1\}$ [^553]. Consideramos a criação de um novo par de funções de base multiplicando um $h_m \in M$ com um dos pares refletores em $C$ [^553].

### Conclusão
Os GAMs oferecem uma abordagem flexível e interpretável para modelar dados não lineares [^552]. Ao utilizar funções *smoother* para capturar relações complexas, os GAMs conseguem superar as limitações dos modelos lineares, mantendo a capacidade de interpretar o efeito de cada variável preditora na variável resposta [^552]. O algoritmo de *backfitting* fornece um método eficiente para ajustar os modelos, enquanto as técnicas de regularização ajudam a prevenir o *overfitting* [^552].

### Referências
[^552]: Página 552
[^553]: Página 553
<!-- END -->