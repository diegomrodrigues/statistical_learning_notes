## Crescimento de Árvores em Modelos de Funções de Base Adaptativas

### Introdução
Este capítulo explora o processo de crescimento de árvores de decisão, especificamente no contexto de **Classification and Regression Trees (CART)**, e como esses modelos se encaixam no framework mais amplo de **Adaptive Basis Function Models (ABM)** [^1]. Os modelos CART são construídos através da partição recursiva do espaço de entrada, definindo um modelo local em cada região resultante [^2]. O algoritmo de crescimento de árvores busca encontrar a melhor forma de particionar os dados, um problema NP-completo, o que leva ao uso de procedimentos *greedy* para encontrar uma estimativa de máxima verossimilhança (MLE) localmente ótima [^3].

### Conceitos Fundamentais

O crescimento de uma árvore CART envolve a seleção iterativa da melhor variável e valor para dividir os dados, buscando minimizar uma função de custo [^3]. A função de *split* é definida como:

$$(j^*, t^*) = \underset{j \in \{1,...,D\}}{\text{argmin}} \ \underset{t \in T_j}{\text{min}} \ cost(\{x_i, y_i : x_{ij} < t\}) + cost(\{x_i, y_i : x_{ij} > t\})$$

onde $j^*$ é a variável escolhida para a divisão, $t^*$ é o valor de corte para essa variável, e $cost(D)$ é a função de custo associada ao conjunto de dados $D$ [^3].

Para problemas de regressão, a função de custo é definida como a soma dos quadrados dos resíduos:

$$cost(D) = \sum_{i \in D} (y_i - \bar{y})^2$$

onde $\bar{y}$ é a média da variável resposta no conjunto de dados $D$ [^3].

O processo de crescimento da árvore continua recursivamente, particionando os dados até que um dos seguintes critérios de parada seja atingido [^3]:

*   A árvore excede a profundidade máxima permitida.
*   A distribuição da resposta é homogênea (por exemplo, todos os rótulos são da mesma classe).
*   O número de exemplos em um nó é muito pequeno.

A redução no custo (ou ganho) resultante de uma divisão é definida como:

$$Delta = \frac{cost(D) - \left(\frac{|D_L|}{|D|}cost(D_L) + \frac{|D_R|}{|D|}cost(D_R)\right)}{cost(D)}$$

onde $D_L$ e $D_R$ representam os subconjuntos de dados resultantes da divisão à esquerda e à direita, respectivamente [^3]. O processo de crescimento da árvore busca maximizar essa redução no custo a cada etapa.

Em problemas de classificação, a qualidade de uma divisão pode ser medida por várias métricas, incluindo a taxa de má classificação, entropia e o índice de Gini [^3].

**Algoritmos de Crescimento de Árvores:**
Algoritmos como CART, C4.5 e ID3 são implementações populares que escolhem a melhor variável e valor para dividir os dados com base em funções de custo [^3]. O algoritmo 6.1 [^4] apresenta o procedimento recursivo para crescer uma árvore de classificação/regressão.

**Conexão com Adaptive Basis Function Models (ABM):**
Um modelo CART pode ser visto como um caso especial de um ABM, onde as funções de base definem as regiões do espaço de entrada, e os pesos especificam o valor da resposta em cada região [^2]. A equação (16.4) [^2] formaliza essa relação:

$$f(x) = E[y|x] = \sum_{m=1}^{M} w_m I(x \in R_m) = \sum_{m=1}^{M} w_m \phi(x; v_m)$$

onde $R_m$ é a m-ésima região, $w_m$ é a resposta média nessa região, e $v_m$ codifica a variável de divisão e o valor de corte no caminho da raiz até a m-ésima folha.

### Conclusão
O crescimento de árvores de decisão, como implementado em modelos CART, oferece uma abordagem flexível e intuitiva para a modelagem não linear. Ao particionar recursivamente o espaço de entrada e ajustar modelos locais em cada região, os modelos CART podem capturar relações complexas nos dados. A conexão com ABMs fornece uma estrutura teórica para entender como os modelos CART se encaixam em uma classe mais ampla de modelos adaptativos. No entanto, é importante notar que o processo de crescimento da árvore é suscetível a overfitting, e técnicas como poda e validação cruzada são necessárias para construir modelos robustos [^3].

### Referências

[^1]: Page 543 of *Pattern Recognition and Machine Learning*.
[^2]: Page 544 of *Pattern Recognition and Machine Learning*.
[^3]: Page 545 of *Pattern Recognition and Machine Learning*.
[^4]: Page 546 of *Pattern Recognition and Machine Learning*.
<!-- END -->