## Kernel Methods in Adaptive Basis Function Models

### Introdução
Como mencionado na introdução deste capítulo [^1], métodos de kernel são uma ferramenta poderosa para criar modelos não lineares para regressão e classificação. Eles se baseiam na ideia de usar uma função kernel $\kappa(x, x')$ para medir a similaridade entre vetores de dados [^1]. A predição $f(x)$ é expressa como uma função do input $x$ e pesos $w$, com a função kernel $\kappa$ medindo a similaridade entre pontos de dados e protótipos $\mu_k$ [^1]. Em essência, esses modelos realizam um *template matching*, comparando o input $x$ aos protótipos armazenados $\mu_k$ [^1].

### Conceitos Fundamentais
A predição em modelos de kernel assume a forma $f(x) = w^T \phi(x)$, onde $\phi(x)$ é um vetor de funções kernel avaliadas em relação aos protótipos [^1]:
$$ \phi(x) = [\kappa(x, \mu_1), ..., \kappa(x, \mu_N)] $$
Os protótipos $\mu_k$ podem ser todo o conjunto de dados de treinamento ou um subconjunto [^1]. A eficácia desses métodos depende da escolha de uma boa função kernel para medir a similaridade entre os vetores de dados [^1]. A definição de uma função kernel adequada pode ser desafiadora, e o processo pode ser computacionalmente caro [^1].

Uma abordagem para aprender os parâmetros de uma função kernel é maximizar a *marginal likelihood* [^1]. Por exemplo, usando o kernel ARD (Automatic Relevance Determination):\
$$ \kappa(x, x') = \theta_0 \exp \left( - \frac{1}{2} \sum_{j=1}^D \theta_j (x_j - x'_j)^2 \right) $$
Podemos estimar os $\theta_j$ e, assim, realizar uma forma de seleção não linear de features [^1]. No entanto, esses métodos podem ser computacionalmente dispendiosos. Métodos como *multiple kernel learning* usam uma combinação convexa de kernels base, $\kappa(x, x') = \sum_j w_j \kappa_j(x, x')$, e estimam os pesos de mistura $w_j$ [^1]. Essa abordagem também depende de ter bons kernels base e pode ser computacionalmente cara [^1].

Uma alternativa aos métodos de kernel é aprender features úteis $\phi(x)$ diretamente dos dados de entrada, o que leva ao conceito de um modelo de função base adaptativo (ABM) [^1]:
$$ f(x) = w_0 + \sum_{m=1}^M w_m \phi_m(x) $$
Aqui, $\phi_m(x)$ é a *m-ésima* função base, aprendida a partir dos dados [^2]. Tipicamente, as funções base são paramétricas, de modo que podemos escrever $\phi_m(x) = \phi(x; v_m)$, onde $v_m$ são os parâmetros da função base [^2]. O conjunto completo de parâmetros é denotado por $\theta = (w_0, w_{1:M}, \{v_m\}_{m=1}^M)$ [^2]. O modelo resultante não é mais linear nos parâmetros, então só podemos computar uma estimativa MLE ou MAP localmente ótima de $\theta$ [^2].

### Conclusão
Os métodos de kernel oferecem uma abordagem flexível para modelagem não linear, mas a escolha e otimização da função kernel são cruciais e podem ser computacionalmente desafiadoras [^1]. Modelos de função base adaptativos (ABM) fornecem uma alternativa, aprendendo diretamente features úteis dos dados [^2]. Ambos os métodos têm suas vantagens e desvantagens e a escolha entre eles depende do problema em questão e dos recursos computacionais disponíveis [^1, 2].

### Referências
[^1]: Capítulo 16, Seção 16.1
[^2]: Capítulo 16, Seção 16.1

<!-- END -->