## Pruning de Árvores em Modelos de Função Base Adaptativa (ABM)

### Introdução
Em modelos de função base adaptativa, como as árvores de classificação e regressão (CART), o *overfitting* é uma preocupação central. O *overfitting* ocorre quando o modelo se ajusta excessivamente aos dados de treinamento, capturando ruídos e variações específicas da amostra, em vez de padrões generalizáveis [^545]. Isso resulta em um desempenho ruim em dados não vistos. Para mitigar esse problema, técnicas de *pruning* são empregadas para simplificar a estrutura da árvore e melhorar sua capacidade de generalização [^549]. Este capítulo explora em detalhe os métodos de *pruning* em CART, suas motivações teóricas e implicações práticas.

### Conceitos Fundamentais
#### Estratégias de Prevenção de *Overfitting*
Existem duas abordagens principais para evitar o *overfitting* durante o crescimento de uma árvore CART [^549]:
1.  **Parada Antecipada (Early Stopping)**: Interromper o processo de crescimento da árvore se a diminuição no erro não justificar o aumento na complexidade do modelo.
2.  ***Pruning***: Crescer uma árvore completa e, em seguida, podar as ramificações que contribuem minimamente para a redução do erro.

Embora a parada antecipada possa parecer intuitiva, ela pode ser *míope* [^549]. Isso significa que ela pode interromper o crescimento da árvore prematuramente, perdendo divisões subsequentes que poderiam levar a uma melhor generalização. Por exemplo, em problemas como o XOR (Figura 14.2(c) mencionado em [^549] de um contexto anterior), nenhuma divisão inicial isolada parece promissora, mas sua combinação pode ser altamente preditiva.

#### O Processo de *Pruning*
A abordagem padrão é crescer uma árvore "completa" e, em seguida, realizar o *pruning* [^549]. O *pruning* envolve os seguintes passos:
1.  **Crescimento da Árvore Completa**: Inicialmente, a árvore é expandida até que cada nó terminal contenha um número mínimo de amostras ou atinja um critério de homogeneidade [^546].
2.  **Avaliação do Erro de Validação Cruzada (CV)**: O erro de validação cruzada é avaliado em cada subárvore possível [^549]. Isso envolve dividir os dados em *k* partes, treinar a árvore em *k-1* partes e avaliar o desempenho na parte restante. Este processo é repetido *k* vezes, e os resultados são combinados para obter uma estimativa do erro de generalização.
3.  **Seleção da Subárvore Ótima**: A subárvore com o menor erro de CV é selecionada. No entanto, para evitar a escolha de modelos excessivamente complexos, uma abordagem comum é selecionar a menor subárvore cujo erro de CV esteja dentro de um desvio padrão do mínimo [^549].

#### Critérios de Parada e *Heurísticas*
Durante o crescimento da árvore, várias *heurísticas* podem ser usadas para decidir quando parar a divisão de um nó [^546]:
*   **Redução Insuficiente no Custo**: A divisão é interrompida se a redução no custo (erro) for muito pequena. Uma medida normalizada da redução no custo pode ser definida como:

    $$     \Delta = \frac{\text{custo}(D) - \frac{|D_L|}{\left|D\right|}\text{custo}(D_L) + \frac{|D_R|}{\left|D\right|} \text{custo}(D_R)}{\text{custo}(D)}\     $$

    Onde:

    *   $D$ é o conjunto de dados no nó atual.
    *   $D_L$ e $D_R$ são os conjuntos de dados nos nós filhos esquerdo e direito, respectivamente.
    *   $\text{custo}(D)$ é a função de custo (por exemplo, erro quadrático médio ou entropia) para o conjunto de dados $D$.
    *   $|D|$ denota o número de exemplos em $D$.
*   **Profundidade Máxima Atingida**: A árvore para de crescer se atingir a profundidade máxima desejada.
*   **Homogeneidade Suficiente**: A divisão é interrompida se a distribuição da resposta (variável alvo) em $D_L$ ou $D_R$ for suficientemente homogênea (por exemplo, se todos os rótulos forem os mesmos) [^546].
*   **Número Mínimo de Exemplos**: A divisão é interrompida se o número de exemplos em $D_L$ ou $D_R$ for muito pequeno [^546].

#### Funções de Custo para Avaliação de *Splits*
A escolha da função de custo depende se o objetivo é regressão ou classificação [^546].

*   **Regressão**: O custo é tipicamente o erro quadrático médio:
    $$     \text{custo}(D) = \sum_{i \in D} (y_i - \bar{y})^2\     $$
    Onde $\bar{y}$ é a média dos valores de resposta no conjunto de dados $D$.
*   **Classificação**: Várias medidas podem ser usadas, incluindo:
    *   *Misclassification Rate*:
        $$         \frac{1}{|D|} \sum_{i \in D} I(y_i \neq \hat{y}) = 1 - \pi_{\hat{y}}\         $$
        onde $\hat{y}$ é a classe mais provável e $\pi_{\hat{y}}$ é a fração de exemplos na classe mais provável.
    *   *Entropia (Deviance)*:
        $$         H(\pi) = - \sum_{c=1}^C \pi_c \log \pi_c\         $$
        onde $C$ é o número de classes e $\pi_c$ é a probabilidade da classe $c$.
    *   *Índice de Gini*:
        $$         \sum_{c=1}^C \pi_c(1 - \pi_c) = \sum_{c=1}^C \pi_c - \sum_{c=1}^C \pi_c^2 = 1 - \sum_{c=1}^C \pi_c^2\         $$
#### Complexidade Computacional
A busca pela partição ideal dos dados é um problema NP-completo [^545] (Hyafil and Rivest 1976). Portanto, algoritmos gananciosos, como os mostrados no Algoritmo 6 (mencionado em [^545] de um contexto anterior), são comumente usados para encontrar soluções localmente ótimas. Esses algoritmos selecionam a melhor característica e valor para divisão de forma iterativa, sem garantia de encontrar a melhor árvore possível.

### Conclusão
O *pruning* de árvores é uma técnica essencial para prevenir o *overfitting* em modelos CART e melhorar sua capacidade de generalização. Ao avaliar o erro de validação cruzada em subárvores e selecionar a subárvore ideal, o *pruning* ajuda a criar modelos mais robustos e precisos. As *heurísticas* de parada e as funções de custo desempenham um papel crucial na determinação de quando interromper o crescimento da árvore e como avaliar a qualidade das divisões. Embora a busca pela partição ideal seja um problema complexo, algoritmos gananciosos fornecem soluções práticas para construir árvores CART eficazes.

### Referências
[^545]: Classification and regression trees (CART)
[^546]: Basics
[^549]: Pruning a tree
<!-- END -->