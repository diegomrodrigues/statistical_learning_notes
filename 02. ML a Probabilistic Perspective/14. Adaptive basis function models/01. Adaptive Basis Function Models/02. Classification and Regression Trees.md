## Classificação e Regressão com Árvores (CART)

### Introdução
Este capítulo se aprofunda no uso de **Classification and Regression Trees (CART)**, também conhecidas como árvores de decisão, dentro do contexto mais amplo de **Adaptive Basis Function Models (ABM)**. Como introduzido anteriormente no Capítulo 16 [^1], ABMs são modelos da forma $f(x) = w_0 + \sum_{m=1}^{M} w_m \phi_m(x)$ [^1], onde $\phi_m(x)$ são funções de base adaptativas aprendidas a partir dos dados. As árvores CART se encaixam naturalmente nessa estrutura, definindo partições recursivas do espaço de entrada e modelos locais dentro de cada região [^2]. Este capítulo explorará os detalhes de como as árvores CART funcionam como ABMs, suas vantagens e desvantagens, e como construir e otimizar tais modelos.

### Conceitos Fundamentais
As árvores CART são construídas particionando recursivamente o espaço de entrada e definindo um modelo local em cada região resultante [^2]. Essa estrutura pode ser representada como uma árvore, onde cada folha corresponde a uma região no espaço de entrada [^2]. Para regressão, cada região está associada a uma resposta média, enquanto que para classificação, cada região está associada a uma distribuição sobre os rótulos de classe [^2].

O modelo pode ser expresso como $f(x) = \sum w_m I(x \in R_m)$ [^2], onde $R_m$ é a *m*-ésima região e $w_m$ é a resposta média nessa região. Essa formulação demonstra que a árvore CART é essencialmente um ABM, onde as funções de base definem as regiões e os pesos especificam o valor da resposta em cada região [^2].

**Vantagens das Árvores CART:**
- **Interpretabilidade:** As árvores são fáceis de visualizar e interpretar, permitindo uma compreensão clara das decisões do modelo [^2, 3].
- **Tipos de Dados Mistos:** As árvores podem lidar com variáveis de entrada tanto contínuas quanto categóricas [^2, 3].
- **Insensibilidade a Transformações Monotônicas:** As transformações monotônicas das variáveis de entrada não afetam o desempenho da árvore, pois as divisões são baseadas em *rankings* [^2, 3].
- **Seleção Automática de Variáveis:** O processo de construção da árvore seleciona automaticamente as variáveis mais relevantes para a predição [^2, 3].
- **Robustez a Outliers:** As árvores são relativamente robustas a outliers, uma vez que as divisões são menos sensíveis a valores extremos [^2, 3].
- **Escalabilidade:** As árvores podem escalar bem para grandes conjuntos de dados [^2, 3].

**Processo de Particionamento:**
O processo de construção de uma árvore CART envolve divisões *axis-parallel*, o que significa que cada divisão é feita ao longo de um único eixo da variável de entrada [^2]. Por exemplo, o primeiro nó pode verificar se $x_1$ é menor que um limiar $t_1$ [^2]. Se a resposta for sim, o espaço é dividido em duas regiões, e o processo continua recursivamente para cada região [^2]. O resultado dessas divisões é uma superfície constante por partes [^2].

**Função de Custo:**
A escolha da variável e do limiar para dividir cada nó é determinada pela minimização de uma função de custo [^4]. Para regressão, a função de custo é geralmente a soma dos quadrados dos resíduos [^4]:
$$cost(D) = \sum_{i \in D} (y_i - \bar{y})^2$$ [^4]
onde $D$ é o conjunto de dados no nó e $\bar{y}$ é a média da variável de resposta no conjunto de dados [^4].

Para classificação, a função de custo pode ser a taxa de erro de classificação, a entropia ou o índice de Gini [^4]. A entropia é definida como [^5]:
$$H(\pi) = - \sum_{c=1}^{C} \pi_c \log \pi_c$$ [^5]
onde $\pi_c$ é a proporção de exemplos na classe *c* no nó [^5].

**Crescimento da Árvore:**
O algoritmo para crescer uma árvore CART é um procedimento recursivo ganancioso [^3]. Começando com todos os dados no nó raiz, o algoritmo procura a melhor divisão com base na minimização da função de custo [^3]. O processo é repetido para cada nó filho até que um critério de parada seja atingido [^3].

Os critérios de parada comuns incluem [^4]:
- O número máximo de níveis da árvore é atingido.
- O número mínimo de exemplos em um nó é atingido.
- A redução no custo da divisão é menor do que um limiar.
- Todos os exemplos em um nó pertencem à mesma classe (nó puro).

**Poda da Árvore:**
As árvores CART tendem a superajustar os dados de treinamento, especialmente se a árvore for permitida crescer muito [^3, 7]. Para evitar o superajuste, é comum podar a árvore após ela ter sido construída [^7]. A poda envolve a remoção de ramos da árvore que não contribuem significativamente para a precisão preditiva [^7].

Um método comum de poda é o *cost complexity pruning*, que envolve a minimização da seguinte função [^7]:
$$C(T) = Err(T) + \alpha |T|$$
onde $Err(T)$ é a taxa de erro de classificação da árvore *T*, $|T|$ é o número de nós terminais na árvore, e $\alpha$ é um parâmetro de penalidade que controla o *trade-off* entre precisão e complexidade [^7].

**Árvores CART para Classificação:**
Para generalizar para classificação, armazenamos a distribuição dos rótulos de classe em cada folha em vez da resposta média [^2].
Se usarmos testes da forma $X_j = k$, então tomar expectativas sobre os valores de $X_j$ fornece a informação mútua entre $X$ e $Y$ [^5].

### Conclusão
As árvores CART são uma ferramenta poderosa e versátil para modelagem preditiva, oferecendo interpretabilidade e a capacidade de lidar com dados complexos. Como um tipo de Adaptive Basis Function Model (ABM), as árvores CART se encaixam em uma estrutura mais ampla que permite a construção de modelos não lineares flexíveis. Este capítulo explorou os conceitos fundamentais das árvores CART, incluindo sua construção, poda e uso em regressão e classificação. As árvores CART podem ser usadas como weak learners em métodos de ensemble, como *boosting* e *random forests*, para melhorar ainda mais sua precisão preditiva.

### Referências
[^1]: Capítulo 16, Introdução.
[^2]: Seção 16.2, Classification and regression trees (CART).
[^3]: Seção 16.2.4, Pros and cons of trees.
[^4]: Seção 16.2.2, Growing a tree.
[^5]: Seção 16.2.2.2, Classification cost.
[^6]: Seção 16.4, Boosting.
[^7]: Seção 16.2.3, Pruning a tree.
<!-- END -->