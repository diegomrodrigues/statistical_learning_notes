## Forward Stagewise Additive Modeling em Modelos de Função Base Adaptativa

### Introdução
Este capítulo explora o **Forward Stagewise Additive Modeling** no contexto de **Modelos de Função Base Adaptativa (ABM)**, que são modelos da forma $$f(x) = w_0 + \sum_{m=1}^{M} w_m \phi_m(x)$$ [^1]. O foco principal é detalhar o processo iterativo de construção do modelo, destacando o papel crucial do número fixo de iterações ($M$) como o principal parâmetro de ajuste e as técnicas de *early stopping* e critérios de seleção de modelo como AIC e BIC para mitigar o *overfitting* [^1]. Este método, sendo uma forma de *boosting*, busca resolver o problema de otimização de minimizar a soma das funções de perda [^13]:
$$ \min_{f} \sum_{i=1}^{N} L(y_i, f(x_i)) $$
onde $L(y, \hat{y})$ é uma função de perda e $f$ é um ABM.

### Conceitos Fundamentais
O **Forward Stagewise Additive Modeling** constrói um modelo aditivo de forma iterativa [^13]. O processo começa com uma inicialização de uma função $f_0(x)$ e, em cada iteração $m$, calcula $(\beta_m, \gamma_m)$ que minimiza a soma das funções de perda:
$$ (\beta_m, \gamma_m) = \underset{\beta, \gamma}{\operatorname{argmin}} \sum_{i} L(y_i, f_{m-1}(x_i) + \beta \phi(x; \gamma)) $$
onde $\phi(x; \gamma)$ é uma função base parametrizada por $\gamma$ e $\beta$ é o coeficiente associado [^13]. A função $f_m(x)$ é então atualizada como:
$$ f_m(x) = f_{m-1}(x) + \beta_m \phi(x; \gamma_m) $$
Este processo é repetido por um número fixo de iterações $M$, que atua como o principal parâmetro de ajuste do método [^13].

**A escolha da função de perda $L$ é crucial** [^13]. Algumas opções comuns incluem:
*   **Erro Quadrático:** $L(y, f(x)) = (y - f(x))^2$ [^14]
*   **Erro Absoluto:** $L(y, f(x)) = |y - f(x)|$ [^14]
*   **Logloss:** $L(y, f(x)) = \log(1 + e^{-yf(x)})$ [^14]
*   **Perda Exponencial:** $L(y, f(x)) = e^{-yf(x)}$ [^14]

Cada uma dessas funções de perda tem suas próprias propriedades e leva a diferentes algoritmos de *boosting*. Por exemplo, o uso do erro quadrático leva ao *L2Boosting*, enquanto a perda exponencial leva ao *AdaBoost* [^14].

Para evitar o *overfitting*, duas técnicas são comumente empregadas [^13]:

1.  ***Early Stopping:*** Monitorar o desempenho do modelo em um conjunto de validação separado e interromper o treinamento quando o desempenho começar a diminuir [^13].
2.  ***Critérios de Seleção de Modelo:*** Utilizar critérios como AIC (Akaike Information Criterion) ou BIC (Bayesian Information Criterion) para selecionar o valor ideal de $M$ [^13].

Em vez de atualizar totalmente o modelo com $\beta_m \phi(x; \gamma_m)$, uma atualização parcial pode ser aplicada [^15]:
$$ f_m(x) = f_{m-1}(x) + \nu \beta_m \phi(x; \gamma_m) $$
onde $0 < \nu \le 1$ é um parâmetro de *step-size*. Valores pequenos de $\nu$ são comumente usados, como $\nu = 0.1$, e essa técnica é conhecida como *shrinkage* [^15].

### Conclusão
O **Forward Stagewise Additive Modeling** oferece uma abordagem flexível e poderosa para construir modelos de função base adaptativa. Ao ajustar cuidadosamente o número de iterações ($M$) e empregar técnicas de regularização como *early stopping* e *shrinkage*, é possível obter modelos com bom desempenho preditivo e evitar o *overfitting*. A escolha da função de perda e das funções base $\phi(x; \gamma)$ também desempenha um papel crucial no sucesso do método.

### Referências
[^1]: Page 543, 544
[^13]: Page 555
[^14]: Page 556
[^15]: Page 557
<!-- END -->