## Custo em Regressão e Árvores de Decisão

### Introdução
Este capítulo explora o conceito de custo em um contexto de regressão, particularmente no âmbito de árvores de decisão, que são modelos de funções de base adaptativas [^1]. Árvores de Classificação e Regressão (CART) são construídas particionando recursivamente o espaço de entrada e definindo um modelo local em cada região resultante [^3]. Compreender como o custo é definido e usado para otimizar a estrutura da árvore é crucial para construir modelos preditivos eficazes.

### Conceitos Fundamentais

#### Definição de Custo em Regressão
Em um cenário de regressão, o custo $cost(D)$ para um conjunto de dados $D$ é definido como a soma dos quadrados das diferenças entre os valores observados $y_i$ e a média $\bar{y}$ da variável resposta [^4]:
$$cost(D) = \sum_{i \in D} (y_i - \bar{y})^2$$
onde $\bar{y} = \frac{1}{|D|}\sum_{i \in D} y_i$ é a média da variável resposta no conjunto de dados $D$ [^5].

Esta definição de custo é usada para avaliar a homogeneidade de um nó em uma árvore de regressão. Um nó com baixo custo indica que os valores da variável resposta dentro desse nó são semelhantes, sugerindo que a árvore fez uma boa divisão.

#### Custo Residual e Modelos Lineares em Folhas
Uma alternativa à utilização da média como modelo local é ajustar um modelo de regressão linear para cada folha [^5]. Nesse caso, o custo é medido usando o erro residual [^5]. Os *features* usados no modelo linear são escolhidos no caminho da raiz até a folha [^5].

#### Crescimento da Árvore e Função de *Split*
O objetivo ao crescer uma árvore CART é encontrar as divisões que minimizem o custo total [^3]. A função de *split* escolhe o melhor *feature* e o melhor valor para esse *feature* da seguinte forma [^3]:
$$(j^*, t^*) = \underset{j \in \{1, ..., D\}}{\arg \min} \underset{t \in T_j}{\min} cost(\{x_i, y_i : x_{ij} < t\}) + cost(\{x_i, y_i : x_{ij} > t\})$$
onde $j^*$ é o melhor *feature* para dividir, $t^*$ é o melhor valor de *threshold* para esse *feature*, e $T_j$ é o conjunto de valores de *threshold* possíveis para o *feature* $j$ [^3].

A equação acima demonstra que o algoritmo busca o *feature* e o *threshold* que resultam na menor soma dos custos dos dois nós filhos resultantes da divisão.

#### Critérios de Parada e *Pruning*
O crescimento da árvore continua recursivamente até que um critério de parada seja atingido [^4]. Os critérios de parada comuns incluem [^4]:
*   A redução no custo é muito pequena.
*   A árvore excedeu a profundidade máxima desejada.
*   A distribuição da resposta em $D_L$ ou $D_R$ é suficientemente homogênea.
*   O número de exemplos em $D_L$ ou $D_R$ é muito pequeno.

Para evitar *overfitting*, é comum crescer uma árvore "completa" e, em seguida, realizar o *pruning* [^7]. O *pruning* envolve a remoção de *branches* que dão o menor aumento no erro [^7].

#### Custo de Classificação

Embora o contexto inicial seja de regressão, o texto também aborda o custo de classificação. Para avaliar a qualidade de uma divisão em um cenário de classificação, primeiro ajustamos um modelo *multinoulli* aos dados na folha que satisfaz o teste $X_j < t$ [^5]. As probabilidades condicionais de classe são estimadas da seguinte forma [^5]:
$$hat{\pi}_c = \frac{1}{|D|} \sum_{i \in D} \mathbb{I}(y_i = c)$$
onde $D$ são os dados na folha e $\mathbb{I}$ é a função indicadora [^5].

Com isso, várias medidas de erro comuns podem ser usadas para avaliar uma partição proposta [^5], como *misclassification rate*, entropia ou *Gini index* [^6].

### Conclusão
A definição e utilização do custo são fundamentais para a construção de árvores de decisão eficazes em cenários de regressão. Ao escolher divisões que minimizem o custo e empregar técnicas de *pruning* para evitar o *overfitting*, podemos criar modelos preditivos que generalizem bem para dados não vistos. Além disso, o conceito de custo pode ser estendido para cenários de classificação, onde outras medidas de erro são usadas para avaliar a qualidade das divisões.

### Referências
[^1]: 16 Adaptive basis function models
[^2]: 16.2 Classification and regression trees (CART)
[^3]: 16.2.1 Basics
[^4]: 16.2.2 Growing a tree
[^5]: 16.2.2.1 Regression cost
[^6]: 16.2.2.2 Classification cost
[^7]: 16.2.3 Pruning a tree
<!-- END -->