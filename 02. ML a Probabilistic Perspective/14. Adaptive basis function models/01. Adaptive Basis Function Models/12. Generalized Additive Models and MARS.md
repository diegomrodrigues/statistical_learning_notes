## Generalized Additive Models and Multivariate Adaptive Regression Splines

### Introdução
Este capítulo explora os **Generalized Additive Models (GAMs)** e sua extensão, os **Multivariate Adaptive Regression Splines (MARS)**, ambos pertencentes à classe de modelos de função de base adaptativa [^1]. GAMs e MARS são alternativas aos métodos de kernel [^1] discutidos anteriormente, oferecendo abordagens para modelar relações não lineares entre variáveis de entrada e saída sem a necessidade de especificar manualmente kernels complexos [^1]. Os GAMs, em particular, fornecem uma estrutura flexível para modelar relações não lineares ao somar funções de variáveis de entrada individuais [^1]. Os MARS expandem essa capacidade ao incorporar efeitos de interação entre variáveis, permitindo uma decomposição ANOVA da função [^1].

### Conceitos Fundamentais

#### Generalized Additive Models (GAMs)
Os **Generalized Additive Models (GAMs)** representam uma abordagem para criar modelos não lineares com múltiplas entradas [^1]. A predição em um GAM é obtida através da soma de funções de variáveis de entrada individuais [^1]:

$$ f(x) = \alpha + f_1(x_1) + \dots + f_D(x_D) $$

onde $x$ é o vetor de entrada, $f_i$ são funções não lineares de cada variável de entrada $x_i$, e $\alpha$ é um termo constante [^1, 16]. Cada função $f_i$ pode ser modelada por um *scatterplot smoother* ou *regression splines* [^1]. Além disso, o modelo completo pode ser mapeado para uma distribuição de probabilidade usando uma *função de ligação*, similar ao que é feito em GLMs (Generalized Linear Models) [^1]. Isso confere aos GAMs a flexibilidade de modelar uma variedade de tipos de resposta, como respostas contínuas, binárias ou de contagem [^10].

Se *regression splines* são utilizadas para modelar as funções $f_j$, então cada $f_j(x_j)$ pode ser expressa como $\phi_j^T(x_j)\beta_j$, onde $\phi_j$ é uma função de base e $\beta_j$ são os coeficientes [^10]. O modelo GAM pode ser escrito como $f(x) = \beta^T\Phi(x)$, onde $\Phi(x) = [1, \phi_1(x_1), \dots, \phi_D(x_D)]$ [^10]. No entanto, é mais comum usar *smoothing splines* para as funções $f_j$ [^10]. Nesse caso, o objetivo se torna minimizar a seguinte função de custo:

$$ J(\alpha, f_1, \dots, f_D) = \sum_{i=1}^N \left( y_i - \alpha - \sum_{j=1}^D f_j(x_{ij}) \right)^2 + \sum_{j=1}^D \lambda_j \int f''_j(t_j)^2 dt_j $$

onde $\lambda_j$ é a força do regularizador para a função $f_j$ [^10].

Para ajustar o modelo usando MLE (Maximum Likelihood Estimation), o algoritmo de *backfitting* é utilizado [^10]. Este algoritmo ajusta iterativamente cada função $f_j$, mantendo as outras fixas [^10]. No entanto, a constante $\alpha$ não é unicamente identificável, uma vez que constantes podem ser adicionadas ou subtraídas das funções $f_j$ [^10]. Para resolver isso, assume-se que $\sum_{i=1}^N f_j(x_{ij}) = 0$ para todo $j$ [^10].

#### Multivariate Adaptive Regression Splines (MARS)
Os **Multivariate Adaptive Regression Splines (MARS)** estendem os GAMs ao permitir efeitos de interação entre as variáveis de entrada [^1, 11]. Em geral, uma decomposição ANOVA pode ser criada para capturar esses efeitos [^11]:

$$ f(x) = \beta_0 + \sum_{j=1}^D f_j(x_j) + \sum_{j,k} f_{jk}(x_j, x_k) + \sum_{j,k,l} f_{jkl}(x_j, x_k, x_l) + \dots $$

onde $f_{jk}$ representa a interação entre as variáveis $x_j$ e $x_k$, e assim por diante [^11]. No entanto, permitir muitas interações de ordem superior pode levar a um número excessivo de parâmetros [^11]. Portanto, uma busca *greedy* é comumente usada para decidir quais variáveis adicionar ao modelo [^11].

O algoritmo MARS ajusta modelos da forma acima, utilizando um produto tensorial de *regression splines* para representar as funções de regressão multidimensional [^11]. Por exemplo, para uma entrada 2D, podemos usar [^11]:

$$ f(x_1, x_2) = \beta_0 + \sum_m \beta_{1m}(x_1 - t_{1m})_+ + \sum_m \beta_{2m}(t_{2m} - x_2)_+ + \sum_m \beta_{12m}(x_1 - t_{1m})_+(t_{2m} - x_2)_+ $$

onde $(x-t)_+$ denota a função *hinge*, que é igual a $x-t$ se $x > t$ e 0 caso contrário, e $t_{jm}$ são os nós (*knots*) [^11].

Para criar essa função, começamos com um conjunto de funções de base candidatas da forma [^11]:

$$ C = \{(x_j - t)_+, (t - x_j)_+ : t \in \{x_{1j}, \dots, x_{Nj}\}, j = 1, \dots, D\} $$

Estas são *linear splines* onde os nós estão em todos os valores observados para aquela variável [^11]. O algoritmo considera *splines* que inclinam para cima em ambas as direções, o que é chamado de *reflecting pair* [^11]. O algoritmo começa com $M = \{1\}$ e considera criar um novo par de funções de base multiplicando uma $h_m \in M$ com um dos *reflecting pairs* em $C$ [^11].

#### Relação com CART
O procedimento MARS está intimamente relacionado com o CART (Classification and Regression Trees) [^12]. Substituir as funções de base lineares por funções degrau $\mathbb{I}(x_j > t)$ e $\mathbb{I}(x_j < t)$ e restringir que, uma vez que uma variável está envolvida em uma multiplicação por um termo candidato, essa variável seja substituída pela interação, garante que o modelo resultante possa ser representado como uma árvore [^12]. Nesse caso, a estratégia de crescimento MARS é a mesma que a estratégia de crescimento CART [^12].

### Conclusão
GAMs e MARS oferecem abordagens flexíveis e poderosas para modelagem não linear em dados complexos [^1, 11]. Ao permitir a modelagem de relações aditivas e interações entre variáveis, esses modelos fornecem insights valiosos sobre a estrutura subjacente dos dados [^1, 11]. Embora GAMs e MARS não tenham sido o foco principal dos capítulos anteriores, eles se encaixam naturalmente no contexto de modelos de função de base adaptativa [^1], oferecendo alternativas valiosas aos métodos de kernel discutidos anteriormente [^1]. Além disso, a relação entre MARS e árvores de decisão destaca as conexões entre diferentes abordagens de modelagem não linear [^12], enriquecendo ainda mais a compreensão do cenário de modelagem adaptativa [^1].

### Referências
[^1]: Capítulo 16 do texto fornecido.
[^10]: Página 552 do texto fornecido.
[^11]: Página 553 do texto fornecido.
[^12]: Página 554 do texto fornecido.
<!-- END -->