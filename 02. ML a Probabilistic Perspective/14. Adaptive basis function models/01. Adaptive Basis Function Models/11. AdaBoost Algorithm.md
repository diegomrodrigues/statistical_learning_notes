## Abordagens de Limite Superior Convexo Alternativas e AdaBoost

### Introdução

Este capítulo explora abordagens alternativas de limite superior convexo e o algoritmo AdaBoost no contexto dos modelos de funções de base adaptativas (ABM). Como discutido na introdução do Capítulo 16 [^16], os ABMs são modelos da forma $f(x) = w_0 + \sum_{m=1}^M w_m \phi_m(x)$, onde as funções de base $\phi_m(x)$ são aprendidas diretamente dos dados. Anteriormente, discutimos a importância de funções de perda convexas e seus minimizadores populacionais. Agora, investigaremos a perda exponencial como uma alternativa à logloss e suas implicações no algoritmo AdaBoost.

### Conceitos Fundamentais

Uma alternativa aos limites superiores convexos é a **perda exponencial**, definida como $L(\hat{y}, f) = \exp(-\hat{y}f)$ [^16]. Essa função de perda oferece algumas vantagens computacionais em relação à logloss. A escolha da função de perda influencia diretamente o algoritmo de otimização utilizado para aprender os parâmetros do modelo.

#### O Algoritmo AdaBoost

A perda exponencial leva ao algoritmo **AdaBoost** [^16]. Este algoritmo computa a função ótima a ser adicionada como:

$$\phi_m = \underset{\phi}{\operatorname{argmin}} \sum_i w_{i,m} I(\hat{y}_i \neq \phi(x_i))$$

onde $w_{i,m}$ são os pesos atribuídos a cada amostra $i$ na iteração $m$, e $I(\hat{y}_i \neq \phi(x_i))$ é uma função indicadora que vale 1 se a previsão $\phi(x_i)$ estiver incorreta e 0 caso contrário [^16]. Essencialmente, AdaBoost busca minimizar o erro ponderado das classificações.

Na próxima iteração, os pesos são atualizados da seguinte forma:

$$w_{i,m+1} = w_{i,m} \exp(-\beta_m \hat{y}_i \phi(x_i))$$

onde $\beta_m$ é um parâmetro que controla a taxa de atualização dos pesos [^16]. Se uma amostra é classificada corretamente, seu peso é diminuído; se é classificada incorretamente, seu peso é aumentado. Isso faz com que o AdaBoost foque em amostras que são difíceis de classificar, *adaptando-se* aos dados, justificando o nome *Adaptive Boosting*.

**Caixa de Destaque:** A chave para o AdaBoost é a atualização adaptativa dos pesos, que permite que o algoritmo foque em amostras difíceis e aprenda um classificador forte combinando classificadores fracos.

#### Relação com Gradient Descent

É importante notar que o AdaBoost pode ser interpretado como uma forma de **gradient descent** no espaço funcional [^16]. Em vez de otimizar diretamente os parâmetros do modelo, o AdaBoost adiciona funções de base (classificadores fracos) iterativamente, de forma a diminuir o erro. Essa perspectiva conecta AdaBoost a uma classe mais ampla de algoritmos de otimização.

### Conclusão

O algoritmo AdaBoost, impulsionado pela perda exponencial, é uma técnica poderosa para construir modelos de funções de base adaptativas. Sua capacidade de focar em amostras difíceis através da atualização adaptativa dos pesos o torna robusto e eficaz em muitas aplicações. Como mencionado anteriormente, a escolha da função de perda é crucial, e a perda exponencial oferece vantagens computacionais em relação à logloss. Além disso, a interpretação do AdaBoost como gradient descent no espaço funcional fornece uma visão mais profunda de seu funcionamento interno e o conecta a outros algoritmos de otimização. A discussão sobre boosting como uma forma de regularização $l_1$ no Capítulo 16 [^16] fornece uma perspectiva adicional sobre por que o AdaBoost funciona tão bem na prática.

### Referências

[^16]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. New York: Springer.

<!-- END -->