## Ensemble Methods: Bagging and Random Forests in Adaptive Basis Function Models

### Introdução
Este capítulo explora o uso de **métodos de ensemble**, especificamente *bagging* e *random forests*, no contexto de modelos de função de base adaptativa (ABM) [^1]. Como vimos anteriormente, os ABMs procuram aprender funções de base úteis diretamente dos dados [^1]. Aqui, investigaremos como os métodos de ensemble podem ser empregados para melhorar a precisão preditiva e reduzir a variância dos modelos CART (Classification and Regression Trees), que são uma forma de ABM [^2].

### Conceitos Fundamentais
Os **modelos CART**, também conhecidos como *árvores de decisão*, particionam recursivamente o espaço de entrada e definem um modelo local em cada região resultante [^2]. O modelo pode ser representado por uma árvore, com um nó folha por região [^2]. Embora os modelos CART sejam fáceis de interpretar e podem lidar com entradas discretas e contínuas mistas [^3], eles podem ter menor precisão preditiva e podem ser instáveis devido ao processo hierárquico de crescimento da árvore, tornando-os estimadores de alta variância [^3].

**Métodos de ensemble** como *bagging* (bootstrap aggregating) e *random forests* são usados para reduzir a variância e melhorar a precisão preditiva, treinando várias árvores em diferentes subconjuntos dos dados e/ou subconjuntos escolhidos aleatoriamente de variáveis de entrada [^3]. Isso decorrelaciona os aprendizes de base para aumentar a redução da variância [^3].

*   **Bagging**: Envolve re-executar o algoritmo de aprendizado em diferentes subconjuntos dos dados [^3]. Essa técnica pode resultar em preditores altamente correlacionados, o que limita a quantidade de redução de variância que é possível [^3].
*   **Random Forests**: Decorrelacionam os aprendizes de base aprendendo árvores com base em um subconjunto escolhido aleatoriamente de variáveis de entrada [^3]. Os random forests reduzem a variância calculando a média de várias estimativas de diferentes árvores treinadas em diferentes subconjuntos dos dados, escolhidos aleatoriamente com reposição [^3].

A combinação de *bagging* e seleção aleatória de variáveis de entrada em *random forests* é uma técnica poderosa para reduzir a variância e melhorar a precisão preditiva [^3]. Modelos como *random forests* frequentemente têm uma precisão preditiva muito boa [^3] e têm sido amplamente utilizados em muitas aplicações [^3].

A **Equação (16.15)** [^3] representa a fórmula para calcular o ensemble:
$$\
f(x) = \frac{1}{M} \sum_{m=1}^{M} f_m(x)
$$
onde $f_m$ é a m-ésima árvore e M é o número total de árvores no ensemble.

### Conclusão

Os métodos de ensemble, como *bagging* e *random forests*, oferecem uma abordagem eficaz para reduzir a variância e melhorar a precisão preditiva em modelos de função de base adaptativa, como os modelos CART [^3]. Ao treinar várias árvores em diferentes subconjuntos dos dados e/ou selecionando aleatoriamente subconjuntos de variáveis de entrada, esses métodos decorrelacionam os aprendizes de base e reduzem o overfitting [^3]. Embora a interpretabilidade possa ser comprometida ao usar vários árvores [^3], os ganhos em precisão preditiva e robustez tornam os *random forests* uma escolha valiosa no conjunto de ferramentas de modelagem [^3].

### Referências
[^1]: Page 543 - Introduction of Adaptive basis function models
[^2]: Page 544 - Classification and regression trees (CART)
[^3]: Page 550-551 - Random forests
<!-- END -->