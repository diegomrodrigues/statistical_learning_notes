## Classification Cost in Adaptive Basis Function Models

### Introdução
Em continuidade aos modelos de função de base adaptativa (ABM), este capítulo explora em detalhes o custo de classificação em árvores de classificação e regressão (CART) [^1]. Como vimos anteriormente, CART modelam a função de predição particionando recursivamente o espaço de entrada e definindo um modelo local em cada região resultante [^1]. No contexto da classificação, cada folha da árvore representa uma distribuição sobre os rótulos de classe. Este capítulo se aprofunda nas métricas utilizadas para avaliar a qualidade dessas distribuições e, consequentemente, a eficácia das partições propostas.

### Conceitos Fundamentais

O processo de classificação em árvores CART envolve a adaptação de um modelo **multinoulli** aos dados em cada folha que satisfaz o teste $X_j < t$ [^2]. Este modelo estima as probabilidades condicionais de classe $\hat{\pi}_c$ da seguinte forma [^2]:

$$hat{\pi}_c = \frac{1}{|D|} \sum_{i \in D} I(y_i = c)$$

onde:

*   $D$ é o conjunto de dados na folha.
*   $y_i$ é o rótulo da classe para a $i$-ésima amostra.
*   $c$ é o rótulo da classe.
*   $I(y_i = c)$ é uma função indicadora que retorna 1 se $y_i = c$ e 0 caso contrário.

Em outras palavras, $\hat{\pi}_c$ é a proporção de amostras na folha que pertencem à classe $c$.

Para avaliar uma partição proposta, várias medidas de erro comuns são utilizadas [^2]:

1.  **Taxa de má classificação (Misclassification rate):** Esta é a fração de amostras que seriam classificadas incorretamente se atribuíssemos a todas as amostras na folha o rótulo de classe mais provável [^2]. É calculada como:

    $$1 - \hat{\pi}_{\hat{c}}$$

    onde $\hat{c} = \underset{c}{\mathrm{argmax}} \\, \hat{\pi}_c$ é a classe mais provável.
2.  **Entropia (Entropy):** A entropia mede a impureza da distribuição de classe na folha [^2]. É calculada como:

    $$H(\pi) = -\sum_c \hat{\pi}_c \log \hat{\pi}_c$$

    Uma entropia mais baixa indica uma distribuição de classe mais pura (ou seja, a maioria das amostras na folha pertence à mesma classe).
3.  **Índice de Gini (Gini index):** O índice de Gini também mede a impureza da distribuição de classe na folha [^2]. É calculado como:

    $$sum_c \hat{\pi}_c (1 - \hat{\pi}_c) = \sum_c \hat{\pi}_c - \sum_c \hat{\pi}_c^2 = 1 - \sum_c \hat{\pi}_c^2$$

    Similar à entropia, um índice de Gini mais baixo indica uma distribuição de classe mais pura.

Um aspecto crucial é que minimizar a entropia é equivalente a maximizar o ganho de informação (information gain) entre o teste $X_j < t$ e o rótulo da classe $Y$ [^2]. O ganho de informação é definido como [^2]:

$$infoGain(X_j < t, Y) = H(Y) - H(Y|X_j < t)$$

onde $H(Y)$ é a entropia da distribuição de classe antes da divisão, e $H(Y|X_j < t)$ é a entropia condicional da distribuição de classe dado que $X_j < t$. Expandindo a definição, temos [^2]:

$$infoGain(X_j < t, Y) = \sum_{c} p(y=c) \log p(y=c) + \sum_{c} p(y=c|X_j < t) \log p(c|X_j < t)$$

O objetivo do algoritmo CART é encontrar a divisão (ou seja, a variável $j$ e o limiar $t$) que maximiza o ganho de informação, o que, por sua vez, minimiza a entropia das folhas resultantes [^2].

As medidas de entropia e Gini são muito similares e mais sensíveis às mudanças na probabilidade das classes do que a taxa de má classificação [^2]. Por exemplo, considere um problema de classificação binária com 400 casos em cada classe [^2]. Suponha que uma divisão crie os nós (300,100) e (100,300), enquanto outra divisão crie os nós (200,400) e (200,0) [^2]. Ambas as divisões produzem uma taxa de má classificação de 0.25 [^2]. No entanto, a última parece preferível, já que um dos nós é *puro*, ou seja, contém apenas uma classe [^2]. A entropia cruzada e as medidas de Gini favorecerão esta última escolha [^2].

### Conclusão
A escolha da métrica de custo de classificação (taxa de má classificação, entropia ou índice de Gini) pode influenciar a estrutura da árvore CART resultante e, consequentemente, o desempenho do modelo [^2]. Minimizar a entropia ou o índice de Gini tende a levar a árvores mais equilibradas e precisas, especialmente quando as classes não são uniformemente distribuídas [^2]. Compreender as nuances de cada métrica e seu impacto no processo de construção da árvore é fundamental para desenvolver modelos de classificação eficazes com base em ABMs.

### Referências
[^1]: Chapter 16. Adaptive basis function models. Page 544.
[^2]: Chapter 16. Adaptive basis function models. Page 547.
<!-- END -->