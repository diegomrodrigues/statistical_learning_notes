## Consistent Gaussian Priors and Invariance in MLPs

### Introdução
Em redes neurais feedforward (MLPs), a regularização desempenha um papel crucial para evitar o overfitting e melhorar a capacidade de generalização do modelo. Uma abordagem comum é utilizar priors Gaussianos nos pesos da rede. No entanto, a escolha inadequada dos parâmetros de regularização pode comprometer certas propriedades desejáveis do modelo, como a *invariância* sob transformações lineares dos dados de entrada e saída [^573]. Este capítulo explora como priors Gaussianos consistentes, com diferentes intensidades de regularização para as camadas, podem ser utilizados para aprimorar a propriedade de invariância em MLPs [^573].

### Conceitos Fundamentais
A propriedade de invariância desejada em um modelo de regressão de rede neural significa que, se escalarmos e deslocarmos linearmente as entradas e/ou saídas da rede, o modelo deve aprender a prever a mesma função [^573]. Isso deve ser alcançado através do ajuste adequado dos pesos internos e termos de bias da rede.

Para entender a necessidade de diferentes forças de regularização para as camadas, considere um MLP com duas camadas. Seja $x$ a entrada, $z$ a camada oculta, e $y$ a saída. As equações do modelo são:

$$ z = g(Vx) $$

$$ y = w^Tz $$

onde $V$ é a matriz de pesos da primeira camada, $w$ é o vetor de pesos da segunda camada, e $g$ é a função de ativação não linear [^563].

Agora, suponha que escalemos a entrada por um fator $\lambda_x$ e a saída por um fator $\lambda_y$:

$$ x' = \lambda_x x $$

$$ y' = \lambda_y y $$

Para que o modelo aprenda a mesma função com os dados transformados, os pesos internos precisam ser ajustados. Idealmente, gostaríamos que:

$$ z' = g(V'x') $$

$$ y' = w'^Tz' $$

onde $V'$ e $w'$ são os novos pesos.

Se utilizarmos o mesmo parâmetro de regularização para $V$ e $w$, o modelo pode não conseguir se ajustar adequadamente para compensar as mudanças de escala nas entradas e saídas [^573]. Isso ocorre porque a escala ideal para os pesos da primeira camada ($V$) pode ser diferente da escala ideal para os pesos da segunda camada ($w$).

Para mitigar esse problema, podemos usar priors Gaussianos separados para os pesos de cada camada, com diferentes parâmetros de regularização [^573]:

$$ p(\theta) = N(W|0, \frac{1}{\alpha_w}I)N(V|0, \frac{1}{\alpha_v}I)N(b|0, \frac{1}{\alpha_b}I)N(c|0, \frac{1}{\alpha_c}I) $$

onde $\alpha_v$ e $\alpha_w$ são os parâmetros de precisão (inverso da variância) para os priors Gaussianos nos pesos da primeira e segunda camadas, respectivamente, e $b$ e $c$ são os termos de bias com precisões $\alpha_b$ e $\alpha_c$ [^573]. Ao ajustar independentemente esses parâmetros de regularização, permitimos que o modelo aprenda a escala ideal para cada camada, melhorando a propriedade de invariância.

### Conclusão
A utilização de priors Gaussianos consistentes com diferentes forças de regularização para as camadas de um MLP pode melhorar significativamente a propriedade de invariância do modelo sob transformações lineares dos dados. Essa abordagem permite que o modelo aprenda a mesma função, mesmo quando as entradas e saídas são escaladas e deslocadas, através do ajuste adequado dos pesos internos e termos de bias. A escolha dos parâmetros de regularização $\alpha_v$ e $\alpha_w$ pode ser feita por validação cruzada ou métodos bayesianos empíricos [^573].

### Referências
[^563]: Capítulo 16, seção 16.5: Feedforward neural networks (multilayer perceptrons)
[^573]: Capítulo 16, seção 16.5.6.1: Consistent Gaussian priors *
<!-- END -->