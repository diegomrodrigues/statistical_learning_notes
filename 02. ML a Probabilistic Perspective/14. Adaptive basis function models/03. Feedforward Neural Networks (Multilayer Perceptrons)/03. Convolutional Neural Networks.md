## Redes Neurais Convolucionais (CNNs)

### Introdução
Expandindo sobre a discussão de Feedforward Neural Networks (MLPs) [^21], este capítulo aprofunda-se em uma variação especializada e poderosa: as Redes Neurais Convolucionais (CNNs). As CNNs são particularmente eficazes no processamento de dados com estrutura de grade, como imagens e áudio [^1], e se destacam no aprendizado automático de hierarquias espaciais de *features*, permitindo capturar padrões complexos com menos parâmetros em comparação com redes totalmente conectadas [^1]. Este capítulo explorará os componentes e princípios fundamentais das CNNs, enfocando sua capacidade de atingir invariância de translação através do compartilhamento de pesos [^1].

### Conceitos Fundamentais
As Redes Neurais Convolucionais (CNNs) são um tipo especializado de Redes Neurais *Feedforward* (MLPs) [^1], projetadas para processar dados que possuem uma estrutura de grade, como sinais 1D (voz ou texto) ou sinais 2D (imagens) [^1]. Elas são particularmente adequadas para tarefas onde a localização espacial das características é importante, como reconhecimento de imagem [^1].

**Camadas Convolucionais:** No coração de uma CNN estão as camadas convolucionais. Em vez de conectar cada neurônio em uma camada a todos os neurônios na camada anterior (como em MLPs totalmente conectadas), as camadas convolucionais têm **campos receptivos locais** [^1]. Cada neurônio em uma camada convolucional está conectado apenas a uma pequena região da camada anterior. Esta região é conhecida como o campo receptivo do neurônio [^1].

**Compartilhamento de Peso:** Uma característica fundamental das CNNs é o compartilhamento de peso ou *weight tying* [^1]. Isso significa que os mesmos pesos são usados para todos os neurônios em uma determinada camada convolucional. Em outras palavras, em vez de aprender pesos separados para cada conexão, a rede aprende um conjunto de pesos que são compartilhados em toda a entrada [^1]. Isso reduz drasticamente o número de parâmetros na rede, tornando-a mais fácil de treinar e menos propensa a *overfitting* [^1].

**Invariância de Translação:** O compartilhamento de peso permite que as CNNs alcancem a invariância de translação [^1]. Isso significa que a rede pode detectar padrões, não importa onde eles estejam localizados na entrada [^1]. Por exemplo, se uma CNN foi treinada para reconhecer gatos em imagens, ela poderá reconhecer um gato, não importa se o gato está no canto superior esquerdo, no canto inferior direito ou em qualquer outro lugar da imagem [^1].

**Exemplo:** Considere o reconhecimento de um rosto em uma imagem. A rede deve ser capaz de identificar um rosto, mesmo que ele esteja em diferentes posições na imagem. Compartilhar os pesos permite que a rede aprenda *features* que são relevantes para um rosto, independentemente de sua localização [^1].

**Formalização Matemática:**
Considere uma camada convolucional com entrada $x$ e saída $y$. A operação convolucional pode ser expressa como:
$$\ny[i, j] = (w * x)[i, j] + b = \sum_{m} \sum_{n} w[m, n] \cdot x[i-m, j-n] + b\n$$
onde:
*   $y[i, j]$ é o valor do neurônio na posição $(i, j)$ na camada de saída
*   $w$ é o *kernel* ou filtro convolucional (os pesos compartilhados)
*   $x[i-m, j-n]$ é o valor de entrada na posição $(i-m, j-n)$
*   $b$ é o *bias*
*   $*$ denota a operação de convolução
*   A soma é realizada sobre as dimensões do *kernel*

Esta operação é repetida em toda a entrada, resultando em um mapa de *features* que representa a resposta do filtro em diferentes locais da entrada.

### Conclusão
As Redes Neurais Convolucionais oferecem uma arquitetura poderosa e eficiente para o processamento de dados estruturados em forma de grade. Ao empregar campos receptivos locais e compartilhamento de peso, as CNNs reduzem o número de parâmetros, evitam o *overfitting* e alcançam a invariância de translação, tornando-as ideais para tarefas como reconhecimento de imagem, processamento de áudio e processamento de linguagem natural [^1]. Sua capacidade de aprender automaticamente hierarquias espaciais de *features* as torna uma ferramenta fundamental no aprendizado profundo.

### Referências
[^1]: Texto fornecido no contexto.
[^21]: Capítulo anterior sobre *Feedforward Neural Networks (Multilayer Perceptrons)*.

<!-- END -->