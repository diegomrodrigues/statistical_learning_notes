## Semi-Supervised Embedding in Feedforward Neural Networks

### Introdução
Este capítulo aborda o conceito de **semi-supervised embedding** como uma técnica de regularização para redes neurais *feedforward* (MLPs). O objetivo é utilizar informações adicionais sobre similaridade entre objetos para orientar a aprendizagem das representações nas camadas ocultas, melhorando a generalização do modelo. A regularização por *embedding* semi-supervisionado incentiva que objetos similares sejam mapeados para representações similares nas camadas ocultas, aproveitando informações sobre pares de objetos similares e dissimilares [^575].

### Conceitos Fundamentais

A regularização em redes neurais é crucial para evitar o *overfitting*, especialmente quando se lida com conjuntos de dados limitados. A ideia central do *semi-supervised embedding* é incorporar informações adicionais sobre a relação entre os dados, mesmo que não tenhamos rótulos completos para todos eles.

O contexto [^575] introduz o conceito de *semi-supervised embedding* como uma forma de regularizar redes neurais *feedforward*, incentivando que as camadas ocultas atribuam representações similares a objetos similares. Este método aproveita informações adicionais (side information) sobre pares de objetos similares e dissimilares.

Formalmente, considere um conjunto de dados onde alguns pares de exemplos $(x_i, x_j)$ são rotulados como similares ($S_{ij} = 1$) ou dissimilares ($S_{ij} = 0$). O objetivo é que a rede neural, ao aprender uma função $f(x)$, produza *embeddings* $f(x_i)$ e $f(x_j)$ que reflitam essa similaridade.

Uma **função de perda** comum para implementar essa regularização é definida em [^575]:
$$
L(f(x_i), f(x_j), S_{ij}) =
\begin{cases}
||f(x_i) - f(x_j)||^2, & \text{se } S_{ij} = 1 \\
\max(0, m - ||f(x_i) - f(x_j)||^2), & \text{se } S_{ij} = 0
\end{cases}
$$
onde $m$ é uma margem mínima desejada para a distância entre *embeddings* de objetos dissimilares.

A **função de perda total** para treinar a rede neural é então a combinação da função de perda padrão (como *cross-entropy* para classificação) e a função de perda de *embedding* semi-supervisionado [^576]:
$$
\sum_{i \in \mathcal{L}} \text{NLL}(f(x_i), y_i) + \lambda \sum_{i,j \in \mathcal{U}} L(f(x_i), f(x_j), S_{ij})
$$
onde:
- $\mathcal{L}$ é o conjunto de dados rotulados.
- $\mathcal{U}$ é o conjunto de dados não rotulados ou pares com informações de similaridade.
- $\text{NLL}(f(x_i), y_i)$ é a *negative log-likelihood* para o exemplo rotulado $x_i$ com rótulo $y_i$.
- $\lambda$ é um parâmetro de *trade-off* que controla a importância da regularização.

O contexto [^576] menciona que essa função objetivo pode ser otimizada utilizando *stochastic gradient descent* (SGD). A cada iteração, seleciona-se um exemplo rotulado e um par de exemplos não rotulados (similares ou dissimilares) para realizar uma atualização do gradiente.

### Conclusão
O *semi-supervised embedding* é uma técnica eficaz para regularizar redes neurais *feedforward*, especialmente quando se tem informações adicionais sobre a similaridade entre os dados. Ao incorporar essas informações na função de perda, podemos orientar a aprendizagem das representações nas camadas ocultas, melhorando a generalização do modelo e sua capacidade de lidar com dados não rotulados. A escolha adequada da função de perda de *embedding*, do parâmetro de *trade-off* $\lambda$ e do método de otimização são cruciais para o sucesso dessa técnica.

### Referências
[^575]: Texto fornecido: "Semi-supervised embedding is an approach to regularize deep feedforward neural networks by encouraging the hidden layers to assign similar objects to similar representations, leveraging side information consisting of sets of pairs of similar and dissimilar objects. Semi-supervised embedding regularizes deep feedforward neural networks by encouraging hidden layers to assign similar objects to similar representations, leveraging side information about pairs of similar and dissimilar objects."
[^576]: "Such an objective can be easily optimized by stochastic gradient descent. At each itera-tion, pick a random labeled training example, (xn, Yn), and take a gradient step to optimize NLL(f(xi), Yi). Then pick a random pair of similar unlabeled examples xi, xj (these can sometimes be generated on the fly rather than stored in advance), and make a gradient step to optimize L(f(xi), f(xj), 1), Finally, pick a random unlabeled example xk, which with high probability is dissimilar to xi, and make a gradient step to optimize XL(f(xi), f(xk),0)."
<!-- END -->