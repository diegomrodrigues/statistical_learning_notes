## Backpropagation in Multilayer Perceptrons
### Introdução
O algoritmo de **backpropagation** é fundamental para o treinamento de Redes Neurais Feedforward (Multilayer Perceptrons - MLPs) [^563]. Ele permite o ajuste iterativo dos pesos da rede para minimizar o erro entre as saídas previstas e as saídas reais, tornando possível o aprendizado de modelos com camadas ocultas [^569]. Este capítulo detalha o algoritmo de backpropagation, sua derivação matemática e suas implicações práticas.

### Conceitos Fundamentais
O algoritmo de backpropagation é um método para calcular o vetor gradiente da **Negative Log-Likelihood (NLL)** em MLPs [^569]. Ele aplica a regra da cadeia do cálculo diferencial, computando os erros localmente e propagando-os para trás através da rede [^569]. Isso permite o ajuste dos pesos para minimizar a função de perda [^569].

Para simplificar a notação, considera-se um modelo com apenas uma camada oculta [^569]. Sejam $x_n$ a *n*-ésima entrada, $a_n = Vx_n$ o valor pré-sináptico da camada oculta, e $z_n = g(a_n)$ o valor pós-sináptico da camada oculta, onde $V$ é a matriz de pesos da primeira camada e $g$ é a função de ativação [^569]. A saída da rede é então calculada como $\hat{y}_n = h(Wz_n)$, onde $W$ é a matriz de pesos da segunda camada e $h$ é a função de ativação da camada de saída [^570].

A função de perda a ser minimizada é a NLL, que, no caso de regressão com $K$ saídas, é dada por [^571]:
$$ J(\theta) = - \sum_n \sum_k (\hat{y}_{nk}(\theta) - y_{nk})^2 $$
E, no caso de classificação com $K$ classes, é dada pela entropia cruzada [^571]:
$$ J(\theta) = - \sum_n \sum_k y_{nk} \log \hat{y}_{nk}(\theta) $$
O objetivo é calcular o gradiente de $J(\theta)$ em relação aos pesos $V$ e $W$ [^571]. Isso é feito aplicando a regra da cadeia, propagando os erros de trás para frente através da rede [^571].

Para os pesos da camada de saída ($W$), temos [^571]:
$$ \nabla_{W_k} J_n = \frac{\partial J_n}{\partial W_k} = \frac{\partial J_n}{\partial b_{nk}} \frac{\partial b_{nk}}{\partial W_k} = \delta_{nk} z_n $$
Onde $b_{nk} = W_k^T z_n$ e $\delta_{nk} = (\hat{y}_{nk} - y_{nk})$ é o sinal de erro [^571]. Portanto, o gradiente é o produto da entrada pré-sináptica (zn) pelo sinal de erro (δnk) [^571].

Para os pesos da camada de entrada ($V$), temos [^571]:
$$ \nabla_{V_j} J_n = \frac{\partial J_n}{\partial V_j} = \frac{\partial J_n}{\partial a_{nj}} \frac{\partial a_{nj}}{\partial V_j} = \delta_{nj} x_n $$
Onde
$$ \delta_{nj} = \frac{\partial J_n}{\partial a_{nj}} = \sum_k \frac{\partial J_n}{\partial b_{nk}} \frac{\partial b_{nk}}{\partial a_{nj}} = \sum_k \delta_{nk} W_{kj} g'(a_{nj}) $$
Aqui, $g'(a_{nj})$ é a derivada da função de ativação $g$ avaliada em $a_{nj}$ [^571]. Para unidades tanh, $g'(a) = 1 - \tanh^2(a)$, e para unidades sigmoid, $g'(a) = \sigma(a)(1 - \sigma(a))$ [^571].

O algoritmo de backpropagation pode ser resumido nos seguintes passos [^572]:
1. **Forward Pass:** Calcule os valores de ativação para cada camada, começando pela camada de entrada e propagando para a frente até a camada de saída [^572].
2. **Backward Pass:** Calcule o erro na camada de saída e propague-o para trás através da rede, calculando os gradientes dos pesos em cada camada [^572].
3. **Atualização dos Pesos:** Ajuste os pesos da rede usando os gradientes calculados, geralmente utilizando um algoritmo de otimização como o gradiente descendente [^572].

É importante notar que os parâmetros de uma rede neural não são *identificáveis* [^572]. Por exemplo, podemos mudar o sinal dos pesos que entram em uma unidade oculta, desde que também mudemos o sinal dos pesos que saem dela [^572]. Existem $H$ tais simetrias de inversão de sinal, levando a $2^H$ configurações equivalentes [^572]. Similarmente, podemos mudar a identidade das unidades ocultas sem afetar a verossimilhança [^572]. Existem $H!$ tais permutações [^572]. O número total de configurações de parâmetros equivalentes (com a mesma verossimilhança) é, portanto, $H!2^H$ [^572].

Para mitigar o overfitting, é comum usar técnicas de regularização, como o **early stopping** [^572]. Isso envolve interromper o treinamento quando o erro no conjunto de validação começa a aumentar [^572]. Outra técnica é impor um *prior* sobre os parâmetros e usar a estimativa MAP [^572]. É comum usar um *prior* $N(0, \alpha^{-1}I)$ (equivalente à regularização $L_2$) [^572], onde $\alpha$ é a precisão (força) do *prior* [^572]. Na literatura de redes neurais, isso é chamado de **weight decay**, pois incentiva pesos pequenos e, portanto, modelos mais simples [^572].

### Conclusão
O algoritmo de backpropagation é uma ferramenta essencial para o treinamento de MLPs, permitindo o aprendizado de modelos complexos com múltiplas camadas [^569]. Embora o algoritmo seja suscetível a problemas como mínimos locais e overfitting, técnicas de regularização e otimização podem ser empregadas para melhorar o desempenho e a generalização do modelo [^572]. A capacidade de calcular eficientemente os gradientes da função de perda em relação aos pesos da rede é o que torna possível o treinamento de redes neurais profundas com um grande número de parâmetros [^569]. <!-- END -->