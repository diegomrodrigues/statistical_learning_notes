## Feedforward Neural Networks (Multilayer Perceptrons)

### Introdução
Este capítulo explora em profundidade as **Feedforward Neural Networks (FNNs)**, também conhecidas como **Multilayer Perceptrons (MLPs)**, uma classe fundamental de modelos em *machine learning*. As FNNs representam uma série de modelos de regressão logística empilhados uns sobre os outros, culminando em uma camada final que pode ser tanto uma regressão logística quanto uma regressão linear, dependendo da natureza do problema – classificação ou regressão [^1].

### Conceitos Fundamentais

Uma **FNN** é estruturada como uma sequência de modelos de regressão logística sobrepostos, utilizando uma **função de ativação não linear** ou *transfer function* nas camadas ocultas para aprender padrões complexos [^1]. Este processo de empilhamento e ativação não linear permite que as FNNs modelem relações complexas entre entradas e saídas, algo que modelos lineares não conseguem capturar.

**Arquitetura e Formulação Matemática**

Uma FNN com duas camadas pode ser expressa matematicamente como:
$$\np(y|x, \theta) = N(y|w^Tz(x), \sigma^2)\n$$
$$\nz(x) = g(Vx)\n$$
onde:
*   $p(y|x, \theta)$ representa a probabilidade de saída $y$ dado a entrada $x$ e os parâmetros $\theta$.
*   $N(y|w^Tz(x), \sigma^2)$ é a distribuição normal com média $w^Tz(x)$ e variância $\sigma^2$.
*   $z(x)$ é a saída da camada oculta.
*   $g$ é uma **função de ativação não linear** ou *transfer function*.
*   $V$ é a matriz de pesos da primeira camada.
*   $w$ é o vetor de pesos da segunda camada.
*   $\sigma^2$ é a variância do ruído.

Os parâmetros do modelo MLP são $\theta = (V, W)$, as matrizes de peso da primeira e segunda camadas [^1]. O modelo geral pode ser escrito como:

$$\nx_n \rightarrow^V a_n \rightarrow^g z_n \rightarrow^W b_n \rightarrow^h \hat{y}_n\n$$

onde:
*   $x_n$ é a entrada.
*   $a_n = Vx_n$ é a saída pré-sináptica da primeira camada.
*   $z_n = g(a_n)$ é a saída pós-sináptica da primeira camada (camada oculta).
*   $b_n = Wz_n$ é a saída pré-sináptica da camada de saída.
*   $\hat{y}_n = h(b_n)$ é a saída pós-sináptica da camada de saída.

O algoritmo de treinamento é conhecido como **backpropagation**, porque os erros da camada 1 podem ser computados passando os erros da camada 2 de volta através da matriz $W$ [^1]. Este processo iterativo de ajuste de pesos permite que a rede aprenda a mapear entradas para saídas de maneira eficaz.

**Exemplo de Regressão com Duas Camadas**

Se estivermos resolvendo um problema de regressão com duas camadas, o modelo terá a forma [^1]:
$$\np(y|x, \theta) = N(y|w^Tz(x), \sigma^2)\n$$
$$\nz(x) = g(Vx) = [g(v^T_1x),..., g(v^T_Hx)]\n$$
onde $g$ é uma **função de ativação não linear** ou *transfer function*.

### Conclusão

As **Feedforward Neural Networks** representam uma ferramenta poderosa e flexível para modelagem de dados complexos. Sua capacidade de aprender representações não lineares e ajustar seus parâmetros por meio do algoritmo de **backpropagation** as torna adequadas para uma ampla gama de aplicações, desde classificação até regressão.

### Referências
[^1]: Texto fornecido.
<!-- END -->