## Feature Extraction in Multilayer Perceptrons

### Introdução
Este capítulo explora o papel crucial das **unidades ocultas** em Redes Neurais Feedforward (Multilayer Perceptrons - MLPs) na **extração de características** ou **construção de características**. A capacidade de aprender combinações não lineares das entradas originais torna as MLPs particularmente eficazes em cenários onde as *features* originais, por si só, não são informativas [^1]. Este processo é essencial para transformar os dados de entrada em uma representação mais útil para tarefas subsequentes, como classificação ou regressão.

### Conceitos Fundamentais
Em um MLP, a camada oculta, que é uma *função determinística da entrada*, é definida como $z(x) = \phi(x, V)$ [^1]. Aqui, $H$ representa o número de unidades ocultas e $V$ é a *matriz de pesos* que conecta as entradas aos nós ocultos [^1]. O vetor $w$ representa os *pesos* que conectam os nós ocultos à saída [^1].

A **extração de características** ou **construção de características** refere-se ao processo pelo qual as unidades ocultas aprendem combinações não lineares das entradas originais [^1]. Este processo pode ser crucial quando as *features* originais não são individualmente informativas [^1]. Por exemplo, em problemas de visão computacional, *pixels* individuais raramente são informativos por si só; é a combinação de *pixels* que revela a presença de objetos [^22]. Da mesma forma, em processamento de linguagem natural, combinações de palavras (ou *n-grams*) podem ser mais informativas do que palavras isoladas.

Para ilustrar a importância da não linearidade, considere o caso em que a *função de ativação* $g$ é linear [^22]. Nesse cenário, toda a rede se reduz a um modelo de regressão linear, perdendo a capacidade de modelar relações complexas nos dados [^22]. A não linearidade introduzida pelas *funções de ativação* permite que a rede capture interações e dependências complexas, tornando possível a extração de *features* relevantes [^22].

**Exemplo:**
Em uma tarefa de reconhecimento de imagens, cada *pixel* individualmente pode não ser informativo. No entanto, a combinação de *pixels* pode revelar bordas, texturas e formas, que são características essenciais para identificar objetos [^22]. As unidades ocultas aprendem a detectar essas combinações de *pixels*, transformando a entrada original em uma representação mais abstrata e útil.

**Funções de Ativação:**
A escolha da *função de ativação* é crucial para o desempenho da rede. Funções comuns incluem a *função sigmóide* ($ \sigma(x) = \frac{1}{1 + e^{-x}} $) e a *tangente hiperbólica* ($ tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $) [^28]. A *tangente hiperbólica* mapeia os valores para o intervalo [-1, 1] e é frequentemente preferida para as camadas ocultas, enquanto a *função sigmóide* mapeia para o intervalo [0, 1] e é comumente usada na camada de saída para problemas de classificação binária [^28]. A escolha da função depende da natureza do problema e das características dos dados.

**Arquiteturas Convolucionais:**
Em redes neurais convolucionais (CNNs), a extração de *features* é realizada por meio de *camadas convolucionais* que aplicam *filtros* (ou *kernels*) para detectar padrões locais nos dados de entrada [^23]. Esses *filtros* são aprendidos durante o treinamento e atuam como detectores de *features* específicas, como bordas ou texturas. O conceito de *campos receptivos locais* e o *compartilhamento de pesos* são fundamentais nas CNNs, permitindo que a rede aprenda *features* invariantes a translações [^23].

### Conclusão
A capacidade das MLPs de aprender combinações não lineares das entradas originais, por meio das unidades ocultas, é o que lhes confere poder e flexibilidade para lidar com uma ampla gama de problemas complexos [^1]. Este processo de extração de características é fundamental para transformar dados brutos em representações mais informativas e úteis para tarefas de *machine learning* [^1]. A escolha da *função de ativação* e a arquitetura da rede (por exemplo, CNNs) desempenham papéis cruciais na eficácia da extração de *features* [^22, 23].

### Referências
[^1]: Página 564
[^22]: Página 564
[^23]: Página 565
[^28]: Página 570
<!-- END -->