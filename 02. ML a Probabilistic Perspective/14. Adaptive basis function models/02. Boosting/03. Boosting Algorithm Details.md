## Forward Stagewise Additive Modeling em Boosting

### Introdução
Este capítulo explora o algoritmo de **Boosting**, focando especificamente no método de **Forward Stagewise Additive Modeling** [^555]. O objetivo do Boosting é resolver o problema de otimização definido na equação 16.25 [^555], utilizando modelos aditivos de base (ABM) como os descritos na equação 16.3 [^1]. O Forward Stagewise Additive Modeling, um componente crucial do Boosting, será detalhado, juntamente com técnicas de *shrinkage* para melhorar a performance do modelo.

### Conceitos Fundamentais

O **algoritmo de Boosting** é inicializado definindo $f_0(x) = \underset{f}{\text{argmin}} \sum L(y_i, f(x_i; 1))$ [^555]. Em cada iteração *m*, o algoritmo calcula $(\beta_m, \gamma_m) = \underset{\beta, \gamma}{\text{argmin}} \sum L(y_i, f_{m-1}(x_i) + \beta\phi(x; \gamma))$, define $f_m(x) = f_{m-1}(x) + \beta_m\phi(x; \gamma_m)$, e continua por um número fixo de iterações *M*, que é um parâmetro de ajuste [^555].

O método de **Forward Stagewise Additive Modeling** (FSAM) é utilizado, onde os parâmetros anteriores não são reajustados [^555]. Uma característica importante do FSAM é a possibilidade de realizar *“partial updates”* da forma $f_m(x) = f_{m-1}(x) + \nu\beta_m\phi(x; \gamma_m)$, onde $0 < \nu < 1$ é um parâmetro de tamanho de passo, comumente definido para um valor pequeno (e.g., 0.1), em uma técnica chamada **shrinkage** [^555]. O *shrinkage* ajuda a regularizar o modelo, reduzindo o risco de *overfitting*.

> A chave do método Forward Stagewise Additive Modeling é que não se volta atrás e ajusta os parâmetros anteriores. [^555]

A escolha do número de iterações *M* é crucial. Tipicamente, *M* é o principal parâmetro de ajuste do método [^555]. Uma abordagem comum é monitorar o desempenho em um conjunto de validação separado e parar quando o desempenho começar a diminuir, um processo conhecido como **early stopping** [^555]. Alternativamente, critérios de seleção de modelo como AIC ou BIC podem ser utilizados [^555].

Em termos práticos, um melhor desempenho (conjunto de teste) pode ser obtido realizando *“partial updates”* da forma $f_m(x) = f_{m-1}(x) + \nu\beta_m\phi(x; \gamma_m)$ [^555]. Aqui, $0 < \nu < 1$ é um parâmetro de *step-size*. Na prática, é comum utilizar um valor pequeno, como $\nu = 0.1$. Isso é chamado de **shrinkage** [^555].

### Conclusão

O Forward Stagewise Additive Modeling é um método poderoso para construir modelos de Boosting. A técnica de *shrinkage*, implementada através do parâmetro $\nu$, permite um controle mais fino sobre o processo de aprendizado, melhorando a capacidade de generalização do modelo. A escolha apropriada do número de iterações *M*, juntamente com o uso de *early stopping*, é fundamental para evitar o *overfitting* e obter um desempenho ótimo. A formulação apresentada é genérica e pode ser aplicada a diversas funções de perda, embora a escolha da função de perda possa influenciar a forma de resolver o subproblema na Equação 16.33 [^555].

### Referências
[^1]: Page 1, "An alternative approach is to dispense with kernels altogether, and try to learn useful features φ(x) directly from the input data."
[^555]: Page 555, "The boosting algorithm initializes by defining fo(x) = argmin Σ L(yi, f(xi; 1)), then computes (βm, γm) = argmin Σ L(yi, fm-1(xi) + βφ(x; γ)) at each iteration m, sets fm(x) = fm-1(x) + βmφ(x; γm), and continues for a fixed number of iterations M, which is a tuning parameter. Forward stagewise additive modeling is used, where we do not go back and adjust earlier parameters, and we can perform "partial updates" of the form fm(x) = fm-1(x) + νβmφ(x; γm), where 0 < ν < 1 is a step-size parameter, commonly set to a small value (e.g., 0.1), in a technique called shrinkage."
<!-- END -->