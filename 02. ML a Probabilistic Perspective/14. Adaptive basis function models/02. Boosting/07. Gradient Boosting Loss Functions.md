## Gradient Boosting: A Generic Approach to Boosting

### Introdução
O Gradient Boosting é uma generalização do método de Boosting que permite a criação de novas versões de Boosting para diferentes funções de perda [^555]. Ele se baseia no cálculo do gradiente da função de perda e no ajuste de um *weak learner* para aproximar o sinal do gradiente negativo. Essa abordagem oferece uma maneira flexível de lidar com diversas funções de perda, incluindo regressão robusta e regressão de Poisson [^555]. Este capítulo explora os fundamentos do Gradient Boosting, sua relação com o *functional gradient descent* e como ele se adapta a diferentes funções de perda.

### Conceitos Fundamentais

#### Gradient Boosting como Functional Gradient Descent
O Gradient Boosting pode ser interpretado como uma forma de *functional gradient descent* [^555]. O objetivo do Boosting é resolver o seguinte problema de otimização [^555]:
$$\
\min_{f} \sum_{i=1}^{N} L(y_i, f(x_i))
$$\
onde $L(y, \hat{y})$ é uma função de perda e $f$ é um modelo *adaptive basis-function model (ABM)* [^555]. Em vez de derivar novas versões de Boosting para cada função de perda, o Gradient Boosting fornece uma versão genérica [^560]. O gradiente de $L(f)$ é avaliado em $f = f_{m-1}$ [^560]:
$$\
g_{im} = \frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}|_{f=f_{m-1}}
$$ [^560]

Os gradientes de algumas funções de perda comuns são fornecidos na Tabela 16.1 [^556]. Em seguida, é feita uma atualização [^560]:
$$\
f_m = f_{m-1} - \rho_m g_m
$$ [^560]

#### Ajuste do Weak Learner ao Gradiente Negativo
Na sua forma atual, o *functional gradient descent* não é muito útil, pois otimiza $f$ em um conjunto fixo de $N$ pontos, sem aprender uma função que possa generalizar [^561]. Para contornar essa limitação, o algoritmo é modificado para ajustar um *weak learner* para aproximar o sinal do gradiente negativo [^561]. Ou seja, a seguinte atualização é utilizada [^561]:
$$\
\gamma_m = \underset{\gamma}{\text{argmin}} \sum_{i=1}^{N} (-g_{im} - \phi(x_i; \gamma))^2
$$ [^561]
onde $\phi(x; \gamma)$ é o *weak learner* [^561].

#### Algoritmo do Gradient Boosting
O algoritmo geral do Gradient Boosting é resumido da seguinte forma [^561]:

1. Inicialize $f_0(x) = \underset{\gamma}{\text{argmin}} \sum_{i=1}^{N} L(y_i, \phi(x_i; \gamma))$ [^561].
2. Para $m = 1:M$ [^561]:
    * Calcule o resíduo do gradiente usando $r_{im} = - \left[ \frac{\partial L(y_i, f(x_i))}{\partial f(x_i)} \right]_{f(x_i)=f_{m-1}(x_i)}$ [^561].
    * Use o *weak learner* para computar $\gamma_m$, que minimiza $\sum_{i=1}^{N} (r_{im} - \phi(x_i; \gamma_m))^2$ [^561].
    * Atualize $f_m(x) = f_{m-1}(x) + \nu \phi(x; \gamma_m)$ [^561].
3. Retorne $f(x) = f_M(x)$ [^561].

#### Exemplos de Gradient Boosting com Diferentes Funções de Perda
*   **L2Boosting (Least Squares Boosting):** Quando a função de perda utilizada é o erro quadrático, o algoritmo resultante é o L2Boosting [^561].
*   **BinomialBoost:** Ao aplicar o algoritmo para a *log-loss*, obtemos um algoritmo conhecido como BinomialBoost [^561]. Uma vantagem sobre o LogitBoost é que não é necessário realizar o ajuste ponderado [^561].

#### Sparse Boosting
Uma abordagem para *sparse boosting* envolve usar um *weak learner* que busca, dentre todas as variáveis possíveis $j = 1:D$, aquela que melhor prediz o vetor residual [^561]:
$$\
j(m) = \underset{j}{\text{argmin}} \sum_{i=1}^{N} (r_{im} - \beta_{jm}x_{ij})^2
$$ [^561]

O coeficiente $\beta_{jm}$ é dado por [^561]:
$$\
\beta_{jm} = \frac{\sum_{i=1}^{N} x_{ij}r_{im}}{\sum_{i=1}^{N} x_{ij}^2}
$$ [^561]

A atualização do modelo é então realizada da seguinte forma [^561]:
$$\
\phi_m(x) = \beta_{j(m),m} x_{j(m)}
$$ [^561]

#### MART (Multivariate Adaptive Regression Trees)
É comum usar modelos CART como *weak learners* [^562]. Árvores rasas são preferíveis para reduzir a variância [^562]. A altura da árvore é um parâmetro adicional [^562]. Se J = 2, obtemos um *stump*, que só pode dividir uma única variável [^562]. Se J = 3, permitimos interações de duas variáveis, etc [^562].

### Conclusão
O Gradient Boosting oferece uma abordagem genérica e flexível para o desenvolvimento de algoritmos de Boosting, adaptados a diferentes funções de perda. Ao combinar o functional gradient descent com *weak learners*, como árvores de decisão, o Gradient Boosting se torna uma ferramenta poderosa para uma variedade de problemas de regressão e classificação.

### Referências
[^555]: Capítulo 16, página 555
[^556]: Capítulo 16, página 556
[^560]: Capítulo 16, página 560
[^561]: Capítulo 16, página 561
[^562]: Capítulo 16, página 562
<!-- END -->