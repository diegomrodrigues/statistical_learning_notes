## Forward Stagewise Additive Modeling in Boosting

### Introdução
O *boosting* é um algoritmo ganancioso para ajustar modelos de função de base adaptativa (ABM), conforme discutido no Capítulo 16 [^554]. Este capítulo explora o *forward stagewise additive modeling*, um método fundamental dentro do framework de boosting, que visa resolver o problema de otimização de minimizar a soma de uma função de perda sobre os dados de treinamento, onde o modelo é um ABM [^555].  Este capítulo se concentrará na formulação, análise e propriedades deste método, com ênfase particular no caso em que a perda de erro quadrático é utilizada.

### Conceitos Fundamentais
O objetivo do boosting é resolver o seguinte problema de otimização [^555]:
$$ \min_{f} \sum_{i=1}^{N} L(y_i, f(x_i)) \qquad (16.25) $$
onde $L(y, \hat{y})$ é uma função de perda e $f$ é um ABM, como definido na Equação 16.3 [^1].

O *forward stagewise additive modeling* aborda este problema de forma incremental. Em vez de otimizar $f$ diretamente, o algoritmo constrói o modelo de forma iterativa, adicionando uma nova função de base a cada passo. Se utilizarmos a perda de erro quadrático, a estimativa ótima é dada por [^555]:
$$ f^*(x) = \underset{f(x)}{\text{argmin}} \mathbb{E}_{y|x} [(Y - f(x))^2] = \mathbb{E}[Y|x] \qquad (16.26) $$
Este resultado demonstra que, com a perda de erro quadrático, o modelo de boosting ideal aproxima a expectativa condicional da variável alvo dado as características de entrada. Isso conecta o boosting com a teoria da regressão e fornece uma justificativa teórica para seu uso em problemas de regressão.

O algoritmo *forward stagewise* procede da seguinte forma. Inicializamos com uma função $f_0(x)$ [^557]. Por exemplo, se utilizamos a perda de erro quadrático, podemos definir $f_0(x) = \bar{y}$, onde $\bar{y}$ é a média da variável de resposta. Alternativamente, para log-loss ou perda exponencial, podemos definir $f_0(x) = \frac{1}{2}\log{\frac{\hat{\pi}}{1-\hat{\pi}}}$, onde $\hat{\pi} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(y_i = 1)$.

Em cada iteração $m$, computamos [^557]:
$$ (\beta_m, \gamma_m) = \underset{\beta, \gamma}{\text{argmin}} \sum_{i=1}^{N} L(y_i, f_{m-1}(x_i) + \beta \phi(x_i; \gamma)) \qquad (16.33) $$
e então definimos [^557]:
$$ f_m(x) = f_{m-1}(x) + \beta_m \phi(x; \gamma_m) \qquad (16.34) $$
Um aspecto crucial é que não retrocedemos e ajustamos os parâmetros anteriores [^557]. É por isso que o método é chamado de *forward stagewise additive modeling*.

Continuamos este processo por um número fixo de iterações $M$ [^557]. De fato, $M$ é o principal parâmetro de ajuste do método. Frequentemente, escolhemos $M$ monitorando o desempenho em um conjunto de validação separado e, em seguida, parando quando o desempenho começa a diminuir; isso é chamado de *early stopping*. Alternativamente, podemos usar critérios de seleção de modelo, como AIC ou BIC.

Na prática, um melhor desempenho (conjunto de teste) pode ser obtido realizando “atualizações parciais” da forma [^557]:
$$ f_m(x) = f_{m-1}(x) + \nu \beta_m \phi(x; \gamma_m) \qquad (16.35) $$
Aqui, $0 < \nu < 1$ é um parâmetro de tamanho de passo. Na prática, é comum usar um valor pequeno, como $\nu = 0.1$. Isso é chamado de *shrinkage*.

### Conclusão

O *forward stagewise additive modeling* oferece uma abordagem flexível e poderosa para construir modelos preditivos, especialmente quando combinado com algoritmos de aprendizado fraco como árvores de decisão [^554]. Sua capacidade de aproximar a expectativa condicional sob perda de erro quadrático, juntamente com técnicas de regularização como *shrinkage* e *early stopping*, tornam-no uma ferramenta valiosa no arsenal de qualquer cientista de dados. A escolha da função de perda e do algoritmo de aprendizado fraco permite adaptar o boosting a uma ampla gama de problemas de regressão e classificação.

### Referências
[^1]: Chapter 16. Adaptive basis function models
[^554]: Boosting (Schapire and Freund 2012) is a greedy algorithm for fitting adaptive basis-function models of the form in Equation 16.3, where the om are generated by an algorithm called a weak learner or a base learner.
[^555]: The goal of boosting is to solve the following optimization problem:\nmin∑ L(yi, f (xi)) and L(y, ŷ) is some loss function, and f is assumed to be an ABM model as in Equation 16.3. If we use squared error loss, the optimal estimate is given by f*(x) = argmin = Ey\x [(Y − f(x))²] = E [Y|x]
[^557]: In each iteration m, we compute (βm, 7m) = argmin ∑ L(Yi, fm-1(xi) + βφ(x;y)) and then we set fm(x) = fm-1(x) + βm¢(x;ym) The key point is that we do not go back and adjust earlier parameters. This is why the method is called forward stagewise additive modeling.

<!-- END -->