## AdaBoost: Um Algoritmo de Boosting para Classificação Binária

### Introdução
Este capítulo explora o algoritmo **AdaBoost** (Adaptive Boosting), um método de *boosting* amplamente utilizado para problemas de classificação binária [^554]. O AdaBoost se destaca por sua capacidade de combinar múltiplos classificadores "fracos" em um classificador forte, minimizando uma função de perda exponencial [^554]. O algoritmo funciona iterativamente, ajustando pesos de amostra para focar em instâncias de treinamento mal classificadas, permitindo que classificadores subsequentes "corrijam" os erros de seus predecessores.

### Conceitos Fundamentais

#### Funcionamento do AdaBoost
O AdaBoost minimiza iterativamente a seguinte função de perda exponencial [^554]:

$$ L_m(\phi) = \sum_i \exp[-y_i(f_{m-1}(x_i) + \beta\phi(x_i))] = \sum_i w_{i,m} \exp(-\beta\hat{y}_i\phi(x_i)) $$

onde:

*   $L_m(\phi)$ é a função de perda a ser minimizada na etapa $m$.
*   $y_i \in \{-1, +1\}$ é o rótulo da classe para a amostra $i$.
*   $f_{m-1}(x_i)$ é o classificador combinado construído até a etapa $m-1$.
*   $\phi(x_i)$ é o classificador fraco a ser adicionado na etapa $m$.
*   $\beta$ é o peso atribuído ao classificador fraco $\phi(x_i)$.
*   $w_{i,m} = \exp(-y_i \cdot f_{m-1}(x_i))$ é o peso aplicado à amostra $i$ na etapa $m$.
*   $\hat{y}_i \in \{-1, +1\}$ é o rótulo da classe para a amostra $i$.

O classificador fraco ideal a ser adicionado é dado por [^554]:

$$ \phi_m = \underset{\phi}{\operatorname{argmin}} \sum_i W_{i,m} I(\hat{y}_i \neq \phi(x_i)) $$

onde $I(\cdot)$ é a função indicadora que retorna 1 se a condição for verdadeira e 0 caso contrário. Em outras palavras, $\phi_m$ é o classificador fraco que minimiza o erro ponderado nas amostras de treinamento.

O peso $\beta_m$ atribuído ao classificador fraco $\phi_m$ é calculado como [^554]:

$$ \beta_m = \frac{1}{2} \log\left(\frac{1 - \text{err}_m}{\text{err}_m}\right) $$

onde $\text{err}_m$ é o erro ponderado do classificador fraco $\phi_m$ [^558]:

$$ \text{err}_m = \frac{\sum_{i=1}^N w_{i,m} I[\hat{y}_i \neq \phi_m(x_i)]}{\sum_{i=1}^N w_{i,m}} $$

O processo de atualização de pesos é crucial para o desempenho do AdaBoost. Os pesos são atualizados da seguinte forma [^559]:

$$ w_{i,m+1} = w_{i,m} e^{-\beta_m \hat{y}_i \phi_m(x_i)} = w_{i,m} e^{\beta_m (2\mathbb{I}(y_i \neq \phi_m(x_i)) - 1)} = w_{i,m} e^{2\beta_m \mathbb{I}(y_i \neq \phi_m(x_i))} e^{-\beta_m} $$

Esta atualização pode ser simplificada, eliminando o termo $e^{-\beta_m}$ [^559], pois este termo cancela na etapa de normalização. O algoritmo AdaBoost.M1 é resumido no Algoritmo 16.2 [^559].

#### Algoritmo 16.2: AdaBoost.M1
1.  Inicialize os pesos: $w_i = 1/N$ para todo $i$ [^559].
2.  Para $m = 1$ até $M$ [^559]:
    *   Ajuste um classificador $\phi_m(x)$ ao conjunto de treinamento usando os pesos $w$ [^559].
    *   Calcule o erro ponderado: $\text{err}_m = \frac{\sum_{i=1}^N w_{i,m} I[\hat{y}_i \neq \phi_m(x_i)]}{\sum_{i=1}^N w_{i,m}}$ [^559].
    *   Calcule $\alpha_m = \log[(1 - \text{err}_m)/\text{err}_m]$ [^559].
    *   Atualize os pesos: $w_i \leftarrow w_i \exp[\alpha_m I(\hat{y}_i \neq \phi_m(x_i))]$ [^559].
3.  Retorne o classificador final: $\text{sgn}[\sum_{m=1}^M \alpha_m \phi_m(x)]$ [^559].

#### Vantagens e Desvantagens do AdaBoost
O AdaBoost possui várias vantagens notáveis:

*   **Simplicidade e Facilidade de Implementação:** O algoritmo é relativamente simples de entender e implementar [^554].
*   **Versatilidade:** Pode ser usado com diversos tipos de classificadores fracos [^554].
*   **Resistência ao Overfitting:** Surpreendentemente, o AdaBoost é resistente ao overfitting, mesmo após o erro de treinamento atingir zero [^555].
*   **Alta Precisão Preditiva:** Em muitos casos, o AdaBoost alcança alta precisão preditiva [^555].

No entanto, também possui algumas desvantagens:

*   **Sensibilidade a Outliers:** O algoritmo é sensível a outliers, pois atribui pesos elevados a amostras mal classificadas [^559].
*   **Potencial para Modelos Complexos:** Pode levar a modelos complexos se o número de iteracões for muito grande [^559].

### Conclusão
AdaBoost é um algoritmo de *boosting* poderoso e versátil para classificação binária. Sua capacidade de combinar classificadores fracos em um classificador forte, juntamente com sua resistência ao overfitting, o tornam uma escolha popular em diversas aplicações. No entanto, é importante estar ciente de sua sensibilidade a outliers e do potencial para modelos complexos. Outras variações, como o LogitBoost [^559], abordam algumas dessas limitações.

### Referências
[^554]: Capítulo 16 do texto fornecido.
[^555]: Capítulo 16 do texto fornecido.
[^558]: Capítulo 16 do texto fornecido.
[^559]: Capítulo 16 do texto fornecido.

<!-- END -->