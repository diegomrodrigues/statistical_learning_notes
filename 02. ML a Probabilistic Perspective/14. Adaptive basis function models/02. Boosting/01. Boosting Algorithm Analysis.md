## Boosting: Um Algoritmo Ganancioso para Modelos Adaptativos de Função de Base

### Introdução
O *boosting* é um algoritmo ganancioso utilizado para ajustar modelos adaptativos de função de base (ABM), onde *weak learners* ou *base learners* são aplicados sequencialmente a versões ponderadas dos dados, atribuindo maior peso aos exemplos mal classificados [^554]. O objetivo do boosting é resolver o problema de otimização minf Σ L(yi, f(xi)), onde L(y, ŷ) é uma função de perda e f é um modelo ABM [^555].

### Conceitos Fundamentais

O *boosting* pode ser interpretado como uma forma de *gradient descent* no espaço de funções [^555]. O algoritmo aplica o *weak learner* sequencialmente a versões ponderadas dos dados, dando mais peso aos exemplos mal classificados. O boosting maximiza a margem nos dados de treinamento [^554].

Com a perda de erro quadrático, a estimativa ótima é [^555]:
$$ f*(x) = \underset{f(x)}{\text{argmin}} = E_{y|x}[(Y – f(x))^2] = E[Y|x] $$

Para classificação binária, *logloss* é frequentemente usado, o qual fornece um limite superior convexo na perda 0-1, e a estimativa ótima é [^555]:
$$ f*(x) = \frac{1}{2} \log\left(\frac{p(\tilde{y} = 1|x)}{p(\tilde{y} = -1|x)}\right) $$
onde ŷ ∈ {−1,+1} [^555].

O processo de *boosting* envolve a construção sequencial de modelos, onde cada modelo subsequente tenta corrigir os erros cometidos pelos modelos anteriores [^554]. Isso é alcançado ajustando os pesos dos dados, de modo que os exemplos mal classificados recebam maior atenção nas iterações subsequentes [^554].

#### Forward Stagewise Additive Modeling
O objetivo do *boosting* é resolver o seguinte problema de otimização [^555]:
$$ \underset{f}{\text{min}} \sum_{i=1}^{N} L(y_i, f(x_i)) $$
onde $L(y, \hat{y})$ é alguma função de perda, e $f$ é assumido ser um modelo ABM como na Equação 16.3 [^1]. Escolhas comuns para a função de perda estão listadas na Tabela 16.1 [^556].

Se usarmos a perda de erro quadrático, a estimativa ótima é dada por [^555]:
$$ f^*(x) = \underset{f(x)}{\text{argmin}} E_{y|x} [(Y - f(x))^2] = E[Y|x] $$

#### L2Boosting
Suponha que usamos a perda de erro quadrático [^557]. Então, no passo $m$, a perda tem a forma [^557]:
$$ L(y_i, f_{m-1}(x_i) + \beta\phi(x_i; \gamma)) = (r_{im} - \phi(x_i; \gamma))^2 $$
onde $r_{im} = y_i - f_{m-1}(x_i)$ é o resíduo atual [^558], e nós definimos $\beta = 1$ sem perda de generalidade [^558]. Portanto, podemos encontrar a nova função base usando o *weak learner* para prever $r_m$ [^558]. Isso é chamado de *L2boosting*, ou *least squares boosting* [^558].

#### AdaBoost
Considere um problema de classificação binária com perda exponencial [^558]. No passo $m$, temos que minimizar [^558]:
$$ L_m(\phi) = \sum_{i=1}^{N} \text{exp}[-y_i(f_{m-1}(x_i) + \beta\phi(x_i))] = \sum_{i=1}^{N} w_{i,m} \text{exp}(-\beta \tilde{y}_i\phi(x_i)) $$
onde $w_{i,m} = \text{exp}(-y_i f_{m-1}(x_i))$ é um peso aplicado ao caso de dado $i$, e $\tilde{y}_i \in \{-1, +1\}$ [^558]. Podemos reescrever este objetivo como segue [^558]:
$$ L_m = e^{-\beta} \sum_{\tilde{y}_i = \phi(x_i)} w_{i,m} + e^{\beta} \sum_{\tilde{y}_i \neq \phi(x_i)} w_{i,m} $$
$$ = (e^{\beta} - e^{-\beta}) \sum_{i=1}^{N} w_{i,m} \mathbb{I}(\tilde{y}_i \neq \phi(x_i)) + e^{-\beta} \sum_{i=1}^{N} w_{i,m} $$
Consequentemente, a função ótima para adicionar é [^558]:
$$ \phi_m = \underset{\phi}{\text{argmin}} \sum_{i=1}^{N} w_{i,m} \mathbb{I}(\tilde{y}_i \neq \phi(x_i)) $$
Isso pode ser encontrado aplicando o *weak learner* a uma versão ponderada do conjunto de dados, com pesos $w_{i,m}$ [^558]. Substituindo $\phi_m$ em $L_m$ e resolvendo para $\beta$, encontramos [^558]:
$$ \beta_m = \frac{1}{2} \text{log} \frac{1 - \text{err}_m}{\text{err}_m} $$
onde [^559]:
$$ \text{err}_m = \frac{\sum_{i=1}^{N} w_{i,m} \mathbb{I}[\tilde{y}_i \neq \phi_m(x_i)]}{\sum_{i=1}^{N} w_{i,m}} $$
A atualização geral se torna [^559]:
$$ f_m(x) = f_{m-1}(x) + \beta_m \phi_m(x) $$
Com isso, os pesos na próxima iteração se tornam [^559]:
$$ w_{i,m+1} = w_{i,m} e^{-\beta_m \tilde{y}_i \phi_m(x_i)} = w_{i,m} e^{\beta_m (2\mathbb{I}(y_i \neq \phi_m(x_i)) - 1)} = w_{i,m} e^{2\beta_m \mathbb{I}(y_i \neq \phi_m(x_i))} e^{-\beta_m} $$

### Conclusão
O boosting, como um algoritmo ganancioso para modelos adaptativos de função de base, oferece uma abordagem poderosa para resolver problemas complexos de otimização em aprendizado de máquina [^554]. Sua capacidade de combinar *weak learners* em um modelo forte, juntamente com sua resistência ao *overfitting*, o torna uma técnica valiosa em diversas aplicações [^555].

### Referências
[^554]: Capítulo 16, Introduction.
[^555]: Capítulo 16, Forward stagewise additive modeling.
[^556]: Table 16.1.
[^557]: Capítulo 16, L2boosting.
[^558]: Capítulo 16, AdaBoost.
[^559]: Capítulo 16, where.
<!-- END -->