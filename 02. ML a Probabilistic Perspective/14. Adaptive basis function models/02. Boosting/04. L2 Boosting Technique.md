## L2boosting: Least Squares Boosting

### Introdução
Este capítulo explora o **L2boosting**, uma técnica de boosting que utiliza o erro quadrático como função de perda. O L2boosting, também conhecido como *least squares boosting* [^558], é uma abordagem que visa minimizar a soma dos quadrados dos resíduos, permitindo uma adaptação eficaz do modelo aos dados. O objetivo principal do L2boosting é encontrar uma nova função base utilizando um weak learner para prever o resíduo atual [^557], o que o torna equivalente ao boosting de mínimos quadrados.

### Conceitos Fundamentais
No L2boosting, a função de perda para o erro quadrático assume a forma [^557]:

$$
L(y_i, f_{m-1}(x_i) + \beta\phi(x_i; \gamma)) = (r_{im} - \phi(x_i; \gamma))^2
$$

onde:
- $y_i$ é o valor real da variável dependente para a i-ésima observação.
- $f_{m-1}(x_i)$ é a previsão do modelo no passo anterior (m-1) para a i-ésima observação.
- $\beta$ é o peso atribuído à nova função base.
- $\phi(x_i; \gamma)$ é a nova função base, parametrizada por $\gamma$, que será adicionada ao modelo.
- $r_{im} = y_i - f_{m-1}(x_i)$ é o resíduo atual, representando a diferença entre o valor real e a previsão do modelo no passo anterior [^557].

O objetivo é encontrar a função base $\phi(x_i; \gamma)$ que minimize a soma dos quadrados dos resíduos [^557]. Isso é feito utilizando um **weak learner** para prever os resíduos $r_m$. O weak learner é um modelo simples, como uma árvore de decisão rasa ou um modelo linear, que é capaz de capturar padrões nos dados, mas não é complexo o suficiente para ajustar o ruído [^554].

O algoritmo L2boosting pode ser resumido da seguinte forma:
1. Inicializar o modelo $f_0(x) = 0$ [^557].
2. Para $m = 1$ até $M$ (número de iterações):
   a. Calcular os resíduos $r_{im} = y_i - f_{m-1}(x_i)$ para cada observação $i$ [^557].
   b. Ajustar um weak learner aos resíduos $r_m$ para obter uma nova função base $\phi(x; \gamma)$ [^557].
   c. Calcular o peso $\beta$ atribuído à nova função base. No caso do erro quadrático, $\beta$ pode ser calculado por meio de mínimos quadrados.
   d. Atualizar o modelo: $f_m(x) = f_{m-1}(x) + \beta\phi(x; \gamma)$ [^557].
3. Retornar o modelo final $f_M(x)$.

Este processo iterativo permite que o modelo se ajuste gradualmente aos dados, corrigindo os erros cometidos nos passos anteriores. O L2boosting é uma técnica eficaz para melhorar a precisão de modelos de aprendizado de máquina, especialmente quando combinada com weak learners adequados. Em particular, como mencionado, o L2boosting, com uma escolha apropriada de weak learner, pode fornecer os mesmos resultados que o LARS (Least Angle Regression) [^558].

### Conclusão
O L2boosting é uma técnica de boosting poderosa e flexível que utiliza o erro quadrático como função de perda. Sua capacidade de minimizar os resíduos por meio de weak learners permite uma adaptação eficaz aos dados, resultando em modelos de alta precisão. A equivalência com o least squares boosting e a possibilidade de obter os mesmos resultados que o LARS destacam a relevância do L2boosting no campo do aprendizado de máquina. Além disso, a conexão com métodos de seleção de variáveis, como o LARS, oferece oportunidades para aprimorar a interpretabilidade e a eficiência dos modelos resultantes.

### Referências
[^557]: Chapter 16, Adaptive basis function models, page 557
[^558]: Chapter 16, Adaptive basis function models, page 558
[^554]: Chapter 16, Adaptive basis function models, page 554

<!-- END -->