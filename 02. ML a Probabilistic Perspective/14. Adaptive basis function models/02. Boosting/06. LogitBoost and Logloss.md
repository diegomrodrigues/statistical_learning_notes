## LogitBoost: Linear Punishment for Enhanced Probability Estimation

### Introdução
Em métodos de **Boosting**, o objetivo é combinar múltiplos *learners* fracos para criar um *learner* forte. No entanto, a escolha da **função de perda** (loss function) desempenha um papel crucial no desempenho e nas propriedades do algoritmo resultante. A **perda exponencial**, utilizada no AdaBoost, pode ser excessivamente sensível a exemplos mal classificados, dando-lhes um peso desproporcional. Para mitigar esse problema e permitir a extração de probabilidades mais precisas, o **LogitBoost** emprega a **logloss** [^17].

### Conceitos Fundamentais
#### Limitações da Perda Exponencial
A **perda exponencial**, definida como $L(\tilde{y}, f) = exp(-\tilde{y}f)$ [^14], penaliza os erros de forma exponencial. Embora isso possa levar a uma rápida redução do erro de treinamento, também pode tornar o algoritmo excessivamente sensível a *outliers* e exemplos mal rotulados [^17]. Essa sensibilidade resulta em um peso desproporcionalmente alto atribuído a esses exemplos, o que pode prejudicar a generalização e a robustez do modelo [^17]. Além disso, a perda exponencial não corresponde ao logaritmo de uma função de massa de probabilidade (pmf) para variáveis binárias, o que impede a recuperação direta de estimativas de probabilidade a partir da função aprendida [^17].

#### Vantagens da Logloss
A **logloss**, também conhecida como **perda de entropia cruzada** ou **perda logística**, é definida como $L(y, \hat{y}) = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$ ou, para o caso binário, $L(ỹ, f) = log(1 + e^{-ỹf})$ [^14]. Ao contrário da perda exponencial, a logloss penaliza os erros de forma linear, o que a torna menos sensível a *outliers* e mais robusta a exemplos mal rotulados [^17]. Essa penalização linear permite que o LogitBoost se concentre em melhorar o desempenho geral, em vez de se deixar dominar por exemplos individuais difíceis. Além disso, a logloss está diretamente relacionada ao logaritmo da função de massa de probabilidade (pmf) para variáveis binárias, permitindo a extração de estimativas de probabilidade bem calibradas a partir da função aprendida [^17]. Especificamente, a probabilidade de pertencimento à classe 1 pode ser estimada como $p(y=1|x) = \frac{1}{1 + e^{-2f(x)}}$ [^18].

#### O Algoritmo LogitBoost
O LogitBoost, detalhado no Algorithm 16.3 [^18], utiliza uma abordagem de Newton para minimizar a logloss. O algoritmo procede da seguinte forma:
1.  Inicializa os pesos $w_i = 1/N$ e as probabilidades $\pi_i = 1/2$ [^18].
2.  Para cada iteração $m$:
    *   Calcula a resposta de trabalho $z_i = \frac{y_i^* - \pi_i}{\pi_i(1-\pi_i)}$ [^18].
    *   Calcula os pesos $w_i = \pi_i(1 - \pi_i)$ [^18].
    *   Ajusta um learner fraco $φ_m$ para minimizar $\sum_{i=1}^{N} w_i(z_i - φ(x_i))^2$ [^18].
    *   Atualiza $f(x) \leftarrow f(x) + φ_m(x)$ [^18].
    *   Atualiza $\pi_i = \frac{1}{1 + exp(-2f(x_i))}$ [^18].
3.  Retorna $f(x) = sgn[\sum_{m=1}^{M} α_m φ_m(x)]$ [^18].

Essa abordagem iterativa permite que o LogitBoost refine gradualmente a função aprendida, ajustando os pesos e as probabilidades com base nos erros cometidos nas iterações anteriores.

### Conclusão
O LogitBoost oferece uma alternativa atraente ao AdaBoost, especialmente quando a robustez a *outliers* e a estimativa precisa de probabilidades são importantes. Ao empregar a logloss em vez da perda exponencial, o LogitBoost reduz a sensibilidade a exemplos mal classificados e permite a extração de estimativas de probabilidade bem calibradas. Essas propriedades tornam o LogitBoost uma escolha valiosa para uma ampla gama de aplicações de classificação.

### Referências
[^17]: Capítulo 16, página 559: "The trouble with exponential loss is that it puts a lot of weight on misclassified examples, as is apparent from the exponential blowup on the left hand side of Figure 16.9. This makes the method very sensitive to outliers (mislabeled examples). In addition, $e^{-yf}$ is not the logarithm of any pmf for binary variables $y \in \{-1, +1\}$; consequently we cannot recover probability estimates from $f(x)$."
[^14]: Capítulo 16, página 556: "Exponential loss $exp(-yif(xi))$"
[^14]: Capítulo 16, página 556: "Logloss $log(1+e^{-Yifi})$"
[^18]: Capítulo 16, página 560: "Algorithm 16.3: LogitBoost, for binary classification with log-loss"
<!-- END -->