## Bayesian Inference in Linear Gaussian Systems: Noisy Measurements

### Introdução
Este capítulo explora a aplicação da inferência Bayesiana em sistemas Gaussianos lineares, especificamente no contexto de inferir uma quantidade escalar desconhecida a partir de medições ruidosas. A combinação de um *prior* Gaussiano com uma *likelihood* Gaussiana resulta em uma *posterior* que equilibra as informações do *prior* com as das medições [^1]. Este tópico é fundamental para diversos modelos que serão explorados em capítulos futuros [^1].

### Conceitos Fundamentais

Considere o cenário onde temos $N$ medições ruidosas $y_i$ de uma quantidade subjacente $x$. Assumimos que cada medição tem uma precisão fixa $\lambda_y = 1/\sigma^2$ [^1]. Além disso, temos um *prior* Gaussiano sobre $x$ dado por $p(x) = N(x|\mu_0, \lambda_0^{-1})$, onde $\mu_0$ é a média *a priori* e $\lambda_0^{-1}$ é a variância *a priori* [^1].

O objetivo é calcular a distribuição *a posteriori* de $x$ dado o conjunto de medições $y = \\{y_1, y_2, ..., y_N\\}$. Utilizando o teorema de Bayes, podemos expressar a *posterior* como:

$$p(x|y) \propto p(y|x)p(x)$$

Como tanto o *prior* quanto a *likelihood* são Gaussianos, a *posterior* também será Gaussiana, dada por $p(x|y) = N(x|\mu_N, \lambda_N^{-1})$, onde $\mu_N$ e $\lambda_N^{-1}$ são a média e a variância *a posteriori*, respectivamente [^1].

As equações para calcular $\lambda_N$ e $\mu_N$ são derivadas da combinação das distribuições Gaussianas do *prior* e da *likelihood* [^1]:

$$\lambda_N = \lambda_0 + N\lambda_y$$

$$\mu_N = \frac{N\lambda_y \bar{y} + \lambda_0 \mu_0}{\lambda_N}$$

onde $\bar{y}$ é a média das medições, dada por $\bar{y} = \frac{1}{N}\sum_{i=1}^{N} y_i$.

**Interpretação:**

*   $\lambda_N$ representa a precisão *a posteriori*, que é a soma da precisão *a priori* ($\lambda_0$) e a precisão total das medições ($N\lambda_y$) [^1]. Isso significa que, quanto mais medições tivermos ou quanto mais precisas forem as medições, maior será a precisão *a posteriori*.
*   $\mu_N$ representa a média *a posteriori*, que é uma média ponderada da média *a priori* ($\mu_0$) e da média das medições ($\bar{y}$) [^1]. Os pesos são determinados pelas precisões relativas do *prior* e das medições. Se a precisão das medições for alta em relação à precisão do *prior*, a média *a posteriori* estará mais próxima da média das medições, e vice-versa.

**Exemplo:**

Considere que temos um *prior* sobre a temperatura de um objeto como $N(x|25, 0.1)$, ou seja, acreditamos que a temperatura é de 25 graus Celsius com uma precisão de 0.1. Realizamos 5 medições com um termômetro com precisão $\lambda_y = 0.5$, e obtemos as seguintes leituras: 26, 27, 25.5, 26.5, 27. A média das medições é $\bar{y} = 26.4$.

Usando as equações acima, podemos calcular a *posterior*:\n
$$\lambda_N = 0.1 + 5 \cdot 0.5 = 2.6$$
$$\mu_N = \frac{5 \cdot 0.5 \cdot 26.4 + 0.1 \cdot 25}{2.6} = \frac{66 + 2.5}{2.6} = 26.346$$

Portanto, a *posterior* é $N(x|26.346, 1/2.6)$, o que significa que nossa estimativa atualizada da temperatura é de 26.346 graus Celsius, com uma precisão de 2.6.

### Conclusão
A inferência Bayesiana fornece uma estrutura coerente para combinar conhecimento prévio com evidências empíricas, resultando em uma estimativa *a posteriori* que reflete tanto o *prior* quanto os dados observados. A formulação Gaussiana linear simplifica os cálculos e permite uma interpretação intuitiva dos resultados [^1]. Este modelo básico pode ser estendido e adaptado para lidar com cenários mais complexos, como a fusão de sensores com diferentes precisões ou a interpolação de dados ruidosos [^26].

### Referências
[^1]: Capítulo 4, "Gaussian models"
<!-- END -->