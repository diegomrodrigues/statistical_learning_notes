## Linear Gaussian Systems: Bayesian Inference and Properties

### Introdução
Este capítulo aprofunda o conceito de **Sistemas Lineares Gaussianos**, explorando a inferência Bayesiana em sistemas onde uma variável oculta (x) e uma observação ruidosa (y) seguem distribuições Gaussianas [^1]. Sistemas Lineares Gaussianos, caracterizados por um *prior* Gaussiano e uma *likelihood* Gaussiana, oferecem um arcabouço analítico para modelar e inferir sobre variáveis latentes em uma variedade de aplicações [^1].

### Conceitos Fundamentais

Um **Sistema Linear Gaussiano** é definido por um *prior* Gaussiano [^1]:
$$ p(x) = N(x|\mu_x, \Sigma_x) $$
e uma *likelihood* Gaussiana [^1]:
$$ p(y|x) = N(y|Ax + b, \Sigma_y) $$
onde \(A\) é uma matriz de tamanho \(D_y \times D_x\) [^1]. Aqui, \(x\) representa a variável latente, \(y\) a observação, \(\mu_x\) e \(\Sigma_x\) são a média e a covariância do *prior* sobre \(x\), e \(Ax + b\) e \(\Sigma_y\) são a média e a covariância da *likelihood* de \(y\) dado \(x\).

O objetivo principal é realizar a **inferência Bayesiana** de \(x\) dado \(y\), ou seja, calcular a distribuição *posterior* \(p(x|y)\) [^1]. Pelo Teorema de Bayes, temos:
$$ p(x|y) = \frac{p(y|x)p(x)}{p(y)} $$
onde \(p(y)\) é a evidência, que serve como fator de normalização [^1].

**Teorema 4.4.1 (Regra de Bayes para Sistemas Lineares Gaussianos)**: Dado um sistema linear Gaussiano, como definido em [^1], a distribuição *posterior* \(p(x|y)\) é dada por [^1]:
$$ p(x|y) = N(x|\mu_{x|y}, \Sigma_{x|y}) $$
$$ \Sigma_{x|y} = (\Sigma_x^{-1} + A^T \Sigma_y^{-1} A)^{-1} $$
$$ \mu_{x|y} = \Sigma_{x|y} [A^T \Sigma_y^{-1} (y - b) + \Sigma_x^{-1} \mu_x] $$
A constante de normalização \(p(y)\) é dada por [^1]:
$$ p(y) = N(y|A\mu_x + b, \Sigma_y + A\Sigma_x A^T) $$

**Exemplo 4.4.2.1: Inferindo um escalar desconhecido a partir de medições ruidosas** [^1]
Suponha que façamos N medições ruidosas \(y_i\) de uma certa quantidade subjacente \(x\). Assumimos que o ruído de medição tem precisão fixa \(\lambda_y = 1/\sigma^2\), então a *likelihood* é [^1]:
$$ p(y_i|x) = N(y_i|x, \lambda_y^{-1}) $$
Agora vamos usar um *prior* Gaussiano para o valor da fonte desconhecida [^1]:
$$ p(x) = N(x|\mu_0, \lambda_0^{-1}) $$
Queremos calcular \(p(x|y_1,...,y_N,\sigma^2)\). Podemos converter isso para uma forma que nos permita aplicar a regra de Bayes para Gaussianas definindo \(y=(y_1,...,y_N)\), \(A = 1^T\) (um vetor linha 1xN de 1s) e \(\Sigma_y^{-1} = diag(\lambda_y I)\). Então obtemos [^1]:

$$ p(x|y) = N(x|\mu_N,\lambda_N^{-1}) $$
$$ \lambda_N = \lambda_0 + N \lambda_y $$
$$ \mu_N = \frac{N \lambda_y \bar{y} + \lambda_0 \mu_0}{\lambda_N} = \frac{N \lambda_y}{\lambda_N} \bar{y} + \frac{\lambda_0}{\lambda_N} \mu_0 $$

### Conclusão
Sistemas Lineares Gaussianos oferecem um arcabouço poderoso e tratável para a inferência Bayesiana [^1]. A propriedade de que o *posterior* também é Gaussiano simplifica significativamente os cálculos e permite a análise de modelos mais complexos [^1]. As aplicações de sistemas lineares gaussianos são vastas e incluem rastreamento, fusão de sensores e modelos de espaços de estados [^1].

### Referências
[^1]: Murphy, Kevin P. *Machine learning: a probabilistic perspective*. MIT press, 2012.

<!-- END -->