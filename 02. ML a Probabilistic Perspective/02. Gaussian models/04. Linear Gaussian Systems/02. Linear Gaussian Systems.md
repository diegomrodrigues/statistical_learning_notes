## Inferência em Sistemas Gaussianos Lineares: A Distribuição Posterior e a Normalização

### Introdução
Este capítulo aprofunda o estudo dos **sistemas gaussianos lineares**, com foco especial na derivação e interpretação da **distribuição posterior** e da **constante de normalização** [^1]. Sistemas gaussianos lineares são fundamentais em diversas áreas, incluindo inferência Bayesiana, filtragem de Kalman e modelagem de espaços de estados. O conhecimento detalhado da distribuição posterior, $p(x|y)$, e da constante de normalização, $p(y)$, é essencial para a aplicação eficaz desses modelos.

### Conceitos Fundamentais

Em um sistema gaussiano linear, a relação entre as variáveis latentes $x$ e as observações $y$ é modelada por uma distribuição gaussiana [^1]. Especificamente, assume-se que a distribuição *a priori* de $x$ é gaussiana, dada por $p(x) = N(x|\mu_x, \Sigma_x)$, e a distribuição condicional de $y$ dado $x$ também é gaussiana, expressa como $p(y|x) = N(y|Ax + b, \Sigma_y)$ [^1]. Onde $A$ é uma matriz que define a relação linear entre $x$ e $y$, e $b$ é um vetor de deslocamento. As matrizes $\Sigma_x$ e $\Sigma_y$ representam as matrizes de covariância de $x$ e $y$, respectivamente.

A **distribuição posterior** $p(x|y)$ representa a crença atualizada sobre $x$ após observar $y$. Em um sistema gaussiano linear, a posterior também é gaussiana, o que simplifica significativamente a inferência [^1]. A forma da posterior é dada por:

$$p(x|y) = N(x|\mu_{x|y}, \Sigma_{x|y})$$

onde a **matriz de covariância posterior** é:

$$Sigma_{x|y} = (\Sigma_x^{-1} + A^T \Sigma_y^{-1} A)^{-1}$$

e o **vetor de média posterior** é:

$$mu_{x|y} = \Sigma_{x|y} [A^T \Sigma_y^{-1} (y - b) + \Sigma_x^{-1} \mu_x]$$

A derivação dessas equações envolve a aplicação do teorema de Bayes e a manipulação de distribuições gaussianas [^1]. A matriz de covariância posterior $\Sigma_{x|y}$ representa a incerteza restante sobre $x$ após observar $y$. A média posterior $\mu_{x|y}$ é uma combinação ponderada da média *a priori* $\mu_x$ e da informação contida na observação $y$.

A **constante de normalização** $p(y)$ é a probabilidade marginal de observar $y$, independentemente do valor de $x$. Em um sistema gaussiano linear, $p(y)$ também é gaussiana, dada por:

$$p(y) = N(y|A\mu_x + b, \Sigma_y + A\Sigma_x A^T)$$

Essa constante é crucial para a inferência Bayesiana, pois garante que a posterior seja uma distribuição de probabilidade válida [^1]. Ela também pode ser usada para comparar diferentes modelos, através do cálculo do *Bayes factor*.

### Conclusão

Este capítulo apresentou uma análise detalhada da distribuição posterior e da constante de normalização em sistemas gaussianos lineares [^1]. Através da aplicação do teorema de Bayes e da manipulação de distribuições gaussianas, derivamos expressões explícitas para a média e covariância da posterior, bem como para a constante de normalização.

<!-- END -->