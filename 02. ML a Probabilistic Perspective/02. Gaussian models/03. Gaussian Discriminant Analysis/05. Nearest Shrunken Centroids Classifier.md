## Nearest Shrunken Centroids Classifier in Gaussian Discriminant Analysis

### Introdução
Este capítulo explora o classificador de **Nearest Shrunken Centroids** como um método para **Diagonal LDA** (Linear Discriminant Analysis). Este classificador se baseia em um subconjunto de *features* e realiza a estimativa MAP (Maximum A Posteriori) para Diagonal LDA com um prior de Laplace, que promove a *sparsity* [^13]. Como vimos anteriormente na seção sobre Linear Discriminant Analysis (LDA) [^4.2.2], uma simplificação é alcançada quando as matrizes de covariância são *tied* ou *shared* entre as classes. Expandindo este conceito, o classificador de Nearest Shrunken Centroids introduz um mecanismo para selecionar um subconjunto de *features*, melhorando a interpretabilidade e, potencialmente, a acurácia em problemas de alta dimensionalidade [^4.2.8].

### Conceitos Fundamentais
O classificador de Nearest Shrunken Centroids define a média da *feature* específica da classe, $\mu_{cj}$, em termos da média da *feature* independente da classe, $m_j$, e um *offset* específico da classe, $\Delta_{cj}$, conforme a seguinte equação [^13]:

$$\mu_{cj} = m_j + \Delta_{cj}$$

Aqui, $m_j$ representa a média da *feature* $j$ sobre todas as classes, e $\Delta_{cj}$ representa o *offset* da classe $c$ para a *feature* $j$. A ideia central é impor um *prior* nas *offsets* $\Delta_{cj}$ para forçar muitos deles a serem exatamente zero, levando a um modelo *sparse*. Este *prior* é tipicamente um *prior* de Laplace, que promove a *sparsity*.

A motivação para usar um prior de Laplace é induzir *sparsity* no modelo [^4.2.8]. Ao forçar alguns $\Delta_{cj}$ a serem zero, certas *features* são efetivamente removidas do modelo, o que pode melhorar a generalização e a interpretabilidade, especialmente em contextos de alta dimensão onde o número de *features* é grande em relação ao número de amostras.

A estimativa MAP para os $\Delta_{cj}$ é obtida maximizando a densidade *a posteriori*, que é proporcional ao produto da *likelihood* e o *prior*. O prior de Laplace tem a forma:

$$p(\Delta_{cj}) \propto \exp\left(-\lambda |\Delta_{cj}|\right)$$

onde $\lambda$ é um parâmetro de *shrinkage* que controla a *sparsity* do modelo. Um $\lambda$ maior resulta em mais *offsets* sendo forçados a zero.

O processo de *shrinkage* envolve a modificação das médias das *features* específicas da classe, $\mu_{cj}$, "encolhendo-as" em direção à média global, $m_j$. Isso é feito ajustando os valores de $\Delta_{cj}$ com base no parâmetro $\lambda$. Se um $\Delta_{cj}$ é pequeno o suficiente, ele é definido como zero, efetivamente removendo a *feature* $j$ para a classe $c$.

A regra de decisão para classificar uma nova observação $x$ é baseada na distância da observação aos *centroids* encolhidos. A classe prevista é a classe cujo *centroid* encolhido está mais próximo de $x$.

### Conclusão
O classificador de Nearest Shrunken Centroids oferece uma abordagem interessante para a classificação em cenários de alta dimensão, combinando a simplicidade do Diagonal LDA com a capacidade de selecionar *features* relevantes por meio do *shrinkage* [^4.2.8]. Ao impor um *prior* de Laplace nos *offsets* específicos da classe, o modelo se torna *sparse* e mais interpretável, o que pode levar a um melhor desempenho de generalização. A escolha do parâmetro de *shrinkage* $\lambda$ é crucial e pode ser feita usando técnicas de validação cruzada [^4.2.8].

### Referências
[^13]: Seção 4.2.8
[^4.2.2]: Seção 4.2.2
[^4.2.8]: Seção 4.2.8
<!-- END -->