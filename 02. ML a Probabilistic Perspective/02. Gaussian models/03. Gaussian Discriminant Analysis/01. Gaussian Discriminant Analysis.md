## Gaussian Discriminant Analysis: A Probabilistic Approach to Classification

### Introdução
Este capítulo explora o **Gaussian Discriminant Analysis (GDA)**, uma técnica de classificação que utiliza a distribuição normal multivariada (MVN) para modelar as densidades condicionais de classe [^1]. GDA é um classificador *generativo* que assume que os dados de cada classe são gerados a partir de uma distribuição gaussiana [^5]. Compreender o MVN é fundamental para entender GDA [^1]. A análise discriminante gaussiana (GDA), apesar de ser um classificador generativo, não discriminativo, pode ser vista como equivalente a naive Bayes se as matrizes de covariância ∑c forem diagonais [^5].

### Conceitos Fundamentais

**Definição Formal:**
O GDA modela a probabilidade condicional de uma entrada *x* dado a classe *y = c* utilizando uma MVN, expressa como:
$$np(x|y = c, \theta) = N(x|\mu_c, \Sigma_c)$$
onde *c* representa o rótulo da classe e $\theta$ representa os parâmetros do modelo [^5].  Aqui, $\mu_c$ é o vetor de médias da classe *c* e $\Sigma_c$ é a matriz de covariância da classe *c* [^5].

**MVN (Multivariate Normal Distribution):**
Como vimos anteriormente, a função densidade de probabilidade (pdf) para um MVN em *D* dimensões é definida como [^1]:
$$nN(x|\mu, \Sigma) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} \exp \left[ -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right]$$
onde *x* é um vetor de dados, $\mu$ é o vetor de médias e $\Sigma$ é a matriz de covariância [^1].

**Mahalanobis Distance:**
A expressão dentro do expoente na equação acima é a Distância de Mahalanobis entre um vetor de dados *x* e o vetor de médias $\mu$ [^2].  Esta distância pode ser reescrita usando a decomposição de autovalores de $\Sigma$, onde $\Sigma = U \Lambda U^T$, com *U* sendo uma matriz ortonormal de autovetores e $\Lambda$ sendo uma matriz diagonal de autovalores [^2].

**Gaussian Discriminant Analysis (GDA):**
Uma importante aplicação de MVNs é definir as densidades condicionais de classe em um classificador generativo [^5]. Ou seja:
$$np(x|y = c, \theta) = N(x|\mu_c, \Sigma_c)$$
A técnica resultante é chamada de análise discriminante (Gaussiana) ou GDA [^5].

**Quadratic Discriminant Analysis (QDA):**
A probabilidade *a posteriori* sobre os rótulos das classes é dada pela Equação 2.13. Podemos obter mais informações sobre este modelo inserindo a definição da densidade gaussiana, como segue [^6]:
$$np(y = c|x, \theta) = \frac{\pi_c |2\pi\Sigma_c|^{-\frac{1}{2}} \exp \left[ -\frac{1}{2} (x - \mu_c)^T \Sigma_c^{-1} (x - \mu_c) \right]}{\sum_{c'} \pi_{c'} |2\pi\Sigma_{c'}|^{-\frac{1}{2}} \exp \left[ -\frac{1}{2} (x - \mu_{c'})^T \Sigma_{c'}^{-1} (x - \mu_{c'}) \right]}$$
Limiarizar isso resulta em uma função quadrática de *x*. O resultado é conhecido como **Quadratic Discriminant Analysis (QDA)** [^6].

**Linear Discriminant Analysis (LDA):**
Agora consideramos um caso especial em que as matrizes de covariância são *tied* ou *shared* entre as classes, $\Sigma_c = \Sigma$ [^7]. Neste caso, podemos simplificar a Equação 4.33 como segue:
$$np(y = c|x, \theta) \propto \pi_c \exp \left[ -\frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c + x^T \Sigma^{-1} \mu_c - \frac{1}{2} x^T \Sigma^{-1} x \right]$$
$$n= \exp \left[ \mu_c^T \Sigma^{-1} x - \frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c + \log \pi_c \right] \exp \left[ -\frac{1}{2} x^T \Sigma^{-1} x \right]$$
Como o termo quadrático $x^T \Sigma^{-1} x$ é independente de *c*, ele será cancelado no numerador e denominador [^7]. Se definirmos:
$$n\gamma_c = \frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c + \log \pi_c$$
$$n\beta_c = \Sigma^{-1} \mu_c$$
então podemos escrever:
$$np(y = c|x, \theta) = \frac{e^{\beta_c^T x + \gamma_c}}{\sum_{c'} e^{\beta_{c'}^T x + \gamma_{c'}}} = S(\eta)_c$$
onde $\eta = [\beta_1^T x + \gamma_1, ..., \beta_C^T x + \gamma_C]$, e *S* é a função *softmax*, definida como:
$$nS(\eta)_c = \frac{e^{\eta_c}}{\sum_{c'=1}^C e^{\eta_{c'}}}$$
Uma propriedade interessante da Equação acima é que, se tomarmos os logs, terminamos com uma função linear de *x* [^8]. Assim, a fronteira de decisão entre quaisquer duas classes, digamos *c* e *c'*, será uma linha reta [^8]. Portanto, esta técnica é chamada de **Linear Discriminant Analysis ou LDA** [^8].

### Conclusão

O GDA oferece uma abordagem probabilística para classificação, aproveitando a distribuição normal multivariada para modelar a distribuição dos dados dentro de cada classe. Ao entender os fundamentos do MVN, a distância de Mahalanobis e os conceitos de QDA e LDA, pode-se apreciar totalmente as capacidades e limitações do GDA.

### Referências
[^1]: Gaussian models
[^2]: Gaussian models
[^5]: Gaussian discriminant analysis
[^6]: Gaussian discriminant analysis
[^7]: Gaussian discriminant analysis
[^8]: Gaussian discriminant analysis
<!-- END -->