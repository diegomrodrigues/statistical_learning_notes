## Linear Discriminant Analysis (LDA)
### Introdução
Em continuidade ao Gaussian Discriminant Analysis (GDA), este capítulo explora o Linear Discriminant Analysis (LDA) como um caso especial do GDA [^1]. LDA simplifica o GDA ao impor a restrição de que as matrizes de covariância sejam compartilhadas entre as classes, resultando em fronteiras de decisão lineares. Esta simplificação não só reduz a complexidade computacional, mas também pode levar a modelos mais robustos, especialmente em cenários com dados limitados.

### Conceitos Fundamentais
**Gaussian Discriminant Analysis (GDA)** é um modelo generativo que define as densidades condicionais de classe como Gaussianas [^1]. Especificamente,
$$np(x|y = c, \theta) = \mathcal{N}(x|\mu_c, \Sigma_c)$$
onde $\mu_c$ é o vetor de média da classe $c$ e $\Sigma_c$ é a matriz de covariância da classe $c$.

**Linear Discriminant Analysis (LDA)** é uma variante do GDA que assume que as matrizes de covariância são iguais para todas as classes [^1]:
$$Sigma_c = \Sigma, \forall c$$
Esta restrição tem implicações significativas na forma das fronteiras de decisão.

A **fronteira de decisão** entre duas classes $c$ e $c'$ é definida pelos pontos $x$ onde as probabilidades *a posteriori* são iguais:
$$np(y = c|x) = p(y = c'|x)$$
Usando a regra de Bayes, podemos reescrever esta condição em termos das densidades condicionais de classe e das probabilidades *a priori*:
$$n\frac{p(x|y = c)p(y = c)}{p(x)} = \frac{p(x|y = c')p(y = c')}{p(x)}$$
$$np(x|y = c)p(y = c) = p(x|y = c')p(y = c')$$
Tomando o logaritmo de ambos os lados, temos:
$$n\log p(x|y = c) + \log p(y = c) = \log p(x|y = c') + \log p(y = c')$$
Substituindo a densidade Gaussiana, obtemos:
$$n\log \left( \frac{1}{(2\pi)^{D/2}|\Sigma_c|^{1/2}} \exp \left\{ -\frac{1}{2} (x - \mu_c)^T \Sigma_c^{-1} (x - \mu_c) \right\} \right) + \log \pi_c = \log \left( \frac{1}{(2\pi)^{D/2}|\Sigma_{c'}|^{1/2}} \exp \left\{ -\frac{1}{2} (x - \mu_{c'})^T \Sigma_{c'}^{-1} (x - \mu_{c'}) \right\} \right) + \log \pi_{c'}$$
No LDA, como $\Sigma_c = \Sigma$ para todas as classes, o termo $|\Sigma_c|$ é o mesmo para todas as classes e pode ser cancelado. A equação simplifica para:
$$n-\frac{1}{2} (x - \mu_c)^T \Sigma^{-1} (x - \mu_c) + \log \pi_c = -\frac{1}{2} (x - \mu_{c'})^T \Sigma^{-1} (x - \mu_{c'}) + \log \pi_{c'}$$
Expandindo e reorganizando os termos, obtemos:
$$n-\frac{1}{2} x^T \Sigma^{-1} x + x^T \Sigma^{-1} \mu_c -\frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c + \log \pi_c = -\frac{1}{2} x^T \Sigma^{-1} x + x^T \Sigma^{-1} \mu_{c'} -\frac{1}{2} \mu_{c'}^T \Sigma^{-1} \mu_{c'} + \log \pi_{c'}$$
Observe que o termo quadrático $x^T \Sigma^{-1} x$ se cancela, resultando em uma equação linear em $x$:
$$nx^T \Sigma^{-1} (\mu_c - \mu_{c'}) = \frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c - \frac{1}{2} \mu_{c'}^T \Sigma^{-1} \mu_{c'} + \log \pi_{c'} - \log \pi_c$$
Esta equação define uma **fronteira de decisão linear** entre as classes $c$ e $c'$.

Definindo $\gamma_c = -\frac{1}{2}\mu_c^T \Sigma^{-1} \mu_c + \log \pi_c$ e $\beta_c = \Sigma^{-1} \mu_c$ [^3], podemos escrever a fronteira de decisão como:
$$n\beta_c^T x + \gamma_c = \beta_{c'}^T x + \gamma_{c'}$$
$$n(\beta_c - \beta_{c'})^T x = \gamma_{c'} - \gamma_c$$
Esta é a equação de um hiperplano, confirmando que as fronteiras de decisão no LDA são lineares [^3].

**Estimativa dos parâmetros:**
Para estimar os parâmetros do LDA, precisamos estimar as médias de classe $\mu_c$, a matriz de covariância compartilhada $\Sigma$ e as probabilidades *a priori* $\pi_c$. Usando a estimativa de máxima verossimilhança (MLE), temos:
$$n\hat{\mu}_c = \frac{1}{N_c} \sum_{i:y_i = c} x_i$$
$$n\hat{\Sigma} = \frac{1}{N - C} \sum_{c=1}^C \sum_{i:y_i = c} (x_i - \hat{\mu}_c)(x_i - \hat{\mu}_c)^T$$
$$n\hat{\pi}_c = \frac{N_c}{N}$$
onde $N_c$ é o número de amostras na classe $c$, $N$ é o número total de amostras e $C$ é o número de classes. Observe que a estimativa de $\Sigma$ é uma média ponderada das matrizes de covariância de cada classe, ponderada pelo número de amostras em cada classe, e dividida por $N-C$ para obter uma estimativa não viesada.

**Regularização:**
Em cenários com dados limitados, a estimativa de $\Sigma$ pode ser mal condicionada ou singular. Para mitigar este problema, técnicas de regularização podem ser aplicadas. Uma técnica comum é o *shrinkage*, onde a matriz de covariância é combinada com uma matriz diagonal:
$$n\hat{\Sigma}_{regularized} = \lambda \hat{\Sigma} + (1 - \lambda) \frac{1}{D} tr(\hat{\Sigma}) I$$
onde $\lambda \in [0, 1]$ é um parâmetro de regularização e $I$ é a matriz identidade [^3]. Esta técnica encolhe os autovalores de $\hat{\Sigma}$ em direção à média, resultando em uma estimativa mais estável.

### Conclusão
O Linear Discriminant Analysis (LDA) oferece uma alternativa computacionalmente eficiente ao Gaussian Discriminant Analysis (GDA) através da imposição de covariâncias compartilhadas entre as classes. Essa simplificação resulta em fronteiras de decisão lineares, o que pode ser vantajoso em cenários com dados limitados ou quando a linearidade é uma suposição razoável. Embora o LDA possa ser menos flexível que o GDA, sua robustez e interpretabilidade o tornam uma ferramenta valiosa em diversas aplicações.

### Referências
[^1]: Gaussian models.
[^2]: Visualization of a 2 dimensional Gaussian density.
[^3]: Linear discriminant analysis (LDA).
<!-- END -->