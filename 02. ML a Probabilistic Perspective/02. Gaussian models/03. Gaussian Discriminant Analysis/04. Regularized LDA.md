## Regularized Linear Discriminant Analysis (LDA)

### Introdução
Em Gaussian Discriminant Analysis (GDA), uma das principais limitações surge quando as matrizes de covariância são estimadas via Maximum Likelihood Estimation (MLE), especialmente em cenários de alta dimensionalidade onde o número de amostras é pequeno [^105]. Nesses casos, a MLE pode levar a um *overfitting* significativo, resultando em matrizes de covariância singulares ou mal condicionadas, o que prejudica a performance preditiva do modelo [^105]. Para mitigar esses problemas, técnicas de regularização são empregadas. Este capítulo se concentrará na Regularized LDA, que envolve o *tying* das matrizes de covariância e a realização da estimativa MAP (Maximum a Posteriori) de $\Sigma$ usando um *prior* Inverse Wishart [^107].

### Conceitos Fundamentais
Em LDA, assume-se que as matrizes de covariância são iguais para todas as classes, ou seja, $\Sigma_c = \Sigma$ [^107]. A Regularized LDA estende essa abordagem, aplicando a estimativa MAP para $\Sigma$, utilizando um *prior* Inverse Wishart. O objetivo é encontrar uma estimativa de $\Sigma$ que seja mais robusta e generalizável, especialmente quando o número de *features* é comparável ou maior que o número de amostras.

A regularização é controlada por um parâmetro $\lambda$, que ajusta a contribuição relativa entre a estrutura diagonal da matriz de covariância MLE ($\text{diag}(\Sigma_{MLE})$) e a própria matriz de covariância MLE ($\Sigma_{MLE}$). A matriz de covariância regularizada é expressa como:

$$Sigma = \lambda \text{diag}(\Sigma_{MLE}) + (1 - \lambda) \Sigma_{MLE} \quad \text{[107]}$$

Aqui, $\lambda$ é um valor entre 0 e 1. Quando $\lambda = 0$, a estimativa regularizada é idêntica à MLE. À medida que $\lambda$ se aproxima de 1, a matriz de covariância regularizada se torna cada vez mais dominada pela sua diagonal, o que equivale a assumir que as *features* são independentes [^107].

O uso de um *prior* Inverse Wishart tem várias vantagens. Primeiro, é um *prior* conjugado para a matriz de covariância de uma distribuição Gaussiana, o que significa que a distribuição *a posteriori* também é uma Inverse Wishart, simplificando os cálculos [^107]. Segundo, permite incorporar conhecimento prévio sobre a estrutura da matriz de covariância.

#### Estimativa MAP com Prior Inverse Wishart
A distribuição Inverse Wishart é definida como:

$$IW(\Sigma|S, \nu) = \frac{|\Sigma|^{-\frac{\nu + D + 1}{2}} \exp(-\frac{1}{2}\text{tr}(S\Sigma^{-1}))}{Z_{IW}}$$

onde $\nu$ são os graus de liberdade, $S$ é a matriz de escala, e $Z_{IW}$ é a constante de normalização [^125]. A média e o modo da distribuição Inverse Wishart são dados por:

$$text{mean} = \frac{S}{\nu - D - 1}, \quad \text{mode} = \frac{S}{\nu + D + 1}$$

onde $D$ é a dimensionalidade dos dados [^126].

Na Regularized LDA, a estimativa MAP de $\Sigma$ é obtida maximizando a distribuição *a posteriori* $p(\Sigma|D)$, que é proporcional ao produto da *likelihood* $p(D|\Sigma)$ e do *prior* $p(\Sigma)$:

$$p(\Sigma|D) \propto p(D|\Sigma)p(\Sigma)$$

Dado que o *prior* é Inverse Wishart, a distribuição *a posteriori* também é uma Inverse Wishart com parâmetros atualizados:

$$p(\Sigma|D) = IW(\Sigma|S_N, \nu_N)$$

onde $S_N$ e $\nu_N$ são os parâmetros *a posteriori* [^129].

#### Cálculo Eficiente com SVD
Quando a dimensão dos dados $D$ é maior que o número de amostras $N$, a matriz $\Sigma_{MLE}$ se torna singular, impossibilitando o cálculo direto de $\Sigma_{MLE}^{-1}$ [^107]. Para contornar essa limitação, a Singular Value Decomposition (SVD) é utilizada. A SVD da matriz de *design* $X$ é dada por:

$$X = UDV^T$$

onde $U$ é uma matriz ortogonal $N \times N$, $V$ é uma matriz $D \times N$, e $D$ é uma matriz diagonal $N \times N$ [^107]. Utilizando a SVD, podemos reescrever a matriz de covariância MLE em um espaço de dimensão inferior. Definindo $Z = UD$ e $\mu_z = V^T\mu$, a matriz de covariância MLE pode ser expressa como [^107]:

$$Sigma_{MLE} = V \hat{\Sigma}_z V^T$$

onde $\hat{\Sigma}_z$ é a matriz de covariância empírica de $Z$. Essa abordagem permite evitar a inversão direta de uma matriz $D \times D$, substituindo-a pela inversão de uma matriz $N \times N$, o que é computacionalmente mais eficiente quando $N < D$ [^107].

### Conclusão
A Regularized LDA oferece uma abordagem eficaz para lidar com problemas de *overfitting* em cenários de alta dimensionalidade. Ao combinar o *tying* das matrizes de covariância com a estimativa MAP usando um *prior* Inverse Wishart e técnicas de cálculo eficiente baseadas em SVD, é possível obter modelos mais robustos e generalizáveis. O parâmetro de regularização $\lambda$ permite ajustar o compromisso entre a estrutura diagonal e a estrutura completa da matriz de covariância, adaptando o modelo às características específicas dos dados [^107].

### Referências
[^105]: Hastie et al. (2009), p652
[^107]: Seção 4.2.6
[^125]: Seção 4.5.1
[^126]: Seção 4.5.1
[^129]: Seção 4.6.2
<!-- END -->