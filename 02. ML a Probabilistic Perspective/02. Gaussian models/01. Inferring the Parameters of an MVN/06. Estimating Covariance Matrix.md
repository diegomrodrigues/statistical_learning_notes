## Estimativa da Matriz de Covariância via MAP

### Introdução
Este capítulo aborda a estimativa da matriz de covariância em um modelo Gaussiano Multivariado (MVN) utilizando a inferência Bayesiana. Em particular, focaremos na utilização do **posterior mode** (ou mean) para obter uma estimativa Maximum A Posteriori (MAP) [^1]. Essa abordagem se mostra especialmente útil em situações onde a estimativa de Máxima Verossimilhança (MLE) é mal condicionada [^1]. O objetivo é fornecer uma visão detalhada do processo, incluindo as vantagens e as nuances matemáticas envolvidas.

### Conceitos Fundamentais
Conforme mencionado anteriormente [^1], a estimativa da matriz de covariância pode ser realizada utilizando o **posterior mode** ou **mean**, resultando em uma estimativa MAP que é uma combinação convexa do prior mode e da MLE.  Este método é particularmente vantajoso quando a MLE é mal condicionada.

**Estimativa de Máxima Verossimilhança (MLE)**

A MLE para a matriz de covariância, denotada como $\Sigma_{mle}$, é dada por [^3]:

$$\
\Sigma_{mle} = \frac{1}{N}\sum_{i=1}^{N} (x_i - \bar{x})(x_i - \bar{x})^T = \frac{1}{N}\sum_{i=1}^{N} x_i x_i^T - \bar{x}\bar{x}^T
$$

onde $x_i$ são as amostras independentes e identicamente distribuídas (iid) provenientes de uma distribuição $N(\mu, \Sigma)$, e $\bar{x}$ é a média amostral [^3]:

$$\
\bar{x} = \frac{1}{N} \sum_{i=1}^{N} x_i
$$

No entanto, a MLE pode apresentar problemas, especialmente em dimensões elevadas, como mencionado na seção de estratégias para prevenção de overfitting [^10]. Se o número de amostras *N* for menor que a dimensionalidade *D* (N < D), a MLE para a matriz de covariância será singular [^10]. Mesmo quando N > D, a MLE pode ser mal condicionada, ou seja, próxima de ser singular [^10].

**Estimativa Maximum A Posteriori (MAP)**

Para mitigar os problemas da MLE, podemos recorrer à inferência Bayesiana e obter uma estimativa MAP. A estimativa MAP incorpora um conhecimento prévio (prior) sobre a matriz de covariância, regularizando a solução e evitando soluções singulares ou mal condicionadas.

A estimativa MAP é obtida maximizando a distribuição a posteriori:

$$\
\Sigma_{map} = \arg \max_{\Sigma} p(\Sigma | D)
$$

onde *D* representa os dados observados. Assumindo um prior conjugado para a matriz de covariância, como a distribuição Inversa de Wishart [^32], podemos obter uma expressão analítica para a estimativa MAP.

A distribuição Inversa de Wishart é definida como [^30]:

$$\
IW(\Sigma|S, \nu) = \frac{|\Sigma|^{-(\nu+D+1)/2}}{Z_{IW}} exp\left(-\frac{1}{2}tr(S\Sigma^{-1})\right)
$$

onde $\nu$ são os graus de liberdade e $S$ é a matriz de escala.  O mean e o mode da distribuição Inversa de Wishart são [^30]:

$$\
mean = \frac{S}{\nu - D - 1}, \quad mode = \frac{S}{\nu + D + 1}
$$

Combinando a verossimilhança Gaussiana com o prior Inverso de Wishart, a distribuição a posteriori também será uma Inversa de Wishart, com parâmetros atualizados [^33]:

$$\
p(\Sigma | D) = IW(\Sigma | S_N, \nu_N)
$$

onde:
$$\
\nu_N = \nu_0 + N
$$
$$\
S_N = S_0 + S_\mu = S_0 + \sum_{i=1}^{N} (x_i - \bar{x})(x_i - \bar{x})^T
$$

Assim, a estimativa MAP (utilizando o mode da distribuição a posteriori) é dada por [^33]:

$$\
\Sigma_{map} = \frac{S_N}{\nu_N + D + 2} = \frac{S_0 + S_\mu}{\nu_0 + N + D + 2}
$$

Essa estimativa é uma combinação convexa do prior e da MLE. Para ver isso, podemos reescrever a expressão acima como [^34]:

$$\
\Sigma_{map} = \frac{\nu_0}{\nu_0 + N} \frac{S_0}{\nu_0 + D + 2} + \frac{N}{\nu_0 + N} \frac{S_\mu}{N} = \lambda \Sigma_0 + (1 - \lambda) \Sigma_{mle}
$$

onde $\lambda = \frac{\nu_0}{\nu_0 + N}$ controla a quantidade de *shrinkage* em direção ao prior, e $\Sigma_0$ é o mode do prior [^34]. O termo *shrinkage* se refere ao fato de que a estimativa MAP "encolhe" a estimativa MLE em direção a um valor mais regularizado, definido pelo prior.

**Estimativa com Prior Data-Dependente**

Uma abordagem comum para definir o prior é usar um prior data-dependente [^34], como $S_0 = diag(\Sigma_{mle})$. Nesse caso, a estimativa MAP é dada por [^34]:

$$\
\Sigma_{map}(i, j) =
\begin{cases}
\Sigma_{mle}(i, j) & \text{se } i = j \\\\
(1 - \lambda) \Sigma_{mle}(i, j) & \text{se } i \neq j
\end{cases}
$$

Ou seja, os elementos diagonais são iguais às estimativas de máxima verossimilhança, enquanto os elementos fora da diagonal são "encolhidos" em direção a zero [^34]. Essa técnica é conhecida como *shrinkage estimation* ou *regularized estimation* [^34].

### Conclusão
A estimativa da matriz de covariância em um MVN é um problema fundamental em muitas áreas da estatística e aprendizado de máquina. A estimativa MLE, embora simples, pode ser problemática em dimensões elevadas. A estimativa MAP, utilizando um prior adequado, oferece uma alternativa robusta que regulariza a solução e evita problemas de singularidade e mau condicionamento. A escolha do prior e a determinação do parâmetro de *shrinkage* são aspectos importantes a serem considerados na prática.

### Referências
[^1]: Seção "To estimate the covariance matrix..." do contexto fornecido.
[^3]: Seção 4.1.3 "MLE for an MVN" do contexto fornecido.
[^10]: Seção 4.2.5 "Strategies for preventing overfitting" do contexto fornecido.
[^30]: Seção 4.5 "Digression: The Wishart distribution *" do contexto fornecido.
[^32]: Seção 4.6.2 "Posterior distribution of Σ *" do contexto fornecido.
[^33]: Seção 4.6.3 "Posterior distribution of µ and Σ *" do contexto fornecido.
[^34]: Seção 4.6.2.1 "MAP estimation" do contexto fornecido.

<!-- END -->