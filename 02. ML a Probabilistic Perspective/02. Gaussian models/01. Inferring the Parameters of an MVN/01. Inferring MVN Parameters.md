## Inferência Bayesiana dos Parâmetros de uma MVN

### Introdução
Este capítulo explora a inferência dos parâmetros de uma **Multivariate Normal (MVN)**, um tópico fundamental em modelagem estatística e análise de dados [^4]. O conhecimento prévio de conceitos de álgebra linear e cálculo matricial é essencial para a compreensão deste capítulo [^1]. Abordaremos a inferência Bayesiana para estimar os parâmetros de uma MVN, que mitiga o overfitting e fornece uma medida de confiança nas estimativas [^3].

### Conceitos Fundamentais

Para inferir os parâmetros de uma MVN, seguimos uma abordagem Bayesiana, que envolve o cálculo das distribuições posteriores para a média ($\mu$) e a matriz de covariância ($\Sigma$) dado o conjunto de dados $D$. O objetivo é calcular $p(\mu|D, \Sigma)$, $p(\Sigma|D, \mu)$, e finalmente a distribuição conjunta $p(\mu, \Sigma|D)$ [^0].

**Distribuição a priori e posterior:**
Na inferência Bayesiana, começamos com uma distribuição a priori sobre os parâmetros, $p(\mu)$ e $p(\Sigma)$. Em seguida, atualizamos essas distribuições com base nos dados observados $D$, usando a função de verossimilhança $p(D|\mu, \Sigma)$. A distribuição posterior é então proporcional ao produto da distribuição a priori e da verossimilhança:
$$
p(\mu, \Sigma|D) \propto p(D|\mu, \Sigma) p(\mu, \Sigma)
$$

**Função de Verossimilhança:**
A função de verossimilhança para $N$ amostras i.i.d. $x_i \sim N(\mu, \Sigma)$ é dada por:
$$
p(D|\mu, \Sigma) = \prod_{i=1}^{N} N(x_i|\mu, \Sigma)
$$
O logaritmo da verossimilhança (log-likelihood) é mais conveniente para cálculos:
$$
l(\mu, \Sigma) = \log p(D|\mu, \Sigma) = \sum_{i=1}^{N} \log N(x_i|\mu, \Sigma)
$$
Expandindo a expressão da MVN [^1]:
$$
N(x|\mu, \Sigma) = \frac{1}{(2\pi)^{D/2} |\Sigma|^{1/2}} \exp\left[-\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu)\right]
$$
Temos:
$$
l(\mu, \Sigma) = -\frac{N}{2} \log((2\pi)^D |\Sigma|) - \frac{1}{2} \sum_{i=1}^{N} (x_i - \mu)^T \Sigma^{-1} (x_i - \mu)
$$
A verossimilhança tem uma forma relacionada ao exponencial de um traço envolvendo $\Sigma^{-1}$ e os dados, que pode ser simplificada usando propriedades de matrizes [^0].
A verossimilhança para $N$ amostras i.i.d. $x_i \sim N(\mu, \Sigma)$ é dada por $p(D|\mu) = N(\bar{x}|\mu, \frac{1}{N} \Sigma)$, onde $\bar{x}$ é a média amostral [^0].  Matematicamente, isso significa:
$$
p(D|\mu) = \frac{1}{(2\pi)^{D/2} |\frac{1}{N}\Sigma|^{1/2}} \exp\left[-\frac{1}{2} (\bar{x} - \mu)^T (\frac{1}{N}\Sigma)^{-1} (\bar{x} - \mu)\right]
$$

**Inferência de $\mu$ com $\Sigma$ conhecido:**
Assumindo $\Sigma$ conhecido, podemos inferir $\mu$ usando a distribuição posterior $p(\mu|D, \Sigma)$. Usando uma distribuição a priori conjugada Gaussiana, $p(\mu) = N(\mu|m_0, V_0)$, a distribuição posterior para $\mu$ também será Gaussiana [^32].
$$
p(\mu|D, \Sigma) = N(\mu|m_N, V_N)
$$
Onde:
$$
V_N^{-1} = V_0^{-1} + N\Sigma^{-1}
$$
$$
m_N = V_N (\Sigma^{-1} (N\bar{x}) + V_0^{-1} m_0)
$$
A distribuição posterior para $\mu$ é uma Gaussiana com média $m_N$ e covariância $V_N$.

**Inferência de $\Sigma$ com $\mu$ conhecido:**
Assumindo $\mu$ conhecido, podemos inferir $\Sigma$ usando a distribuição posterior $p(\Sigma|D, \mu)$. Usando uma distribuição a priori conjugada Inverse Wishart [^32], $p(\Sigma) = IW(\Sigma|S_0, \nu_0)$, a distribuição posterior para $\Sigma$ também será Inverse Wishart.
$$
p(\Sigma|D, \mu) = IW(\Sigma|S_N, \nu_N)
$$
Onde:
$$
\nu_N = \nu_0 + N
$$
$$
S_N = S_0 + \sum_{i=1}^{N} (x_i - \mu)(x_i - \mu)^T
$$
A distribuição posterior para $\Sigma$ é uma Inverse Wishart com parâmetros $S_N$ e $\nu_N$.

**Inferência conjunta de $\mu$ e $\Sigma$:**
Para inferir conjuntamente $\mu$ e $\Sigma$, usamos uma distribuição a priori Normal-Inverse-Wishart (NIW) [^33]:
$$
p(\mu, \Sigma) = NIW(\mu, \Sigma|m_0, \kappa_0, \nu_0, S_0)
$$
A distribuição posterior também será NIW:
$$
p(\mu, \Sigma|D) = NIW(\mu, \Sigma|m_N, \kappa_N, \nu_N, S_N)
$$
Com os parâmetros atualizados:
$$
m_N = \frac{\kappa_0 m_0 + N \bar{x}}{\kappa_0 + N}
$$
$$
\kappa_N = \kappa_0 + N
$$
$$
\nu_N = \nu_0 + N
$$
$$
S_N = S_0 + \sum_{i=1}^{N} (x_i - \bar{x})(x_i - \bar{x})^T + \frac{\kappa_0 N}{\kappa_0 + N} (\bar{x} - m_0)(\bar{x} - m_0)^T
$$

### Conclusão
Este capítulo apresentou a inferência Bayesiana dos parâmetros de uma MVN. A escolha de distribuições a priori conjugadas simplifica significativamente os cálculos, resultando em distribuições posteriores que pertencem à mesma família. A inferência Bayesiana fornece uma maneira natural de incorporar conhecimento prévio e quantificar a incerteza nos parâmetros estimados.

### Referências
[^0]:  Trecho inicial do contexto fornecido.
[^1]:  Seção 4.1.2 "Basics" do contexto fornecido.
[^3]:  Seção 4.1.3 "MLE for an MVN" do contexto fornecido.
[^4]:  Seção 4 "Gaussian models" do contexto fornecido.
[^32]: Seção 4.6 "Inferring the parameters of an MVN" do contexto fornecido.
[^33]: Seção 4.6.3 "Posterior distribution of $\mu$ and $\Sigma$ *" do contexto fornecido.
<!-- END -->