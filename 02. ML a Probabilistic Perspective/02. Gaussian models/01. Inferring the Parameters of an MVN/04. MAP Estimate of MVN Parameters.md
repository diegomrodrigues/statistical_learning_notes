## Estimativa MAP para a Matriz de Covariância em MVNs

### Introdução
Este capítulo explora a estimativa **Maximum a Posteriori (MAP)** para a matriz de covariância em um Modelo Normal Multivariado (MVN), com o objetivo de mitigar o *overfitting* [^3]. A estimativa MAP combina informações da amostra de dados com um conhecimento prévio, representado por uma distribuição *prior*. Discutiremos como a escolha da *prior* influencia a estimativa final, e como podemos expressar a estimativa MAP como uma combinação convexa entre a *prior* e a estimativa de **Máxima Verossimilhança (MLE)**.

### Conceitos Fundamentais

#### Estimativa MAP e Overfitting

Como mencionado anteriormente [^3], a estimativa MAP visa reduzir o *overfitting*, um problema comum em modelos estatísticos, particularmente quando o número de parâmetros a serem estimados é grande em relação ao tamanho da amostra. No contexto da matriz de covariância, o *overfitting* pode levar a estimativas singulares ou mal condicionadas, especialmente quando o número de amostras *N* é menor que a dimensão *D* do MVN.

#### Estimativa MAP da Matriz de Covariância

A estimativa MAP da matriz de covariância é dada por [^texto_original]:
$$ \hat{\Sigma}_{map} = \frac{S_N}{\nu_N + D + 1} = \frac{S_0 + S_{\mu}}{N_0 + N} $$
onde:
*   $S_N$ é a matriz de dispersão *a posteriori*
*   $\nu_N$ são os graus de liberdade *a posteriori*
*   $S_0$ é a matriz de dispersão *a priori*
*   $S_{\mu}$ é a matriz de dispersão centrada na média amostral
*   $N_0$ é o tamanho da amostra *a priori*
*   $N$ é o tamanho da amostra real
*   $D$ é a dimensão do MVN

#### *Prior* Não Informativa e MLE

Se utilizarmos uma *prior* uniforme imprópria, correspondente a $N_0 = 0$ e $S_0 = 0$, recuperamos a estimativa MLE [^texto_original]. Isso ocorre porque, nesse caso, a *prior* não exerce influência sobre a estimativa *a posteriori*, que é inteiramente determinada pelos dados.

#### *Prior* Informativa e Combinação Convexa

Para utilizar uma *prior* informativa adequada, podemos definir $\mu = \bar{x}$, de forma que $S_{\mu} = S$, onde $\bar{x}$ é a média amostral e *S* é a matriz de dispersão amostral [^texto_original]. Nesse caso, a estimativa MAP pode ser reescrita como uma combinação convexa da moda *a priori* e da estimativa MLE [^texto_original].

Seja $\Sigma_0$ a moda *a priori*. Então, a moda *a posteriori* pode ser reescrita como [^texto_original]:

$$ \hat{\Sigma}_{map} = \frac{N_0}{N_0 + N} \frac{S_0}{N_0} + \frac{N}{N_0 + N} \frac{S}{N} = \lambda \Sigma_0 + (1 - \lambda) \hat{\Sigma}_{mle} $$
onde:

*   $\lambda = \frac{N_0}{N_0 + N}$  é o peso atribuído à *prior*
*   $(1 - \lambda) = \frac{N}{N_0 + N}$  é o peso atribuído à MLE
*   $\hat{\Sigma}_{mle} = \frac{S}{N}$ é a estimativa MLE da matriz de covariância

Essa formulação explicita como a estimativa MAP equilibra o conhecimento *a priori* (representado por $\Sigma_0$) com as informações fornecidas pelos dados (representadas por $\hat{\Sigma}_{mle}$). O parâmetro $\lambda$ controla a força dessa combinação.

**Exemplo:**

Considere o caso em que $N_0 = 10$, $N = 90$, e $\Sigma_0$ é uma matriz diagonal com valores unitários. Se a estimativa MLE $\hat{\Sigma}_{mle}$ for uma matriz com elementos fora da diagonal significativos, a estimativa MAP irá "encolher" esses elementos em direção a zero, devido à influência da *prior* diagonal.

**Observação:**
A escolha da *prior* e dos seus parâmetros ($N_0$, $S_0$) é crucial para o desempenho da estimativa MAP. Uma *prior* mal escolhida pode levar a resultados piores do que a MLE. Técnicas de validação cruzada podem ser usadas para otimizar os parâmetros da *prior*.

### Conclusão

A estimativa MAP oferece uma abordagem para estimar a matriz de covariância em MVNs, que permite incorporar conhecimento *a priori* e mitigar o *overfitting*. Ao expressar a estimativa MAP como uma combinação convexa da *prior* e da MLE, podemos entender melhor como a *prior* influencia a estimativa final. A escolha da *prior* e seus parâmetros é fundamental e deve ser feita com cuidado, possivelmente utilizando técnicas de validação cruzada.

### Referências
[^3]: Gaussian models, MLE for an MVN.
[^texto_original]: "To address the issue of overfitting, the MAP estimate is given by \\(\hat{\Sigma}_{map} = \frac{S_N}{\nu_N + D + 1} = \frac{S_0 + S_{\mu}}{N_0 + N}\). If we use an improper uniform prior, corresponding to \\(N_0 = 0\) and \\(S_0 = 0\), we recover the MLE. To use a proper informative prior, let \\(\mu = \bar{x}\), so \\(S_{\mu} = S\). Then we can rewrite the MAP estimate as a convex combination of the prior mode and the MLE. To see this, let \\(\Sigma_0\) be the prior mode. Then the posterior mode can be rewritten as \\(\hat{\Sigma}_{map} = \frac{N_0}{N_0 + N} \frac{S_0}{N_0} + \frac{N}{N_0 + N} \frac{S}{N} = \lambda \Sigma_0 + (1 - \lambda) \hat{\Sigma}_{mle}\). MAP estimation involves using the posterior mode for the mean and covariance matrix. One can show that the MAP estimate is given by SN/VN+D+1"
<!-- END -->