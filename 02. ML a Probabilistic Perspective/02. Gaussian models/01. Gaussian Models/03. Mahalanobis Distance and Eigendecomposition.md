## Mahalanobis Distance in Gaussian Models

### Introdução

Este capítulo explora a **distância de Mahalanobis** no contexto de modelos Gaussianos, uma métrica essencial para analisar a dispersão de dados multivariados [^1]. Ao contrário da distância Euclidiana, a distância de Mahalanobis leva em consideração a **estrutura de covariância** dos dados, tornando-a particularmente útil quando as variáveis são correlacionadas [^2]. A **decomposição em autovalores** da matriz de covariância permite uma interpretação geométrica da distância, relacionando-a com a forma elíptica das curvas de nível de distribuições Gaussianas [^2].

### Conceitos Fundamentais

A **distância de Mahalanobis** entre um vetor de dados $x$ e o vetor de média $\mu$ é definida como [^2]:

$$(x - \mu)^T \Sigma^{-1}(x - \mu)$$

onde $\Sigma$ é a matriz de covariância dos dados. Esta distância ajusta a distância Euclidiana pela estrutura de covariância, efetivamente "descorrelacionando" e "normalizando" os dados [^2].

Para entender melhor esta quantidade, realizamos a **decomposição em autovalores** da matriz de covariância $\Sigma$ [^2]:

$$Sigma = U \Lambda U^T$$

onde $U$ é uma matriz ortonormal de autovetores e $\Lambda$ é uma matriz diagonal de autovalores. Substituindo esta decomposição na fórmula da distância de Mahalanobis, obtemos [^2]:

$$(x - \mu)^T \Sigma^{-1}(x - \mu) = (x - \mu)^T (U \Lambda U^T)^{-1} (x - \mu) = (x - \mu)^T U \Lambda^{-1} U^T (x - \mu)$$

Definindo $y = U^T(x - \mu)$, podemos reescrever a distância como [^2]:

$$y^T \Lambda^{-1} y = \sum_{i=1}^D \frac{y_i^2}{\lambda_i}$$

onde $y_i = u_i^T (x - \mu)$ e $u_i$ é a *i*-ésima coluna de $U$, contendo o *i*-ésimo autovetor, e $\lambda_i$ é o *i*-ésimo autovalor [^2].

Esta formulação revela que a distância de Mahalanobis pode ser interpretada como uma **distância Euclidiana** em um sistema de coordenadas transformado, onde os dados são [^2]:
1.  Deslocados por $\mu$
2.  Rotacionados por $U$
3.  Escalonados pelos autovalores $\Lambda$

As **curvas de nível de igual densidade de probabilidade** para uma distribuição Gaussiana formam elipses [^2]. Os **autovetores** determinam a **orientação** da elipse, enquanto os **autovalores** determinam o seu **alongamento** [^2]. A Figura 4.1 [^2] ilustra esta visualização para uma densidade Gaussiana bidimensional. A equação para uma elipse em 2D é dada por [^2]:

$$frac{y_1^2}{\lambda_1} + \frac{y_2^2}{\lambda_2} = 1$$

### Conclusão

A distância de Mahalanobis é uma ferramenta poderosa para analisar dados Gaussianos multivariados, pois leva em consideração a estrutura de covariância dos dados [^2]. A decomposição em autovalores da matriz de covariância fornece uma interpretação geométrica clara da distância, relacionando-a com a forma elíptica das curvas de nível de distribuições Gaussianas [^2]. Esta métrica é amplamente utilizada em diversas aplicações, incluindo classificação, detecção de outliers e análise de agrupamentos [^6].

### Referências
[^1]: Page 1, "In this chapter, we discuss the multivariate Gaussian or multivariate normal (MVN), which is the most widely used joint probability density function for continuous variables."
[^2]: Page 2, "The expression inside the exponent is the Mahalanobis distance between a data vector x and the mean vector µ... In general, we see that the Mahalanobis distance corresponds to Euclidean distance in a transformed coordinate system, where we shift by µ and rotate by U."

<!-- END -->