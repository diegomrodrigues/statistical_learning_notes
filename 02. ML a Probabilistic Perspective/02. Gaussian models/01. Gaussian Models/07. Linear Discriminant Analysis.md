## Linear Discriminant Analysis (LDA) em Modelos Gaussianos

### Introdução
Este capítulo aprofunda a análise do **Linear Discriminant Analysis (LDA)** no contexto dos **Modelos Gaussianos**, expandindo o conceito de **Gaussian Discriminant Analysis (GDA)** [^121]. LDA surge como um caso especial onde as matrizes de covariância são *tied* ou *shared* entre as classes, simplificando a regra de decisão e resultando em fronteiras de decisão lineares [^7]. A discriminabilidade de um sinal em relação ao ruído de fundo é quantificada usando o *d-prime*, que relaciona a diferença nas médias ao desvio padrão do ruído [^106]. Além disso, a posterior pode ser expressa usando a função *softmax* [^7].

### Conceitos Fundamentais

Em **Gaussian Discriminant Analysis (GDA)**, as densidades condicionais de classe são modeladas como Gaussianas multivariadas [^5]:

$$ p(x|y = c, \theta) = N(x|\mu_c, \Sigma_c) $$

Onde $x$ é o vetor de características, $y$ é a classe, $\mu_c$ é o vetor de média da classe $c$, e $\Sigma_c$ é a matriz de covariância da classe $c$ [^5].

**Linear Discriminant Analysis (LDA)** simplifica o GDA ao assumir que todas as classes compartilham a mesma matriz de covariância [^7]:

$$ \Sigma_c = \Sigma, \forall c $$

Sob essa suposição, a Equação 4.33 [^6] pode ser simplificada [^7]:

$$ p(y = c|x, \theta) \propto \pi_c \exp \left\{ -\frac{1}{2} (x - \mu_c)^T \Sigma^{-1} (x - \mu_c) \right\} $$

$$ = \exp \left\{ \mu_c^T \Sigma^{-1} x - \frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c + \log \pi_c \right\} \exp \left\{ -\frac{1}{2} x^T \Sigma^{-1} x \right\} $$

Como o termo quadrático $x^T \Sigma^{-1} x$ é independente de $c$, ele se cancela no numerador e denominador [^7]. Definindo [^7]:

$$ \gamma_c = -\frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c + \log \pi_c $$

$$ \beta_c = \Sigma^{-1} \mu_c $$

Podemos escrever a posterior como [^8]:

$$ p(y = c|x, \theta) = \frac{e^{\beta_c^T x + \gamma_c}}{\sum_{c'} e^{\beta_{c'}^T x + \gamma_{c'}}} = S(\eta)_c $$

Onde $S$ é a função *softmax* e $\eta$ é um vetor cujos elementos são $\beta_c^T x + \gamma_c$ [^8].

Uma propriedade importante da Equação 4.38 [^8] é que, ao tomar os logs, obtemos uma função linear de $x$. Isso ocorre porque o termo $x^T \Sigma^{-1} x$ se cancela no numerador e denominador [^8]. Portanto, a fronteira de decisão entre duas classes, digamos $c$ e $c'$, será uma linha reta [^8]. Daí o nome *Linear Discriminant Analysis* [^8].

Para o caso binário, a posterior se torna [^8]:

$$ p(y = 1|x, \theta) = \frac{e^{\beta_1^T x + \gamma_1}}{e^{\beta_1^T x + \gamma_1} + e^{\beta_0^T x + \gamma_0}} $$

$$ = \text{sigm} \left( (\beta_1 - \beta_0)^T x + (\gamma_1 - \gamma_0) \right) $$

Onde $\text{sigm}(a) = \frac{1}{1 + e^{-a}}$ é a função sigmóide [^8].

Em psicologia e teoria de detecção de sinais, a discriminabilidade de um sinal do ruído de fundo é frequentemente medida usando o *d-prime* [^106]:

$$ d' = \frac{|\mu_1 - \mu_0|}{\sigma} $$

Onde $\mu_1$ é a média do sinal, $\mu_0$ é a média do ruído, e $\sigma$ é o desvio padrão do ruído [^106]. Um *d-prime* maior indica que o sinal é mais fácil de discriminar do ruído [^106].

### Conclusão
LDA oferece uma abordagem eficiente e linear para classificação em cenários onde a suposição de covariâncias compartilhadas é razoável [^7]. A simplicidade de LDA, juntamente com sua interpretabilidade, torna-o uma ferramenta valiosa em diversas aplicações [^10]. No entanto, é crucial reconhecer as limitações dessa suposição e considerar alternativas como QDA quando as covariâncias da classe diferem significativamente [^6].

### Referências
[^7]: Seção 4.2.2
[^5]: Seção 4.2
[^6]: Seção 4.2.1
[^8]: Seção 4.2.3
[^10]: Seção 4.2.5
[^106]: Seção 4.2.4
[^121]: Seção 4.2

<!-- END -->