## Gaussian Discriminant Analysis

### Introdução
Este capítulo explora o **Gaussian Discriminant Analysis (GDA)**, uma técnica de classificação generativa que se baseia em modelos Gaussianos. O GDA utiliza a **multivariate normal (MVN)** para definir as densidades condicionais de classe [^5]. Exploraremos a equivalência entre GDA e naive Bayes sob certas condições e discutiremos a regra de decisão baseada na distância de Mahalanobis [^5].

### Conceitos Fundamentais

O GDA é um classificador generativo que assume que as densidades condicionais de classe seguem uma distribuição normal multivariada [^5]. Formalmente, temos:

$$ p(x|y = c, \theta) = N(x|\mu_c, \Sigma_c) $$

onde:
- $x$ é o vetor de características.
- $y$ é a variável de classe, com $c$ representando uma classe específica.
- $\theta$ representa os parâmetros do modelo.
- $\mu_c$ é o vetor de médias para a classe $c$.
- $\Sigma_c$ é a matriz de covariância para a classe $c$.

A **função de densidade de probabilidade (pdf)** para uma MVN em $D$ dimensões é dada por [^1]:

$$ N(x|\mu, \Sigma) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) \right] $$

A expressão dentro do exponencial é a **distância de Mahalanobis** entre um vetor de dados $x$ e o vetor de média $\mu$ [^2].

**Equivalência com Naive Bayes:**

Uma propriedade importante do GDA é que, se a matriz de covariância $\Sigma_c$ for diagonal, o GDA se torna equivalente ao naive Bayes [^5]. Isso ocorre porque uma matriz de covariância diagonal implica que as características são condicionalmente independentes dada a classe.

**Regra de Decisão:**

A regra de decisão no GDA envolve calcular a probabilidade de $x$ sob cada densidade condicional de classe e escolher a classe que maximiza essa probabilidade [^5]:

$$ \hat{y}(x) = \underset{c}{\operatorname{argmax}} \  p(y = c|x, \theta) $$

Usando o teorema de Bayes, podemos expressar a probabilidade posterior como:

$$ p(y = c|x, \theta) = \frac{p(x|y = c, \theta) p(y = c)}{\sum_{c'} p(x|y = c', \theta) p(y = c')} $$

Onde $p(y=c)$ é a probabilidade *a priori* da classe $c$.

**Quadratic Discriminant Analysis (QDA)**

Substituindo a definição da densidade Gaussiana, obtemos [^6]:

$$ p(y = c|x, \theta) = \frac{\pi_c |2\pi\Sigma_c|^{-\frac{1}{2}} \exp [-\frac{1}{2} (x - \mu_c)^T \Sigma_c^{-1} (x - \mu_c)]}{\sum_{c'} \pi_{c'} |2\pi\Sigma_{c'}|^{-\frac{1}{2}} \exp [-\frac{1}{2} (x - \mu_{c'})^T \Sigma_{c'}^{-1} (x - \mu_{c'})]} $$

Onde $\pi_c$ é a probabilidade *a priori* da classe *c*. Limiarizar isso resulta em uma função quadrática de *x*. O resultado é conhecido como **Quadratic Discriminant Analysis (QDA)** [^6].

**Linear Discriminant Analysis (LDA)**

Um caso especial do QDA ocorre quando as matrizes de covariância são amarradas ou compartilhadas entre as classes ($\Sigma_c = \Sigma$) [^7]. Nesse caso, podemos simplificar a Equação 4.33 da seguinte forma [^7]:

$$ p(y = c|x, \theta) \propto \pi_c \exp[-\frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c + \mu_c^T \Sigma^{-1} x - \frac{1}{2} x^T \Sigma^{-1} x] $$
$$ = \exp[\mu_c^T \Sigma^{-1} x - \frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c + log \pi_c] \exp[-\frac{1}{2} x^T \Sigma^{-1} x] $$

Como o termo quadrático $x^T \Sigma^{-1} x$ é independente de *c*, ele será cancelado no numerador e no denominador [^7]. Se definirmos [^7]:

$$ \gamma_c = -\frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c + log \pi_c $$
$$ \beta_c = \Sigma^{-1} \mu_c $$

Então podemos escrever [^8]:

$$ p(y = c|x, \theta) = \frac{e^{\beta_c^T x + \gamma_c}}{\sum_{c'} e^{\beta_{c'}^T x + \gamma_{c'}}} = S(\eta)_c $$

Onde $S$ é a função *softmax*, definida como [^8]:

$$ S(\eta)_c = \frac{e^{\eta_c}}{\sum_{c'=1}^{C} e^{\eta_{c'}}} $$

A função *softmax* é assim chamada porque atua um pouco como a função *max* [^8].

Uma propriedade interessante da Equação 4.38 é que, se tomarmos os logs, acabamos com uma função linear de *x* [^8]. Portanto, o limite de decisão entre quaisquer duas classes, digamos *c* e *c'*, será uma linha reta [^8]. Portanto, essa técnica é chamada de **análise discriminante linear ou LDA** [^8].

**MLE para análise discriminante**
A maneira mais simples de ajustar um modelo de análise discriminante é usar a máxima verossimilhança. A função de log-verossimilhança é a seguinte [^10]:
$$ \log p(\mathcal{D}|\theta) = \sum_{i=1}^{N} \sum_{c=1}^{C} \mathbb{I}(y_i = c) \log \pi_c + \sum_{c=1}^{C} \sum_{i:y_i=c} \log \mathcal{N}(x_i|\mu_c, \Sigma_c) $$
Vemos que isso se decompõe em um termo para $\pi$ e $C$ termos para cada $\mu_c$ e $\Sigma_c$ [^10]. Portanto, podemos estimar esses parâmetros separadamente [^10]. Para o *a priori* de classe, temos $\hat{\pi}_c = \frac{N_c}{N}$, como com o Naive Bayes [^10]. Para as densidades condicionais de classe, apenas particionamos os dados com base no rótulo de classe e calculamos a MLE para cada Gaussiana [^10]:
$$ \mu_c = \frac{1}{N_c} \sum_{i:y_i=c} x_i $$
$$ \Sigma_c = \frac{1}{N_c} \sum_{i:y_i=c} (x_i - \mu_c) (x_i - \mu_c)^T $$

### Conclusão

O GDA é uma ferramenta poderosa para classificação, especialmente quando as suposições Gaussianas são válidas. A relação com o naive Bayes oferece flexibilidade na modelagem, e a distância de Mahalanobis fornece uma métrica útil para classificação baseada em centroides. As diferentes variações de matrizes de covariância compartilhadas e não compartilhadas levam a diferentes limites de decisão e complexidade do modelo, como QDA e LDA. A escolha entre esses modelos depende das características dos dados e dos objetivos da tarefa de classificação.

### Referências
[^1]: Seção 4.1.2
[^2]: Seção 4.1.3
[^5]: Seção 4.2
[^6]: Seção 4.2.1
[^7]: Seção 4.2.2
[^8]: Seção 4.2.2
[^10]: Seção 4.2.4
<!-- END -->