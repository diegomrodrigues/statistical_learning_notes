## A Função Densidade de Probabilidade da Gaussiana Multivariada

### Introdução
Este capítulo explora em profundidade a **função densidade de probabilidade (pdf)** da **distribuição Gaussiana multivariada (MVN)**, um conceito fundamental em modelos Gaussianos [^1]. A MVN é amplamente utilizada para modelar a distribuição conjunta de variáveis contínuas e desempenha um papel crucial em diversas áreas, incluindo estimação de parâmetros e inferência Bayesiana [^1]. Este capítulo se baseia em conceitos de álgebra linear e cálculo matricial [^1], e visa fornecer uma compreensão abrangente da formulação matemática e das propriedades da pdf da MVN.

### Conceitos Fundamentais
A pdf para uma MVN em $D$ dimensões é definida matematicamente como [^1]:

$$
N(x|\mu, \Sigma) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} exp[-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)]
$$

onde $\mu$ é o vetor de médias e $\Sigma$ é a matriz de covariância [^1].

**Componentes da Fórmula:**

*   **x**: Um vetor $D$-dimensional de variáveis aleatórias [^1].
*   **μ**: O vetor de médias $D$-dimensional, representando o centro da distribuição [^1].
*   **Σ**: A matriz de covariância $D \times D$, que descreve a variância e a covariância entre as diferentes dimensões [^1].
*   **|Σ|**: O determinante da matriz de covariância Σ [^1].
*   **Σ⁻¹**: A inversa da matriz de covariância Σ [^1].
*   **(x - μ)ᵀ**: O transposto do vetor diferença entre x e μ [^1].

**Interpretação da Fórmula:**

A fórmula da pdf da MVN pode ser interpretada como um produto de duas partes principais: um fator de normalização e uma função exponencial.

*   **Fator de Normalização**: $\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}$ garante que a integral da pdf sobre todo o espaço seja igual a 1, satisfazendo assim a propriedade fundamental de uma função densidade de probabilidade [^1]. Este fator depende da dimensão $D$ e do determinante da matriz de covariância Σ [^1].
*   **Função Exponencial**: $exp[-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)]$ determina a forma da distribuição. O termo $(x-\mu)^T \Sigma^{-1}(x-\mu)$ é conhecido como a **distância de Mahalanobis** [^2] entre o ponto x e a média μ. Esta distância é uma medida da distância ponderada pela matriz de covariância, levando em conta a correlação entre as variáveis [^2].

**Eigendecomposição da Matriz de Covariância:**

Para entender melhor a influência da matriz de covariância, podemos realizar uma **eigendecomposição** de Σ [^2], expressando-a como:

$$
\Sigma = U \Lambda U^T
$$

onde U é uma matriz ortonormal cujas colunas são os autovetores de Σ, e Λ é uma matriz diagonal contendo os autovalores correspondentes [^2]. Os autovetores definem a orientação dos eixos principais da elipse (ou hiperelipsoide em dimensões superiores) que representa a distribuição Gaussiana, enquanto os autovalores determinam o comprimento desses eixos [^2].

Utilizando a eigendecomposição, podemos reescrever a distância de Mahalanobis como [^2]:

$$
(x - \mu)^T \Sigma^{-1} (x - \mu) = \sum_{i=1}^{D} \frac{y_i^2}{\lambda_i}
$$

onde $y_i = u_i^T (x - \mu)$ e $\lambda_i$ são os autovalores de Σ [^2]. Isso mostra que a distância de Mahalanobis corresponde à distância euclidiana em um sistema de coordenadas transformado, onde deslocamos por μ e rotacionamos por U [^2].

**Contornos de Probabilidade Constante:**

Os contornos de probabilidade constante de uma Gaussiana multivariada formam elipses [^2]. Os autovetores da matriz de covariância determinam a orientação da elipse, enquanto os autovalores determinam o quão alongada ela é [^2]. A equação de uma elipse em 2D é dada por [^2]:

$$
\frac{y_1^2}{\lambda_1} + \frac{y_2^2}{\lambda_2} = 1
$$

**Exemplo:**

Em duas dimensões, a MVN pode ser visualizada como uma elipse centrada na média μ [^2]. Os eixos principais da elipse são alinhados com os autovetores da matriz de covariância Σ, e o comprimento desses eixos é determinado pelos autovalores correspondentes [^2].

### Conclusão
A função densidade de probabilidade da Gaussiana multivariada é uma ferramenta essencial para modelagem estatística e inferência. Sua forma matemática, expressa em termos do vetor de médias e da matriz de covariância, permite uma descrição concisa e poderosa da distribuição conjunta de múltiplas variáveis contínuas. Através da eigendecomposição da matriz de covariância, podemos obter uma compreensão geométrica da forma da distribuição, identificando seus eixos principais e seus respectivos comprimentos. Essa compreensão é crucial para a aplicação eficaz de modelos Gaussianos em uma variedade de problemas práticos [^1].

### Referências
[^1]: Gaussian Models OCR.
[^2]: Gaussian Models OCR.
<!-- END -->