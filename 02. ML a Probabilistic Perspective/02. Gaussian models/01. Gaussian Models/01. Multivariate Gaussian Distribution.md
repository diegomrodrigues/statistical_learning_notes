## The Multivariate Gaussian Distribution

### Introdução
Este capítulo explora a **distribuição Gaussiana multivariada (MVN)**, também conhecida como distribuição normal multivariada, que é uma função de densidade de probabilidade conjunta para variáveis contínuas [^1]. A MVN é fundamental para muitos modelos estatísticos e depende fortemente da álgebra linear e do cálculo matricial para lidar com dados de alta dimensão [^1]. Este capítulo visa fornecer uma compreensão profunda da MVN, abrangendo sua definição, propriedades e aplicações.

### Conceitos Fundamentais

A distribuição Gaussiana multivariada (MVN) em $D$ dimensões é definida por sua função de densidade de probabilidade (pdf) [^1]:
$$\
N(\mathbf{x}|\mathbf{\mu}, \mathbf{\Sigma}) = \frac{1}{(2\pi)^{D/2}|\mathbf{\Sigma}|^{1/2}} \exp\left[-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right]
$$
onde $\mathbf{x}$ é um vetor de dados $D$-dimensional, $\mathbf{\mu}$ é o vetor de médias $D$-dimensional, e $\mathbf{\Sigma}$ é a matriz de covariância $D \times D$ [^1].
A expressão dentro do exponencial representa a **distância de Mahalanobis** entre o vetor de dados $\mathbf{x}$ e o vetor de médias $\mathbf{\mu}$ [^1]. Para entender melhor essa quantidade, podemos realizar uma **decomposição em autovalores** de $\mathbf{\Sigma}$, escrevendo $\mathbf{\Sigma} = \mathbf{U}\mathbf{\Lambda}\mathbf{U}^T$, onde $\mathbf{U}$ é uma matriz ortonormal de autovetores satisfazendo $\mathbf{U}^T\mathbf{U} = \mathbf{I}$, e $\mathbf{\Lambda}$ é uma matriz diagonal de autovalores [^1]. Usando a decomposição em autovalores, temos:
$$\
\mathbf{\Sigma}^{-1} = \mathbf{U}\mathbf{\Lambda}^{-1}\mathbf{U}^T = \sum_{i=1}^{D} \frac{1}{\lambda_i} \mathbf{u}_i \mathbf{u}_i^T
$$
onde $\mathbf{u}_i$ é a $i$-ésima coluna de $\mathbf{U}$, contendo o $i$-ésimo autovetor [^1]. Assim, a distância de Mahalanobis pode ser reescrita como:
$$\
(\mathbf{x} - \mathbf{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu}) = \sum_{i=1}^{D} \frac{1}{\lambda_i} (\mathbf{x} - \mathbf{\mu})^T \mathbf{u}_i \mathbf{u}_i^T (\mathbf{x} - \mathbf{\mu}) = \sum_{i=1}^{D} \frac{y_i^2}{\lambda_i}
$$
onde $y_i = \mathbf{u}_i^T (\mathbf{x} - \mathbf{\mu})$ [^1]. No caso bidimensional, a equação para uma elipse é dada por [^1]:
$$\
\frac{y_1^2}{\lambda_1} + \frac{y_2^2}{\lambda_2} = 1
$$
Portanto, os contornos de igual densidade de probabilidade de uma Gaussiana estão ao longo de elipses [^1]. Os autovetores determinam a orientação da elipse, e os autovalores determinam o quão alongada ela é [^1]. Em geral, a distância de Mahalanobis corresponde à distância euclidiana em um sistema de coordenadas transformado, onde deslocamos por $\mathbf{\mu}$ e rotacionamos por $\mathbf{U}$ [^1].

**Teorema 4.1.1 (MLE para uma Gaussiana)**. Se temos $N$ amostras *iid* $x_i \sim N(\mu, \Sigma)$, então o MLE para os parâmetros é dado por [^1]:

$$\
\mathbf{\mu}_{mle} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{x}_i = \bar{\mathbf{x}}
$$

$$\
\mathbf{\Sigma}_{mle} = \frac{1}{N} \sum_{i=1}^{N} (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T = \frac{1}{N} \sum_{i=1}^{N} \mathbf{x}_i \mathbf{x}_i^T - \bar{\mathbf{x}}\bar{\mathbf{x}}^T
$$

Ou seja, o MLE é apenas a média empírica e a covariância empírica [^1]. No caso univariado, obtemos os seguintes resultados familiares [^1]:

$$\
\mu = \frac{1}{N} \sum_{i=1}^{N} x_i = \bar{x}
$$

$$\
\hat{\sigma}^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})^2 = \frac{1}{N} \sum_{i=1}^{N} x_i^2 - \bar{x}^2
$$

**Teorema 4.1.2.** Seja $q(x)$ qualquer densidade satisfazendo $\int q(x)x_i x_j = \Sigma_{ij}$. Seja $p = N(0, \Sigma)$. Então $h(q) \le h(p)$.

*Prova.* (De (Cover and Thomas 1991, p234).) Nós temos [^1]:

$$\
0 \le KL(q||p) = \int q(x) \log \frac{q(x)}{p(x)} dx
$$

$$\
= -h(q) - \int q(x) \log p(x) dx
$$

$$\
= -h(q) - \int p(x) \log p(x) dx
$$

$$\
= -h(q) + h(p)
$$
onde o passo chave na Equação 4.28 (marcado com um *) segue porque $q$ e $p$ rendem os mesmos momentos para a forma quadrática codificada por $\log p(x)$ [^1]. $\blacksquare$

### Conclusão

Este capítulo apresentou a distribuição Gaussiana multivariada (MVN), destacando sua importância como uma função de densidade de probabilidade conjunta para variáveis contínuas [^1]. Exploramos a definição da MVN, a interpretação geométrica da distância de Mahalanobis, e as estimativas de máxima verossimilhança para seus parâmetros [^1]. Além disso, discutimos a derivação da MVN como a distribuição de máxima entropia sujeita a momentos especificados [^1]. A compreensão desses conceitos fornece uma base sólida para a aplicação da MVN em uma variedade de problemas estatísticos e de aprendizado de máquina.

### Referências
[^1]: OCR do texto fornecido.
<!-- END -->