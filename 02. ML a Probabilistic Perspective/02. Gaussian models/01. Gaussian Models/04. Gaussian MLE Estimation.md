## Estimação de Máxima Verossimilhança para Modelos Gaussianos Multivariados

### Introdução
Este capítulo explora a aplicação da **Estimação de Máxima Verossimilhança (MLE)** para estimar os parâmetros de um modelo **Gaussiano Multivariado (MVN)** [^1]. O MVN, também conhecido como distribuição normal multivariada, é amplamente utilizado como uma função de densidade de probabilidade conjunta para variáveis contínuas [^1]. A MLE é um método fundamental para estimar parâmetros estatísticos e, neste contexto, nos permite encontrar os valores mais prováveis para a média e a matriz de covariância que descrevem os dados observados. A compreensão da MLE para MVNs é crucial, pois forma a base para muitos modelos estatísticos avançados [^1].

### Conceitos Fundamentais

#### Estimação de Máxima Verossimilhança (MLE) para MVN

A MLE busca encontrar os parâmetros de um modelo estatístico que maximizam a função de verossimilhança, ou equivalentemente, o logaritmo da função de verossimilhança (log-likelihood) [^1]. Para um MVN, os parâmetros de interesse são a média ($\mu$) e a matriz de covariância ($\Sigma$).

**Estimadores MLE:**

Dado um conjunto de $N$ amostras independentes e identicamente distribuídas (iid) $x_i \sim \mathcal{N}(\mu, \Sigma)$ [^3], os estimadores MLE para a média e a matriz de covariância são dados por:

*   **Média:** O estimador MLE para a média é a média empírica (média amostral) [^3]:
    $$\mu_{mle} = \frac{1}{N} \sum_{i=1}^{N} x_i = \bar{x}$$
*   **Matriz de Covariância:** O estimador MLE para a matriz de covariância é a covariância empírica (matriz de covariância amostral) [^3]:
    $$\Sigma_{mle} = \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})(x_i - \bar{x})^T$$

**Log-Likelihood e Truque do Traço:**

A função log-likelihood para um MVN é utilizada na MLE [^1]. Para facilitar a otimização, especialmente em contextos de alta dimensionalidade, o *truque do traço* é empregado [^1, 3]. Este truque permite reescrever a função log-likelihood em termos do traço de uma matriz, o que simplifica os cálculos e a derivação.

A função log-likelihood para um MVN é dada por [^4]:

$$l(\mu, \Sigma) = \log p(\mathcal{D} \mid \mu, \Sigma) = \frac{N}{2} \log |\Lambda| - \frac{1}{2} \sum_{i=1}^{N} (x_i - \mu)^T \Lambda (x_i - \mu)$$

onde $\Lambda = \Sigma^{-1}$ é a **matriz de precisão** e $\mathcal{D}$ representa o conjunto de dados [^1, 4]. A **matriz de dispersão (scatter matrix)** $S_{\mu}$ é definida como [^1, 4]:

$$S_{\mu} = \sum_{i=1}^{N} (x_i - \mu)(x_i - \mu)^T$$

Utilizando o truque do traço, a função log-likelihood pode ser reescrita como [^3, 4]:

$$l(\Lambda) = \frac{N}{2} \log |\Lambda| - \frac{1}{2} \text{tr} [S_{\mu} \Lambda]$$

**Derivação dos Estimadores MLE:**

A derivação dos estimadores MLE para $\mu$ e $\Sigma$ envolve maximizar a função log-likelihood em relação a esses parâmetros. Para a média $\mu$, a derivada da log-likelihood em relação a $\mu$ é calculada e igualada a zero [^4]:

$$\frac{\partial l}{\partial \mu} = \Sigma^{-1} \sum_{i=1}^{N} (x_i - \mu) = 0$$

Resolvendo para $\mu$, obtemos o estimador MLE:

$$\hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} x_i = \bar{x}$$

Para a matriz de covariância $\Sigma$, a derivada da log-likelihood em relação a $\Lambda$ (e, equivalentemente, a $\Sigma$) é calculada e igualada a zero [^4]:

$$\frac{\partial l(\Lambda)}{\partial \Lambda} = \frac{N}{2} \Lambda^{-T} - \frac{1}{2} S_{\mu} = 0$$

Resolvendo para $\Sigma$, obtemos o estimador MLE:

$$\Sigma = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)(x_i - \mu)^T$$

Substituindo $\mu$ pelo seu estimador MLE $\bar{x}$, obtemos a matriz de covariância empírica centrada em $\mu$ [^1, 4].

### Conclusão

A MLE fornece um método direto e intuitivo para estimar os parâmetros de um MVN [^1]. Os estimadores MLE para a média e a matriz de covariância são a média empírica e a matriz de covariância empírica, respectivamente [^1]. O uso do truque do traço simplifica a otimização da função log-likelihood [^1, 3]. No entanto, é importante notar que a MLE pode apresentar problemas em contextos de alta dimensionalidade, como overfitting [^10]. Nesses casos, técnicas de regularização ou inferência Bayesiana podem ser mais apropriadas [^3].

### Referências
[^1]: Seção 4.1.3
[^3]: Seção 4.1.3.1
[^4]: Seção 4
<!-- END -->