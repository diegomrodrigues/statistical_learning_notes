## Entropia Máxima e a Distribuição Gaussiana Multivariada

### Introdução

Este capítulo explora a distribuição Gaussiana multivariada (**MVN**) e sua propriedade de maximizar a entropia sob certas condições. A MVN é uma das distribuições de probabilidade conjuntas mais amplamente utilizadas para variáveis contínuas [^1]. A capacidade de estimar com segurança apenas os dois primeiros momentos a partir dos dados torna a MVN particularmente útil [^1].

### Conceitos Fundamentais

A distribuição Gaussiana multivariada, também conhecida como distribuição normal multivariada, é definida em $D$ dimensões por seus parâmetros de média ($\mu$) e matriz de covariância ($\Sigma$). A função densidade de probabilidade (**pdf**) para uma MVN é dada por [^1]:

$$\
N(x|\mu, \Sigma) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} \exp \left[ -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right]
$$

onde [^1]:
- $x$ é um vetor de dados $D$-dimensional.
- $\mu$ é o vetor de média $D$-dimensional.
- $\Sigma$ é a matriz de covariância $D \times D$.
- $|\Sigma|$ é o determinante da matriz de covariância.

A expressão dentro do exponencial representa a distância de Mahalanobis entre um vetor de dados $x$ e o vetor de média $\mu$ [^1]. A matriz de covariância $\Sigma$ pode ser decomposta em $U \Lambda U^T$, onde $U$ é uma matriz ortonormal de autovetores e $\Lambda$ é uma matriz diagonal de autovalores [^1].

Um dos motivos pelos quais a distribuição Gaussiana é tão amplamente utilizada é que ela maximiza a entropia sujeita a uma média e covariância especificadas [^1]. A entropia (diferencial) de uma distribuição Gaussiana $N(\mu, \Sigma)$ é dada por [^1]:

$$\
h(N(\mu, \Sigma)) = \frac{1}{2} \ln [(2\pi e)^D |\Sigma|]
$$

Esta propriedade de **entropia máxima** é crucial quando apenas os dois primeiros momentos podem ser estimados de forma confiável a partir dos dados. Ao usar uma Gaussiana, estamos essencialmente fazendo o mínimo de suposições adicionais além desses momentos [^1].

Para demonstrar que a MVN tem entropia máxima, considera-se uma densidade arbitrária $q(x)$ que satisfaz $\int q(x) x_i x_j dx = \Sigma_{ij}$. Seja $p = N(0, \Sigma)$. Então, $h(q) \leq h(p)$ [^1]. A prova utiliza a divergência de Kullback-Leibler ($KL$) [^1]:

$$\
0 \leq KL(q||p) = \int q(x) \log \frac{q(x)}{p(x)} dx
$$

Expandindo a expressão e utilizando o fato de que $q$ e $p$ compartilham os mesmos momentos para a forma quadrática codificada por $\log p(x)$, obtemos [^1]:

$$\
KL(q||p) = -h(q) - \int q(x) \log p(x) dx = -h(q) - \int p(x) \log p(x) dx = -h(q) + h(p)
$$

Portanto, $h(q) \leq h(p)$, mostrando que a MVN tem entropia máxima dado $\Sigma$ [^1]. $\blacksquare$

### Conclusão

A distribuição Gaussiana multivariada é uma ferramenta fundamental em modelagem estatística, particularmente útil quando os dados são de alta dimensão [^1]. Sua propriedade de maximizar a entropia, juntamente com a facilidade de cálculo e análise, a torna uma escolha valiosa em uma ampla gama de aplicações [^1].

### Referências

[^1]: Gaussian Models.
<!-- END -->