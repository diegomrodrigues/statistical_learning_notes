## Maximum Entropy Property of the Multivariate Gaussian

### Introdução
Este capítulo explora a propriedade de **máxima entropia** da distribuição Gaussiana multivariada (MVN) sob restrições de média e covariância especificadas [^1, ^5]. Como a distribuição gaussiana é um dos modelos estatísticos mais utilizados, entender suas justificativas teóricas é fundamental. Este capítulo visa fornecer uma explicação rigorosa e detalhada desta propriedade, demonstrando por que a MVN é frequentemente escolhida quando apenas os dois primeiros momentos de uma distribuição são conhecidos [^5].

### Conceitos Fundamentais

A distribuição gaussiana multivariada, ou distribuição normal multivariada (MVN), é amplamente utilizada como uma função de densidade de probabilidade conjunta para variáveis contínuas [^1]. A densidade de probabilidade para uma MVN em $D$ dimensões é definida como [^1]:
$$
N(\mathbf{x}|\boldsymbol{\mu}, \mathbf{\Sigma}) = \frac{1}{(2\pi)^{D/2}|\mathbf{\Sigma}|^{1/2}} \exp \left[ -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right]
$$
onde $\boldsymbol{\mu}$ é o vetor de médias e $\mathbf{\Sigma}$ é a matriz de covariância [^1].

A **entropia diferencial** de uma distribuição $p(x)$ é definida como [^5]:
$$
h(p) = - \int p(x) \log p(x) dx
$$
A propriedade de máxima entropia da MVN afirma que, dentre todas as distribuições com uma média $\boldsymbol{\mu}$ e covariância $\mathbf{\Sigma}$ especificadas, a MVN tem a maior entropia. Isso significa que, ao usar uma MVN, estamos fazendo o mínimo de suposições adicionais além das restrições impostas pela média e covariância [^5].

**Teorema 4.1.2** [^5]: Seja $q(x)$ qualquer densidade satisfazendo $\int q(x)x_ix_j dx = \Sigma_{ij}$. Seja $p = N(0, \Sigma)$. Então $h(q) \leq h(p)$.

*Prova* [^5]:
A divergência de Kullback-Leibler (KL) entre duas distribuições $q(x)$ e $p(x)$ é definida como:
$$
KL(q||p) = \int q(x) \log \frac{q(x)}{p(x)} dx
$$
A divergência de KL é sempre não negativa, ou seja, $KL(q||p) \geq 0$ [^5]. Portanto,
$$
0 \leq KL(q||p) = \int q(x) \log \frac{q(x)}{p(x)} dx = -h(q) - \int q(x) \log p(x) dx
$$
Rearranjando, temos:
$$
h(q) \leq - \int q(x) \log p(x) dx
$$
Para $p(x) = N(0, \Sigma)$, temos [^5]:
$$
\log p(x) = -\frac{1}{2} \log [(2\pi)^D |\Sigma|] - \frac{1}{2} x^T \Sigma^{-1} x
$$
Substituindo na desigualdade, obtemos:
$$
h(q) \leq - \int q(x) \left( -\frac{1}{2} \log [(2\pi)^D |\Sigma|] - \frac{1}{2} x^T \Sigma^{-1} x \right) dx
$$
$$
h(q) \leq \frac{1}{2} \log [(2\pi)^D |\Sigma|] + \frac{1}{2} \int q(x) x^T \Sigma^{-1} x dx
$$
Como $\int q(x)x_ix_j dx = \Sigma_{ij}$, temos $\int q(x) x^T \Sigma^{-1} x dx = \text{tr}(\Sigma^{-1} \Sigma) = D$. Portanto:
$$
h(q) \leq \frac{1}{2} \log [(2\pi)^D |\Sigma|] + \frac{1}{2} D
$$
$$
h(q) \leq \frac{1}{2} \log [(2\pi e)^D |\Sigma|] = h(p)
$$
Assim, $h(q) \leq h(p)$, onde $p(x)$ é a MVN com média zero e covariância $\mathbf{\Sigma}$ [^5]. $\blacksquare$

**Corolário:** A entropia da MVN é dada por [^5]:
$$
h(N(\boldsymbol{\mu}, \mathbf{\Sigma})) = \frac{1}{2} \ln[(2\pi e)^D |\mathbf{\Sigma}|]
$$

### Conclusão
A propriedade de máxima entropia da distribuição Gaussiana multivariada fornece uma justificativa teórica sólida para seu uso generalizado em uma variedade de aplicações [^5]. Ao escolher uma MVN com uma média e covariância especificadas, estamos selecionando a distribuição que faz o mínimo de suposições adicionais sobre os dados [^5]. Isso torna a MVN uma escolha robusta e flexível quando apenas os dois primeiros momentos de uma distribuição são conhecidos ou podem ser estimados de forma confiável.
<!-- END -->