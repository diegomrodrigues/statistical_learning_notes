## Entropia Diferencial da Distribuição Gaussiana

### Introdução
Este capítulo explora a entropia diferencial da distribuição Gaussiana, um conceito fundamental na teoria da informação e estatística. A entropia diferencial quantifica a incerteza associada a uma variável aleatória contínua, e a distribuição Gaussiana, dada sua prevalência em modelagem estatística, merece uma análise detalhada de sua entropia. Este capítulo irá basear-se nos conceitos fundamentais de modelos Gaussianos introduzidos anteriormente [^1] [^2] [^3] [^4] [^5] [^6] [^7] [^8] [^9] [^10] [^11] [^12] [^13] [^14] [^15] [^16] [^17] [^18] [^19] [^20] [^21] [^22] [^23] [^24] [^25] [^26] [^27] [^28] [^29] [^30] [^31] [^32] [^33] [^34] [^35] [^36] [^37] [^38] [^39] [^40] [^41] [^42] [^43] [^44] [^45] [^46], e culminará numa discussão sobre sua relação com a maximização da entropia.

### Conceitos Fundamentais
A **entropia diferencial** $h(X)$ de uma variável aleatória contínua $X$ com função densidade de probabilidade (pdf) $p(x)$ é definida como:
$$ h(X) = -\int p(x) \log p(x) dx $$
Para uma distribuição Gaussiana multivariada (MVN) $N(\mu, \Sigma)$ em $D$ dimensões, a pdf é dada por [^1]:
$$ N(x|\mu, \Sigma) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} \exp \left[ -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right] $$
Onde $\mu$ é o vetor de médias e $\Sigma$ é a matriz de covariância.
A **entropia diferencial** $h(N(\mu, \Sigma))$ de uma distribuição Gaussiana é expressa matematicamente, refletindo a dispersão e incerteza da distribuição [^5]:
$$ h(N(\mu, \Sigma)) = \frac{1}{2} \ln \left[ (2\pi e)^D |\Sigma| \right] $$
Essa equação revela que a entropia da Gaussiana depende apenas da matriz de covariância $\Sigma$ e da dimensão $D$. Em particular, a entropia aumenta com o aumento da dimensão e com o determinante da matriz de covariância, $|\Sigma|$, que representa o volume da elipse de concentração da distribuição [^2].

**Maximização da Entropia**:
Um resultado importante é que, entre todas as distribuições com uma matriz de covariância $\Sigma$ especificada, a distribuição Gaussiana maximiza a entropia [^5]. Este resultado é crucial porque justifica o uso frequente da distribuição Gaussiana em modelagem estatística. Quando temos apenas informações sobre os dois primeiros momentos (média e covariância) de uma distribuição, a escolha da Gaussiana é a mais conservadora, no sentido de que faz o menor número de suposições adicionais sobre a forma da distribuição [^5].

**Teorema 4.1.2** [^5]. *Seja $q(x)$ qualquer densidade satisfazendo $\int q(x)x_ix_j dx = \Sigma_{ij}$. Seja $p = N(0, \Sigma)$. Então $h(q) \le h(p)$.*

A prova deste teorema envolve o uso da divergência de Kullback-Leibler (KL) [^5]:
$$KL(q||p) = \int q(x) \log \frac{q(x)}{p(x)} dx$$
A divergência KL é sempre não negativa, $KL(q||p) \ge 0$ [^5]. Expandindo a divergência KL, temos:
$$0 \le KL(q||p) = -h(q) - \int q(x) \log p(x) dx = -h(q) + h(p)$$
A chave para essa prova é que $q$ e $p$ compartilham os mesmos momentos de primeira e segunda ordem para a forma quadrática codificada por $\log p(x)$ [^5].

### Conclusão
A entropia diferencial da distribuição Gaussiana oferece uma medida quantitativa de sua incerteza e dispersão. A propriedade de maximização da entropia, sujeita a uma covariância especificada, solidifica a importância da Gaussiana como uma escolha padrão em modelagem estatística, especialmente quando as informações disponíveis são limitadas aos primeiros dois momentos. A capacidade de expressar a entropia em termos do determinante da matriz de covariância fornece uma interpretação geométrica da incerteza associada à distribuição.

### Referências
[^1]: Capítulo 4, Seção 4.1.2
[^2]: Capítulo 4, Seção 4.1.2, Equação 4.1
[^3]: Capítulo 4, Seção 4.1.1
[^4]: Capítulo 4, Seção 4.1.3
[^5]: Capítulo 4, Seção 4.1.4
[^6]: Capítulo 4, Seção 4.2
[^7]: Capítulo 4, Seção 4.2.1
[^8]: Capítulo 4, Seção 4.2.2
[^9]: Capítulo 4, Seção 4.2.3
[^10]: Capítulo 4, Seção 4.2.4
[^11]: Capítulo 4, Seção 4.2.5
[^12]: Capítulo 4, Seção 4.2.6
[^13]: Capítulo 4, Seção 4.2.7
[^14]: Capítulo 4, Seção 4.2.8
[^15]: Capítulo 4, Seção 4.3
[^16]: Capítulo 4, Seção 4.3.1
[^17]: Capítulo 4, Seção 4.3.2
[^18]: Capítulo 4, Seção 4.3.3
[^19]: Capítulo 4, Seção 4.3.4
[^20]: Capítulo 4, Seção 4.4
[^21]: Capítulo 4, Seção 4.4.1
[^22]: Capítulo 4, Seção 4.4.2
[^23]: Capítulo 4, Seção 4.5
[^24]: Capítulo 4, Seção 4.6
[^25]: Capítulo 4, Seção 4.6.1
[^26]: Capítulo 4, Seção 4.6.2
[^27]: Capítulo 4, Seção 4.6.3
[^28]: Capítulo 4, Seção 4.7
[^29]: Capítulo 4, Exercício 4.1
[^30]: Capítulo 4, Exercício 4.2
[^31]: Capítulo 4, Exercício 4.3
[^32]: Capítulo 4, Exercício 4.4
[^33]: Capítulo 4, Exercício 4.5
[^34]: Capítulo 4, Exercício 4.6
[^35]: Capítulo 4, Exercício 4.7
[^36]: Capítulo 4, Exercício 4.8
[^37]: Capítulo 4, Exercício 4.9
[^38]: Capítulo 4, Exercício 4.10
[^39]: Capítulo 4, Exercício 4.11
[^40]: Capítulo 4, Exercício 4.12
[^41]: Capítulo 4, Exercício 4.13
[^42]: Capítulo 4, Exercício 4.14
[^43]: Capítulo 4, Exercício 4.15
[^44]: Capítulo 4, Exercício 4.16
[^45]: Capítulo 4, Exercício 4.17
[^46]: Capítulo 4, Exercício 4.18

<!-- END -->