## Maximum Likelihood Estimation in Gaussian Discriminant Analysis

### Introdução
Este capítulo explora o uso da **Maximum Likelihood Estimation (MLE)** no contexto da **Gaussian Discriminant Analysis (GDA)**. GDA é uma técnica de classificação generativa que assume que as densidades condicionais de classe seguem uma distribuição gaussiana [^1, 4.2]. O objetivo da MLE é encontrar os parâmetros das gaussianas que melhor se ajustam aos dados de treinamento, permitindo a classificação de novas amostras com base na probabilidade de pertencerem a cada classe. A partir do modelo ajustado, é possível classificar um vetor de características usando a seguinte regra de decisão [^6, 4.31]:
$$\
\hat{y}(x) = \underset{c}{\operatorname{argmax}} \left[ \log p(y=c|\pi) + \log p(x|\theta_c) \right]
$$

### Conceitos Fundamentais
A MLE para análise discriminante envolve maximizar a função de log-verossimilhança [^1]:
$$\
\log p(D|\theta) = \sum_{i=1}^{N} \sum_{c=1}^{C} I(y_i = c) \log \pi_c + \sum_{c=1}^{C} \sum_{i: y_i = c} \log N(x_i|\mu_c, \Sigma_c)
$$
onde:
- $D$ representa o conjunto de dados de treinamento
- $\theta$ representa os parâmetros do modelo, incluindo as probabilidades *a priori* das classes ($\pi_c$), os vetores de média ($\mu_c$) e as matrizes de covariância ($\Sigma_c$) para cada classe $c$
- $N$ é o número total de amostras no conjunto de dados
- $C$ é o número de classes
- $I(y_i = c)$ é uma função indicadora que é 1 se a amostra $i$ pertence à classe $c$, e 0 caso contrário
- $N(x_i|\mu_c, \Sigma_c)$ é a função de densidade de probabilidade gaussiana multivariada (MVN) para a amostra $x_i$ sob a classe $c$ [^1, 4.1]

A função de log-verossimilhança se **fatoriza** em termos para $\pi$, $\mu_c$ e $\Sigma_c$, permitindo a estimação separada [^1]. Isso significa que podemos encontrar os estimadores de máxima verossimilhança para cada conjunto de parâmetros independentemente.

**Estimativa das Probabilidades *a Priori* das Classes ($\pi_c$)**:

As probabilidades *a priori* das classes são estimadas como a frequência relativa de cada classe no conjunto de dados de treinamento:
$$\
\hat{\pi}_c = \frac{N_c}{N}
$$
onde $N_c$ é o número de amostras pertencentes à classe $c$.

**Estimativa dos Vetores de Média ($\mu_c$)**:

O vetor de média para cada classe é estimado como a média amostral das amostras pertencentes a essa classe [^1, 4.6]:
$$\
\hat{\mu}_c = \frac{1}{N_c} \sum_{i: y_i = c} x_i
$$

**Estimativa das Matrizes de Covariância ($\Sigma_c$)**:

A matriz de covariância para cada classe é estimada como a covariância amostral das amostras pertencentes a essa classe [^1, 4.7]:
$$\
\hat{\Sigma}_c = \frac{1}{N_c} \sum_{i: y_i = c} (x_i - \hat{\mu}_c)(x_i - \hat{\mu}_c)^T
$$

**Linear Discriminant Analysis (LDA) como Caso Especial**:

Em LDA, assume-se que todas as classes compartilham a mesma matriz de covariância [^7, 4.2.2]: $\Sigma_c = \Sigma, \forall c$.  Neste caso, a estimativa da matriz de covariância comum é uma média ponderada das matrizes de covariância de cada classe [^10, 4.53].

**Regularização**:

Em situações onde o número de amostras é pequeno em relação à dimensionalidade dos dados, a MLE pode levar a *overfitting*. Para mitigar esse problema, técnicas de regularização podem ser aplicadas. Uma abordagem comum é usar a *Regularized Discriminant Analysis (RDA)*, que envolve a aplicação de um *prior* sobre as matrizes de covariância [^11, 4.2.6].

### Conclusão
A MLE fornece uma abordagem direta para estimar os parâmetros de um modelo GDA. No entanto, é importante estar ciente dos potenciais problemas de *overfitting*, especialmente em dados de alta dimensão, e considerar técnicas de regularização apropriadas. A GDA é um exemplo de modelo generativo, e sua relação com modelos discriminativos como regressão logística será explorada em outros capítulos.

### Referências
[^1]: *Gaussian models*
<!-- END -->