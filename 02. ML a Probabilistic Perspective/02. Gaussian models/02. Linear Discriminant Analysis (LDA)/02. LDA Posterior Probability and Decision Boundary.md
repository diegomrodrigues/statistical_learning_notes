## Análise Discriminante Linear e Fronteiras de Decisão Lineares

### Introdução
Este capítulo explora em profundidade a Análise Discriminante Linear (LDA), com um foco especial nas fronteiras de decisão lineares que emergem desta técnica. A LDA é uma ferramenta poderosa para classificação, especialmente útil quando as classes podem ser razoavelmente modeladas por distribuições Gaussianas multivariadas com matrizes de covariância compartilhadas. Este capítulo detalha a formulação matemática da LDA, derivando as fronteiras de decisão e explorando o caso especial de duas classes. As referências aos modelos Gaussianos [^1] e à Análise Discriminante Gaussiana (GDA) [^30] fornecem uma base para entender a LDA como uma simplificação da GDA, onde a restrição de covariâncias compartilhadas leva a fronteiras de decisão lineares.

### Conceitos Fundamentais

A LDA assume que as classes condicionais seguem uma distribuição Gaussiana multivariada (MVN) [^1]. A probabilidade *a posteriori* de uma classe $c$ dado um vetor de características $x$ e os parâmetros $\theta$ é dada por [^4.38]:

$$ p(y = c|x, \theta) = \frac{\pi_c \exp[\mu_c^T \Sigma^{-1} x - \frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c]}{\sum_{c'} \pi_{c'} \exp[\mu_{c'}^T \Sigma^{-1} x - \frac{1}{2} \mu_{c'}^T \Sigma^{-1} \mu_{c'}]}\ $$

onde $\pi_c$ é a probabilidade *a priori* da classe $c$, $\mu_c$ é a média da classe $c$, e $\Sigma$ é a matriz de covariância compartilhada entre todas as classes [^4.2.2]. A chave da LDA reside na simplificação que leva a fronteiras de decisão lineares. As fronteiras de decisão são determinadas por [^4.36, 4.37]:

$$ \gamma_c = \frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c + \log \pi_c\ $$

$$ \beta_c = \Sigma^{-1} \mu_c\ $$

Substituindo $\gamma_c$ e $\beta_c$ na equação da probabilidade *a posteriori*, resulta numa função linear de $x$. Isso é um resultado direto da matriz de covariância compartilhada, que permite o cancelamento de termos quadráticos em $x$ [^4.2.2].

**Caso de Duas Classes:**

No cenário de duas classes, a probabilidade *a posteriori* é dada por [^4.44, 4.45]:

$$ p(y = 1|x, \theta) = \frac{1}{1 + e^{(\beta_0 - \beta_1)^T x + (\gamma_0 - \gamma_1) }} = \text{sigm}((\beta_1 - \beta_0)^T x + (\gamma_1 - \gamma_0))\ $$

Este resultado mostra que a probabilidade *a posteriori* é uma função sigmóide de uma função linear de $x$, indicando que a fronteira de decisão é uma linha reta [^4.45]. A função `sigm` é a função sigmóide, definida como $\text{sigm}(z) = \frac{1}{1 + e^{-z}}$.

**Interpretação Geométrica:**

A fronteira de decisão linear pode ser expressa como [^4.50]:

$$ w^T x_0 = -(\gamma_1 - \gamma_0)\ $$

onde $w = \beta_1 - \beta_0 = \Sigma^{-1} (\mu_1 - \mu_0)$ e $x_0$ é um ponto na fronteira de decisão [^4.48, 4.49]. O vetor $w$ é normal à fronteira de decisão e aponta na direção de $\mu_1 - \mu_0$. O ponto $x_0$ é um ponto na fronteira de decisão. Se as classes tiverem *a priori* iguais e $\Sigma = \sigma^2 I$, então $x_0$ está no ponto médio entre as médias das classes [^4.50].

**Discriminabilidade:**

A discriminabilidade das classes é quantificada pelo d-prime [^4.51]:

$$ d' = \frac{\mu_1 - \mu_0}{\sigma}\ $$

onde $\sigma$ é o desvio padrão do ruído. Um d-prime maior indica que as classes são mais fáceis de discriminar [^4.51].

### Conclusão

A Análise Discriminante Linear oferece uma abordagem elegante e eficiente para problemas de classificação, aproveitando as propriedades das distribuições Gaussianas multivariadas com covariâncias compartilhadas. A imposição dessa restrição leva a fronteiras de decisão lineares, o que simplifica significativamente a complexidade do modelo e facilita a interpretação. A formulação matemática detalhada neste capítulo fornece uma base sólida para a compreensão e aplicação da LDA em uma variedade de cenários.

### Referências
[^1]: Seção 4.1.2
[^30]: Seção 4.2
[^4.2.2]: Seção 4.2.2
[^4.38]: Equação 4.38
[^4.36]: Equação 4.36
[^4.37]: Equação 4.37
[^4.44]: Equação 4.44
[^4.45]: Equação 4.45
[^4.50]: Equação 4.50
[^4.48]: Equação 4.48
[^4.49]: Equação 4.49
[^4.51]: Equação 4.51
<!-- END -->