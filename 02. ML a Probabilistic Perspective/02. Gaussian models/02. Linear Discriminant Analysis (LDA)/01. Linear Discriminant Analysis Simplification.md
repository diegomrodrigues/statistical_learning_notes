## Linear Decision Boundaries in Linear Discriminant Analysis (LDA)

### Introdução

Este capítulo explora em detalhes o conceito de fronteiras de decisão lineares no contexto da Análise Discriminante Linear (LDA). A LDA, como um método de classificação, simplifica o problema ao impor restrições sobre as matrizes de covariância das classes, resultando em fronteiras de decisão com propriedades específicas. Vamos mergulhar nas implicações matemáticas e práticas dessa simplificação, detalhando como ela afeta o desempenho e a aplicabilidade do modelo. A discussão se baseará nos conceitos de modelos Gaussianos [^1], Análise Discriminante Gaussiana [^2], e, em particular, na simplificação introduzida pela LDA [^3].

### Conceitos Fundamentais

A Análise Discriminante Linear (LDA) é uma técnica de classificação que assume que os dados de cada classe seguem uma distribuição Gaussiana multivariada (MVN) [^1].  Na Análise Discriminante Gaussiana (GDA), as densidades condicionais de classe são definidas em um classificador generativo, onde $p(x|y = c, \theta) = N(x|\mu_c, \Sigma_c)$ [^2]. No entanto, a LDA introduz uma simplificação crucial: as matrizes de covariância de todas as classes são consideradas iguais, ou seja, $\Sigma_c = \Sigma$ para todas as classes *c* [^3]. Esta restrição tem um impacto profundo na forma da fronteira de decisão entre as classes.

Para entender o efeito dessa restrição, considere a regra de decisão geral para classificar um vetor de características *x* em uma das classes *c*. Na GDA, a decisão é baseada em qual classe maximiza o log da probabilidade posterior [^6]:

$$\
\hat{y}(x) = \underset{c}{\operatorname{argmax}} \left[ \log p(y = c|\pi) + \log p(x|\theta_c) \right]
$$

onde $\pi$ é a probabilidade a priori da classe *c*, e $\theta_c$ representa os parâmetros da distribuição Gaussiana para a classe *c* (ou seja, $\mu_c$ e $\Sigma_c$). Ao substituir a distribuição Gaussiana e simplificar, obtemos [^6]:

$$\
\hat{y}(x) = \underset{c}{\operatorname{argmin}} (x - \mu_c)^T \Sigma_c^{-1} (x - \mu_c)
$$

Na LDA, como $\Sigma_c = \Sigma$ para todas as classes, a expressão acima se simplifica.  A probabilidade condicional da classe é proporcional a [^7]:

$$\
p(y = c|x, \theta) \propto \pi_c \exp \left[ -\frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c + x^T \Sigma^{-1} \mu_c - \frac{1}{2} x^T \Sigma^{-1} x \right]
$$

Observe que o termo quadrático $x^T \Sigma^{-1} x$ é independente da classe *c*. Isso significa que ele se cancela ao comparar as probabilidades posteriores de diferentes classes, resultando em uma fronteira de decisão linear [^7].  Definindo $\gamma_c = \frac{1}{2}\mu_c^T \Sigma^{-1} \mu_c + \log \pi_c$ e $\beta_c = \Sigma^{-1} \mu_c$, a equação pode ser reescrita como [^8]:

$$\
p(y = c|x, \theta) = \frac{e^{\beta_c^T x + \gamma_c}}{\sum_{c'} e^{\beta_{c'}^T x + \gamma_{c'}}} = S(\eta)_c
$$

onde $S(\eta)$ é a função softmax. A razão pela qual a fronteira é linear é que o termo $x^T \Sigma^{-1}x$ se cancela do numerador e denominador. A fronteira de decisão entre quaisquer duas classes *c* e *c'* é dada por [^8]:

$$\
x^T (\beta_{c'} - \beta_c) = \gamma_c' - \gamma_c
$$

Esta equação representa um hiperplano em um espaço *D*-dimensional, indicando que a fronteira de decisão é linear [^8].

**Vantagens da Fronteira Linear:**
*   **Simplicidade:** Modelos lineares são fáceis de interpretar e implementar.
*   **Eficiência:** A computação é mais rápida em comparação com modelos não lineares, especialmente em conjuntos de dados de alta dimensão.
*   **Menos parâmetros:** A LDA tem menos parâmetros para estimar do que a GDA, o que pode ajudar a evitar overfitting, especialmente quando o número de amostras é pequeno em relação à dimensionalidade dos dados.

**Desvantagens da Fronteira Linear:**
*   **Capacidade limitada:** A LDA pode não ser adequada para problemas onde as classes não são linearmente separáveis.
*   **Restrição da covariância:** A suposição de covariância compartilhada pode ser uma simplificação excessiva para alguns conjuntos de dados, levando a um desempenho inferior em comparação com modelos mais flexíveis.

### Conclusão

A LDA oferece uma abordagem eficiente e interpretável para a classificação, particularmente útil quando as classes são aproximadamente Gaussianas e têm matrizes de covariância semelhantes. A imposição de uma matriz de covariância compartilhada leva a fronteiras de decisão lineares, simplificando o modelo e reduzindo o risco de overfitting. No entanto, é crucial reconhecer as limitações dessa simplificação e considerar modelos mais flexíveis, como a QDA ou outros métodos não lineares, quando as classes não são linearmente separáveis ou quando a suposição de covariância compartilhada não é válida. A escolha entre LDA e outras técnicas de classificação deve ser guiada por uma análise cuidadosa das características dos dados e dos requisitos específicos do problema em questão.

### Referências
[^1]: Seção 4.1
[^2]: Seção 4.2
[^3]: Seção 4.2.2
[^6]: Seção 4.2
[^7]: Seção 4.2.2
[^8]: Seção 4.2.2
<!-- END -->