{
  "topics": [
    {
      "topic": "Inferring the Parameters of an MVN",
      "sub_topics": [
        "To infer the parameters of an MVN, we first compute p(\\\\mu|D, \\\\Sigma), then p(\\\\Sigma|D, \\\\mu), and finally the joint p(\\\\mu, \\\\Sigma|D); the likelihood has a form related to the exponential of a trace involving \\\\Sigma^-1 and the data, which can be simplified using matrix properties. The likelihood for \\\\(N\\\\) i.i.d. samples \\\\(x_i \\\\sim N(\\\\mu, \\\\Sigma)\\\\) is given by \\\\(p(D|\\\\mu) = N(\\\\bar{x}|\\\\mu, \\\\frac{1}{N} \\\\Sigma)\\\\), where \\\\(\\\\bar{x}\\\\) is the sample mean.",
        "When inferring the parameters of an MVN, the posterior distribution of \\\\(\\\\mu\\\\) given \\\\(D\\\\) and \\\\(\\\\Sigma\\\\) is Gaussian, and if we use a conjugate prior (Gaussian), then we can derive a Gaussian posterior for \\\\(\\\\mu\\\\) based on the results in Section 4.4.2.2. We get \\\\(p(\\\\mu|D, \\\\Sigma) = N(\\\\mu|m_N, V_N)\\\\), \\\\(V_N^{-1} = V_0^{-1} + N\\\\Sigma^{-1}\\\\), \\\\(m_N = V_N(\\\\Sigma^{-1}(N\\\\bar{x}) + V_0^{-1}m_0)\\\\). Using a conjugate prior \\\\(p(\\\\mu) = N(\\\\mu|m_0, V_0)\\\\), the posterior distribution for \\\\(\\\\mu\\\\) is \\\\(p(\\\\mu|D, \\\\Sigma) = N(\\\\mu|m_N, V_N)\\\\), where \\\\(V_N^{-1} = V_0^{-1} + N\\\\Sigma^{-1}\\\\) and \\\\(m_N = V_N (\\\\Sigma^{-1}(N\\\\bar{x}) + V_0^{-1} m_0)\\\\). The posterior for the mean is equal to the MLE. We also see that the posterior variance goes down as 1/N, which is a standard result from frequentist statistics.",
        "The corresponding conjugate prior for \\\\(\\\\Sigma\\\\) is known as the inverse Wishart distribution, and we can use the results from Section 4.5.1. The likelihood has the form \\\\(p(D|\\\\mu, \\\\Sigma) \\\\propto |\\\\Sigma|^{-\\\\frac{N}{2}} exp[-\\\\frac{1}{2} tr(\\\\Sigma^{-1}S_{\\\\mu})]\\\\). Using a conjugate prior \\\\(IW(\\\\Sigma|S_0^{-1}, \\\\nu_0)\\\\), the posterior distribution for \\\\(\\\\Sigma\\\\) is \\\\(IW(\\\\Sigma|S_N^{-1}, \\\\nu_N)\\\\), where \\\\(\\\\nu_N = \\\\nu_0 + N\\\\) and \\\\(S_N = S_0 + S_{\\\\mu\\\\}). The posterior strength vN is the prior strength v0 plus the number of observations N, and the posterior scatter matrix SN is the prior scatter matrix S0 plus the data scatter matrix S\\\\mu.",
        "To address the issue of overfitting, the MAP estimate is given by \\\\(\\\\hat{\\\\Sigma}_{map} = \\\\frac{S_N}{\\\\nu_N + D + 1} = \\\\frac{S_0 + S_{\\\\mu}}{N_0 + N}\\\\). If we use an improper uniform prior, corresponding to \\\\(N_0 = 0\\\\) and \\\\(S_0 = 0\\\\), we recover the MLE. To use a proper informative prior, let \\\\(\\\\mu = \\\\bar{x}\\\\), so \\\\(S_{\\\\mu} = S\\\\). Then we can rewrite the MAP estimate as a convex combination of the prior mode and the MLE. To see this, let \\\\(\\\\Sigma_0\\\\) be the prior mode. Then the posterior mode can be rewritten as \\\\(\\\\hat{\\\\Sigma}_{map} = \\\\frac{N_0}{N_0 + N} \\\\frac{S_0}{N_0} + \\\\frac{N}{N_0 + N} \\\\frac{S}{N} = \\\\lambda \\\\Sigma_0 + (1 - \\\\lambda) \\\\hat{\\\\Sigma}_{mle}\\\\). MAP estimation involves using the posterior mode for the mean and covariance matrix. One can show that the MAP estimate is given by SN/VN+D+1",
        "Using a Normal-Inverse-Wishart (NIW) distribution as the prior, defined as \\\\(NIW(\\\\mu, \\\\Sigma|m_0, \\\\kappa_0, V_0, S_0)\\\\), the posterior is also NIW with updated parameters, and the posterior marginal for \\\\(\\\\mu\\\\) has a multivariate Student T distribution.",
        "To estimate the covariance matrix, we can use the posterior mode (or mean), which gives a MAP estimate that is a convex combination of the prior mode and the MLE; this approach is particularly useful when the MLE is ill-conditioned.",
        "When estimating a covariance matrix, shrinkage estimation or regularized estimation is used, where diagonal entries are equal to their ML estimates and off-diagonal elements are 'shrunk' somewhat towards 0; if the data is 1d, the standard conjugate prior is the inverse Gamma distribution, or a scaled inverse chi-squared distribution.",
        "The posterior predictive for MVN is given by p(x|D) = \\\\intN(x|\\\\mu, \\\\Sigma)NIW(\\\\mu, \\\\Sigma|m\\\\nu, \\\\kappa\\\\nu, \\\\nu\\\\nu, S\\\\nu)d\\\\mud\\\\Sigma, which has the form of a multivariate Student-T distribution; for scalar data, the results are widely used in statistics, and it is conventional to use the normal inverse chi-squared or NIX distribution.",
        "The Bayesian t-test can be used to test the hypothesis that \\\\mu \\\\neq \\\\muo for some known value \\\\muo, given values xi ~ N(\\\\mu, \\\\sigma^2); a simple way to perform such a test is just to check if \\\\muo \\\\in 10.95(\\\\mu|D). If it is not, then we can be 95% sure that \\\\mu \\\\neq \\\\muo."
      ]
    },
    {
      "topic": "Digression: The Wishart Distribution",
      "sub_topics": [
        "The Wishart distribution is the generalization of the Gamma distribution to positive definite matrices and is defined as \\\\(Wi(A|S, \\\\nu) = \\\\frac{|A|^{(\\\\nu-D-1)/2} \\\\exp(-\\\\frac{1}{2} \\\\text{tr}(AS^{-1}))}{Z_{Wi}}\\\\) where \\\\(\\\\nu\\\\) is the degrees of freedom, \\\\(S\\\\) is the scale matrix, and \\\\(Z_{Wi}\\\\) is the normalization constant.",
        "The mean and mode of the Wishart distribution \\\\(Wi(S, \\\\nu)\\\\) are given by \\\\(\\\\text{mean} = \\\\nu S\\\\) and \\\\(\\\\text{mode} = (\\\\nu - D - 1)S\\\\), respectively, where the mode only exists if \\\\(\\\\nu > D + 1\\\\).",
        "The inverse Wishart distribution, a multidimensional generalization of the inverse Gamma, is defined as \\\\(IW(\\\\Sigma|S, \\\\nu) = \\\\frac{|S|^{-\\\\nu/2} |\\\\Sigma|^{-(D+\\\\nu+1)/2} \\\\exp(-\\\\frac{1}{2} \\\\text{tr}(S\\\\Sigma^{-1}))}{Z_{IW}}\\\\), where \\\\(Z_{IW}\\\\) is the normalization constant.",
        "The mean and mode of the inverse Wishart distribution \\\\(IW(\\\\Sigma|S, \\\\nu)\\\\) are given by \\\\(\\\\text{mean} = \\\\frac{S}{\\\\nu - D - 1}\\\\) and \\\\(\\\\text{mode} = \\\\frac{S}{\\\\nu + D + 1}\\\\), respectively."
      ]
    },
    {
      "topic": "Gaussian Discriminant Analysis",
      "sub_topics": [
        "Gaussian Discriminant Analysis (GDA) is a classification technique that defines class-conditional densities using MVNs, expressed as \\\\(p(x|y = c, \\\\theta) = N(x|\\\\mu_c, \\\\Sigma_c)\\\\), where \\\\(c\\\\) represents the class label.",
        "Quadratic Discriminant Analysis (QDA) arises when the posterior over class labels is a quadratic function of \\\\(x\\\\), achieved by plugging in the definition of the Gaussian density into the posterior probability formula.",
        "Linear Discriminant Analysis (LDA) is a special case of GDA where the covariance matrices are tied or shared across classes \\\\(\\\\Sigma_c = \\\\Sigma\\\\), simplifying the posterior and resulting in linear decision boundaries.",
        "Regularized LDA involves tying the covariance matrices and performing MAP estimation of \\\\(\\\\Sigma\\\\) using an inverse Wishart prior, expressed as \\\\(\\\\Sigma = \\\\lambda \\\\text{diag}(\\\\Sigma_{MLE}) + (1 - \\\\lambda) \\\\Sigma_{MLE}\\\\), where \\\\(\\\\lambda\\\\) controls the amount of regularization.",
        "The nearest shrunken centroids classifier is a method for diagonal LDA that depends on a subset of the features and performs MAP estimation for diagonal LDA with a sparsity-promoting (Laplace) prior. It defines the class-specific feature mean \\\\(\\\\mu_{cj}\\\\) in terms of the class-independent feature mean \\\\(m_j\\\\) and a class-specific offset \\\\(\\\\Delta_{cj}\\\\), expressed as \\\\(\\\\mu_{cj} = m_j + \\\\Delta_{cj}\\\\)."
      ]
    },
    {
      "topic": "Maximum Entropy Derivation of the Gaussian",
      "sub_topics": [
        "The multivariate Gaussian distribution maximizes entropy when constrained to a specified mean and covariance, a property that justifies its widespread use due to its minimal assumptions beyond the first two moments.",
        "The differential entropy h(N(\\\\u03bc, \\\\u03a3)) of a Gaussian distribution is expressed mathematically, reflecting the distribution's dispersion and uncertainty, and is maximized compared to other distributions with the same covariance.",
        "The maximum entropy property indicates that, given a fixed covariance \\\\u03a3, the Gaussian distribution N(0, \\\\u03a3) has the highest entropy h(p) among all distributions q(x) satisfying \\\\u222b q(x)xixj dx = \\\\u03a3ij, highlighting its role as a minimally informative distribution."
      ]
    }
  ]
}