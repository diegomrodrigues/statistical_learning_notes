## 15.2.3 Efeito dos Parâmetros do Kernel em Gaussian Processes

### Introdução
Em Gaussian Processes (GPs), os parâmetros do kernel, também conhecidos como *hiperparâmetros*, desempenham um papel crucial na determinação do desempenho preditivo do modelo [^5]. Esses parâmetros, como o comprimento da escala ($l$) e a escala vertical ($\sigma_f$) no kernel exponencial quadrático (SE), controlam a forma e a flexibilidade da função aprendida [^5]. A otimização desses hiperparâmetros é, portanto, uma etapa fundamental na construção de modelos GP eficazes [^5]. Este capítulo se aprofunda na influência dos parâmetros do kernel, explorando métodos para sua estimação e os compromissos entre ajuste de dados e complexidade do modelo.

### Conceitos Fundamentais

O kernel, ou função de covariância, define a semelhança entre os pontos de dados e, portanto, a forma das funções amostradas do GP [^1]. Um kernel amplamente utilizado é o kernel exponencial quadrático (SE), também conhecido como kernel Gaussiano ou RBF (Radial Basis Function) [^3]. Em uma dimensão, o kernel SE é dado por [^3]:

$$ \kappa(x, x') = \sigma_f^2 \exp\left(-\frac{(x - x')^2}{2l^2}\right)\ $$

onde:

*   $l$ (comprimento da escala): Controla a escala horizontal sobre a qual a função varia [^3]. Um valor pequeno de $l$ permite que a função mude rapidamente, resultando em maior flexibilidade, enquanto um valor grande de $l$ força a função a variar mais lentamente, resultando em uma função mais suave [^3].
*   $\sigma_f^2$ (escala vertical): Controla a variação vertical da função [^3]. Ele determina a magnitude da covariância e, portanto, a amplitude dos valores da função.
*   $\sigma_y^2$ (variância do ruído): Leva em consideração o ruído de observação [^5]. Representa a incerteza inerente nas medições dos dados.

A escolha apropriada desses parâmetros é crucial para obter um bom desempenho preditivo [^5].

**Influência dos parâmetros do kernel:**

*   **Comprimento da escala ($l$):** Um comprimento de escala curto leva a um bom ajuste aos dados de treinamento, mas pode resultar em alta complexidade e overfitting [^5]. A função se torna muito "wiggly" e captura ruído nos dados [^5]. Por outro lado, um comprimento de escala longo leva a um ajuste ruim, mas a baixa complexidade [^5]. A função se torna muito suave e não consegue capturar os padrões subjacentes nos dados [^5].
*   **Escala vertical ($\sigma_f^2$):** Controla a magnitude da covariância entre os pontos de dados [^5]. Um valor alto de $\sigma_f^2$ indica uma forte correlação, enquanto um valor baixo indica uma correlação mais fraca.
*   **Variância do ruído ($\sigma_y^2$):** Controla a quantidade de ruído nos dados [^5]. Um valor alto de $\sigma_y^2$ indica que os dados são ruidosos, enquanto um valor baixo indica que os dados são relativamente limpos.

**Estimando os parâmetros do kernel:**

Os parâmetros do kernel são tipicamente estimados maximizando a *marginal likelihood* $p(y|X)$ [^5], que equilibra o ajuste dos dados e a complexidade do modelo [^5]. A marginal likelihood é obtida integrando a função sobre todas as funções possíveis $f$ [^7]:

$$ p(y|X) = \int p(y|f, X)p(f|X) df\ $$

onde $p(y|f, X)$ é a likelihood dos dados dado a função $f$ e $p(f|X)$ é a prior sobre as funções definida pelo GP [^7].

Maximizar diretamente a marginal likelihood pode ser computacionalmente caro, pois envolve a inversão de uma matriz de $N \times N$, que tem complexidade de tempo $O(N^3)$ [^5]. No entanto, existem várias técnicas para otimizar esse processo [^5].

**Métodos para estimar os parâmetros do kernel:**

1.  **Busca exaustiva:** Envolve pesquisar em uma grade discreta de valores para os parâmetros do kernel e selecionar os valores que maximizam a marginal likelihood [^5]. Este método é computacionalmente caro, mas pode ser útil para explorar o espaço de parâmetros [^5].
2.  ***Empirical Bayes***: Este método maximiza a marginal likelihood usando métodos de otimização contínua [^5]. Isso é mais rápido do que a busca exaustiva, pois não requer a pesquisa em uma grade discreta de valores [^5].
3.  **Gradiente Descendente:** A marginal likelihood é uma função diferenciável dos parâmetros do kernel, então podemos usar gradiente descendente para encontrar os valores que maximizam a marginal likelihood [^5]. Isso requer o cálculo da derivada da marginal likelihood com respeito aos parâmetros do kernel, o que leva tempo $O(N^2)$ por hiperparâmetro [^5].
4.  **Multiple Kernel Learning (MKL):** Em vez de otimizar os parâmetros do kernel diretamente, o MKL define o kernel como uma soma ponderada de kernels básicos, $\kappa(x, x') = \sum_j w_j \kappa_j(x, x')$, e otimiza os pesos $w_j$ [^9]. Isso pode ser útil quando temos diferentes tipos de dados que queremos combinar [^9].

**Desafios na estimativa dos parâmetros do kernel:**

A marginal likelihood pode ser não-convexa, o que significa que pode ter mínimos locais [^7]. Isso pode dificultar a busca pelos valores globais ótimos dos parâmetros do kernel. Para mitigar esse problema, é comum usar vários pontos de partida para o algoritmo de otimização ou usar um otimizador global [^7].

### Conclusão

A escolha e a otimização dos parâmetros do kernel são etapas críticas na construção de modelos GP eficazes [^5]. Os parâmetros do kernel controlam a forma e a flexibilidade da função aprendida e, portanto, afetam diretamente o desempenho preditivo do modelo [^5]. A marginal likelihood fornece uma maneira de equilibrar o ajuste dos dados e a complexidade do modelo, e vários métodos estão disponíveis para otimizar os parâmetros do kernel com base na marginal likelihood [^5]. No entanto, a não-convexidade da marginal likelihood pode apresentar desafios, e técnicas adicionais podem ser necessárias para garantir que os parâmetros do kernel sejam otimizados corretamente [^7].

### Referências

[^1]: Página 1, Parágrafo 4: "A GP assumes that p(f(x1), ..., f (xv)) is jointly Gaussian, with some mean μ(x) and covariance ∑(x) given by ∑ij = κ(xi, xj), where k is a positive definite kernel function"
[^3]: Página 3, Parágrafo 2: "Here l controls the horizontal length scale over which the function varies, and of controls the vertical variation."
[^5]: Página 5, Parágrafo 1: "The predictive performance of GPs depends exclusively on the suitability of the chosen kernel."
[^5]: Página 5, Parágrafo 2: "Here l is the horizontal scale over which the function changes, o controls the vertical scale of the function, and of is the noise variance. Figure 15.3 illustrates the effects of changing these parameters."
[^5]: Página 5, Parágrafo 2: "In Figure 15.3(b), we reduce the length scale to l = 0.3 (the other parameters were optimized by maximum (marginal) likelihood, a technique we discuss below); now the function looks more “wiggly”. Also, the uncertainty goes up faster, since the effective distance from the training points increases more rapidly."
[^5]: Página 5, Parágrafo 2: "In Figure 15.3(c), we increase the length scale to l = 3; now the function looks smoother."
[^7]: Página 7, Parágrafo 1: "To estimate the kernel parameters, we could use exhaustive search over a discrete grid of values, with validation loss as an objective, but this can be quite slow."
[^7]: Página 7, Parágrafo 2: "Since p(f|X) = N(f|0, K), and p(y|f) = Π¿N(yi|fi, o), the marginal likelihood is given by log p(y|X) = log(y|0, K₁₄) = -1/2 y Kly - 1/2 log |K| - N/2 log(2π)"
[^9]: Página 10, Parágrafo 1: "A quite different approach to optimizing kernel parameters known as multiple kernel learning. The idea is to define the kernel as a weighted sum of base kernels, κ(x, x') = ∑jWjkj (x, x'), and then to optimize the weights w; instead of the kernel parameters themselves."
<!-- END -->