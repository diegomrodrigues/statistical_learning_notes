## Gaussian Processes com Observações Ruidosas

### Introdução
Em continuidade ao conceito de **Gaussian Processes (GPs)** apresentado anteriormente [^1, 2], este capítulo aprofunda o tratamento de dados ruidosos em cenários de regressão. Em aplicações do mundo real, é comum que as observações sejam corrompidas por ruído, o que exige que os GPs aproximem a função subjacente sem interpolar os dados perfeitamente [^1, 4]. Este capítulo detalha como incorporar a variância do ruído no modelo GP, ajustando as fórmulas para a média e a covariância preditivas posteriores.

### Conceitos Fundamentais

#### Modelagem do Ruído
Considere um cenário onde as observações $y$ são dadas por $y = f(x) + \epsilon$, onde $f(x)$ é a função subjacente e $\epsilon$ representa o ruído. Assume-se que o ruído segue uma distribuição normal com média zero e variância $\sigma_y^2$, ou seja, $\epsilon \sim N(0, \sigma_y^2)$ [^4].

#### Covariância das Respostas Observadas
A covariância das respostas observadas $y$ dado o conjunto de dados $X$ é expressa como:

$$\
cov[y|X] = K + \sigma_y^2 I
$$

onde $K$ é a matriz de covariância obtida através da função kernel $k(x_i, x_j)$ avaliada nos pontos de treinamento, e $I$ é a matriz identidade [^4]. O termo $\sigma_y^2 I$ adiciona a variância do ruído à diagonal da matriz de covariância, refletindo a incerteza nas observações.

#### Densidade Preditiva Posterior
A densidade preditiva posterior $p(f_*|X_*, X, y)$ para um novo ponto $x_*$ é uma distribuição normal dada por:

$$\
p(f_*|X_*, X, y) = N(f_*|\mu_*, \Sigma_*)
$$

onde $\mu_*$ e $\Sigma_*$ são a média e a covariância preditivas posteriores, respectivamente. Estas fórmulas são ajustadas para levar em conta a variância do ruído [^4]. De acordo com [^4], as equações para $\mu_*$ e $\Sigma_*$ são dadas por:

$$\
\mu_* = K_{*y}K_y^{-1}y
$$

$$\
\Sigma_* = k_{**} - K_{*y}K_y^{-1}K_{y*}
$$

onde $K_{y}$ é a covariância de $y$, $K_{*y}$ é a covariância entre os pontos de teste e os pontos de treinamento, e $k_{**}$ é a covariância nos pontos de teste [^4].

#### Média Preditiva Posterior como Soma Ponderada de Kernels
A média preditiva posterior $f_*$ pode ser expressa como uma soma ponderada de funções kernel avaliadas nos pontos de treinamento:

$$\
f_* = \sum_{i=1}^N \alpha_i k(x_i, x_*)
$$

onde $\alpha = K_y^{-1} y$ [^4]. Esta expressão destaca que a previsão é uma combinação linear de funções kernel centradas nos pontos de treinamento, com pesos ajustados pelos dados observados e pelo ruído. Os coeficientes $\alpha_i$ refletem a influência de cada ponto de treinamento na previsão em $x_*$.

### Conclusão

A modelagem de observações ruidosas em GPs é crucial para aplicações práticas. Ao incorporar a variância do ruído na matriz de covariância e ajustar as fórmulas para a média e a covariância preditivas posteriores, os GPs conseguem fornecer previsões robustas e calibradas, mesmo em presença de incerteza nos dados [^4]. A expressão da média preditiva posterior como uma soma ponderada de kernels oferece uma interpretação clara de como os GPs fazem previsões, combinando informações dos dados observados com a suavidade imposta pela função kernel [^4].

### Referências
[^1]: Rasmussen, C. E., & Williams, C. K. I. (2006). *Gaussian Processes for Machine Learning*. MIT Press.
[^2]: Williams, C. K. I., & Rasmussen, C. E. (2006). Gaussian processes for machine learning. *The MIT Press*.
[^4]: Página 518 do texto fornecido.

<!-- END -->