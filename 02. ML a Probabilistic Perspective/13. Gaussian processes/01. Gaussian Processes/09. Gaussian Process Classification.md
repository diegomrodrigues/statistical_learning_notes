## Gaussian Processes para Modelos Lineares Generalizados (GLMs)
### Introdução
Este capítulo estende a aplicação de **Gaussian Processes (GPs)** para o cenário de **Modelos Lineares Generalizados (GLMs)**, com um foco particular na classificação [^1]. Como a *prior* Gaussiana não é conjugada à *likelihood* de Bernoulli/Multinoulli, aproximações são necessárias [^1]. Abordaremos as aproximações comuns, incluindo a aproximação Gaussiana, expectation propagation, métodos variacionais e MCMC, com ênfase na aproximação Gaussiana, que é a mais simples e rápida [^1].

### Conceitos Fundamentais
#### Classificação Binária com GPs
Em problemas de classificação binária, o modelo é definido como:
$$
p(y_i|x_i) = \sigma(y_i f(x_i))
$$
onde $\sigma(z)$ é a função sigmoide (logistic regression) ou probit [^1]. A função $f$ é modelada como um processo Gaussiano com média zero e kernel $\kappa$, ou seja, $f \sim GP(0, \kappa)$ [^1].

#### Computando a Posterior
O cálculo da *posterior* em GPs para classificação envolve definir o *log posterior* não normalizado $l(f)$ [^1]:

$$
l(f) = \log p(y|f) + \log p(f|X)
$$

e usar métodos iterativos, como *iteratively reweighted least squares (IRLS)*, para encontrar a estimativa *maximum a posteriori (MAP)* [^1]. A aproximação Gaussiana da *posterior* é então dada por [^1]:
$$
p(f|X, y) \approx N(\hat{f}, (K^{-1} + W)^{-1})
$$

onde $\hat{f}$ é a estimativa MAP e $W$ é uma matriz diagonal das segundas derivadas do *log likelihood* [^1].

Para garantir a estabilidade numérica, define-se $B = I_N + WKW$, que possui autovalores limitados e pode ser invertida com segurança [^1, 15.52]. A matriz $B$ tem autovalores limitados inferiormente por 1 e superiormente por $1 + \sum_{ij} max_{ij} K_{ij}$, assegurando sua invertibilidade [^1, 15.52].

#### Classificação Multi-classe com GPs
A extensão de GPs para classificação multi-classe envolve o uso de uma função latente por classe e a aproximação da *posterior* com uma distribuição Gaussiana [^1].

#### GPs para Regressão de Poisson
GPs podem ser aplicados à regressão de Poisson para *spatial disease mapping*, modelando o risco relativo de eventos em diferentes regiões [^1]. Os dados são modelados como $Y_i \sim Poi(e_i r_i)$, onde $e_i$ é o número esperado de eventos e $r_i$ é o risco relativo, com $f = \log(r) \sim GP(0, \kappa)$ regularizando o problema [^1].

#### Numerically Stable Computation
Para implementar as equações de forma numericamente estável, é melhor evitar a inversão de K ou W [^1, 15.3.1.4]. Rasmussen and Williams sugerem definir $B = I_N + WKW$ [^1, 15.3.1.4, 15.52]. O *IRLS update* se torna [^1, 15.3.1.4, 15.53, 15.54]:
$$
f^{new} = (K^{-1} + W)^{-1} (Wf + \nabla \log p(y|f))
$$
$$
= K (I - WB^{-1}WK)b = K (b - W L^T \backslash (L \backslash (WKb)))
$$
onde $b = Wf + \nabla \log p(y|f)$ e $B = LL^T$ é uma decomposição de Cholesky [^1, 15.3.1.4, 15.55, 15.56].

### Conclusão
Este capítulo apresentou a extensão de GPs para o cenário de GLMs, com foco na classificação. A necessidade de aproximações devido à não conjugação da prior Gaussiana com a likelihood de Bernoulli/Multinoulli foi discutida, e a aproximação Gaussiana foi detalhada. As aplicações em classificação binária, multi-classe e regressão de Poisson foram exploradas, juntamente com técnicas para garantir a estabilidade numérica.

### Referências
[^1]: Gaussian Processes
[^15.52]: Gaussian Processes
[^15.3.1.4]: Gaussian Processes
[^15.53]: Gaussian Processes
[^15.54]: Gaussian Processes
[^15.55]: Gaussian Processes
[^15.56]: Gaussian Processes
<!-- END -->