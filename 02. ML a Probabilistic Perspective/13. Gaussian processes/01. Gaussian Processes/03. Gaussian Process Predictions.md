## Predições com Processos Gaussianos para Observações Livres de Ruído

### Introdução
Em continuidade ao estudo dos Processos Gaussianos (GPs) [^1], este capítulo se aprofunda nas predições realizadas com GPs, focando especificamente no cenário de observações livres de ruído. Como mencionado anteriormente, os GPs definem uma *prior* sobre funções que pode ser convertida em uma *posterior* após a observação de dados [^1]. Exploraremos como essa *posterior* é utilizada para fazer predições e como a ausência de ruído nas observações influencia o comportamento do modelo.

### Conceitos Fundamentais

As predições com GPs envolvem o cálculo da **distribuição preditiva *posterior***. No caso de observações sem ruído, uma característica fundamental dos GPs é que eles atuam como **interpoladores** [^3]. Isso significa que o modelo retorna os valores observados exatos nos pontos de treinamento, sem incerteza associada [^3]. Essa propriedade é crucial em cenários onde os dados são considerados precisos.

A **distribuição preditiva *posterior*** para observações sem ruído é uma **Gaussiana** [^3], definida por:

$$
p(f_*|X_*, X, f) = \mathcal{N}(f_*|\mu_*, \Sigma_*)
$$

onde:

*   $f_*$ representa os valores da função a serem preditos nos novos pontos $X_*$
*   $X$ e $f$ são os pontos de treinamento e seus respectivos valores observados
*   $\mu_*$ é o vetor de médias e $\Sigma_*$ é a matriz de covariância

As equações para calcular a média $\mu_*$ e a covariância $\Sigma_*$ são dadas por [^3]:

$$
\mu_* = \mu(X_*) + K_*(K^{-1})(f - \mu(X))
$$

$$
\Sigma_* = K_{**} - K_*(K^{-1})K_*
$$

onde:

*   $\mu(X_*)$ é a função média *a priori* avaliada nos pontos de teste $X_*$
*   $\mu(X)$ é a função média *a priori* avaliada nos pontos de treinamento $X$
*   $K_* = \kappa(X, X_*)$ é a matriz de covariância entre os pontos de treinamento $X$ e os pontos de teste $X_*$
*   $K = \kappa(X, X)$ é a matriz de covariância entre os pontos de treinamento $X$
*   $K_{**} = \kappa(X_*, X_*)$ é a matriz de covariância entre os pontos de teste $X_*$
*   $\kappa(.,.)$ é a função *kernel* utilizada [^1]

A média $\mu_*$ interpola os dados de treinamento, garantindo que a predição nos pontos de treinamento corresponda exatamente aos valores observados [^3]. A covariância $\Sigma_*$ reflete o aumento da incerteza à medida que nos afastamos dos pontos de dados observados [^3]. Essa incerteza preditiva aumenta em regiões com dados de treinamento esparsos ou inexistentes, refletindo a confiança do modelo nas regiões onde a informação é limitada [^3].

Essa propriedade de aumento da incerteza longe dos dados observados é crucial para **exploração** e **tomada de decisão** em ambientes incertos [^3]. Ao quantificar a incerteza, o modelo permite uma avaliação mais robusta dos riscos e benefícios associados a diferentes ações.

A Figura 15.2 [^3] ilustra este processo. No lado esquerdo, são mostradas amostras da *prior*, $p(f|X)$, utilizando um *kernel squared exponential* (também conhecido como *Gaussian kernel* ou *RBF kernel*). No lado direito, são mostradas amostras da *posterior*, $p(f_*|X_*, X, f)$, após condicionamento em 5 observações sem ruído. A área sombreada representa $E[f(x)] \pm 2std(f(x))$, mostrando o aumento da incerteza à medida que nos afastamos dos dados observados [^3].

### Conclusão

Em suma, a capacidade dos Processos Gaussianos de fornecer predições com incerteza calibrada, especialmente em cenários com dados precisos, os torna uma ferramenta valiosa para diversas aplicações. A propriedade de interpolação em observações sem ruído, combinada com o aumento da incerteza em regiões inexploradas, permite uma tomada de decisão mais informada e uma exploração mais eficiente em ambientes incertos.

### Referências
[^1]: Capítulo 15, Gaussian Processes
[^3]: Seção 15.2.1, Predictions using noise-free observations
<!-- END -->