## Gaussian Processes: Fundamentos e Aplicações

### Introdução
Gaussian Processes (GPs) oferecem uma abordagem bayesiana não paramétrica para modelagem de funções, onde a inferência é realizada diretamente sobre as funções, em vez de representações paramétricas [^1]. Esta característica é fundamental para capturar a incerteza do modelo e gerar saídas probabilísticas [^1]. Este capítulo explora os fundamentos teóricos dos GPs e suas aplicações em regressão, classificação e outros cenários.

### Conceitos Fundamentais

Um **Processo Gaussiano** é definido como uma coleção de variáveis aleatórias, qualquer subconjunto finito das quais possui uma distribuição Gaussiana conjunta [^1]. Um GP é completamente especificado por sua **função média** $\mu(x)$ e sua **função de covariância** (ou *kernel*) $\kappa(x_i, x_j)$ [^1]. A função média representa o valor esperado da função em um determinado ponto de entrada, enquanto o kernel quantifica a similaridade entre pares de pontos de entrada [^1].

A função de covariância $\kappa(x, x\')$ em GPs deve ser *positiva definida* [^1]. Isso garante que a matriz de covariância resultante seja positiva definida e que a distribuição Gaussiana conjunta seja bem definida [^1]. Funções de covariância comuns incluem o **kernel exponencial quadrático** (também conhecido como *kernel Gaussiano* ou *RBF*), definido como [^1]:

$$kappa(x, x\') = \sigma_f^2 \exp\left(-\frac{(x - x\')^2}{2l^2}\right)$$n\nonde $l$ controla a escala de comprimento e $\sigma_f^2$ a variação vertical [^1]. O parâmetro $l$ influencia a suavidade da função, enquanto $\sigma_f^2$ controla a amplitude da função [^3].

**Prior sobre funções:** Um GP define uma distribuição *a priori* sobre funções, o que significa que antes de observar quaisquer dados, temos uma crença sobre como a função pode se comportar [^1]. Esta *prior* é expressa como uma distribuição Gaussiana conjunta sobre os valores da função em qualquer conjunto finito de pontos [^1].
**Posterior sobre funções:** Após observar os dados, podemos atualizar nossa *prior* para obter uma distribuição *a posteriori* sobre funções [^1]. Esta *posterior* representa nossa crença atualizada sobre a função, levando em consideração as informações fornecidas pelos dados observados [^1].

**GPs para Regressão:** Na regressão, o objetivo é prever o valor de uma variável contínua *y* dado um ponto de entrada *x* [^2]. Em GPs para regressão, colocamos uma *prior* sobre a função de regressão *f*, denotada por [^2]:

$$f(x) \sim \mathcal{GP}(m(x), \kappa(x, x\'))$$n\nonde *m(x)* é a função média e *κ(x, x’)* é o kernel ou função de covariância [^2]. Normalmente, a função média é definida como *m(x) = 0*, pois o GP é flexível o suficiente para modelar a média arbitrariamente bem [^2].

**Predições com observações sem ruído:** Suponha que observamos um conjunto de treinamento *D = {(xi, fi), i = 1 : N}*, onde *fi = f(xi)* é a observação sem ruído da função avaliada em *xi* [^3]. Dado um conjunto de teste *X* de tamanho *N* × *D*, queremos prever as saídas da função *f*** [^3]. A distribuição conjunta tem a seguinte forma [^3]:

$$begin{pmatrix} \mathbf{f} \\\\ \mathbf{f_*} \end{pmatrix} \sim \mathcal{N}\left( \begin{pmatrix} \boldsymbol{\mu} \\\\ \boldsymbol{\mu_*} \end{pmatrix}, \begin{pmatrix} \mathbf{K} & \mathbf{K_*} \\\\ \mathbf{K_*^T} & \mathbf{K_{**}} \end{pmatrix} \right)$$n\nonde $\mathbf{K} = \kappa(\mathbf{X}, \mathbf{X})$ é $N \times N$, $\mathbf{K_*} = \kappa(\mathbf{X}, \mathbf{X_*})$ é $N \times N_*$, e $\mathbf{K_{**}} = \kappa(\mathbf{X_*},\mathbf{X_*})$ é $N_* \times N_*$ [^3].

**Predições com observações ruidosas:** Agora, considere o caso em que observamos uma versão ruidosa da função subjacente, $y = f(x) + \epsilon$, onde $\epsilon \sim \mathcal{N}(0, \sigma_\epsilon^2)$ [^4]. Neste caso, o modelo não é obrigado a interpolar os dados, mas deve chegar "perto" dos dados observados [^4]. A covariância das respostas ruidosas observadas é [^4]:

$$cov[y_p, y_q] = \kappa(x_p, x_q) + \sigma_\epsilon^2 \delta_{pq}$$n\nonde $\delta_{pq} = I(p = q)$ [^4]. Em outras palavras,\n$$cov[y|X] = K + \sigma_\epsilon^2 I_N = K_y$$n\n**Efeito dos parâmetros do kernel:** O desempenho preditivo dos GPs depende exclusivamente da adequação do kernel escolhido [^5]. Suponha que escolhamos o seguinte kernel exponencial quadrático (SE) para as observações ruidosas [^5]:

$$kappa_y(x_p, x_q) = \sigma_f^2 \exp\left(-\frac{1}{2l^2}(x_p - x_q)^2\right) + \sigma_\epsilon^2 \delta_{pq}$$n\nAqui, $l$ é a escala horizontal sobre a qual a função muda, $\sigma_f^2$ controla a escala vertical da função e $\sigma_\epsilon^2$ é a variância do ruído [^5].

**Estimando os parâmetros do kernel:** Para estimar os parâmetros do kernel, podemos usar uma busca exaustiva sobre uma grade discreta de valores, com a perda de validação como objetivo, mas isso pode ser bastante lento [^7]. Aqui, consideramos uma abordagem Bayesiana empírica, que nos permitirá usar métodos de otimização contínua, que são muito mais rápidos [^7]. Em particular, maximizaremos a verossimilhança marginal [^7]:

$$p(\mathbf{y}|\mathbf{X}) = \int p(\mathbf{y}|\mathbf{f}, \mathbf{X}) p(\mathbf{f}|\mathbf{X}) d\mathbf{f}$$n\n**Múltiplo aprendizado de kernel:** Uma abordagem bastante diferente para otimizar os parâmetros do kernel conhecida como *múltiplo aprendizado de kernel* [^10]. A ideia é definir o kernel como uma soma ponderada de kernels de base, $\kappa(\mathbf{x}, \mathbf{x}\') = \sum_j w_j \kappa_j(\mathbf{x}, \mathbf{x}\')$, e então otimizar os pesos $w_j$ em vez dos próprios parâmetros do kernel [^10].

### Conclusão

Os Gaussian Processes oferecem uma estrutura poderosa e flexível para modelagem de funções, permitindo a inferência bayesiana direta sobre funções e a quantificação da incerteza do modelo. Através da escolha apropriada de kernels e da otimização de hiperparâmetros, os GPs podem ser adaptados para uma ampla gama de tarefas de regressão e classificação. As limitações computacionais associadas a grandes conjuntos de dados podem ser mitigadas através de métodos de aproximação.

### Referências
[^1]: Capítulo 15, Gaussian processes, página 515
[^2]: Capítulo 15, Gaussian processes, página 516
[^3]: Capítulo 15, Gaussian processes, página 517
[^4]: Capítulo 15, Gaussian processes, página 518
[^5]: Capítulo 15, Gaussian processes, página 519
[^7]: Capítulo 15, Gaussian processes, página 521
[^10]: Capítulo 15, Gaussian processes, página 524
<!-- END -->