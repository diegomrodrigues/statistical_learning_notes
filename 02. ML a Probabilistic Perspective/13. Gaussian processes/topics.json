{
  "topics": [
    {
      "topic": "Gaussian Processes",
      "sub_topics": [
        "Gaussian Processes (GPs) define a prior over functions, enabling Bayesian inference directly over functions themselves, rather than their parametric representations, which is crucial for understanding model uncertainty and probabilistic outputs. A GP assumes that the joint distribution of function values at any finite set of points is Gaussian, characterized by a mean function \\u03bc(x) and a covariance function (kernel) \\u03ba(xi, xj), where the kernel measures the similarity between input points. The covariance function \\u03ba(x, x\") in GPs must be positive definite, ensuring that the resulting covariance matrix is positive definite and the joint Gaussian distribution is well-defined. Common choices include the squared exponential kernel (Gaussian or RBF kernel), defined as \\u03ba(x, x\") = \\u03c3f^2 exp(-(x - x\")^2 / (2l^2)), where l controls the length scale and \\u03c3f^2 the vertical variation.",
        "GP regression involves defining a GP prior over the regression function, denoted as f(x) ~ GP(m(x), \\u03ba(x, x\")), where m(x) is the mean function (often set to zero for simplicity) and \\u03ba(x, x\") is the kernel function determining the covariance between function values at different input points; the kernel must be positive definite to ensure a valid Gaussian process. For a finite set of points, this defines a joint Gaussian distribution p(f|X) = N(f|\\u03bc, K), where Kij = \\u03ba(xi, xj) and \\u03bc = (m(x1), ..., m(xN)).",
        "Predictions with GPs involve computing the posterior predictive distribution. For noise-free observations, GPs act as interpolators, returning the exact observed values at training points with no uncertainty, which is a key characteristic for scenarios where data is assumed to be precise. The posterior predictive distribution for noise-free observations is a Gaussian with a mean that interpolates the training data and a covariance that reflects increased uncertainty away from the observed data points, given by p(f*|X*, X, f) = N(f*|\\u03bc*, \\u03a3*), where \\u03bc* = \\u03bc(X*) + K*(K^-1)(f - \\u03bc(X)) and \\u03a3* = K** - K*(K^-1)K*.  The predictive uncertainty increases as we move away from the observed data, reflecting the model's confidence in regions with sparse or no training data, which is a crucial property for exploration and decision-making in uncertain environments.",
        "With noisy observations (y = f(x) + \\u03b5, where \\u03b5 ~ N(0, \\u03c3y^2)), GPs must approximate the underlying function without perfectly interpolating the data, reflecting real-world scenarios where measurements are subject to error. The covariance of the observed responses becomes cov[y|X] = K + \\u03c3y^2*I, where I is the identity matrix. The posterior predictive density is then p(f*|X*, X, y) = N(f*|\\u03bc*, \\u03a3*), with adjusted formulas for \\u03bc* and \\u03a3* that account for the noise variance. The posterior predictive mean can be expressed as f* = \\u03a3 \\u03b1i\\u03ba(xi, x*), where \\u03b1 = Ky\\u207b\\u00b9y, highlighting that the prediction is a weighted sum of kernel functions evaluated at the training points, adjusted by the observed data and noise.",
        "Kernel parameters (hyperparameters) in GPs, such as the length scale l and vertical scale \\u03c3f in the squared exponential (SE) kernel, significantly influence predictive performance; the length scale controls the horizontal scale over which the function changes, while the vertical scale controls the function's vertical variation, and the noise variance \\u03c3y\\u00b2 accounts for observation noise.  They are typically estimated by maximizing the marginal likelihood p(y|X), which balances data fit and model complexity. Estimating kernel parameters (hyperparameters) can be done via exhaustive search or empirical Bayes, where the latter maximizes the marginal likelihood using continuous optimization methods, offering faster computation. The marginal likelihood balances data fit and model complexity, where short length scales in the SE kernel lead to good fit but high complexity, and long length scales lead to poor fit but low complexity. Maximizing the log marginal likelihood involves computing its derivative with respect to the kernel parameters, which requires O(N\\u00b3) time for matrix inversion and O(N\\u00b2) time per hyperparameter for gradient computation.",
        "Multiple kernel learning (MKL) optimizes kernel parameters by defining the kernel as a weighted sum of base kernels, \\u03ba(x, x\") = \\u03a3j wj \\u03baj(x, x\"), and optimizing the weights wj instead of the kernel parameters themselves; this is useful for fusing different kinds of data and can be approached using risk-minimization and convex optimization or variational Bayes.",
        "Computational efficiency in GPs is a concern due to the O(N^3) complexity of matrix inversions. The predictive mean is given by f* = k*\\u1d40Ky\\u207b\\u00b9y, and for numerical stability, it is better to compute a Cholesky decomposition Ky = LL\\u1d40 rather than directly inverting Ky. An alternative to Cholesky decomposition is to solve the linear system Kya = y using conjugate gradients (CG), which takes O(kN\\u00b2) time for k iterations, or O(N\\u00b3) time for the exact solution when k = N. Approximation methods are also employed for large datasets, trading off accuracy for speed.",
        "Semi-parametric GPs combine a linear model for the mean of the process with a GP to model the residuals, expressed as f(x) = \\u03b2\\u1d40\\u03c6(x) + r(x), where r(x) ~ GP(0, \\u03ba(x, x\")). Assuming a prior on \\u03b2 (\\u03b2 ~ N(b, B)), we can integrate out these parameters to get a new GP with covariance function \\u03ba(x, x\") + \\u03c6(x)\\u1d40B\\u03c6(x\"), offering a flexible approach to modeling complex functions.",
        "GPs can be extended to the GLM setting, particularly for classification, but the Gaussian prior is not conjugate to the Bernoulli/multinoulli likelihood, requiring approximations. Common approximations include Gaussian approximation, expectation propagation, variational methods, and MCMC, with Gaussian approximation being the simplest and fastest. In binary classification, the model is defined as p(yi|xi) = \\u03c3(yif(xi)), where \\u03c3(z) is the sigmoid or probit function, and f ~ GP(0, \\u03ba). Computing the posterior in GP classification involves defining the unnormalized log posterior l(f) and using iterative methods like iteratively reweighted least squares (IRLS) to find the maximum a posteriori (MAP) estimate; the Gaussian approximation to the posterior is then given by p(f|X, y) \\u2248 N(f, (K\\u207b\\u00b9 + W)\\u207b\\u00b9), where W is a diagonal matrix of second derivatives of the log likelihood. Numerically stable computation involves defining B = IN + WKW, which has bounded eigenvalues and can be inverted safely. Multi-class classification extends GPs to cases with multiple classes, using one latent function per class and approximating the posterior with a Gaussian distribution. GPs can be applied to Poisson regression for spatial disease mapping, modeling the relative risk of events in different regions. The data is modeled as Yi ~ Poi(eiri), where ei is the expected number of events and ri is the relative risk, with f = log(r) ~ GP(0, \\u03ba) regularizing the problem.",
        "Bayesian linear regression with D-dimensional features and a prior p(w) = N(0, \\u03a3) is closely related to GPs. Bayesian linear regression is equivalent to a GP with covariance function \\u03ba(x, x\") = x\\u1d40\\u03a3x\", but this is a degenerate covariance function with at most D non-zero eigenvalues, which can lead to underfitting or overconfidence.",
        "A linear smoother is a regression function that is a linear function of the training outputs, f(x*) = \\u03a3 wi(x*)yi, where wi(x*) is the weight function. GP regression is a linear smoother, with the mean of the posterior predictive distribution given by f(x*) = \\u03a3 yiwi(x*), where wi(x*) = [(K + \\u03c3y\\u00b2IN)\\u207b\\u00b9k*]i. The effective degrees of freedom of a linear smoother is defined as dof = tr(K(K + \\u03c3y\\u00b2I)\\u207b\\u00b9) = \\u03a3 \\u03bbi/(\\u03bbi + \\u03c3y\\u00b2), which specifies how 'wiggly' the curve is.",
        "The SVM objective for binary classification is J(w) = (1/2)||w||\\u00b2 + C\\u03a3(1 - yifi)+, which can be rewritten in terms of kernel functions as J(f) = (1/2)f\\u1d40K\\u207b\\u00b9f + C\\u03a3(1 - yifi)+. Comparing this to MAP estimation for GP classifiers, J(f) = (1/2)f\\u1d40K\\u207b\\u00b9f - \\u03a3 log p(yi|fi), reveals similarities, but the hinge loss in SVMs leads to sparse solutions.",
        "Sparse kernel machines are linear models with basis function expansion of the form \\u03c6(x) = [\\u03ba(x, x\\u2081), ..., \\u03ba(x, xN)], equivalent to a GP with kernel \\u03ba(x, x\") = \\u03a3 \\u03c6j(x)\\u03c6j(x\").",
        "In binary classification, a neural network is defined by a logistic regression model applied to a logistic regression model: p(y|x, \\u03b8) = Ber(y|sigm(w\\u1d40sigm(Vx))). As the number of hidden units H approaches infinity, under certain conditions, a neural network converges to a Gaussian process, providing a connection between these two classes of models.",
        "The GP-LVM combines kernels with probabilistic PCA for dimensionality reduction. The method involves maximizing the likelihood of the data given a latent variable Z, where Z is mapped to the observed data Y through a Gaussian process. In the dual problem of GP-LVM, a prior is placed on the weights, and the objective is to maximize the likelihood with respect to the latent variables; this results in a kernel-based formulation that can be solved using eigenvalue methods. GP-LVM learns a kernelized mapping from the latent space to the observed space, whereas kernel PCA learns a mapping from the observed space to the latent space; GP-LVM often produces better embeddings and classification results compared to kernel PCA.",
        "The primary limitation of GPs is the O(N\\u00b3) time complexity, leading to the development of approximation methods that reduce computational costs. These methods typically aim to reduce the computational complexity to O(M\\u00b2N), where M is a user-specified parameter."
      ]
    },
    {
      "topic": "Kernel Parameters",
      "sub_topics": [
        "The Squared Exponential (SE) kernel, also known as the Gaussian or RBF kernel, is commonly used in GPs and is defined by a length scale parameter (l) and a vertical scale parameter (\\u03c3f), which control the smoothness and amplitude of the function, respectively. The length scale (l) determines the horizontal scale over which the function varies; smaller values lead to wigglier functions that fit the data more closely, while larger values result in smoother functions that capture broader trends. The vertical scale (\\u03c3f) controls the vertical variation or amplitude of the function, influencing the model's sensitivity to changes in the input data and affecting the overall scale of the predicted outputs. The noise variance (\\u03c3y) represents the amount of noise in the observations and affects the model's confidence in the training data; higher noise variance leads to smoother predictions, while lower values allow the model to fit the data more closely. The predictive performance of GPs depends critically on the suitability of the chosen kernel and its parameters; selecting appropriate values requires careful consideration of the underlying data characteristics and the desired model behavior."
      ]
    }
  ]
}