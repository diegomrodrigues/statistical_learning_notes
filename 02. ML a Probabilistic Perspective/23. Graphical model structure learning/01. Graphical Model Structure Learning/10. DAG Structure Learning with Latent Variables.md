## Aprendizagem da Estrutura de DAGs com Variáveis Latentes

### Introdução
A aprendizagem da estrutura de modelos gráficos, especificamente DAGs (Directed Acyclic Graphs), é um problema fundamental em diversas áreas, como descoberta de conhecimento e estimativa de densidade [^26.1]. No entanto, a suposição de dados completos nem sempre se sustenta, especialmente devido a dados faltantes ou variáveis latentes. Nesses casos, o cálculo da verossimilhança marginal envolve integrar ou somar sobre as variáveis ocultas, um processo geralmente intratável que requer métodos de aproximação [^26.5]. Este capítulo explora métodos para lidar com a aprendizagem da estrutura de DAGs na presença de variáveis latentes, complementando os métodos de aprendizagem de estrutura já abordados [^26.4].

### Conceitos Fundamentais
Quando os dados estão incompletos ou variáveis latentes estão presentes, a verossimilhança marginal $p(D|G)$ é dada por [^26.5]:
$$\np(D|G) = \int \sum_h p(D, h|\theta, G)p(\theta|G) d\theta\n$$
onde $h$ representa os dados ocultos ou faltantes. A intratabilidade desta integral e soma exige o uso de métodos de aproximação.

#### Aproximação de Cheeseman-Stutz (CS)
A aproximação de **Cheeseman-Stutz (CS)** [^26.5.1.2] é um método para aproximar a verossimilhança marginal na presença de variáveis latentes. Envolve os seguintes passos:
1. **Estimação MAP:** Calcula-se uma estimativa MAP (Maximum A Posteriori) dos parâmetros $\theta$ usando o algoritmo EM (Expectation-Maximization).
2. **Preenchimento de Variáveis Ocultas:** As variáveis ocultas são preenchidas com seus valores esperados, obtendo-se dados "completos" $\overline{D} = D(\hat{\theta})$.
3. **Verossimilhança Marginal Exata:** A equação da verossimilhança marginal exata é aplicada a esses dados preenchidos.
4. **Correção:** Uma correção é aplicada para compensar o valor exponencialmente menor devido a não somar sobre todos os valores de $h$.

A verossimilhança marginal aproximada é então:
$$\nlog \, p(D|G) \approx log \, p(\overline{D}|G) + log \, p(D|\hat{\theta}, G) - log \, p(\overline{D}|\hat{\theta}, G)\n$$
Onde o primeiro termo $p(\overline{D}|G)$ pode ser calculado usando a verossimilhança marginal exata nos dados preenchidos, o segundo termo $p(D|\hat{\theta}, G)$ envolve uma soma exponencial (resolvida por um algoritmo de inferência) e o terceiro termo $p(\overline{D}|\hat{\theta}, G)$ pode ser computado inserindo os dados preenchidos na verossimilhança regular [^26.5.1.2].

#### Variational Bayes EM
O **Variational Bayes EM** [^26.5.1.3] utiliza suposições de fatoração para aproximar a distribuição posterior, fornecendo um limite inferior para o log da verossimilhança marginal e uma aproximação mais precisa do que o BIC (Bayesian Information Criterion) ou Cheeseman-Stutz. A ideia chave é assumir a seguinte fatoração [^26.5.1.3]:
$$\np(\theta, z_{1:N}|D) \approx q(\theta)q(z) = q(\theta) \prod_i q(z_i)\n$$
onde $z_i$ são as variáveis latentes no caso $i$. No passo E, atualizamos $q(z_i)$ e, no passo M, atualizamos $q(\theta)$. A energia livre variacional correspondente fornece um limite inferior para o log da verossimilhança marginal [^26.5.1.3].

#### Structural EM
O **Structural EM** [^26.5.2] é um algoritmo mais eficiente que preenche os dados usando um grafo vizinho candidato e seus parâmetros e, em seguida, avalia a pontuação de todos os vizinhos usando os dados preenchidos. Isso aproxima a diferença nas verossimilhanças marginais, tornando a seleção do modelo mais eficiente.
1. Inicialize com um grafo $G_0$ e parâmetros $\theta_0$.
2. Preencha os dados $D(\hat{G_0}, \hat{\theta_0})$ usando o modelo $G_0$ com parâmetros MAP $\theta_0$.
3. Avalie a pontuação BIC modificada [^26.5.2]:
$$\nscore_{BIC}(G, D) \approx log \, p(D|\hat{\theta}, G) - \frac{log \, N}{2}dim(G) + log \, p(G) + log \, p(\theta|G)\n$$
para todos os vizinhos de $G_0$ usando os dados preenchidos $D$.
4. Escolha o melhor vizinho e repita.

### Conclusão

A aprendizagem da estrutura de DAGs com variáveis latentes é um problema desafiador que requer métodos de aproximação para lidar com a intratabilidade da verossimilhança marginal. As aproximações de Cheeseman-Stutz, Variational Bayes EM e Structural EM oferecem diferentes abordagens para aproximar a verossimilhança marginal e realizar a seleção do modelo de forma eficiente. A escolha do método apropriado depende das características específicas do problema e dos recursos computacionais disponíveis.

### Referências
[^26.1]: Introduction, Graphical model structure learning
[^26.4]: Learning DAG structures, Graphical model structure learning
[^26.5]: Learning DAG structure with latent variables, Graphical model structure learning
[^26.5.1.2]: Cheeseman-Stutz approximation, Learning DAG structure with latent variables
[^26.5.1.3]: Variational Bayes EM, Learning DAG structure with latent variables
[^26.5.2]: Structural EM, Learning DAG structure with latent variables
<!-- END -->