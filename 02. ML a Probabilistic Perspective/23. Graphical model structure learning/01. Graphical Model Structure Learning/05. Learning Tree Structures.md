## Aprendizado de Estruturas de Árvore em Modelos Gráficos

### Introdução
Este capítulo explora o aprendizado de estruturas de árvore no contexto do aprendizado de estruturas de modelos gráficos [^1]. O aprendizado de modelos gráficos envolve a descoberta de relações de dependência condicional entre variáveis, o que é fundamental para tarefas como estimativa de densidade, predição e descoberta de conhecimento [^1]. A complexidade inerente ao aprendizado de estruturas de grafos em geral, um problema NP-difícil [^4], motiva a consideração de árvores como um caso especial, dada a sua eficiente capacidade de aprendizado e adequação para inferência exata [^4]. Este capítulo se aprofundará nos aspectos teóricos e práticos do aprendizado de estruturas de árvore, explorando representações direcionadas e não direcionadas, o algoritmo de Chow-Liu e a busca por florestas MAP.

### Conceitos Fundamentais

**Representações de Árvore:** Árvores podem ser representadas como grafos direcionados ou não direcionados, mantendo a mesma complexidade de parâmetros [^4].
- **Grafos Direcionados:** Em uma árvore direcionada, a distribuição conjunta é definida como o produto das probabilidades condicionais de cada nó dado seu pai [^5]:
$$p(x|T) = \prod_{t \in V} p(x_t | x_{pa(t)})$$
onde $V$ é o conjunto de nós e $pa(t)$ denota o pai do nó $t$. O nó raiz $r$ não possui pai, então $pa(r) = \emptyset$ [^5].
- **Grafos Não Direcionados:** Em uma árvore não direcionada, a distribuição conjunta é expressa em termos de potenciais de aresta e nó [^5]:
$$p(x|T) = \prod_{t \in V} p(x_t) \prod_{(s, t) \in E} \frac{p(x_s, x_t)}{p(x_s)p(x_t)}$$
onde $E$ é o conjunto de arestas. Essa representação é mais simétrica e útil para o aprendizado da estrutura [^5].

**Equivalência entre Representações:** Uma árvore pode ser representada tanto como um grafo direcionado quanto como um grafo não direcionado sem alterar o número de parâmetros ou a capacidade de inferência [^5]. A escolha entre as representações depende da tarefa. A representação não direcionada é simétrica e útil para o aprendizado da estrutura, enquanto a representação direcionada é mais conveniente para o aprendizado de parâmetros [^5].

**Algoritmo de Chow-Liu:** Este algoritmo é usado para encontrar a estrutura de árvore de máxima verossimilhança (ML) [^6]. O algoritmo se baseia na ideia de encontrar uma árvore geradora máxima ponderada, onde os pesos das arestas são a informação mútua entre os pares de variáveis [^6].
1. **Cálculo da Informação Mútua:** A informação mútua entre as variáveis $x_s$ e $x_t$ é calculada usando a distribuição empírica [^6]:
$$I(x_s, x_t|\Theta_{st}) = \sum_j \sum_k p_{emp}(x_s = j, x_t = k) \log \frac{p_{emp}(x_s = j, x_t = k)}{p_{emp}(x_s = j)p_{emp}(x_t = k)}$$
2. **Construção da Árvore Geradora Máxima:** A árvore é construída usando algoritmos como o de Prim ou Kruskal, que têm complexidade de tempo $O(E \log V)$, onde $E$ é o número de arestas e $V$ é o número de nós [^6].

**Aprendizado de Florestas MAP:** Em vez de aprender uma única árvore, pode ser desejável aprender uma floresta de árvores, especialmente quando a inferência em uma floresta é mais rápida do que em uma única árvore complexa [^6]. Enquanto o critério de máxima verossimilhança sempre escolherá adicionar mais arestas, o uso de uma verossimilhança marginal ou uma verossimilhança penalizada (como BIC) pode levar a uma solução que é uma floresta [^6].

### Conclusão
O aprendizado de estruturas de árvore oferece uma abordagem eficiente para modelar dependências entre variáveis, com aplicações em diversas áreas. A escolha entre representações direcionadas e não direcionadas, juntamente com algoritmos como o de Chow-Liu e a busca por florestas MAP, fornece um conjunto de ferramentas flexíveis para a descoberta de conhecimento e a estimativa de densidade [^1]. Embora as árvores representem um caso especial de modelos gráficos, sua eficiência e interpretabilidade as tornam uma escolha valiosa em muitas aplicações práticas. <!-- END -->