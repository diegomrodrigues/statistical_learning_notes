## Aprendizado de Estrutura de Modelos Gráficos

### Introdução
Este capítulo aborda o aprendizado da estrutura de modelos gráficos, um tópico fundamental em *machine learning* para expressar suposições de independência condicional entre variáveis [^1]. O objetivo principal é computar a estrutura do grafo, representada por uma matriz de adjacência denotada como $p(G|D)$, onde $G$ representa a estrutura do grafo e $D$ o conjunto de dados [^1]. A dificuldade central reside no fato de que o número de possíveis grafos cresce exponencialmente com o número de nós, tornando a computação da distribuição *posterior* completa $p(G|D)$ inviável [^1].

### Conceitos Fundamentais
O aprendizado de estrutura tem duas aplicações primárias:
1.  **Descoberta de conhecimento:** Identificar a topologia do grafo [^1].
2.  **Estimativa de densidade:** Requer um modelo completamente especificado para estimar a distribuição de probabilidade sobre as variáveis [^1].

Para a descoberta de conhecimento, as *posterior edge marginals* $p(G_{st} = 1|D)$ podem ser computadas para gerar grafos esparsos para visualização [^1]. Já para a estimativa de densidade, busca-se o grafo MAP (Maximum a Posteriori) $\hat{G} \in \underset{G}{\operatorname{argmax}} p(G|D)$ [^1].

#### Abordagens para Descoberta de Conhecimento
Dado que a computação do grafo MAP ou das *posterior edge marginals* é computacionalmente intratável [^2], algumas abordagens heurísticas são utilizadas para visualizar os dados:
*   **Relevance Networks:** Visualização da informação mútua *pairwise* entre variáveis. Um limiar é escolhido, e uma aresta é desenhada entre o nó *i* e o nó *j* se $I(X_i; X_j)$ estiver acima desse limiar [^2]. No caso Gaussiano, $I(X_i; X_j) = -\frac{1}{2} \log(1 - \rho_{ij}^2)$, onde $\rho_{ij}$ é o coeficiente de correlação [^2]. Essa abordagem é popular em biologia de sistemas [^2].
*   **Dependency Networks:** Aprendizado de uma estrutura de modelo gráfico ao ajustar independentemente $D$ distribuições condicionais completas esparsas $p(x_t|x_{-t})$ [^3]. As variáveis escolhidas constituem as entradas para o nó, ou seja, seu *Markov blanket* [^3].

#### Chow-Liu Algorithm
Para encontrar a estrutura de árvore de máxima verossimilhança (ML), podemos utilizar o algoritmo de Chow-Liu [^6]. A função de *log-likelihood* para uma árvore pode ser escrita como:

$$\
\log p(D|\theta, T) = \sum_t \sum_k N_{tk} \log p(x_t = k|\theta) + \sum_{s,t} \sum_{j,k} N_{stjk} \log \frac{p(x_s = j, x_t = k|\theta)}{p(x_s = j|\theta)p(x_t = k|\theta)}\
$$

Onde $N_{stjk}$ é o número de vezes que o nó $s$ está no estado $j$ e o nó $t$ está no estado $k$, e $N_{tk}$ é o número de vezes que o nó $t$ está no estado $k$ [^6]. Definindo $\theta$ como os estimadores de máxima verossimilhança (MLEs), a equação se torna:

$$\
\frac{\log p(D|\theta, T)}{N} = -\sum_{t \in V} \sum_k p_{emp}(x_t = k) \log p_{emp}(x_t = k) + \sum_{(s,t) \in E(T)} I(x_s, x_t|\theta_{st})\
$$

Onde $I(x_s, x_t|\theta_{st})$ é a informação mútua entre $x_s$ e $x_t$ dada a distribuição empírica [^6].

#### Aprendizado de DAGs

O aprendizado de estruturas DAG envolve computar (funções de) $p(G|D)$, onde $G$ é restrito a ser um DAG. Isso é frequentemente chamado de aprendizado de estrutura de rede Bayesiana [^8]. Assumindo dados completos e variáveis categóricas, a verossimilhança pode ser escrita como:

$$\
p(D|G, \theta) = \prod_{i=1}^N \prod_{t=1}^V Cat(x_{it} | x_{i, pa(t)}, \theta_t) = \prod_{t=1}^V \prod_{c=1}^{C_t} \prod_{k=1}^{K_t} \theta_{tck}^{ \sum_{i=1}^N \mathbb{I}(x_{it} = k, x_{i, pa(t)} = c)} = \prod_{t=1}^V \prod_{c=1}^{C_t} \prod_{k=1}^{K_t} \theta_{tck}^{N_{tck}}\
$$

Para evitar *overfitting*, escolhemos o grafo com a máxima verossimilhança marginal, $p(D|G)$ [^10]. Para computar a verossimilhança marginal, precisamos especificar *priors* nos parâmetros. Assumimos independência global e local dos parâmetros *prior*, o que implica que o *prior* para cada linha de cada CPT deve ser um Dirichlet [^11].

### Conclusão
O aprendizado de estrutura de modelos gráficos é um campo rico e desafiador, com aplicações em diversas áreas. A escolha da abordagem depende do objetivo (descoberta de conhecimento ou estimativa de densidade), da natureza dos dados (contínuos ou discretos) e das restrições computacionais. As técnicas discutidas neste capítulo fornecem uma base sólida para explorar e aplicar modelos gráficos em problemas complexos de *machine learning*.

### Referências
[^1]: Section 26.1
[^2]: Section 26.2
[^3]: Section 26.2.2
[^6]: Section 26.3.2
[^8]: Section 26.4
[^10]: Section 26.4.2.2
[^11]: Section 26.4.2.2
<!-- END -->