## Aprendizado de Estruturas DAG

### Introdução
Este capítulo explora o aprendizado de estruturas de **Directed Acyclic Graphs (DAGs)**, um tópico central em *Graphical Model Structure Learning* [^1]. O objetivo é computar $p(G|D)$, a probabilidade da estrutura do grafo $G$ dados os dados $D$, sob a restrição de que $G$ seja um DAG [^1]. Este processo é frequentemente referido como *Bayesian network structure learning* [^1]. Assumimos que não há dados faltantes ou variáveis ocultas (a *complete data assumption*) [^1], e para simplificar, as variáveis são tratadas como categóricas com *tabular CPDs* [^1], embora os resultados possam ser generalizados para dados reais e outros tipos de *CPDs*, como *linear-Gaussian CPDs* [^1].

### Conceitos Fundamentais

Um conceito crucial é o de **Markov equivalence** [^1], que define quando diferentes estruturas DAG codificam o mesmo conjunto de *conditional independence (CI)* assumptions [^1]. Duas estruturas são *Markov equivalent* se possuem o mesmo *undirected skeleton* e o mesmo conjunto de *v-structures* [^1]. Isso leva ao conceito de um *partially directed acyclic graph (PDAG)* ou *essential graph* [^1], que representa uma classe de equivalência de Markov com arestas direcionadas (*compelled*) e não direcionadas (*reversible*) [^1].

A principal dificuldade no aprendizado de estruturas é que o número de grafos possíveis cresce exponencialmente com o número de nós [^1]. Uma delimitação superior simples para o número de grafos é $O(2^{V(V-1)/2})$, onde $V$ é o número de nós [^1]. Assim, a posterior completa $p(G|D)$ é proibitivamente grande [^1]. Em vez de tentar computar a posterior completa, buscamos resumos apropriados da posterior, que dependem da tarefa em questão [^1].

Se o objetivo é *knowledge discovery*, podemos querer computar *posterior edge marginals*, $p(G_{st} = 1|D)$ [^1]. Podemos então plotar o grafo correspondente, onde a espessura de cada aresta representa nossa confiança em sua presença [^1]. Definindo um limiar, podemos gerar um grafo esparso, útil para visualização [^1].

Se o objetivo é *density estimation*, podemos querer computar o *MAP graph*, $\hat{G} \in \text{argmax}_G p(G|D)$ [^1]. Na maioria dos casos, encontrar o grafo globalmente ótimo leva tempo exponencial, então usamos métodos de otimização discreta, como busca heurística [^1]. No entanto, no caso de árvores, podemos encontrar a estrutura do grafo globalmente ótimo de forma eficiente usando métodos exatos [^1].

É importante considerar se aprender um *latent variable model* seria mais apropriado se o objetivo for apenas *density estimation* [^1]. Esses modelos podem capturar a correlação entre as variáveis visíveis através de um conjunto de causas comuns latentes [^1]. Tais modelos são frequentemente mais fáceis de aprender e podem ser aplicados de forma mais eficiente para fins de previsão, pois não exigem realizar inferência em um grafo aprendido com treewidth potencialmente alta [^1]. A desvantagem é que os fatores latentes são frequentemente não identificáveis e, portanto, difíceis de interpretar [^1]. É possível combinar o aprendizado de estrutura de modelo gráfico e o aprendizado de variável latente [^1].

Em alguns casos, modelamos a estrutura causal por trás dos dados para prever os efeitos da manipulação de variáveis [^1]. Esta é uma tarefa muito mais desafiadora [^1].

#### Markov Equivalence Detalhada
A *Markov equivalence* é um conceito fundamental que limita nossa capacidade de aprender estruturas DAG a partir de dados [^8]. Grafos *Markov equivalent* codificam o mesmo conjunto de *conditional independence (CI)* assumptions [^9]. Formalmente, dois DAGs $G_1$ e $G_2$ são *Markov equivalent* se e somente se eles possuem o mesmo *undirected skeleton* e o mesmo conjunto de *v-structures* [^9].

**Definição**: Um *v-structure* em um DAG é um padrão da forma $X \rightarrow Y \leftarrow Z$ onde não existe uma aresta entre $X$ e $Z$ [^9].

**Teorema 26.4.1 (Verma and Pearl 1990)**: Duas estruturas são *Markov equivalent* se e somente se elas têm o mesmo *undirected skeleton* e o mesmo conjunto de *v-structures* [^9].

A importância deste teorema é que, ao aprender a estrutura DAG a partir de dados, não seremos capazes de identificar univocamente todas as direções das arestas, mesmo com uma quantidade infinita de dados [^9]. Podemos aprender a estrutura DAG "até a *Markov equivalence*". Isso também nos adverte para não atribuirmos muito significado à orientação particular das arestas, uma vez que muitas vezes podemos alterá-las sem alterar o modelo de forma observável [^9].

Para representar uma classe de equivalência de Markov, usamos um *partially directed acyclic graph (PDAG)*, também chamado de *essential graph* ou *pattern* [^9]. Em um *PDAG*, algumas arestas são direcionadas e outras não direcionadas [^9]. As arestas não direcionadas representam arestas *reversible*, enquanto as arestas direcionadas representam arestas *compelled* [^9].

Por exemplo, o PDAG $X - Y - Z$ representa a classe de equivalência contendo os DAGs $X \rightarrow Y \rightarrow Z$, $X \leftarrow Y \leftarrow Z$ e $X \leftarrow Y \rightarrow Z$ [^9].

#### Learning Tree Structures
O aprendizado de estrutura para grafos gerais é *NP-hard* [^4]. No entanto, o aprendizado de estrutura para árvores pode ser feito de forma eficiente [^4]. Uma árvore é um grafo onde há um único caminho entre quaisquer dois nós [^5]. Uma árvore dirigida, com um único nó raiz $r$, define uma distribuição conjunta como [^5]:

$$\np(x|T) = \prod_{t \in V} p(x_t|x_{\text{pa}(t)})$$

onde $V$ é o conjunto de nós, $x_t$ é o valor do nó $t$, $\text{pa}(t)$ são os pais de $t$ e $\text{pa}(r) = \emptyset$ [^5].

Uma árvore pode ser representada como um grafo direcionado ou não direcionado [^5]. A representação não direcionada é mais simétrica e útil para aprendizado de estrutura, enquanto a representação direcionada é mais conveniente para aprendizado de parâmetros [^5].

Para encontrar a estrutura da árvore *maximum likelihood (ML)*, podemos usar o algoritmo **Chow-Liu** [^6]. Este algoritmo computa a informação mútua entre cada par de variáveis e, em seguida, constrói uma árvore de abrangência máxima usando essas informações mútuas como pesos de aresta [^6]. O algoritmo **Chow-Liu** tem complexidade $O(E \log V)$, onde $E$ é o número de arestas e $V$ é o número de nós [^6].

### Conclusão

Este capítulo forneceu uma visão geral abrangente do aprendizado de estruturas DAG, desde os conceitos fundamentais de *Markov equivalence* até algoritmos práticos como o algoritmo **Chow-Liu** para aprender estruturas de árvores [^1, ^6]. Também foi abordada a importância de considerar *latent variable models* para *density estimation* e as limitações do aprendizado de estrutura DAG a partir de dados observacionais [^1].

### Referências
[^1]: Capítulo 26, Seção Introdutória
[^4]: Capítulo 26, Seção 26.3
[^5]: Capítulo 26, Seção 26.3.1
[^6]: Capítulo 26, Seção 26.3.2
[^8]: Capítulo 26, Seção 26.4
[^9]: Capítulo 26, Seção 26.4.1
<!-- END -->