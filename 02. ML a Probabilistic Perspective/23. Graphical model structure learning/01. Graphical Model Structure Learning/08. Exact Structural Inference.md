## Inferência Estrutural Exata em Modelos Gráficos

### Introdução
Este capítulo se dedica ao aprendizado da estrutura de modelos gráficos, explorando técnicas para inferir a estrutura do grafo $G$ a partir dos dados $D$, denotado por $p(G|D)$ [^1]. Uma das maiores dificuldades reside no número exponencial de possíveis estruturas de grafos, tornando a computação direta da distribuição posterior impraticável [^1]. Neste contexto, abordaremos a inferência estrutural exata, que computa a distribuição posterior sobre grafos, $p(G|D)$, e as simplificações necessárias para tornar o problema tratável [^2].

### Conceitos Fundamentais
#### Derivando a Verossimilhança
Assumindo que não há dados faltantes e que todas as Conditional Probability Distributions (CPDs) são tabulares, a verossimilhança pode ser escrita como [^10]:

$$\np(D|G, \theta) = \prod_{i=1}^{N} \prod_{t=1}^{V} Cat(x_{it} | x_{i, pa(t)}, \theta_t)\n$$

onde:
*   $N$ é o número de instâncias nos dados.
*   $V$ é o número de nós no grafo.
*   $x_{it}$ é o estado do nó $t$ na instância $i$.
*   $pa(t)$ representa os pais do nó $t$.
*   $\theta_t$ são os parâmetros associados ao nó $t$.
*   $Cat$ é a distribuição categórica.

Esta equação pode ser expandida para [^10]:

$$\np(D|G, \theta) = \prod_{i=1}^{N} \prod_{t=1}^{V} \prod_{c=1}^{C_t} Cat(x_{it} | \theta_{tc})^{(x_{i, pa(t)} = c)} = \prod_{i=1}^{N} \prod_{t=1}^{V} \prod_{c=1}^{C_t} \prod_{k=1}^{K_t} \theta_{tck}^{(x_{it}=k, x_{i, pa(t)}=c)} = \prod_{t=1}^{V} \prod_{c=1}^{C_t} \prod_{k=1}^{K_t} \theta_{tck}^{N_{tck}}\n$$

onde:
*   $C_t$ é o número de combinações de estados dos pais do nó $t$.
*   $K_t$ é o número de estados do nó $t$.
*   $N_{tck}$ é o número de vezes que o nó $t$ está no estado $k$ e seus pais estão no estado $c$.

#### Derivando a Verossimilhança Marginal
A escolha do grafo com a máxima verossimilhança invariavelmente seleciona um grafo completamente conectado (sujeito à restrição de aciclicidade), pois isso maximiza o número de parâmetros [^10]. Para evitar o overfitting, escolhemos o grafo com a máxima verossimilhança marginal, $p(D|G)$ [^10]. Para calcular a verossimilhança marginal, precisamos especificar priors para os parâmetros. Fazemos duas suposições padrão [^10]:
1.  **Independência Global dos Priors dos Parâmetros**: $$p(\theta) = \prod_{t=1}^{V} p(\theta_t)$$ [^10]
2.  **Independência Local dos Priors dos Parâmetros**: $$p(\theta_t) = \prod_{c=1}^{C_t} p(\theta_{tc})$$ [^11]

Essas suposições implicam que o prior para cada linha de cada CPT deve ser um Dirichlet [^11]:

$$p(\theta_{tc}) = Dir(\theta_{tc} | \alpha_{tc})$$ [^11]

Dado essas suposições, podemos escrever a verossimilhança marginal de qualquer DAG como [^11]:

$$p(D|G) = \prod_{t=1}^{V} \prod_{c=1}^{C_t} \int \prod_{i: x_{i, pa(t)} = c} Cat(x_{it} | \theta_{tc}) Dir(\theta_{tc}) d\theta_{tc} = \prod_{t=1}^{V} \prod_{c=1}^{C_t} \frac{B(N_{tc} + \alpha_{tc})}{B(\alpha_{tc})} = \prod_{t=1}^{V} \prod_{c=1}^{C_t} \frac{\Gamma(N_{tc})}{\Gamma(N_{tc} + \alpha_{tc})} \prod_{k=1}^{K_t} \frac{\Gamma(N_{tck} + \alpha_{tck})}{\Gamma(\alpha_{tck})} = \prod_{t=1}^{V} score(N_{t, pa(t)})$$

onde:
*   $B$ é a função Beta.
*   $\Gamma$ é a função Gamma.
*   $N_{tc} = \sum_k N_{tck}$ e $\alpha_{tc} = \sum_k \alpha_{tck}$.
*   $score(N_{t, pa(t)})$ é uma função de pontuação local.

Portanto, a verossimilhança marginal se decompõe ou fatoriza de acordo com a estrutura do grafo [^11].

#### Definindo o Prior
A escolha dos hiperparâmetros $\alpha_{tck}$ é crucial. Uma opção comum é usar um prior de Jeffreys, mas isso viola a propriedade de *equivalência de verossimilhança* [^11]. Essa propriedade afirma que se $G_1$ e $G_2$ são Markov equivalentes, eles devem ter a mesma verossimilhança marginal [^11]. Geiger e Heckerman (1997) provaram que, para grafos completos, o único prior que satisfaz a equivalência de verossimilhança e a independência de parâmetros é o prior de Dirichlet, onde os pseudo-contagens têm a forma [^11]:

$$\alpha_{tck} = \alpha \cdot p_0(x_t = k, x_{pa(t)} = c)$$

onde $\alpha > 0$ é chamado de *tamanho amostral equivalente* e $p_0$ é alguma distribuição de probabilidade conjunta a priori [^11]. Isso é chamado de prior BDe (Bayesian Dirichlet equivalent) [^11].

Para derivar os hiperparâmetros para outras estruturas de grafo, Geiger e Heckerman (1997) invocaram uma suposição adicional chamada *modularidade de parâmetros*, que diz que se o nó $X_t$ tem os mesmos pais em $G_1$ e $G_2$, então $p(\theta_t|G_1) = p(\theta_t|G_2)$ [^12]. Com essa suposição, podemos sempre derivar $\alpha_t$ para um nó $t$ em qualquer outro grafo marginalizando os pseudo-contagens na Equação 26.29 [^12].

Tipicamente, a distribuição a priori $p_0$ é assumida como uniforme sobre todas as possíveis configurações conjuntas [^12]. Nesse caso, temos [^12]:

$$\alpha_{tck} = \frac{\alpha}{K_t C_t}$$

Uma vez que $p_0(x_t = k, x_{pa(t)} = c) = \frac{1}{K_t C_t}$. Assim, se somarmos os pseudo-contagens sobre todas as entradas $C_t \times K_t$ na CPT, obtemos um tamanho amostral equivalente total de $\alpha$. Isso é chamado de prior BDeu, onde "u" significa uniforme [^12]. Este é o prior mais amplamente utilizado para aprender estruturas de redes bayesianas [^12].

#### Exemplo Trabalhado Simples
Considere um exemplo com dois nós binários, $X_1$ e $X_2$, e os seguintes 8 casos de dados [^12]:

```
X1 X2
1  1
1  2
1  1
2  2
1  1
2  1
1  1
2  2
```

Estamos interessados em dois grafos possíveis: $G_1$ é $X_1 \rightarrow X_2$ e $G_2$ é o grafo desconectado [^12]. As contagens empíricas para o nó 1 em $G_1$ são $N_1 = (5, 3)$ e para o nó 2 são [^12]:

```
      X2 = 1  X2 = 2
X1 = 1  4       1
X1 = 2  1       2
```

O prior BDeu para $G_1$ é $\alpha_1 = (\alpha/2, \alpha/2), \alpha_{2|X_1=1} = (\alpha/4, \alpha/4)$ e $\alpha_{2|X_1=2} = (\alpha/4, \alpha/4)$ [^12]. Para $G_2$, o prior para $\theta_1$ é o mesmo, e para $\theta_2$ é $\alpha_{2|X_1=1} = (\alpha/2, \alpha/2)$ e $\alpha_{2|X_1=2} = (\alpha/2, \alpha/2)$ [^12]. Se definirmos $\alpha = 4$ e usarmos o prior BDeu, encontramos $p(D|G_1) = 7.2150 \times 10^{-6}$ e $p(D|G_2) = 6.7465 \times 10^{-6}$ [^12]. Portanto, as probabilidades posteriores, sob um prior de grafo uniforme, são $p(G_1|D) = 0.51678$ e $p(G_2|D) = 0.48322$ [^12].

### Conclusão
Neste capítulo, detalhamos o processo de inferência estrutural exata em modelos gráficos, focando na derivação da verossimilhança marginal e nas suposições necessárias para tornar o cálculo tratável. O exemplo trabalhado ilustra como a aplicação de priors Bayesianos pode levar a conclusões quantitativas sobre diferentes estruturas de grafos, permitindo a comparação e seleção de modelos [^12]. Os detalhes apresentados aqui servem como base para abordagens mais avançadas em aprendizado de estrutura que buscam aproximar a distribuição posterior sobre grafos em cenários mais complexos.

### Referências
[^1]: 26.1 Introduction
[^2]: 26.4.2 Exact structural inference
[^10]: 26.4.2.1 Deriving the likelihood
[^11]: 26.4.2.2 Deriving the marginal likelihood
[^12]: 26.4.2.4 Simple worked example
<!-- END -->