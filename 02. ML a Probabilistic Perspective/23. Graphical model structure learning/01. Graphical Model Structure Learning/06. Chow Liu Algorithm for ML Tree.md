## Aprendizagem da Estrutura de Árvores com o Algoritmo de Chow-Liu

### Introdução
Este capítulo aprofunda-se no aprendizado da estrutura de árvores, um caso especial de aprendizado da estrutura de modelos gráficos, explorando o algoritmo de Chow-Liu para encontrar a estrutura de árvore de máxima verossimilhança (ML). A discussão se baseia nos conceitos de modelos gráficos e aprendizado de estrutura introduzidos anteriormente [^1]. Dado que o aprendizado da estrutura para grafos gerais é NP-difícil [^4], focar em árvores oferece uma abordagem eficiente e tratável.

### Conceitos Fundamentais
O **algoritmo de Chow-Liu** [^6] é um método para encontrar a estrutura de árvore de máxima verossimilhança (ML) para um conjunto de dados. A ideia central do algoritmo é transformar o problema de aprendizado da estrutura de árvores em um problema de encontrar a árvore geradora máxima (MST) em um grafo ponderado.

1.  **Peso das Arestas:** O algoritmo atribui pesos às arestas do grafo, onde cada peso representa a *informação mútua* entre os nós conectados pela aresta. A informação mútua $I(y_s, y_t|\Theta_{st})$ [^6] quantifica a dependência estatística entre as variáveis $y_s$ e $y_t$, com $\Theta_{st}$ representando os parâmetros associados a essa relação.

2.  **Árvore Geradora Máxima (MST):** O algoritmo então procura a MST, que é uma árvore que conecta todos os nós do grafo com o máximo peso total das arestas. Algoritmos clássicos como o de Prim e o de Kruskal [^6] são frequentemente usados para encontrar a MST.

3.  **Complexidade de Tempo:** Os algoritmos de Prim e Kruskal têm uma complexidade de tempo de $O(E \log V)$, onde $E$ é o número de arestas e $V$ é o número de nós no grafo. No contexto do algoritmo de Chow-Liu, isso se traduz em uma complexidade geral de tempo de $O(NV^2 + V^2 \log V)$ [^6], onde o primeiro termo ($NV^2$) representa o custo de computar as estatísticas suficientes (informações mútuas) e o segundo termo ($V^2 \log V$) representa o custo de encontrar a MST.

4.  **Log-verossimilhança:** A equação para a log-verossimilhança para uma árvore é dada por:
    $$
    \log p(D|\Theta, T) = \sum_{t} \sum_{k} N_{tk} \log p(x_t = k|\Theta) + \sum_{s,t} \sum_{j,k} N_{stjk} \log \frac{p(x_s = j, x_t = k|\Theta)}{p(x_s = j|\Theta)p(x_t = k|\Theta)}
    $$
    onde $N_{stjk}$ é o número de vezes que o nó $s$ está no estado $j$ e o nó $t$ está no estado $k$, e $N_{tk}$ é o número de vezes que o nó $t$ está no estado $k$ [^6].

5.  **Equivalência entre Árvores Direcionadas e Não Direcionadas:** Uma árvore pode ser representada como um grafo direcionado ou não direcionado, sem alterar o número de parâmetros ou a complexidade do aprendizado [^5]. A representação não direcionada é útil para o aprendizado da estrutura, enquanto a representação direcionada é mais conveniente para o aprendizado de parâmetros [^5].

### Conclusão
O algoritmo de Chow-Liu oferece uma maneira eficiente de aprender a estrutura de modelos gráficos em forma de árvore, maximizando a verossimilhança dos dados. Apesar das limitações inerentes à estrutura de árvore, o algoritmo fornece uma base sólida para modelar dependências entre variáveis e pode ser estendido para estruturas mais complexas, como florestas de árvores [^6], que permitem inferência paralela e melhor seleção de estrutura através de critérios de verossimilhança marginal ou penalizada.
<!-- END -->
### Referências
[^1]: Página 1, "26. Graphical model structure learning"
[^4]: Página 4, "Since the problem of structure learning for general graphs is NP-hard (Chickering 1996), we start by considering the special case of trees."
[^5]: Página 5, "Thus a tree can be represented as either an undirected or directed graph: the number of parameters is the same, and hence the complexity of learning is the same."
[^6]: Página 6, "Thus the tree topology that maximizes the likelihood can be found by computing the maximum weight spanning tree, where the edge weights are the pairwise mutual informations, I(ys, Yt|Θst). This is called the Chow-Liu algorithm (Chow and Liu 1968)."
