{
  "topics": [
    {
      "topic": "Graphical Model Structure Learning",
      "sub_topics": [
        "Graphical models express conditional independence assumptions between variables, and structure learning involves computing the graph structure, represented as an adjacency matrix, denoted as p(G|D), where G is the graph structure and D is the dataset. This is a foundational task in machine learning for understanding variable relationships. Structure learning has two primary applications: knowledge discovery, which focuses on identifying the graph topology, and density estimation, which requires a fully specified model to estimate the probability distribution over the variables; these applications highlight the versatility of structure learning in different analytical contexts. For knowledge discovery, posterior edge marginals p(Gst = 1|D) can be computed to generate sparse graphs for visualization, while for density estimation, the MAP graph \\u011c \\u2208 argmax p(G|D) is sought.",
        "The number of possible graph structures grows exponentially with the number of nodes, making it computationally challenging to explore the entire space of graph structures, necessitating the use of approximate methods and summaries of the posterior distribution. Computing the full posterior p(G|D) is prohibitively large, which necessitates seeking appropriate summaries of the posterior based on the specific task, such as knowledge discovery or density estimation. Finding the globally optimal graph typically takes exponential time, necessitating discrete optimization methods like heuristic search; this illustrates the computational challenges in model optimization.",
        "Relevance networks visualize pairwise mutual information I(Xi; Xj) between random variables by drawing an edge if the information exceeds a threshold; in the Gaussian case, this mutual information is \\u2212\\u00bd log(1 \\u2212 \\u03c1ij^2), where \\u03c1ij is the correlation coefficient, effectively visualizing the covariance graph. However, relevance networks tend to produce dense graphs due to the interconnectedness of variables, making them a computationally tractable alternative when MAP estimation is not feasible. In the Gaussian case, visualizing \\u2211, known as the covariance graph, helps in understanding variable dependencies; this is often used in systems biology to visualize gene interactions.",
        "Dependency networks learn a graphical model structure by independently fitting D sparse full-conditional distributions p(xt|x_t), where the chosen variables form the Markov blanket of the node; this approach uses sparse regression or classification methods, such as l1-regularized linear regression or Bayesian variable selection, to fit each conditional probability distribution (CPD), offering an advantage over relevance networks by avoiding redundant variable selection. Theoretical conditions exist under which l1-regularized linear regression can recover the true graph structure, assuming the data was generated from a sparse Gaussian graphical model.",
        "Learning tree structures involves finding fully specified joint probability models suitable for density estimation, prediction, and knowledge discovery; since general graph structure learning is NP-hard, trees are considered as a special case due to their efficient learnability and suitability for exact inference. Trees can be represented as directed or undirected graphs with equivalent parameter complexity, with undirected representations being symmetric and useful for structure learning.",
        "The Chow-Liu algorithm finds the maximum likelihood (ML) tree structure by computing the maximum weight spanning tree, where edge weights are pairwise mutual informations I(ys, yt|\\u0398st); Prim's and Kruskal's algorithms, with a time complexity of O(E log V), are used to find the max spanning tree (MST), making the overall running time O(NV^2 + V^2 log V), where the first term is the cost of computing sufficient statistics. Learning a forest (multiple trees) instead of a single tree can speed up inference due to parallel belief propagation, and using marginal or penalized likelihood criteria enables the selection of a forest structure.",
        "Learning DAG structures involves computing p(G|D) under the constraint that G is a Directed Acyclic Graph (DAG), often referred to as Bayesian network structure learning; it is typically assumed that there is no missing data or hidden variables (the complete data assumption), and for simplicity, variables are often treated as categorical with tabular CPDs, though results can be generalized to real-valued data and other CPD types like linear-Gaussian CPDs. Markov equivalence defines when different DAG structures encode the same set of conditional independence (CI) assumptions; two structures are Markov equivalent if they have the same undirected skeleton and the same set of v-structures, leading to the concept of a partially directed acyclic graph (PDAG) or essential graph that represents a Markov equivalence class with directed (compelled) and undirected (reversible) edges.",
        "Exact structural inference computes the exact posterior over graphs, p(G|D), deriving the likelihood assuming no missing data and tabular CPDs, and making standard assumptions like global prior parameter independence and local prior parameter independence to derive the marginal likelihood of any DAG, which decomposes or factorizes according to the graph structure. A simple worked example illustrates how to compute posterior probabilities for different graph structures with binary nodes, using the BDeu prior (a uniform BDe prior), to demonstrate the application of Bayesian methods in a basic scenario, which allows to compare different graph structures quantitatively.",
        "Scaling up to larger graphs requires approximating the mode of the posterior using dynamic programming, which, unfortunately, takes V^2V time and space, making it intractable beyond about 16 nodes; consequently, a more common method is greedy hill climbing, which proposes small changes to the current graph at each step and moves to the neighboring graph that most increases the posterior. Approximating other functions of the posterior involves computing the probability that each edge is present, p(Gst = 1|D), or the probability of a path between nodes, using techniques like dynamic programming or sampling DAGs from the posterior with Metropolis Hastings algorithm, enabling knowledge discovery by identifying significant relationships and connections.",
        "Learning DAG structure with latent variables addresses situations where the complete data assumption does not hold due to missing data or hidden variables; in such cases, the marginal likelihood is computed by integrating or summing over the hidden variables, a process that is generally intractable and requires approximation methods. The Cheeseman-Stutz approximation (CS) is a method for approximating the marginal likelihood in the presence of latent variables; it involves first computing a MAP estimate of the parameters using EM, then filling in the hidden variables with their expectation, and finally using the exact marginal likelihood equation on this filled-in data, with corrections to account for the exponentially smaller value due to not summing over all values of h. Variational Bayes EM uses factorization assumptions to approximate the posterior, providing a lower bound on the log marginal likelihood and a more accurate approximation than BIC or Cheeseman-Stutz. Structural EM fills in data using a candidate neighboring graph and its parameters, then evaluates the score of all neighbors using the filled-in data, approximating the difference in marginal likelihoods for efficient model selection.",
        "Causal models predict the effects of interventions or manipulations, represented by DAGs, and are based on the causal Markov assumption, where A directly causes B if manipulating A changes B. Causal DAGs assume causal sufficiency, including all relevant variables, and can answer causal questions using perfect interventions, represented by Pearl's do calculus and graph surgery. Graph surgery involves cutting arcs coming into nodes set by intervention, preventing information flow from intervened nodes back to their parents, enabling probabilistic inference in the mutilated graph. Simpson's paradox demonstrates that statistical relationships can be reversed by including additional factors, requiring causal reasoning and the identification of confounding variables to resolve. Learning causal DAG structures involves distinguishing DAGs within the equivalence class using interventional data, where certain variables have been set, and modifying Bayesian scoring criteria to handle mixed observational and experimental data.",
        "Learning undirected Gaussian graphical models is easier than learning DAG structure because there is no need to worry about acyclicity; however, it is harder since the likelihood does not decompose, precluding local search methods. The task of computing the MLE for a (non-decomposable) GGM is called covariance selection. The graphical lasso learns a sparse GRF structure by exploiting the 1:1 correspondence between zeros in the precision matrix and absent edges in the graph. This is achieved by using an objective that encourages zeros in the precision matrix, analogous to lasso. Bayesian inference for GGM structure, though computationally intensive, can integrate out parameters and perform posterior inference in the space of graphs, using summaries like posterior edge marginals. Handling non-Gaussian data using copulas generalizes graphical lasso by transforming data to joint Gaussianity, using univariate monotonic transformations and then applying graphical lasso to the transformed data. Learning undirected discrete graphical models, such as MRFs/CRFs, extends graphical lasso with group lasso to handle multiple parameters per edge, but requires managing the computational cost of evaluating the objective and gradient.",
        "Handling non-Gaussian data using copulas generalizes graphical lasso to non-Gaussian data by estimating univariate monotonic transformations to make the data jointly Gaussian, allowing application of standard techniques; learning undirected discrete graphical models is harder than the Gaussian case due to the intractability of computing the partition function. Graphical lasso for MRFs/CRFs extends the graphical lasso idea to discrete MRFs and CRFs, requiring the use of group lasso to handle multiple parameters associated with each edge, which involves minimizing an objective function with group lasso penalties to learn sparse structure, and the objective can be optimized using algorithms like projected quasi-Newton methods.",
        "Learning the structure for UGMs with discrete variables is harder than the Gaussian case, because computing the partition function Z(\\u03b8), which is needed for parameter estimation, has complexity comparable to computing the permanent of a matrix. Graphical lasso can be extended to the discrete MRF and CRF case, and to learn sparse structure, we can minimize an objective function that includes log potentials and a group lasso penalty."
      ]
    }
  ]
}