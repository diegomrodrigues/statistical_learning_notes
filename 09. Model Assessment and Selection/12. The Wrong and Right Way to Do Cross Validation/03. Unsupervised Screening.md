Okay, here's the augmented text with Mermaid diagrams, focusing on the mathematical and statistical concepts as requested:

## Unsupervised Screening for Feature Selection in Model Assessment

<imagem: Um mapa mental complexo que ilustra a hierarquia dos m√©todos de sele√ß√£o de vari√°veis, com um ramo dedicado aos m√©todos supervisionados (e.g., regulariza√ß√£o L1, L2) e outro para os n√£o supervisionados (e.g., an√°lise de componentes principais, m√©todos de clustering), mostrando como esses m√©todos convergem para um modelo final.>

### Introdu√ß√£o

A capacidade de generaliza√ß√£o de um m√©todo de aprendizado, ou seja, a sua performance em dados de teste independentes, √© um aspecto crucial na pr√°tica do aprendizado de m√°quina [^7.1]. A avalia√ß√£o desta performance guia a escolha do modelo e nos d√° uma medida da qualidade do modelo selecionado. Este cap√≠tulo aborda t√©cnicas chave para a avalia√ß√£o de performance e como elas s√£o usadas para selecionar modelos, explorando inicialmente a rela√ß√£o entre vi√©s, vari√¢ncia e complexidade do modelo. No contexto espec√≠fico de sele√ß√£o de vari√°veis, o screening n√£o supervisionado √© uma ferramenta √∫til, embora n√£o totalmente abordada nos t√≥picos fornecidos [^7.2]. Vamos expandir sobre esse conceito no presente cap√≠tulo. O screening n√£o supervisionado, por sua natureza, n√£o considera diretamente as labels das classes ou vari√°veis target, mas busca identificar padr√µes e estruturas inerentes nos dados.

### Conceitos Fundamentais

**Conceito 1: O Problema da Classifica√ß√£o e a Generaliza√ß√£o**

Em problemas de classifica√ß√£o, o objetivo √© atribuir uma inst√¢ncia a uma categoria espec√≠fica, dado um conjunto de atributos. Um modelo com boa generaliza√ß√£o consegue realizar essa tarefa de forma eficaz em dados n√£o vistos anteriormente. M√©todos lineares de classifica√ß√£o, como LDA e Regress√£o Log√≠stica, buscam encontrar fronteiras de decis√£o lineares que separam as diferentes classes, o que pode levar a trade-offs entre **vi√©s e vari√¢ncia** [^7.2]. Um modelo complexo pode se ajustar bem aos dados de treinamento, levando a um baixo vi√©s, mas com alta vari√¢ncia, ou seja, grande sensibilidade a pequenas varia√ß√µes nos dados de treinamento e pouca capacidade de generalizar. Um modelo muito simples, por outro lado, ter√° alto vi√©s e baixa vari√¢ncia, subajustando os dados e perdendo informa√ß√µes importantes.

**Lemma 1: Decomposi√ß√£o do Erro Quadr√°tico M√©dio em Vi√©s e Vari√¢ncia**

O erro quadr√°tico m√©dio (MSE) de um modelo preditivo pode ser decomposto em tr√™s componentes: o ru√≠do irredut√≠vel, o vi√©s ao quadrado e a vari√¢ncia. A formula√ß√£o desse lemma √©:
$$
Err(x_0) = E[(Y - \hat{f}(x_0))^2 | X = x_0] = \sigma^2 + Bias^2(\hat{f}(x_0)) + Var(\hat{f}(x_0)),
$$
onde $\sigma^2$ representa o ru√≠do irredut√≠vel, $Bias(\hat{f}(x_0)) = E[\hat{f}(x_0)] - f(x_0)$ o vi√©s e $Var(\hat{f}(x_0)) = E[(\hat{f}(x_0) - E[\hat{f}(x_0)])^2]$ a vari√¢ncia [^7.3]. O objetivo √© encontrar um modelo que minimize esse erro total, considerando a compensa√ß√£o entre vi√©s e vari√¢ncia. $\blacksquare$
```mermaid
graph TB
    subgraph "MSE Decomposition"
        direction TB
        A["MSE(x‚ÇÄ) = E[(Y - fÃÇ(x‚ÇÄ))¬≤ | X=x‚ÇÄ]"]
        B["Bias¬≤(fÃÇ(x‚ÇÄ)) = (E[fÃÇ(x‚ÇÄ)] - f(x‚ÇÄ))¬≤"]
        C["Var(fÃÇ(x‚ÇÄ)) = E[(fÃÇ(x‚ÇÄ) - E[fÃÇ(x‚ÇÄ)])¬≤]"]
        D["œÉ¬≤ (Irreducible Error)"]
        A --> B
        A --> C
        A --> D
    end
```

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo de regress√£o que tenta prever a altura de pessoas com base em seu peso. Vamos simular dados para ilustrar o tradeoff.
```python
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(42)
X = np.linspace(50, 120, 50)  # Weight
true_slope = 0.05
true_intercept = 140
Y = true_slope * X + true_intercept + np.random.normal(0, 5, 50) # Height

# Simple linear model
def linear_model(x, slope, intercept):
    return slope * x + intercept

# Complex model (polynomial)
def polynomial_model(x, coefs):
    return np.polyval(coefs, x)

# Fit simple model with least squares
slope_simple = np.cov(X, Y)[0,1]/np.var(X)
intercept_simple = np.mean(Y) - slope_simple*np.mean(X)

# Fit a 3rd order polynomial
coefs_complex = np.polyfit(X,Y,3)


# Calculate predictions
y_simple_pred = linear_model(X, slope_simple, intercept_simple)
y_complex_pred = polynomial_model(X, coefs_complex)

# Calculate MSE
mse_simple = np.mean((Y - y_simple_pred)**2)
mse_complex = np.mean((Y - y_complex_pred)**2)


print(f"MSE (Simple): {mse_simple:.2f}")
print(f"MSE (Complex): {mse_complex:.2f}")

# Generate test data (slightly different from train)
X_test = np.linspace(55,115,50)
Y_test = true_slope * X_test + true_intercept + np.random.normal(0, 5, 50)

y_simple_test_pred = linear_model(X_test, slope_simple, intercept_simple)
y_complex_test_pred = polynomial_model(X_test, coefs_complex)

# Calculate MSE on test data
mse_simple_test = np.mean((Y_test - y_simple_test_pred)**2)
mse_complex_test = np.mean((Y_test - y_complex_test_pred)**2)

print(f"MSE Test (Simple): {mse_simple_test:.2f}")
print(f"MSE Test (Complex): {mse_complex_test:.2f}")


# Plotting
plt.figure(figsize=(10, 5))
plt.scatter(X, Y, label='Data de Treino')
plt.plot(X, y_simple_pred, color='red', label='Modelo Simples')
plt.plot(X, y_complex_pred, color='green', label='Modelo Complexo')
plt.xlabel('Peso (kg)')
plt.ylabel('Altura (cm)')
plt.title('Trade-off entre Vi√©s e Vari√¢ncia')
plt.legend()
plt.show()


plt.figure(figsize=(10, 5))
plt.scatter(X_test, Y_test, label='Dados de Teste')
plt.plot(X_test, y_simple_test_pred, color='red', label='Modelo Simples')
plt.plot(X_test, y_complex_test_pred, color='green', label='Modelo Complexo')
plt.xlabel('Peso (kg)')
plt.ylabel('Altura (cm)')
plt.title('Performance em Dados de Teste')
plt.legend()
plt.show()

```
> Nesse exemplo, o modelo simples (linear) pode ter um vi√©s maior (n√£o capturar algumas nuances), enquanto o modelo complexo (polinomial) pode se ajustar muito aos dados de treino, apresentando uma vari√¢ncia mais alta e uma generaliza√ß√£o pior (maior MSE no teste). O erro irredut√≠vel √© causado pelo ru√≠do nos dados.

**Conceito 2: Linear Discriminant Analysis (LDA)**

LDA √© uma t√©cnica para classifica√ß√£o que busca projetar os dados em um espa√ßo de menor dimens√£o de forma que as classes fiquem o mais separadas poss√≠vel. As premissas b√°sicas do LDA incluem a normalidade multivariada das classes e a igualdade das matrizes de covari√¢ncia. A fun√ß√£o discriminante linear obtida pelo LDA serve para definir fronteiras de decis√£o entre as classes [^7.3].
```mermaid
graph LR
    subgraph "Linear Discriminant Analysis (LDA)"
        direction TB
        A["Data with features (X)"] --> B["LDA Transformation"]
        B --> C["Projected Data (X_lda)"]
        C --> D["Maximizing between-class variance"]
        C --> E["Minimizing within-class variance"]
    end
```

**Corol√°rio 1: A Rela√ß√£o entre Proje√ß√µes LDA e Subespa√ßos Reduzidos**

Sob as premissas do LDA, a fun√ß√£o discriminante linear pode ser vista como uma proje√ß√£o dos dados em um subespa√ßo de dimens√£o reduzida, maximizando a raz√£o entre a vari√¢ncia entre classes e a vari√¢ncia intra-classe. Isso resulta em uma representa√ß√£o dos dados que √© otimizada para a classifica√ß√£o, reduzindo a dimensionalidade sem perda significativa de informa√ß√£o relevante para a tomada de decis√£o. $\blacksquare$

> üí° **Exemplo Num√©rico:** Imagine um conjunto de dados com duas classes, cada uma com duas features. As features para cada classe tem uma distribui√ß√£o gaussiana bidimensional, e a classe 1 tem m√©dia $\mu_1 = [2, 2]$ e a classe 2 tem m√©dia $\mu_2 = [4, 4]$. Ambas as classes tem a mesma matriz de covari√¢ncia $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$. LDA vai projetar os dados em uma dimens√£o, maximizando a separa√ß√£o entre os pontos das classes, e este eixo resultante ser√° o discriminante linear.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>
> # Generate synthetic data
> np.random.seed(42)
> mean1 = [2, 2]
> mean2 = [4, 4]
> cov = [[1, 0.5], [0.5, 1]]
> X1 = np.random.multivariate_normal(mean1, cov, 100)
> X2 = np.random.multivariate_normal(mean2, cov, 100)
> X = np.concatenate((X1, X2))
> y = np.array([0] * 100 + [1] * 100)
>
> # Apply LDA
> lda = LinearDiscriminantAnalysis()
> X_lda = lda.fit_transform(X, y)
>
> # Plotting the data and the projection
> plt.figure(figsize=(8, 6))
> plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', label = 'Original Data')
>
> # Calculate the direction of the projection
> coef = lda.coef_
> slope = -coef[0,0]/coef[0,1]
>
> # Define the projection line
> x_proj = np.linspace(np.min(X[:,0]),np.max(X[:,0]),100)
> y_proj = slope*x_proj
>
> # Plot the projection line
> plt.plot(x_proj, y_proj, color = 'red', label='LDA Projection Line')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
> plt.title('LDA Projection')
> plt.legend()
> plt.show()
>
> plt.figure(figsize=(8,6))
> plt.scatter(X_lda, np.zeros_like(X_lda), c=y, cmap='viridis', edgecolors='k', label = 'Projected Data')
> plt.xlabel('LDA Component')
> plt.title('Data Projected onto one dimension')
> plt.legend()
> plt.show()
> ```
> Aqui podemos ver que LDA projeta os dados em uma dimens√£o, e os pontos se tornam mais separados nesta proje√ß√£o. A dire√ß√£o da proje√ß√£o √© determinada pelos coeficientes calculados pelo LDA.

**Conceito 3: Logistic Regression e Maximiza√ß√£o da Verossimilhan√ßa**

Logistic Regression √© outro m√©todo linear de classifica√ß√£o que modela a probabilidade de uma inst√¢ncia pertencer a uma classe atrav√©s de uma fun√ß√£o log√≠stica (sigmoid). A fun√ß√£o logit, que √© o logaritmo da raz√£o de chances (odds), √© modelada como uma fun√ß√£o linear das vari√°veis preditoras. Os par√¢metros do modelo s√£o estimados via maximiza√ß√£o da verossimilhan√ßa, ou seja, maximizando a probabilidade dos dados observados dado o modelo [^7.4]. Este modelo, ao contr√°rio do LDA, n√£o assume normalidade dos dados, mas sim a distribui√ß√£o de Bernoulli para a vari√°vel resposta.
```mermaid
graph LR
    subgraph "Logistic Regression"
        direction TB
        A["Input Features (X)"] --> B["Linear Combination: z = XŒ≤"]
        B --> C["Sigmoid Function: p(y=1|x) = 1/(1 + e^-z)"]
        C --> D["Likelihood Maximization"]
    end
```

> üí° **Exemplo Num√©rico:** Vamos simular um problema de classifica√ß√£o bin√°ria para ilustrar a Regress√£o Log√≠stica. Suponha que queremos classificar e-mails como "spam" ou "n√£o spam" baseado em duas features: n√∫mero de palavras e presen√ßa de links.
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Generate synthetic data
np.random.seed(42)
n_samples = 200
X = np.random.rand(n_samples, 2) * 10
y = (1 / (1 + np.exp(-(0.5 * X[:, 0] - 0.3 * X[:, 1] + 1))) > 0.5).astype(int)

# Split data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a Logistic Regression model
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)

# Make predictions
y_pred = logistic_model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Visualize decision boundary
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
Z = logistic_model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k')
plt.xlabel("Number of words")
plt.ylabel("Presence of links")
plt.title("Logistic Regression Decision Boundary")
plt.show()

print(f"Coefficients: {logistic_model.coef_}")
print(f"Intercept: {logistic_model.intercept_}")
```
> Nesse exemplo, o modelo de regress√£o log√≠stica aprende a fronteira de decis√£o para separar as classes, usando a fun√ß√£o sigm√≥ide para estimar a probabilidade de um e-mail ser spam. A acur√°cia nos dados de teste indica o qu√£o bem o modelo generaliza. Os coeficientes mostram a influ√™ncia das features na decis√£o.

> ‚ö†Ô∏è **Nota Importante**: Modelos lineares s√£o eficientes para dados linearmente separ√°veis, mas podem n√£o capturar rela√ß√µes n√£o lineares.
> ‚ùó **Ponto de Aten√ß√£o**: Em problemas com classes desbalanceadas, pode ser necess√°rio ajustar os pesos das classes na regress√£o log√≠stica para obter melhores resultados.
> ‚úîÔ∏è **Destaque**: A regress√£o log√≠stica estima as probabilidades de classe diretamente, o que a torna √∫til em aplica√ß√µes onde essas probabilidades s√£o importantes.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

<imagem: Um diagrama de fluxo detalhado mostrando como a regress√£o linear de matrizes indicadoras leva a decis√µes de classifica√ß√£o. O diagrama inclui etapas como codifica√ß√£o de classes, estimativa de coeficientes via m√≠nimos quadrados, aplica√ß√£o de regras de decis√£o, e compara√ß√£o com m√©todos probabil√≠sticos, como regress√£o log√≠stica.>
```mermaid
graph TB
    subgraph "Linear Regression for Classification"
        direction TB
        A["Categorical Labels"] --> B["Indicator Matrix"]
        B --> C["Linear Regression: Y_indicator = XŒ≤"]
         C --> D["Predictions"]
         D --> E["Decision Rule"]
         E --> F["Class Predictions"]
    end
```

A regress√£o linear pode ser adaptada para problemas de classifica√ß√£o atrav√©s da regress√£o de uma matriz de indicadores. Nesse m√©todo, cada classe √© representada por uma vari√°vel indicadora que assume o valor 1 quando a inst√¢ncia pertence √†quela classe e 0 caso contr√°rio. Um modelo de regress√£o linear √© ent√£o ajustado a essas vari√°veis indicadoras, e as predi√ß√µes s√£o convertidas em decis√µes de classe atrav√©s de alguma regra de decis√£o. Por exemplo, a classe com o maior valor de predi√ß√£o pode ser selecionada como a classe predita [^7.2].

Este m√©todo tem limita√ß√µes. A regress√£o de indicadores n√£o modela diretamente a probabilidade de cada classe. Em vez disso, ela busca ajustar um modelo linear aos indicadores de classe, o que pode levar a resultados fora do intervalo [0,1] para problemas de classifica√ß√£o bin√°ria. Al√©m disso, os m√≠nimos quadrados podem ser sens√≠veis a outliers, e a covari√¢ncia entre os indicadores de classe pode influenciar o ajuste do modelo [^7.3].

**Lemma 2: Equival√™ncia entre Proje√ß√µes Lineares e Discriminantes Lineares**

Em condi√ß√µes espec√≠ficas, as proje√ß√µes nos hiperplanos de decis√£o gerados pela regress√£o linear da matriz de indicadores podem ser equivalentes √†s obtidas com LDA, especialmente quando as classes s√£o aproximadamente esf√©ricas e t√™m vari√¢ncias semelhantes. A prova deste lemma envolve a demonstra√ß√£o de que os coeficientes obtidos por m√≠nimos quadrados se relacionam com as m√©dias de cada classe e a matriz de covari√¢ncia combinada [^7.3]. $\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos simular um caso simples de classifica√ß√£o bin√°ria, onde podemos ajustar uma regress√£o linear para as classes usando um c√≥digo indicador, e comparar com LDA.
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score

# Generate synthetic data
np.random.seed(42)
n_samples = 100
X = np.random.rand(n_samples, 2) * 10
y = (0.5 * X[:, 0] - 0.3 * X[:, 1] + 1 > 0).astype(int)

# Indicator matrix
indicator_matrix = np.zeros((n_samples, 2))
indicator_matrix[np.arange(n_samples), y] = 1

# Linear regression on indicator matrix
linear_reg = LinearRegression()
linear_reg.fit(X, indicator_matrix)
y_pred_linear_reg = np.argmax(linear_reg.predict(X),axis = 1)


# LDA
lda = LinearDiscriminantAnalysis()
lda.fit(X,y)
y_pred_lda = lda.predict(X)

print("Linear Regression Accuracy: ", accuracy_score(y, y_pred_linear_reg))
print("LDA Accuracy: ", accuracy_score(y, y_pred_lda))

# Visualization
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
Z_linear_reg = np.argmax(linear_reg.predict(np.c_[xx.ravel(), yy.ravel()]), axis=1).reshape(xx.shape)
Z_lda = lda.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.contourf(xx, yy, Z_linear_reg, cmap=plt.cm.RdBu, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k')
plt.title('Linear Regression Decision Boundary')

plt.subplot(1, 2, 2)
plt.contourf(xx, yy, Z_lda, cmap=plt.cm.RdBu, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k')
plt.title('LDA Decision Boundary')
plt.show()

```
> Aqui, vemos que a regress√£o linear nas matrizes indicadoras pode gerar resultados compar√°veis aos do LDA, especialmente quando as classes s√£o aproximadamente esf√©ricas. As fronteiras de decis√£o s√£o similares, como esperado pela equival√™ncia entre os dois m√©todos sob certas condi√ß√µes.

**Corol√°rio 2: Simplifica√ß√£o da An√°lise do Modelo com a Equival√™ncia**

Este corol√°rio estabelece que, sob certas condi√ß√µes, a an√°lise e otimiza√ß√£o de modelos de regress√£o linear para classifica√ß√£o podem ser simplificadas utilizando os conceitos e resultados obtidos da an√°lise discriminante linear, facilitando a interpreta√ß√£o e compara√ß√£o entre os m√©todos [^7.3].

A regress√£o log√≠stica, como mencionado anteriormente, oferece uma abordagem mais natural para a modelagem de probabilidades de classe, evitando os problemas de extrapola√ß√£o fora do intervalo [0,1] que podem ocorrer na regress√£o de indicadores [^7.4]. No entanto, em cen√°rios onde a principal preocupa√ß√£o √© a fronteira de decis√£o linear e n√£o as probabilidades, a regress√£o de indicadores pode ser suficiente e computacionalmente mais eficiente [^7.2].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

<imagem: Um mapa mental que ilustra a hierarquia dos m√©todos de regulariza√ß√£o em modelos de classifica√ß√£o, com foco em t√©cnicas de regulariza√ß√£o L1 e L2. O mapa mental mostra como a regulariza√ß√£o L1 promove a sparsity, enquanto a regulariza√ß√£o L2 reduz a complexidade do modelo, ambos visando melhorar a generaliza√ß√£o.>
```mermaid
graph TB
    subgraph "Regularization Techniques"
        direction TB
        A["Model with High Dimensional Features"] --> B["L1 Regularization (Lasso)"]
         A --> C["L2 Regularization (Ridge)"]
         B --> D["Sparsity: Feature Selection"]
         C --> E["Reduced Model Complexity"]
         D & E --> F["Improved Generalization"]
    end
```

A sele√ß√£o de vari√°veis √© um passo crucial para lidar com conjuntos de dados de alta dimensionalidade, onde o n√∫mero de vari√°veis preditoras √© grande. A regulariza√ß√£o, por sua vez, √© uma t√©cnica utilizada para evitar overfitting, penalizando a complexidade do modelo. M√©todos de regulariza√ß√£o, como penaliza√ß√µes L1 (Lasso) e L2 (Ridge), podem ser combinados em modelos log√≠sticos para controle de sparsity e estabilidade [^7.5].

A penaliza√ß√£o L1 adiciona √† fun√ß√£o de custo um termo proporcional √† soma dos valores absolutos dos coeficientes, promovendo solu√ß√µes esparsas, ou seja, for√ßando alguns coeficientes a serem exatamente zero [^7.4.4]. Isso resulta na sele√ß√£o de um subconjunto de vari√°veis relevantes para o modelo. A penaliza√ß√£o L2 adiciona um termo proporcional √† soma dos quadrados dos coeficientes, reduzindo a magnitude dos coeficientes e suavizando a fun√ß√£o de decis√£o [^7.4.4].
```mermaid
graph TB
    subgraph "L1 and L2 Regularization"
        direction TB
        A["Cost Function (J(Œ≤))"]
        B["L1 Penalty: ŒªŒ£|Œ≤_j|"]
        C["L2 Penalty: ŒªŒ£Œ≤_j¬≤"]
         A --> B
         A --> C
        B --> D["Sparsity"]
        C --> E["Coefficient Shrinkage"]
        D --> F["Feature Selection"]
    end
```

**Lemma 3: Penaliza√ß√£o L1 e Sparsity em Classifica√ß√£o Log√≠stica**

A penaliza√ß√£o L1, ao adicionar um termo proporcional √† soma dos valores absolutos dos coeficientes √† fun√ß√£o de custo da regress√£o log√≠stica, leva a uma solu√ß√£o onde muitos coeficientes s√£o exatamente zero. A prova desse lemma envolve mostrar que a n√£o-diferenciabilidade do termo L1 na origem promove esse comportamento, convergindo para solu√ß√µes esparsas [^7.4.4]. $\blacksquare$

**Prova do Lemma 3:** A fun√ß√£o de custo regularizada na regress√£o log√≠stica com penaliza√ß√£o L1 √© dada por
$$
J(\beta) = -\sum_{i=1}^{N} [y_i \log(p(x_i)) + (1-y_i)\log(1-p(x_i))] + \lambda \sum_{j=1}^{p}|\beta_j|,
$$
onde $p(x_i)$ √© a probabilidade estimada da classe para a inst√¢ncia $x_i$, e $\lambda$ √© o par√¢metro de regulariza√ß√£o. A n√£o-diferenciabilidade do termo $\lambda \sum_{j=1}^{p}|\beta_j|$ na origem for√ßa alguns coeficientes a se tornarem zero quando o $\lambda$ √© suficientemente grande, promovendo a esparsidade. $\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos usar a Regress√£o Log√≠stica com Regulariza√ß√£o L1 (Lasso) e L2 (Ridge), comparando os resultados em um dataset simulado.
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

# Generate synthetic data with 10 features, few relevant
np.random.seed(42)
n_samples = 100
n_features = 10
X = np.random.rand(n_samples, n_features)
true_coef = np.array([0.8, -0.5, 0.3, 0, 0, 0, 0, 0, 0, 0])  # Only first 3 features are relevant
y = (np.dot(X, true_coef) + np.random.normal(0, 0.5, n_samples) > 0).astype(int)
# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Logistic Regression with L1 (Lasso)
lasso_model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)  # C is inverse of lambda
lasso_model.fit(X_train, y_train)
y_pred_lasso = lasso_model.predict(X_test)
accuracy_lasso = accuracy_score(y_test, y_pred_lasso)

# Logistic Regression with L2 (Ridge)
ridge_model = LogisticRegression(penalty='l2', C=1.0)
ridge_model.fit(X_train, y_train)
y_pred_ridge = ridge_model.predict(X_test)
accuracy_ridge = accuracy_score(y_test, y_pred_ridge)


print(f"Lasso Accuracy: {accuracy_lasso:.2f}")
print(f"Ridge Accuracy: {accuracy_ridge:.2f}")

print("Lasso Coefficients: ", lasso_model.coef_)
print("Ridge Coefficients: ", ridge_model.coef_)

# Varying the regularization strenght
C_values = [0.001, 0.01, 0.1, 1, 10, 100]
lasso_coefs = []
ridge_coefs = []

for c in C_values:
    lasso_model_c = LogisticRegression(penalty='l1', solver='liblinear', C=c)
    lasso_model_c.fit(X_train, y_train)
    lasso_coefs.append(lasso_model_c.coef_[0])

    ridge_model_c = LogisticRegression(penalty='l2', C=c)
    ridge_model_c.fit(X_train, y_train)
    ridge_coefs.append(ridge_model_c.coef_[0])

lasso_coefs = np.array(lasso_coefs)
ridge_coefs = np.array(ridge_coefs)

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
for i in range(n_features):
    plt.plot(C_values, lasso_coefs[:,i], label=f'Feature {i}')
plt.xscale('log')
plt.xlabel('C (Regularization Strength)')
plt.ylabel('Lasso Coefficients')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.subplot(1, 2, 2)
for i in range(n_features):
    plt.plot(C_values, ridge_coefs[:,i], label=f'Feature {i}')
plt.xscale('log')
plt.xlabel('C (Regularization Strength)')
plt.ylabel('Ridge Coefficients')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.show()
```
> Com esse exemplo, podemos ver como a regulariza√ß√£o L1 (Lasso) leva a coeficientes esparsos (muitos exatamente zero) e como a regulariza√ß√£o L2 (Ridge) reduz a magnitude dos coeficientes, ambas tentando melhorar a generaliza√ß√£o. Podemos observar como os coeficientes variam com o par√¢metro de regulariza√ß√£o C.

**Corol√°rio 3: Implica√ß√µes da Sparsity na Interpretabilidade**

A esparsidade induzida pela penaliza√ß√£o L1 melhora a interpretabilidade do modelo, pois permite identificar quais vari√°veis t√™m um efeito significativo na predi√ß√£o. Isso √© importante em aplica√ß√µes onde a compreens√£o do modelo √© t√£o relevante quanto sua capacidade preditiva [^7.4.5].

> ‚ö†Ô∏è **Ponto Crucial**: A combina√ß√£o de penaliza√ß√µes L1 e L2 (Elastic Net) permite obter modelos com esparsidade e estabilidade, aproveitando as vantagens de ambas as abordagens.

### Separating Hyperplanes e Perceptrons
```mermaid
graph TB
    subgraph "Separating Hyperplanes"
        direction TB
        A["Data Points (X, y)"]
        B["Hyperplane (w‚ãÖx + b = 0)"]
        C["Margin"]
        A --> B
        B --> C
        C --> D["Support Vectors"]
        D --> E["Maximized Margin"]
    end
```

A ideia de maximizar a margem de separa√ß√£o entre as classes leva ao conceito de **hiperplanos √≥timos**, que s√£o utilizados em m√©todos de classifica√ß√£o como o **Support Vector Machine (SVM)** [^7.5.2]. A margem √© a dist√¢ncia entre o hiperplano de decis√£o e os pontos de dados mais pr√≥ximos de cada classe, tamb√©m conhecidos como **vetores de suporte**. A maximiza√ß√£o dessa margem resulta em modelos que s√£o mais robustos e t√™m maior capacidade de generaliza√ß√£o. O problema de encontrar o hiperplano √≥timo pode ser formulado como um problema de otimiza√ß√£o quadr√°tica, que pode ser resolvido atrav√©s do uso do dual de Wolfe.
```mermaid
graph TB
    subgraph "Perceptron Algorithm"
        direction TB
        A["Initialize weights (w) and bias (b)"]
        B["For each data point x_i"]
        C["Calculate w‚ãÖx_i + b"]
        D["If y_i(w‚ãÖx_i + b) ‚â§ 0:"]
        E["Update weights: w <- w + Œ∑y_i x_i and b <- b + Œ∑y_i"]
        B --> C
        C --> D
         D --> E
    end
```

O **Perceptron de Rosenblatt** √© um algoritmo de aprendizado de m√°quina que busca encontrar um hiperplano que separa as classes, ou seja, uma **fronteira de decis√£o linear**. Sob condi√ß√µes de separabilidade, o algoritmo do perceptron √© garantido de convergir para uma solu√ß√£o que separa as classes. No entanto, se os dados n√£o forem linearmente separ√°veis, o algoritmo pode n√£o convergir ou demorar muito tempo para isso [^7.5.1].

> üí° **Exemplo Num√©rico:** Vamos simular um conjunto de dados linearmente separ√°vel para ilustrar o funcionamento do Perceptron.
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Perceptron
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Generate linearly separable data
np.random.seed(42)
n_samples = 100
X = np.random.rand(n_samples, 2) * 10
y = (X[:, 0] - X[:, 1] + 2 > 0).astype(int)  # Linearly separable

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Perceptron model
perceptron = Perceptron(max_iter=1000, tol=1e-3, eta0 = 0.1)
perceptron.fit(X_train, y_train)
y_pred = perceptron.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print(f"Coeficients: {perceptron.coef_}")
print(f"Intercept: {perceptron.intercept_}")

# Decision Boundary visualization
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
Z = perceptron.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(8, 6