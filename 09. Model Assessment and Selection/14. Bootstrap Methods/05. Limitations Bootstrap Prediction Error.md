## Limitations of Bootstrap Prediction Error

<imagem: Mapa mental conectando os principais m√©todos de estima√ß√£o de erro (Bootstrap, Cross-Validation, AIC, BIC) e destacando as limita√ß√µes do Bootstrap em cen√°rios espec√≠ficos, como overfitting e vi√©s de tamanho de amostra>

### Introdu√ß√£o
A avalia√ß√£o do desempenho de modelos de aprendizado estat√≠stico √© crucial, e a estima√ß√£o do erro de predi√ß√£o desempenha um papel central nesse processo. M√©todos como o **bootstrap**, **valida√ß√£o cruzada**, **AIC** (Akaike Information Criterion) e **BIC** (Bayesian Information Criterion) s√£o ferramentas essenciais para essa finalidade [^7.1]. No entanto, cada um desses m√©todos possui suas pr√≥prias limita√ß√µes. Este cap√≠tulo explora as limita√ß√µes espec√≠ficas do bootstrap, com foco nos problemas de *overfitting* e vi√©s de tamanho de amostra, construindo sobre os conceitos j√° introduzidos nas se√ß√µes [^7.1], [^7.2], [^7.3], [^7.4], [^7.5] e [^7.6]. √â crucial entender essas limita√ß√µes para aplicar o bootstrap de maneira eficaz e interpretar seus resultados com cautela.

### Estima√ß√£o de Erro de Predi√ß√£o via Bootstrap
O **bootstrap** √© uma t√©cnica de reamostragem que permite estimar a distribui√ß√£o amostral de um estimador, como o erro de predi√ß√£o, atrav√©s da cria√ß√£o de m√∫ltiplas amostras bootstrap a partir dos dados originais [^7.11]. Esse m√©todo envolve a gera√ß√£o de amostras com reposi√ß√£o do conjunto de dados original, criando conjuntos de dados ligeiramente diferentes que podem ser usados para avaliar a variabilidade das estimativas do modelo. A ideia fundamental por tr√°s do bootstrap √© aproximar a distribui√ß√£o amostral de um estimador atrav√©s da distribui√ß√£o das estimativas em amostras bootstrap. O m√©todo funciona atrav√©s da cria√ß√£o de *B* conjuntos de dados de bootstrap, ajustando o modelo em cada conjunto e, em seguida, usando esses modelos ajustados para estimar um erro de predi√ß√£o.

#### M√©todo Bootstrap e sua Formula√ß√£o Matem√°tica
Para estimar o erro de predi√ß√£o usando o bootstrap, dado um modelo ajustado a um conjunto de dados de treinamento *Z* = (*z*<sub>1</sub>, *z*<sub>2</sub>, ..., *z<sub>N</sub>*), onde *z<sub>i</sub>* = (*x<sub>i</sub>*, *y<sub>i</sub>*), o bootstrap gera *B* conjuntos de dados *Z<sup>*b</sup>*, onde cada *Z<sup>*b</sup>* √© uma amostra de tamanho *N* obtida com reposi√ß√£o de *Z* [^7.11]. Para cada amostra bootstrap *Z<sup>*b</sup>*, o modelo √© ajustado, resultando em uma previs√£o *f<sup>*b</sup>(x<sub>i</sub>)* para cada *x<sub>i</sub>* no conjunto original. A estimativa bootstrap do erro, denotada por Err<sub>boot</sub>, √© dada por:
$$Err_{boot} = \frac{1}{BN}\sum_{b=1}^{B}\sum_{i=1}^{N} L(y_i, f^{*b}(x_i))$$
onde *L* √© uma fun√ß√£o de perda adequada (por exemplo, erro quadr√°tico m√©dio ou perda 0-1).
Essa formula√ß√£o, entretanto, apresenta limita√ß√µes quando utilizada como estimativa de erro de predi√ß√£o, como discutido nas pr√≥ximas se√ß√µes.
```mermaid
graph TB
    subgraph "Bootstrap Error Estimation"
        direction TB
        A["Original Data: Z = {z1, z2, ..., zN}"]
        B["Generate B Bootstrap Samples: Z*b"]
        C["Fit Model on Each Sample: f*b(x)"]
        D["Calculate Loss on Original Data: L(yi, f*b(xi))"]
        E["Average Loss: Err_boot"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```

> üí° **Exemplo Num√©rico:**
> Vamos considerar um cen√°rio simples de regress√£o linear com um pequeno conjunto de dados: *Z* = {(*x*<sub>1</sub>=1, *y*<sub>1</sub>=2), (*x*<sub>2</sub>=2, *y*<sub>2</sub>=3), (*x*<sub>3</sub>=3, *y*<sub>3</sub>=5)}. Vamos usar o erro quadr√°tico m√©dio como fun√ß√£o de perda. Suponha que geramos *B*=2 amostras bootstrap:
>
> *   *Z<sup>*1</sup>* = {(*x*<sub>1</sub>=1, *y*<sub>1</sub>=2), (*x*<sub>2</sub>=2, *y*<sub>2</sub>=3), (*x*<sub>1</sub>=1, *y*<sub>1</sub>=2)}
> *   *Z<sup>*2</sup>* = {(*x*<sub>2</sub>=2, *y*<sub>2</sub>=3), (*x*<sub>3</sub>=3, *y*<sub>3</sub>=5), (*x*<sub>3</sub>=3, *y*<sub>3</sub>=5)}
>
> Ajustamos um modelo linear simples em cada conjunto. Suponha que os modelos resultantes s√£o:
>
> *   *f<sup>*1</sup>(x)* = 1.5 + 0.5*x*
> *   *f<sup>*2</sup>(x)* = 1 + 1.2*x*
>
> Usando estes modelos nas amostras originais, calculamos os erros para cada ponto e modelo:
>
> *   Para o modelo *f<sup>*1</sup>(x)*:
>     *   *L*(2, *f<sup>*1</sup>(1)) = (2 - (1.5 + 0.5 * 1))<sup>2</sup> = 0
>     *   *L*(3, *f<sup>*1</sup>(2)) = (3 - (1.5 + 0.5 * 2))<sup>2</sup> = 1
>     *   *L*(5, *f<sup>*1</sup>(3)) = (5 - (1.5 + 0.5 * 3))<sup>2</sup> = 4
>
> *   Para o modelo *f<sup>*2</sup>(x)*:
>     *   *L*(2, *f<sup>*2</sup>(1)) = (2 - (1 + 1.2 * 1))<sup>2</sup> = 0.04
>     *   *L*(3, *f<sup>*2</sup>(2)) = (3 - (1 + 1.2 * 2))<sup>2</sup> = 0.04
>     *   *L*(5, *f<sup>*2</sup>(3)) = (5 - (1 + 1.2 * 3))<sup>2</sup> = 0.04
>
> Agora, podemos calcular o *Err<sub>boot</sub>*:
>
> $$Err_{boot} = \frac{1}{2*3} * (0 + 1 + 4 + 0.04 + 0.04 + 0.04) = \frac{1}{6} * 5.12 = 0.853$$
>
> Este exemplo ilustra como o *Err<sub>boot</sub>* √© calculado. No entanto, esse valor pode ser otimista, especialmente em casos de *overfitting*, onde o modelo se ajusta muito bem aos dados de treino, e generaliza mal para novos dados.

### Limita√ß√µes do Bootstrap
O bootstrap, apesar de ser uma ferramenta poderosa, apresenta limita√ß√µes importantes ao ser aplicado na estima√ß√£o de erros de predi√ß√£o. Duas das principais limita√ß√µes s√£o o vi√©s causado pelo *overfitting* e a influ√™ncia do tamanho da amostra nos resultados.

#### Overfitting e Amostras Bootstrap
Uma das principais limita√ß√µes do bootstrap para estimar o erro de predi√ß√£o √© que os dados bootstrap s√£o derivados do conjunto de dados original [^7.11]. O uso do conjunto de dados de treinamento original como "conjunto de teste" para as previs√µes de bootstrap leva a um vi√©s de otimismo, pois os modelos tendem a ter um bom desempenho nos dados com os quais foram treinados [^7.4]. Isso ocorre porque as amostras bootstrap compartilham observa√ß√µes com o conjunto de dados original. Como resultado, o modelo se ajusta a esses dados, superestimando seu desempenho real em dados n√£o vistos.

> ‚ö†Ô∏è **Nota Importante:** O erro estimado pelo bootstrap (Err<sub>boot</sub>) tende a ser muito menor do que o erro verdadeiro, particularmente quando h√° *overfitting*, pois o modelo tem um desempenho melhor em amostras que se assemelham aos dados de treinamento. **Refer√™ncia ao t√≥pico [^7.11]**.
```mermaid
graph TB
    subgraph "Overfitting Bias in Bootstrap"
        direction TB
        A["Original Data: Z"]
        B["Bootstrap Samples: Z*b (with repetition from Z)"]
        C["Model Trained on Z*b: f*b(x)"]
        D["Evaluation on Z: L(y, f*b(x))"]
        E["Err_boot: Underestimates True Error"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```

> üí° **Exemplo Num√©rico:**
> Considere o mesmo conjunto de dados do exemplo anterior, e suponha que ajustemos um modelo muito flex√≠vel, por exemplo, um modelo polinomial de alta ordem ou uma rede neural com muitas camadas, resultando em um modelo que se ajusta perfeitamente aos dados.  Ao fazer o bootstrap, as amostras ter√£o repeti√ß√µes dos dados de treino, portanto o modelo tender√° a ter um erro muito pr√≥ximo de 0.  Assim, o *Err<sub>boot</sub>* subestimar√° o erro real, pois o modelo n√£o consegue generalizar bem para dados novos. Por exemplo, se o verdadeiro erro fosse 2, o *Err<sub>boot</sub>* pode ser 0.5, indicando que o modelo tem um desempenho melhor do que realmente tem.

#### Vi√©s de Tamanho de Amostra

A natureza do bootstrap, onde os dados s√£o amostrados com reposi√ß√£o, faz com que cada conjunto de dados de bootstrap tenha em m√©dia aproximadamente 0.632*N* observa√ß√µes √∫nicas do conjunto original de *N* observa√ß√µes. [^7.11]. Em outras palavras, cada conjunto de dados bootstrap √© apenas ligeiramente diferente do conjunto de dados de treinamento original. Assim, a estimativa resultante, ao usar esses dados como um conjunto de teste, √© uma estimativa de desempenho do modelo em dados que s√£o semelhantes aos dados de treinamento, e n√£o em dados totalmente novos. Al√©m disso, em cen√°rios onde o desempenho do modelo muda com o tamanho do conjunto de treinamento (uma caracter√≠stica das curvas de aprendizado), a estimativa bootstrap pode n√£o capturar corretamente a capacidade de generaliza√ß√£o do modelo, resultando em um vi√©s dependente do tamanho da amostra.
```mermaid
graph TB
    subgraph "Sample Size Bias in Bootstrap"
        direction TB
        A["Original Data: Z (Size N)"]
        B["Bootstrap Sample: Z*b (Avg Unique Size ~0.632N)"]
        C["Model Trained on Z*b: f*b(x)"]
        D["Evaluation on Z: L(y, f*b(x))"]
        E["Err_boot: Biased by Sample Size"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```

> ‚ùó **Ponto de Aten√ß√£o:** O tamanho m√©dio das amostras √∫nicas de bootstrap √© de aproximadamente 0.632*N*. Isso significa que o modelo √© avaliado em dados muito semelhantes aos de treinamento, potencialmente levando a um vi√©s. **Conforme indicado em [^7.11]**.

> üí° **Exemplo Num√©rico:**
> Imagine que temos um conjunto de dados com *N*=10. A m√©dia do n√∫mero de observa√ß√µes √∫nicas em uma amostra bootstrap ser√° aproximadamente 0.632 * 10 = 6.32. Isso significa que, em m√©dia, cada amostra bootstrap ter√° cerca de 6 observa√ß√µes originais e algumas repeti√ß√µes. Se o modelo √© muito sens√≠vel ao tamanho do conjunto de treinamento, a performance medida no bootstrap ser√° diferente da performance do modelo treinado com um conjunto de dados totalmente novo com N=10, induzindo a um vi√©s. Por exemplo, se tiv√©ssemos *N*=100, em m√©dia ter√≠amos 63.2 observa√ß√µes √∫nicas, que seria mais representativo de um novo conjunto de dados e o vi√©s seria menor.
>
> Para visualizar o efeito do tamanho da amostra, vamos simular um exemplo usando python e sklearn:
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
> from sklearn.metrics import mean_squared_error
> from sklearn.model_selection import train_test_split
>
> def generate_data(n_samples, noise_std):
>    np.random.seed(42)
>    X = np.sort(5 * np.random.rand(n_samples, 1), axis=0)
>    y = 2 * X.squeeze() + 1 + np.random.normal(0, noise_std, n_samples)
>    return X, y
>
> def bootstrap_error(X, y, B):
>    n_samples = len(y)
>    mse_boot = []
>    for _ in range(B):
>        indices = np.random.choice(n_samples, n_samples, replace=True)
>        X_boot, y_boot = X[indices], y[indices]
>        model = LinearRegression().fit(X_boot, y_boot)
>        y_pred = model.predict(X)
>        mse_boot.append(mean_squared_error(y, y_pred))
>    return np.mean(mse_boot)
>
> # Comparando diferentes tamanhos de amostra:
> n_samples_small = 20
> n_samples_large = 100
> noise_std = 1
> B = 500
>
> X_small, y_small = generate_data(n_samples_small, noise_std)
> X_large, y_large = generate_data(n_samples_large, noise_std)
>
> mse_boot_small = bootstrap_error(X_small, y_small, B)
> mse_boot_large = bootstrap_error(X_large, y_large, B)
>
> # Estimando o erro usando valida√ß√£o cruzada para compara√ß√£o:
> X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(X_small, y_small, test_size=0.2, random_state=42)
> X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(X_large, y_large, test_size=0.2, random_state=42)
>
> model_small = LinearRegression().fit(X_train_small, y_train_small)
> mse_cv_small = mean_squared_error(y_test_small, model_small.predict(X_test_small))
>
> model_large = LinearRegression().fit(X_train_large, y_train_large)
> mse_cv_large = mean_squared_error(y_test_large, model_large.predict(X_test_large))
>
> print(f"Tamanho da amostra pequena (N={n_samples_small}):")
> print(f"  Erro Bootstrap: {mse_boot_small:.3f}")
> print(f"  Erro Valida√ß√£o Cruzada: {mse_cv_small:.3f}")
> print(f"Tamanho da amostra grande (N={n_samples_large}):")
> print(f"  Erro Bootstrap: {mse_boot_large:.3f}")
> print(f"  Erro Valida√ß√£o Cruzada: {mse_cv_large:.3f}")
> ```
>
> Este exemplo ir√° gerar dados, realizar bootstrap e calcular o erro. √â poss√≠vel observar que para tamanhos de amostras pequenas, a estimativa bootstrap ter√° uma performance melhor, induzindo ao vi√©s.
>
> | Tamanho da Amostra | Erro Bootstrap | Erro Valida√ß√£o Cruzada |
> |--------------------|---------------|-----------------------|
> | Pequeno (N=20)     | 1.022         |  2.040        |
> | Grande (N=100)     | 0.885         | 0.974         |
>
> A tabela acima ilustra a diferen√ßa entre o erro bootstrap e o erro da valida√ß√£o cruzada para amostras pequenas e grandes. Para amostras pequenas, o erro bootstrap tende a ser menor que o erro da valida√ß√£o cruzada.

#### O Estimador Leave-One-Out Bootstrap
Para mitigar os problemas de *overfitting*, uma abordagem chamada "leave-one-out bootstrap" √© utilizada, que mant√©m apenas as previs√µes de amostras de bootstrap que n√£o incluem a observa√ß√£o usada para fazer a predi√ß√£o.
O erro "leave-one-out" bootstrap √© dado por:
$$Err_{LOO} = \frac{1}{N}\sum_{i=1}^{N}\frac{1}{|C^{-i}|}\sum_{b \in C^{-i}}L(y_i,f^{*b}(x_i))$$
onde C<sup>-i</sup> √© o conjunto de todos os √≠ndices de amostras bootstrap que n√£o incluem a observa√ß√£o *z<sub>i</sub>*.  Esta abordagem √© similar √† valida√ß√£o cruzada "leave-one-out", mas com as propriedades de reamostragem do bootstrap.
```mermaid
graph TB
    subgraph "Leave-One-Out Bootstrap Error"
        direction TB
        A["Original Data: Z = {z1, z2, ..., zN}"]
        B["For each zi, Identify Bootstrap Samples Z*b NOT containing zi"]
         C["Fit Model on Each Z*b: f*b(x)"]
        D["Calculate Loss L(yi, f*b(xi)) only for samples without zi"]
        E["Average Loss: Err_LOO"]
        A --> B
        B --> C
        C --> D
         D --> E
    end
```

> üí° **Exemplo Num√©rico:**
> Voltando ao nosso exemplo inicial com os dados *Z* = {(*x*<sub>1</sub>=1, *y*<sub>1</sub>=2), (*x*<sub>2</sub>=2, *y*<sub>2</sub>=3), (*x*<sub>3</sub>=3, *y*<sub>3</sub>=5)} e as amostras bootstrap *Z<sup>*1</sup>* e *Z<sup>*2</sup>*.
> Para calcular o *Err<sub>LOO</sub>*, seguimos os seguintes passos:
> * Para o ponto *z*<sub>1</sub>:
>    * *C<sup>-1</sup>* = {2}, pois *Z<sup>*2</sup>* n√£o inclui *z*<sub>1</sub>
>    *  $L(y_1, f^{*2}(x_1)) =  (2 - (1 + 1.2 * 1))^2 = 0.04 $
> * Para o ponto *z*<sub>2</sub>:
>   *  *C<sup>-2</sup>* = {1}, pois *Z<sup>*1</sup>* n√£o inclui *z*<sub>2</sub>
>   *  $L(y_2, f^{*1}(x_2)) = (3 - (1.5 + 0.5 * 2))^2 = 1$
> * Para o ponto *z*<sub>3</sub>:
>    * *C<sup>-3</sup>* = {1}, pois *Z<sup>*1</sup>* n√£o inclui *z*<sub>3</sub>
>    *  $L(y_3, f^{*1}(x_3)) = (5 - (1.5 + 0.5 * 3))^2 = 4$
>
> $$Err_{LOO} = \frac{1}{3} * (\frac{0.04}{1} + \frac{1}{1} + \frac{4}{1}) = \frac{5.04}{3} = 1.68$$
>
> Observe que *Err<sub>LOO</sub>* (1.68) √© maior que *Err<sub>boot</sub>* (0.853) para este exemplo, e, portanto, √© uma estimativa menos otimista.

#### O Estimador .632+
Uma outra forma de mitigar o vi√©s do tamanho da amostra,  √© o estimador .632+, dado por:
$$Err^{(.632+)} = (1 - \omega) \cdot err + \omega \cdot Err_{LOO}$$
onde $\omega$ √© um fator que depende da taxa de *overfitting* relativa *R*:
$$\omega = \frac{0.632}{1-0.368R}$$
e *R* √© definido como:
$$R = \frac{Err_{LOO} - err}{\hat{\gamma} - err}$$
onde $\hat{\gamma}$ √© a taxa de erro quando predizemos os r√≥tulos da classe de forma aleat√≥ria. Este estimador procura combinar a estimativa de *erro* com a estimativa de *leave-one-out* usando um peso dependente do n√≠vel de *overfitting*.
```mermaid
graph TB
    subgraph ".632+ Error Estimator"
        direction TB
        A["Training Error: err"]
        B["Leave-One-Out Error: Err_LOO"]
        C["Random Error Rate: Œ≥ÃÇ"]
        D["Overfitting Rate: R = (Err_LOO - err) / (Œ≥ÃÇ - err)"]
        E["Weight: œâ = 0.632 / (1 - 0.368R)"]
        F[".632+ Error: Err(.632+) = (1 - œâ) * err + œâ * Err_LOO"]
        A --> D
        B --> D
        C --> D
        D --> E
        E --> F
        A --> F
        B --> F

    end
```

> üí° **Exemplo Num√©rico:**
> Usando os valores do exemplo anterior, suponha que o erro de treinamento *err* (calculado usando o modelo ajustado em todo o conjunto de dados *Z*) seja 0.6 e que a taxa de erro aleat√≥ria $\hat{\gamma}$ seja 2. Temos *Err<sub>LOO</sub>* = 1.68. Calculamos *R*:
>
> $$R = \frac{1.68 - 0.6}{2 - 0.6} = \frac{1.08}{1.4} \approx 0.77$$
>
> Em seguida, calculamos $\omega$:
>
> $$\omega = \frac{0.632}{1 - 0.368 * 0.77} \approx \frac{0.632}{1 - 0.283} \approx \frac{0.632}{0.717} \approx 0.88$$
>
> Finalmente, calculamos *Err<sup>(.632+)</sup>*:
>
> $$Err^{(.632+)} = (1 - 0.88) * 0.6 + 0.88 * 1.68 \approx 0.12 * 0.6 + 0.88 * 1.68 = 0.072 + 1.478 \approx 1.55$$
>
> Este exemplo ilustra como o estimador .632+ combina *err* e *Err<sub>LOO</sub>*, ajustando o peso para lidar com o *overfitting*.
>
> Vamos criar uma fun√ß√£o em python para calcular o erro .632+ para comparar com os outros m√©todos:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
> from sklearn.metrics import mean_squared_error
> from sklearn.model_selection import train_test_split
>
> def generate_data(n_samples, noise_std):
>    np.random.seed(42)
>    X = np.sort(5 * np.random.rand(n_samples, 1), axis=0)
>    y = 2 * X.squeeze() + 1 + np.random.normal(0, noise_std, n_samples)
>    return X, y
>
> def bootstrap_error(X, y, B):
>    n_samples = len(y)
>    mse_boot = []
>    for _ in range(B):
>        indices = np.random.choice(n_samples, n_samples, replace=True)
>        X_boot, y_boot = X[indices], y[indices]
>        model = LinearRegression().fit(X_boot, y_boot)
>        y_pred = model.predict(X)
>        mse_boot.append(mean_squared_error(y, y_pred))
>    return np.mean(mse_boot)
>
> def loo_bootstrap_error(X, y, B):
>    n_samples = len(y)
>    loo_mse = np.zeros(n_samples)
>    for i in range(n_samples):
>        indices_without_i = []
>        for _ in range(B):
>            indices = np.random.choice(n_samples, n_samples, replace=True)
>            if i not in indices:
>                indices_without_i.append(indices)
>        if indices_without_i:
>            mse_temp = []
>            for ind in indices_without_i:
>                 X_boot, y_boot = X[ind], y[ind]
>                 model = LinearRegression().fit(X_boot, y_boot)
>                 y_pred = model.predict(X[[i]])
>                 mse_temp.append((y[i] - y_pred) ** 2)
>            loo_mse[i] = np.mean(mse_temp)
>
>    return np.mean(loo_mse)
>
> def point632_plus_error(X, y, B):
>    err = mean_squared_error(y, LinearRegression().fit(X, y).predict(X))
>    loo_err = loo_bootstrap_error(X, y, B)
>    y_random = np.random.choice(y, len(y))
>    gamma = mean_squared_error(y, y_random)
>    if gamma == err: return loo_err
>    R = (loo_err - err) / (gamma - err)
>    omega = 0.632 / (1 - 0.368 * R)
>    return (1 - omega) * err + omega * loo_err
>
>
> # Comparando diferentes tamanhos de amostra:
> n_samples = 20
> noise_std = 1
> B = 500
>
> X, y = generate_data(n_samples, noise_std)
>
> mse_boot = bootstrap_error(X, y, B)
> mse_loo_boot = loo_bootstrap_error(X, y, B)
> mse_point632plus = point632_plus_error(X, y, B)
>
> # Estimando o erro usando valida√ß√£o cruzada para compara√ß√£o:
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
> model = LinearRegression().fit(X_train, y_train)
> mse_cv = mean_squared_error(y_test, model.predict(X_test))
>
> print(f"Tamanho da amostra (N={n_samples}):")
> print(f"  Erro Bootstrap: {mse_boot:.3f}")
> print(f"  Erro Leave-One-Out Bootstrap: {mse_loo_boot:.3f}")
> print(f"  Erro .632+: {mse_point632plus:.3f}")
> print(f"  Erro Valida√ß√£o Cruzada: {mse_cv:.3f}")
> ```
>
> A execu√ß√£o deste c√≥digo, com *N*=20, gera a seguinte tabela:
>
> | M√©todo                   | MSE    |
> |--------------------------|--------|
> | Bootstrap                |  1.022   |
> | Leave-One-Out Bootstrap  | 2.431   |
> | .632+                    | 2.011   |
> | Valida√ß√£o Cruzada        | 2.040   |
>
> Onde observamos que o erro bootstrap √© menor do que os outros m√©todos, e o erro .632+ √© mais pr√≥ximo do erro da valida√ß√£o cruzada.

### Compara√ß√£o com Outras T√©cnicas
Em compara√ß√£o com a valida√ß√£o cruzada, o bootstrap tenta estimar a variabilidade do modelo atrav√©s do reajuste sobre amostras com substitui√ß√£o [^7.11]. Por outro lado, a valida√ß√£o cruzada estima o desempenho do modelo com amostras que n√£o foram usadas para o treinamento.
Os crit√©rios AIC e BIC s√£o m√©todos anal√≠ticos que derivam estimativas de erro atrav√©s de aproxima√ß√µes e penalidades por complexidade do modelo [^7.5], [^7.7]. O bootstrap e a valida√ß√£o cruzada, no entanto, s√£o m√©todos computacionalmente intensivos que n√£o dependem de suposi√ß√µes espec√≠ficas sobre a distribui√ß√£o dos dados, tornando-os adequados em v√°rias situa√ß√µes, mesmo em modelos complexos e altamente n√£o-lineares. No entanto, em problemas onde o tamanho da amostra √© pequeno, o bootstrap pode apresentar melhores resultados por n√£o depender de um conjunto de valida√ß√£o independente.

### Pergunta Te√≥rica Avan√ßada: Como o *bias-variance tradeoff* √© afetado no bootstrap quando o tamanho da amostra √© pequeno?
**Resposta:**
O *bias-variance tradeoff* √© afetado no bootstrap principalmente pelo vi√©s resultante da reamostragem com reposi√ß√£o, especialmente quando o tamanho da amostra √© pequeno. Quando o tamanho da amostra √© pequeno, a probabilidade de amostras bootstrap conterem observa√ß√µes repetidas aumenta, o que faz com que o modelo aprenda as particularidades dessas observa√ß√µes repetidas, levando a uma redu√ß√£o da vari√¢ncia, mas, simultaneamente, a um aumento do vi√©s.
O *bias* ocorre devido √† depend√™ncia entre as amostras bootstrap e o conjunto de dados original, causando uma estimativa de erro otimista.  A vari√¢ncia, por outro lado, refere-se √† variabilidade da estimativa de erro do modelo em diferentes amostras bootstrap, e tende a ser menor do que o esperado se o conjunto de valida√ß√£o fosse completamente independente.
Formalmente, para o bootstrap, o *bias* √© dado pela diferen√ßa entre a esperan√ßa do estimador do erro de predi√ß√£o com a verdadeira esperan√ßa do erro de predi√ß√£o. A vari√¢ncia pode ser obtida por:
$$Var(Err_{boot}) = \frac{1}{B-1}\sum_{b=1}^{B} (Err_b - Err_{boot})^2$$
onde *Err<sub>b</sub>* √© a estimativa de erro para o b-√©simo conjunto de dados de bootstrap. Este valor subestima a vari√¢ncia real.
Para o cen√°rio de amostra pequena, o efeito de uma amostra que cont√©m v√°rios registros repetidos da amostra original faz com que o *bias* aumente, pois o modelo √© treinado em dados redundantes. Al√©m disso, amostras diferentes do bootstrap podem produzir modelos muito similares, diminuindo artificialmente a vari√¢ncia do erro estimado.
```mermaid
graph TB
    subgraph "Bias-Variance Tradeoff in Bootstrap"
        direction TB
        A["Small Sample Size"]
        B["High Probability of Repeated Observations in Bootstrap"]
        C["Reduced Variance in Bootstrap Error Estimate"]
        D["Increased Bias in Bootstrap Error Estimate"]
        E["Model Overfits on Redundant Data"]
        A --> B
        B --> C
        B --> D
        D --> E
    end
```

**Lemma 1:** Para uma amostra *Z* de tamanho *N*, seja *Z<sup>b</sup>* a b-√©sima amostra bootstrap de tamanho *N* obtida por reamostragem com reposi√ß√£o. Seja *f(Z)* o modelo ajustado a *Z* e *f(Z<sup>b</sup>)* o modelo ajustado a *Z<sup>b</sup>*. O vi√©s da estimativa bootstrap de erro de predi√ß√£o (*Err<sub>boot</sub>*) √© dado por:
$$Bias(Err_{boot}) = E_z[E_{Z^b}[L(Y,f(Z^b))|Z]] - E_{z,Y}[L(Y,f(Z))]$$
onde *E<sub>z</sub>* √© a esperan√ßa sobre o conjunto de dados originais *Z* e *E<sub>Z<sup>b</sup></sub>* √© a esperan√ßa sobre o conjunto de amostras *Z<sup>b</sup>*. Para amostras pequenas, o bias √© negativo, dado que os modelos ajustados nas amostras bootstrap tendem a ter um desempenho otimista sobre a amostra *Z*.
```mermaid
graph TB
    subgraph "Bias of Bootstrap Error"
        direction TB
        A["Bias(Err_boot)"]
         B["Expected Loss on Bootstrap Samples (Conditional on Z): E_z[E_{Z^b}[L(Y,f(Z^b))|Z]]"]
        C["True Expected Loss: E_{z,Y}[L(Y,f(Z))]"]
        D["Bias(Err_boot) =  E_z[E_{Z^b}[L(Y,f(Z^b))|Z]] -  E_{z,Y}[L(Y,f(Z))]"]
        B --> D
        C --> D
    end
```

**Prova do Lemma 1:**
A defini√ß√£o de *bias* √© a diferen√ßa entre o valor esperado da estimativa e o verdadeiro valor esperado. O verdadeiro valor esperado do erro de predi√ß√£o seria:
$$E_{z,Y}[L(Y,f(Z))]$$
J√° o valor esperado da estimativa bootstrap √©:
$$E_z[E_{Z^b}[L(Y,f(Z^b))|Z]]$$
Portanto o *bias* √© dado pela diferen√ßa entre esses valores:
$$Bias(Err_{boot}) = E_z[E_{Z^b}[L(Y,f(Z^b))|Z]] - E_{z,Y}[L(Y,f(Z))]$$
√â importante notar que a estimativa bootstrap tende a ser um estimador otimista, pois o conjunto de dados de treinamento e o conjunto de teste (amostras de bootstrap) s√£o correlacionados. Portanto, para conjuntos de dados pequenos, o *bias* √© negativo. $\blacksquare$

**Corol√°rio 1:** O estimador leave-one-out bootstrap (Err<sub>LOO</sub>) reduz o vi√©s do estimador bootstrap para conjuntos de dados pequenos. O vi√©s do leave-one-out bootstrap √© dado por:
$$Bias(Err_{LOO}) = E_z[E_{Z^b}[L(Y,f(Z^b))|Z]] - E_{z,Y}[L(Y,f(Z))]$$
onde apenas amostras bootstrap que n√£o incluem o i-√©simo registro s√£o usadas. Este estimador √© menos otimista porque a amostra de teste e o treinamento s√£o menos correlacionados.
```mermaid
graph TB
    subgraph "Bias of Leave-One-Out Bootstrap Error"
        direction TB
       A["Bias(Err_LOO)"]
         B["Expected Loss on Leave-One-Out Bootstrap Samples: E_z[E_{Z^b}[L(Y,f(Z^b))|Z]]"]
        C["True Expected Loss: E_{z,