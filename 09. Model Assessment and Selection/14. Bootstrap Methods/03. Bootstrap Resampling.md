## Avalia√ß√£o e Sele√ß√£o de Modelos com Bootstrap Resampling

```mermaid
graph LR
    subgraph "Model Evaluation and Selection"
        direction TB
        A["Model Assessment"]
        B["Model Selection"]
        C["Generalization Performance"]
        D["Bias-Variance Tradeoff"]
        E["Bootstrap Resampling"]
        F["Cross-Validation"]
        G["AIC/BIC"]
        A --> C
        A --> D
        A --> E
        A --> F
        A --> G
        B --> C
        B --> D
        B --> E
        B --> F
        B --> G
    end
```

### Introdu√ß√£o
A capacidade de um m√©todo de aprendizado de generalizar, ou seja, prever resultados em dados de teste independentes, √© crucial na pr√°tica [^7.1]. A avalia√ß√£o dessa capacidade orienta a escolha de modelos e nos fornece uma medida da qualidade do modelo selecionado. Este cap√≠tulo aborda os principais m√©todos de avalia√ß√£o de desempenho e como eles s√£o utilizados na sele√ß√£o de modelos, iniciando com a discuss√£o do tradeoff entre **bias, variance e complexidade do modelo** [^7.1].

### Conceitos Fundamentais
**Conceito 1: Generaliza√ß√£o e Erro de Predi√ß√£o**
A **generaliza√ß√£o** de um modelo refere-se √† sua capacidade de realizar previs√µes precisas em dados n√£o vistos [^7.1]. O **erro de predi√ß√£o** √© a m√©trica usada para quantificar o qu√£o bem um modelo se ajusta aos dados de teste, medindo a discrep√¢ncia entre as previs√µes e os valores reais. A complexidade do modelo afeta esse erro, com modelos simples tendendo a ter um alto *bias* e baixa *variance*, enquanto modelos complexos tendendo a ter baixo *bias* e alta *variance* [^7.2]. O objetivo √© encontrar um equil√≠brio entre os dois para minimizar o erro total. Um erro de previs√£o muito baixo nos dados de treinamento (um modelo "overfit") geralmente implica numa m√° generaliza√ß√£o.

**Lemma 1:** A decomposi√ß√£o do erro de predi√ß√£o em *bias* e *variance*.
Dado um modelo $\hat{f}(X)$ estimado a partir de um conjunto de treinamento $T$ e um valor alvo $Y$, o erro de predi√ß√£o em um ponto $x_0$ pode ser decomposto em:
$$ Err(x_0) = E[(Y - \hat{f}(x_0))^2 | X = x_0] = \sigma^2 + Bias^2(\hat{f}(x_0)) + Var(\hat{f}(x_0)) $$
Onde $\sigma^2$ √© a *variance* do erro irredut√≠vel, $Bias^2$ √© o quadrado do *bias* (a diferen√ßa entre o valor esperado da estimativa e o valor verdadeiro) e $Var$ √© a *variance* da estimativa [^7.3]. Este lemma fundamenta a compreens√£o de que modelos complexos podem reduzir o *bias*, mas aumentar a *variance*, e vice-versa. A prova formal detalhada est√° dispon√≠vel na se√ß√£o 7.3 do texto [^7.3]. $\blacksquare$

```mermaid
graph TB
    subgraph "Prediction Error Decomposition"
        direction TB
        A["Err(x_0)"]
        B["œÉ¬≤ (Irreducible Error)"]
        C["Bias¬≤(fÃÇ(x_0))"]
        D["Var(fÃÇ(x_0))"]
        A --> B
        A --> C
        A --> D
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:1px
    style C fill:#ccf,stroke:#333,stroke-width:1px
    style D fill:#ccf,stroke:#333,stroke-width:1px
```

> üí° **Exemplo Num√©rico:** Considere um cen√°rio onde queremos prever a altura de uma pessoa (Y) com base no seu peso (X).
>
> 1. **Modelo Simples (Alto Bias, Baixa Variance):** Um modelo linear simples, $\hat{f}(X) = 1.0 + 0.1X$, pode n√£o capturar bem a rela√ß√£o real, especialmente em pesos extremos. Suponha que a altura real para um peso de 80 kg seja 1.80m. O modelo pode prever, em m√©dia, 1.0 + 0.1 * 80 = 1.80m. Por√©m, devido a sua simplicidade, o modelo ter√° alta tend√™ncia (alto bias), e pouca variabilidade de estimativa em diferentes datasets (baixa variance).
>
> 2. **Modelo Complexo (Baixo Bias, Alta Variance):** Um modelo polinomial de alta ordem, $\hat{f}(X) = 0.5 + 0.2X - 0.001X^2$, pode se ajustar muito bem aos dados de treinamento, incluindo ru√≠do, mas pode ter grande varia√ß√£o em diferentes amostras. Em uma amostra, pode prever 1.75m para 80kg e em outra amostra, 1.85m. O modelo, agora, ter√° baixa tend√™ncia (baixo bias), e alta variabilidade de estimativa (alta variance).
>
>  Vamos considerar que o valor real da altura para um peso de 80kg seja 1.80m, com um erro irredut√≠vel ($\sigma^2$) de 0.01.
>
>   - Para o **modelo simples**: $Bias^2$ pode ser (1.82-1.80)^2 = 0.0004 e a $Var$ ser 0.0001.
>    - Para o **modelo complexo**: $Bias^2$ pode ser (1.805-1.80)^2 = 0.000025 e a $Var$ ser 0.01.
>
>  Podemos calcular o erro em $x_0 = 80$ para ambos os modelos:
>
>  - $Err_{simples}(80) = 0.01 + 0.0004 + 0.0001 = 0.0105$
>   - $Err_{complexo}(80) = 0.01 + 0.000025 + 0.01 = 0.020025$
>
> Este exemplo ilustra que, embora o modelo complexo tenha um *bias* menor, a sua *variance* maior leva a um erro total maior, destacando o tradeoff entre *bias* e *variance*. Um modelo ideal buscaria um equil√≠brio que minimize a soma dos dois componentes, incluindo o erro irredut√≠vel.
>
> ```mermaid
> graph LR
> A[Modelo Simples] -->|Alto Bias, Baixa Variance| B(Erro Total: 0.0105);
> C[Modelo Complexo] -->|Baixo Bias, Alta Variance| D(Erro Total: 0.020025);
> ```

**Conceito 2: Test Error, Expected Test Error e Training Error**
O **test error** ($Err_T$) √© o erro de predi√ß√£o em um conjunto de teste espec√≠fico, medido como a m√©dia da fun√ß√£o de perda $L$ sobre os dados de teste, dados um conjunto de treinamento fixo $T$ [^7.2]:
$$Err_T = E[L(Y, f(X)) | T]$$
O **expected test error** ($Err$) √© a m√©dia do test error sobre todos os poss√≠veis conjuntos de treinamento [^7.2]:
$$Err = E[Err_T] = E[L(Y, f(X))]$$
O **training error** (*err*) √© a m√©dia da fun√ß√£o de perda sobre o conjunto de treinamento utilizado para ajustar o modelo [^7.2]:
$$err = \frac{1}{N}\sum_{i=1}^N L(Y_i, f(x_i))$$
O *training error* geralmente √© uma estimativa otimista do *test error*, pois o modelo j√° est√° ajustado aos dados de treinamento [^7.2].

```mermaid
graph LR
    subgraph "Error Types"
        direction TB
        A["Training Error (err)"]
        B["Test Error (Err_T)"]
        C["Expected Test Error (Err)"]
        A -->|Optimistic Estimate| B
        B --> C
    end
```

> üí° **Exemplo Num√©rico:** Considere um modelo de regress√£o linear tentando prever o pre√ßo de uma casa com base no seu tamanho em metros quadrados.
>
> 1.  **Training Error:** Ajustamos o modelo linear em um conjunto de 100 casas e obtemos um erro quadr√°tico m√©dio de treinamento de *err* = 5000. Este erro √© calculado usando os mesmos dados que foram usados para construir o modelo.
>
> 2.  **Test Error:** Avaliamos o mesmo modelo usando um conjunto de teste separado de 50 casas e obtemos um erro quadr√°tico m√©dio de *Err_T* = 7000. Este valor √© maior do que o erro de treinamento, refletindo o fato de que o modelo n√£o se ajustou t√£o bem em novos dados.
>
> 3.  **Expected Test Error:** Se repetirmos o processo v√°rias vezes, selecionando diferentes conjuntos de treinamento e teste, e calculamos a m√©dia dos *test errors* obtidos, chegamos ao *expected test error*. Assumindo que isso foi feito 10 vezes, com *test errors* variando entre 6500 e 8000, podemos obter um *Err*  = 7200.
>
>  Este exemplo ilustra como o *training error* pode ser uma estimativa otimista do desempenho do modelo, e como o *test error* em um conjunto de teste espec√≠fico pode diferir do erro m√©dio esperado.

**Corol√°rio 1:** A diferen√ßa entre *training error* e *test error*.
Devido ao sobreajuste (*overfitting*), o *training error* tende a ser menor que o *test error*, especialmente em modelos complexos. A diferen√ßa entre os dois erros √© chamada de **otimismo**, que pode ser estimada para ajustar o *training error* e torn√°-lo um preditor mais acurado do erro de generaliza√ß√£o [^7.4].

**Conceito 3: Log-Likelihood e Deviance**
Para respostas qualitativas ou categ√≥ricas, as **probabilidades** $p_k(X) = Pr(G = k | X)$ s√£o modeladas para cada classe $k$. A **fun√ß√£o de perda** ( *loss function* ) pode ser o *0-1 loss* ou o **log-likelihood** [^7.2]:
$$L(G, P(X)) = -2\sum_{k=1}^K I(G=k)\log(p_k(X)) = -2\log(p_G(X))$$
Onde o termo $-2 \times log-likelihood$ √© frequentemente referido como **deviance**. O *training error* √© a vers√£o an√°loga da amostragem da log-likelihood [^7.2].

```mermaid
graph LR
    subgraph "Log-Likelihood and Deviance"
        direction TB
        A["Loss Function (L(G, P(X)))"]
        B["-2 * Œ£ I(G=k) * log(p_k(X))"]
        C["-2 * log(p_G(X)) (Deviance)"]
        A --> B
        B --> C
    end
```

> üí° **Exemplo Num√©rico:** Imagine que estamos classificando e-mails em duas categorias: "spam" e "n√£o spam" (K=2).
>
> 1. **Modelo 1 (Pior):** Ap√≥s treinar um modelo, para um e-mail que √© realmente "spam" (G=1), o modelo prev√™ com probabilidade $p_1(X) = 0.1$ (spam) e $p_2(X) = 0.9$ (n√£o spam).
>
> 2. **Modelo 2 (Melhor):** Para o mesmo e-mail "spam", um modelo melhor prev√™ com probabilidade $p_1(X) = 0.8$ (spam) e $p_2(X) = 0.2$ (n√£o spam).
>
> A fun√ß√£o de perda log-likelihood para o modelo 1 (pior) √©:
>
> $L_1(G, P(X)) = -2 \log(0.1) \approx 4.60$
>
> A fun√ß√£o de perda log-likelihood para o modelo 2 (melhor) √©:
>
> $L_2(G, P(X)) = -2 \log(0.8) \approx 0.44$
>
> Como podemos ver, o modelo melhor (modelo 2), tem um menor valor para a fun√ß√£o de perda log-likelihood, o que indica um melhor ajuste aos dados. Em uma amostra de 100 e-mails, a m√©dia do log-likelihood, ou deviance, seria usada para avaliar o desempenho geral do classificador.

> ‚ö†Ô∏è **Nota Importante**: A escolha da fun√ß√£o de perda ( *loss function* ) influencia o comportamento do modelo e a interpreta√ß√£o dos erros.

> ‚ùó **Ponto de Aten√ß√£o**: O *training error* tende a ser otimista, sendo necess√°rio m√©todos para estimar corretamente o erro de generaliza√ß√£o.

> ‚úîÔ∏è **Destaque**: As defini√ß√µes de *bias* e *variance* s√£o adaptadas para regress√£o e classifica√ß√£o, mantendo a mesma l√≥gica fundamental de tradeoff.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o
```mermaid
graph LR
    subgraph "Linear Regression for Classification"
        direction LR
        A["Input Data (X)"] --> B["Indicator Matrix (Y)"]
        B --> C["Linear Regression (OLS)"]
        C --> D["Estimated Coefficients (Œ≤)"]
        D --> E["Decision Function"]
        E --> F["Class Predictions"]
        F --> G["Evaluation"]
    end
```

A **regress√£o linear** aplicada a uma matriz de indicadores de classes √© uma t√©cnica que busca ajustar um modelo linear para cada classe, onde as vari√°veis dependentes representam a pertin√™ncia de uma observa√ß√£o a cada classe [^7.2]. Cada classe √© codificada como um vetor de indicadores (1 para a classe correspondente, 0 para as demais), e a regress√£o √© aplicada separadamente a cada vetor. Os **coeficientes** estimados por m√≠nimos quadrados (OLS - *Ordinary Least Squares*) s√£o usados para construir uma fun√ß√£o discriminante linear que define as regi√µes de decis√£o.
Apesar de simples, essa abordagem pode sofrer algumas limita√ß√µes:
1. **Extrapola√ß√£o:** As previs√µes podem cair fora do intervalo [0,1], especialmente quando aplicado a dados fora do conjunto de treinamento.
2. **"Masking problem"**: Para mais de duas classes, o fato da fun√ß√£o de regress√£o poder n√£o representar o limiar para cada classe corretamente.
3. **Suposi√ß√£o de igual *variance* para todas as classes**: a qual pode n√£o corresponder ao caso real, levando a uma modelagem inadequada da fronteira de decis√£o. [^7.3]

> üí° **Exemplo Num√©rico:** Considere um problema de classifica√ß√£o com tr√™s classes (A, B, e C) usando duas vari√°veis preditoras (X1 e X2). Para utilizar regress√£o linear, primeiro transformamos a vari√°vel categ√≥rica de sa√≠da (Y) em tr√™s vari√°veis indicadoras: $Y_A$, $Y_B$, e $Y_C$.
>
>  | Observa√ß√£o | X1    | X2   | Classe (Y) | $Y_A$ | $Y_B$ | $Y_C$ |
>  | ---------- | ----- | ---- | ---------- | ----- | ----- | ----- |
>  | 1          | 2     | 3    | A          | 1     | 0     | 0     |
>  | 2          | 4     | 5    | B          | 0     | 1     | 0     |
>  | 3          | 6     | 7    | C          | 0     | 0     | 1     |
>  | 4          | 1     | 2    | A          | 1     | 0     | 0     |
>  | 5          | 3     | 4    | B          | 0     | 1     | 0     |
>
>  Aplicamos a regress√£o linear para cada classe, ou seja, tr√™s regress√µes independentes com as mesmas vari√°veis preditoras (X1 e X2), e as vari√°veis dependentes sendo $Y_A$, $Y_B$ e $Y_C$. Os resultados podem ser:
>
> $\hat{Y_A} =  0.8 - 0.1X_1 + 0.05X_2$
> $\hat{Y_B} =  0.1 + 0.2X_1 - 0.1X_2$
> $\hat{Y_C} =  0.2 - 0.1X_1 + 0.05X_2$
>
> Para classificar um novo ponto, por exemplo, (X1 = 5, X2 = 6), calcular√≠amos:
>
> $\hat{Y_A} = 0.8 - 0.1*5 + 0.05*6 = 0.8 - 0.5 + 0.3 = 0.6$
> $\hat{Y_B} = 0.1 + 0.2*5 - 0.1*6 = 0.1 + 1 - 0.6 = 0.5$
> $\hat{Y_C} = 0.2 - 0.1*5 + 0.05*6 = 0.2 - 0.5 + 0.3 = 0$
>
> A classe prevista seria A, pois tem o maior valor estimado. Este exemplo demonstra como a regress√£o linear pode ser usada para classifica√ß√£o, mas tamb√©m destaca que os valores podem extrapolar os limites [0,1].

**Lemma 2:** Equival√™ncia entre proje√ß√µes de regress√£o linear e discriminantes lineares.
Em condi√ß√µes de covari√¢ncias iguais e gaussianidade das distribui√ß√µes de cada classe, o hiperplano de decis√£o obtido atrav√©s da regress√£o linear da matriz de indicadores √© equivalente ao obtido via An√°lise Discriminante Linear (LDA). Essa equival√™ncia resulta da formula√ß√£o matem√°tica das fun√ß√µes discriminantes lineares em ambos os m√©todos e da otimiza√ß√£o via m√≠nimos quadrados na regress√£o linear. Essa equival√™ncia serve para formalizar o uso de regress√£o linear para classifica√ß√£o com classes gaussianas [^7.3.1]. $\blacksquare$
**Corol√°rio 2:** Simplifica√ß√£o da an√°lise de modelos de classifica√ß√£o linear com covari√¢ncias iguais.
Sob a condi√ß√£o de covari√¢ncias iguais, a an√°lise da fun√ß√£o de decis√£o gerada pela regress√£o linear para classifica√ß√£o pode ser simplificada, pois ela se torna equivalente √† proje√ß√£o gerada pelo LDA. Este resultado demonstra que a escolha do m√©todo de classifica√ß√£o pode ser influenciada pelas premissas sobre as distribui√ß√µes dos dados [^7.3.2].

‚ÄúEm alguns cen√°rios, a regress√£o log√≠stica pode fornecer estimativas mais est√°veis de probabilidade, enquanto a regress√£o de indicadores pode levar a extrapola√ß√µes fora de [0,1].‚Äù [^7.4]
‚ÄúNo entanto, h√° situa√ß√µes em que a regress√£o de indicadores √© suficiente e at√© mesmo vantajosa quando o objetivo principal √© a fronteira de decis√£o linear.‚Äù [^7.2]

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o
```mermaid
graph LR
    subgraph "Variable Selection and Regularization"
        direction TB
        A["Model Complexity"]
        B["Variable Selection"]
        C["Regularization"]
        D["L1 Penalty (Lasso)"]
        E["L2 Penalty (Ridge)"]
        F["Elastic Net"]
        A --> B
        A --> C
        C --> D
        C --> E
        C --> F
    end
```

A **sele√ß√£o de vari√°veis** e a **regulariza√ß√£o** s√£o t√©cnicas essenciais para lidar com a complexidade dos modelos de classifica√ß√£o, especialmente quando se tem um grande n√∫mero de preditores. A **regulariza√ß√£o** adiciona termos de penaliza√ß√£o √† fun√ß√£o de perda, limitando os coeficientes dos modelos para evitar o *overfitting* e promover a generaliza√ß√£o.
A **penaliza√ß√£o L1 (Lasso)**, por exemplo, adiciona √† fun√ß√£o de perda um termo proporcional √† soma dos valores absolutos dos coeficientes, o que tende a gerar modelos com coeficientes esparsos, ou seja, com v√°rios coeficientes iguais a zero, realizando assim uma sele√ß√£o de vari√°veis impl√≠cita [^7.4.4].
A **penaliza√ß√£o L2 (Ridge)** adiciona um termo proporcional √† soma dos quadrados dos coeficientes, o que tende a encolher os coeficientes em dire√ß√£o a zero, reduzindo a *variance* e melhorando a estabilidade do modelo [^7.5].
Combinar as duas abordagens leva ao m√©todo **Elastic Net**, que herda as vantagens de ambas, a esparsidade da L1 e a estabilidade da L2 [^7.5].

> üí° **Exemplo Num√©rico:** Imagine um modelo de regress√£o log√≠stica com muitos preditores, como o exemplo de classifica√ß√£o de pacientes com base em caracter√≠sticas gen√©ticas. O objetivo √© prever a probabilidade de uma pessoa desenvolver uma doen√ßa.
>
> 1. **Regress√£o Log√≠stica sem Regulariza√ß√£o:**
>    $\text{Logit}(p) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_{100} X_{100}$. O modelo sem regulariza√ß√£o pode se ajustar muito bem ao conjunto de treinamento, mas generalizar mal.
>
> 2. **Regress√£o Log√≠stica com Penaliza√ß√£o L1 (Lasso):**
>   $\text{Logit}(p) = \beta_0 + \beta_1 X_1 + \ldots + \beta_{100} X_{100} + \lambda \sum_{j=1}^{100} |\beta_j|$.
>   Se escolhermos um valor adequado de $\lambda$ (por exemplo, $\lambda = 0.1$), o Lasso pode zerar alguns coeficientes, por exemplo, $\beta_{3} = \beta_{10} = \beta_{25} = 0$. O modelo torna-se esparso, selecionando as vari√°veis mais relevantes:
>   $\text{Logit}(p) \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_4 X_4 + \ldots$
>
> 3. **Regress√£o Log√≠stica com Penaliza√ß√£o L2 (Ridge):**
>   $\text{Logit}(p) = \beta_0 + \beta_1 X_1 + \ldots + \beta_{100} X_{100} + \lambda \sum_{j=1}^{100} \beta_j^2$.
>   Com um valor adequado de $\lambda$ (por exemplo, $\lambda = 0.5$), o Ridge n√£o zera os coeficientes, mas os encolhe em dire√ß√£o a zero, reduzindo sua influ√™ncia e a vari√¢ncia do modelo.
>
> 4. **Elastic Net:** Uma combina√ß√£o das duas penalidades, que tem as vantagens de esparsidade e estabilidade.
>
> Esta tabela compara os m√©todos:
>
> | M√©todo       | Penalidade                               | Esparsidade | Estabilidade |
> |--------------|-----------------------------------------|-------------|-------------|
> | Sem Regulariza√ß√£o | Nenhuma                                  | N√£o         | Baixa       |
> | Lasso        | $\lambda \sum_{j=1}^{p} |\beta_j|$         | Sim         | M√©dia       |
> | Ridge        | $\lambda \sum_{j=1}^{p} \beta_j^2$      | N√£o         | Alta        |
> | Elastic Net | $\lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2$ | Sim        | Alta       |

**Lemma 3:** A penaliza√ß√£o L1 e a esparsidade.
A penaliza√ß√£o L1 aplicada a modelos lineares, como a regress√£o log√≠stica, induz a esparsidade na solu√ß√£o, ou seja, muitos coeficientes da fun√ß√£o discriminante tender√£o a ser zero. Isso pode ser formalmente demonstrado pela an√°lise das condi√ß√µes de otimalidade do problema de maximiza√ß√£o de verossimilhan√ßa com a penalidade L1, pois a restri√ß√£o da norma L1 leva a solu√ß√µes com coeficientes em cantos (vertices) de um espa√ßo de busca delimitado [^7.4.4]. $\blacksquare$
**Corol√°rio 3:** Impacto da esparsidade na interpretabilidade.
A esparsidade induzida pela penaliza√ß√£o L1 simplifica a an√°lise do modelo de classifica√ß√£o, pois apenas um subconjunto relevante das vari√°veis de entrada contribui para a decis√£o, tornando o modelo mais interpret√°vel e reduzindo a complexidade computacional [^7.4.5].

> ‚ö†Ô∏è **Ponto Crucial**: A combina√ß√£o de regulariza√ß√µes L1 e L2, como no Elastic Net, permite ajustar tanto a esparsidade quanto a estabilidade do modelo.

### Separating Hyperplanes e Perceptrons
A ideia de **hiperplanos separadores** √© fundamental em classifica√ß√£o linear, pois o objetivo √© encontrar um hiperplano que divida o espa√ßo de entrada em regi√µes correspondentes a diferentes classes. O objetivo passa a ser o de maximizar a margem de separa√ß√£o, que √© a dist√¢ncia entre o hiperplano e os pontos mais pr√≥ximos de cada classe (os *support vectors*). A formula√ß√£o deste problema de otimiza√ß√£o pode ser feita no espa√ßo primal ou dual. A solu√ß√£o deste problema de otimiza√ß√£o pode ser encontrada usando a t√©cnica do **dual de Wolfe**. [^7.5.2]
O **Perceptron**, √© um algoritmo para aprendizado de um classificador linear, originalmente introduzido por Rosenblatt [^7.5.1]. O algoritmo itera ajustando os pesos, e garante a converg√™ncia (sob condi√ß√µes de separabilidade linear) para um hiperplano que separa as classes no caso de dados linearmente separ√°veis [^7.5.1].

### Pergunta Te√≥rica Avan√ßada (Exemplo): Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?
**Resposta:**
O LDA, sob suposi√ß√µes de gaussianidade e covari√¢ncias iguais entre as classes, se torna equivalente √† regra de decis√£o Bayesiana. A regra de decis√£o Bayesiana atribui uma observa√ß√£o √† classe com maior probabilidade *a posteriori* dada a observa√ß√£o, e quando as distribui√ß√µes s√£o gaussianas, essa probabilidade pode ser expressa como uma fun√ß√£o discriminante linear [^7.3]. O LDA estima os par√¢metros desta fun√ß√£o discriminante, e quando as covari√¢ncias s√£o iguais para todas as classes, a fun√ß√£o resultante √© linear. No entanto, o LDA √© uma abordagem discriminativa, enquanto a regra de decis√£o Bayesiana √© uma abordagem gerativa [^7.3.1]. As fun√ß√µes discriminantes lineares do LDA surgem da proje√ß√£o das observa√ß√µes em um subespa√ßo onde as classes s√£o mais separ√°veis. No caso de covari√¢ncias distintas entre as classes, a regra de decis√£o bayesiana leva a fronteiras quadr√°ticas (QDA), e n√£o lineares [^7.3].

```mermaid
graph LR
    subgraph "LDA vs. Bayesian Decision Rule"
        direction TB
        A["LDA"]
        B["Bayesian Decision Rule"]
        C["Gaussian Distributions"]
        D["Equal Covariances"]
        E["Linear Discriminant"]
        F["Discriminative Approach"]
        G["Generative Approach"]
        A --> C
        A --> D
        A --> E
        A --> F
        B --> C
        B --> D
        B --> E
        B --> G
    end
```

**Lemma 4:** Equival√™ncia formal entre LDA e Regra de Decis√£o Bayesiana com covari√¢ncias iguais.
Sob a hip√≥tese de distribui√ß√µes gaussianas para as classes e covari√¢ncias iguais, a fun√ß√£o discriminante linear obtida via LDA √© formalmente equivalente √† fun√ß√£o discriminante Bayesiana [^7.3], [^7.3.3]. Essa equival√™ncia pode ser demonstrada por meio da deriva√ß√£o anal√≠tica de ambas as fun√ß√µes. $\blacksquare$

**Corol√°rio 4:** Fronteiras quadr√°ticas no QDA.
Ao relaxar a hip√≥tese de covari√¢ncias iguais entre as classes, a regra de decis√£o bayesiana leva a uma fun√ß√£o discriminante quadr√°tica, resultando em fronteiras de decis√£o n√£o lineares entre as classes (QDA), e n√£o lineares como no LDA [^7.3].

> ‚ö†Ô∏è **Ponto Crucial**: A diferen√ßa entre LDA e QDA reside nas suposi√ß√µes de covari√¢ncias, com o LDA assumindo igualdade e gerando fronteiras lineares, enquanto o QDA relaxa essa premissa gerando fronteiras quadr√°ticas [^7.3.1].

As perguntas devem ser altamente relevantes, podem envolver deriva√ß√µes matem√°ticas e provas, e focar em an√°lises te√≥ricas.

### Bootstrap Resampling
```mermaid
graph LR
    subgraph "Bootstrap Resampling"
        direction TB
        A["Original Dataset (Z)"]
        B["Bootstrap Samples (Z*b)"]
        C["Model Re-fitting (f*b(x))"]
        D["Statistic Calculation (S(Z*b))"]
        E["Error Estimation (Err_boot)"]
        F["Leave-One-Out Bootstrap"]
        G[".632 Estimator"]
        A --> B
        B --> C
        C --> D
        D --> E
        E --> F
        F --> G
    end
```
O **Bootstrap** √© uma t√©cnica de reamostragem que permite avaliar a acur√°cia estat√≠stica de uma estimativa, gerando m√∫ltiplas vers√µes do conjunto de dados original atrav√©s de amostragem com reposi√ß√£o [^7.11]. Essa t√©cnica pode ser usada para estimar a distribui√ß√£o do erro de previs√£o e outras estat√≠sticas de interesse, fornecendo uma forma de avaliar a estabilidade do modelo.
O processo de bootstrap √© como se segue:
1.  **Reamostragem:** A partir do conjunto de dados original $Z$, v√°rios conjuntos de dados bootstrap $Z^{*b}$ s√£o criados por amostragem com reposi√ß√£o. Cada conjunto $Z^{*b}$ tem o mesmo tamanho do conjunto original.
2.  **Reajuste do modelo:** O modelo √© reajustado em cada conjunto $Z^{*b}$, produzindo um modelo $f^{*b}(x)$.
3.  **Avalia√ß√£o:** A estat√≠stica de interesse $S(Z)$ √© calculada em cada conjunto $Z^{*b}$, e as estat√≠sticas $S(Z^{*b})$ s√£o usadas para estimar a variabilidade da estimativa original $S(Z)$.
Para estimar o erro de predi√ß√£o, uma abordagem bootstrap √© avaliar o qu√£o bem um modelo ajustado em um conjunto de dados bootstrap $Z^{*b}$ prediz os dados do conjunto original, com $Err_{boot}$ sendo:
$$Err_{boot} = \frac{1}{BN}\sum_{b=1}^B\sum_{i=1}^N L(Y_i, f^{*b}(x_i))$$
Essa abordagem, no entanto, pode ser pessimista devido a sobreposi√ß√£o entre os conjuntos de dados de treinamento e teste [^7.11].

> üí° **Exemplo Num√©rico:** Imagine que temos um conjunto de dados com 5 observa√ß√µes (N=5) representando o tempo de rea√ß√£o de um indiv√≠duo a um est√≠mulo visual (em segundos): Z = [0.5, 0.7, 0.6, 0.9, 0.8]. Nosso objetivo √© estimar a m√©dia do tempo de rea√ß√£o e avaliar a incerteza dessa estimativa com bootstrap.
>
> 1. **Reamostragem:** Geramos 3 conjuntos de dados bootstrap (B=3) por amostragem com reposi√ß√£o:
>     - $Z^{*1}$ = [0.7, 0.8, 0.5, 0.8, 0.9]
>     - $Z^{*2}$ = [0.6, 0.6, 0.9, 0.7, 0.5]
>     - $Z^{*3}$ = [0.8, 0.9, 0.7, 0.8, 0.6]
>
> 2. **Reajuste do modelo:** Calculamos a m√©dia (nosso modelo simples) para cada conjunto bootstrap:
>     - $\mu^{*1} = 0.74$
>     - $\mu^{*2} = 0.66$
>     - $\mu^{*3} = 0.76$
>
> 3. **Avalia√ß√£o:** A m√©dia original do conjunto de dados √© $\mu = 0.7$. A distribui√ß√£o das m√©dias bootstrap ($0.74, 0.66, 0.76$) nos permite estimar a variabilidade da m√©dia original, um intervalo de confian√ßa para a m√©dia original, ou calcular o erro padr√£o, etc.
>
> Para ilustrar o *Err_boot*, suponha que temos um modelo de regress√£o linear (muito simples para este exemplo, mas que serve para a ilustra√ß√£o) e a fun√ß√£o de perda quadr√°tica. Suponha tamb√©m que as predi√ß√µes, nos 3 datasets bootstrap, para cada uma das 5 observa√ß√µes originais s√£o dadas na seguinte tabela:
>
>  | Observa√ß√£o (i) | $Y_i$ | $f^{*1}(x_i)$ | $f^{*2}(x_i)$ | $f^{*3}(x_i)$ |
>  |---|---|---|---|---|
>  | 1 | 0.5 | 0.55 | 0.48 | 0.52 |
>  | 2 | 0.7 | 0.72 | 0.68 | 0.73 |
>  | 3 | 0.6 | 0.62 | 0.59 | 0.61 |
>  | 4 | 0.9 | 0.88 | 0.91 | 0.89 |
>  | 5 | 0.8 | 0.79 | 0.81 | 0.78 |
>
> Calculando o erro quadr√°tico m√©dio para cada observa√ß√£o e cada modelo, e somando-os, obtemos o erro *Err_boot* :
>
> $Err_{boot} = \frac{1}{3*5} [ (0.05^2 + 0.02^2 + 0.02^2) + (0.02^2 + 0.02^2 + 0.03^2) + (0.02^2 + 0.01^2 + 0.01^2) + (0.02^2 + 0.01^2 + 0.01^2) + (0.01^2 + 0.01^2 + 0.02^2) ] = 0.00026666...$
>
> Esse √© o erro de previs√£o do modelo com bootstrap, e como o texto aponta, pode ser pessimista devido a sobreposi√ß√£o entre os conjuntos de dados de treinamento e teste.

O **leave-one-out bootstrap** (LOOB) √© uma varia√ß√£o do bootstrap que evita essa sobreposi√ß√£o, avaliando a previs√£o de cada observa√ß√£o nos modelos gerados pelos conjuntos bootstrap onde essa observa√ß√£o n√£o foi utilizada no treinamento:
$$Err^{(1)} = \frac{1}{N} \sum_{i=1}^N \frac{1}{|C^{-i}|} \sum_{b \in C^{-i}}L(Y_i, f^{*b}(x_i))$$
Onde $C^{-i}$ √© o conjunto de *bootstrap samples* que n√£o incluem a observa√ß√£o $i$.
A m√©dia de $Err^{(1)}$ tende a ser enviesada para cima