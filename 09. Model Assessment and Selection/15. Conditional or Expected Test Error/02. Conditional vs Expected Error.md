## Conditional vs. Expected Test Error: A Deep Dive into Model Evaluation
```mermaid
flowchart TD
    A["Dados de Treinamento"] --> B{"M√©todo de Valida√ß√£o (CV, Bootstrap)"};
    B --> C["Estimativa do Erro Esperado (Err)"];
    A --> D["Modelo de Aprendizado"];
    D --> E["Erro Condicional (Errt)"]
    E --> C;
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#afa,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o
A avalia√ß√£o do desempenho de modelos de aprendizado estat√≠stico √© crucial para garantir sua efic√°cia em dados n√£o observados. A **generaliza√ß√£o** de um modelo, ou seja, sua capacidade de prever corretamente em dados novos e independentes, √© o objetivo final. Este cap√≠tulo explora os m√©todos para essa avalia√ß√£o e como eles s√£o usados para sele√ß√£o de modelos. Inicialmente, focamos na rela√ß√£o entre **vi√©s, vari√¢ncia e complexidade do modelo**, para ent√£o aprofundar a distin√ß√£o entre **erro condicional e erro esperado** [^7.1]. Este √∫ltimo ponto √© crucial para entender como diferentes m√©todos de valida√ß√£o operam e quais s√£o suas limita√ß√µes [^7.1].

### Conceitos Fundamentais

**Conceito 1:** O **problema de classifica√ß√£o** envolve atribuir uma categoria ou classe a uma entrada com base em dados de treinamento. M√©todos lineares, embora simples, s√£o um ponto de partida comum, mas podem apresentar um trade-off entre **vi√©s** (tend√™ncia de subajuste) e **vari√¢ncia** (tend√™ncia de sobreajuste) [^7.2]. M√©todos com alta complexidade tendem a se ajustar bem aos dados de treinamento, reduzindo o vi√©s, mas correm o risco de sobreajustar e ter alta vari√¢ncia. Um modelo com complexidade intermedi√°ria √© geralmente preferido, pois equilibra vi√©s e vari√¢ncia, como ilustrado na Figura 7.1 [^7.2].

> üí° **Exemplo Num√©rico:** Imagine que temos um conjunto de dados com pontos em duas dimens√µes, e queremos classificar esses pontos em duas classes (azul e vermelho). Um modelo linear simples (como uma reta) pode n√£o conseguir separar corretamente as classes se elas estiverem dispostas de forma complexa (alta probabilidade de vi√©s/subajuste). Por outro lado, um modelo muito complexo (como uma curva altamente sinuosa) pode se ajustar perfeitamente aos dados de treinamento, mas falhar em classificar novos pontos (alta vari√¢ncia/sobreajuste). Um modelo de complexidade intermedi√°ria (como uma curva suave) pode encontrar um melhor equil√≠brio, com um vi√©s e vari√¢ncia aceit√°veis.

**Lemma 1:** Dado um modelo $f(X)$ e um conjunto de treinamento $T$, a perda esperada (ou risco) pode ser decomposta em uma parte que depende da complexidade do modelo e uma parte que depende dos dados de treinamento [^7.2, 7.3]:

$$
E[L(Y,f(X))] =  E_{T}[E[L(Y,f(X))|T]]
$$
onde $E_{T}$ indica a esperan√ßa sobre diferentes conjuntos de treinamento e $E[L(Y,f(X))|T]$ √© o erro de teste condicional no conjunto $T$. Isto mostra a import√¢ncia do erro esperado (integrado sobre todos os poss√≠veis conjuntos de treino) no problema de avalia√ß√£o de modelos [^7.2, 7.3].
```mermaid
graph LR
    subgraph "Decomposi√ß√£o do Risco Esperado"
        A["Risco Esperado E[L(Y, f(X))]"] --> B["M√©dia sobre Conjuntos de Treinamento E_T[...]"];
        B --> C["Erro Condicional E[L(Y, f(X)) | T]"];
    end
```
> üí° **Exemplo Num√©rico:**  Suponha que temos um modelo de regress√£o linear com par√¢metros $\beta$.  O erro esperado $E[L(Y,f(X))]$ representa o erro m√©dio do modelo ao longo de todos os conjuntos de treinamento poss√≠veis. O erro condicional $E[L(Y,f(X))|T]$ √© o erro espec√≠fico quando o modelo foi treinado em um conjunto espec√≠fico $T$.  Por exemplo, se usarmos um conjunto $T_1$ e calcularmos o erro condicional obtivemos um valor de $0.2$, e se usarmos um conjunto $T_2$ obtemos $0.3$.  O erro esperado ser√° a m√©dia sobre todos os poss√≠veis erros condicionais, algo como $E_{T}[E[L(Y,f(X))|T]] = 0.25$, se considerarmos que estes dois conjuntos foram os √∫nicos a serem considerados.

**Conceito 2:** A **Linear Discriminant Analysis (LDA)** √© um m√©todo de classifica√ß√£o que pressup√µe distribui√ß√µes Gaussianas para cada classe e utiliza uma fun√ß√£o discriminante linear para separar as classes [^7.3]. A LDA assume covari√¢ncias iguais entre as classes, e busca a proje√ß√£o que melhor separa as m√©dias das classes, enquanto minimiza a vari√¢ncia dentro das classes. A formula√ß√£o do LDA envolve a obten√ß√£o de par√¢metros que maximizam a raz√£o entre a vari√¢ncia interclasses e a vari√¢ncia intraclasses [^7.3.1]. O m√©todo constr√≥i uma fronteira de decis√£o linear baseada na an√°lise dessas varia√ß√µes [^7.3.2].
```mermaid
flowchart LR
    subgraph "LDA Formulation"
        A["Dados de Entrada X com Classes"] --> B["Assumir Distribui√ß√µes Gaussianas"];
        B --> C["Calcular M√©dias das Classes (Œº)"];
        B --> D["Calcular Covari√¢ncia Comum (Œ£)"];
        C & D --> E["Maximizar Vari√¢ncia Interclasse / Vari√¢ncia Intraclasse"];
        E --> F["Obter Vetor de Proje√ß√£o (w)"];
        F --> G["Construir Fronteira de Decis√£o Linear"];
    end
```
> üí° **Exemplo Num√©rico:**  Imagine um problema de classifica√ß√£o de duas classes, onde a classe 1 tem m√©dia $\mu_1 = [1, 1]$ e a classe 2 tem m√©dia $\mu_2 = [3, 3]$. Suponha que a matriz de covari√¢ncia compartilhada seja $\Sigma = [[1, 0.5], [0.5, 1]]$. A LDA buscar√° uma dire√ß√£o $w$ que maximize a separa√ß√£o entre essas m√©dias, levando em conta a dispers√£o dos dados.

**Corol√°rio 1:** A fun√ß√£o discriminante linear do LDA, dada por $f(x) = w^T x + b$, pode ser vista como uma proje√ß√£o dos dados $x$ em uma dire√ß√£o $w$. A dire√ß√£o $w$ √© determinada pela diferen√ßa entre as m√©dias das classes e a covari√¢ncia comum das classes, levando a uma decis√£o de classe [^7.3.1]:
$$
w = \Sigma^{-1} (\mu_1 - \mu_2)
$$
onde $\mu_1$ e $\mu_2$ s√£o as m√©dias das classes e $\Sigma$ √© a covari√¢ncia comum. Esta formula√ß√£o implica que a decis√£o √© tomada com base na dist√¢ncia projetada de um ponto em rela√ß√£o √† fronteira de decis√£o [^7.3.3].
```mermaid
graph LR
    subgraph "C√°lculo do Vetor de Proje√ß√£o LDA"
      A["Calcular Diferen√ßa das M√©dias (Œº1 - Œº2)"] --> B["Inverter Matriz de Covari√¢ncia Comum (Œ£‚Åª¬π)"];
       B --> C["Multiplicar Œ£‚Åª¬π * (Œº1 - Œº2) = w"];
      C --> D["Resultado: Vetor de Proje√ß√£o (w)"]
    end
```

> üí° **Exemplo Num√©rico:**  Continuando o exemplo anterior, para calcular $w$ primeiro devemos calcular $\mu_1 - \mu_2 = [-2, -2]$. Em seguida, calculamos $\Sigma^{-1} = [[1.33, -0.66], [-0.66, 1.33]]$. Logo, $w = \Sigma^{-1} (\mu_1 - \mu_2) = [[1.33, -0.66], [-0.66, 1.33]] \cdot [-2, -2] = [-1.33, -1.33]$.  O termo de bias $b$ √© calculado com base na dist√¢ncia das m√©dias para o hiperplano de decis√£o, de modo que a fronteira √© centrada entre as m√©dias.

**Conceito 3:** A **Logistic Regression** modela a probabilidade de um evento (a perten√ßa a uma classe) utilizando uma fun√ß√£o log√≠stica (sigm√≥ide), a qual transforma uma combina√ß√£o linear das entradas em uma probabilidade entre 0 e 1 [^7.4]. Em regress√£o log√≠stica, os par√¢metros do modelo s√£o estimados por meio da maximiza√ß√£o da verossimilhan√ßa. A fun√ß√£o de verossimilhan√ßa √© definida como o produto das probabilidades de cada observa√ß√£o, e a otimiza√ß√£o √© realizada atrav√©s de um algoritmo iterativo [^7.4.1]. A regress√£o log√≠stica oferece um modelo probabil√≠stico flex√≠vel para classifica√ß√£o, com boa interpretabilidade, mas pode ser computacionalmente mais cara do que LDA para grandes conjuntos de dados [^7.4.2, 7.4.3, 7.4.4].
```mermaid
flowchart LR
    subgraph "Logistic Regression Model"
        A["Dados de Entrada X"] --> B["Combina√ß√£o Linear: Œ≤‚ÇÄ + Œ≤‚ÇÅX"];
        B --> C["Fun√ß√£o Log√≠stica (Sigmoide)"];
        C --> D["Probabilidade P(Y=1|X)"];
        D --> E["Maximiza√ß√£o da Verossimilhan√ßa"];
        E --> F["Estimativa dos Par√¢metros (Œ≤)"];
    end
```

> üí° **Exemplo Num√©rico:**  Suponha um problema bin√°rio com uma √∫nica caracter√≠stica $x$. O modelo de regress√£o log√≠stica √© da forma $p(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}$. Se os par√¢metros estimados forem $\beta_0 = -2$ e $\beta_1 = 1$, para um $x=3$, a probabilidade estimada da classe 1 ser√° $p(y=1|x=3) = \frac{1}{1 + e^{-(-2 + 1*3)}} =  \frac{1}{1 + e^{-1}} \approx 0.73$.

> ‚ö†Ô∏è **Nota Importante**: Tanto LDA quanto Logistic Regression s√£o m√©todos lineares para classifica√ß√£o, mas diferem em suas premissas e em como modelam as probabilidades de classe [^7.3, 7.4].

> ‚ùó **Ponto de Aten√ß√£o**: Modelos de classifica√ß√£o podem sofrer com desbalanceamento de classes, onde uma classe ocorre muito mais frequentemente que as outras. Isso pode levar o modelo a favorecer a classe majorit√°ria, necessitando de t√©cnicas de balanceamento [^7.4.2].

> ‚úîÔ∏è **Destaque**: Tanto LDA quanto regress√£o log√≠stica levam a fun√ß√µes discriminantes lineares, embora as estimativas dos par√¢metros possam ser diferentes, refletindo abordagens probabil√≠sticas distintas [^7.5].

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o
```mermaid
flowchart TD
    A["Codifica√ß√£o de Classes (Vari√°veis Indicadoras)"] --> B["Ajuste de Modelo Linear (M√≠nimos Quadrados)"];
    B --> C["Predi√ß√£o: f(x) = XŒ≤"];
    C --> D["Decis√£o de Classe: Argmax sobre f(x)"];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#afa,stroke:#333,stroke-width:2px
```
A **regress√£o linear** pode ser aplicada √† classifica√ß√£o atrav√©s da **regress√£o de indicadores**. Nesse m√©todo, as classes s√£o codificadas usando vari√°veis indicadoras (dummies), onde cada vari√°vel representa uma classe espec√≠fica [^7.2]. Por exemplo, em um problema de classifica√ß√£o bin√°ria, a vari√°vel de sa√≠da $Y$ assume valor 0 para a classe 0 e 1 para a classe 1. Em um problema de classifica√ß√£o com $K$ classes, criamos $K$ vari√°veis indicadoras, cada qual com valor 1 para uma dada classe e 0 para as demais. O objetivo √© ajustar um modelo linear a estas vari√°veis [^7.2]. Para uma vari√°vel $Y$, o ajuste pode ser feito usando o m√©todo de **m√≠nimos quadrados**, minimizando a soma dos erros quadr√°ticos entre as previs√µes e as vari√°veis indicadoras [^7.2]:

$$
\min_\beta \sum_{i=1}^N (y_i - \beta^T x_i)^2
$$

Onde $y_i$ √© o valor da vari√°vel indicadora para a i-√©sima observa√ß√£o, e $x_i$ √© o vetor de atributos dessa mesma observa√ß√£o. No entanto, a regress√£o de indicadores tem limita√ß√µes. As previs√µes lineares podem, em alguns casos, resultar em valores fora do intervalo [0,1], o que dificulta a interpreta√ß√£o como probabilidade. Al√©m disso, a regress√£o de indicadores pode ser menos eficiente em termos de vi√©s e vari√¢ncia quando comparada √† regress√£o log√≠stica em certos cen√°rios [^7.2, 7.3].

> üí° **Exemplo Num√©rico:** Considere um problema de classifica√ß√£o bin√°ria com um √∫nico preditor $x$. Suponha que temos 5 observa√ß√µes:  $x = [1, 2, 3, 4, 5]$ e  $y = [0, 0, 1, 1, 1]$. Usando regress√£o linear, podemos encontrar os coeficientes $\beta_0$ e $\beta_1$ que minimizam o erro quadr√°tico entre a predi√ß√£o $\hat{y} = \beta_0 + \beta_1 x$ e os valores de $y$. Usando m√≠nimos quadrados, por exemplo, podemos obter $\beta_0 \approx -0.6$ e $\beta_1 \approx 0.3$. Note que para alguns valores de x, a predi√ß√£o ser√° menor que 0 ou maior que 1, o que dificulta a interpreta√ß√£o como probabilidade.

**Lemma 2:** Para o caso de uma classifica√ß√£o bin√°ria, onde as classes s√£o codificadas como 0 e 1, a fronteira de decis√£o obtida via regress√£o linear de indicadores √© equivalente ao hiperplano de decis√£o obtido por um discriminante linear, sob a condi√ß√£o que os par√¢metros da regress√£o tenham sido estimados por m√≠nimos quadrados. Isso acontece porque a regress√£o linear buscar√° minimizar o erro quadr√°tico da fun√ß√£o indicadora, resultando em uma separa√ß√£o linear entre as classes [^7.2].

**Corol√°rio 2:** A equival√™ncia do Lemma 2 implica que, em certas condi√ß√µes, a an√°lise discriminante linear (LDA) e a regress√£o de indicadores podem levar a solu√ß√µes muito semelhantes, mas os fundamentos probabil√≠sticos s√£o diferentes. No entanto, √© importante notar que essa equival√™ncia √© v√°lida quando a regress√£o linear √© usada com as vari√°veis indicadoras e a covari√¢ncia entre os grupos de classes n√£o √© muito diferente. A LDA assume uma covari√¢ncia compartilhada e normalidade para cada classe, enquanto a regress√£o linear n√£o imp√µe estas restri√ß√µes, mas pode ter um comportamento menos preciso [^7.3].

‚ÄúEm certos cen√°rios, conforme apontado em [^7.4], a regress√£o log√≠stica pode fornecer estimativas mais est√°veis de probabilidade, enquanto a regress√£o de indicadores pode levar a extrapola√ß√µes fora de [0,1].‚Äù

‚ÄúNo entanto, h√° situa√ß√µes em que a regress√£o de indicadores, de acordo com [^7.2], √© suficiente e at√© mesmo vantajosa quando o objetivo principal √© a fronteira de decis√£o linear.‚Äù

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o
```mermaid
graph TD
    A["Regulariza√ß√£o"] --> B["L1 (Lasso): Penalidade |Œ≤|"];
    A --> C["L2 (Ridge): Penalidade Œ≤¬≤"];
    A --> D["Elastic Net: Combina√ß√£o L1 e L2"];
    B --> E["Sele√ß√£o de Vari√°veis (Sparsity)"];
    C --> F["Redu√ß√£o dos Coeficientes (Estabilidade)"];
    D --> G["Balancear Sparsity e Estabilidade"];
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#aaf,stroke:#333,stroke-width:2px
```
Em problemas de classifica√ß√£o com muitas vari√°veis, a **sele√ß√£o de vari√°veis** e a **regulariza√ß√£o** s√£o cruciais para evitar overfitting e aumentar a interpretabilidade do modelo [^7.4.4]. A **regulariza√ß√£o L1** (Lasso) adiciona uma penalidade √† fun√ß√£o de custo proporcional √† soma dos valores absolutos dos par√¢metros. Isso leva a solu√ß√µes esparsas, em que muitos coeficientes s√£o reduzidos a zero, efetivamente realizando a sele√ß√£o de vari√°veis [^7.4.4]. A **regulariza√ß√£o L2** (Ridge) adiciona uma penalidade proporcional √† soma dos quadrados dos par√¢metros. Isso resulta em solu√ß√µes com coeficientes menores, mas geralmente diferentes de zero, aumentando a estabilidade do modelo [^7.4.4]. A **Elastic Net** combina as penalidades L1 e L2, buscando um equil√≠brio entre sparsity e estabilidade [^7.5].

A regulariza√ß√£o √© incorporada na fun√ß√£o de custo, seja da regress√£o log√≠stica ou de outros modelos, da seguinte maneira. Considere o caso da regress√£o log√≠stica, a fun√ß√£o de custo a ser minimizada se torna:

$$
J(\beta) = - \sum_{i=1}^N [y_i \log(p(x_i)) + (1-y_i) \log(1-p(x_i))] + \lambda \sum_{j=1}^p |\beta_j|
$$
para o caso L1, e

$$
J(\beta) = - \sum_{i=1}^N [y_i \log(p(x_i)) + (1-y_i) \log(1-p(x_i))] + \lambda \sum_{j=1}^p \beta_j^2
$$
para o caso L2, onde $\lambda$ √© um hiperpar√¢metro que controla a for√ßa da regulariza√ß√£o, $p(x_i)$ √© a probabilidade predita pela regress√£o log√≠stica, e $y_i$ √© a classe real.

> üí° **Exemplo Num√©rico:** Suponha que estamos treinando um modelo de regress√£o log√≠stica com 5 preditores ($x_1, x_2, x_3, x_4, x_5$) e as estimativas iniciais dos coeficientes s√£o $\beta = [0.8, -0.5, 0.2, 1.2, -0.1]$. Se usarmos regulariza√ß√£o L1 com $\lambda = 0.5$, a penalidade ser√° $0.5 * (|0.8| + |-0.5| + |0.2| + |1.2| + |-0.1|) = 1.4$. A otimiza√ß√£o do modelo com essa penalidade tender√° a reduzir alguns coeficientes a zero. Com L2, a penalidade seria $0.5 * (0.8^2 + (-0.5)^2 + 0.2^2 + 1.2^2 + (-0.1)^2) = 1.135$, que levaria a coeficientes menores mas n√£o necessariamente zero.

**Lemma 3:** A penaliza√ß√£o L1 em um modelo de classifica√ß√£o log√≠stica leva √† esparsidade dos coeficientes. Isso pode ser provado mostrando que, na otimiza√ß√£o da fun√ß√£o de custo, a derivada da fun√ß√£o de penalidade L1 (que √© a fun√ß√£o sinal) faz com que os coeficientes tendam a zero, se o efeito do atributo for fraco. Os coeficientes que n√£o tiverem magnitude suficiente para compensar a penalidade, convergir√£o para zero, portanto, selecionando as vari√°veis mais importantes.

**Prova do Lemma 3:** A fun√ß√£o de custo com L1 √© n√£o-diferenci√°vel em $\beta=0$. No entanto, se utilizarmos o conceito de subgradiente, podemos analisar o problema. O subgradiente da parte penalizada do custo, $\lambda \sum_{j=1}^p |\beta_j|$, com respeito a $\beta_j$ √© $\lambda sign(\beta_j)$, onde $sign(\beta_j) = 1$ se $\beta_j>0$, $-1$ se $\beta_j<0$ e qualquer valor em [-1,1] se $\beta_j = 0$. Para qualquer $\beta_j$ n√£o nulo, a dire√ß√£o do subgradiente o empurra em dire√ß√£o a zero (j√° que a penalidade √© sempre positiva). No caso de $\beta_j=0$, o subgradiente est√° entre $-\lambda$ e $\lambda$, e ele s√≥ n√£o ser√° nulo se a derivada do restante da fun√ß√£o de custo tiver uma magnitude maior que $\lambda$, caso contr√°rio o par√¢metro permanecer√° em $\beta_j=0$. $\blacksquare$

**Corol√°rio 3:** A esparsidade induzida pela penaliza√ß√£o L1 simplifica o modelo e melhora sua interpretabilidade, pois apenas um subconjunto de vari√°veis contribui significativamente para a predi√ß√£o. Isso facilita a identifica√ß√£o dos fatores mais relevantes para a classifica√ß√£o [^7.4.5].

> ‚ö†Ô∏è **Ponto Crucial**: L1 e L2 podem ser combinadas (Elastic Net) para aproveitar vantagens de ambos os tipos de regulariza√ß√£o, conforme discutido em [^7.5].

### Separating Hyperplanes e Perceptrons
```mermaid
flowchart TD
    subgraph "Hiperplanos Separadores"
        A["Dados de Entrada X com Classes"] --> B["Definir Hiperplano Separador"];
        B --> C["Maximizar Margem (Dist√¢ncia aos Vetores de Suporte)"];
        C --> D["Otimizar Par√¢metros do Hiperplano"];
        D --> E["Fronteira de Decis√£o √ìtima"];
    end
    subgraph "Perceptron"
        F["Inicializa√ß√£o dos Pesos"] --> G["Classificar Amostra"];
        G --> H{"Classifica√ß√£o Incorreta?"};
        H -- Sim --> I["Atualizar Pesos"];
        I --> G;
        H -- N√£o --> J["Converg√™ncia"];
    end
    style A fill:#aaf,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
```
Os **hiperplanos separadores** s√£o utilizados em classifica√ß√£o para dividir o espa√ßo de caracter√≠sticas em regi√µes correspondentes √†s diferentes classes [^7.5.2]. O objetivo √© encontrar um hiperplano que maximize a margem de separa√ß√£o entre as classes, ou seja, a dist√¢ncia m√≠nima entre o hiperplano e os pontos mais pr√≥ximos de cada classe, chamados de vetores de suporte. Este conceito √© a base dos Support Vector Machines (SVMs). A formula√ß√£o matem√°tica desse problema de otimiza√ß√£o √© dada pela maximiza√ß√£o da margem, que geralmente √© implementada utilizando o dual de Wolfe [^7.5.2]. Essa formula√ß√£o leva a solu√ß√µes que s√£o combina√ß√µes lineares dos pontos de suporte.

O **Perceptron** de Rosenblatt √© um algoritmo de aprendizado que busca um hiperplano separador, atualizando iterativamente seus par√¢metros com base na classifica√ß√£o incorreta das amostras. O algoritmo garante converg√™ncia quando os dados s√£o linearmente separ√°veis, e cada atualiza√ß√£o move o hiperplano em dire√ß√£o a uma melhor separa√ß√£o. A formula√ß√£o da atualiza√ß√£o dos par√¢metros envolve a adi√ß√£o de um m√∫ltiplo do vetor de entrada correspondente a amostras mal classificadas ao vetor de pesos, at√© que todos os dados sejam classificados corretamente [^7.5.1].

> üí° **Exemplo Num√©rico:** Imagine que temos um perceptron com pesos iniciais $w = [0.1, -0.2]$ e bias $b=0.3$. Temos um ponto $x=[2, 1]$ que √© da classe positiva (y=1), mas a predi√ß√£o √© dada por $\hat{y} = w^T x + b = 0.1*2 + (-0.2)*1 + 0.3 = 0.3$ que √© menor que 0, indicando que o ponto foi classificado incorretamente. A atualiza√ß√£o do peso √© feita somando o vetor de entrada $x$ ao vetor de peso $w$, resultando em  $w_{new} = w_{old} + \eta x$, onde $\eta$ √© a taxa de aprendizado (por exemplo 0.1). $w_{new} = [0.1, -0.2] + 0.1*[2, 1] = [0.3, -0.1]$. Isso leva a uma predi√ß√£o mais favor√°vel para a classifica√ß√£o correta.

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

**Resposta:** Sob a hip√≥tese de distribui√ß√µes Gaussianas e covari√¢ncias iguais, a LDA se torna equivalente √† regra de decis√£o Bayesiana, e ambas as abordagens levam √† mesma fronteira de decis√£o linear [^7.3]. A LDA assume que as classes s√£o Gaussianas com covari√¢ncias iguais. A regra de decis√£o Bayesiana, por sua vez, busca classificar uma amostra na classe com maior probabilidade posterior [^7.3]. Em outras palavras, a regra de decis√£o Bayesiana, para este caso particular, se transforma na regra de decis√£o do LDA.
```mermaid
graph TB
    subgraph "Equival√™ncia LDA e Bayes"
        A["Distribui√ß√µes Gaussianas com Covari√¢ncias Iguais"] --> B["LDA: Proje√ß√£o Maximiza Separa√ß√£o de M√©dias"];
        A --> C["Regra de Decis√£o Bayesiana: Classificar na Classe com Maior Probabilidade Posterior"];
        B & C --> D["Mesma Fronteira de Decis√£o Linear: w^T x + b = 0"];
    end
```
O limite de decis√£o entre duas classes, sob essas condi√ß√µes, √© um hiperplano perpendicular √† linha que une as m√©dias das classes e √© dado por [^7.3]:

$$
w^T x + b = 0
$$

onde $w$ √© o vetor de pesos, $x$ √© o vetor de caracter√≠sticas e $b$ √© o termo de bias. A diferen√ßa fundamental entre as formula√ß√µes √© que o LDA obt√©m o vetor $w$ pela invers√£o da covari√¢ncia emp√≠rica e a regra de decis√£o bayesiana obt√©m o mesmo vetor $w$ usando as probabilidades das classes. Em termos matem√°ticos, ambas as formas de encontrar o hiperplano s√£o equivalentes [^7.3.3].

**Lemma 4:** Se as distribui√ß√µes das classes forem Gaussianas com covari√¢ncias iguais, a regra de decis√£o Bayesiana leva √† mesma fronteira de decis√£o linear que o LDA. Formalmente, isso pode ser comprovado mostrando que o log-odds ratio das probabilidades posteriores nas duas abordagens √© equivalente, resultando na mesma formula√ß√£o da fronteira de decis√£o [^7.3, 7.3.1].

**Corol√°rio 4:** A Relaxando a hip√≥tese de covari√¢ncias iguais, a regra de decis√£o bayesiana leva a fronteiras de decis√£o quadr√°ticas, conhecido como Quadratic Discriminant Analysis (QDA), que permite diferentes formas de regi√µes de decis√£o [^7.3]. Isso mostra a flexibilidade da abordagem Bayesiana em rela√ß√£o ao LDA, que √© mais restritivo [^7.3].

> ‚ö†Ô∏è **Ponto Crucial**: A ado√ß√£o ou n√£o de covari√¢ncias iguais impacta fortemente o tipo de fronteira de decis√£o (linear vs. quadr√°tica), conforme discutido em [^7.3.1].

### Conclus√£o

A escolha de modelos e a avalia√ß√£o de seu desempenho s√£o etapas cruciais no aprendizado estat√≠stico. Este cap√≠tulo explorou a import√¢ncia da distin√ß√£o entre **erro condicional** (Errt) e **erro esperado** (Err), mostrando como m√©todos como cross-validation e bootstrap fornecem estimativas, majoritariamente, do erro esperado, o qual quantifica o desempenho do modelo em m√©dia sobre todos os poss√≠veis conjuntos de treino. Modelos lineares, como LDA, regress√£o log√≠stica e regress√£o de indicadores, foram detalhados, e suas limita√ß√µes foram discutidas, especialmente no contexto do trade-off entre vi√©s e vari√¢ncia. T√©cnicas de regulariza√ß√£o, como Lasso e Ridge, podem ser incorporadas para evitar overfitting. A abordagem bayesiana, que leva a uma formula√ß√£o similar do BIC, tamb√©m foi abordada para sele√ß√£o de modelos. Uma discuss√£o detalhada sobre a validade do cross-validation tamb√©m foi apresentada, mostrando seus limites.
```mermaid
graph TB
    subgraph "Conclus√£o"
    A["Diferen√ßa entre Errt e Err"] --> B["Estimativas do Erro Esperado: CV e Bootstrap"];
    A --> C["Modelos Lineares: LDA, Regress√£o Log√≠stica, Regress√£o de Indicadores"];
    C --> D["Trade-off Vi√©s-Vari√¢ncia"];
    C --> E["Regulariza√ß√£o: Lasso e Ridge"];
    A --> F["Abordagem Bayesiana para Sele√ß√£o de Modelos"];
    A --> G["Limites do Cross-Validation"];
    end
```

### Footnotes
[^7.1]: "The generalization performance of a learning method relates to its predic-tion capability on independent test data. Assessment of this performance is extremely important in practice, since it guides the choice of learning method or model, and gives us a measure of the quality of the ultimately chosen model." *(Trecho de <Model Assessment and Selection>)*
[^7.2]: "Figure 7.1 illustrates the important issue in assessing the ability of a learn-ing method to generalize. Consider first the case of a quantitative or interval scale response. We have a target variable Y, a vector of inputs X, and a prediction model f(X) that has been estimated from a training set T." *(Trecho de <Model Assessment and Selection>)*
[^7.3]: "The story is similar for a qualitative or categorical response G taking one of K values in a set G, labeled for convenience as 1, 2, ..., K. Typically we model the probabilities pk(X) = Pr(G = k|X) (or some monotone transformations fr(X)), and then ƒú(X) = arg maxk √ék(X)." *(Trecho de <Model Assessment and Selection>)*
[^7.3.1]:  "Typically we model the probabilities pk(X) = Pr(G = k|X) (or some monotone transformations fr(X)), and then ƒú(X) = arg maxk √ék(X). In some cases, such as 1-nearest neighbor classification (Chapters 2 and 13) we produce ƒú(X) directly. Typical loss functions are" *(Trecho de <Model Assessment and Selection>)*
[^7.3.2]: "The quantity -2 √ó the log-likelihood is sometimes referred to as the deviance." *(Trecho de <Model Assessment and Selection>)*
[^7.3.3]: "Again, test error here is Err„Öú = E[L(G, ƒú(X))|T], the population mis-classification error of the classifier trained on T, and Err is the expected misclassification error." *(Trecho de <Model Assessment and Selection>)*
[^7.4]:  "The log-likelihood can be used as a loss-function for general response densities, such as the Poisson, gamma, exponential, log-normal and others." *(Trecho de <Model Assessment and Selection>)*
[^7.4.1]: "If Pro(x) (Y) is the density of Y, indexed by a parameter 0(X) that depends on the predictor X, then L(Y,0(X)) = ‚àí2. log Pro(x) (Y)." *(Trecho de <Model Assessment and Selection>)*
[^7.4.2]: "The "-2" in the definition makes the log-likelihood loss for the Gaussian distribution match squared-error loss." *(Trecho de <Model Assessment and Selection>)*
[^7.4.3]: "For ease of exposition, for the remainder of this chapter we will use Y and f(X) to represent all of the above situations, since we focus mainly on the quantitative response (squared-error loss) setting." *(Trecho de <Model Assessment and Selection>)*
[^7.4.4]: "In this chapter we describe a number of methods for estimating the expected test error for a model." *(Trecho de <Model Assessment and Selection>)*
[^7.4.5]: "Typically our model will have a tuning parameter or parameters a and so we can write our predictions as fa(x). The tuning parameter varies the complexity of our model, and we wish to find the value of a that minimizes error, that is, produces the minimum of the average test error curve in Figure 7.1." *(Trecho de <Model Assessment and Selection>)*
[^7.5]: "Having said this, for brevity we will often suppress the dependence of f(x) on a." *(Trecho de <Model Assessment and Selection>)*
[^7.5.1]: "It is important to note that there are in fact two separate goals that we might have in mind: Model selection: estimating the performance of different models in order to choose the best one." *(Trecho de <Model Assessment and Selection>)*
[^7.5.2]: "Model assessment: having chosen a final model, estimating its predic-tion error (generalization error) on new data." *(Trecho de <Model Assessment and Selection>)*
