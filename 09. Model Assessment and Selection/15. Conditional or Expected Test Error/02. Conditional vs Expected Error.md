## Conditional vs. Expected Test Error: A Deep Dive into Model Evaluation
```mermaid
flowchart TD
    A["Dados de Treinamento"] --> B{"MÃ©todo de ValidaÃ§Ã£o (CV, Bootstrap)"};
    B --> C["Estimativa do Erro Esperado (Err)"];
    A --> D["Modelo de Aprendizado"];
    D --> E["Erro Condicional (Errt)"]
    E --> C;
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#afa,stroke:#333,stroke-width:2px
```

### IntroduÃ§Ã£o
A avaliaÃ§Ã£o do desempenho de modelos de aprendizado estatÃ­stico Ã© crucial para garantir sua eficÃ¡cia em dados nÃ£o observados. A **generalizaÃ§Ã£o** de um modelo, ou seja, sua capacidade de prever corretamente em dados novos e independentes, Ã© o objetivo final. Este capÃ­tulo explora os mÃ©todos para essa avaliaÃ§Ã£o e como eles sÃ£o usados para seleÃ§Ã£o de modelos. Inicialmente, focamos na relaÃ§Ã£o entre **viÃ©s, variÃ¢ncia e complexidade do modelo**, para entÃ£o aprofundar a distinÃ§Ã£o entre **erro condicional e erro esperado** [^7.1]. Este Ãºltimo ponto Ã© crucial para entender como diferentes mÃ©todos de validaÃ§Ã£o operam e quais sÃ£o suas limitaÃ§Ãµes [^7.1].

### Conceitos Fundamentais

**Conceito 1:** O **problema de classificaÃ§Ã£o** envolve atribuir uma categoria ou classe a uma entrada com base em dados de treinamento. MÃ©todos lineares, embora simples, sÃ£o um ponto de partida comum, mas podem apresentar um trade-off entre **viÃ©s** (tendÃªncia de subajuste) e **variÃ¢ncia** (tendÃªncia de sobreajuste) [^7.2]. MÃ©todos com alta complexidade tendem a se ajustar bem aos dados de treinamento, reduzindo o viÃ©s, mas correm o risco de sobreajustar e ter alta variÃ¢ncia. Um modelo com complexidade intermediÃ¡ria Ã© geralmente preferido, pois equilibra viÃ©s e variÃ¢ncia, como ilustrado na Figura 7.1 [^7.2].

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine que temos um conjunto de dados com pontos em duas dimensÃµes, e queremos classificar esses pontos em duas classes (azul e vermelho). Um modelo linear simples (como uma reta) pode nÃ£o conseguir separar corretamente as classes se elas estiverem dispostas de forma complexa (alta probabilidade de viÃ©s/subajuste). Por outro lado, um modelo muito complexo (como uma curva altamente sinuosa) pode se ajustar perfeitamente aos dados de treinamento, mas falhar em classificar novos pontos (alta variÃ¢ncia/sobreajuste). Um modelo de complexidade intermediÃ¡ria (como uma curva suave) pode encontrar um melhor equilÃ­brio, com um viÃ©s e variÃ¢ncia aceitÃ¡veis.

**Lemma 1:** Dado um modelo $f(X)$ e um conjunto de treinamento $T$, a perda esperada (ou risco) pode ser decomposta em uma parte que depende da complexidade do modelo e uma parte que depende dos dados de treinamento [^7.2, 7.3]:

$$
E[L(Y,f(X))] =  E_{T}[E[L(Y,f(X))|T]]
$$
onde $E_{T}$ indica a esperanÃ§a sobre diferentes conjuntos de treinamento e $E[L(Y,f(X))|T]$ Ã© o erro de teste condicional no conjunto $T$. Isto mostra a importÃ¢ncia do erro esperado (integrado sobre todos os possÃ­veis conjuntos de treino) no problema de avaliaÃ§Ã£o de modelos [^7.2, 7.3].
```mermaid
graph LR
    subgraph "DecomposiÃ§Ã£o do Risco Esperado"
        A["Risco Esperado E[L(Y, f(X))]"] --> B["MÃ©dia sobre Conjuntos de Treinamento E_T[...]"];
        B --> C["Erro Condicional E[L(Y, f(X)) | T]"];
    end
```
> ðŸ’¡ **Exemplo NumÃ©rico:**  Suponha que temos um modelo de regressÃ£o linear com parÃ¢metros $\beta$.  O erro esperado $E[L(Y,f(X))]$ representa o erro mÃ©dio do modelo ao longo de todos os conjuntos de treinamento possÃ­veis. O erro condicional $E[L(Y,f(X))|T]$ Ã© o erro especÃ­fico quando o modelo foi treinado em um conjunto especÃ­fico $T$.  Por exemplo, se usarmos um conjunto $T_1$ e calcularmos o erro condicional obtivemos um valor de $0.2$, e se usarmos um conjunto $T_2$ obtemos $0.3$.  O erro esperado serÃ¡ a mÃ©dia sobre todos os possÃ­veis erros condicionais, algo como $E_{T}[E[L(Y,f(X))|T]] = 0.25$, se considerarmos que estes dois conjuntos foram os Ãºnicos a serem considerados.

**Conceito 2:** A **Linear Discriminant Analysis (LDA)** Ã© um mÃ©todo de classificaÃ§Ã£o que pressupÃµe distribuiÃ§Ãµes Gaussianas para cada classe e utiliza uma funÃ§Ã£o discriminante linear para separar as classes [^7.3]. A LDA assume covariÃ¢ncias iguais entre as classes, e busca a projeÃ§Ã£o que melhor separa as mÃ©dias das classes, enquanto minimiza a variÃ¢ncia dentro das classes. A formulaÃ§Ã£o do LDA envolve a obtenÃ§Ã£o de parÃ¢metros que maximizam a razÃ£o entre a variÃ¢ncia interclasses e a variÃ¢ncia intraclasses [^7.3.1]. O mÃ©todo constrÃ³i uma fronteira de decisÃ£o linear baseada na anÃ¡lise dessas variaÃ§Ãµes [^7.3.2].
```mermaid
flowchart LR
    subgraph "LDA Formulation"
        A["Dados de Entrada X com Classes"] --> B["Assumir DistribuiÃ§Ãµes Gaussianas"];
        B --> C["Calcular MÃ©dias das Classes (Î¼)"];
        B --> D["Calcular CovariÃ¢ncia Comum (Î£)"];
        C & D --> E["Maximizar VariÃ¢ncia Interclasse / VariÃ¢ncia Intraclasse"];
        E --> F["Obter Vetor de ProjeÃ§Ã£o (w)"];
        F --> G["Construir Fronteira de DecisÃ£o Linear"];
    end
```
> ðŸ’¡ **Exemplo NumÃ©rico:**  Imagine um problema de classificaÃ§Ã£o de duas classes, onde a classe 1 tem mÃ©dia $\mu_1 = [1, 1]$ e a classe 2 tem mÃ©dia $\mu_2 = [3, 3]$. Suponha que a matriz de covariÃ¢ncia compartilhada seja $\Sigma = [[1, 0.5], [0.5, 1]]$. A LDA buscarÃ¡ uma direÃ§Ã£o $w$ que maximize a separaÃ§Ã£o entre essas mÃ©dias, levando em conta a dispersÃ£o dos dados.

**CorolÃ¡rio 1:** A funÃ§Ã£o discriminante linear do LDA, dada por $f(x) = w^T x + b$, pode ser vista como uma projeÃ§Ã£o dos dados $x$ em uma direÃ§Ã£o $w$. A direÃ§Ã£o $w$ Ã© determinada pela diferenÃ§a entre as mÃ©dias das classes e a covariÃ¢ncia comum das classes, levando a uma decisÃ£o de classe [^7.3.1]:
$$
w = \Sigma^{-1} (\mu_1 - \mu_2)
$$
onde $\mu_1$ e $\mu_2$ sÃ£o as mÃ©dias das classes e $\Sigma$ Ã© a covariÃ¢ncia comum. Esta formulaÃ§Ã£o implica que a decisÃ£o Ã© tomada com base na distÃ¢ncia projetada de um ponto em relaÃ§Ã£o Ã  fronteira de decisÃ£o [^7.3.3].
```mermaid
graph LR
    subgraph "CÃ¡lculo do Vetor de ProjeÃ§Ã£o LDA"
      A["Calcular DiferenÃ§a das MÃ©dias (Î¼1 - Î¼2)"] --> B["Inverter Matriz de CovariÃ¢ncia Comum (Î£â»Â¹)"];
       B --> C["Multiplicar Î£â»Â¹ * (Î¼1 - Î¼2) = w"];
      C --> D["Resultado: Vetor de ProjeÃ§Ã£o (w)"]
    end
```

> ðŸ’¡ **Exemplo NumÃ©rico:**  Continuando o exemplo anterior, para calcular $w$ primeiro devemos calcular $\mu_1 - \mu_2 = [-2, -2]$. Em seguida, calculamos $\Sigma^{-1} = [[1.33, -0.66], [-0.66, 1.33]]$. Logo, $w = \Sigma^{-1} (\mu_1 - \mu_2) = [[1.33, -0.66], [-0.66, 1.33]] \cdot [-2, -2] = [-1.33, -1.33]$.  O termo de bias $b$ Ã© calculado com base na distÃ¢ncia das mÃ©dias para o hiperplano de decisÃ£o, de modo que a fronteira Ã© centrada entre as mÃ©dias.

**Conceito 3:** A **Logistic Regression** modela a probabilidade de um evento (a pertenÃ§a a uma classe) utilizando uma funÃ§Ã£o logÃ­stica (sigmÃ³ide), a qual transforma uma combinaÃ§Ã£o linear das entradas em uma probabilidade entre 0 e 1 [^7.4]. Em regressÃ£o logÃ­stica, os parÃ¢metros do modelo sÃ£o estimados por meio da maximizaÃ§Ã£o da verossimilhanÃ§a. A funÃ§Ã£o de verossimilhanÃ§a Ã© definida como o produto das probabilidades de cada observaÃ§Ã£o, e a otimizaÃ§Ã£o Ã© realizada atravÃ©s de um algoritmo iterativo [^7.4.1]. A regressÃ£o logÃ­stica oferece um modelo probabilÃ­stico flexÃ­vel para classificaÃ§Ã£o, com boa interpretabilidade, mas pode ser computacionalmente mais cara do que LDA para grandes conjuntos de dados [^7.4.2, 7.4.3, 7.4.4].
```mermaid
flowchart LR
    subgraph "Logistic Regression Model"
        A["Dados de Entrada X"] --> B["CombinaÃ§Ã£o Linear: Î²â‚€ + Î²â‚X"];
        B --> C["FunÃ§Ã£o LogÃ­stica (Sigmoide)"];
        C --> D["Probabilidade P(Y=1|X)"];
        D --> E["MaximizaÃ§Ã£o da VerossimilhanÃ§a"];
        E --> F["Estimativa dos ParÃ¢metros (Î²)"];
    end
```

> ðŸ’¡ **Exemplo NumÃ©rico:**  Suponha um problema binÃ¡rio com uma Ãºnica caracterÃ­stica $x$. O modelo de regressÃ£o logÃ­stica Ã© da forma $p(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}$. Se os parÃ¢metros estimados forem $\beta_0 = -2$ e $\beta_1 = 1$, para um $x=3$, a probabilidade estimada da classe 1 serÃ¡ $p(y=1|x=3) = \frac{1}{1 + e^{-(-2 + 1*3)}} =  \frac{1}{1 + e^{-1}} \approx 0.73$.

> âš ï¸ **Nota Importante**: Tanto LDA quanto Logistic Regression sÃ£o mÃ©todos lineares para classificaÃ§Ã£o, mas diferem em suas premissas e em como modelam as probabilidades de classe [^7.3, 7.4].

> â— **Ponto de AtenÃ§Ã£o**: Modelos de classificaÃ§Ã£o podem sofrer com desbalanceamento de classes, onde uma classe ocorre muito mais frequentemente que as outras. Isso pode levar o modelo a favorecer a classe majoritÃ¡ria, necessitando de tÃ©cnicas de balanceamento [^7.4.2].

> âœ”ï¸ **Destaque**: Tanto LDA quanto regressÃ£o logÃ­stica levam a funÃ§Ãµes discriminantes lineares, embora as estimativas dos parÃ¢metros possam ser diferentes, refletindo abordagens probabilÃ­sticas distintas [^7.5].

### RegressÃ£o Linear e MÃ­nimos Quadrados para ClassificaÃ§Ã£o
```mermaid
flowchart TD
    A["CodificaÃ§Ã£o de Classes (VariÃ¡veis Indicadoras)"] --> B["Ajuste de Modelo Linear (MÃ­nimos Quadrados)"];
    B --> C["PrediÃ§Ã£o: f(x) = XÎ²"];
    C --> D["DecisÃ£o de Classe: Argmax sobre f(x)"];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#afa,stroke:#333,stroke-width:2px
```
A **regressÃ£o linear** pode ser aplicada Ã  classificaÃ§Ã£o atravÃ©s da **regressÃ£o de indicadores**. Nesse mÃ©todo, as classes sÃ£o codificadas usando variÃ¡veis indicadoras (dummies), onde cada variÃ¡vel representa uma classe especÃ­fica [^7.2]. Por exemplo, em um problema de classificaÃ§Ã£o binÃ¡ria, a variÃ¡vel de saÃ­da $Y$ assume valor 0 para a classe 0 e 1 para a classe 1. Em um problema de classificaÃ§Ã£o com $K$ classes, criamos $K$ variÃ¡veis indicadoras, cada qual com valor 1 para uma dada classe e 0 para as demais. O objetivo Ã© ajustar um modelo linear a estas variÃ¡veis [^7.2]. Para uma variÃ¡vel $Y$, o ajuste pode ser feito usando o mÃ©todo de **mÃ­nimos quadrados**, minimizando a soma dos erros quadrÃ¡ticos entre as previsÃµes e as variÃ¡veis indicadoras [^7.2]:

$$
\min_\beta \sum_{i=1}^N (y_i - \beta^T x_i)^2
$$

Onde $y_i$ Ã© o valor da variÃ¡vel indicadora para a i-Ã©sima observaÃ§Ã£o, e $x_i$ Ã© o vetor de atributos dessa mesma observaÃ§Ã£o. No entanto, a regressÃ£o de indicadores tem limitaÃ§Ãµes. As previsÃµes lineares podem, em alguns casos, resultar em valores fora do intervalo [0,1], o que dificulta a interpretaÃ§Ã£o como probabilidade. AlÃ©m disso, a regressÃ£o de indicadores pode ser menos eficiente em termos de viÃ©s e variÃ¢ncia quando comparada Ã  regressÃ£o logÃ­stica em certos cenÃ¡rios [^7.2, 7.3].

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um problema de classificaÃ§Ã£o binÃ¡ria com um Ãºnico preditor $x$. Suponha que temos 5 observaÃ§Ãµes:  $x = [1, 2, 3, 4, 5]$ e  $y = [0, 0, 1, 1, 1]$. Usando regressÃ£o linear, podemos encontrar os coeficientes $\beta_0$ e $\beta_1$ que minimizam o erro quadrÃ¡tico entre a prediÃ§Ã£o $\hat{y} = \beta_0 + \beta_1 x$ e os valores de $y$. Usando mÃ­nimos quadrados, por exemplo, podemos obter $\beta_0 \approx -0.6$ e $\beta_1 \approx 0.3$. Note que para alguns valores de x, a prediÃ§Ã£o serÃ¡ menor que 0 ou maior que 1, o que dificulta a interpretaÃ§Ã£o como probabilidade.

**Lemma 2:** Para o caso de uma classificaÃ§Ã£o binÃ¡ria, onde as classes sÃ£o codificadas como 0 e 1, a fronteira de decisÃ£o obtida via regressÃ£o linear de indicadores Ã© equivalente ao hiperplano de decisÃ£o obtido por um discriminante linear, sob a condiÃ§Ã£o que os parÃ¢metros da regressÃ£o tenham sido estimados por mÃ­nimos quadrados. Isso acontece porque a regressÃ£o linear buscarÃ¡ minimizar o erro quadrÃ¡tico da funÃ§Ã£o indicadora, resultando em uma separaÃ§Ã£o linear entre as classes [^7.2].

**CorolÃ¡rio 2:** A equivalÃªncia do Lemma 2 implica que, em certas condiÃ§Ãµes, a anÃ¡lise discriminante linear (LDA) e a regressÃ£o de indicadores podem levar a soluÃ§Ãµes muito semelhantes, mas os fundamentos probabilÃ­sticos sÃ£o diferentes. No entanto, Ã© importante notar que essa equivalÃªncia Ã© vÃ¡lida quando a regressÃ£o linear Ã© usada com as variÃ¡veis indicadoras e a covariÃ¢ncia entre os grupos de classes nÃ£o Ã© muito diferente. A LDA assume uma covariÃ¢ncia compartilhada e normalidade para cada classe, enquanto a regressÃ£o linear nÃ£o impÃµe estas restriÃ§Ãµes, mas pode ter um comportamento menos preciso [^7.3].

â€œEm certos cenÃ¡rios, conforme apontado em [^7.4], a regressÃ£o logÃ­stica pode fornecer estimativas mais estÃ¡veis de probabilidade, enquanto a regressÃ£o de indicadores pode levar a extrapolaÃ§Ãµes fora de [0,1].â€

â€œNo entanto, hÃ¡ situaÃ§Ãµes em que a regressÃ£o de indicadores, de acordo com [^7.2], Ã© suficiente e atÃ© mesmo vantajosa quando o objetivo principal Ã© a fronteira de decisÃ£o linear.â€

### MÃ©todos de SeleÃ§Ã£o de VariÃ¡veis e RegularizaÃ§Ã£o em ClassificaÃ§Ã£o
```mermaid
graph TD
    A["RegularizaÃ§Ã£o"] --> B["L1 (Lasso): Penalidade |Î²|"];
    A --> C["L2 (Ridge): Penalidade Î²Â²"];
    A --> D["Elastic Net: CombinaÃ§Ã£o L1 e L2"];
    B --> E["SeleÃ§Ã£o de VariÃ¡veis (Sparsity)"];
    C --> F["ReduÃ§Ã£o dos Coeficientes (Estabilidade)"];
    D --> G["Balancear Sparsity e Estabilidade"];
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#aaf,stroke:#333,stroke-width:2px
```
Em problemas de classificaÃ§Ã£o com muitas variÃ¡veis, a **seleÃ§Ã£o de variÃ¡veis** e a **regularizaÃ§Ã£o** sÃ£o cruciais para evitar overfitting e aumentar a interpretabilidade do modelo [^7.4.4]. A **regularizaÃ§Ã£o L1** (Lasso) adiciona uma penalidade Ã  funÃ§Ã£o de custo proporcional Ã  soma dos valores absolutos dos parÃ¢metros. Isso leva a soluÃ§Ãµes esparsas, em que muitos coeficientes sÃ£o reduzidos a zero, efetivamente realizando a seleÃ§Ã£o de variÃ¡veis [^7.4.4]. A **regularizaÃ§Ã£o L2** (Ridge) adiciona uma penalidade proporcional Ã  soma dos quadrados dos parÃ¢metros. Isso resulta em soluÃ§Ãµes com coeficientes menores, mas geralmente diferentes de zero, aumentando a estabilidade do modelo [^7.4.4]. A **Elastic Net** combina as penalidades L1 e L2, buscando um equilÃ­brio entre sparsity e estabilidade [^7.5].

A regularizaÃ§Ã£o Ã© incorporada na funÃ§Ã£o de custo, seja da regressÃ£o logÃ­stica ou de outros modelos, da seguinte maneira. Considere o caso da regressÃ£o logÃ­stica, a funÃ§Ã£o de custo a ser minimizada se torna:

$$
J(\beta) = - \sum_{i=1}^N [y_i \log(p(x_i)) + (1-y_i) \log(1-p(x_i))] + \lambda \sum_{j=1}^p |\beta_j|
$$
para o caso L1, e

$$
J(\beta) = - \sum_{i=1}^N [y_i \log(p(x_i)) + (1-y_i) \log(1-p(x_i))] + \lambda \sum_{j=1}^p \beta_j^2
$$
para o caso L2, onde $\lambda$ Ã© um hiperparÃ¢metro que controla a forÃ§a da regularizaÃ§Ã£o, $p(x_i)$ Ã© a probabilidade predita pela regressÃ£o logÃ­stica, e $y_i$ Ã© a classe real.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que estamos treinando um modelo de regressÃ£o logÃ­stica com 5 preditores ($x_1, x_2, x_3, x_4, x_5$) e as estimativas iniciais dos coeficientes sÃ£o $\beta = [0.8, -0.5, 0.2, 1.2, -0.1]$. Se usarmos regularizaÃ§Ã£o L1 com $\lambda = 0.5$, a penalidade serÃ¡ $0.5 * (|0.8| + |-0.5| + |0.2| + |1.2| + |-0.1|) = 1.4$. A otimizaÃ§Ã£o do modelo com essa penalidade tenderÃ¡ a reduzir alguns coeficientes a zero. Com L2, a penalidade seria $0.5 * (0.8^2 + (-0.5)^2 + 0.2^2 + 1.2^2 + (-0.1)^2) = 1.135$, que levaria a coeficientes menores mas nÃ£o necessariamente zero.

**Lemma 3:** A penalizaÃ§Ã£o L1 em um modelo de classificaÃ§Ã£o logÃ­stica leva Ã  esparsidade dos coeficientes. Isso pode ser provado mostrando que, na otimizaÃ§Ã£o da funÃ§Ã£o de custo, a derivada da funÃ§Ã£o de penalidade L1 (que Ã© a funÃ§Ã£o sinal) faz com que os coeficientes tendam a zero, se o efeito do atributo for fraco. Os coeficientes que nÃ£o tiverem magnitude suficiente para compensar a penalidade, convergirÃ£o para zero, portanto, selecionando as variÃ¡veis mais importantes.

**Prova do Lemma 3:** A funÃ§Ã£o de custo com L1 Ã© nÃ£o-diferenciÃ¡vel em $\beta=0$. No entanto, se utilizarmos o conceito de subgradiente, podemos analisar o problema. O subgradiente da parte penalizada do custo, $\lambda \sum_{j=1}^p |\beta_j|$, com respeito a $\beta_j$ Ã© $\lambda sign(\beta_j)$, onde $sign(\beta_j) = 1$ se $\beta_j>0$, $-1$ se $\beta_j<0$ e qualquer valor em [-1,1] se $\beta_j = 0$. Para qualquer $\beta_j$ nÃ£o nulo, a direÃ§Ã£o do subgradiente o empurra em direÃ§Ã£o a zero (jÃ¡ que a penalidade Ã© sempre positiva). No caso de $\beta_j=0$, o subgradiente estÃ¡ entre $-\lambda$ e $\lambda$, e ele sÃ³ nÃ£o serÃ¡ nulo se a derivada do restante da funÃ§Ã£o de custo tiver uma magnitude maior que $\lambda$, caso contrÃ¡rio o parÃ¢metro permanecerÃ¡ em $\beta_j=0$. $\blacksquare$

**CorolÃ¡rio 3:** A esparsidade induzida pela penalizaÃ§Ã£o L1 simplifica o modelo e melhora sua interpretabilidade, pois apenas um subconjunto de variÃ¡veis contribui significativamente para a prediÃ§Ã£o. Isso facilita a identificaÃ§Ã£o dos fatores mais relevantes para a classificaÃ§Ã£o [^7.4.5].

> âš ï¸ **Ponto Crucial**: L1 e L2 podem ser combinadas (Elastic Net) para aproveitar vantagens de ambos os tipos de regularizaÃ§Ã£o, conforme discutido em [^7.5].

### Separating Hyperplanes e Perceptrons
```mermaid
flowchart TD
    subgraph "Hiperplanos Separadores"
        A["Dados de Entrada X com Classes"] --> B["Definir Hiperplano Separador"];
        B --> C["Maximizar Margem (DistÃ¢ncia aos Vetores de Suporte)"];
        C --> D["Otimizar ParÃ¢metros do Hiperplano"];
        D --> E["Fronteira de DecisÃ£o Ã“tima"];
    end
    subgraph "Perceptron"
        F["InicializaÃ§Ã£o dos Pesos"] --> G["Classificar Amostra"];
        G --> H{"ClassificaÃ§Ã£o Incorreta?"};
        H -- Sim --> I["Atualizar Pesos"];
        I --> G;
        H -- NÃ£o --> J["ConvergÃªncia"];
    end
    style A fill:#aaf,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
```
Os **hiperplanos separadores** sÃ£o utilizados em classificaÃ§Ã£o para dividir o espaÃ§o de caracterÃ­sticas em regiÃµes correspondentes Ã s diferentes classes [^7.5.2]. O objetivo Ã© encontrar um hiperplano que maximize a margem de separaÃ§Ã£o entre as classes, ou seja, a distÃ¢ncia mÃ­nima entre o hiperplano e os pontos mais prÃ³ximos de cada classe, chamados de vetores de suporte. Este conceito Ã© a base dos Support Vector Machines (SVMs). A formulaÃ§Ã£o matemÃ¡tica desse problema de otimizaÃ§Ã£o Ã© dada pela maximizaÃ§Ã£o da margem, que geralmente Ã© implementada utilizando o dual de Wolfe [^7.5.2]. Essa formulaÃ§Ã£o leva a soluÃ§Ãµes que sÃ£o combinaÃ§Ãµes lineares dos pontos de suporte.

O **Perceptron** de Rosenblatt Ã© um algoritmo de aprendizado que busca um hiperplano separador, atualizando iterativamente seus parÃ¢metros com base na classificaÃ§Ã£o incorreta das amostras. O algoritmo garante convergÃªncia quando os dados sÃ£o linearmente separÃ¡veis, e cada atualizaÃ§Ã£o move o hiperplano em direÃ§Ã£o a uma melhor separaÃ§Ã£o. A formulaÃ§Ã£o da atualizaÃ§Ã£o dos parÃ¢metros envolve a adiÃ§Ã£o de um mÃºltiplo do vetor de entrada correspondente a amostras mal classificadas ao vetor de pesos, atÃ© que todos os dados sejam classificados corretamente [^7.5.1].

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine que temos um perceptron com pesos iniciais $w = [0.1, -0.2]$ e bias $b=0.3$. Temos um ponto $x=[2, 1]$ que Ã© da classe positiva (y=1), mas a prediÃ§Ã£o Ã© dada por $\hat{y} = w^T x + b = 0.1*2 + (-0.2)*1 + 0.3 = 0.3$ que Ã© menor que 0, indicando que o ponto foi classificado incorretamente. A atualizaÃ§Ã£o do peso Ã© feita somando o vetor de entrada $x$ ao vetor de peso $w$, resultando em  $w_{new} = w_{old} + \eta x$, onde $\eta$ Ã© a taxa de aprendizado (por exemplo 0.1). $w_{new} = [0.1, -0.2] + 0.1*[2, 1] = [0.3, -0.1]$. Isso leva a uma prediÃ§Ã£o mais favorÃ¡vel para a classificaÃ§Ã£o correta.

### Pergunta TeÃ³rica AvanÃ§ada: Quais as diferenÃ§as fundamentais entre a formulaÃ§Ã£o de LDA e a Regra de DecisÃ£o Bayesiana considerando distribuiÃ§Ãµes Gaussianas com covariÃ¢ncias iguais?

**Resposta:** Sob a hipÃ³tese de distribuiÃ§Ãµes Gaussianas e covariÃ¢ncias iguais, a LDA se torna equivalente Ã  regra de decisÃ£o Bayesiana, e ambas as abordagens levam Ã  mesma fronteira de decisÃ£o linear [^7.3]. A LDA assume que as classes sÃ£o Gaussianas com covariÃ¢ncias iguais. A regra de decisÃ£o Bayesiana, por sua vez, busca classificar uma amostra na classe com maior probabilidade posterior [^7.3]. Em outras palavras, a regra de decisÃ£o Bayesiana, para este caso particular, se transforma na regra de decisÃ£o do LDA.
```mermaid
graph TB
    subgraph "EquivalÃªncia LDA e Bayes"
        A["DistribuiÃ§Ãµes Gaussianas com CovariÃ¢ncias Iguais"] --> B["LDA: ProjeÃ§Ã£o Maximiza SeparaÃ§Ã£o de MÃ©dias"];
        A --> C["Regra de DecisÃ£o Bayesiana: Classificar na Classe com Maior Probabilidade Posterior"];
        B & C --> D["Mesma Fronteira de DecisÃ£o Linear: w^T x + b = 0"];
    end
```
O limite de decisÃ£o entre duas classes, sob essas condiÃ§Ãµes, Ã© um hiperplano perpendicular Ã  linha que une as mÃ©dias das classes e Ã© dado por [^7.3]:

$$
w^T x + b = 0
$$

onde $w$ Ã© o vetor de pesos, $x$ Ã© o vetor de caracterÃ­sticas e $b$ Ã© o termo de bias. A diferenÃ§a fundamental entre as formulaÃ§Ãµes Ã© que o LDA obtÃ©m o vetor $w$ pela inversÃ£o da covariÃ¢ncia empÃ­rica e a regra de decisÃ£o bayesiana obtÃ©m o mesmo vetor $w$ usando as probabilidades das classes. Em termos matemÃ¡ticos, ambas as formas de encontrar o hiperplano sÃ£o equivalentes [^7.3.3].

**Lemma 4:** Se as distribuiÃ§Ãµes das classes forem Gaussianas com covariÃ¢ncias iguais, a regra de decisÃ£o Bayesiana leva Ã  mesma fronteira de decisÃ£o linear que o LDA. Formalmente, isso pode ser comprovado mostrando que o log-odds ratio das probabilidades posteriores nas duas abordagens Ã© equivalente, resultando na mesma formulaÃ§Ã£o da fronteira de decisÃ£o [^7.3, 7.3.1].

**CorolÃ¡rio 4:** A Relaxando a hipÃ³tese de covariÃ¢ncias iguais, a regra de decisÃ£o bayesiana leva a fronteiras de decisÃ£o quadrÃ¡ticas, conhecido como Quadratic Discriminant Analysis (QDA), que permite diferentes formas de regiÃµes de decisÃ£o [^7.3]. Isso mostra a flexibilidade da abordagem Bayesiana em relaÃ§Ã£o ao LDA, que Ã© mais restritivo [^7.3].

> âš ï¸ **Ponto Crucial**: A adoÃ§Ã£o ou nÃ£o de covariÃ¢ncias iguais impacta fortemente o tipo de fronteira de decisÃ£o (linear vs. quadrÃ¡tica), conforme discutido em [^7.3.1].

### ConclusÃ£o

A escolha de modelos e a avaliaÃ§Ã£o de seu desempenho sÃ£o etapas cruciais no aprendizado estatÃ­stico. Este capÃ­tulo explorou a importÃ¢ncia da distinÃ§Ã£o entre **erro condicional** (Errt) e **erro esperado** (Err), mostrando como mÃ©todos como cross-validation e bootstrap fornecem estimativas, majoritariamente, do erro esperado, o qual quantifica o desempenho do modelo em mÃ©dia sobre todos os possÃ­veis conjuntos de treino. Modelos lineares, como LDA, regressÃ£o logÃ­stica e regressÃ£o de indicadores, foram detalhados, e suas limitaÃ§Ãµes foram discutidas, especialmente no contexto do trade-off entre viÃ©s e variÃ¢ncia. TÃ©cnicas de regularizaÃ§Ã£o, como Lasso e Ridge, podem ser incorporadas para evitar overfitting. A abordagem bayesiana, que leva a uma formulaÃ§Ã£o similar do BIC, tambÃ©m foi abordada para seleÃ§Ã£o de modelos. Uma discussÃ£o detalhada sobre a validade do cross-validation tambÃ©m foi apresentada, mostrando seus limites.
```mermaid
graph TB
    subgraph "ConclusÃ£o"
    A["DiferenÃ§a entre Errt e Err"] --> B["Estimativas do Erro Esperado: CV e Bootstrap"];
    A --> C["Modelos Lineares: LDA, RegressÃ£o LogÃ­stica, RegressÃ£o de Indicadores"];
    C --> D["Trade-off ViÃ©s-VariÃ¢ncia"];
    C --> E["RegularizaÃ§Ã£o: Lasso e Ridge"];
    A --> F["Abordagem Bayesiana para SeleÃ§Ã£o de Modelos"];
    A --> G["Limites do Cross-Validation"];
    end
```

### Footnotes
[^7.1]: "The generalization performance of a learning method relates to its predic-tion capability on independent test data. Assessment of this performance is extremely important in practice, since it guides the choice of learning method or model, and gives us a measure of the quality of the ultimately chosen model." *(Trecho de <Model Assessment and Selection>)*
[^7.2]: "Figure 7.1 illustrates the important issue in assessing the ability of a learn-ing method to generalize. Consider first the case of a quantitative or interval scale response. We have a target variable Y, a vector of inputs X, and a prediction model f(X) that has been estimated from a training set T." *(Trecho de <Model Assessment and Selection>)*
[^7.3]: "The story is similar for a qualitative or categorical response G taking one of K values in a set G, labeled for convenience as 1, 2, ..., K. Typically we model the probabilities pk(X) = Pr(G = k|X) (or some monotone transformations fr(X)), and then Äœ(X) = arg maxk ÃŽk(X)." *(Trecho de <Model Assessment and Selection>)*
[^7.3.1]:  "Typically we model the probabilities pk(X) = Pr(G = k|X) (or some monotone transformations fr(X)), and then Äœ(X) = arg maxk ÃŽk(X). In some cases, such as 1-nearest neighbor classification (Chapters 2 and 13) we produce Äœ(X) directly. Typical loss functions are" *(Trecho de <Model Assessment and Selection>)*
[^7.3.2]: "The quantity -2 Ã— the log-likelihood is sometimes referred to as the deviance." *(Trecho de <Model Assessment and Selection>)*
[^7.3.3]: "Again, test error here is Errã…œ = E[L(G, Äœ(X))|T], the population mis-classification error of the classifier trained on T, and Err is the expected misclassification error." *(Trecho de <Model Assessment and Selection>)*
[^7.4]:  "The log-likelihood can be used as a loss-function for general response densities, such as the Poisson, gamma, exponential, log-normal and others." *(Trecho de <Model Assessment and Selection>)*
[^7.4.1]: "If Pro(x) (Y) is the density of Y, indexed by a parameter 0(X) that depends on the predictor X, then L(Y,0(X)) = âˆ’2. log Pro(x) (Y)." *(Trecho de <Model Assessment and Selection>)*
[^7.4.2]: "The "-2" in the definition makes the log-likelihood loss for the Gaussian distribution match squared-error loss." *(Trecho de <Model Assessment and Selection>)*
[^7.4.3]: "For ease of exposition, for the remainder of this chapter we will use Y and f(X) to represent all of the above situations, since we focus mainly on the quantitative response (squared-error loss) setting." *(Trecho de <Model Assessment and Selection>)*
[^7.4.4]: "In this chapter we describe a number of methods for estimating the expected test error for a model." *(Trecho de <Model Assessment and Selection>)*
[^7.4.5]: "Typically our model will have a tuning parameter or parameters a and so we can write our predictions as fa(x). The tuning parameter varies the complexity of our model, and we wish to find the value of a that minimizes error, that is, produces the minimum of the average test error curve in Figure 7.1." *(Trecho de <Model Assessment and Selection>)*
[^7.5]: "Having said this, for brevity we will often suppress the dependence of f(x) on a." *(Trecho de <Model Assessment and Selection>)*
[^7.5.1]: "It is important to note that there are in fact two separate goals that we might have in mind: Model selection: estimating the performance of different models in order to choose the best one." *(Trecho de <Model Assessment and Selection>)*
[^7.5.2]: "Model assessment: having chosen a final model, estimating its predic-tion error (generalization error) on new data." *(Trecho de <Model Assessment and Selection>)*
