## Effective Parameters in Generalized Cross-Validation (GCV)

```mermaid
graph LR
    subgraph "Generalized Cross-Validation (GCV) Overview"
        direction LR
        A["Input Data: 'y'"] --> B["Model Prediction: '≈∑' = 'Sy'"]
        B --> C["Residuals: 'y - ≈∑'"]
        C --> D["GCV Calculation"]
        D --> E["Output: GCV(f)"]
        B --> F["Smoothing Matrix: 'S'"]
        F --> D
        F --> G["trace(S) / N"]
        G --> D
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#cfc,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o
Este cap√≠tulo aborda a import√¢ncia de avaliar o desempenho de modelos estat√≠sticos e como selecionar o modelo mais adequado para um determinado problema. Em particular, o foco √© no **Generalized Cross-Validation (GCV)** como um m√©todo para estimar o erro de predi√ß√£o, principalmente em modelos lineares. O objetivo principal √© entender como o conceito de **effective number of parameters** surge no contexto do GCV, derivando uma formula√ß√£o para o c√°lculo do erro de predi√ß√£o em fun√ß√£o da complexidade do modelo [^7.1]. Al√©m disso, exploramos o conceito de **optimism** do erro de treino e como o GCV busca mitigar esse problema [^7.4].

### Conceitos Fundamentais
**Conceito 1: Generalized Cross-Validation (GCV)**
O GCV √© uma t√©cnica que estima o erro de predi√ß√£o de um modelo atrav√©s de uma aproxima√ß√£o do leave-one-out cross-validation (LOOCV) para modelos lineares. Em vez de refazer o modelo N vezes (onde N √© o n√∫mero de observa√ß√µes), como no LOOCV, o GCV usa uma √∫nica matriz 'S' para calcular os res√≠duos. Matematicamente, para um modelo linear onde ≈∑ = Sy, o erro de predi√ß√£o no GCV √© dado por:
$$
GCV(f) = \frac{1}{N} \sum_{i=1}^{N} \left( \frac{y_i - \hat{y}_i}{1 - \text{trace}(S)/N} \right)^2
$$
onde $y_i$ s√£o as observa√ß√µes, $\hat{y}_i$ s√£o as predi√ß√µes, 'S' √© a matriz de "smoothing" que define a rela√ß√£o entre $y$ e $\hat{y}$ e $trace(S)$ √© a soma dos elementos da diagonal principal de S, tamb√©m conhecido como **effective degrees of freedom** [^7.10].

> üí° **Exemplo Num√©rico:**
> Suponha que temos um conjunto de dados com $N = 100$ observa√ß√µes e ajustamos um modelo linear tal que a matriz S tem um `trace(S) = 5`. Os valores de $y_i$ e $\hat{y}_i$ para as primeiras 5 observa√ß√µes s√£o:
>
> | i   | $y_i$ | $\hat{y}_i$ |
> |-----|-------|-------------|
> | 1   | 2.5   | 2.3         |
> | 2   | 3.1   | 3.4         |
> | 3   | 4.0   | 3.8         |
> | 4   | 2.8   | 3.0         |
> | 5   | 3.5   | 3.3         |
>
> O erro GCV para essas observa√ß√µes √© calculado como:
>  ```python
>  import numpy as np
>
>  y = np.array([2.5, 3.1, 4.0, 2.8, 3.5])
>  y_hat = np.array([2.3, 3.4, 3.8, 3.0, 3.3])
>  N = 100
>  trace_S = 5
>  gcv_values = ((y - y_hat) / (1 - trace_S/N))**2
>  gcv_mean = np.mean(gcv_values)
>  print(f"GCV para as primeiras 5 observa√ß√µes: {gcv_values}")
>  print(f"M√©dia do GCV para as 5 primeiras observa√ß√µes: {gcv_mean}")
> ```
>
> O c√≥digo Python calcula os valores de GCV para as primeiras 5 observa√ß√µes e a m√©dia desses valores.
>
> Os res√≠duos ajustados $(y_i - \hat{y}_i)/(1 - \text{trace}(S)/N)$ s√£o penalizados pelo termo $(1 - \text{trace}(S)/N)$, que cresce com o aumento do n√∫mero efetivo de par√¢metros.

**Lemma 1: Matriz de "Smoothing" S**
A matriz S, tamb√©m conhecida como matriz de *hat*, √© crucial na formula√ß√£o do GCV. Se um modelo ajustado pode ser expresso como ≈∑ = Sy, onde y √© o vetor de respostas, ent√£o a matriz S descreve como as respostas s√£o "smoothed" pelo modelo.  A matriz S depende dos vetores de entrada x·µ¢, mas n√£o das respostas y·µ¢ [^7.6].
$$
\hat{y} = S y
$$
**Prova:**
A matriz S depende dos preditores e da estrutura do modelo, e √© utilizada para prever os valores de y, como mostrado em [^7.3.1].
$\blacksquare$

```mermaid
graph LR
    subgraph "Smoothing Matrix S"
        direction LR
        A["Input Data 'x_i'"] --> B["Model Structure"]
        B --> C["Smoothing Matrix 'S'"]
        C --> D["Predictions '≈∑'"]
        E["Responses 'y'"] --> D
        C --> F["'≈∑' = 'Sy'"]
    end
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#cfc,stroke:#333,stroke-width:2px
```

**Corol√°rio 1: Rela√ß√£o entre o Res√≠duo e a Matriz S**
Em modelos lineares, √© poss√≠vel expressar o res√≠duo do modelo em termos dos elementos da matriz S.  Seja $\hat{y}_i$ a predi√ß√£o na i-√©sima observa√ß√£o, ent√£o o res√≠duo cruzado √©:
$$
y_i - \hat{y}_{-i}(x_i) = \frac{y_i - \hat{y}_i}{1 - S_{ii}}
$$
onde $S_{ii}$ √© o i-√©simo elemento da diagonal principal de S [^7.10].

> üí° **Exemplo Num√©rico:**
> Considere a observa√ß√£o $i=1$ do exemplo anterior, com $y_1 = 2.5$ e $\hat{y}_1 = 2.3$. Suponha que o elemento diagonal correspondente da matriz S seja $S_{11} = 0.04$.
> O res√≠duo cruzado para esta observa√ß√£o √© ent√£o:
> $$
> y_1 - \hat{y}_{-1}(x_1) = \frac{2.5 - 2.3}{1 - 0.04} = \frac{0.2}{0.96} \approx 0.208
> $$
> Este valor √© o res√≠duo que ter√≠amos se o modelo fosse ajustado sem utilizar a observa√ß√£o $i=1$. Este res√≠duo √© ent√£o usado no c√°lculo do GCV.

**Conceito 2: Effective Number of Parameters**
Em GCV, o termo trace(S) √© interpretado como o n√∫mero efetivo de par√¢metros do modelo, o que permite comparar modelos de diferentes complexidades. Essa m√©trica est√° relacionada com o n√∫mero de graus de liberdade do modelo. Se S for uma proje√ß√£o ortogonal numa base de M fun√ß√µes, ent√£o trace(S) = M [^7.6].

```mermaid
graph LR
    subgraph "Effective Parameters"
        direction TB
        A["Smoothing Matrix 'S'"] --> B["trace(S)"]
        B --> C["Effective Number of Parameters"]
        C --> D["Model Complexity Measure"]
        C --> E["Degrees of Freedom"]
    end
    style B fill:#ccf,stroke:#333,stroke-width:2px
```

**Lemma 2: N√∫mero Efetivo de Par√¢metros em Modelos Lineares**
Para modelos lineares, a soma dos elementos da diagonal principal da matriz S, trace(S), representa o n√∫mero efetivo de par√¢metros do modelo. Matematicamente, df(S) = trace(S) [^7.6].
**Prova:**
A express√£o df(S) = trace(S) √© diretamente derivada da defini√ß√£o de modelos lineares, onde a predi√ß√£o ≈∑ √© uma combina√ß√£o linear dos dados de entrada y, definida pela matriz S. Este resultado √© fundamental para a compreens√£o da complexidade de um modelo linear, como indicado em [^7.6]. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Imagine um modelo linear simples com dois preditores (al√©m do intercepto). Se a matriz S for uma proje√ß√£o ortogonal, ent√£o trace(S) = 3. Isso significa que o modelo tem tr√™s graus de liberdade. Em um modelo mais complexo, como um modelo polinomial de grau 5, trace(S) ser√° maior (p.ex. 6). O GCV usa essa informa√ß√£o para ajustar a avalia√ß√£o do erro.

**Conceito 3: Optimism do Erro de Treino**
O erro de treino (erro observado nos dados de treino) geralmente √© otimista, pois o modelo se ajusta aos dados com os quais foi treinado. O GCV visa corrigir esse otimismo usando o effective number of parameters e os res√≠duos ajustados. A m√©trica de *optimism* do erro √© dada por:
$$
op = Errin - err
$$
onde $Errin$ √© o erro de predi√ß√£o da amostra e $err$ o erro de treino [^7.4].

> üí° **Exemplo Num√©rico:**
> Considere um cen√°rio onde o erro de treino de um modelo √© de 0.5 e o erro de predi√ß√£o da amostra (obtido usando valida√ß√£o cruzada) √© de 0.7. O otimismo do erro de treino seria:
> $$
> op = 0.7 - 0.5 = 0.2
> $$
> O GCV procura aproximar o erro de predi√ß√£o (0.7) usando o erro de treino (0.5) e o effective number of parameters para corrigir o otimismo.

> ‚ö†Ô∏è **Nota Importante**: O GCV aproxima o erro de predi√ß√£o corrigindo o erro de treino com um termo que depende da complexidade do modelo (trace(S)) [^7.10].
> ‚ùó **Ponto de Aten√ß√£o**: A escolha de uma m√©trica adequada para medir o erro √© crucial e pode depender do problema (regress√£o ou classifica√ß√£o).  Em classifica√ß√£o, pode-se usar o log-likelihood. [^7.2]
> ‚úîÔ∏è **Destaque**: Embora o GCV seja geralmente aplicado em modelos lineares, existem extens√µes para outros modelos [^7.1].

### Regress√£o Linear e M√≠nimos Quadrados para GCV
```mermaid
graph LR
    subgraph "Linear Regression and GCV"
        direction TB
        A["Input Matrix 'X'"] --> B["'S' = 'X(X^T X)^-1 X^T'"]
        B --> C["Predictions: '≈∑ = Sy'"]
        D["Response Vector 'y'"] --> C
        C --> E["Residuals ('y - ≈∑')"]
        E --> F["GCV Calculation"]
        B --> G["trace(S) = 'p' (number of predictors)"]
        G --> F
    end
    style B fill:#ccf,stroke:#333,stroke-width:2px
```

A Regress√£o Linear √© um caso especial de modelos lineares onde a matriz S pode ser expressa explicitamente em termos das matrizes de input X. No contexto do GCV, para um modelo linear com ajuste por m√≠nimos quadrados:
$$
\hat{y} = X(X^T X)^{-1}X^T y = S y
$$
Onde $S = X(X^T X)^{-1}X^T$,  onde X √© a matriz de design, e y √© o vetor de respostas. Nesse caso,  trace(S) = p, o n√∫mero de preditores (par√¢metros) em X.

**Lemma 3: GCV em Regress√£o Linear**
Para um modelo linear ajustado por m√≠nimos quadrados, o erro GCV √© expresso como:
$$
GCV(f) = \frac{1}{N} \sum_{i=1}^N \frac{(y_i - \hat{y}_i)^2}{(1-p/N)^2}
$$
onde p √© o n√∫mero de preditores (par√¢metros) no modelo, ou seja, trace(S) = p [^7.10].
**Prova:**
A prova √© direta, substituindo $trace(S) = p$ na f√≥rmula geral do GCV.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
> Suponha que temos um modelo de regress√£o linear com $p=4$ preditores (incluindo o intercepto) e $N=100$ observa√ß√µes. O erro GCV √© calculado como:
> $$
> GCV(f) = \frac{1}{100} \sum_{i=1}^{100} \frac{(y_i - \hat{y}_i)^2}{(1-4/100)^2}
> $$
> A penaliza√ß√£o no denominador $(1-4/100)^2 = (0.96)^2 = 0.9216$.  O erro quadr√°tico m√©dio dos res√≠duos √© inflacionado pelo fator $1/0.9216 \approx 1.085$, corrigindo o erro de treino otimista.
>
> Se o erro de treino fosse $\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 = 0.6$, o GCV seria $0.6 * 1.085 = 0.651$.

**Corol√°rio 2: GCV como Corre√ß√£o do Erro de Treino**
O GCV pode ser visto como uma corre√ß√£o do erro de treino, penalizando modelos com maior n√∫mero de par√¢metros.  A parte  $\frac{1}{(1-p/N)^2}$ age como um fator de penaliza√ß√£o que aumenta √† medida que a complexidade do modelo (p) se aproxima do tamanho da amostra (N).
*Limita√ß√µes da Regress√£o Linear para Classifica√ß√£o e GCV:*
- Em problemas de classifica√ß√£o, regress√£o linear de indicadores pode levar a valores fora do intervalo [0,1] [^4.2].
- O GCV assume res√≠duos Gaussianos, o que pode n√£o ser realista em alguns contextos [^7.2].
- O GCV √© uma aproxima√ß√£o, n√£o sendo um substituto exato para o LOOCV [^7.10].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em GCV
Quando a regulariza√ß√£o √© aplicada em modelos lineares, o c√°lculo do effective number of parameters muda,  afetando o GCV. Por exemplo, em *ridge regression*:
$$
\hat{y} = X(X^T X + \alpha I)^{-1} X^T y = S_\alpha y
$$
onde $S_\alpha$ √© a matriz de *smoothing* em ridge regression e $\alpha$ √© o par√¢metro de regulariza√ß√£o. O trace de $S_\alpha$, usado no GCV, representa o n√∫mero efetivo de par√¢metros do modelo regularizado, que ser√° menor que o n√∫mero de par√¢metros originais quando $\alpha>0$ [^7.6].

```mermaid
graph LR
    subgraph "Ridge Regression and GCV"
        direction TB
         A["Input Matrix 'X'"] --> B["'S_Œ±' = 'X(X^T X + Œ±I)^-1 X^T'"]
        B --> C["Predictions: '≈∑ = S_Œ±y'"]
        D["Response Vector 'y'"] --> C
        C --> E["Residuals ('y - ≈∑')"]
        E --> F["GCV Calculation"]
        B --> G["trace(S_Œ±) - Effective parameters depend on Œ±"]
        G --> F
        H["Regularization Parameter 'Œ±'"] --> G
    end
    style B fill:#ccf,stroke:#333,stroke-width:2px
```

**Lemma 4: Regulariza√ß√£o em GCV**
Com regulariza√ß√£o, a matriz $S$ n√£o √© mais uma proje√ß√£o ortogonal, e o n√∫mero efetivo de par√¢metros, trace(S), √© um valor que depende de $\alpha$ e, geralmente, √© menor que o n√∫mero de par√¢metros originais [^7.3].
$$df(\alpha) = trace(X(X^TX + \alpha I)^{-1} X^T)$$
**Prova:**
A prova segue diretamente das propriedades da matriz $S$ em regress√£o regularizada onde $S$ n√£o √© mais uma proje√ß√£o ortogonal [^7.3]. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um modelo com $p=10$ preditores. Em regress√£o linear sem regulariza√ß√£o, $trace(S) = p = 10$. Em ridge regression, o trace(S) √© uma fun√ß√£o de $\alpha$.
>
> Para um valor de $\alpha = 0.1$, suponha que o `trace(S) = 6` calculado seja $df(0.1) = trace(X(X^TX + 0.1I)^{-1} X^T) = 6$. Para um valor de $\alpha = 1$, suponha que  `trace(S) = 3`. Note que $trace(S) < 10$.
>
> O GCV em ridge regression penaliza menos o erro de treino quando $\alpha$ aumenta. Isso ocorre pois a regulariza√ß√£o reduz a complexidade do modelo.
> ```python
> import numpy as np
> from sklearn.linear_model import Ridge
> from sklearn.datasets import make_regression
>
> X, y = make_regression(n_samples=100, n_features=10, random_state=42)
>
> alpha_values = [0.1, 1.0, 10.0]
>
> for alpha in alpha_values:
>    ridge = Ridge(alpha=alpha, fit_intercept=False)
>    ridge.fit(X,y)
>    S = X @ np.linalg.inv(X.T @ X + alpha * np.eye(X.shape[1])) @ X.T
>    effective_params = np.trace(S)
>    print(f"Alpha: {alpha}, Effective Parameters: {effective_params:.2f}")
> ```
> Este c√≥digo Python demonstra como o effective number of parameters muda com o par√¢metro de regulariza√ß√£o $\alpha$.

**Corol√°rio 3: Impacto da Regulariza√ß√£o no GCV**
A regulariza√ß√£o impacta o c√°lculo do GCV ao reduzir o n√∫mero efetivo de par√¢metros, reduzindo assim a complexidade do modelo e penalizando modelos mais complexos no processo de ajuste [^7.3].
> ‚ö†Ô∏è **Ponto Crucial**: A escolha de $\alpha$ pode ser feita minimizando o erro GCV, criando um processo de sele√ß√£o de par√¢metros [^7.6].
### Separating Hyperplanes e GCV
O conceito de hiperplanos separadores pode ser usado em classifica√ß√£o atrav√©s de modelos lineares. GCV, embora concebido primariamente para regress√£o, pode ser aplicado para avaliar o desempenho de modelos de classifica√ß√£o linear. Os detalhes da aplica√ß√£o dependem da fun√ß√£o de custo escolhida para avalia√ß√£o do modelo, que se adapta a outras m√©tricas como *log-likelihood* em modelos de regress√£o log√≠stica [^7.2].

### Pergunta Te√≥rica Avan√ßada: Qual a rela√ß√£o entre o effective number of parameters e o vi√©s e vari√¢ncia em modelos lineares regularizados como a ridge regression?
**Resposta:**
Em modelos lineares regularizados, como a *ridge regression*, o effective number of parameters √© uma medida de complexidade do modelo que influencia o vi√©s e a vari√¢ncia. Regularizar um modelo adiciona vi√©s √†s estimativas dos par√¢metros, mas reduz a vari√¢ncia. O GCV busca um balan√ßo entre vi√©s e vari√¢ncia atrav√©s da otimiza√ß√£o do effective number of parameters. O aumento do par√¢metro de regulariza√ß√£o $\alpha$ leva a uma diminui√ß√£o no effective number of parameters, reduzindo a vari√¢ncia do modelo ao custo de aumentar o vi√©s.

**Lemma 5: Rela√ß√£o entre effective number of parameters e vi√©s e vari√¢ncia**
O effective number of parameters, trace(S), controla a complexidade do modelo. Reduzir trace(S), atrav√©s da regulariza√ß√£o, implica um aumento no vi√©s e uma redu√ß√£o na vari√¢ncia.
$$Err(x_0) = \sigma^2 + \text{Bias}^2(f(x_0)) + \text{Var}(f(x_0))$$

**Prova:**
A prova √© baseada na an√°lise da decomposi√ß√£o de vi√©s e vari√¢ncia, onde a complexidade do modelo controla o balan√ßo entre as duas componentes [^7.3]. $\blacksquare$
```mermaid
graph TB
    subgraph "Bias-Variance Tradeoff"
        direction TB
        A["Model Complexity: 'trace(S)'"] --> B["Bias Component"]
        A --> C["Variance Component"]
        B --> D["Total Error: 'MSE'"]
        C --> D
        E["Regularization Parameter Œ±"] --> A
        style A fill:#ccf,stroke:#333,stroke-width:2px
    end
```
**Corol√°rio 4: Balan√ßo Bias-Vari√¢ncia no GCV**
O objetivo do GCV √© encontrar o valor ideal de trace(S) que minimiza o erro total, balanceando vi√©s e vari√¢ncia. A complexidade do modelo √© controlada indiretamente atrav√©s da escolha do par√¢metro de regulariza√ß√£o (ex: $\alpha$), que afeta trace(S) [^7.3.1].
> ‚ö†Ô∏è **Ponto Crucial**: Modelos com alto vi√©s (como modelos muito simples ou excessivamente regularizados) tendem a subestimar o erro real, enquanto modelos com alta vari√¢ncia (como modelos muito complexos) tendem a sobreajustar o ru√≠do nos dados de treinamento [^7.2].

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que para um dado conjunto de dados, testamos tr√™s modelos de ridge regression com diferentes valores de $\alpha$: $\alpha_1 = 0.01$, $\alpha_2 = 1$ e $\alpha_3 = 10$. Ap√≥s o treinamento e avalia√ß√£o, obtemos os seguintes resultados:
>
> | Modelo       | $\alpha$ | Effective Parameters (trace(S)) | Vi√©s¬≤ | Vari√¢ncia | MSE (Erro M√©dio Quadr√°tico) |
> |--------------|----------|---------------------------------|-------|-----------|-----------------------------|
> | Ridge 1    | 0.01     | 9.5                             | 0.05  | 0.20      | 0.25                        |
> | Ridge 2    | 1        | 6                               | 0.15  | 0.15      | 0.30                        |
> | Ridge 3    | 10       | 2                               | 0.30  | 0.10      | 0.40                        |
>
> O modelo com $\alpha = 0.01$ tem o menor vi√©s, mas a maior vari√¢ncia. O modelo com $\alpha = 10$ tem o maior vi√©s, mas a menor vari√¢ncia. O modelo com $\alpha = 1$ oferece um bom balan√ßo entre vi√©s e vari√¢ncia, com o menor erro quadr√°tico m√©dio total.  O GCV tenderia a escolher o modelo com $\alpha=1$ neste caso.

### Conclus√£o
O Generalized Cross-Validation (GCV) √© uma ferramenta √∫til para estimar o erro de predi√ß√£o em modelos lineares. Ele corrige o otimismo do erro de treino, usando o conceito de effective number of parameters para controlar a complexidade do modelo e equilibrar vi√©s e vari√¢ncia. Ao aproximar o leave-one-out cross-validation de maneira eficiente, o GCV oferece uma alternativa computacionalmente vi√°vel para modelos complexos, como os regularizados. Embora tenha limita√ß√µes, como a pressuposi√ß√£o de res√≠duos Gaussianos e o foco em modelos lineares, o GCV √© uma ferramenta valiosa na sele√ß√£o de modelos.
<!-- END SECTION -->
### Footnotes
[^7.1]: *‚ÄúIn this chapter we describe and illustrate the key methods for performance assessment, and show how they are used to select models. We begin the chapter with a discussion of the interplay between bias, variance and model complexity.‚Äù* *(Trecho de Model Assessment and Selection)*
[^7.2]: *‚ÄúThe story is similar for a qualitative or categorical response G taking one of K values in a set G, labeled for convenience as 1, 2, ..., K. Typically we model the probabilities pk(X) = Pr(G = k|X) (or some monotone transformations fr(X)), and then ƒú(X) = arg maxk √ék(X). In some cases, such as 1-nearest neighbor classification (Chapters 2 and 13) we produce G(X) directly. Typical loss functions are‚Äù* *(Trecho de Model Assessment and Selection)*
[^7.3]: *‚ÄúAs in Chapter 2, if we assume that Y = f(X) + Œµ where E(Œµ) = 0 and Var(Œµ) = œÉŒµ, we can derive an expression for the expected prediction error of a regression fit f(X) at an input point X = x0, using squared-error loss:‚Äù* *(Trecho de Model Assessment and Selection)*
[^7.3.1]: *‚ÄúThe test error Err(x0) for a ridge regression fit fa(xo) is identical in form to (7.11), except the linear weights in the variance term are different: h(x) = X(XTX + Œ±I)-1x0. The bias term will also be different.‚Äù* *(Trecho de Model Assessment and Selection)*
[^7.4]: *‚ÄúThe difference between Errin and the training error err: op = Errin ‚Äì err.‚Äù* *(Trecho de Model Assessment and Selection)*
[^7.6]: *‚ÄúThe concept of "number of parameters" can be generalized, especially to models where regularization is used in the fitting. Suppose we stack the outcomes Y1, Y2,..., yn into a vector y, and similarly for the predictions ≈∑. Then a linear fitting method is one for which we can write ≈∑ = Sy,‚Äù* *(Trecho de Model Assessment and Selection)*
[^7.10]: *‚ÄúGeneralized cross-validation provides a convenient approximation to leave-one out cross-validation, for linear fitting under squared-error loss. As defined in Section 7.6, a linear fitting method is one for which we can write ≈∑ = Sy.‚Äù* *(Trecho de Model Assessment and Selection)*
[^4.2]: *‚ÄúThe loss function for measuring errors between Y and f(X) is denoted by L(Y, f(X)). Typical choices are‚Äù* *(Trecho de Model Assessment and Selection)*
[^7.1]: *‚ÄúIn this chapter we describe a number of methods for estimating the expected test error for a model. Typically our model will have a tuning parameter or parameters a and so we can write our predictions as fa(x). The tuning parameter varies the complexity of our model, and we wish to find the value of a that minimizes error, that is, produces the minimum of the average test error curve in Figure 7.1. Having said this, for brevity we will often suppress the dependence of f(x) on a.‚Äù* *(Trecho de Model Assessment and Selection)*
<!-- END DOCUMENT -->
