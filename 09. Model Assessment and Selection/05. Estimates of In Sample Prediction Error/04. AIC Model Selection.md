## AIC para Sele√ß√£o de Modelos

<imagem: Diagrama ilustrando o processo de sele√ß√£o de modelos usando AIC, comparando modelos simples com modelos complexos e mostrando o ponto √≥timo onde o AIC √© minimizado.>

### Introdu√ß√£o

A avalia√ß√£o e sele√ß√£o de modelos s√£o etapas cruciais no processo de aprendizado estat√≠stico e de m√°quina. A capacidade de generaliza√ß√£o de um modelo, ou seja, sua habilidade de prever resultados em dados n√£o vistos, √© um dos principais indicadores de sua qualidade [^7.1]. O **Akaike Information Criterion (AIC)** √© uma das ferramentas estat√≠sticas mais utilizadas para a sele√ß√£o de modelos, fornecendo uma forma quantitativa de comparar diferentes modelos e escolher aquele que melhor equilibra o ajuste aos dados e a complexidade do modelo. Este cap√≠tulo ir√° explorar os detalhes do AIC, sua fundamenta√ß√£o te√≥rica e aplica√ß√µes pr√°ticas.

### Conceitos Fundamentais

**Conceito 1: Generaliza√ß√£o e Erro de Predi√ß√£o**

O objetivo prim√°rio de qualquer modelo de aprendizado √© a generaliza√ß√£o. Um bom modelo n√£o apenas se ajusta bem aos dados de treinamento, mas tamb√©m √© capaz de fazer previs√µes precisas em novos dados. O **erro de predi√ß√£o**, por sua vez, mede a discrep√¢ncia entre as previs√µes do modelo e os valores reais [^7.2]. O erro de predi√ß√£o pode ser decomposto em tr√™s componentes: o **erro irredut√≠vel**, o **vi√©s** (bias) e a **vari√¢ncia** [^7.3]. O erro irredut√≠vel representa a varia√ß√£o inerente nos dados, enquanto vi√©s e vari√¢ncia est√£o relacionados com a capacidade do modelo de se ajustar e generalizar. Modelos mais simples tendem a ter um vi√©s alto e uma vari√¢ncia baixa, enquanto modelos mais complexos apresentam vi√©s baixo e vari√¢ncia alta. A complexidade do modelo √© um fator crucial na sele√ß√£o, e o AIC auxilia na busca pelo equil√≠brio ideal [^7.2].

**Lemma 1: Decomposi√ß√£o do Erro de Predi√ß√£o**
O erro de predi√ß√£o esperado para um modelo de regress√£o linear pode ser decomposto como:
$$ Err(x_0) = \sigma^2 + Bias^2(f(x_0)) + Var(f(x_0)) $$
onde $\sigma^2$ √© o erro irredut√≠vel, $Bias^2(f(x_0))$ √© o quadrado do vi√©s, e $Var(f(x_0))$ √© a vari√¢ncia. Esta decomposi√ß√£o √© fundamental para entender o *trade-off* entre vi√©s e vari√¢ncia, e o AIC √© projetado para encontrar modelos que minimizem o erro de predi√ß√£o total [^7.3].
$\blacksquare$

> ```mermaid
> graph TD
> subgraph "Decomposition of Prediction Error"
>     direction TB
>     A["Expected Prediction Error: Err(x‚ÇÄ)"]
>     B["Irreducible Error: œÉ¬≤"]
>     C["Bias Squared: Bias¬≤(f(x‚ÇÄ))"]
>     D["Variance: Var(f(x‚ÇÄ))"]
>     A --> B
>     A --> C
>     A --> D
> end
> ```
> üí° **Exemplo Num√©rico:** Vamos supor que temos um modelo de regress√£o linear para prever o pre√ßo de casas. Ap√≥s treinar nosso modelo, podemos analisar o erro de predi√ß√£o em novos dados. Suponha que o erro irredut√≠vel $\sigma^2$ √© 1000, o vi√©s ao quadrado $Bias^2(f(x_0))$ √© 400 e a vari√¢ncia $Var(f(x_0))$ √© 100. Ent√£o, o erro de predi√ß√£o total seria $Err(x_0) = 1000 + 400 + 100 = 1500$. Se aumentarmos a complexidade do modelo, o vi√©s pode diminuir (por exemplo, para 100) mas a vari√¢ncia pode aumentar (por exemplo, para 600), resultando em um erro total de 1700, indicando que o modelo se tornou mais complexo mas n√£o necessariamente melhor em termos de generaliza√ß√£o. Um modelo ideal buscaria minimizar este erro total.
> ```mermaid
> graph LR
>     A[Erro Total] --> B(Erro Irredut√≠vel: 1000);
>     A --> C(Vi√©s¬≤: 400);
>     A --> D(Vari√¢ncia: 100);
>     B --> E[Modelo 1];
>     C --> E;
>     D --> E;
>     A --> F(Erro Total: 1500);
>     E --> F;
>
>     A1[Erro Total] --> B1(Erro Irredut√≠vel: 1000);
>     A1 --> C1(Vi√©s¬≤: 100);
>     A1 --> D1(Vari√¢ncia: 600);
>     B1 --> E1[Modelo 2];
>     C1 --> E1;
>     D1 --> E1;
>      A1 --> F1(Erro Total: 1700);
>      E1 --> F1;
>     style A fill:#f9f,stroke:#333,stroke-width:2px
>     style A1 fill:#f9f,stroke:#333,stroke-width:2px
> ```

**Conceito 2: Otimismo da Taxa de Erro de Treinamento**

A taxa de erro de treinamento (training error) √© um indicador da capacidade do modelo de se ajustar aos dados de treinamento. No entanto, ela √© *otimista* e, portanto, pode n√£o ser uma boa estimativa do erro de generaliza√ß√£o [^7.4]. Isso ocorre porque o modelo tende a se ajustar aos ru√≠dos dos dados de treinamento, superestimando sua performance. O **otimismo** da taxa de erro de treinamento √© definido como a diferen√ßa entre o erro in-sample ($Err_{in}$) e o erro de treinamento ($err$) [^7.4]:

$$ op = Err_{in} - err $$

A utiliza√ß√£o do erro de treinamento para escolha de modelos pode levar a *overfitting*, onde o modelo se ajusta muito bem aos dados de treino, mas generaliza mal para dados novos.

**Corol√°rio 1: Otimismo e Covari√¢ncia**
O otimismo da taxa de erro de treinamento pode ser expresso em termos da covari√¢ncia entre as predi√ß√µes e os valores observados:
$$ \omega = \frac{2}{N} \sum_{i=1}^{N} Cov(y_i, \hat{y_i}) $$
onde $y_i$ s√£o os valores observados, $\hat{y_i}$ s√£o as predi√ß√µes do modelo e $N$ √© o n√∫mero de amostras. Este resultado mostra como a for√ßa do ajuste do modelo aos dados afeta o otimismo da taxa de erro de treinamento [^7.4].
$\blacksquare$

> ```mermaid
> graph TD
> subgraph "Optimism and Covariance"
> direction TB
>     A["Optimism (œâ)"]
>     B["Covariance Sum: Œ£ Cov(y·µ¢, ≈∑·µ¢)"]
>     C["Scaling Factor: 2/N"]
>     A --> C
>     C --> B
> end
> ```
> üí° **Exemplo Num√©rico:** Considere um modelo linear com 10 amostras (N=10). As previs√µes $\hat{y_i}$ t√™m uma covari√¢ncia com os valores reais $y_i$. Vamos supor que $\sum_{i=1}^{10} Cov(y_i, \hat{y_i})$ seja igual a 25. Ent√£o, o otimismo da taxa de erro de treinamento seria $\omega = \frac{2}{10} \times 25 = 5$. Isso significa que o erro in-sample √© 5 unidades maior do que o erro de treinamento, indicando que o erro de treinamento √© uma estimativa otimista do erro real do modelo.
> ```python
> import numpy as np
>
> # Dados de exemplo
> y_true = np.array([2, 4, 5, 4, 5, 7, 9, 10, 12, 13])
> y_pred = np.array([2.2, 3.8, 5.3, 4.2, 5.1, 6.8, 9.1, 10.3, 11.8, 13.1])
>
> # Calcula a covari√¢ncia para cada par
> covariances = np.cov(y_true, y_pred)[0, 1]
>
> # Calcula o otimismo
> N = len(y_true)
> optimism = (2/N) * np.sum(covariances)
>
> print(f"Covari√¢ncia: {covariances:.2f}")
> print(f"Otimismo da Taxa de Erro de Treinamento: {optimism:.2f}")
> ```
> Este exemplo demonstra como o otimismo quantifica a diferen√ßa entre o erro observado nos dados de treinamento e o erro real esperado em novos dados.

**Conceito 3: AIC como Estimador do Erro In-Sample**

O AIC surge como uma ferramenta para estimar o erro in-sample, corrigindo o vi√©s da taxa de erro de treinamento. O AIC adiciona uma penalidade que aumenta com a complexidade do modelo, buscando um equil√≠brio entre ajuste e generaliza√ß√£o [^7.5]. Para um modelo com *d* par√¢metros, o AIC √© dado por:

$$ AIC = err + 2\frac{d}{N}\sigma^2 $$

onde err √© o erro de treinamento, *d* √© o n√∫mero de par√¢metros do modelo, N √© o n√∫mero de amostras, e $\sigma^2$ √© uma estimativa da vari√¢ncia do ru√≠do. O AIC fornece uma estimativa do erro de generaliza√ß√£o ao penalizar modelos mais complexos [^7.5].

> ```mermaid
> graph TD
> subgraph "AIC Formula Breakdown"
>     direction TB
>     A["AIC"]
>     B["Training Error: err"]
>     C["Complexity Penalty: 2*(d/N)*œÉ¬≤"]
>     A --> B
>     A --> C
>         subgraph "Complexity Penalty Components"
>             direction LR
>             C --> D["Number of Parameters: d"]
>             C --> E["Number of Samples: N"]
>             C --> F["Noise Variance: œÉ¬≤"]
>
>         end
>
> end
> ```
> üí° **Exemplo Num√©rico:** Considere um modelo com um erro de treinamento `err` de 10, um n√∫mero de par√¢metros `d` igual a 5, um n√∫mero de amostras `N` igual a 100, e uma vari√¢ncia do ru√≠do $\sigma^2$ de 4. O AIC seria calculado como:
> $AIC = 10 + 2 \times \frac{5}{100} \times 4 = 10 + 0.4 = 10.4$. Se outro modelo tiver um erro de treinamento menor, digamos 8, mas com mais par√¢metros, digamos 10, o AIC seria: $AIC = 8 + 2 \times \frac{10}{100} \times 4 = 8 + 0.8 = 8.8$.
>  Um modelo com erro de treinamento 7 e 20 par√¢metros seria: $AIC = 7 + 2 \times \frac{20}{100} \times 4 = 7 + 1.6 = 8.6$. O modelo com AIC mais baixo (8.6) seria preferido. Este exemplo demonstra como o AIC penaliza modelos mais complexos, mesmo que eles tenham um erro de treinamento menor.
>
> | Modelo | Erro Treinamento (err) | Par√¢metros (d) | N | Vari√¢ncia (œÉ¬≤) | AIC |
> |---|---|---|---|---|---|
> | 1 | 10 | 5 | 100 | 4 | 10.4 |
> | 2 | 8 | 10 | 100 | 4 | 8.8 |
> | 3 | 7 | 20 | 100 | 4 | 8.6 |
>
> ```python
> import numpy as np
>
> def calculate_aic(err, d, N, sigma2):
>     """Calcula o AIC."""
>     return err + 2 * (d / N) * sigma2
>
> # Modelo 1
> err1 = 10
> d1 = 5
> N1 = 100
> sigma2_1 = 4
> aic1 = calculate_aic(err1, d1, N1, sigma2_1)
>
> # Modelo 2
> err2 = 8
> d2 = 10
> N2 = 100
> sigma2_2 = 4
> aic2 = calculate_aic(err2, d2, N2, sigma2_2)
>
> # Modelo 3
> err3 = 7
> d3 = 20
> N3 = 100
> sigma2_3 = 4
> aic3 = calculate_aic(err3, d3, N3, sigma2_3)
>
> print(f"AIC do Modelo 1: {aic1:.2f}")
> print(f"AIC do Modelo 2: {aic2:.2f}")
> print(f"AIC do Modelo 3: {aic3:.2f}")
> ```

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

<imagem: Diagrama mostrando como a regress√£o linear de uma matriz de indicadores leva √† classifica√ß√£o, incluindo a codifica√ß√£o de classes, estimativa de coeficientes por m√≠nimos quadrados e a aplica√ß√£o de uma regra de decis√£o.>

A regress√£o linear pode ser aplicada √† classifica√ß√£o usando uma matriz de indicadores. Primeiro, as classes s√£o codificadas usando uma matriz de indicadores. Em seguida, os coeficientes s√£o estimados por m√≠nimos quadrados. Finalmente, uma regra de decis√£o √© utilizada para classificar as amostras com base nas predi√ß√µes do modelo linear [^7.2].
Embora esta abordagem seja direta e simples, ela apresenta algumas limita√ß√µes, como a possibilidade de gerar predi√ß√µes fora do intervalo [0,1] para dados bin√°rios [^7.2]. √â importante ressaltar que para certos problemas de classifica√ß√£o, a regress√£o linear pode levar a resultados satisfat√≥rios, especialmente quando o principal objetivo √© determinar a fronteira de decis√£o linear [^7.2].

**Lemma 2: Erro em Modelos de Regress√£o para Classifica√ß√£o**

Em modelos de classifica√ß√£o via regress√£o linear, o erro m√©dio no treinamento pode ser descrito como:
$$ \frac{1}{N} \sum_{i=1}^N  Err(x_i) = \sigma^2 + \frac{1}{N} \sum_{i=1}^N [f(x_i) - E[f(x_i)]]^2 + \frac{p}{N}\sigma^2 $$
onde $p$ √© o n√∫mero de par√¢metros do modelo e $\sigma^2$ √© a vari√¢ncia do ru√≠do. Este resultado evidencia o trade-off entre complexidade do modelo ($p$) e o erro de treinamento, mostrando que modelos mais complexos (com maior $p$) podem ter um erro de treinamento menor, mas uma generaliza√ß√£o pior [^7.2].
$\blacksquare$
> ```mermaid
> graph TD
> subgraph "Error in Regression for Classification"
> direction TB
>     A["Average Training Error: (1/N)Œ£ Err(x·µ¢)"]
>     B["Irreducible Error: œÉ¬≤"]
>     C["Model Fit Term: (1/N)Œ£ [f(x·µ¢) - E[f(x·µ¢)]]¬≤"]
>     D["Complexity Term: (p/N)œÉ¬≤"]
>     A --> B
>     A --> C
>     A --> D
> end
> ```
> üí° **Exemplo Num√©rico:** Suponha um problema de classifica√ß√£o bin√°ria com 100 amostras e vari√¢ncia do ru√≠do $\sigma^2 = 1$. Um modelo simples com 2 par√¢metros (p=2) tem um erro m√©dio $\frac{1}{N} \sum_{i=1}^N [f(x_i) - E[f(x_i)]]^2 = 0.5$. O erro m√©dio total seria: $\frac{1}{100} \sum_{i=1}^{100} Err(x_i) = 1 + 0.5 + \frac{2}{100} * 1 = 1.52$. Um modelo mais complexo com 10 par√¢metros e erro m√©dio $\frac{1}{N} \sum_{i=1}^N [f(x_i) - E[f(x_i)]]^2 = 0.2$ teria erro m√©dio total de $\frac{1}{100} \sum_{i=1}^{100} Err(x_i) = 1 + 0.2 + \frac{10}{100} * 1 = 1.3$. Embora o modelo mais complexo tenha um erro menor devido ao termo $[f(x_i) - E[f(x_i)]]^2$, a penalidade por complexidade √© maior, ilustrando o trade-off.
> ```mermaid
> graph LR
>     A[Erro M√©dio Total Modelo Simples] --> B(œÉ¬≤: 1);
>     A --> C(Vi√©s¬≤: 0.5);
>     A --> D(Complexidade: 0.02);
>     B --> E[Erro M√©dio Total: 1.52];
>     C --> E;
>     D --> E;
>
>      A1[Erro M√©dio Total Modelo Complexo] --> B1(œÉ¬≤: 1);
>     A1 --> C1(Vi√©s¬≤: 0.2);
>     A1 --> D1(Complexidade: 0.1);
>     B1 --> E1[Erro M√©dio Total: 1.3];
>     C1 --> E1;
>     D1 --> E1;
>     style A fill:#f9f,stroke:#333,stroke-width:2px
>      style A1 fill:#f9f,stroke:#333,stroke-width:2px
> ```

**Corol√°rio 2: Equival√™ncia em Proje√ß√µes de Hiperplanos**

Em algumas condi√ß√µes, a regress√£o linear e a an√°lise discriminante linear (LDA) produzem proje√ß√µes nos hiperplanos de decis√£o que s√£o equivalentes. Esta equival√™ncia, no entanto, n√£o √© garantida em todos os casos, e a LDA pode ser mais robusta em cen√°rios onde os dados apresentam caracter√≠sticas espec√≠ficas [^7.3].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

<imagem: Diagrama de fluxo mostrando a aplica√ß√£o de penalidades L1 e L2 em modelos de regress√£o log√≠stica, incluindo a fun√ß√£o de custo, otimiza√ß√£o e sele√ß√£o de vari√°veis.>
Em contextos de classifica√ß√£o, m√©todos de sele√ß√£o de vari√°veis e regulariza√ß√£o s√£o essenciais para lidar com problemas de alta dimensionalidade e evitar o *overfitting* [^7.5]. A **regulariza√ß√£o** adiciona termos de penaliza√ß√£o √† fun√ß√£o de custo do modelo, controlando a magnitude dos par√¢metros e induzindo *sparsity* [^7.4]. As penalidades L1 e L2 s√£o comumente empregadas, com a penalidade L1 promovendo a sele√ß√£o de vari√°veis e a penalidade L2 promovendo a estabilidade do modelo [^7.5].

Em modelos de regress√£o log√≠stica, a fun√ß√£o de custo √© baseada na verossimilhan√ßa logar√≠tmica. A regulariza√ß√£o adiciona um termo penalizador √† fun√ß√£o de custo, resultando em um problema de otimiza√ß√£o que busca equilibrar o ajuste aos dados e a complexidade do modelo [^7.4]. A regulariza√ß√£o L1 (Lasso) pode induzir *sparsity* nos coeficientes, enquanto a regulariza√ß√£o L2 (Ridge) reduz a magnitude dos coeficientes [^7.4].

**Lemma 3: Regulariza√ß√£o L1 e Sparsity**

A penaliza√ß√£o L1 em modelos de classifica√ß√£o log√≠stica leva a coeficientes esparsos, ou seja, muitos coeficientes s√£o levados a zero, resultando na sele√ß√£o de um subconjunto de vari√°veis relevantes. Esta propriedade √© decorrente do formato da penaliza√ß√£o L1 que for√ßa a solu√ß√£o a se concentrar nos coeficientes mais importantes [^7.4.4].

**Prova do Lemma 3:**
Considere a fun√ß√£o de custo de regress√£o log√≠stica com penaliza√ß√£o L1:
$$ L(\beta) = -\sum_{i=1}^N [y_i \log(\sigma(x_i^T\beta)) + (1-y_i) \log(1-\sigma(x_i^T\beta))] + \lambda \sum_{j=1}^p |\beta_j| $$
Onde $\sigma$ √© a fun√ß√£o sigm√≥ide e $\lambda$ √© um par√¢metro de regulariza√ß√£o. A penalidade L1 √© n√£o-diferenci√°vel em zero, e durante o processo de otimiza√ß√£o, ela for√ßa muitos $\beta_j$ a se tornarem exatamente zero, promovendo a *sparsity*. A prova formal envolve a an√°lise das condi√ß√µes de otimalidade e as propriedades da norma L1 [^7.4.4]. $\blacksquare$

> ```mermaid
> graph TD
> subgraph "L1 Regularized Logistic Regression"
>     direction TB
>     A["Cost Function: L(Œ≤)"]
>     B["Log-Likelihood Term: -Œ£ [y·µ¢log(œÉ(x·µ¢·µÄŒ≤)) + (1-y·µ¢)log(1-œÉ(x·µ¢·µÄŒ≤))]"]
>     C["L1 Penalty Term: ŒªŒ£|Œ≤‚±º|"]
>     A --> B
>     A --> C
> end
> ```
> üí° **Exemplo Num√©rico:** Em um problema de classifica√ß√£o com 10 vari√°veis, ao usar regulariza√ß√£o L1 (Lasso), o modelo pode definir alguns coeficientes $\beta_j$ como exatamente zero. Por exemplo, ap√≥s o treinamento, os coeficientes podem ser: $\beta = [0.5, 0, 0, 1.2, 0, -0.8, 0, 0.3, 0, 0]$, indicando que apenas as vari√°veis 1, 4, 6 e 8 s√£o relevantes para a classifica√ß√£o. Isso demonstra o efeito de *sparsity* da penaliza√ß√£o L1.
> ```python
> import numpy as np
> from sklearn.linear_model import LogisticRegression
> from sklearn.preprocessing import StandardScaler
> from sklearn.pipeline import Pipeline
> from sklearn.datasets import make_classification
>
> # Gerar dados sint√©ticos
> X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_redundant=0, random_state=42)
>
> # Cria um pipeline com normaliza√ß√£o e regress√£o log√≠stica com regulariza√ß√£o L1
> pipeline = Pipeline([
>     ('scaler', StandardScaler()),
>     ('model', LogisticRegression(penalty='l1', solver='liblinear', C=0.1, random_state=42))
> ])
>
> # Treina o modelo
> pipeline.fit(X, y)
>
> # Obt√©m os coeficientes do modelo
> coefficients = pipeline.named_steps['model'].coef_[0]
>
> print("Coeficientes do Modelo com Regulariza√ß√£o L1:")
> for i, coef in enumerate(coefficients):
>     print(f"Coeficiente {i+1}: {coef:.3f}")
>
> ```
> Este exemplo mostra como a regulariza√ß√£o L1 zera alguns coeficientes, simplificando o modelo e selecionando as vari√°veis mais relevantes.

**Corol√°rio 3: Interpretabilidade e Regulariza√ß√£o L1**
A *sparsity* induzida pela regulariza√ß√£o L1 melhora a interpretabilidade do modelo, pois reduz o n√∫mero de vari√°veis a serem consideradas. Isso √© particularmente √∫til em problemas de alta dimensionalidade, onde modelos com um grande n√∫mero de vari√°veis podem ser dif√≠ceis de entender e interpretar [^7.4.5].

> ‚ö†Ô∏è **Ponto Crucial**: A combina√ß√£o de penalidades L1 e L2, conhecida como Elastic Net, pode ser utilizada para aproveitar as vantagens de ambas as formas de regulariza√ß√£o, combinando a sele√ß√£o de vari√°veis (L1) com a estabilidade do modelo (L2) [^7.5].

### Separating Hyperplanes e Perceptrons

<imagem: Diagrama ilustrando hiperplanos separadores e o algoritmo do Perceptron, incluindo a margem de separa√ß√£o, o hiperplano √≥timo e o processo iterativo de ajuste dos par√¢metros.>

A ideia de **hiperplanos separadores** √© fundamental na classifica√ß√£o linear. Um hiperplano √≥timo √© aquele que maximiza a margem de separa√ß√£o entre as classes, isto √©, a dist√¢ncia entre o hiperplano e os pontos mais pr√≥ximos de cada classe [^7.5.2]. O problema de encontrar o hiperplano √≥timo pode ser formulado como um problema de otimiza√ß√£o, que pode ser resolvido usando o dual de Wolfe.

O Perceptron de Rosenblatt √© um algoritmo de aprendizado para classifica√ß√£o linear. Ele ajusta iterativamente os pesos do modelo at√© que os pontos sejam corretamente classificados [^7.5.1]. O Perceptron converge sob a condi√ß√£o de que os dados sejam linearmente separ√°veis.

### Pergunta Te√≥rica Avan√ßada: Como AIC se relaciona com a complexidade de um modelo e quando o BIC pode ser prefer√≠vel?

**Resposta:**
O AIC estima o erro de generaliza√ß√£o adicionando uma penalidade √† taxa de erro de treinamento, proporcional ao n√∫mero de par√¢metros do modelo. Essa penalidade visa evitar o *overfitting* e favorecer modelos com boa performance em dados n√£o vistos [^7.5]. O AIC se baseia em uma aproxima√ß√£o da dist√¢ncia de Kullback-Leibler entre o modelo verdadeiro e o modelo ajustado. Em contextos onde a complexidade do modelo √© mal definida, o AIC pode ser adaptado atrav√©s do uso do n√∫mero efetivo de par√¢metros, que leva em conta a estrutura do modelo.

O **Bayesian Information Criterion (BIC)**, por outro lado, √© derivado de uma perspectiva Bayesiana e penaliza modelos complexos de forma mais rigorosa que o AIC, favorecendo modelos mais simples [^7.7]. O BIC √© assintoticamente consistente, o que significa que, com um n√∫mero infinito de amostras, ele selecionar√° o modelo correto com probabilidade 1. Em contraste, o AIC pode escolher modelos muito complexos. Portanto, o BIC pode ser prefer√≠vel em cen√°rios onde se busca um modelo mais parcimonioso e com um bom poder preditivo.  A escolha entre AIC e BIC depende, em grande parte, do tamanho da amostra. Para amostras pequenas, o AIC pode ser prefer√≠vel, j√° que o BIC tende a subestimar a complexidade do modelo. Para amostras grandes, o BIC pode ser mais adequado devido √† sua consist√™ncia assint√≥tica [^7.7].

**Lemma 4: Formula√ß√£o do BIC**
O BIC pode ser expresso como:
$$ BIC = -2\log(L) + (\log(N))d $$
Onde $L$ √© a verossimilhan√ßa maximizada do modelo, $N$ √© o n√∫mero de amostras, e $d$ √© o n√∫mero de par√¢metros do modelo. Esta formula√ß√£o penaliza modelos complexos, com a penalidade sendo proporcional a $\log(N)$, que √© maior que a penalidade de AIC.  [^7.7].
$\blacksquare$

> ```mermaid
> graph TD
> subgraph "BIC Formula Breakdown"
>     direction TB
>     A["BIC"]
>     B["Negative Log-Likelihood: -2log(L)"]
>     C["Complexity Penalty: log(N)*d"]
>     A --> B
>     A --> C
>         subgraph "Complexity Penalty Components"
>         direction LR
>            C --> D["Number of Samples: N"]
>            C --> E["Number of Parameters: d"]
>         end
> end
> ```
> üí° **Exemplo Num√©rico:** Suponha que tenhamos dois modelos. O Modelo A tem uma verossimilhan√ßa m√°xima de $L_A = 100$, com 5 par√¢metros, enquanto o Modelo B tem uma verossimilhan√ßa m√°xima de $L_B = 120$, com 10 par√¢metros. O n√∫mero de amostras $N$ √© 50. Usando a f√≥rmula do BIC:
>
> $\text{BIC}_A = -2 \log(100) + (\log(50)) \times 5 \approx -2 \times 4.605 + 3.912 \times 5 \approx -9.21 + 19.56 = 10.35$
>
> $\text{BIC}_B = -2 \log(120) + (\log(50)) \times 10 \approx -2 \times 4.787 + 3.912 \times 10 \approx -9.57 + 39.12 = 29.55$
>
> Apesar do modelo B ter uma maior verossimilhan√ßa, seu BIC √© muito maior, indicando que o modelo A √© prefer√≠vel devido √† sua menor complexidade e ao tamanho da amostra.
>
> ```python
> import numpy as np
>
> def calculate_bic(likelihood, num_params, num_samples):
>     """Calcula o BIC."""
>     return -2 * np.log(likelihood) + np.log(num_samples) * num_params
>
> # Modelo A
> likelihood_A = 100
> num_params_A = 5
> num_samples = 50
> bic_A = calculate_bic(likelihood_A, num_params_A, num_samples)
>
> # Modelo B
> likelihood_B = 120
> num_params_B = 10
> num_samples = 50
> bic_B = calculate_bic(likelihood_B, num_params_B, num_samples)
>
> print(f"BIC do Modelo A: {bic_A:.2f}")
> print(f"BIC do Modelo B: {bic_B:.2f}")
> ```
> Este exemplo demonstra como o BIC penaliza modelos complexos mais fortemente que o AIC, favorecendo modelos mais simples quando o tamanho da amostra √© consider√°vel.

**Corol√°rio 4: AIC e BIC sob o modelo Gaussiano**
Sob o modelo Gaussiano, assumindo a vari√¢ncia $\sigma^2$ conhecida, o AIC e BIC podem ser expressos como:
$$ AIC = \frac{N}{\sigma^2} err + 2d $$
$$ BIC = \frac{N}{\sigma^2} err + (\log(N))d $$
Esta formula√ß√£o evidencia o trade-off entre ajuste (via o erro err) e a complexidade (via o n√∫mero de par√¢metros $d$), bem como a penalidade maior imposta pelo BIC em compara√ß√£o ao AIC  [^7.7].
$\blacksquare$
> ```mermaid
> graph TD
> subgraph "AIC and BIC under Gaussian Model"
>     direction TB
>     A["AIC Gaussian"]
>     B["BIC Gaussian"]
>     C["Weighted Training Error: (N/œÉ¬≤)err"]
>     D["AIC Complexity Penalty: 2d"]
>     E["BIC Complexity Penalty: log(N)d"]
>     A --> C
>     A --> D
>     B --> C
>     B --> E
> end
> ```
> üí° **Exemplo Num√©rico:** Considere um cen√°rio com 100 amostras ($N = 100$) e uma vari√¢ncia conhecida $\sigma^2=1$. Um modelo A tem um erro de treinamento (err) de 5 e 3 par√¢metros ($d=3$). Um modelo B tem um erro de treinamento de 4 e 8 par√¢metros ($d=8$).
> Usando as formulas de AIC e BIC:
>
> $\text{AIC}_A = \frac{100}{1} \times 5 + 2 \times 3 = 500 + 6 = 506$
> $\text{BIC}_A = \frac{100}{1} \times 5 + (\log(100)) \times 3 = 500 + 4.605 \times 3 = 500 + 13.815 = 513.815$
>
> $\text{AIC}_B = \frac{100}{1} \times 4 + 2 \times 8 = 400 + 16 = 416$
> $\text{BIC}_B = \frac{100}{1} \times 4 + (\log(100)) \times 8 = 400 + 4.605 \times 8 = 400 + 36.84 = 436.84$
>
> Nesse cen√°rio, o AIC favorece o modelo B, enquanto o BIC tamb√©m favorece o modelo B, mas com uma diferen√ßa menor que o AIC. Isso ilustra como a penalidade imposta pelo BIC √© mais forte que a penalidade do AIC.

>
> | Model | err | d | AIC | BIC |
> |---|---|---|---|---|
> | A | 5 | 3 | 506 | 513.815 |
> | B | 4 | 8 | 416 | 436.84 |
>
> ```python
> import numpy as np
>
> def calculate_aic_gauss(N, err, d, sigma2):
>    """Calcula o AIC sob o modelo gaussiano."""
>    return (N / sigma2) * err + 2 * d
>
> def calculate_bic_gauss(N, err, d, sigma2):
>   """Calcula o BIC sob o modelo gaussiano."""
>   return (N / sigma2) * err + np.log(N) * d
>
> # Modelo A
> N = 100
> err_A = 5
> d_A = 3
> sigma2 = 1
> aic_A = calculate_aic_gauss(N, err_A, d_A, sigma2)
> bic_A = calculate_bic_gauss(N, err_A, d_A, sigma2)
>
> # Modelo B
> err_B = 4
> d_B = 8
> aic_B = calculate_aic_gauss(N, err_B, d_B, sigma2)
> bic_B = calculate_bic_gauss(N, err_B, d_B, sigma2)
>
> print(f"AIC do Modelo A: {aic_A:.2f}")
> print(f"BIC do Modelo A: {bic_A:.2f}")
> print(f"AIC do Modelo B: {aic_B:.2f}")
> print(f"BIC do Modelo B: {bic_B:.2f}")
> ```

> ‚ö†Ô∏è **Ponto Crucial**: A escolha entre AIC e BIC depende do problema em quest√£o e das prioridades de modelagem. O AIC √© mais apropriado quando a precis√£o preditiva √© crucial, enquanto o BIC √© mais indicado quando a parcim√¥nia e a interpretabilidade s√£o desejadas [^7.7].

### Conclus√£o
O AIC oferece uma abordagem robusta e quantitativa para a sele√ß√£o de modelos. Ele fornece uma estimativa do erro de generaliza√ß√£o, penalizando modelos complexos, o que evita o *overfitting* e favorece modelos mais simples, mas igualmente precisos. O AIC se baseia em uma aproxima√ß√£o da dist√¢ncia de Kullback-Leibler entre o modelo verdadeiro e o modelo ajustado. O AIC √© muito utilizado tanto em modelos de regress√£o linear, mas principalmente em modelos de regress√£o n√£o linear, devido √†