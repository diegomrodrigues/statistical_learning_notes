## Model Selection and the Bias-Variance Tradeoff

<imagem: Diagrama complexo mostrando a rela√ß√£o entre Bias, Variance e Complexidade do Modelo, com curvas representando diferentes m√©todos de sele√ß√£o de modelo>

### Introdu√ß√£o

A sele√ß√£o de modelos √© um passo crucial na constru√ß√£o de modelos de aprendizado estat√≠stico. O desempenho de um modelo em dados de teste independentes, conhecido como **generaliza√ß√£o**, √© o que realmente importa em aplica√ß√µes pr√°ticas [^7.1]. A escolha do m√©todo de aprendizado ou modelo, e a avalia√ß√£o de sua qualidade s√£o etapas que devem ser conduzidas de forma cuidadosa [^7.1]. Este cap√≠tulo explora m√©todos para avalia√ß√£o de desempenho e sele√ß√£o de modelos, come√ßando com a discuss√£o sobre a intera√ß√£o entre **vi√©s**, **vari√¢ncia** e **complexidade do modelo** [^7.1].

```mermaid
graph TB
    subgraph "Model Selection Process"
        direction TB
        A["Define Problem and Data"] --> B["Choose Model Class"]
        B --> C["Estimate Model Parameters"]
        C --> D["Evaluate Performance (Generalization)"]
        D --> E["Model Selection (Bias-Variance Tradeoff)"]
    end
```

### Conceitos Fundamentais

**Conceito 1:** O **problema de classifica√ß√£o**, em sua ess√™ncia, busca encontrar uma fun√ß√£o $f(X)$ que prediz corretamente a vari√°vel resposta $Y$, a partir de um vetor de *inputs* $X$. No contexto de modelos lineares, a complexidade e o n√∫mero de par√¢metros do modelo s√£o fatores que influenciam diretamente no *trade-off* entre vi√©s e vari√¢ncia [^7.2]. Modelos mais simples, com menos par√¢metros, tendem a ter alto vi√©s, ou seja, dificuldade em capturar a verdadeira rela√ß√£o entre $X$ e $Y$, mas baixa vari√¢ncia, pois s√£o menos sens√≠veis a varia√ß√µes nos dados de treinamento. Por outro lado, modelos mais complexos com muitos par√¢metros, t√™m menor vi√©s e maior vari√¢ncia, sendo mais flex√≠veis para se ajustarem a padr√µes espec√≠ficos nos dados de treinamento, mas mais suscet√≠veis a ru√≠dos e menos robustos em dados n√£o observados [^7.2].

> üí° **Exemplo Num√©rico:** Imagine que queremos modelar a rela√ß√£o entre a altura de uma pessoa ($X$) e seu peso ($Y$).
> *   **Modelo Simples (Alto Vi√©s, Baixa Vari√¢ncia):** Um modelo linear simples como $Y = \beta_0 + \beta_1 X$, onde $Y$ √© o peso e $X$ a altura, pode n√£o capturar nuances como diferen√ßas de composi√ß√£o corporal (massa muscular vs. gordura). Este modelo tem alto vi√©s (subestima ou superestima sistematicamente o peso para muitas pessoas), mas baixa vari√¢ncia (as predi√ß√µes n√£o mudam muito com dados diferentes).
> *   **Modelo Complexo (Baixo Vi√©s, Alta Vari√¢ncia):** Um modelo polinomial de alta ordem, como $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3$, pode se ajustar muito bem aos dados de treinamento. No entanto, ele pode se ajustar muito bem a um conjunto espec√≠fico de dados e falhar em generalizar para novos dados (alta vari√¢ncia). Pequenas mudan√ßas nos dados de treinamento podem levar a grandes mudan√ßas na curva preditiva.
>
> O *trade-off* aqui √© que um modelo simples pode n√£o se adequar bem aos dados (alto vi√©s), enquanto um modelo complexo pode se ajustar excessivamente aos dados de treinamento e ser ruim para novos dados (alta vari√¢ncia). O objetivo √© encontrar o modelo com a complexidade ideal que equilibre esses dois aspectos.

**Lemma 1:** Em um modelo linear, o vi√©s pode ser definido como a diferen√ßa entre o valor m√©dio predito pelo modelo e o verdadeiro valor da vari√°vel resposta, enquanto a vari√¢ncia √© a varia√ß√£o da predi√ß√£o em rela√ß√£o √† sua m√©dia. Formalmente, seja $E[f(X)]$ a predi√ß√£o m√©dia do modelo para um determinado $X$ e $f(X)$ o verdadeiro valor da vari√°vel resposta, o vi√©s pode ser expresso como $Bias(f(X)) =  E[f(X)] - f(X)$. A vari√¢ncia, por sua vez, √© $Var(f(X)) = E[(f(X) - E[f(X)])^2]$. A *decomposi√ß√£o do erro em vi√©s e vari√¢ncia* revela que o erro total da predi√ß√£o √© a soma do quadrado do vi√©s, a vari√¢ncia e um erro irredut√≠vel [^7.3].

$$
Err(X) = \sigma^2 + Bias^2(f(X)) + Var(f(X))
$$

onde $\sigma^2$ √© a vari√¢ncia do erro irredut√≠vel [^7.3]. $\blacksquare$

```mermaid
graph TB
    subgraph "Error Decomposition"
        direction TB
        A["Total Error: Err(X)"]
        B["Bias¬≤ Component: Bias¬≤(f(X))"]
        C["Variance Component: Var(f(X))"]
        D["Irreducible Error: œÉ¬≤"]
        A --> B
        A --> C
        A --> D
        style A fill:#f9f,stroke:#333,stroke-width:2px
        style B fill:#ccf,stroke:#333,stroke-width:1px
        style C fill:#ccf,stroke:#333,stroke-width:1px
        style D fill:#ccf,stroke:#333,stroke-width:1px
    end
```

> üí° **Exemplo Num√©rico:** Vamos supor que o verdadeiro modelo para uma vari√°vel $Y$ √© $f(X) = 2X + 1$, e temos um conjunto de dados onde $X$ e $Y$ s√£o gerados com algum ru√≠do aleat√≥rio. Consideremos dois modelos:
>
> 1.  **Modelo 1 (Simples):** $f_1(X) = 1.5X + 0.5$
> 2.  **Modelo 2 (Complexo):** $f_2(X) = 2X + 1 + \epsilon$, onde $\epsilon$ representa um erro aleat√≥rio.
>
>   Suponha que, para um ponto espec√≠fico $X=2$, o verdadeiro valor de $Y$ seja $f(2) = 2*2 + 1 = 5$.
>
>   *   **C√°lculo do Vi√©s (Bias):**
>       *   Para o Modelo 1: $Bias(f_1(2)) = E[f_1(2)] - f(2) = (1.5*2 + 0.5) - 5 = 3.5 - 5 = -1.5$. O vi√©s √© -1.5, indicando que o modelo sistematicamente subestima o valor real.
>       *   Para o Modelo 2: $Bias(f_2(2)) = E[f_2(2)] - f(2) = (2*2 + 1 + E[\epsilon]) - 5 = (5 + 0) - 5 = 0$, pois assumimos que o erro tem m√©dia zero. O vi√©s √© 0, indicando que o modelo n√£o tem erro sistem√°tico.
>
>   *   **C√°lculo da Vari√¢ncia (Variance):**
>       *   Para o Modelo 1: $Var(f_1(2)) = E[(f_1(2) - E[f_1(2)])^2] = E[(3.5 - 3.5)^2] = 0$. A vari√¢ncia √© 0, indicando que a predi√ß√£o √© sempre a mesma para diferentes amostras de treinamento.
>       *   Para o Modelo 2: $Var(f_2(2)) = E[(f_2(2) - E[f_2(2)])^2] = E[(5 + \epsilon - 5)^2] = E[\epsilon^2] = \sigma^2_{\epsilon}$. A vari√¢ncia √© igual √† vari√¢ncia do erro, indicando que a predi√ß√£o varia com a varia√ß√£o do erro.
>
>   *   **Erro Total:**
>       *   $Err(2) = \sigma^2 + Bias^2(f_1(2)) + Var(f_1(2)) = \sigma^2 + (-1.5)^2 + 0 = \sigma^2 + 2.25$
>       *   $Err(2) = \sigma^2 + Bias^2(f_2(2)) + Var(f_2(2)) = \sigma^2 + 0^2 + \sigma^2_{\epsilon} = \sigma^2 + \sigma^2_{\epsilon}$.
>
>   O Modelo 1 tem um vi√©s maior mas vari√¢ncia zero, enquanto o Modelo 2 tem vi√©s zero e vari√¢ncia igual √† vari√¢ncia do ru√≠do. O erro total considera ambos os aspectos junto com o erro irredut√≠vel ($\sigma^2$).

**Conceito 2:** A **Linear Discriminant Analysis (LDA)** √© um m√©todo de classifica√ß√£o que assume que as classes s√£o geradas por distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia [^7.3]. A fronteira de decis√£o entre duas classes √© linear e obtida atrav√©s da proje√ß√£o dos dados em um subespa√ßo que maximiza a separa√ß√£o entre as m√©dias das classes e minimiza a vari√¢ncia dentro de cada classe [^7.3.1]. As suposi√ß√µes de normalidade e covari√¢ncias iguais s√£o cruciais para a formula√ß√£o do LDA, o que pode levar a erros em cen√°rios onde essas suposi√ß√µes n√£o s√£o v√°lidas [^7.3.2], [^7.3.3]. O LDA busca maximizar a raz√£o entre a vari√¢ncia entre as classes e a vari√¢ncia dentro das classes, definindo um discriminante linear que separa as classes de forma √≥tima sob as premissas estabelecidas.

**Corol√°rio 1:** O discriminante linear obtido pelo LDA resulta em proje√ß√µes dos dados que maximizam a raz√£o entre a vari√¢ncia entre classes e a vari√¢ncia dentro das classes. Formalmente, seja $S_W$ a matriz de covari√¢ncia dentro da classe e $S_B$ a matriz de covari√¢ncia entre as classes, o discriminante linear $w$ √© obtido maximizando a fun√ß√£o $w^T S_B w / w^T S_W w$. Esse corol√°rio demonstra que o LDA encontra uma proje√ß√£o que otimiza a separa√ß√£o linear entre as classes, sob as condi√ß√µes de gaussianidade e covari√¢ncia igual [^7.3.1].

```mermaid
graph LR
    subgraph "LDA Optimization"
        direction LR
        A["Within-class covariance matrix: S_W"]
        B["Between-class covariance matrix: S_B"]
        C["Discriminant vector: w"]
        D["Maximize criterion: w^T S_B w / w^T S_W w"]
        A & B --> D
        C --> D
    end
```

> üí° **Exemplo Num√©rico:** Considere um problema de classifica√ß√£o bin√°ria com duas classes (A e B) e duas caracter√≠sticas ($X_1$ e $X_2$). Suponha que as m√©dias e as matrizes de covari√¢ncia das classes s√£o:
>
> *   Classe A: $\mu_A = [1, 1]^T$, $\Sigma_A = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$
> *   Classe B: $\mu_B = [3, 3]^T$, $\Sigma_B = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$
>
> A matriz de covari√¢ncia pooled (combinada) √© $S_W = \Sigma_A = \Sigma_B$ (j√° que s√£o iguais). A matriz de covari√¢ncia entre as classes √© $S_B = (\mu_A - \mu_B)(\mu_A - \mu_B)^T = \begin{bmatrix} 2 & 2 \\ 2 & 2 \end{bmatrix}$.
>
> O objetivo do LDA √© encontrar o discriminante linear $w$ que maximiza a raz√£o $w^T S_B w / w^T S_W w$. A solu√ß√£o para esse problema √© dada pelo autovetor da matriz $S_W^{-1}S_B$ correspondente ao maior autovalor.
>
> 1.  **Calculando $S_W^{-1}$:** A inversa de $S_W$ (ou $\Sigma_A$) √© $S_W^{-1} = \begin{bmatrix} 1.333 & -0.667 \\ -0.667 & 1.333 \end{bmatrix}$
>
> 2.  **Calculando $S_W^{-1}S_B$:**
>    $S_W^{-1}S_B = \begin{bmatrix} 1.333 & -0.667 \\ -0.667 & 1.333 \end{bmatrix} \begin{bmatrix} 2 & 2 \\ 2 & 2 \end{bmatrix} = \begin{bmatrix} 1.333 & 1.333 \\ 1.333 & 1.333 \end{bmatrix}$
>
> 3.  **Autovetor:** O autovetor correspondente ao maior autovalor da matriz resultante √© $w = [1, 1]^T$ (normalizado). Esse vetor √© o discriminante linear que maximiza a separa√ß√£o entre as classes.
>
> A fronteira de decis√£o √© dada pela reta perpendicular a $w$ e que passa pelo ponto m√©dio das m√©dias das classes, $\frac{\mu_A + \mu_B}{2} = [2, 2]^T$. Em termos pr√°ticos, dados novos pontos, eles ser√£o classificados de acordo com a sua posi√ß√£o em rela√ß√£o a essa fronteira.

**Conceito 3:** A **Logistic Regression** √© um modelo probabil√≠stico para classifica√ß√£o que estima a probabilidade de um evento ocorrer atrav√©s da fun√ß√£o *logit* [^7.4]. Ao contr√°rio do LDA, a regress√£o log√≠stica n√£o faz suposi√ß√µes sobre a distribui√ß√£o dos dados de entrada, e a fronteira de decis√£o resultante √© dada pelo limite onde a probabilidade estimada √© igual a 0.5 [^7.4.1]. A otimiza√ß√£o dos par√¢metros na regress√£o log√≠stica √© realizada atrav√©s da maximiza√ß√£o da verossimilhan√ßa, e o modelo resultante pode ser interpretado como a probabilidade log-odds sendo modelada linearmente em rela√ß√£o aos *inputs* [^7.4.2], [^7.4.3]. Embora a regress√£o log√≠stica tamb√©m resulte em fronteiras de decis√£o lineares, sua formula√ß√£o e os par√¢metros resultantes s√£o diferentes daqueles obtidos via LDA [^7.4.4], [^7.4.5].

```mermaid
graph LR
    subgraph "Logistic Regression Model"
    direction LR
        A["Input Features: X"]
        B["Linear Combination: Œ≤_0 + Œ≤_1 X"]
        C["Logit Function: 1 / (1 + e^(-(Œ≤_0 + Œ≤_1 X)))"]
        D["Probability: P(Y=1|X)"]
        A --> B
        B --> C
        C --> D
    end
```

> üí° **Exemplo Num√©rico:** Vamos considerar um problema de classifica√ß√£o bin√°ria com uma √∫nica vari√°vel preditora $X$. A regress√£o log√≠stica modela a probabilidade de um evento ($Y=1$) como:
>
>   $$ P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}} $$
>
>   Suponha que, ap√≥s o treinamento do modelo, obtivemos os seguintes par√¢metros: $\beta_0 = -3$ e $\beta_1 = 1.5$.
>
>   *   **C√°lculo da Probabilidade:** Se temos um novo dado com $X=2$, a probabilidade de $Y=1$ √©:
>       $$ P(Y=1|X=2) = \frac{1}{1 + e^{-(-3 + 1.5 * 2)}} = \frac{1}{1 + e^{0}} = \frac{1}{2} = 0.5 $$
>
>   *   **Fronteira de Decis√£o:** A fronteira de decis√£o ocorre quando a probabilidade √© 0.5. Isso acontece quando o argumento da exponencial √© zero:
>       $$ \beta_0 + \beta_1 X = 0 \Rightarrow -3 + 1.5 X = 0 \Rightarrow X = 2 $$
>
>       Portanto, a fronteira de decis√£o para este modelo √© $X = 2$. Se $X > 2$, a probabilidade de $Y=1$ √© maior que 0.5, e se $X < 2$, a probabilidade √© menor que 0.5.
>   *   **Interpreta√ß√£o:** O par√¢metro $\beta_1=1.5$ indica que para cada unidade de aumento em $X$, as *log-odds* (logaritmo da raz√£o de chances) de $Y=1$ aumentam em 1.5. O par√¢metro $\beta_0 = -3$ √© o valor das *log-odds* quando $X=0$.

> ‚ö†Ô∏è **Nota Importante**: A escolha entre LDA e Regress√£o Log√≠stica depende das suposi√ß√µes sobre os dados e do objetivo da an√°lise. Em alguns casos, o LDA pode ser mais eficiente, enquanto em outros a Regress√£o Log√≠stica pode ser mais robusta [^7.4.1].
> ‚ùó **Ponto de Aten√ß√£o**: Em situa√ß√µes com classes n√£o balanceadas, a Regress√£o Log√≠stica pode requerer ajustes para lidar com o desbalanceamento e evitar que a classe majorit√°ria domine a decis√£o do modelo [^7.4.2].
> ‚úîÔ∏è **Destaque**: As estimativas de par√¢metros em LDA e em regress√£o log√≠stica est√£o relacionadas, e em algumas situa√ß√µes espec√≠ficas, podem produzir resultados similares, especialmente quando as suposi√ß√µes do LDA s√£o v√°lidas [^7.5].

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
flowchart TD
  subgraph Regress√£o de Indicadores
    A["Encode Classes as Indicators"] --> B["Estimate Coefficients via LS"]
    B --> C["Apply Decision Rule"]
    C --> D["Compare with Probabilistic Methods"]
  end
```
**Explica√ß√£o:** Este diagrama representa o fluxo do processo de regress√£o de indicadores e como ele se relaciona √† classifica√ß√£o, **conforme descrito nos t√≥picos [1](4.2)**.

A **regress√£o linear aplicada a uma matriz de indicadores** pode ser utilizada para classifica√ß√£o, onde cada classe √© representada por uma coluna na matriz de *inputs*, com valores 0 ou 1. Ao usar m√≠nimos quadrados para estimar os coeficientes, o modelo busca minimizar o erro entre a previs√£o e o valor real dos indicadores de classe [^4.2]. No entanto, a regress√£o linear n√£o √© um m√©todo apropriado para estimar probabilidades, uma vez que as previs√µes podem extrapolar os limites [0, 1]. A regress√£o linear, no contexto de classifica√ß√£o, busca obter uma fronteira linear de separa√ß√£o entre as classes, mas n√£o fornece diretamente uma interpreta√ß√£o probabil√≠stica das predi√ß√µes [^4.1]. Embora a regress√£o linear seja simples de implementar, a aplica√ß√£o direta para classifica√ß√£o pode levar a problemas como o *masking problem*, onde a presen√ßa de outras classes pode influenciar o resultado para uma classe espec√≠fica [^4.3].

> üí° **Exemplo Num√©rico:** Considere um problema de classifica√ß√£o com 3 classes (A, B, e C) e duas caracter√≠sticas $X_1$ e $X_2$. Vamos usar um conjunto de dados simulado:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Dados simulados
> X = np.array([[1, 2], [2, 3], [3, 4], [1, 4], [2, 5], [3, 6], [4, 1], [5, 2], [6, 3], [4, 3], [5, 4], [6, 5]])
> y = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 1, 2]) # Classes A=0, B=1, C=2
>
> # Matriz de indicadores
> y_encoded = np.eye(3)[y]
>
> # Regress√£o Linear
> model = LinearRegression()
> model.fit(X, y_encoded)
>
> # Previs√µes para novos pontos
> new_points = np.array([[2.5, 3.5], [4.5, 2.5]])
> predictions = model.predict(new_points)
>
> print("Coeficientes:", model.coef_)
> print("Intercepto:", model.intercept_)
> print("Previs√µes:", predictions)
>
> # Atribui√ß√£o de classes
> predicted_classes = np.argmax(predictions, axis=1)
> print("Classes Preditas:", predicted_classes)
>
> ```
>
> **Sa√≠da (exemplo):**
> ```
> Coeficientes: [[ 0.066  0.066]
>  [-0.066 -0.066]
>  [ 0.    0.   ]]
> Intercepto: [ 0.533  0.333  0.133]
> Previs√µes: [[0.333 0.333 0.333]
>  [ 0.466 0.266 0.266]]
> Classes Preditas: [0 0]
> ```
> *   **Interpreta√ß√£o:**
>     *   A matriz `y_encoded` transforma as classes em vetores one-hot encoding.
>     *   A regress√£o linear ajusta os coeficientes para cada classe.
>     *   As `previs√µes` para os novos pontos fornecem uma pontua√ß√£o para cada classe, onde o `argmax` determina a classe predita.
>     *   Note que os valores previstos n√£o s√£o probabilidades, eles podem ser menores que 0 ou maiores que 1.

**Lemma 2:** Em um problema de classifica√ß√£o bin√°ria, a regress√£o linear em uma matriz de indicadores, sob certas condi√ß√µes de ortogonalidade e separabilidade, pode ser equivalente a projetar os dados sobre um hiperplano de decis√£o linear, similar ao que √© feito pelo LDA, com a restri√ß√£o de que a matriz de covari√¢ncia entre classes seja igual [^4.2]. Este *lemma* demonstra que a regress√£o linear pode, em cen√°rios espec√≠ficos, alcan√ßar resultados compar√°veis aos de discriminantes lineares.

**Corol√°rio 2:** A aplica√ß√£o de regress√£o linear a uma matriz de indicadores, sob as premissas do *lemma* anterior, leva √† forma√ß√£o de hiperplanos de decis√£o que, sob as premissas estabelecidas, maximizam a separa√ß√£o das classes [^4.3]. Este corol√°rio ressalta a equival√™ncia em cen√°rios espec√≠ficos entre regress√£o linear e discriminantes lineares em termos de gera√ß√£o de fronteiras de decis√£o.

> "Em alguns cen√°rios, conforme apontado em [^4.4], a regress√£o log√≠stica pode fornecer estimativas mais est√°veis de probabilidade, enquanto a regress√£o de indicadores pode levar a extrapola√ß√µes fora de [0,1]. No entanto, h√° situa√ß√µes em que a regress√£o de indicadores, de acordo com [^4.2], √© suficiente e at√© mesmo vantajosa quando o objetivo principal √© a fronteira de decis√£o linear.‚Äù

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

A **sele√ß√£o de vari√°veis** e a **regulariza√ß√£o** s√£o t√©cnicas importantes para controlar a complexidade e melhorar a generaliza√ß√£o de modelos de classifica√ß√£o. Ao reduzir o n√∫mero de vari√°veis usadas ou ao regularizar os par√¢metros do modelo, podemos evitar o *overfitting* e obter modelos mais robustos e interpret√°veis [^4.4.4]. A regulariza√ß√£o L1 e L2 s√£o amplamente utilizadas em modelos log√≠sticos para controlar a magnitude dos par√¢metros, sendo que a penalidade L1 promove a esparsidade dos coeficientes, enquanto a penalidade L2 induz a estimativas menores e mais est√°veis [^4.5]. A regulariza√ß√£o se encaixa na formula√ß√£o da fun√ß√£o de custo atrav√©s da adi√ß√£o de termos de penaliza√ß√£o, que s√£o ponderados por um fator de regulariza√ß√£o. Essa fun√ß√£o de custo combina a verossimilhan√ßa com esses termos de penaliza√ß√£o, e √© minimizada durante o treinamento do modelo [^4.4.4].

```mermaid
graph LR
    subgraph "Regularized Logistic Regression"
        direction LR
        A["Log-Likelihood Loss"]
        B["L1 Penalty: Œª||Œ≤||_1"]
        C["L2 Penalty: Œª||Œ≤||¬≤"]
        D["Regularized Cost Function"]
        A --> D
        B --> D
        C --> D
    end
```

> üí° **Exemplo Num√©rico:** Vamos aplicar a regulariza√ß√£o L1 (Lasso) e L2 (Ridge) em um modelo de regress√£o log√≠stica usando um conjunto de dados sint√©tico:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LogisticRegression
> from sklearn.preprocessing import StandardScaler
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import accuracy_score
>
> # Gerar dados sint√©ticos
> np.random.seed(42)
> X = np.random.rand(100, 10) # 100 amostras com 10 features
> true_coeffs = np.array([1, -2, 0.5, 0, 0, 0, 0, 0.8, -0.3, 0]) # Coeficientes verdadeiros (esparso)
> probabilities = 1 / (1 + np.exp(-(X @ true_coeffs + np.random.normal(0, 0.5, 100))))
> y = (probabilities > 0.5).astype(int)
>
> # Dividir dados em treino e teste
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # Normalizar os dados
> scaler = StandardScaler()
> X_train_scaled = scaler.fit_transform(X_train)
> X_test_scaled = scaler.transform(X_test)
>
> # Modelos
> model_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=1.0) # Lasso
> model_l2 = LogisticRegression(penalty='l2', C=1.0) # Ridge
>
> # Treinamento e avalia√ß√£o
> model_l1.fit(X_train_scaled, y_train)
> model_l2.fit(X_train_scaled, y_train)
>
> y_pred_l1 = model_l1.predict(X_test_scaled)
> y_pred_l2 = model_l2.predict(X_test_scaled)
>
> accuracy_l1 = accuracy_score(y_test, y_pred_l1)
> accuracy_l2 = accuracy_score(y_test, y_pred_l2)
>
> print("Acur√°cia Lasso:", accuracy_l1)
> print("Acur√°cia Ridge:", accuracy_l2)
>
> print("Coeficientes Lasso:", model_l1.coef_)
> print("Coeficientes Ridge:", model_l2.coef_)
> ```
>
> **Sa√≠da (exemplo):**
>
> ```
> Acur√°cia Lasso: 0.8666666666666667
> Acur√°cia Ridge: 0.8333333333333334
> Coeficientes Lasso: [[ 0.799 -1.534  0.465  0.    -0.    -0.   -0.   0.874 -0.231  0.   ]]
> Coeficientes Ridge: [[ 0.718 -1.499  0.412 -0.056 -0.157 -0.089 0.129  0.710 -0.165 -0.116]]
> ```
> *   **Interpreta√ß√£o:**
>     *   O c√≥digo gera dados simulados com alguns coeficientes verdadeiros iguais a zero, simulando um cen√°rio esparso.
>     *   A regulariza√ß√£o L1 (Lasso) tende a zerar os coeficientes menos importantes, enquanto a L2 (Ridge) reduz seus valores, mas geralmente n√£o os zera.
>     *   Nesse exemplo, a acur√°cia dos dois modelos √© compar√°vel, mas o Lasso mostra a propriedade de sele√ß√£o de vari√°veis, atribuindo valores nulos a alguns coeficientes.

**Lemma 3:** Ao adicionar uma penalidade L1 √† fun√ß√£o de custo da regress√£o log√≠stica, obtemos coeficientes esparsos, ou seja, muitos deles iguais a zero. Isso pode ser demonstrado atrav√©s da an√°lise das condi√ß√µes de otimalidade para a minimiza√ß√£o da fun√ß√£o de custo regularizada [^4.4.4].

**Prova do Lemma 3:** Seja a fun√ß√£o de custo regularizada dada por
$$
J(\beta) = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p(x_i)) + (1-y_i)\log(1-p(x_i))] + \lambda \sum_{j=1}^{p} |\beta_j|
$$
onde $p(x_i)$ √© a probabilidade predita para a observa√ß√£o $i$, $\beta$ √© o vetor de coeficientes, e $\lambda$ √© o par√¢metro de regulariza√ß√£o. As condi√ß√µes de otimalidade s√£o encontradas onde o gradiente de $J(\beta)$ √© igual a zero. O termo de penalidade L1, $|\beta_j|$, introduz um ponto de n√£o diferenciabilidade em $\beta_j = 0$. No ponto de otimalidade, se $\beta_j = 0$, o subgradiente do termo L1 deve cancelar o gradiente da fun√ß√£o de verossimilhan√ßa, resultando em muitos coeficientes $\beta_j$ iguais a zero, levando a esparsidade [^4.4.3], [^4.4.4]. $\blacksquare$

```mermaid
graph TB
    subgraph "L1 Regularization Effect"
    direction TB
        A["Cost Function: J(Œ≤) with L1 Penalty"]
        B["Gradient of Log-Likelihood"]
        C["Subgradient of L1 Penalty: Œª * sign(Œ≤_j)"]
        D["Sparsity: Œ≤_j = 0 for less important features"]
        A --> B
        A --> C
        B & C --> D
    end
```

> üí° **Exemplo Num√©rico:** Considere a fun√ß√£o de custo com penalidade L1. Vamos analisar o efeito de um pequeno $\lambda$ (pouca regulariza√ß√£o) e um grande $\lambda$ (muita regulariza√ß√£o) em um modelo log√≠stico com um √∫nico coeficiente $\beta$:
>
> $$ J(\beta) = -\frac{1}{N}\sum_{i=1}^{N} [y_i \log(p(x_i)) + (1-y_i)\log(1-p(x_i))] + \lambda |\beta| $$
>
> Suponha que a parte da verossimilhan√ßa seja tal que, sem regulariza√ß√£o, o valor √≥timo para $\beta$ seja $\beta^* = 2$. Agora considere:
>
> *   **$\lambda = 0.1$ (pouca regulariza√ß√£o):** A penalidade adiciona $0.1 |\beta|$ √† fun√ß√£o de custo. O valor √≥timo de $\beta$ ser√° pr√≥ximo de 2, pois a penalidade n√£o √© muito forte.
> *   **$\lambda = 1$ (muita regulariza√ß√£o):** A penalidade adiciona $1 |\beta|$ √† fun√ß√£o de custo. O valor √≥timo de $\beta$ pode ser reduzido significativamente, possivelmente chegando perto de zero. A for√ßa da penaliza√ß√£o √© muito maior e "puxa" o coeficiente em dire√ß√£o a zero.
>
> A penalidade L1 for√ßa o coeficiente para 0, enquanto a penalidade L2 apenas reduz a magnitude do mesmo.

**Corol√°rio 3:** A esparsidade dos coeficientes obtida atrav√©s da regulariza√ß√£o L1 em modelos classificat√≥rios torna o modelo resultante mais f√°cil de interpretar, uma vez que apenas as vari√°veis mais relevantes para a classifica√ß√£o s√£o selecionadas, ao passo que as vari√°veis menos relevantes tem seus coeficientes zerados. Isso facilita a compreens√£o sobre quais caracter√≠sticas tem maior influ√™ncia na decis√£o do modelo [^4.4.5].

> ‚ö†Ô∏è **Ponto Crucial**: A combina√ß√£o de L1 e L2 atrav√©s da t√©cnica Elastic Net pode ser uma forma eficiente de combinar a sele√ß√£o de vari√°veis e a estabilidade, atrav√©s de penaliza√ß√µes distintas para cada componente [^4.5].

### Separating Hyperplanes e Perceptrons

A ideia de **hiperplanos separadores** parte do princ√≠pio de encontrar um hiperplano que maximize a margem entre as classes, ou seja, a dist√¢ncia entre o hiperplano e os pontos de cada classe mais pr√≥ximos. O problema de encontrar esse hiperplano √≥timo √© um problema de otimiza√ß√£o que pode ser resolvido atrav√©s do uso da dualidade de Wolfe, onde a solu√ß√£o pode ser expressa como uma combina√ß√£o linear de pontos de suporte. Os pontos de suporte s√£o aqueles pontos dos dados que s√£o mais pr√≥ximos do hiperplano de decis√£o e que definem a margem. O Perceptron de Rosenblatt √© um algoritmo iterativo para encontrar um hiperplano separador, cuja converg√™ncia √© garantida sob condi√ß√µes de separabilidade linear dos dados [^4.5.1], [^4.5.2].

```mermaid
graph LR
    subgraph "Separating Hyperplane"
        direction LR
        A["Data Points"]
        B["Hyperplane"]
        C["Margin"]
        D["Support Vectors"]
        A --"Define"--> B
        B --"Maximize"--> C
        C --"Defined By"--> D
    end
```

> üí° **Exemplo Num√©rico:** Vamos ilustrar o funcionamento de um Perceptron simples com um exemplo bidimensional:
>
> Suponha que temos dois grupos de pontos:
>
> * Classe A (negativa): (1,1), (2,1), (1,2)
> * Classe B (positiva): (3,3), (4,3), (3,4)
>
> O objetivo do Perceptron √© encontrar uma linha (hiperplano) que separe essas classes.
>
> 1.  **Inicializa√ß√£o:** Atribu√≠mos um vetor de pesos aleat√≥rio, digamos $w = [0.1, 0.1]$ e um bias $b = 0$.
> 2.  **Itera√ß√£o:** Para cada ponto de dado $(x_i, y_i)$:
>   *   Calculamos a sa√≠da do perceptron: $\hat{y_i} = w \cdot x_i + b$
>   *   Se $\hat{y_i} \geq 0$ e $y_i$ √© classe negativa ou $\hat{y_i} < 0$ e $y_i$ √© classe positiva, atualizamos os pesos e o bias:
>   $w_{new} = w + \alpha \cdot y_i \cdot x_i$
>   $b_{new} = b + \alpha \cdot y_i$
>
>   Onde $\alpha$ √© a taxa de aprendizado (ex: 0.1) e  $y_i$ √© -1 para classe A e 1 para classe B.
>
> 3.  **Repeti√ß√£o:** Repetimos o passo 2 at√© que todos os pontos sejam classificados corretamente ou at√© um n√∫mero m√°ximo de itera√ß√µes.
>
> Ap√≥s algumas itera√ß√µes, o Perceptron ir√° encontrar um hiperpl