## Limita√ß√µes do AIC na Sele√ß√£o de Modelos Estat√≠sticos

```mermaid
flowchart LR
    subgraph "Model Selection Process"
    direction TB
        A["Data"] --> B("Candidate Models")
        B --> C{"AIC, BIC, Cross-validation..."}
        C --> D["Model Evaluation"]
        D --> E["Selected Model"]
        E --> F["Bias-Variance Tradeoff"]
        F --> G["Generalization Performance"]
        end
```

### Introdu√ß√£o

A sele√ß√£o de modelos √© uma etapa crucial na an√°lise de dados e aprendizado de m√°quina, influenciando diretamente a generaliza√ß√£o e a interpretabilidade dos resultados. O **Akaike Information Criterion (AIC)** √© uma ferramenta popular para auxiliar nessa tarefa, fornecendo uma estimativa da qualidade relativa de diferentes modelos para um dado conjunto de dados. No entanto, √© fundamental compreender as limita√ß√µes do AIC para utiliz√°-lo de forma eficaz e evitar interpreta√ß√µes equivocadas [^7.1]. Este cap√≠tulo aprofunda as limita√ß√µes do AIC, explorando situa√ß√µes em que seu uso pode ser problem√°tico ou inadequado, baseado principalmente nas discuss√µes presentes no material fornecido, em particular nas se√ß√µes sobre bias, variance, model assessment e sele√ß√£o [^7.1, ^7.2, ^7.3, ^7.4, ^7.5, ^7.6, ^7.7].

### Conceitos Fundamentais

**Conceito 1: AIC e a Estimativa do Erro de Teste**

O AIC √© uma medida de qualidade de modelos que tenta estimar o erro de generaliza√ß√£o, ou seja, o desempenho do modelo em dados n√£o vistos.  O AIC adiciona uma penalidade para a complexidade do modelo, equilibrando o ajuste aos dados de treinamento (que tende a favorecer modelos mais complexos) com a capacidade de generaliza√ß√£o [^7.5]. Formalmente, o AIC √© definido como:

$$AIC = -2 \log(\hat{L}) + 2d$$

onde $\hat{L}$ √© a verossimilhan√ßa maximizada do modelo e $d$ √© o n√∫mero de par√¢metros. O termo $-2 \log(\hat{L})$ mede o qu√£o bem o modelo se ajusta aos dados, enquanto $2d$ penaliza modelos com muitos par√¢metros, evitando overfitting [^7.5]. O objetivo do AIC √© selecionar o modelo que minimiza essa express√£o, indicando um bom trade-off entre ajuste e complexidade.

> üí° **Exemplo Num√©rico:** Suponha que temos dois modelos, $M_1$ e $M_2$, ajustados a um conjunto de dados. O modelo $M_1$ tem 3 par√¢metros e uma log-verossimilhan√ßa maximizada $\hat{L}_1 = 100$, enquanto $M_2$ tem 5 par√¢metros e uma log-verossimilhan√ßa maximizada $\hat{L}_2 = 110$. Calculando o AIC para cada modelo:
>
> $\text{AIC}_1 = -2 \log(100) + 2 \times 3 = -2 \times 4.605 + 6 = -9.21 + 6 = -3.21$
>
> $\text{AIC}_2 = -2 \log(110) + 2 \times 5 = -2 \times 4.700 + 10 = -9.40 + 10 = 0.6$
>
> Neste caso, o modelo $M_1$ tem um AIC menor (-3.21), indicando que ele √© prefer√≠vel em rela√ß√£o a $M_2$ (AIC = 0.6), dado o trade-off entre o ajuste e a complexidade.

**Lemma 1: Rela√ß√£o do AIC com a Estimativa do Erro de Teste**

Baseando-se em [^7.4], o AIC pode ser interpretado como uma corre√ß√£o do erro de treinamento, adicionando uma estimativa do otimismo do erro de treinamento:

$$ E_y(Err_{in}) = E_y(err) + \frac{2}{N}\sum_{i=1}^N Cov(\hat{y}_i, y_i) $$

onde $Err_{in}$ √© o erro de previs√£o in-sample, $err$ √© o erro de treinamento, e $Cov(\hat{y}_i, y_i)$ √© a covari√¢ncia entre as previs√µes e os valores reais. Essa rela√ß√£o revela que o AIC ajusta o erro de treinamento considerando a complexidade do modelo, que est√° ligada √† magnitude da covari√¢ncia [^7.4].

```mermaid
graph LR
    subgraph "AIC Error Estimation"
      direction TB
        A["E_y(Err_{in})"] --> B["E_y(err)"]
        A --> C["(2/N) * sum(Cov(≈∑_i, y_i))"]
        B & C --> D["Adjusted Training Error"]
    end
```

> üí° **Exemplo Num√©rico:** Suponha que, para um modelo de regress√£o com 100 observa√ß√µes, o erro de treinamento $err$ seja 0.5. A soma das covari√¢ncias $ \sum_{i=1}^{100} Cov(\hat{y}_i, y_i) $ √© igual a 10. Usando a f√≥rmula do Lemma 1:
>
> $E_y(Err_{in}) = 0.5 + \frac{2}{100} \times 10 = 0.5 + 0.2 = 0.7$.
>
> Isto indica que o erro de generaliza√ß√£o (aproximado por $Err_{in}$) √© maior que o erro de treinamento devido ao otimismo do ajuste no conjunto de treino. O AIC tenta capturar esse aumento.

**Conceito 2: Deriva√ß√£o do AIC a Partir da Teoria da Informa√ß√£o**

O AIC √© derivado de princ√≠pios da teoria da informa√ß√£o, especificamente a diverg√™ncia de Kullback-Leibler (KL), que mede a diferen√ßa entre duas distribui√ß√µes de probabilidade. Em termos pr√°ticos, o AIC tenta encontrar o modelo que, dentre os modelos candidatos, est√° mais pr√≥ximo da "verdadeira" distribui√ß√£o geradora dos dados [^7.5, ^7.7].  A minimiza√ß√£o do AIC pode ser vista como a minimiza√ß√£o de uma estimativa da dist√¢ncia entre o modelo proposto e o modelo "verdadeiro".

**Corol√°rio 1: AIC e Modelos Gaussianos**

Para modelos gaussianos com vari√¢ncia $\sigma^2$ conhecida, o AIC √© equivalente √† estat√≠stica $C_p$:

$$AIC \approx C_p = err + \frac{2d}{N} \sigma^2$$

Essa equival√™ncia surge pois, para modelos gaussianos, a log-verossimilhan√ßa est√° diretamente relacionada ao erro quadr√°tico m√©dio [^7.5]. Isso demonstra que, para esses casos, o AIC √© diretamente relacionado ao erro de treinamento corrigido pela complexidade do modelo, sendo um bom indicador do erro de generaliza√ß√£o [^7.5].

```mermaid
graph LR
    subgraph "AIC and Gaussian Models"
        direction LR
        A["AIC"] -- "‚âà" --> B["C_p"]
        B --> C["err"]
        B --> D["(2d/N) * œÉ¬≤"]
        C & D --> E["Adjusted Error"]
    end
```

> üí° **Exemplo Num√©rico:** Consideremos um modelo de regress√£o linear com 5 par√¢metros ajustado a 50 observa√ß√µes. O erro quadr√°tico m√©dio (MSE) $err$ √© 0.4, e a vari√¢ncia dos erros $\sigma^2$ √© estimada como 0.2. Usando a f√≥rmula do $C_p$:
>
> $C_p = 0.4 + \frac{2 \times 5}{50} \times 0.2 = 0.4 + 0.04 = 0.44$
>
> Este valor √© uma aproxima√ß√£o do AIC, e nos diz que o erro de treinamento (0.4) √© ajustado pela complexidade do modelo (0.04) para obter uma estimativa mais precisa do erro de generaliza√ß√£o.

**Conceito 3: Limita√ß√µes Fundamentais do AIC**

Apesar de sua ampla aplicabilidade, o AIC possui limita√ß√µes que devem ser consideradas.  Uma limita√ß√£o crucial √© que ele √© assintoticamente consistente como um crit√©rio de sele√ß√£o de modelos, ou seja, quando o tamanho da amostra ($N$) tende ao infinito, o AIC tem uma tend√™ncia de selecionar o modelo verdadeiro, *se ele estiver entre os candidatos*. Mas, para tamanhos de amostra finitos, o AIC pode ser inconsistente e selecionar um modelo mais complexo do que o necess√°rio, especialmente quando a verdadeira estrutura √© mais simples [^7.7].

> ‚ö†Ô∏è **Nota Importante**: O AIC, ao penalizar a complexidade dos modelos, tem como objetivo encontrar o melhor trade-off entre ajuste aos dados de treinamento e capacidade de generaliza√ß√£o, mas essa penalidade pode ser insuficiente para amostras pequenas. **Refer√™ncia ao t√≥pico [^7.5]**.

> ‚ùó **Ponto de Aten√ß√£o**: A natureza assint√≥tica do AIC significa que, para amostras finitas, ele pode favorecer modelos complexos e apresentar resultados sub√≥timos. **Conforme indicado em [^7.7]**.

> ‚úîÔ∏è **Destaque**: Embora o AIC seja amplamente usado, ele n√£o garante a sele√ß√£o do melhor modelo em todos os cen√°rios, e pode ser necess√°rio considerar outros crit√©rios de sele√ß√£o de modelos para uma an√°lise robusta. **Baseado no t√≥pico [^7.7]**.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
flowchart TD
    subgraph "AIC in Regression vs. Classification"
        direction LR
        A[("Regression")] --> B("Log-likelihood linked to quadratic error")
        B --> C("AIC estimates generalization error well")
        D[("Classification")] --> E("Log-likelihood not directly linked to 0-1 loss")
        E --> F("AIC problematic for some metrics")
        end
```

**Explica√ß√£o:** Este diagrama destaca como a aplica√ß√£o direta do AIC pode ser mais adequada para problemas de regress√£o, enquanto exige cautela na classifica√ß√£o devido √† diferente natureza das fun√ß√µes de perda [^7.5].

Quando se usa o AIC em classifica√ß√£o, em que o objetivo √© minimizar o erro de classifica√ß√£o (por exemplo, usando 0-1 loss), a rela√ß√£o direta entre log-verossimilhan√ßa e erro de generaliza√ß√£o nem sempre √© garantida. O AIC √© derivado do erro quadr√°tico ou log-verossimilhan√ßa, que s√£o fun√ß√µes de custo suaves, mas o erro de classifica√ß√£o √© uma fun√ß√£o discreta, e o AIC pode n√£o ser a escolha mais adequada. Para problemas de classifica√ß√£o, outras medidas podem ser mais apropriadas, como a valida√ß√£o cruzada ou abordagens bayesianas.

**Lemma 2: Penaliza√ß√£o do AIC em Modelos Lineares**

Em modelos lineares, a penaliza√ß√£o do AIC $2d/N$ se baseia no n√∫mero de par√¢metros $d$ e no tamanho da amostra $N$, como demostrado em [^7.5]. Se o n√∫mero de par√¢metros for grande em rela√ß√£o ao tamanho da amostra, a penaliza√ß√£o do AIC ser√° maior, favorecendo modelos mais simples e evitando overfitting. No entanto, em situa√ß√µes com muitos preditores e um n√∫mero limitado de observa√ß√µes, a penaliza√ß√£o do AIC pode n√£o ser suficiente para prevenir o overfitting. Isso tamb√©m se relaciona ao problema de selection bias em situa√ß√µes com grande dimensionalidade [^7.5, ^7.10].

> üí° **Exemplo Num√©rico:** Considere um modelo linear com 10 preditores ajustado em um conjunto de dados com 30 observa√ß√µes, resultando em 11 par√¢metros (incluindo o intercepto). Se a log-verossimilhan√ßa √© $\hat{L} = 80$, o AIC √©:
>
> $AIC = -2 \times 80 + 2 \times 11 = -160 + 22 = -138$
>
> Agora, suponha que um modelo mais simples com 5 preditores (6 par√¢metros) resulte em uma log-verossimilhan√ßa de $\hat{L} = 75$. O AIC ser√°:
>
> $AIC = -2 \times 75 + 2 \times 6 = -150 + 12 = -138$
>
> Mesmo com uma verossimilhan√ßa ligeiramente menor, o modelo mais simples tem o mesmo AIC. Para um tamanho de amostra pequeno, a penaliza√ß√£o de complexidade pode n√£o ser suficiente para favorecer o modelo mais simples.

**Corol√°rio 2: AIC e a Dimensionalidade dos Dados**

Em cen√°rios de alta dimensionalidade (muitos preditores), o AIC pode favorecer modelos complexos, pois os graus de liberdade efetivos do modelo podem ser muito maiores do que o n√∫mero de par√¢metros. Essa limita√ß√£o √© explicitamente abordada em [^7.5], onde se comenta que "a fun√ß√£o AIC(a) fornece uma estimativa da curva de erro de teste", mas essa fun√ß√£o tamb√©m pode se tornar complexa quando as fun√ß√µes base s√£o escolhidas de forma adaptativa. A equival√™ncia direta entre n√∫mero de par√¢metros e complexidade deixa de ser v√°lida, e pode levar a resultados menos precisos.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
  subgraph "Model Selection Methods"
    direction TB
    A[("AIC")] --> B[("Regularization")]
    A --> C[("Cross-Validation")]
    A --> D[("Bootstrap")]
     B & C & D --> E[("Model Selection")]
     E-->F("Tradeoffs and Limitations")
     end
```

Ao usar m√©todos de sele√ß√£o de vari√°veis e regulariza√ß√£o em classifica√ß√£o, √© crucial entender como o AIC se encaixa nesses processos. A regulariza√ß√£o, como L1 e L2, visa diminuir a complexidade do modelo adicionando um termo de penaliza√ß√£o √† fun√ß√£o de custo, o que pode mitigar o overfitting, mas n√£o substitui a necessidade de analisar o erro de generaliza√ß√£o atrav√©s de medidas como o AIC [^7.5]. O AIC tenta ajustar o n√∫mero de par√¢metros de maneira global, enquanto a regulariza√ß√£o faz um ajuste mais local e diretamente sobre os pesos do modelo [^7.4, ^7.6].

**Lemma 3: AIC e Modelos Regularizados**

Em modelos regularizados, como a regress√£o log√≠stica com penaliza√ß√£o L1, o n√∫mero de par√¢metros $d$ no AIC pode n√£o ser o n√∫mero de par√¢metros explicitamente inclu√≠dos no modelo, mas sim o n√∫mero de par√¢metros efetivos, considerando a penaliza√ß√£o. Ou seja, em modelos com regulariza√ß√£o, $d$ n√£o representa diretamente o n√∫mero de coeficientes, mas sim o grau de liberdade efetivo da fun√ß√£o, de acordo com [^7.6]:
$$df(S) = trace(S)$$
onde S representa a matriz de proje√ß√£o linear, e pode ser uma matriz de pondera√ß√µes regularizadas.

**Prova do Lemma 3:**  A deriva√ß√£o da express√£o para o n√∫mero efetivo de par√¢metros em modelos regularizados pode ser obtida a partir do conceito de degrees of freedom, que se refere ao n√∫mero de par√¢metros que contribuem significativamente para a varia√ß√£o do modelo. Em modelos sem regulariza√ß√£o, esse n√∫mero coincide com o n√∫mero total de par√¢metros. No entanto, com regulariza√ß√£o, alguns par√¢metros s√£o reduzidos, resultando em um n√∫mero efetivo de par√¢metros menor. A formaliza√ß√£o via $trace(S)$ permite quantificar essa redu√ß√£o e corrigir as estimativas de erro [^7.6]. $\blacksquare$

```mermaid
graph LR
    subgraph "Regularized Models and AIC"
        direction TB
        A["d in AIC"] --> B["Effective Number of Parameters"]
        B --> C["df(S) = trace(S)"]
        C --> D["Regularized Projection Matrix S"]
    end
```

> üí° **Exemplo Num√©rico:** Considere um modelo de regress√£o linear com penaliza√ß√£o L1 (Lasso) com 10 preditores. Inicialmente, temos 11 par√¢metros (incluindo o intercepto). Ap√≥s a regulariza√ß√£o, alguns coeficientes s√£o for√ßados a zero, resultando em 5 coeficientes n√£o nulos.  No entanto, o n√∫mero efetivo de par√¢metros ($df(S)$)  pode n√£o ser exatamente 5, devido √† natureza da regulariza√ß√£o. Suponha que  $trace(S)$ seja calculado como 6.  O AIC usar√° este valor 6 para a penaliza√ß√£o, em vez de 11, refletindo a complexidade efetiva do modelo.

**Corol√°rio 3: AIC e Elastic Net**

A penaliza√ß√£o Elastic Net, que combina L1 e L2, introduz um n√≠vel adicional de complexidade na defini√ß√£o do n√∫mero efetivo de par√¢metros em rela√ß√£o ao AIC. Embora a penaliza√ß√£o auxilie no controle do overfitting, a escolha do par√¢metro $\alpha$ do Elastic Net pode ser um desafio e o uso do AIC para essa escolha ainda apresenta limita√ß√µes. Em particular, o AIC em problemas de classifica√ß√£o n√£o √© um crit√©rio de sele√ß√£o ideal quando a fun√ß√£o de perda n√£o est√° diretamente relacionada √† log-verossimilhan√ßa. [^7.6]

> ‚ö†Ô∏è **Ponto Crucial**: Ao usar regulariza√ß√£o e o AIC, √© crucial considerar que o n√∫mero de par√¢metros na f√≥rmula do AIC refere-se ao n√∫mero efetivo de par√¢metros, e n√£o apenas ao n√∫mero de par√¢metros declarados. **Conforme discutido em [^7.6]**.

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
    subgraph "Hyperplanes and AIC"
    direction TB
        A["Hyperplane Choice"] --> B["Model Complexity"]
        B --> C["Vapnik-Chervonenkis (VC) Theory"]
        C --> D["AIC Limitations"]
        D --> E["Perceptron Complexity"]
         E -->F["Regularization and Margin"]

    end
```

A escolha de hiperplanos separadores √≥timos, como em Support Vector Machines (SVMs), √© influenciada pela complexidade dos modelos.  A teoria de Vapnik-Chervonenkis (VC) √© uma alternativa para quantificar essa complexidade, e ela fornece uma medida mais geral da complexidade de um modelo que o n√∫mero de par√¢metros. O AIC, que penaliza explicitamente por par√¢metros adicionais, pode ter dificuldade em lidar com a complexidade de modelos como o Perceptron [^7.9].  Em modelos baseados em hyperplanes, a regulariza√ß√£o e o ajuste fino da margem de separa√ß√£o s√£o mais cr√≠ticos para a generaliza√ß√£o do que o controle da quantidade de par√¢metros.

### Pergunta Te√≥rica Avan√ßada (Exemplo): Qual a rela√ß√£o entre a penalidade do AIC e a complexidade de modelos n√£o lineares, como redes neurais?

**Resposta:**

A penalidade do AIC, baseada no n√∫mero de par√¢metros, nem sempre reflete a complexidade real de modelos n√£o-lineares. Em redes neurais, por exemplo, o n√∫mero de par√¢metros pode ser muito alto, mas nem todos os par√¢metros contribuem igualmente para a capacidade de generaliza√ß√£o do modelo. A estrutura da rede, suas ativa√ß√µes e o processo de treinamento, que n√£o s√£o capturados pelo n√∫mero bruto de par√¢metros, afetam a complexidade do modelo. O AIC pode falhar em capturar corretamente essas complexidades em modelos n√£o lineares, o que pode levar a uma subestima√ß√£o da complexidade do modelo [^7.6].

**Lemma 4: AIC e a Complexidade em Modelos N√£o-Lineares**

A abordagem de Vapnik-Chervonenkis (VC) para medir a complexidade de modelos, discutida em [^7.9], introduz a no√ß√£o de que a capacidade de um modelo em "shatter" pontos n√£o √© linearmente relacionada com o n√∫mero de par√¢metros. A teoria VC oferece uma forma de quantificar essa capacidade de "shatter" e fornecer limites para a generaliza√ß√£o, demonstrando a limita√ß√£o do AIC em refletir a verdadeira complexidade de modelos n√£o lineares [^7.9].

```mermaid
graph LR
    subgraph "Non-linear Models and AIC"
        direction TB
        A["AIC Penalty"] --> B["Number of Parameters"]
        B --> C["Does not reflect true complexity of non-linear models"]
        C --> D["VC Theory provides better complexity measure"]
        D --> E["'shattering' points"]
    end
```

> üí° **Exemplo Num√©rico:** Considere uma rede neural com 3 camadas ocultas, totalizando 1000 par√¢metros. O AIC considera a penaliza√ß√£o baseada nesse n√∫mero,  mas a complexidade real da rede depende da arquitetura, das fun√ß√µes de ativa√ß√£o e do treinamento.  A teoria VC poderia quantificar a complexidade da rede de uma forma mais hol√≠stica. Por exemplo, um modelo com grande capacidade de "shatter" pode ter um risco de generaliza√ß√£o maior, mesmo se o AIC sugerir o contr√°rio.

**Corol√°rio 4: Limita√ß√µes do AIC para Modelos com Regulariza√ß√£o**

Em modelos com regulariza√ß√£o, como redes neurais com weight decay, o AIC se baseia em um n√∫mero efetivo de par√¢metros que est√° diretamente relacionado √† magnitude da penaliza√ß√£o. Assim, o AIC n√£o capta a verdadeira complexidade da arquitetura da rede, mas apenas uma estimativa aproximada da complexidade induzida pela regulariza√ß√£o. Uma regulariza√ß√£o forte pode diminuir o n√∫mero efetivo de par√¢metros, mas pode levar a um trade-off desfavor√°vel entre bias e variance, e o AIC pode n√£o capturar corretamente essa situa√ß√£o [^7.6].

### Conclus√£o

O AIC √© uma ferramenta valiosa para a sele√ß√£o de modelos, especialmente quando combinado com outras t√©cnicas como cross-validation e bootstrap, conforme apresentado no cap√≠tulo 7 do livro. No entanto, suas limita√ß√µes, especialmente em problemas de classifica√ß√£o e com modelos n√£o-lineares, precisam ser consideradas. O AIC n√£o substitui uma avalia√ß√£o cuidadosa da adequa√ß√£o de modelos e uma an√°lise criteriosa dos objetivos da an√°lise. Abordagens como a teoria de VC e medidas de erro mais robustas, combinadas com regulariza√ß√£o e avalia√ß√£o da estabilidade das previs√µes, s√£o fundamentais para a sele√ß√£o de modelos eficazes e para uma an√°lise estat√≠stica robusta.

<!-- END DOCUMENT -->
### Footnotes

[^7.1]: "The generalization performance of a learning method relates to its predic-tion capability on independent test data. Assessment of this performance is extremely important in practice, since it guides the choice of learning method or model, and gives us a measure of the quality of the ultimately chosen model." *(Trecho de Model Assessment and Selection)*

[^7.2]: "Figure 7.1 illustrates the important issue in assessing the ability of a learning method to generalize. Consider first the case of a quantitative or interval scale response. We have a target variable Y, a vector of inputs X, and a prediction model f(X) that has been estimated from a training set T." *(Trecho de Model Assessment and Selection)*

[^7.3]: "As in Chapter 2, if we assume that $Y = f(X) + \epsilon$ where $E(\epsilon) = 0$ and $Var(\epsilon) = \sigma_\epsilon$, we can derive an expression for the expected prediction error of a regression fit f(X) at an input point X = x0, using squared-error loss" *(Trecho de Model Assessment and Selection)*

[^7.4]: "Discussions of error rate estimation can be confusing, because we have to make clear which quantities are fixed and which are random¬π. Before we continue, we need a few definitions, elaborating on the material of Sec-tion 7.2. Given a training set $T = \{(x_1, y_1), (x_2, y_2), \ldots, (x_N, y_N)\}$ the gen-eralization error of a model f is" *(Trecho de Model Assessment and Selection)*

[^7.5]: "The general form of the in-sample estimates is $Err_{in} = err + \omega$, where $\omega$ is an estimate of the average optimism. Using expression (7.24), applicable when $d$ parameters are fit under squared error loss, leads to a version of the so-called $C_p$ statistic," *(Trecho de Model Assessment and Selection)*

[^7.6]: "The concept of "number of parameters" can be generalized, especially to models where regularization is used in the fitting. Suppose we stack the outcomes $y_1, y_2,\ldots, y_n$ into a vector $y$, and similarly for the predictions $\hat{y}$. Then a linear fitting method is one for which we can write" *(Trecho de Model Assessment and Selection)*

[^7.7]: "The Bayesian information criterion (BIC), like AIC, is applicable in settings where the fitting is carried out by maximization of a log-likelihood. The generic form of BIC is $BIC = -2\log \text{lik} + (\log N) \cdot d$." *(Trecho de Model Assessment and Selection)*
[^7.9]: "A difficulty in using estimates of in-sample error is the need to specify the number of parameters (or the complexity) d used in the fit. Although the effective number of parameters introduced in Section 7.6 is useful for some nonlinear models, it is not fully general. The Vapnik-Chervonenkis (VC) theory provides such a general measure of complexity, and gives associated bounds on the optimism." *(Trecho de Model Assessment and Selection)*
[^7.10]: "Consider a classification problem with a large number of predictors, as may arise, for example, in genomic or proteomic applications. A typical strategy for analysis might be as follows:" *(Trecho de Model Assessment and Selection)*
