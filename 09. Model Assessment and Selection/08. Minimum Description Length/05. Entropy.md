## Avalia√ß√£o e Sele√ß√£o de Modelos: Foco na Entropia

```mermaid
graph LR
    subgraph "Model Evaluation Methods"
        A["AIC"]
        B["BIC"]
        C["Cross-Validation"]
        D["Bootstrap"]
        E["Minimize Entropia"]
        A --> E
        B --> E
        C --> E
        D --> E
        style E fill:#f9f,stroke:#333,stroke-width:2px
        subgraph "Entropia"
            F["Conditional Entropy"]
            G["Expected Entropy"]
            E --> F
            E --> G
        end
    end
```

### Introdu√ß√£o

A **generaliza√ß√£o** de um m√©todo de aprendizado refere-se √† sua capacidade de prever resultados em dados de teste independentes. Avaliar essa performance √© crucial na pr√°tica, pois direciona a escolha do m√©todo ou modelo de aprendizado e mensura a qualidade do modelo escolhido [^7.1]. Este cap√≠tulo se dedica a descrever e ilustrar os m√©todos de avalia√ß√£o de performance e como eles s√£o usados na sele√ß√£o de modelos, com √™nfase na intera√ß√£o entre **vi√©s**, **vari√¢ncia** e **complexidade do modelo** [^7.1].

### Conceitos Fundamentais

**Conceito 1: Generaliza√ß√£o e Erro de Predi√ß√£o**

A **generaliza√ß√£o** √© a habilidade de um modelo em prever resultados em dados n√£o vistos durante o treinamento. O erro de predi√ß√£o √© a m√©trica que quantifica o desempenho do modelo, medindo a discrep√¢ncia entre os valores preditos e os valores reais. Um modelo com alta generaliza√ß√£o deve apresentar baixo erro de predi√ß√£o em novos dados. O erro de predi√ß√£o √© normalmente quantificado usando uma fun√ß√£o de perda $L(Y, f(X))$ [^7.1]. As escolhas mais comuns s√£o o **erro quadr√°tico** $L(Y, f(X)) = (Y - f(X))^2$ e o **erro absoluto** $L(Y, f(X)) = |Y - f(X)|$ [^7.1].

> ‚ö†Ô∏è **Nota Importante**: A avalia√ß√£o da capacidade de generaliza√ß√£o √© essencial para garantir que o modelo n√£o esteja sobreajustando os dados de treinamento, o que levaria a um baixo desempenho em dados n√£o vistos.

**Lemma 1:** *A rela√ß√£o entre o erro de predi√ß√£o esperado e a complexidade do modelo*. O erro de predi√ß√£o esperado $Err = E[L(Y, f(X))]$ √© decomposto em vi√©s ao quadrado e vari√¢ncia [^7.2]. Modelos com alta complexidade tendem a ter baixo vi√©s, mas alta vari√¢ncia, e vice-versa. O erro de predi√ß√£o esperado √© minimizado quando se encontra um equil√≠brio entre o vi√©s e a vari√¢ncia [^7.2].

$$Err(x_0) = \sigma^2 + Bias^2(f(x_0)) + Var(f(x_0))$$

Onde $\sigma^2$ √© a vari√¢ncia do erro irredut√≠vel. O **vi√©s** quantifica o qu√£o longe a m√©dia das estimativas est√° do valor real e a **vari√¢ncia** quantifica a varia√ß√£o das estimativas em diferentes conjuntos de treinamento.

```mermaid
graph LR
    subgraph "Error Decomposition"
        direction TB
        A["Total Prediction Error: Err(x_0)"]
        B["Irreducible Error: œÉ¬≤"]
        C["Bias¬≤ Component: Bias¬≤(f(x_0))"]
        D["Variance Component: Var(f(x_0))"]
        A --> B
        A --> C
        A --> D
    end
```

**Prova do Lemma 1:**
Expandindo o termo $E[(Y-f(x_0))^2 | X=x_0]$, temos:

$$E[(Y-f(x_0))^2 | X=x_0] = E[(Y-E[f(x_0)| X=x_0] + E[f(x_0)| X=x_0] - f(x_0))^2 | X=x_0]$$
$$= E[(Y-E[f(x_0)| X=x_0])^2 | X=x_0] + E[(E[f(x_0)| X=x_0]-f(x_0))^2| X=x_0] + 2E[(Y-E[f(x_0)| X=x_0])(E[f(x_0)| X=x_0]-f(x_0)) | X=x_0]$$
O terceiro termo √© zero, pois $E[Y - E[f(x_0)] | X=x_0]=0$. Assim, temos
$$E[(Y-f(x_0))^2 | X=x_0] = E[(Y-E[f(x_0)| X=x_0])^2 | X=x_0] + E[(E[f(x_0)| X=x_0]-f(x_0))^2| X=x_0]$$
O primeiro termo √© a vari√¢ncia do erro irredut√≠vel, enquanto o segundo termo pode ser decomposto:
$$E[(E[f(x_0)| X=x_0]-f(x_0))^2| X=x_0] = [E[f(x_0)| X=x_0] - f(x_0)]^2 + E[(f(x_0)-E[f(x_0)|X=x_0])^2 | X=x_0]$$
O primeiro termo √© o bias ao quadrado e o segundo termo √© a vari√¢ncia, o que completa a prova. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Vamos considerar um cen√°rio onde queremos modelar uma rela√ß√£o entre uma vari√°vel de entrada $X$ e uma vari√°vel de sa√≠da $Y$. Suponha que a rela√ß√£o verdadeira seja $Y = 2X + 3 + \epsilon$, onde $\epsilon$ √© um erro aleat√≥rio com m√©dia zero e vari√¢ncia $\sigma^2 = 1$. Agora vamos comparar dois modelos:
>
> **Modelo 1 (Simples):**  $f_1(X) = 2.1X + 2.8$
> **Modelo 2 (Complexo):** $f_2(X) = 2X + 3 + 0.5X^2 + \eta$, onde $\eta$ √© um erro aleat√≥rio com m√©dia zero e vari√¢ncia muito maior que $\epsilon$.
>
> Vamos analisar o erro para um ponto $x_0 = 1$:
>
> Para o **Modelo 1:**
>   - Valor real: $Y(1) = 2(1) + 3 = 5$
>   - Predi√ß√£o: $f_1(1) = 2.1(1) + 2.8 = 4.9$
>   - Vi√©s: $E[f_1(1)] - Y(1) = 4.9 - 5 = -0.1$.  $Bias^2 = (-0.1)^2 = 0.01$
>   - Vari√¢ncia: Suponha que a varia√ß√£o das estimativas do modelo seja baixa, $Var(f_1(1)) = 0.1$.
>
> Para o **Modelo 2:**
>   - Valor real: $Y(1) = 5$
>   - Predi√ß√£o: $f_2(1) = 2(1) + 3 + 0.5(1)^2 + \eta = 5.5 + \eta $
>   - Vi√©s: Se considerarmos que o termo quadr√°tico consegue capturar a rela√ß√£o, $E[f_2(1)] - Y(1) = 5.5 - 5= 0.5 $. $Bias^2 = (0.5)^2 = 0.25$
>   - Vari√¢ncia: Como o modelo √© muito mais sens√≠vel a varia√ß√µes nos dados de treinamento, supomos que $Var(f_2(1)) = 2$.
>
>  O erro total para cada modelo, para um ponto $x_0=1$:
>   - $Err_1(1) = 1 + 0.01 + 0.1 = 1.11$
>   - $Err_2(1) = 1 + 0.25 + 2 = 3.25$
>
> Notamos que o modelo mais simples (Modelo 1) tem um vi√©s baixo e uma vari√¢ncia baixa, o que resulta em um erro total menor. J√° o modelo mais complexo (Modelo 2) tem um vi√©s maior e uma vari√¢ncia muito maior, o que aumenta significativamente o erro. Este exemplo ilustra como modelos mais complexos podem ter uma vari√¢ncia maior, e como √© crucial o balan√ßo entre vi√©s e vari√¢ncia para minimizar o erro de predi√ß√£o.

**Conceito 2: Entropia e Perda Log-Verossimilhan√ßa**

A **entropia** √© uma medida da incerteza ou desordem em uma distribui√ß√£o de probabilidade [^7.2]. Na classifica√ß√£o, a entropia pode ser utilizada como fun√ß√£o de perda. Para um resposta categ√≥rica $G$ que assume $K$ valores, a fun√ß√£o de perda baseada na **entropia** √© dada por:

$$ L(G, p(X)) = -2 \sum_{k=1}^K I(G=k) \log(p_k(X)) = -2 \log p_G(X) $$

Onde $p_k(X)$ √© a probabilidade predita de que $G=k$, e $I$ √© uma fun√ß√£o indicadora. A express√£o $-2 \log p_G(X)$ √© chamada de **desvio** (deviance) e √© proporcional √† **perda log-verossimilhan√ßa** [^7.2]. O objetivo √© minimizar a perda log-verossimilhan√ßa, que equivale a maximizar a verossimilhan√ßa dos dados.

**Corol√°rio 1:** Em modelos de classifica√ß√£o, a minimiza√ß√£o da fun√ß√£o de perda da entropia (ou seja, a maximiza√ß√£o da log-verossimilhan√ßa) busca encontrar o modelo cujas probabilidades preditas se alinham melhor com as probabilidades verdadeiras das classes, assim como em uma fun√ß√£o de perda do tipo *cross-entropy* [^7.2].

> üí° **Exemplo Num√©rico:**
>
> Suponha um problema de classifica√ß√£o bin√°ria onde temos duas classes, 0 e 1. Temos um modelo que produz as seguintes probabilidades preditas para dois dados de teste:
>
> - Dado 1:  Valor real $G_1 = 1$. Modelo prediz $p_1(X) = 0.8$ para classe 1 e $1 - p_1(X) = 0.2$ para classe 0.
> - Dado 2: Valor real $G_2 = 0$. Modelo prediz $p_2(X) = 0.3$ para classe 1 e $1 - p_2(X) = 0.7$ para classe 0.
>
> A perda log-verossimilhan√ßa para cada observa√ß√£o √© dada por:
>
> - Dado 1: $L(G_1, p(X)) = -2 \log(p_1(X)) = -2 \log(0.8) \approx 0.446$
> - Dado 2: $L(G_2, p(X)) = -2 \log(1 - p_2(X)) = -2 \log(0.7) \approx 0.713$
>
> A perda log-verossimilhan√ßa total para essas duas observa√ß√µes seria:
>  $L_{total} = 0.446 + 0.713 = 1.159$
>
>  Um modelo ideal faria uma previs√£o de 1 para dado 1 e de 0 para dado 2, com probabilidades pr√≥ximas de 1. Um modelo com perdas log-verossimilhan√ßa menores √© prefer√≠vel. Este exemplo demonstra como a fun√ß√£o de perda penaliza previs√µes incorretas ou inseguras, incentivando o modelo a convergir para previs√µes mais confiantes e corretas.
>

**Conceito 3: Erro de Treinamento vs Erro de Teste**

O **erro de treinamento** √© a m√©dia da fun√ß√£o de perda nos dados de treinamento:

$$ err = \frac{1}{N} \sum_{i=1}^{N} L(Y_i, f(x_i)) $$

O **erro de teste** √© o erro de predi√ß√£o em um conjunto de dados independente, e √© uma medida mais realista do desempenho do modelo. Modelos muito complexos tendem a apresentar um erro de treinamento muito baixo, mas um erro de teste mais alto, um fen√¥meno conhecido como *overfitting* [^7.2]. O erro de teste esperado √© dado por:

$$ Err = E[L(Y, f(X))] = E[Err_T] $$
Onde $Err_T$ √© o erro de teste condicional ao conjunto de treinamento.

```mermaid
graph LR
    subgraph "Error Types"
        direction LR
        A["Training Error"] --> B["Calculated on Training Data"]
        A --> C["Biased towards overfitted models"]
        D["Test Error"] --> E["Calculated on Independent Test Data"]
        D --> F["Realistic measure of performance"]
    end
```

> ‚ùó **Ponto de Aten√ß√£o**: O erro de treinamento n√£o √© uma boa estimativa do erro de teste. √â necess√°rio utilizar m√©todos como valida√ß√£o cruzada para estimar o erro de teste [^7.2].

> üí° **Exemplo Num√©rico:**
>
> Vamos criar um exemplo simples para ilustrar a diferen√ßa entre erro de treinamento e erro de teste. Suponha que temos um conjunto de dados com 10 pontos e queremos ajustar um modelo polinomial. Dividimos os dados em 7 pontos para treinamento e 3 pontos para teste. Vamos comparar tr√™s modelos:
>
>   - **Modelo 1:**  $f_1(X) = \beta_0 + \beta_1 X$ (modelo linear).
>   - **Modelo 2:**  $f_2(X) = \beta_0 + \beta_1 X + \beta_2 X^2$ (modelo quadr√°tico).
>   - **Modelo 3:** $f_3(X) = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \beta_4 X^4 + \beta_5 X^5 + \beta_6 X^6$ (modelo polinomial de grau 6).
>
> Vamos usar o erro quadr√°tico como fun√ß√£o de perda. Simulando os dados e ajustando os modelos, obtemos (valores hipot√©ticos):
>
> | Modelo     | Erro de Treinamento | Erro de Teste |
> |------------|--------------------|---------------|
> | Modelo 1   |      0.8      |     1.2     |
> | Modelo 2   |      0.5      |      0.8    |
> | Modelo 3   |      0.1      |      3.5    |
>
>Observamos que:
>   - O Modelo 3 (complexo) tem o menor erro de treinamento, mas o maior erro de teste, um claro sinal de *overfitting*. Ele se ajusta perfeitamente aos dados de treinamento mas falha em generalizar para dados n√£o vistos.
>   - O Modelo 1 (simples) tem um erro de treinamento maior mas um erro de teste menor que o Modelo 3, mostrando uma melhor capacidade de generaliza√ß√£o.
>   - O Modelo 2 apresenta um bom balan√ßo entre ajuste e generaliza√ß√£o, com erros de treinamento e teste aceit√°veis.
>
>Este exemplo mostra que o erro de treinamento pode ser enganoso, e √© crucial avaliar o modelo em dados de teste independentes para estimar seu desempenho em dados novos.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
    subgraph "Linear Regression for Classification"
        direction TB
        A["Encode Classes: Indicator Matrix"]
        B["Estimate Coefficients: Least Squares"]
        C["Decision Rule: Apply Threshold"]
        D["Comparison: Probabilistic Methods"]
        A --> B
        B --> C
        C --> D
        subgraph "Limitations"
          E["Extrapolations outside [0,1]"]
          D --> E
        end
        subgraph "Advantages"
          F["Simplicity of Implementation"]
          D --> F
        end
    end
```

A regress√£o linear pode ser usada para classifica√ß√£o, embora com algumas limita√ß√µes [^7.1]. Uma abordagem √© **codificar as classes** usando uma **matriz indicadora**, e realizar uma regress√£o sobre essas indica√ß√µes [^7.2]. Para duas classes, $y_i = 1$ ou $y_i = 0$ podem ser os valores de resposta para o dado $i$. A regress√£o de indicadores trata o problema de classifica√ß√£o como um problema de regress√£o, ajustando um modelo linear aos dados de entrada e depois usando um limiar para tomar decis√µes de classe. Essa abordagem tem a vantagem de ser simples de implementar, mas pode levar a extrapola√ß√µes fora de $[0,1]$ para as probabilidades preditas, o que n√£o ocorre em abordagens probabil√≠sticas como a regress√£o log√≠stica [^7.2].

**Lemma 2:** *Equival√™ncia entre Proje√ß√µes de Regress√£o Linear e Discriminantes Lineares em Condi√ß√µes Espec√≠ficas*.
Em condi√ß√µes de covari√¢ncias iguais entre as classes, os hiperplanos de decis√£o gerados por regress√£o linear em matriz de indicadores se aproximam dos discriminantes lineares. A regress√£o linear ajusta um hiperplano que tenta prever as classes e minimiza a soma dos erros quadr√°ticos. Quando as classes est√£o bem separadas e a covari√¢ncia √© constante entre elas, a regress√£o linear tende a formar fronteiras de decis√£o lineares semelhantes √†quelas obtidas por Linear Discriminant Analysis (LDA) [^7.2].

**Prova do Lemma 2:**
Seja $\mathbf{X}$ a matriz de dados, com $\mathbf{y}$ a vari√°vel de resposta codificada como 0 ou 1. A solu√ß√£o de m√≠nimos quadrados para os coeficientes $\hat{\beta}$ √© dada por $\hat{\beta} = (\mathbf{X^T X})^{-1} \mathbf{X^T y}$. O hiperplano de decis√£o em regress√£o linear √© obtido atrav√©s de um limiar em $ \mathbf{x}^T\hat{\beta}$. Em LDA, os coeficientes s√£o dados por $\hat{\beta}_{LDA} = \Sigma^{-1}(\mu_1-\mu_0)$, onde $\mu_1$ e $\mu_0$ s√£o as m√©dias das classes 1 e 0 e $\Sigma$ √© a matriz de covari√¢ncia comum. Se as covari√¢ncias forem id√™nticas e as classes forem aproximadamente gaussianas com desvios padr√£o similares, a regress√£o linear ir√° ajustar coeficientes que se aproximar√£o dos coeficientes obtidos pelo LDA, projetando os dados em um hiperplano semelhante. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha um conjunto de dados com duas classes, onde temos duas vari√°veis preditoras ($X_1$ e $X_2$). As classes s√£o codificadas como 0 e 1.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Dados de exemplo
> X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9,11], [10,10]])
> y = np.array([0, 0, 1, 1, 0, 1, 1])
>
> # Ajuste do modelo de regress√£o linear
> model = LinearRegression()
> model.fit(X, y)
>
> # Coeficientes obtidos
> beta_0 = model.intercept_
> beta_1 = model.coef_[0]
> beta_2 = model.coef_[1]
>
> print(f"Beta_0: {beta_0:.2f}")
> print(f"Beta_1: {beta_1:.2f}")
> print(f"Beta_2: {beta_2:.2f}")
>
> # C√°lculo do Limiar de Decis√£o (geralmente 0.5)
> def predict(x, threshold=0.5):
>    prediction = beta_0 + beta_1 * x[0] + beta_2 * x[1]
>    return 1 if prediction >= threshold else 0
>
> # Testando com um novo dado
> new_x = np.array([4, 5])
> predicted_class = predict(new_x)
> print(f"Classe predita para {new_x}: {predicted_class}")
>
> ```
>
> Este c√≥digo mostra como podemos usar regress√£o linear para classifica√ß√£o, obtendo os coeficientes do modelo e utilizando um limiar (neste caso, 0.5) para decidir a classe de cada ponto. O limiar transforma o problema de regress√£o em um problema de classifica√ß√£o.
>
> **Interpreta√ß√£o**: Os coeficientes $\beta_0$, $\beta_1$, e $\beta_2$ definem o hiperplano que separa as classes. O sinal e a magnitude dos coeficientes indicam a influ√™ncia de cada vari√°vel na predi√ß√£o da classe. A regress√£o linear, ao minimizar o erro quadr√°tico, encontra um hiperplano que tenta separar as classes da melhor forma poss√≠vel.

**Corol√°rio 2:** A equival√™ncia entre regress√£o linear e LDA sob certas condi√ß√µes simplifica a an√°lise do modelo, pois a regress√£o linear √© mais f√°cil de implementar e calcular do que o LDA, especialmente quando as classes est√£o bem separadas e os dados s√£o aproximadamente gaussianos [^7.3].
Em algumas situa√ß√µes, quando as classes n√£o s√£o bem separadas, o resultado da regress√£o linear para classifica√ß√£o pode n√£o ser √≥timo.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Regularization Techniques"
        direction TB
        A["Variable Selection & Regularization"]
        B["L1 Regularization (Lasso)"]
        C["L2 Regularization (Ridge)"]
        D["Model Complexity Control"]
        E["Logistic Regression Models"]
        F["Bias-Variance Trade-off"]
        A --> D
        B --> D
        C --> D
        B --> E
        C --> E
        D --> F
    end
```

Em modelos de classifica√ß√£o, a sele√ß√£o de vari√°veis e regulariza√ß√£o s√£o t√©cnicas para controlar a complexidade e evitar o *overfitting* [^7.4]. Regulariza√ß√£o, como **penaliza√ß√µes L1 e L2**, s√£o geralmente adicionadas √† fun√ß√£o de perda para penalizar coeficientes grandes e promover modelos mais simples e est√°veis [^7.4.4]. A regulariza√ß√£o L1, por exemplo, induz a esparsidade nos coeficientes, tornando o modelo mais interpret√°vel. Regulariza√ß√£o L2, por outro lado, promove um modelo mais est√°vel, reduzindo a magnitude dos coeficientes [^7.5]. Em modelos log√≠sticos, a regulariza√ß√£o √© adicionada √† log-verossimilhan√ßa [^7.4.4].

**Lemma 3:** *A Penaliza√ß√£o L1 em Regress√£o Log√≠stica Promove a Esparsidade*. A penaliza√ß√£o L1 adiciona a soma dos valores absolutos dos coeficientes √† fun√ß√£o de perda, o que for√ßa alguns coeficientes a serem exatamente zero, resultando em modelos mais esparsos e menos complexos [^7.4.4].

**Prova do Lemma 3:**
A regress√£o log√≠stica com penaliza√ß√£o L1 busca minimizar a fun√ß√£o de custo:
$$J(\beta) = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(\sigma(\beta^Tx_i)) + (1-y_i)\log(1-\sigma(\beta^Tx_i))] + \lambda ||\beta||_1$$
onde $\sigma(z) = \frac{1}{1+e^{-z}}$, $y_i \in \{0,1\}$ e $\lambda$ √© o par√¢metro de regulariza√ß√£o. A penalidade $L1$, $\lambda ||\beta||_1 = \lambda \sum_j |\beta_j|$,  n√£o √© diferenci√°vel em $\beta_j = 0$, o que gera uma solu√ß√£o esparsa. Ou seja, durante a otimiza√ß√£o, muitos coeficientes $\beta_j$ s√£o for√ßados a serem exatamente zero para minimizar a fun√ß√£o de custo. A intui√ß√£o √© que a penalidade $L1$ reduz coeficientes n√£o essenciais a zero mais rapidamente que $L2$, pois possui derivada constante, levando a um modelo esparso com apenas as vari√°veis mais relevantes. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos demonstrar o efeito da regulariza√ß√£o L1 (Lasso) na regress√£o log√≠stica usando um exemplo com 5 vari√°veis preditoras:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LogisticRegression
> from sklearn.preprocessing import StandardScaler
>
> # Dados de exemplo com 5 vari√°veis preditoras
> np.random.seed(42)
> X = np.random.randn(100, 5)
> y = np.random.randint(0, 2, 100)
>
> # Padroniza√ß√£o das vari√°veis
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)
>
> # Modelo sem regulariza√ß√£o
> model_no_reg = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)
> model_no_reg.fit(X_scaled, y)
>
> # Modelo com regulariza√ß√£o L1 (Lasso)
> model_l1_reg = LogisticRegression(penalty='l1', C=0.1, solver='liblinear', max_iter=1000)
> model_l1_reg.fit(X_scaled, y)
>
> # Coeficientes
> coef_no_reg = model_no_reg.coef_[0]
> coef_l1_reg = model_l1_reg.coef_[0]
>
> print("Coeficientes sem regulariza√ß√£o:", coef_no_reg)
> print("Coeficientes com regulariza√ß√£o L1:", coef_l1_reg)
>
> # Comparando a quantidade de coeficientes n√£o nulos
> non_zero_no_reg = np.sum(coef_no_reg != 0)
> non_zero_l1_reg = np.sum(coef_l1_reg != 0)
> print(f"N√∫mero de coeficientes n√£o nulos sem regulariza√ß√£o: {non_zero_no_reg}")
> print(f"N√∫mero de coeficientes n√£o nulos com regulariza√ß√£o L1: {non_zero_l1_reg}")
> ```
>
> Neste exemplo, observamos que a regulariza√ß√£o L1 (Lasso) zera alguns coeficientes, resultando em um modelo mais esparso. Isso ocorre porque a penalidade L1 adiciona o valor absoluto dos coeficientes √† fun√ß√£o de custo, o que incentiva a que alguns coeficientes sejam zero durante o processo de otimiza√ß√£o.
>
> **Interpreta√ß√£o**: Modelos com regulariza√ß√£o L1 tendem a usar um n√∫mero menor de vari√°veis para fazer previs√µes, resultando em modelos mais interpret√°veis e com menor risco de *overfitting*. O par√¢metro `C` controla a for√ßa da regulariza√ß√£o, sendo um par√¢metro de ajuste.

**Corol√°rio 3:** A esparsidade induzida pela penaliza√ß√£o L1 aumenta a interpretabilidade do modelo, pois as vari√°veis com coeficientes n√£o nulos s√£o as mais relevantes para a predi√ß√£o [^7.4.5]. A regulariza√ß√£o L1 e L2 podem ser combinadas, resultando na regulariza√ß√£o *Elastic Net*, que busca as vantagens de ambas as regulariza√ß√µes [^7.5].

> ‚ö†Ô∏è **Ponto Crucial**: A escolha entre L1, L2 ou Elastic Net depende do problema. L1 induz esparsidade, L2 estabiliza, e Elastic Net √© um compromisso.

### Separating Hyperplanes e Perceptrons

A ideia de **hiperplanos separadores** surge da busca por maximizar a margem de separa√ß√£o entre classes, com o objetivo de construir fronteiras lineares √≥timas [^7.5.2]. Esse problema pode ser formulado como um problema de otimiza√ß√£o, onde se busca o hiperplano que melhor separa as classes, maximizando a margem entre elas e minimizando os erros de classifica√ß√£o. O **Perceptron de Rosenblatt** √© um algoritmo que aprende esses hiperplanos [^7.5.1]. Ele converge para uma solu√ß√£o √≥tima quando os dados s√£o linearmente separ√°veis.

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

**Resposta:**
A **Linear Discriminant Analysis (LDA)** assume que as classes seguem distribui√ß√µes gaussianas com a mesma matriz de covari√¢ncia. A regra de decis√£o Bayesiana, por sua vez, busca classificar um ponto para a classe com maior probabilidade a posteriori. Quando as classes seguem distribui√ß√µes gaussianas com covari√¢ncias iguais, as fronteiras de decis√£o resultantes de LDA s√£o lineares e as probabilidades a posteriori derivadas da regra de Bayes levam a um classificador linear [^7.3].

```mermaid
graph LR
    subgraph "LDA vs Bayesian Decision Rule"
        direction TB
        A["LDA Assumption: Gaussian distributions with equal covariances"]
        B["Bayesian Decision Rule: Maximize posterior probability"]
        C["Equal Covariances: Linear Decision Boundary"]
        D["Bayes and LDA equivalence"]
         A --> C
        B --> C
        C --> D
    end
```

**Lemma 4:** *Equival√™ncia entre LDA e Regra de Decis√£o Bayesiana sob certas suposi√ß√µes*. Se assumirmos que as classes seguem distribui√ß√µes gaussianas com covari√¢ncias iguais, ent√£o a regra de decis√£o Bayesiana se reduz a uma fun√ß√£o discriminante linear, semelhante √† fun√ß√£o discriminante do LDA [^7.3, 7.3.3].

**Prova do Lemma 4:**
Seja $p(x|G=k)$ a densidade gaussiana da classe $k$, com m√©dia $\mu_k$ e covari√¢ncia comum $\Sigma$, e seja $p(G=k) = \pi_k$ a probabilidade a priori da classe $k$. A regra de Bayes √© classificar $x$ na classe que maximiza $p(G=k|x) \propto p(x|G=k) p(G=k)$. Como as densidades s√£o gaussianas, temos que:
$$p(x|G=k) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} exp(-\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k))$$
Ao tomarmos o logaritmo e desconsiderarmos os termos constantes, temos:
$$log p(G=k|x) \propto -\frac{1}{2}x^T\Sigma^{-1}x + x^T\Sigma^{-1}\mu_k -\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + log \pi_k$$
O termo $\frac{1}{2}x^T\Sigma^{-1}x$ √© comum a todas as classes e pode ser desprezado, o que leva a uma fun√ß√£o discriminante linear do tipo:
$$\delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + log \pi_k$$
Esta √© a mesma fun√ß√£o discriminante do LDA. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar a equival√™ncia entre LDA e a regra de decis√£o Bayesiana com distribui√ß√µes Gaussianas com covari√¢ncias iguais, vamos considerar um exemplo bidimensional com duas classes. Suponha que as classes tenham as seguintes caracter√≠sticas:
>
> - Classe 0: M√©dia $\mu_0 = [1, 1]$, Covari√¢ncia $\Sigma = [[1, 0], [0, 1]]$
> - Classe 1: M√©dia $\mu_1 = [3, 3]$, Covari√¢ncia $\Sigma = [[1, 0], [0, 1]]$
>
> Usando a regra de decis√£o Bayesiana:
>
> 1.  Calculamos a probabilidade a posteriori para cada classe dado um ponto $x$
>
>  $$p(G=k|x) \propto p(x|G=k) p(G=k)$$
>
> 2.  Assumindo probabilidades a priori iguais, $p(G=0)=p(G=1) = 0.5$, a decis√£o se resume a:
>  Classificar $x$ na classe $k$ se $p(x|G=k)$ √© a maior. Ou, equivalentemente, classificar em $k$ se $\log p(x|G=k)$ √© maior.
>
> 3. Como a densidade gaussiana √©:
> $$p(x|G=k) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} exp(-\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k))$$
> E como $\Sigma$ √© a mesma para as duas classes, $\log p(x|G=k)$ se simplifica para
>
> $$  \log p(x|G=k) \propto  -\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) =  x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k  $$
>
> 4. A fronteira de decis√£o entre as duas classes √© quando
> $$ x^T\Sigma^{-1}\mu_0 - \frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0 = x^T\Sigma^{-1}\mu_1 - \frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1 $$
>
> Que se simplifica em:
>
> $$x^T\Sigma^{-1}(\mu_0 - \mu_1) = \frac{1}{2}(\mu_0^T\Sigma^{-1}\mu_0 - \mu_1^T\Sigma^{-1}\mu_1)$$
>
> Usando os valores num√©ricos, podemos calcular a equa√ß√£o da reta que separa as duas classes. Este resultado √© id√™ntico √† fronteira de decis√£o encontrada pelo LDA, que √© um discriminante linear com coeficientes proporcionais a $\Sigma^{-1}(\mu_1-\mu_0)$.
>
> O ponto essencial √© que, com covari√¢ncias iguais, o logaritmo da verossimilhan√ßa das classes se reduz a uma fun√ß√£o linear, o que nos leva a um classificador linear. A regra de decis√£o Bayesiana e o LDA, portanto, resultam na mesma fronteira de decis√£o linear.

**Corol√°rio 4:** Se as covari√¢ncias das classes n√£o forem iguais, a fronteira de decis√£o n√£o ser√° mais linear. Nesse caso, temos o Quadratic Discriminant Analysis (QDA), que considera covari√¢ncias distintas, resultando em fronteiras quadr√°ticas [^7.3].

> ‚ö†Ô∏è **Ponto Crucial**: A igualdade das matrizes de covari√¢ncia define se a fronteira de decis√£o ser√° linear ou quadr√°tica.

### Conclus√£o

Este cap√≠tulo apresentou uma vis√£o detalhada dos conceitos fundamentais de avalia√ß√£o e sele√ß√£o de modelos, com foco em entropia, vi√©s, vari√¢ncia, e complexidade de modelos. Exploramos as rela√ß√µes entre regress√£o linear e discriminantes lineares, m√©todos de regulariza√ß√£o, hiperplanos separadores e perceptrons, e aprofundamos a discuss√£o sobre a entropia e perda log-verossimilhan√ßa.