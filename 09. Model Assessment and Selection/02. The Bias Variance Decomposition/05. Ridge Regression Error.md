## Avalia√ß√£o e Sele√ß√£o de Modelos: Foco no Erro da Ridge Regression

<imagem: Um diagrama complexo que ilustra o tradeoff bias-vari√¢ncia, mostrando curvas de erro de treinamento e teste em rela√ß√£o √† complexidade do modelo para Ridge Regression, destacando o ponto de √≥timo desempenho.>

### Introdu√ß√£o

A capacidade de generaliza√ß√£o de um m√©todo de aprendizado refere-se √† sua habilidade de realizar previs√µes precisas em dados de teste independentes. A avalia√ß√£o desse desempenho √© crucial na pr√°tica, pois orienta a escolha do m√©todo ou modelo de aprendizado e fornece uma medida da qualidade do modelo escolhido [^7.1]. Este cap√≠tulo aborda os principais m√©todos para avalia√ß√£o de desempenho, mostrando como eles s√£o utilizados na sele√ß√£o de modelos, com foco na an√°lise do erro da **Ridge Regression** e sua rela√ß√£o com **vi√©s, vari√¢ncia e complexidade do modelo** [^7.1].

### Conceitos Fundamentais

**Conceito 1:** O **problema de classifica√ß√£o** (ou regress√£o) busca construir um modelo $f(X)$ que aproxime a vari√°vel resposta $Y$ com base nas vari√°veis de entrada $X$. M√©todos lineares, como a regress√£o linear e a Ridge Regression, s√£o frequentemente empregados, mas est√£o sujeitos a um tradeoff entre vi√©s e vari√¢ncia. Modelos mais simples (baixa complexidade) tendem a ter alto vi√©s e baixa vari√¢ncia, enquanto modelos mais complexos (alta complexidade) tendem a ter baixo vi√©s e alta vari√¢ncia. O objetivo √© encontrar a complexidade ideal que minimize o erro de generaliza√ß√£o [^7.2]. A escolha de um modelo linear implica em um vi√©s inicial (modelo estruturalmente restrito) mas pode levar a menor vari√¢ncia, dada uma quantidade de dados finita.

> üí° **Exemplo Num√©rico:** Imagine que estamos tentando ajustar um modelo para prever o pre√ßo de casas (`Y`) com base no tamanho em metros quadrados (`X`).
>
> 1.  **Modelo Simples (Alto Vi√©s, Baixa Vari√¢ncia):** Usamos um modelo linear simples, como $Y = \beta_0 + \beta_1X$. Esse modelo pode n√£o capturar nuances na rela√ß√£o entre tamanho e pre√ßo (alto vi√©s), mas seus coeficientes $\beta_0$ e $\beta_1$ s√£o relativamente est√°veis com mudan√ßas nos dados de treinamento (baixa vari√¢ncia).
> 2.  **Modelo Complexo (Baixo Vi√©s, Alta Vari√¢ncia):** Usamos um modelo polinomial de grau 5, como $Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \beta_4X^4 + \beta_5X^5$. Esse modelo pode se ajustar muito bem aos dados de treinamento (baixo vi√©s), mas seus coeficientes s√£o sens√≠veis a pequenas mudan√ßas nos dados, levando a resultados diferentes em novos conjuntos de dados (alta vari√¢ncia).
> 3.  **Objetivo:** O objetivo √© encontrar um modelo com complexidade intermedi√°ria, que minimize o erro tanto nos dados de treinamento quanto nos dados de teste. Em outras palavras, um modelo que generalize bem.

**Lemma 1:** Em modelos de regress√£o linear, o erro de previs√£o pode ser decomposto em tr√™s componentes: o erro irredut√≠vel, o vi√©s ao quadrado e a vari√¢ncia. A **decomposi√ß√£o vi√©s-vari√¢ncia** √© fundamental para entender como diferentes modelos se comportam em rela√ß√£o ao overfitting e underfitting [^7.3].

```mermaid
graph TD
    subgraph "Bias-Variance Decomposition"
        direction TB
        A["Total Error (Err(x‚ÇÄ))"]
        B["Irreducible Error (œÉ¬≤)"]
        C["Bias¬≤ Component: (E[fÃÇ(x‚ÇÄ)] - f(x‚ÇÄ))¬≤"]
        D["Variance Component: E[(fÃÇ(x‚ÇÄ) - E[fÃÇ(x‚ÇÄ)])¬≤]"]
        A --> B
        A --> C
        A --> D
    end
```

$$
Err(x_0) = \sigma^2 + [Ef(x_0) - f(x_0)]^2 + E[f(x_0) - Ef(x_0)]^2
$$

Onde:

-   $\sigma^2$ √© o erro irredut√≠vel.
-   $[Ef(x_0) - f(x_0)]^2$ √© o vi√©s ao quadrado.
-   $E[f(x_0) - Ef(x_0)]^2$ √© a vari√¢ncia.

[^7.3]

> üí° **Exemplo Num√©rico:** Vamos considerar um cen√°rio onde o verdadeiro modelo √© $f(x) = 2x$ e temos um conjunto de dados ruidoso $y_i = 2x_i + \epsilon_i$, com $\epsilon_i \sim N(0, 0.5^2)$.  Vamos ajustar dois modelos:  um linear $f_1(x) = \beta_0 + \beta_1x$ e outro  mais simples $f_2(x) = \beta_1x$.  Simulando 100 conjuntos de dados de treinamento, podemos calcular o erro m√©dio de previs√£o em $x_0 = 1$.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> np.random.seed(42)
> n_simulations = 100
> x0 = 1
> true_f_x0 = 2 * x0
> sigma2 = 0.5**2
>
> # Simulate data
> def simulate_data(n_samples):
>     x = np.sort(np.random.rand(n_samples) * 2)
>     epsilon = np.random.normal(0, np.sqrt(sigma2), n_samples)
>     y = 2 * x + epsilon
>     return x.reshape(-1, 1), y
>
> bias_squared_f1 = 0
> variance_f1 = 0
> bias_squared_f2 = 0
> variance_f2 = 0
>
> f1_predictions = []
> f2_predictions = []
>
> for _ in range(n_simulations):
>     x_train, y_train = simulate_data(20)
>
>     # Fit model f1
>     model_f1 = LinearRegression()
>     model_f1.fit(x_train, y_train)
>     f1_x0_pred = model_f1.predict([[x0]])[0]
>     f1_predictions.append(f1_x0_pred)
>
>    # Fit model f2 (without intercept)
>     model_f2 = LinearRegression(fit_intercept=False)
>     model_f2.fit(x_train, y_train)
>     f2_x0_pred = model_f2.predict([[x0]])[0]
>     f2_predictions.append(f2_x0_pred)
>
>
> # Calculate components
> Ef1_x0 = np.mean(f1_predictions)
> Ef2_x0 = np.mean(f2_predictions)
>
> bias_squared_f1 = (Ef1_x0 - true_f_x0)**2
> variance_f1 = np.var(f1_predictions)
>
> bias_squared_f2 = (Ef2_x0 - true_f_x0)**2
> variance_f2 = np.var(f2_predictions)
>
> print(f"Model f1 (Linear with intercept):")
> print(f"  Bias¬≤: {bias_squared_f1:.4f}")
> print(f"  Variance: {variance_f1:.4f}")
> print(f"  Total Error: {bias_squared_f1 + variance_f1 + sigma2:.4f}")
>
> print(f"\nModel f2 (Linear without intercept):")
> print(f"  Bias¬≤: {bias_squared_f2:.4f}")
> print(f"  Variance: {variance_f2:.4f}")
> print(f"  Total Error: {bias_squared_f2 + variance_f2 + sigma2:.4f}")
>
> # Plotting predictions
> plt.figure(figsize=(10,5))
> plt.subplot(1,2,1)
> plt.hist(f1_predictions, bins=20, alpha=0.7, label='Model f1 Predictions')
> plt.axvline(Ef1_x0, color='red', linestyle='dashed', linewidth=1, label='Average Prediction')
> plt.axvline(true_f_x0, color='green', linestyle='dashed', linewidth=1, label='True value')
> plt.title('Model f1 Predictions')
> plt.legend()
>
> plt.subplot(1,2,2)
> plt.hist(f2_predictions, bins=20, alpha=0.7, label='Model f2 Predictions')
> plt.axvline(Ef2_x0, color='red', linestyle='dashed', linewidth=1, label='Average Prediction')
> plt.axvline(true_f_x0, color='green', linestyle='dashed', linewidth=1, label='True value')
> plt.title('Model f2 Predictions')
> plt.legend()
> plt.tight_layout()
> plt.show()
>
> ```
>
> **Interpreta√ß√£o:**
>
>  *   **Modelo f1 (Linear com intercepto):** Tem um vi√©s menor (o valor m√©dio da predi√ß√£o est√° mais perto do valor verdadeiro), e tamb√©m menor vari√¢ncia, com  previs√µes mais concentradas.
> *  **Modelo f2 (Linear sem intercepto):** Tem um vi√©s maior (o valor m√©dio da predi√ß√£o est√° mais longe do valor verdadeiro), mas previs√µes com uma vari√¢ncia menor que modelo f1.
> *  **Erro Irredut√≠vel:** $\sigma^2 = 0.25$ √© constante para ambos os modelos e representa a variabilidade intr√≠nseca dos dados.
> *  A decomposi√ß√£o vi√©s-vari√¢ncia nos permite entender que o modelo f1 tem um erro total menor.

**Conceito 2:** A **Linear Discriminant Analysis (LDA)** busca encontrar a melhor proje√ß√£o linear dos dados para classificar diferentes classes, assumindo que os dados seguem uma distribui√ß√£o normal com a mesma matriz de covari√¢ncia para todas as classes [^4.3]. Embora a LDA seja um m√©todo de classifica√ß√£o, sua formula√ß√£o tamb√©m est√° relacionada a conceitos de regress√£o e proje√ß√µes lineares. Em especial, quando assumimos que os dados seguem uma distribui√ß√£o normal, a fronteira de decis√£o √© dada pela igualdade das fun√ß√µes discriminantes lineares [^4.3.1]. A LDA busca projetar os dados em um subespa√ßo de menor dimens√£o que maximize a separa√ß√£o entre classes.

**Corol√°rio 1:** A fun√ß√£o discriminante linear da LDA √© derivada da diferen√ßa entre fun√ß√µes de densidade gaussianas, cujos par√¢metros s√£o estimativas da m√©dia e da covari√¢ncia para cada classe. A fronteira de decis√£o √© dada pelo conjunto de pontos onde a probabilidade de pertencer a ambas as classes √© igual, o que equivale a fazer a diferen√ßa entre essas fun√ß√µes discriminantes [^4.3.3]. A rela√ß√£o com proje√ß√µes lineares √© que o discriminante linear representa a proje√ß√£o em um eixo que melhor separa as classes.

> üí° **Exemplo Num√©rico:** Suponha que temos duas classes de dados bidimensionais ($X_1, X_2$), com as seguintes m√©dias e covari√¢ncia comum:
> - Classe 1: $\mu_1 = [1, 1]$,
> - Classe 2: $\mu_2 = [2, 2]$,
> - Covari√¢ncia: $\Sigma = \begin{bmatrix} 0.5 & 0 \\ 0 & 0.5 \end{bmatrix}$
>
> A fun√ß√£o discriminante para a classe 1 √©:
> $\delta_1(x) = x^T \Sigma^{-1} \mu_1 - \frac{1}{2} \mu_1^T \Sigma^{-1} \mu_1 + \ln \pi_1$
> e para a classe 2 √©:
> $\delta_2(x) = x^T \Sigma^{-1} \mu_2 - \frac{1}{2} \mu_2^T \Sigma^{-1} \mu_2 + \ln \pi_2$
>
> Assumindo probabilidades a priori iguais ($\pi_1 = \pi_2 = 0.5$), a fronteira de decis√£o √© dada por $\delta_1(x) = \delta_2(x)$.  Nesse caso, a fronteira √© uma linha reta que separa as duas classes.  Um ponto ser√° classificado na classe 1 se $\delta_1(x) > \delta_2(x)$ ou na classe 2 caso contr√°rio.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros das classes
> mu1 = np.array([1, 1])
> mu2 = np.array([2, 2])
> sigma = np.array([[0.5, 0], [0, 0.5]])
> sigma_inv = np.linalg.inv(sigma)
> pi1 = 0.5
> pi2 = 0.5
>
> # Define a fun√ß√£o discriminante
> def delta(x, mu, sigma_inv, pi):
>     return x @ sigma_inv @ mu - 0.5 * mu @ sigma_inv @ mu + np.log(pi)
>
> # Gera os dados
> np.random.seed(42)
> X1 = np.random.multivariate_normal(mu1, sigma, 100)
> X2 = np.random.multivariate_normal(mu2, sigma, 100)
>
> # Gera pontos para visualiza√ß√£o da fronteira
> x_min, x_max = 0, 3
> y_min, y_max = 0, 3
> xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
> grid_points = np.c_[xx.ravel(), yy.ravel()]
>
> # Calcula os discriminantes
> delta1_grid = np.array([delta(x, mu1, sigma_inv, pi1) for x in grid_points])
> delta2_grid = np.array([delta(x, mu2, sigma_inv, pi2) for x in grid_points])
>
> # Calcula as classes preditas
> predicted_classes = np.where(delta1_grid > delta2_grid, 1, 2)
> predicted_classes = predicted_classes.reshape(xx.shape)
>
> # Plota os dados e a fronteira
> plt.figure(figsize=(8, 6))
> plt.scatter(X1[:, 0], X1[:, 1], color='blue', label='Classe 1')
> plt.scatter(X2[:, 0], X2[:, 1], color='red', label='Classe 2')
> plt.contourf(xx, yy, predicted_classes, levels=[0.5, 1.5, 2.5], colors=['lightblue', 'lightcoral'], alpha=0.4)
> plt.xlabel('X1')
> plt.ylabel('X2')
> plt.title('LDA: Fronteira de Decis√£o')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> **Interpreta√ß√£o:** A visualiza√ß√£o mostra a fronteira linear entre as duas classes, destacando que a LDA projeta os dados em um espa√ßo que maximiza a separa√ß√£o. Os pontos azuis e vermelhos representam as duas classes, e a regi√£o entre eles √© a fronteira de decis√£o, que √© uma linha reta.

```mermaid
graph LR
    subgraph "LDA Discriminant Functions"
        direction LR
        A["Input Data (x)"]
        B["Class 1 Parameters (Œº‚ÇÅ, Œ£, œÄ‚ÇÅ)"]
        C["Class 2 Parameters (Œº‚ÇÇ, Œ£, œÄ‚ÇÇ)"]
        D["Discriminant Œ¥‚ÇÅ(x)"]
        E["Discriminant Œ¥‚ÇÇ(x)"]
        F["Decision Boundary: Œ¥‚ÇÅ(x) = Œ¥‚ÇÇ(x)"]
        A & B --> D
        A & C --> E
        D & E --> F
    end
```

**Conceito 3:** A **Logistic Regression** modela a probabilidade de um resultado bin√°rio usando a fun√ß√£o log√≠stica, que mapeia uma combina√ß√£o linear das vari√°veis de entrada para um valor entre 0 e 1 [^4.4]. O logit da probabilidade, que √© a transforma√ß√£o log-odds, √© uma fun√ß√£o linear das vari√°veis de entrada. Os par√¢metros do modelo s√£o estimados por maximiza√ß√£o da fun√ß√£o de verossimilhan√ßa. Diferentemente da LDA que assume gaussianidade, a regress√£o log√≠stica n√£o faz essa suposi√ß√£o sobre as distribui√ß√µes das vari√°veis preditoras, tornando-a mais flex√≠vel em alguns cen√°rios. O logit da probabilidade, representado por $ln(p(x)/(1-p(x)))$, pode ser modelado linearmente [^4.4.1]. A fun√ß√£o de verossimilhan√ßa √© dada por $L(\beta) = \sum_{i} y_i ln(p(x_i)) + (1-y_i)ln(1-p(x_i))$ [^4.4.4]

> ‚ö†Ô∏è **Nota Importante**: A Logistic Regression, ao modelar probabilidades, pode ser mais adequada em situa√ß√µes onde se quer obter uma estimativa da probabilidade de um resultado, diferentemente da LDA que foca na separa√ß√£o de classes [^4.4.1].
> ‚ùó **Ponto de Aten√ß√£o**: Em classes desbalanceadas, a Logistic Regression pode ser mais robusta do que a regress√£o linear direta em matriz de indicadores, pois otimiza um crit√©rio de verossimilhan√ßa mais adequado [^4.4.2].
> ‚úîÔ∏è **Destaque**: Apesar de terem formula√ß√µes diferentes, os par√¢metros estimados por LDA e por Logistic Regression podem ter correla√ß√µes em alguns contextos, especialmente quando a distribui√ß√£o das classes se aproxima de gaussianas com a mesma matriz de covari√¢ncia [^4.5].

> üí° **Exemplo Num√©rico:** Vamos considerar um problema de classifica√ß√£o bin√°ria onde queremos prever se um cliente far√° uma compra (`Y=1`) ou n√£o (`Y=0`) com base na sua idade (`X`).
>
> 1.  **Dados:** Temos 100 clientes, e os dados podem ser simulados da seguinte forma:
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
> import seaborn as sns
> from sklearn.linear_model import LogisticRegression
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import confusion_matrix, accuracy_score
>
> np.random.seed(42)
> n_samples = 100
> ages = np.random.randint(20, 70, n_samples)
>
> # Simulando a probabilidade de compra baseada na idade
> probabilities = 1 / (1 + np.exp(-0.1 * (ages - 45)))  # Fun√ß√£o log√≠stica
>
> # Gerando os r√≥tulos (compra ou n√£o)
> purchases = np.random.binomial(1, probabilities)
>
> # Criando um dataframe pandas
> data = pd.DataFrame({'Age': ages, 'Purchase': purchases})
>
> # Scatter plot
> plt.figure(figsize=(8, 5))
> sns.scatterplot(x='Age', y='Purchase', data=data, alpha=0.6)
> plt.title('Compra vs Idade')
> plt.xlabel('Idade')
> plt.ylabel('Compra (1=Sim, 0=N√£o)')
> plt.show()
>
> # Separa os dados em treinamento e teste
> X_train, X_test, y_train, y_test = train_test_split(data[['Age']], data['Purchase'], test_size=0.2, random_state=42)
>
> # Treina o modelo de regress√£o log√≠stica
> model = LogisticRegression(random_state=42)
> model.fit(X_train, y_train)
>
> # Predi√ß√µes nos dados de teste
> y_pred = model.predict(X_test)
> y_prob = model.predict_proba(X_test)[:, 1] # Probabilidade da classe 1
>
> # Matriz de confus√£o
> cm = confusion_matrix(y_test, y_pred)
> print("Matriz de confus√£o:")
> print(cm)
>
> # Acur√°cia
> accuracy = accuracy_score(y_test, y_pred)
> print(f"\nAcur√°cia: {accuracy:.2f}")
>
> # Plotando a curva log√≠stica
> ages_range = np.linspace(data['Age'].min() - 5, data['Age'].max() + 5, 100).reshape(-1, 1)
> probs_range = model.predict_proba(ages_range)[:, 1]
>
> plt.figure(figsize=(8, 5))
> sns.scatterplot(x='Age', y='Purchase', data=data, alpha=0.6, label='Dados reais')
> plt.plot(ages_range, probs_range, color='red', label='Curva Log√≠stica')
> plt.title('Regress√£o Log√≠stica: Probabilidade de Compra vs Idade')
> plt.xlabel('Idade')
> plt.ylabel('Probabilidade de Compra')
> plt.legend()
> plt.show()
>
> # Imprimindo os coeficientes
> print("\nCoeficientes:")
> print(f"  Intercepto (beta_0): {model.intercept_[0]:.4f}")
> print(f"  Coeficiente da idade (beta_1): {model.coef_[0][0]:.4f}")
>
> ```
>
> **Interpreta√ß√£o:**
> *   A matriz de confus√£o mostra quantos exemplos foram corretamente e incorretamente classificados.
> *   A acur√°cia √© uma medida do desempenho geral do modelo.
> *   A curva log√≠stica representa a probabilidade estimada de compra em fun√ß√£o da idade.
> *   O intercepto e o coeficiente da idade indicam a influ√™ncia da idade na probabilidade de compra. O coeficiente positivo indica que, em geral, a probabilidade de compra aumenta com a idade.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
flowchart TD
    subgraph "Linear Regression for Classification"
        A["Input Data (X)"] --> B["Encode Classes (Indicator Matrix)"]
        B --> C["Estimate Coefficients (Œ≤) via Least Squares"]
        C --> D["Predict Class by Maximum Predicted Value"]
        D --> E["Limitations and Bias"]
    end
```

A regress√£o linear pode ser aplicada para classifica√ß√£o usando uma matriz de indicadores, onde cada coluna representa uma classe e cada linha representa um exemplo. Uma vari√°vel de resposta $Y$ √© codificada usando 1 para a classe pertencente e 0 para as demais.  Os coeficientes da regress√£o s√£o estimados via m√≠nimos quadrados e um novo ponto √© classificado atribuindo-o √† classe cujo indicador tiver o maior valor predito [^4.2]. No entanto, esse m√©todo tem limita√ß√µes. A regress√£o linear n√£o modela diretamente probabilidades e pode levar a valores preditos fora do intervalo \[0,1], al√©m de poder apresentar comportamento inadequado em certas distribui√ß√µes de classes [^4.1], [^4.2].

**Lemma 2:**  Em certas condi√ß√µes, as proje√ß√µes nos hiperplanos de decis√£o gerados pela regress√£o linear de indicadores podem ser equivalentes √†s proje√ß√µes geradas por discriminantes lineares, no sentido de produzir uma fronteira de decis√£o similar [^4.2]. Este resultado depende de uma correta codifica√ß√£o das classes e de uma distribui√ß√£o espec√≠fica para os dados.

**Corol√°rio 2:** Se a codifica√ß√£o das classes for feita de tal forma que cada classe seja representada por um vetor linearmente independente, a regress√£o linear na matriz de indicadores se comportar√° de forma similar a uma an√°lise discriminante. Este corol√°rio destaca uma conex√£o interessante entre abordagens aparentemente distintas [^4.3].

Em cen√°rios com classes desbalanceadas, a regress√£o log√≠stica geralmente oferece estimativas de probabilidade mais est√°veis, enquanto a regress√£o de indicadores pode apresentar extrapola√ß√µes fora do intervalo \[0,1]. Entretanto, em situa√ß√µes onde o foco principal √© a fronteira de decis√£o linear, a regress√£o de indicadores pode ser suficiente [^4.4], [^4.2].

> üí° **Exemplo Num√©rico:** Suponha que temos tr√™s classes (A, B, e C) e dados com duas caracter√≠sticas, $X_1$ e $X_2$. Vamos criar uma matriz de indicadores e aplicar a regress√£o linear para classifica√ß√£o.
>
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> # Dados de exemplo
> np.random.seed(42)
> n_samples = 100
>
> # Gerando dados para as classes
> X_A = np.random.multivariate_normal([1, 1], [[0.3, 0], [0, 0.3]], n_samples//3)
> X_B = np.random.multivariate_normal([3, 3], [[0.4, 0], [0, 0.4]], n_samples//3)
> X_C = np.random.multivariate_normal([1, 4], [[0.3, 0], [0, 0.3]], n_samples-2*(n_samples//3))
>
> X = np.concatenate((X_A, X_B, X_C))
>
> # Criando os r√≥tulos
> y_A = np.zeros(len(X_A))
> y_B = np.ones(len(X_B))
> y_C = np.full(len(X_C),2)
>
> y = np.concatenate((y_A, y_B, y_C))
>
> # Cria√ß√£o da matriz de indicadores
> y_encoded = pd.get_dummies(y).values # converte para one-hot encoding
>
> # Ajuste do modelo de regress√£o linear
> model = LinearRegression()
> model.fit(X, y_encoded)
>
> # Predi√ß√µes
> predictions = model.predict(X)
> predicted_classes = np.argmax(predictions, axis=1)
>
> # Plotting the results
>
> # Create meshgrid for the decision boundaries
> x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
> y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
> Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
> Z = np.argmax(Z, axis=1)
> Z = Z.reshape(xx.shape)
>
> # Generate colors for the classes
> colors = ['blue', 'red', 'green']
>
> # Create a scatter plot with colored points and class boundaries
> plt.figure(figsize=(8, 6))
> plt.contourf(xx, yy, Z, alpha=0.2, cmap=plt.cm.get_cmap('viridis', 3))
> scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', 3))
>
> # Add a legend
> class_labels = ['Class A', 'Class B', 'Class C']
> plt.legend(handles=scatter.legend_elements()[0], labels=class_labels)
>
> plt.xlabel('X1')
> plt.ylabel('X2')
> plt.title('Regress√£o Linear de Indicadores para Classifica√ß√£o')
> plt.grid(True)
> plt.show()
>
> # Mostrando os resultados
> print("Matriz de indicadores:")
> print(pd.get_dummies(y).head())
>
> print("\nPrimeiras 5 predi√ß√µes (valores n√£o normalizados):")
> print(predictions[:5])
>
> print("\nClasses preditas para os 5 primeiros pontos:")
> print(predicted_classes[:5])
>
>
> ```
>
> **Interpreta√ß√£o:**
>
> *   A matriz de indicadores transforma os r√≥tulos de classe em vetores bin√°rios.
> *   A regress√£o linear estima os coeficientes que melhor separam as classes.
> *   A visualiza√ß√£o mostra as regi√µes de decis√£o criadas pelo modelo linear.
> *   Os valores preditos (n√£o normalizados) indicam a proximidade a cada classe.
> *   Ao escolher a classe com maior valor predito para cada observa√ß√£o, fazemos a classifica√ß√£o.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

<imagem: Mapa mental mostrando a interconex√£o entre regulariza√ß√£o L1 e L2, m√©todos de sele√ß√£o de vari√°veis (forward, backward), e sua aplica√ß√£o em LDA e Logistic Regression para controle da complexidade do modelo e aumento da interpretabilidade.>

A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o t√©cnicas importantes para lidar com modelos de alta complexidade.  A regulariza√ß√£o introduz termos de penalidade na fun√ß√£o de custo, buscando modelos mais simples e robustos. A regulariza√ß√£o L1 ($||\beta||_1$) promove a esparsidade dos coeficientes, isto √©, leva a um modelo com poucos preditores relevantes, enquanto a regulariza√ß√£o L2 ($||\beta||_2^2$) reduz a magnitude dos coeficientes, evitando overfitting. A escolha de qual tipo de regulariza√ß√£o depende das caracter√≠sticas dos dados e do objetivo do modelador [^4.5], [^4.4.4].  A regulariza√ß√£o L1 tamb√©m √© conhecida como penaliza√ß√£o LASSO, enquanto a L2 √© conhecida como penaliza√ß√£o Ridge.

**Lemma 3:** A penaliza√ß√£o L1 em regress√£o log√≠stica leva a coeficientes esparsos devido √† sua geometria, que favorece solu√ß√µes com muitos coeficientes iguais a zero. Este comportamento facilita a interpreta√ß√£o dos modelos, pois seleciona um subconjunto de vari√°veis com maior poder preditivo [^4.4.4].

**Prova do Lemma 3:**
A penaliza√ß√£o L1 adiciona o termo $\lambda||\beta||_1 = \lambda \sum_{j}|\beta_j|$ √† fun√ß√£o de custo. A n√£o diferenciabilidade da fun√ß√£o de valor absoluto em 0 for√ßa as solu√ß√µes √≥timas a se localizarem nos eixos coordenados, promovendo esparsidade. A otimiza√ß√£o dessa fun√ß√£o de custo √© realizada iterativamente por m√©todos como o gradiente descendente [^4.4.3]. A combina√ß√£o da regulariza√ß√£o com a fun√ß√£o de verossimilhan√ßa da regress√£o log√≠stica direciona o algoritmo a solu√ß√µes esparsas. $\blacksquare$

```mermaid
graph LR
    subgraph "L1 Regularization in Logistic Regression"
        direction LR
        A["Logistic Loss Function"]
        B["L1 Penalty: Œª||Œ≤||‚ÇÅ"]
        C["Combined Objective"]
        D["Sparse Coefficients (Œ≤)"]
        A & B --> C
        C --> D
    end
```

**Corol√°rio 3:** Modelos com coeficientes esparsos obtidos pela regulariza√ß√£o L1 s√£o mais f√°ceis de interpretar, pois apenas um subconjunto das vari√°veis de entrada √© utilizado na predi√ß√£o [^4.4.5]. Isso auxilia na identifica√ß√£o dos fatores mais relevantes para a classifica√ß√£o.

> ‚ö†Ô∏è **Ponto Crucial**: A combina√ß√£o das penalidades L1 e L2 (Elastic Net) permite um controle mais flex√≠vel sobre a esparsidade e estabilidade do modelo, combinando as vantagens de ambos os tipos de regulariza√ß√£o [^4.5].

> üí° **Exemplo Num√©rico:** Vamos comparar o efeito da regulariza√ß√£o L1 (LASSO) e L2 (Ridge) em um modelo de regress√£o log√≠stica para um problema de classifica√ß√£o. Vamos usar um conjunto de dados simulado com 10 vari√°veis preditoras e 1 vari√°vel resposta bin√°ria.
>
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LogisticRegression
> from sklearn.model_selection import train_test_split
> from sklearn.preprocessing import StandardScaler
> from sklearn.metrics import accuracy_score
>
> # Cria√ß√£o de dados simulados
> np.random.seed(42)
> n_samples = 100
> n_features = 10
>
> X = np.random.normal(0, 1, size=(n_samples, n_features))
> true_betas = np.array([2, -1, 0.5, -0.2, 0, 0, 0, 0, 0, 0])
> logit = X @ true_betas
> probabilities = 1 / (1 + np.exp(-logit))
> y = np.random.binomial(1, probabilities)
>
> # Divide em treinamento e teste
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # Escala os dados
> scaler = StandardScaler()
> X_train_scaled = scaler.fit_transform(X_train)
> X_test_scaled = scaler.transform(X_test)
>
> # Modelo sem regulariza√ß√£o
> model_no_reg = LogisticRegression(penalty=None, solver='lbfgs', random_state=42)
> model_no_reg.fit(X_train_scaled, y_train)
>
> # Modelo com regulariza√ß√£o L1 (LASSO)
> model_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.5, random_state=42)
> model_l1.fit(X_train_scaled, y_train)
>
> # Modelo com regulariza√ß√£o L2 (Ridge)
> model_l2 = LogisticRegression(penalty='l2', solver='lbfgs', C=0.5, random_state=42)
> model_l2.fit(X_train_scaled, y_train)
>
> # Predi√ß√µes
> y_pred_no_reg = model_no_reg.predict(X_test_scaled)
> y_pred_l1 = model_l1.predict(X_test_scaled)
> y_pred_l2 = model_l2.predict(X_test_scaled)
>
> # Avalia√ß√£o
> accuracy_no_reg = accuracy_score(y_test, y_pred_no_reg)
> accuracy_l1 = accuracy_score(y_test, y_pred_l1)
> accuracy_l2 = accuracy_score(y_test, y_pred_l2)
>
> # Exibi√ß√£o dos resultados
> print(f"Acur√°cia (sem regulariza√ß√£o): {accuracy_no_reg:.4f}")
> print(f"Acur√°cia (L1/LASSO):        {accuracy_l1:.4f}")
