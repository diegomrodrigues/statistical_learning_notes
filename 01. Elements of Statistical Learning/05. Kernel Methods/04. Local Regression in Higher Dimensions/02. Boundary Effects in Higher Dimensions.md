## Efeitos de Fronteira e Padronização em Regressão Local de Dimensões Elevadas

### Introdução
Em dimensões elevadas, a precisão da regressão local é significativamente afetada pelos chamados **efeitos de fronteira** [^10]. Estes efeitos ocorrem devido ao aumento da proporção de pontos de dados próximos às fronteiras do espaço amostral, o que pode levar a estimativas enviesadas. Além disso, a **sensibilidade da norma euclidiana** às unidades de medida das coordenadas exige uma padronização cuidadosa dos preditores. Este capítulo explora esses desafios e discute estratégias para mitigar seus impactos.

### Conceitos Fundamentais

#### Efeitos de Fronteira em Dimensões Elevadas
Em baixas dimensões, a maioria dos pontos de dados está localizada no interior do espaço amostral. No entanto, à medida que a dimensionalidade aumenta, a fração de pontos próximos à fronteira também aumenta [^10]. Isso é uma manifestação da **maldição da dimensionalidade**.

> *Em dimensões elevadas, a maioria dos pontos de dados está localizada perto das bordas do espaço amostral.*

A regressão local, que se baseia em médias ponderadas de vizinhos próximos, torna-se mais suscetível a vieses quando muitos desses vizinhos estão localizados em uma região onde a função de regressão não está bem suportada por dados do outro lado da fronteira. Este problema é exacerbado em dimensões elevadas, onde a densidade de dados pode ser esparsa, e a influência de pontos de fronteira pode dominar a estimativa local.

#### Correção de Fronteira via Regressão Polinomial Local
Para mitigar os efeitos de fronteira, a **regressão polinomial local** é uma técnica crucial [^10]. Em vez de ajustar apenas uma constante local (como na regressão de Nadaraya-Watson), a regressão polinomial local ajusta um polinômio de grau *d* aos dados vizinhos ponderados. A solução para o problema de mínimos quadrados ponderados é dada por [^5]:
$$ \min_{\alpha(x_0), \beta_j(x_0), j=1,...,d} \sum_{i=1}^N K_{\lambda}(x_0, x_i) \left[Y_i - \alpha(x_0) - \sum_{j=1}^d \beta_j(x_0)x_i^j \right]^2 $$
onde $K_{\lambda}(x_0, x_i)$ é uma função kernel que pondera os pontos de dados com base na distância de $x_i$ a $x_0$.

Como vimos anteriormente, a regressão linear local (d=1) já apresenta uma melhora em relação a regressão local de ordem zero [^5]. De forma similar, o uso de polinômios de ordem maior pode reduzir o viés em regiões de curvatura da função verdadeira [^5].

Conforme mencionado anteriormente, a escolha do grau *d* do polinômio envolve um *trade-off* entre viés e variância [^5]. Polinômios de grau mais elevado podem reduzir o viés, mas também aumentar a variância, levando a ajustes mais irregulares [^5].

#### Padronização de Preditores
A **norma euclidiana**, usada para calcular distâncias entre pontos de dados, é sensível às unidades de medida das coordenadas [^10]. Se os preditores tiverem escalas diferentes, aqueles com escalas maiores dominarão o cálculo da distância, levando a resultados enviesados.

> *A padronização dos preditores para desvio padrão unitário é crucial devido à sensibilidade da norma euclidiana às unidades de medida das coordenadas.*

Para evitar esse problema, é crucial **padronizar** os preditores antes de aplicar a regressão local [^10]. A padronização geralmente envolve subtrair a média e dividir pelo desvio padrão de cada preditor, resultando em preditores com média zero e desvio padrão unitário.

### Conclusão
Em resumo, a regressão local em dimensões elevadas exige uma atenção cuidadosa aos efeitos de fronteira e à padronização de preditores. A regressão polinomial local oferece uma maneira eficaz de mitigar os efeitos de fronteira, enquanto a padronização dos preditores garante que todas as coordenadas contribuam igualmente para o cálculo da distância. Ao abordar esses desafios, podemos melhorar a precisão e a confiabilidade da regressão local em aplicações de alta dimensão.

### Referências
[^5]: Seções 6.1.1 e 6.1.2.
[^10]: Seção 6.3.
<!-- END -->