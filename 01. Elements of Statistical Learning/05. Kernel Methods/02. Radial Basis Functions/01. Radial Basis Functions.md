## Radial Basis Functions: Uma Abordagem Híbrida para Modelagem Flexível

### Introdução
Este capítulo explora as **Radial Basis Functions (RBFs)**, uma classe de modelos que combina a flexibilidade de funções kernel com a representação explícita de expansões de base [^22]. Em essência, as RBFs tratam as funções kernel $K_\lambda(\xi, x)$ como funções de base, resultando em modelos da forma $f(x) = \sum_{j=1}^M K_{\lambda_j}(\xi_j, x) \beta_j$ [^22]. Aqui, cada elemento da base é indexado por um parâmetro de localização ou protótipo $\xi_j$ e um parâmetro de escala $\lambda_j$ [^22]. A escolha popular para o kernel em redes RBF é a função de densidade Gaussiana padrão [^22]. Este capítulo detalha a formulação, propriedades e métodos de aprendizado associados às RBFs, conectando-as a conceitos previamente discutidos, como funções kernel e expansões de base.

### Conceitos Fundamentais

As RBFs representam uma abordagem híbrida para modelagem de funções, combinando elementos de modelos lineares e métodos baseados em kernel [^22]. A flexibilidade das RBFs reside na capacidade de adaptar tanto a localização quanto a escala das funções de base, permitindo uma aproximação precisa de funções complexas [^22].

**Formulação Matemática**

O modelo RBF é definido como uma soma ponderada de funções kernel, cada uma centrada em um ponto protótipo $\xi_j$ [^22]:

$$ f(x) = \sum_{j=1}^M K_{\lambda_j}(\xi_j, x) \beta_j $$

onde:

*   $x$ é o vetor de entrada.
*   $K_{\lambda_j}(\xi_j, x)$ é a função kernel, geralmente uma função Gaussiana.
*   $\xi_j$ é o centro do kernel (protótipo).
*   $\lambda_j$ é o parâmetro de escala (largura) do kernel.
*   $\beta_j$ é o peso associado ao j-ésimo kernel.
*   $M$ é o número de funções de base (kernels).

**Escolha do Kernel**

A escolha do kernel $K_{\lambda}(\xi, x)$ é crucial para o desempenho do modelo RBF. A função Gaussiana é uma escolha popular devido à sua suavidade e propriedades bem compreendidas [^22]:

$$ K_{\lambda}(\xi, x) = \exp\left(-\frac{\|x - \xi\|^2}{2\lambda^2}\right) $$

Outras opções incluem kernels multiquádricos e inversos multiquádricos, cada um com suas próprias características [^22].

**Aprendizado de Parâmetros**

O aprendizado em modelos RBF envolve a determinação dos parâmetros $\xi_j$, $\lambda_j$ e $\beta_j$ [^22]. Existem diversas abordagens para este problema, incluindo:

1.  **Otimização Conjunta:** Otimizar todos os parâmetros simultaneamente, minimizando uma função de custo, como a soma dos quadrados dos erros [^22]:

    $$     \min_{\{\xi_j, \lambda_j, \beta_j\}} \sum_{i=1}^N \left(y_i - \sum_{j=1}^M K_{\lambda_j}(\xi_j, x_i) \beta_j\right)^2     $$

    Esta abordagem é complexa e geralmente requer algoritmos de otimização não-convexos [^22].
2.  **Aprendizado em Estágios:** Separar o aprendizado dos parâmetros do kernel ($\xi_j$, $\lambda_j$) do aprendizado dos pesos $\beta_j$ [^22].
    *   **Seleção Não Supervisionada dos Centros:** Escolher os centros $\xi_j$ usando métodos de agrupamento (clustering) aplicados aos dados de entrada $x_i$ [^23]. As escalas $\lambda_j$ podem ser definidas como a distância média entre os centros ou tratadas como hiperparâmetros a serem ajustados [^23].
    *   **Aprendizado Supervisionado dos Pesos:** Com os centros e escalas fixos, os pesos $\beta_j$ podem ser determinados resolvendo um problema de mínimos quadrados linear [^23].
3.  **Renormalização:** A renormalização das funções de base pode evitar problemas associados à falta de suporte em certas regiões do espaço de entrada [^23].

    $$     h_j(x) = \frac{K_{\lambda_j}(\xi_j, x)}{\sum_{k=1}^M K_{\lambda_k}(\xi_k, x)}     $$

    Com as funções de base renormalizadas $h_j(x)$, o modelo RBF se torna:

    $$     f(x) = \sum_{j=1}^M h_j(x) \beta_j     $$

    Neste caso, os pesos $\beta_j$ podem ser interpretados como valores locais da função $f(x)$ nos centros $\xi_j$ [^23].

**Conexão com Métodos Kernel**

As RBFs podem ser vistas como uma ponte entre os métodos kernel e as técnicas de ajuste local [^24]. A escolha de funções kernel como funções de base permite que as RBFs capturem relações não lineares nos dados, enquanto a expansão em uma base explícita facilita a interpretação e o controle da complexidade do modelo [^24].

### Conclusão
As Radial Basis Functions oferecem uma abordagem poderosa e flexível para modelagem de funções [^22]. Ao combinar funções kernel com expansões de base, as RBFs proporcionam uma representação rica e adaptável dos dados [^22]. A escolha do kernel, a seleção dos centros e escalas, e o método de aprendizado dos pesos são aspectos cruciais que influenciam o desempenho do modelo [^22]. As RBFs encontram aplicações em diversas áreas, incluindo regressão, classificação e aproximação de funções complexas [^22].

### Referências
[^22]: Seção 6.7 do texto.
[^23]: Seção 6.7 do texto.
[^24]: Seção 6.7 do texto.
<!-- END -->