## Aprendizado dos Parâmetros em Funções de Base Radial

### Introdução
Este capítulo aprofunda a discussão sobre **Funções de Base Radial (RBFs)**, especificamente focando no aprendizado dos parâmetros que definem essas funções. Como vimos anteriormente, as RBFs oferecem uma abordagem flexível para modelagem não linear, combinando características de expansões de base e métodos de kernel [^22]. A capacidade de localizar a influência das funções de base permite que as RBFs capturem padrões complexos nos dados [^22]. Exploraremos diferentes estratégias para otimizar os parâmetros cruciais das RBFs: os centros (ξj), as larguras (λj) e os pesos (βj) [^22].

### Conceitos Fundamentais

O aprendizado dos parâmetros {ξj, λj, βj} em RBFs pode ser abordado de diversas maneiras [^22]:

1.  **Otimização Direta da Soma dos Quadrados:**
    -   Esta abordagem envolve a otimização da soma dos quadrados dos erros em relação a todos os parâmetros simultaneamente [^22].
    -   A função objetivo é definida como:
        $$\
        \min_{\{\xi_j, \lambda_j, \beta_j\}} \sum_{i=1}^{N} \left( y_i - \sum_{j=1}^{M} \beta_j \exp\left\{ -\frac{(x_i - \xi_j)^T (x_i - \xi_j)}{\lambda_j^2} \right\} \right)^2
        $$\
        onde *N* é o número de amostras, *M* é o número de funções de base radial, *yi* são os valores observados e *xi* são os valores preditores [^22].
    -   Esta otimização direta, no entanto, frequentemente resulta em um problema não convexo, similar ao encontrado no treinamento de redes neurais [^22]. A não convexidade implica a existência de múltiplos mínimos locais, tornando o processo de otimização desafiador e sensível às condições iniciais.

2.  **Estimativa Separada dos Parâmetros:**
    -   Uma alternativa é estimar {λj, ξj} separadamente de βj [^22].
    -   Tipicamente, os centros ξj e as larguras λj são determinados em um primeiro passo, utilizando métodos não supervisionados [^23]. Por exemplo, pode-se ajustar um modelo de mistura gaussiana aos dados de entrada *xi*, estimando os centros e as variâncias das gaussianas [^23].
    -   Com {λj, ξj} fixos, a estimativa dos pesos βj se torna um problema de mínimos quadrados linear, resolvido de forma eficiente [^23]:
        $$\
        \min_{\{\beta_j\}} \sum_{i=1}^{N} \left( y_i - \sum_{j=1}^{M} \beta_j \exp\left\{ -\frac{(x_i - \xi_j)^T (x_i - \xi_j)}{\lambda_j^2} \right\} \right)^2
        $$\
    -   Essa abordagem simplifica o problema de otimização, mas pode não levar à solução ótima global, pois os parâmetros não são otimizados conjuntamente [^23].

3.  **Utilização de Métodos de Clustering:**
    -   Métodos de *clustering* podem ser empregados para localizar os protótipos ξj [^22].
    -   Algoritmos como o *k-means* podem ser usados para agrupar os dados de entrada, com os centróides dos clusters servindo como os centros ξj das RBFs [^22].
    -   A largura λj pode ser tratada como um hiperparâmetro, ajustado por validação cruzada ou outras técnicas de seleção de modelo [^22].
    -   Semelhante à estimativa separada, essa abordagem reduz a complexidade da otimização, mas pode não capturar as dependências entre os parâmetros [^23].

### Conclusão
A escolha da estratégia de aprendizado dos parâmetros em RBFs depende das características do problema em questão, do tamanho do conjunto de dados e dos recursos computacionais disponíveis. A otimização direta oferece o potencial de encontrar a solução ótima global, mas é computacionalmente intensiva e propensa a mínimos locais [^22]. As abordagens de estimativa separada e clustering simplificam o processo de otimização, mas podem sacrificar a qualidade da solução [^23]. A seleção de um método apropriado requer uma consideração cuidadosa das compensações entre precisão, eficiência computacional e interpretabilidade do modelo.

### Referências
[^22]: Page 22, "Kernel Smoothing Methods"
[^23]: Page 23, "Kernel Smoothing Methods"
<!-- END -->