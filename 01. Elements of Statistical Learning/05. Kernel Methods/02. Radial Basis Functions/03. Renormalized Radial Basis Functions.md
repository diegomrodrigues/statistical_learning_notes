## Renormalized Radial Basis Functions

### Introdução
Este capítulo aprofunda o conceito de **Radial Basis Functions (RBFs)**, focando especificamente nas *renormalized radial basis functions*. Como vimos anteriormente, as RBFs são utilizadas para representar funções como expansões em uma base funcional. As RBFs combinam a flexibilidade dos métodos kernel com a estrutura das expansões em bases funcionais [^22]. No entanto, as RBFs podem apresentar problemas relacionados à falta de suporte em certas regiões do espaço de entrada, levando à criação de "buracos" [^23]. Para mitigar esse problema, introduzimos as renormalized radial basis functions, que garantem que a soma das funções de base seja sempre igual a 1. Além disso, exploraremos a conexão entre o estimador de regressão kernel de Nadaraya-Watson e as renormalized radial basis functions.

### Conceitos Fundamentais

**Renormalized radial basis functions** são definidas como [^23]:
$$ h_j(x) = \frac{D(||x - \xi_j||/\lambda)}{\sum_{k=1}^N D(||x - \xi_k||/\lambda)} $$
onde:
- $x$ é o ponto de entrada.
- $\xi_j$ são os centros das funções de base.
- $\lambda$ é um parâmetro de escala.
- $D$ é uma função radial.
- $N$ é o número de funções de base.

A **normalização** garante que $\sum_{j=1}^N h_j(x) = 1$ para todo $x$ [^23]. Essa propriedade evita a criação de "buracos" no espaço de entrada, onde nenhuma das funções de base tem suporte apreciável. A figura 6.16 ilustra como as Gaussian radial basis functions com largura fixa podem criar buracos, enquanto as renormalized Gaussian radial basis functions evitam esse problema [^23].

O **estimador de regressão kernel de Nadaraya-Watson** [^2, 3, 6] pode ser expresso como uma expansão em renormalized radial basis functions [^23]:
$$ \hat{f}(x_0) = \frac{\sum_{i=1}^N K_\lambda(x_0, x_i) y_i}{\sum_{i=1}^N K_\lambda(x_0, x_i)} = \sum_{i=1}^N y_i h_i(x_0) $$
onde:
- $K_\lambda(x_0, x_i)$ é uma função kernel.
- $y_i$ são os valores de resposta.
- $h_i(x_0)$ são as renormalized radial basis functions.

Esta representação destaca a conexão entre os métodos kernel e as expansões em bases funcionais [^22]. As renormalized radial basis functions podem ser vistas como uma forma de **"automatic kernel carpentry"** [^6], onde a normalização adapta as funções de base para corrigir os vieses devido à assimetria ou falta de suporte.

### Conclusão
As renormalized radial basis functions oferecem uma abordagem eficaz para mitigar o problema dos "buracos" no espaço de entrada, garantindo que a soma das funções de base seja sempre igual a 1 [^23]. Além disso, a conexão com o estimador de Nadaraya-Watson destaca a relação entre os métodos kernel e as expansões em bases funcionais [^22]. Essa abordagem permite uma representação mais robusta e flexível de funções, especialmente em regiões onde os dados são esparsos ou a função subjacente é complexa. As renormalized radial basis functions são especialmente úteis quando as funções de base originais não conseguem fornecer um suporte adequado em todo o espaço de entrada, levando a aproximações imprecisas.

### Referências
[^2]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer science & business media.
[^3]: Wand, M. P., & Jones, M. C. (1994). *Kernel smoothing*. CRC press.
[^6]: Loader, C. (1999). *Local regression and likelihood*. Springer Science & Business Media.
[^22]: Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.
[^23]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer science & business media.

<!-- END -->