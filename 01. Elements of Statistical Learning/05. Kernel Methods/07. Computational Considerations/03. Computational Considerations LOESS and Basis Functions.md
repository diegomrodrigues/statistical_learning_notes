## Considerações Computacionais em Kernel Smoothing: Triangulação vs. Funções de Base

### Introdução
Este capítulo explora as considerações computacionais envolvidas nos métodos de *kernel smoothing*, com foco em técnicas para reduzir a carga computacional. Em particular, comparamos implementações que utilizam esquemas de triangulação com métodos de funções de base [^26].

### Conceitos Fundamentais

Os métodos de *kernel smoothing*, como a regressão local e a estimação de densidade, são *memory-based* [^26]. Isso significa que o modelo é essencialmente o conjunto de dados de treinamento completo, e o ajuste do modelo ocorre no momento da avaliação ou predição. Essa característica pode tornar esses métodos inviáveis para aplicações em tempo real, especialmente com grandes conjuntos de dados.

A complexidade computacional para ajustar o modelo em uma única observação $x_0$ é tipicamente $O(N)$, onde $N$ é o tamanho do conjunto de dados de treinamento [^26]. Essa complexidade surge da necessidade de calcular os pesos do kernel para cada ponto no conjunto de dados em relação ao ponto de consulta $x_0$.

Em contraste, uma expansão em $M$ funções de base custa $O(M)$ para uma única avaliação, e tipicamente $M \sim O(\log N)$ [^26]. No entanto, os métodos de funções de base têm um custo inicial de pelo menos $O(NM^2 + M^3)$ [^26], devido à necessidade de estimar os coeficientes das funções de base.

**Técnicas de Triangulação:**

Implementações populares de regressão local, como a função *loess* em S-PLUS e R, e o procedimento *locfit* (Loader, 1999), empregam esquemas de triangulação para reduzir a carga computacional [^26]. Essas técnicas calculam o ajuste exatamente em $M$ locais cuidadosamente escolhidos, com um custo de $O(NM)$, e então usam técnicas de interpolação para estimar o ajuste em outros locais, com um custo de $O(M)$ por avaliação [^26].

**Comparação com Métodos de Funções de Base:**

A tabela abaixo resume as complexidades computacionais dos métodos de triangulação e funções de base:

| Método                 | Custo Inicial | Custo por Avaliação |
| ---------------------- | ------------- | ------------------- |
| Triangulação           | $O(NM)$      | $O(M)$             |
| Funções de Base         | $O(NM^2 + M^3)$ | $O(M)$             |

Onde:
*   $N$ é o tamanho do conjunto de dados de treinamento
*   $M$ é o número de locais escolhidos para a triangulação ou o número de funções de base.

Em essência, a triangulação troca um custo inicial maior por um custo de avaliação mais baixo, tornando-a uma escolha atraente para conjuntos de dados grandes onde múltiplas avaliações são necessárias.

### Conclusão

A escolha entre técnicas de triangulação e métodos de funções de base depende do tamanho do conjunto de dados, do número de avaliações necessárias e dos recursos computacionais disponíveis. Para conjuntos de dados grandes e múltiplas avaliações, os esquemas de triangulação oferecem uma maneira eficaz de reduzir a carga computacional.

### Referências
[^26]: Página 216 do documento original.
<!-- END -->