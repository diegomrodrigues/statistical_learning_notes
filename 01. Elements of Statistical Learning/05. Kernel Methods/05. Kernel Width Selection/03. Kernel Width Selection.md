## Kernel Width and Local Regression Behavior

### Introdução
Este capítulo explora o comportamento das estimativas de regressão local em relação à largura do kernel utilizado. Como mencionado anteriormente [^1], a flexibilidade na estimativa da função de regressão $f(X)$ é alcançada ajustando um modelo simples em cada ponto de consulta $x_0$, ponderando as observações próximas a este ponto através de um kernel $K_{\lambda}(x_0, x_i)$. O parâmetro $\lambda$ controla a largura do kernel e, consequentemente, o grau de "localidade" da regressão. Este capítulo focará em como a escolha de $\lambda$ afeta o ajuste do modelo, especificamente nos limites de $\lambda$ tendendo a zero e ao infinito.

### Conceitos Fundamentais
A regressão local, como discutido, utiliza um kernel $K_{\lambda}(x_0, x)$ para ponderar as observações próximas ao ponto de interesse $x_0$ [^1]. A largura do kernel, controlada por $\lambda$, desempenha um papel crucial no compromisso entre bias e variância.

**Largura do Kernel Tendendo a Zero:**
Quando a largura do kernel $\lambda$ se aproxima de zero, apenas as observações mais próximas a $x_0$ recebem peso significativo. Em um limite extremo, onde $\lambda \rightarrow 0$, a regressão local se torna uma *interpolação piecewise-linear* dos dados de treinamento [^9]. Isso significa que o modelo ajustado passa exatamente pelos pontos de dados, resultando em um bias muito baixo (ou nulo) nos pontos de treinamento, mas potencialmente com alta variância, especialmente em regiões com poucos dados. A função estimada se torna altamente sensível a ruídos nos dados de treinamento, e pode apresentar grandes oscilações entre os pontos observados.

**Largura do Kernel Tendendo ao Infinito:**
Por outro lado, quando a largura do kernel $\lambda$ tende ao infinito, todas as observações no conjunto de dados recebem pesos aproximadamente iguais. Neste caso, a regressão local se aproxima de um *ajuste global linear por mínimos quadrados* [^9]. O modelo resultante é uma linha reta (ou hiperplano em dimensões superiores) que minimiza o erro quadrático médio sobre todo o conjunto de dados. Isso leva a um bias potencialmente alto, pois o modelo global pode não capturar a complexidade da verdadeira função de regressão, mas a variância é baixa, pois o modelo é menos sensível a ruídos nos dados.

**Transição Suave:**
É importante notar que a transição entre esses dois extremos é suave. À medida que $\lambda$ aumenta a partir de zero, o modelo começa a "suavizar" os dados, reduzindo a variância em detrimento de um aumento no bias. A escolha ideal de $\lambda$ envolve encontrar o equilíbrio certo entre bias e variância para minimizar o erro de generalização.

**Formalização Matemática:**
Para formalizar, considere a regressão linear local expressa na equação (6.8) [^5]:
$$ \hat{f}(x_0) = b(x_0)^T (B^T W(x_0) B)^{-1} B^T W(x_0) y $$
onde $b(x)^T = (1, x)$, $B$ é a matriz de regressão com a $i$-ésima linha $b(x_i)$, e $W(x_0)$ é a matriz diagonal com o $i$-ésimo elemento diagonal $K_{\lambda}(x_0, x_i)$.

*   **Caso $\lambda \rightarrow 0$**: A matriz $W(x_0)$ se torna uma matriz diagonal com 1s apenas nas linhas correspondentes aos pontos de treinamento mais próximos de $x_0$, e 0s nas demais. Se considerarmos apenas o ponto mais próximo, o ajuste se torna uma interpolação linear entre os pontos vizinhos.
*   **Caso $\lambda \rightarrow \infty$**: A matriz $W(x_0)$ se aproxima de uma matriz com todos os elementos diagonais iguais, efetivamente dando o mesmo peso a todos os pontos. Neste caso, a solução se aproxima da solução de mínimos quadrados linear global.

**Exemplo Prático:**
Considere os exemplos dados na Figura 6.1 [^2]. No painel esquerdo, com um k-vizinhos mais próximos, a curva verde resultante é irregular e descontínua. No painel direito, com um kernel Epanechnikov com $\lambda = 0.2$, a curva é mais suave e contínua. Ajustar $\lambda$ permite controlar essa suavidade.

### Conclusão
A largura do kernel $\lambda$ é um parâmetro crítico na regressão local que controla o trade-off entre bias e variância. Valores pequenos de $\lambda$ levam a modelos com baixo bias e alta variância, enquanto valores grandes de $\lambda$ levam a modelos com alto bias e baixa variância. No limite, quando $\lambda$ tende a zero, a regressão local se aproxima de uma interpolação piecewise-linear dos dados de treinamento, e quando $\lambda$ tende ao infinito, se aproxima de um ajuste global linear por mínimos quadrados. A escolha apropriada de $\lambda$ é crucial para obter um bom desempenho de generalização, e técnicas como validação cruzada [^9] são frequentemente utilizadas para selecionar um valor ótimo.

### Referências
[^1]: Página 191: "In this chapter we describe a class of regression techniques that achieve flexibility in estimating the regression function f(X) over the domain IR by fitting a different but simple model separately at each query point xo."
[^2]: Página 192: "FIGURE 6.1. In each panel 100 pairs xi, Yi are generated at random from the blue curve with Gaussian errors...".
[^5]: Página 195: "Define the vector-valued function b(x)T = (1,x). Let B be the N × 2 regression matrix with ith row b(xi), and W(xo) the N × N diagonal matrix with ith diagonal element Kx(xo, xi). Then f(xo) = b(xo) (BTW(xo)B)¯¹BTW(x0)y"
[^9]: Página 199: "Similar arguments apply to local regression estimates, say local linear: as the width goes to zero, the estimates approach a piecewise-linear function that interpolates the training data; as the width gets infinitely large, the fit approaches the global linear least-squares fit to the data."
<!-- END -->