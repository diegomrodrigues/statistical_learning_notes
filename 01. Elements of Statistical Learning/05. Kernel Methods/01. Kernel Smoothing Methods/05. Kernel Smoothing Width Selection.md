## Seleção da Largura do Kernel

### Introdução
A escolha da largura do kernel, denotada pelo parâmetro λ, é um aspecto crucial na aplicação de métodos de *kernel smoothing* [^1]. Este parâmetro controla o grau de suavização aplicado aos dados, influenciando diretamente o compromisso entre viés e variância do estimador resultante. Em continuidade ao conceito de *kernel smoothing*, explorado anteriormente [^1], este capítulo se aprofundará nas estratégias para selecionar λ de forma otimizada.

### Conceitos Fundamentais

A seleção da largura do kernel (parâmetro λ) envolve o equilíbrio entre viés e variância [^1]. A escolha inadequada de λ pode levar a modelos que sofrem de *overfitting* (alta variância, baixo viés) ou *underfitting* (baixa variância, alto viés).

**Trade-off Viés-Variância:**

*   **Janela Estreita (λ pequeno):** Resulta em alta variância e baixo viés. O modelo se ajusta muito aos dados de treinamento, capturando ruídos e flutuações aleatórias, mas pode não generalizar bem para novos dados.

*   **Janela Larga (λ grande):** Resulta em baixa variância e alto viés. O modelo suaviza demais os dados, perdendo detalhes importantes e tendendo a subestimar a complexidade da relação subjacente.

[^1] *Larger λ implies lower variance but higher bias, while smaller λ implies higher variance but lower bias.*

**Tipos de Largura de Janela:**

Existem diferentes abordagens para definir a largura da janela do kernel, cada uma com suas próprias características em relação ao viés e à variância:

*   **Largura de Janela Métrica (constante $h_\lambda(x_0) = \lambda$):** Mantém o viés constante, mas a variância é inversamente proporcional à densidade local dos dados [^1, 3]. Em regiões de alta densidade, a variância é menor, enquanto em regiões de baixa densidade, a variância é maior.
    $$K_\lambda(x_0, x) = D\left(\frac{|x - x_0|}{h_\lambda(x_0)}\right)$$

*   **Largura de Janela do Vizinho Mais Próximo (adaptativa):** Mantém a variância constante, mas o viés varia inversamente com a densidade local [^1, 3]. Em regiões de alta densidade, o viés é menor, enquanto em regiões de baixa densidade, o viés é maior.
    $$h_\lambda(x_0) = |x_0 - x_{[k]}|$$
    Onde $x_{[k]}$ é o k-ésimo vizinho mais próximo de $x_0$.

**Técnicas de Validação Cruzada:**

Para determinar o parâmetro de suavização ideal, são empregadas técnicas de validação cruzada, que avaliam o desempenho do modelo em dados não utilizados no treinamento [^1, 9]. Algumas das técnicas mais comuns incluem:

*   **Validação Cruzada Leave-One-Out (LOOCV):** Cada observação é removida do conjunto de dados, o modelo é treinado com as observações restantes e a observação removida é usada para validar o modelo. Este processo é repetido para cada observação, e o erro médio é calculado.
    $$CV_{(n)} = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{f}_{-i}(x_i))^2$$

*   **Validação Cruzada Generalizada (GCV):** É uma aproximação computacionalmente mais eficiente da LOOCV.
    $$GCV = \frac{1}{N}\sum_{i=1}^N \left(\frac{y_i - \hat{f}(x_i)}{1 - \frac{trace(S_x)}{N}}\right)^2$$

*   **Critério Cp:** Estima o erro de previsão do modelo, levando em consideração o viés e a variância.
    $$C_p = \frac{1}{N}RSS + \frac{2\sigma^2 p}{N}$$

*   **Validação Cruzada K-Fold:** O conjunto de dados é dividido em *k* subconjuntos (folds). O modelo é treinado em *k-1* folds e validado no fold restante. Este processo é repetido *k* vezes, cada vez usando um fold diferente para validação. O erro médio é calculado.

**Graus de Liberdade Efetivos:**

Os graus de liberdade efetivos, definidos como *trace(Sx)*, são utilizados para calibrar a quantidade de suavização [^1, 9]. A matriz *Sx* é a matriz do *smoother*, construída a partir dos kernels equivalentes [^1, 9]. Os graus de liberdade efetivos fornecem uma medida da complexidade do modelo, indicando quantos parâmetros efetivamente estão sendo utilizados para ajustar os dados.

[^1] *The effective degrees of freedom, defined as trace(Sx), are used to calibrate the amount of smoothing, where Sx is the smoother matrix built up from the equivalent kernels.*

### Conclusão
A seleção da largura do kernel é um passo crítico na aplicação de métodos de *kernel smoothing*. A escolha apropriada de λ requer um equilíbrio cuidadoso entre viés e variância, e a utilização de técnicas de validação cruzada para avaliar o desempenho do modelo. A compreensão dos graus de liberdade efetivos permite calibrar a quantidade de suavização, garantindo um ajuste adequado aos dados. Em continuidade, o próximo capítulo explorará a aplicação de métodos de *kernel smoothing* em contextos de regressão local em dimensões superiores.

### Referências
[^1]: Page 193: "Selecting the width of the kernel (parameter λ) involves balancing bias and variance. A narrow window results in high variance and low bias, while a wide window results in low variance and high bias. Larger λ implies lower variance but higher bias, while smaller λ implies higher variance but lower bias. Metric window widths maintain constant bias but have variance inversely proportional to local density, whereas nearest-neighbor window widths keep variance constant but have bias varying inversely with local density. Cross-validation techniques, such as leave-one-out, generalized, Cp, or k-fold, are used to determine the optimal smoothing parameter. The effective degrees of freedom, defined as trace(Sx), are used to calibrate the amount of smoothing, where Sx is the smoother matrix built up from the equivalent kernels."
[^2]: Page 192: "These memory-based methods require in principle little or no training; all the work gets done at evaluation time. The only parameter that needs to be determined from the training data is λ."
[^3]: Page 193: "In (6.3), hx(x0) = A is constant. For k-nearest neighborhoods, the neigh-borhood size k replaces A, and we have hk(x0) = |xo - X[k]| where x[k] is the kth closest xi to xo."
[^4]: Page 193: "Metric window widths (constant h₁(x)) tend to keep the bias of the estimate constant, but the variance is inversely proportional to the local density."
[^5]: Page 193: "Nearest-neighbor window widths exhibit the opposite behavior; the variance stays constant and the absolute bias varies inversely with local density."
[^6]: Page 199: "There is a natural bias-variance tradeoff as we change the width of the averaging window, which is most explicit for local averages:"
[^7]: Page 199: "If the window is narrow, f(x0) is an average of a small number of yi close to xo, and its variance will be relatively large close to that of an individual yi."
[^8]: Page 199: "The bias will tend to be small, again because each of the E(yi) = f(xi) should be close to f(xo)."
[^9]: Page 199: "The effective degrees of freedom is again defined as trace(Sx), and can be used to calibrate the amount of smoothing."
[^10]: Page 199: "Local regression smoothers are linear estimators; the smoother matrix in f = Sxy is built up from the equivalent kernels (6.8), and has ijth entry {Sx}ij = li(xj)."
<!-- END -->