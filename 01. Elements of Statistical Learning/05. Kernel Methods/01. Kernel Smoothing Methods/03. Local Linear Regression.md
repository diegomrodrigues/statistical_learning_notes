## Regressão Linear Local: Mitigando o Viés em Suavização Kernel

### Introdução
Em continuidade ao tópico de **Kernel Smoothing Methods**, este capítulo aprofunda-se na técnica de **regressão linear local** [^1]. Como vimos anteriormente, os métodos de suavização kernel, como a média ponderada por kernel (Nadaraya-Watson), podem apresentar viés, especialmente nas fronteiras do domínio [^4]. A regressão linear local surge como uma solução para mitigar esse viés, ajustando linhas retas em vez de constantes localmente [^1]. Este capítulo explora os fundamentos teóricos, a formulação matemática e as vantagens da regressão linear local em relação aos métodos de suavização kernel mais simples.

### Conceitos Fundamentais

#### A Limitação da Média Ponderada Local
A média ponderada local, como a implementada pelo estimador de Nadaraya-Watson [^2], calcula uma média dos valores de resposta ($y_i$) ponderada pela proximidade dos preditores ($x_i$) ao ponto de interesse ($x_0$). No entanto, esse método assume implicitamente que a função de regressão é aproximadamente constante em uma vizinhança de $x_0$. Essa suposição pode levar a um viés significativo, especialmente nas bordas do domínio, onde a função pode ser mais linear [^4].

#### A Ideia Central da Regressão Linear Local
A regressão linear local aborda essa limitação ajustando localmente uma linha reta aos dados, em vez de uma constante [^1]. Em outras palavras, em vez de minimizar a soma ponderada dos quadrados das diferenças entre os valores de resposta e uma constante, a regressão linear local minimiza a soma ponderada dos quadrados das diferenças entre os valores de resposta e uma linha reta [^5].

#### Formulação Matemática
Formalmente, a regressão linear local resolve um problema de mínimos quadrados ponderados separado em cada ponto alvo $x_0$ [^1]:

$$\
\min_{\alpha(x_0), \beta(x_0)} \sum_i K_\lambda(x_0, x_i) [y_i - \alpha(x_0) - \beta(x_0)x_i]^2
$$

onde:
- $K_\lambda(x_0, x_i)$ é uma função kernel que pondera as observações com base em sua proximidade a $x_0$ [^1]. O parâmetro $\lambda$ controla a largura do kernel, determinando o tamanho da vizinhança local [^1].
- $\alpha(x_0)$ é o intercepto da linha reta ajustada localmente em $x_0$ [^5].
- $\beta(x_0)$ é a inclinação da linha reta ajustada localmente em $x_0$ [^5].

A solução para este problema de otimização fornece estimativas para $\alpha(x_0)$ e $\beta(x_0)$, que são então usadas para prever o valor da função de regressão em $x_0$ [^5]:

$$\
\hat{f}(x_0) = \hat{\alpha}(x_0) + \hat{\beta}(x_0)x_0
$$

#### Linearidade no Valor de Resposta
Uma propriedade importante da regressão linear local é que a estimativa $\hat{f}(x_0)$ é linear em $y$ [^1]:

$$\
\hat{f}(x_0) = \sum_i l_i(x_0)y_i
$$

onde os pesos $l_i(x_0)$ combinam as operações do kernel e dos mínimos quadrados [^1]. Esses pesos são independentes dos valores de resposta $y_i$ [^6].

#### Correção de Assimetria do Kernel e "Automatic Kernel Carpentry"
A regressão linear local efetivamente corrige a assimetria no kernel, especialmente nas fronteiras do domínio [^1, 5]. Essa correção é realizada automaticamente através do processo de mínimos quadrados ponderados, sem a necessidade de modificar explicitamente a função kernel [^1]. Esse fenômeno é conhecido como "automatic kernel carpentry" [^1, 6].

#### Expressão Explícita e Kernel Equivalente
A solução para o problema de mínimos quadrados ponderados pode ser expressa de forma explícita [^5]:

$$\
\hat{f}(x_0) = b(x_0)^T (B^T W(x_0) B)^{-1} B^T W(x_0) y
$$

onde:
- $b(x)^T = (1, x)$ é um vetor de funções [^5].
- $B$ é a matriz de regressão $N \times 2$ com a i-ésima linha sendo $b(x_i)$ [^5].
- $W(x_0)$ é uma matriz diagonal $N \times N$ com o i-ésimo elemento diagonal sendo $K_\lambda(x_0, x_i)$ [^5].

Essa expressão revela que a estimativa é uma combinação linear dos valores de resposta, com pesos efetivos dados por $l_i(x_0)$ [^5, 6]. A Figura 6.4 ilustra o efeito da regressão linear local no kernel equivalente, mostrando como ela modifica automaticamente o kernel de ponderação para corrigir o viés devido à assimetria na janela de suavização [^6].

### Conclusão
A regressão linear local representa um avanço significativo em relação aos métodos de suavização kernel mais simples, oferecendo uma redução substancial no viés, especialmente nas fronteiras do domínio [^1]. Ao ajustar linhas retas localmente, em vez de constantes, a regressão linear local captura melhor a estrutura subjacente dos dados, resultando em estimativas mais precisas e confiáveis [^5]. A propriedade de "automatic kernel carpentry" [^1, 6] torna a regressão linear local uma técnica poderosa e flexível para modelagem não paramétrica.

### Referências
[^1]: Capítulo 6, Kernel Smoothing Methods.
[^2]: Seção 6.1, One-Dimensional Kernel Smoothers.
[^4]: Seção 6.1.1, Local Linear Regression.
[^5]: Seção 6.1.1, Local Linear Regression.
[^6]: Seção 6.1.1, Local Linear Regression.

<!-- END -->