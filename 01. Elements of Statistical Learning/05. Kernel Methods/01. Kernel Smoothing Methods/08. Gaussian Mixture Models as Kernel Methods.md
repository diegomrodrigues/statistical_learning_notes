## Gaussian Mixture Models em Kernel Smoothing

### Introdução
Este capítulo explora o uso de **modelos de mistura**, com ênfase nos **modelos de mistura Gaussianos (GMMs)**, no contexto dos métodos de *kernel smoothing*. GMMs são ferramentas poderosas para **estimação de densidade** e podem ser interpretados como uma forma de método de kernel [^1]. Em continuidade ao que foi apresentado sobre métodos de *kernel smoothing*, vamos detalhar a formulação, o ajuste e as propriedades dos GMMs.

### Conceitos Fundamentais

Um **GMM** é definido como uma soma ponderada de densidades Gaussianas [^1]. Formalmente, a densidade de probabilidade de um GMM é dada por:

$$ f(x) = \sum_{m=1}^{M} \alpha_m \phi(x; \mu_m, \Sigma_m) $$

onde:
- $M$ é o número de componentes Gaussianos na mistura.
- $\alpha_m$ são as **proporções de mistura**, com $\sum_{m=1}^{M} \alpha_m = 1$ e $\alpha_m \geq 0$ para todo $m$.
- $\phi(x; \mu_m, \Sigma_m)$ é a **densidade Gaussiana** com média $\mu_m$ e matriz de covariância $\Sigma_m$.

Os parâmetros do GMM, ou seja, as proporções de mistura $\alpha_m$, as médias $\mu_m$ e as matrizes de covariância $\Sigma_m$, são tipicamente estimados por **máxima verossimilhança**, utilizando o **algoritmo EM** (Expectation-Maximization) [^1].

**Algoritmo EM para GMMs:**

O algoritmo EM é um método iterativo para encontrar estimativas de máxima verossimilhança em modelos com variáveis latentes. No contexto dos GMMs, a variável latente indica a qual componente Gaussiana cada ponto de dado pertence. O algoritmo EM consiste em duas etapas principais:

1.  **Etapa E (Expectation):** Calcula a probabilidade *a posteriori* de cada ponto de dado $x_i$ pertencer a cada componente Gaussiana $m$, dadas as estimativas atuais dos parâmetros. Estas probabilidades são denotadas por $r_{im}$ e calculadas como:

$$ r_{im} = \frac{\alpha_m \phi(x_i; \mu_m, \Sigma_m)}{\sum_{k=1}^{M} \alpha_k \phi(x_i; \mu_k, \Sigma_k)} $$

2.  **Etapa M (Maximization):** Reestima os parâmetros do modelo (proporções de mistura, médias e matrizes de covariância) utilizando as probabilidades *a posteriori* calculadas na etapa E. As fórmulas de reestimação são:

$$ \alpha_m = \frac{1}{N} \sum_{i=1}^{N} r_{im} $$

$$ \mu_m = \frac{\sum_{i=1}^{N} r_{im} x_i}{\sum_{i=1}^{N} r_{im}} $$

$$ \Sigma_m = \frac{\sum_{i=1}^{N} r_{im} (x_i - \mu_m)(x_i - \mu_m)^T}{\sum_{i=1}^{N} r_{im}} $$

onde $N$ é o número total de pontos de dado.

O algoritmo EM itera entre as etapas E e M até que a verossimilhança do modelo convirja para um valor máximo (ou até que a mudança na verossimilhança seja menor que um limiar predefinido).

**Restrições nas Matrizes de Covariância:**

Uma variação importante dos GMMs surge quando as matrizes de covariância são restritas a serem **escalares**, ou seja, $\Sigma_m = \sigma_m^2 I$, onde $I$ é a matriz identidade [^1]. Neste caso, o GMM assume a forma de uma **expansão de base radial**:

$$ f(x) = \sum_{m=1}^{M} \alpha_m \phi(x; \mu_m, \sigma_m^2 I) $$

Esta restrição simplifica o modelo e reduz o número de parâmetros a serem estimados, o que pode ser vantajoso em situações com dados limitados. Além disso, a forma escalar das matrizes de covariância impõe que as componentes Gaussianas sejam isotrópicas (esfericamente simétricas).

### Conclusão

Os modelos de mistura Gaussianos oferecem uma abordagem flexível e poderosa para a **estimação de densidade** e podem ser vistos como uma forma de método de kernel [^1]. A capacidade de modelar distribuições complexas através da combinação de componentes Gaussianas, juntamente com o uso do algoritmo EM para estimar os parâmetros do modelo, torna os GMMs uma ferramenta valiosa em diversas aplicações. Restrições nas matrizes de covariância, como a imposição de escalaridade, podem simplificar o modelo e melhorar sua aplicabilidade em cenários específicos. Como foi mencionado em [^24], o uso do Teorema de Bayes permite uma classificação flexível, e será explorado em detalhe no Capítulo 12.
<!-- END -->