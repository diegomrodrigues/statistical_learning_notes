## Kernel Smoothers Unidimensionais

### Introdução
Este capítulo explora os **métodos de *kernel smoothing***, técnicas de regressão que estimam a função de regressão $E(Y|X = x)$ ajustando modelos simples e diferentes em cada ponto de consulta $x_0$ [^1]. Expandindo o conceito de *k-nearest-neighbor average* apresentado anteriormente [^2], focaremos em *kernel smoothers* unidimensionais, que calculam uma média ponderada em uma vizinhança do ponto alvo, onde os pesos diminuem suavemente com a distância para evitar descontinuidades [^2].

### Conceitos Fundamentais

#### Média Ponderada por Kernel Nadaraya-Watson
A **média ponderada por kernel de Nadaraya-Watson** é um método comum para estimar a função de regressão $f(x_0)$ [^2]. Ele é definido como:
$$ \hat{f}(x_0) = \frac{\sum_{i=1}^{N} K_{\lambda}(x_0, x_i) y_i}{\sum_{i=1}^{N} K_{\lambda}(x_0, x_i)} $$
onde $K_{\lambda}(x_0, x_i)$ é o kernel, que atribui pesos com base na distância de $x_i$ a $x_0$ [^2]. O parâmetro $\lambda$ controla a largura da vizinhança [^1].

#### Kernel Quadrático de Epanechnikov
O **kernel quadrático de Epanechnikov** é usado para dar mais peso aos pontos mais próximos do alvo, resultando em uma função ajustada contínua e suave [^2]. Ele é definido como:
$$ K_{\lambda}(x_0, x) = D\left(\frac{x - x_0}{\lambda}\right) $$
onde
$$ D(t) = \begin{cases} \frac{3}{4}(1 - t^2) & \text{se } |t| \leq 1 \\ 0 & \text{caso contrário} \end{cases} $$
Com este kernel, os pontos entram na vizinhança com peso zero e sua contribuição aumenta gradualmente [^2]. Isso contrasta com a média simples de *k-vizinhos mais próximos*, que pode levar a descontinuidades [^2].

#### Média dos k-Vizinhos Mais Próximos
*Kernel smoothers* unidimensionais também podem usar a **média dos *k*-vizinhos mais próximos** como uma estimativa da função de regressão, relaxando a definição de expectativa condicional [^2]. A estimativa é dada por:
$$ \hat{f}(x) = \text{Ave}(y_i | x_i \in N_k(x)) $$
onde $N_k(x)$ é o conjunto dos $k$ pontos mais próximos de $x$ em distância quadrada, e $\text{Ave}$ denota a média [^2].

#### Generalização
Podemos generalizar a função kernel para:
$$ K_{\lambda}(x_0, x) = D\left(\frac{x - x_0}{h_{\lambda}(x_0)}\right) $$
onde $h_{\lambda}(x_0)$ é uma função de largura que determina a largura da vizinhança em $x_0$ [^2]. No caso do kernel de Epanechnikov, $h_{\lambda}(x_0) = \lambda$ é constante [^2]. Para *k*-vizinhos mais próximos, $h_k(x_0) = |x_0 - x_{[k]}|$ onde $x_{[k]}$ é o $k$-ésimo $x_i$ mais próximo de $x_0$ [^2].

#### Considerações Práticas
Existem vários detalhes que devem ser considerados na prática [^3]:
*   O **parâmetro de suavização** $\lambda$ determina a largura da vizinhança [^3]. Um $\lambda$ grande implica menor variância (médias sobre mais observações), mas maior viés (assume-se que a função verdadeira é constante dentro da janela) [^3].
*   **Larguras de janela métricas** (constante $h_{\lambda}(x)$) tendem a manter o viés da estimativa constante, mas a variância é inversamente proporcional à densidade local [^3].
*   **Larguras de janela do vizinho mais próximo** exibem o comportamento oposto; a variância permanece constante e o viés absoluto varia inversamente com a densidade local [^3].
*   **Problemas surgem com vizinhos mais próximos** quando há empates nos $x_i$ [^3].

### Conclusão
Os *kernel smoothers* unidimensionais oferecem uma maneira flexível de estimar funções de regressão, permitindo que o modelo se adapte aos dados locais [^1]. A escolha do kernel e do parâmetro de suavização desempenha um papel crucial no desempenho do modelo, e diferentes abordagens podem ser mais adequadas dependendo das características dos dados [^3].

### Referências
[^1]: Kernel Smoothing Methods
[^2]: 6.1 One-Dimensional Kernel Smoothers
[^3]: 6.1 One-Dimensional Kernel Smoothers
<!-- END -->