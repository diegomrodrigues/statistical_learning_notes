## Regressão Logística Regularizada L1: Seleção de Variáveis e Shrinkage

### Introdução

Como explorado anteriormente neste capítulo, a **regressão logística** (Seção 4.4) modela as probabilidades posteriores das $K$ classes através de funções lineares em $\mathbf{x}$, garantindo que as probabilidades somem um e permaneçam no intervalo $[0, 1]$ [^2]. A estimação dos parâmetros $\beta$ é tipicamente realizada por **máxima verossimilhança** (maximum likelihood), utilizando a verossimilhança condicional de $G$ dado $X$ (Seção 4.4.1) [^2]. O logaritmo da verossimilhança para $N$ observações é dado por $l(\theta) = \sum_{i=1}^{N} \log p_{g_i}(x_i; \theta)$ [^2]. Para o caso de duas classes, codificando a resposta $y_i$ como 0/1, a log-verossimilhança simplifica para $l(\beta) = \sum_{i=1}^{N} \{ y_i \log p(x_i; \beta) + (1-y_i) \log(1-p(x_i; \beta)) \}$, que pode ser reescrita como $\sum_{i=1}^{N} \{ y_i \beta^T x_i - \log(1 + e^{\beta^T x_i}) \}$ [^2].

Em cenários com muitas variáveis preditoras, ou quando se busca um modelo mais parcimonioso, técnicas de regularização são essenciais. Expandindo o conceito de regularização L1 (lasso) apresentado na Seção 3.4.2 (conforme referenciado em [^1]), este capítulo foca na aplicação da **penalidade L1** à regressão logística [^1]. Esta abordagem permite realizar simultaneamente a **seleção de variáveis** (variable selection) e a **contração dos coeficientes** (shrinkage), maximizando uma versão penalizada da log-verossimilhança [^1]. Abordaremos o problema de otimização resultante, os algoritmos computacionais empregados, com ênfase nas particularidades introduzidas pela penalidade L1, como os perfis de coeficientes *piecewise smooth* e o uso de algoritmos de caminho (path algorithms) [^1], [^12].

### Conceitos Fundamentais

#### O Problema de Otimização Penalizado

A regressão logística regularizada com L1 busca encontrar os coeficientes $\beta_0$ (intercepto) e $\beta$ (vetor de coeficientes das variáveis preditoras) que maximizam a log-verossimilhança penalizada [^1], [^3]. O critério de otimização é formalmente expresso como:

$$ \max_{\beta_0, \beta} \sum_{i=1}^{N} \left[ y_i(\beta_0 + \beta^T x_i) - \log(1 + e^{\beta_0 + \beta^T x_i}) \right] - \lambda \sum_{j=1}^{p} |\beta_j|\ $$ [^3]

Aqui, $y_i \in \{0, 1\}$ são as respostas para as $N$ observações, $x_i$ são os vetores de preditores, $\beta_0$ é o intercepto, $\beta$ é o vetor de coeficientes de dimensão $p$, e $\lambda \ge 0$ é o parâmetro de regularização que controla a intensidade da penalidade L1, $\sum_{j=1}^{p} |\beta_j|$ [^3].

> É prática comum não penalizar o termo de intercepto $\beta_0$ e padronizar os preditores $x_j$ para que tenham média zero e variância unitária antes de aplicar a penalidade, tornando-a assim significativa [^4].

O critério (4.31) é **côncavo** em $(\beta_0, \beta)$ [^5]. Portanto, uma solução global pode ser encontrada.

#### Algoritmos de Solução

A maximização da função objetivo (4.31) [^3] pode ser abordada por métodos de **programação não linear** (nonlinear programming) [^6]. No entanto, uma abordagem alternativa e eficiente explora as aproximações quadráticas utilizadas no algoritmo de Newton-Raphson para ajustar o modelo de regressão logística padrão (Seção 4.4.1) [^7], [^8].

Relembrando a Seção 4.4.1, o passo de Newton para a regressão logística não penalizada pode ser expresso como uma etapa de **mínimos quadrados ponderados iterativos** (Iteratively Reweighted Least Squares - IRLS) [^7]:
$\beta^{new} = (X^T W X)^{-1} X^T W z$ [^7], onde $z = X \beta^{old} + W^{-1}(y - p)$ é a resposta ajustada e $W$ é uma matriz diagonal de pesos $w_{ii} = p(x_i; \beta^{old})(1 - p(x_i; \beta^{old}))$ [^7]. Cada iteração resolve o problema de mínimos quadrados ponderados $\beta^{new} \leftarrow \arg \min_{\beta} (z - X\beta)^T W (z - X\beta)$ [^7].

Para o caso regularizado com L1, pode-se usar a mesma aproximação quadrática da log-verossimilhança em torno da estimativa atual $\beta^{old}$ [^8]. Isso permite resolver o problema (4.31) [^3] através da **aplicação repetida de um algoritmo lasso ponderado** (weighted lasso algorithm) [^8]. Em cada passo, minimiza-se uma aproximação quadrática da log-verossimilhança negativa, acrescida da penalidade L1.

As equações de score para as variáveis com coeficientes não nulos ($\beta_j \neq 0$) no problema L1 penalizado assumem uma forma interessante [^9]:

$$ \mathbf{x}_j^T (\mathbf{y} - \mathbf{p}) = \lambda \cdot \text{sign}(\beta_j)\ $$ [^9]

onde $\mathbf{x}_j$ é a $j$-ésima coluna da matriz $X$, $\mathbf{y}$ é o vetor de respostas e $\mathbf{p}$ é o vetor de probabilidades ajustadas [^9]. Esta equação generaliza a condição de score para o lasso na regressão linear (Eq. 3.58 referenciada em [^11]) e a condição de score para a regressão logística não penalizada ($\mathbf{x}_j^T (\mathbf{y} - \mathbf{p}) = 0$) [^10]. Ela indica que as variáveis ativas (com $\beta_j \neq 0$) estão ligadas na sua *correlação generalizada com os resíduos* $(\mathbf{y} - \mathbf{p})$ [^11].

#### Algoritmos de Caminho e Perfis de Coeficientes

Para o lasso na regressão linear, algoritmos de caminho como o **LAR** (Least Angle Regression) podem computar eficientemente todo o perfil dos coeficientes $\beta(\lambda)$ conforme $\lambda$ varia. No entanto, para a regressão logística L1, esses algoritmos são mais difíceis de aplicar diretamente [^12]. A razão é que os **perfis de coeficientes** $\beta_j(\lambda)$ são **piecewise smooth** (suaves por partes) em função de $\lambda$ (ou de $||\beta(\lambda)||_1$), em vez de serem piecewise linear (lineares por partes) como no lasso linear [^12].

Apesar dessa complexidade, progressos podem ser feitos utilizando as **aproximações quadráticas** mencionadas anteriormente para seguir o caminho [^13]. A Figura 4.13 [^14] ilustra um exemplo desses perfis de coeficientes para os dados de doença cardíaca da África do Sul (Seção 4.4.2), mostrando a trajetória de cada coeficiente $\beta_j$ em função da norma L1 do vetor de coeficientes, $||\beta(\lambda)||_1$. Neste exemplo específico, os perfis parecem quase lineares, mas em outros casos a curvatura pode ser mais pronunciada [^14]. O pacote R `glmpath` utiliza métodos preditor-corretor (predictor-corrector methods) de otimização convexa para identificar os valores exatos de $\lambda$ onde o conjunto ativo de coeficientes não nulos muda [^15].

#### Métodos Eficientes Baseados em Coordenadas

Métodos de **descida de coordenadas** (Coordinate descent methods), referenciados na Seção 3.8.6 [^16], provaram ser extremamente eficientes para computar os perfis de coeficientes em uma grade de valores de $\lambda$ [^16]. O pacote R `glmnet` implementa essa abordagem e pode ajustar problemas de regressão logística L1 muito grandes de forma eficiente, tanto em $N$ (número de observações) quanto em $p$ (número de preditores) [^17]. Seus algoritmos são capazes de explorar a **esparsidade** na matriz de preditores $X$, permitindo lidar com problemas ainda maiores [^17]. Embora projetado para modelos regularizados, `glmnet` também possui opções para ajustes não regularizados [^17].

### Conclusão

A regressão logística regularizada com L1 estende o framework da regressão logística padrão, incorporando a penalidade lasso para induzir esparsidade nos coeficientes. Isso resulta em um método poderoso para **seleção de variáveis** e **shrinkage**, particularmente útil em cenários de alta dimensionalidade. A otimização do critério de log-verossimilhança penalizada, que é côncavo [^5], pode ser realizada por meio de algoritmos que iterativamente resolvem problemas de lasso ponderado, baseados em aproximações quadráticas da log-verossimilhança [^8]. Embora a computação do caminho completo de regularização seja desafiadora devido aos perfis de coeficientes serem *piecewise smooth* [^12], métodos como a descida de coordenadas, implementados em pacotes como `glmnet` [^17], oferecem soluções computacionais altamente eficientes. A abordagem pode ser estendida para modelos multinomiais regularizados com L1, como mencionado brevemente na Seção 18.4 [^18].

### Referências

[^1]: Seção 4.4.4, p. 125: "The L1 penalty used in the lasso (Section 3.4.2) can be used for variable selection and shrinkage with any linear regression model. For logistic regression, we would maximize a penalized version of (4.20):"
[^2]: Seção 4.4.1, p. 120: Discussão sobre a maximização da log-verossimilhança (Eq 4.19, 4.20).
[^3]: Seção 4.4.4, p. 125: Equação (4.31), a função objetivo penalizada.
[^4]: Seção 4.4.4, p. 125: "As with the lasso, we typically do not penalize the intercept term, and standardize the predictors for the penalty to be meaningful."
[^5]: Seção 4.4.4, p. 126: "Criterion (4.31) is concave..."
[^6]: Seção 4.4.4, p. 126: "...a solution can be found using nonlinear programming methods (Koh et al., 2007, for example)."
[^7]: Seção 4.4.1, p. 120-121: Derivação do algoritmo de Newton-Raphson e sua expressão como IRLS (Eqs 4.22, 4.23, 4.25, 4.26, 4.27, 4.28).
[^8]: Seção 4.4.4, p. 126: "Alternatively, using the same quadratic approximations that were used in the Newton algorithm in Section 4.4.1, we can solve (4.31) by repeated application of a weighted lasso algorithm."
[^9]: Seção 4.4.4, p. 126: Equação (4.32), as equações de score para variáveis ativas.
[^10]: Seção 4.4.1, p. 120-121: Equações de score para regressão logística padrão (Eqs 4.21, 4.24).
[^11]: Seção 4.4.4, p. 126: "which generalizes (3.58) in Section 3.4.4; the active variables are tied in their generalized correlation with the residuals."
[^12]: Seção 4.4.4, p. 126: "Path algorithms such as LAR for lasso are more difficult, because the coefficient profiles are piecewise smooth rather than linear."
[^13]: Seção 4.4.4, p. 126: "Nevertheless, progress can be made using quadratic approximations."
[^14]: Seção 4.4.4, p. 126: Figura 4.13 e sua descrição.
[^15]: Seção 4.4.4, p. 126: Descrição do pacote `glmpath`.
[^16]: Seção 4.4.4, p. 126: "Coordinate descent methods (Section 3.8.6) are very efficient for computing the coefficient profiles on a grid of values for λ."
[^17]: Seção 4.4.4, p. 126-127: Descrição do pacote `glmnet` e suas capacidades. (Também mencionado na Seção 4.4.1, p. 121).
[^18]: Seção 4.4.5, p. 127: "See Section 18.4 for more details, and a discussion of L1-regularized multinomial models."

<!-- END -->