## Estimação de Modelos de Regressão Logística por Máxima Verossimilhança

### Introdução

Como estabelecido na Seção 4.4 [^19], o modelo de regressão logística surge do desejo de modelar as probabilidades posteriores das $K$ classes, $Pr(G=k|X=x)$, através de funções lineares em $x$, garantindo ao mesmo tempo que estas probabilidades somem um e permaneçam no intervalo $[0, 1]$. Vimos que o modelo é especificado em termos de $K-1$ transformações log-odds ou logit (Equação 4.17) [^19], resultando nas expressões para as probabilidades $p_k(x; \theta) = Pr(G=k|X=x; \theta)$ (Equação 4.18) [^19]. Para o caso de duas classes ($K=2$), o modelo simplifica-se consideravelmente, envolvendo uma única função linear, $log[p_1(x)/(1-p_1(x))] = \beta_0 + \beta^T x$ (Equação 4.2) [^2]. Este capítulo foca nos métodos de estimação dos parâmetros $\theta$ (ou $\beta$ no caso de duas classes) destes modelos. A abordagem padrão é a **Maximum Likelihood Estimation (MLE)**, utilizando a verossimilhança condicional de $G$ dado $X$ [^20]. Exploraremos o algoritmo de **Newton-Raphson** para maximizar a log-verossimilhança, sua reexpressão como um processo de **Iteratively Reweighted Least Squares (IRLS)** [^21], e discutiremos aspectos relacionados à inferência e construção de modelos, incluindo os testes de **Wald** e **Rao score** [^25].

### Conceitos Fundamentais

#### Estimação por Máxima Verossimilhança (MLE)

Os modelos de regressão logística são tipicamente ajustados por **máxima verossimilhança**, utilizando a verossimilhança condicional de $G$ dado $X$, $Pr(G|X)$ [^20]. Como $Pr(G|X)$ especifica completamente a distribuição condicional, a distribuição multinomial (ou binomial para $K=2$) é apropriada [^20]. A log-verossimilhança para $N$ observações $(x_i, g_i)$ é dada por:

$$\nl(\theta) = \sum_{i=1}^{N} \log p_{g_i}(x_i; \theta)$$ [^20]

onde $p_k(x_i; \theta) = Pr(G=k|X=x_i; \theta)$ como definido na Equação 4.18 [^19].

Vamos detalhar o caso de duas classes ($K=2$), pois os algoritmos se simplificam consideravelmente [^20]. É conveniente codificar a resposta $g_i$ através de uma variável $y_i \in \\{0, 1\\}$, onde $y_i=1$ se $g_i=1$ e $y_i=0$ se $g_i=2$ [^20]. Seja $p(x_i; \beta) = p_1(x_i; \beta) = Pr(G=1|X=x_i; \beta)$, então $p_2(x_i; \beta) = 1 - p(x_i; \beta)$. A log-verossimilhança pode ser escrita como:

$$\nl(\beta) = \sum_{i=1}^{N} \\{ y_i \log p(x_i; \beta) + (1 - y_i) \log(1 - p(x_i; \beta)) \\}$$ [^20]

Lembrando que $p(x_i; \beta) = \frac{\exp(\beta_0 + \beta^T x_i)}{1 + \exp(\beta_0 + \beta^T x_i)}$, podemos reescrever a log-verossimilhança, assumindo que o vetor de entrada $x_i$ inclui o termo constante 1 para acomodar o intercepto $\beta_0$ (portanto, $\beta$ agora tem $p+1$ componentes e $x_i$ também):

$$\nl(\beta) = \sum_{i=1}^{N} \\{ y_i \beta^T x_i - \log(1 + e^{\beta^T x_i}) \\}$$ [^20]

Para maximizar a log-verossimilhança, calculamos suas derivadas em relação a $\beta$ e igualamos a zero. Estas são as **score equations**:

$$\frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^{N} x_i (y_i - p(x_i; \beta)) = 0$$ [^20]

Estas são $p+1$ equações não lineares em $\beta$ [^20]. Note que, como o primeiro componente de $x_i$ é 1 (para o intercepto), a primeira equação de score implica $\sum_{i=1}^{N} y_i = \sum_{i=1}^{N} p(x_i; \beta)$, ou seja, o número esperado de respostas da classe 1 deve igualar o número observado [^20].

#### O Algoritmo de Newton-Raphson

Para resolver as score equations (Equação 4.21), utilizamos o algoritmo de **Newton-Raphson**, que requer a matriz das segundas derivadas, ou matriz **Hessiana** [^20]. A matriz Hessiana da log-verossimilhança é:

$$\frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T} = - \sum_{i=1}^{N} x_i x_i^T p(x_i; \beta) (1 - p(x_i; \beta))$$ [^20]

> A log-verossimilhança para o modelo logístico é côncava [^21], garantindo que o método de Newton-Raphson, se convergir, encontrará o máximo global.

Partindo de um valor inicial $\beta^{old}$, uma única atualização de Newton é dada por:

$$\beta^{new} = \beta^{old} - \left( \frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T} \right)^{-1} \frac{\partial l(\beta)}{\partial \beta}$$ [^20]

onde as derivadas são avaliadas em $\beta^{old}$.

É conveniente escrever o score e o Hessiano em notação matricial. Seja $\mathbf{y}$ o vetor de respostas $y_i$, $\mathbf{X}$ a matriz $N \times (p+1)$ de entradas $x_i^T$, $\mathbf{p}$ o vetor de probabilidades ajustadas $p(x_i; \beta^{old})$, e $\mathbf{W}$ uma matriz diagonal $N \times N$ com elementos $w_{ii} = p(x_i; \beta^{old})(1 - p(x_i; \beta^{old}))$ [^21]. Então, as score equations e a matriz Hessiana podem ser escritas como:

$$\frac{\partial l(\beta)}{\partial \beta} = \mathbf{X}^T (\mathbf{y} - \mathbf{p})$$ [^21]

$$\frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T} = - \mathbf{X}^T \mathbf{W} \mathbf{X}$$ [^21]

Substituindo estas expressões na atualização de Newton (Equação 4.23), obtemos:

$$\beta^{new} = \beta^{old} + (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^T (\mathbf{y} - \mathbf{p})$$ [^21]

#### Iteratively Reweighted Least Squares (IRLS)

A etapa de atualização de Newton (Equação 4.26) pode ser reexpressa de uma forma reveladora. Manipulando a equação:

$$\beta^{new} = (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^T \mathbf{W} (\mathbf{X} \beta^{old} + \mathbf{W}^{-1} (\mathbf{y} - \mathbf{p}))$$ [^21]

Definindo a **adjusted response** (resposta ajustada) como:

$$\mathbf{z} = \mathbf{X} \beta^{old} + \mathbf{W}^{-1} (\mathbf{y} - \mathbf{p})$$ [^21]

A atualização de Newton torna-se:

$$\beta^{new} = (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^T \mathbf{W} \mathbf{z}$$ [^21]

Esta é precisamente a solução para um problema de **weighted least squares (WLS)** da resposta $\mathbf{z}$ nos preditores $\mathbf{X}$ com a matriz de pesos diagonal $\mathbf{W}$ [^21]. Especificamente, $\beta^{new}$ minimiza:

$$(\mathbf{z} - \mathbf{X}\beta)^T \mathbf{W} (\mathbf{z} - \mathbf{X}\beta)$$ [^21]

Como o vetor de probabilidades $\mathbf{p}$, e consequentemente a matriz de pesos $\mathbf{W}$ e a resposta ajustada $\mathbf{z}$, dependem de $\beta^{old}$, esta etapa de WLS deve ser repetida iterativamente [^21]. Este algoritmo é, portanto, conhecido como **Iteratively Reweighted Least Squares (IRLS)** [^21].

Um bom valor inicial para $\beta$ é $\beta = 0$ [^21]. A convergência não é garantida, mas tipicamente ocorre, pois a log-verossimilhança é côncava [^21]. Em casos raros onde a log-verossimilhança diminui (overshooting), a redução do tamanho do passo (step size halving) pode garantir a convergência [^21]. Para o caso multiclasse ($K>3$), o algoritmo de Newton também pode ser expresso como um IRLS, mas envolve um vetor de $K-1$ respostas e uma matriz de pesos não diagonal por observação, o que o torna menos conveniente computacionalmente [^21]. Alternativas como métodos de coordinate-descent podem ser mais eficientes [^21].

#### Inferência e Construção de Modelos

A conexão com WLS é útil para além do algoritmo de estimação. A teoria assintótica da máxima verossimilhança estabelece que, se o modelo estiver correto, o estimador $\hat{\beta}$ é consistente (converge para o verdadeiro $\beta$) [^25]. Além disso, um teorema do limite central mostra que a distribuição assintótica de $\hat{\beta}$ é Normal multivariada:

> **Distribuição Assintótica de $\hat{\beta}$:**
> $\hat{\beta} \xrightarrow{d} N(\beta, (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1})$ [^25]

Esta distribuição pode ser derivada diretamente do ajuste por WLS, mimetizando a inferência da teoria normal [^25]. A matriz de covariância estimada $(\mathbf{X}^T \hat{\mathbf{W}} \mathbf{X})^{-1}$ é frequentemente utilizada para calcular erros padrão para os coeficientes $\hat{\beta}_j$. O **Wald test** é comumente usado para testar a significância de um coeficiente (i.e., testar $H_0: \beta_j = 0$) [^22, ^25]. Este teste baseia-se na estatística Z (Z score), que é o coeficiente estimado dividido pelo seu erro padrão [^22]. Um Z score com valor absoluto maior que aproximadamente 2 é considerado significativo ao nível de 5% [^22]. Este teste é usado para a exclusão de termos do modelo [^25]. Por exemplo, na análise de dados de doenças cardíacas da África do Sul (Tabela 4.2), os Z scores são apresentados, e o teste de Wald sugere quais variáveis podem não ser significativas na presença das outras [^22].

A construção de modelos em regressão logística pode ser custosa, pois cada modelo ajustado requer iteração (via IRLS) [^25]. Existem atalhos populares para avaliar a inclusão ou exclusão de termos sem a necessidade de reajustar completamente o modelo [^25]:

1.  O **Rao score test**: Testa a inclusão de um termo no modelo atual [^25].
2.  O **Wald test**: Como mencionado, testa a exclusão de um termo do modelo atual [^25].

Ambos os testes podem ser vistos como adicionar ou remover um termo do ajuste por WLS usando os mesmos pesos ($\mathbf{W}$) do modelo atual, permitindo cálculos eficientes [^25]. Outra ferramenta importante é a análise de deviance, que compara a log-verossimilhança de modelos aninhados [^24]. A deviance residual de um modelo ajustado é menos duas vezes a sua log-verossimilhança [^24]. A diferença nas deviances entre dois modelos aninhados segue aproximadamente uma distribuição qui-quadrado sob a hipótese nula de que o modelo menor é adequado.

Implementações de software, como a função `glm` no R, exploram totalmente essas conexões, permitindo que objetos de modelos lineares generalizados (incluindo regressão logística) sejam tratados de forma semelhante a objetos de modelos lineares [^25].

### Conclusão

A estimação de parâmetros em modelos de regressão logística é realizada predominantemente via **Maximum Likelihood Estimation (MLE)**, maximizando a log-verossimilhança condicional. O algoritmo de **Newton-Raphson** fornece a base computacional, que pode ser elegantemente interpretado como um processo de **Iteratively Reweighted Least Squares (IRLS)**. Esta conexão com WLS não só facilita a implementação, mas também fornece a base para a inferência assintótica sobre os coeficientes, incluindo o cálculo de erros padrão e a aplicação do **Wald test**. Dada a natureza iterativa do ajuste, a construção de modelos pode ser computacionalmente intensiva. Testes como o **Wald test** (para exclusão) e o **Rao score test** (para inclusão) oferecem atalhos eficientes baseados na aproximação WLS. Em geral, a regressão logística é considerada uma ferramenta robusta e segura para classificação e inferência, dependendo de menos suposições do que métodos como LDA, especialmente quando as suposições de LDA (como normalidade e covariâncias iguais) não são satisfeitas [^28].

### Referências

[^1]: Page 101: Introduction to Linear Methods for Classification.
[^2]: Page 102: Definition of the two-class logistic model and logit transformation (Eqs 4.1, 4.2).
[^3]: Page 103: Linear Regression of an Indicator Matrix.
[^4]: Page 104: Rationale and limitations of linear regression for classification.
[^5]: Page 105: Masking problem in linear regression for K>3 classes.
[^6]: Page 106: Example of masking and comparison with LDA.
[^7]: Page 107: Table 4.1 showing error rates, including logistic regression. Introduction to LDA.
[^8]: Page 108: LDA derivation assuming Gaussian classes with common covariance. Linear log-odds (Eq 4.9).
[^9]: Page 109: LDA discriminant functions (Eq 4.10) and parameter estimation.
[^10]: Page 110: Connection between LDA and least squares for two classes. QDA definition (Eq 4.12).
[^11]: Page 111: Comparison of QDA and LDA in expanded space. Discussion on model stability (bias-variance).
[^12]: Page 112: Regularized Discriminant Analysis (RDA) (Eq 4.13).
[^13]: Page 113: Computations for LDA/QDA via diagonalization. Reduced-Rank LDA.
[^14]: Page 114: Optimal subspaces for LDA (Fisher's problem).
[^15]: Page 115: Canonical variates plots.
[^16]: Page 116: Fisher's problem as maximizing Rayleigh quotient (Eq 4.15). Discriminant coordinates / canonical variates.
[^17]: Page 117: Dimension reduction in LDA for classification. Role of prior probabilities $\pi_k$.
[^18]: Page 118: Decision boundaries in reduced LDA space.
[^19]: Page 119: Formal definition of Logistic Regression for K classes (Eqs 4.17, 4.18).
[^20]: Page 120: Section 4.4.1 Fitting Logistic Regression Models. MLE using conditional likelihood (Eq 4.19). Two-class log-likelihood (Eq 4.20). Score equations (Eq 4.21). Newton-Raphson and Hessian (Eq 4.22). Newton update step (Eq 4.23).
[^21]: Page 121: Matrix form of score and Hessian (Eqs 4.24, 4.25). Matrix Newton step (Eq 4.26). Re-expression as WLS. Adjusted response z (Eq 4.27). IRLS algorithm and WLS minimization (Eq 4.28). Convergence discussion. Multiclass case and alternatives.
[^22]: Page 122: Example: South African Heart Disease. Table 4.2 with Z scores (Wald test). Discussion on model selection and interpretation.
[^23]: Page 123: Scatterplot matrix for the example.
[^24]: Page 124: Stepwise model results (Table 4.3). Mention of analysis of deviance. Interpretation of coefficients (odds ratios). Section 4.4.3 Quadratic Approximations and Inference. Self-consistency relation (Eq 4.29).
[^25]: Page 125: Weights $w_i$ for IRLS. Pearson chi-square (Eq 4.30). Asymptotic theory: consistency and normality of $\hat{\beta}$. Cost of model building. Rao score test and Wald test as shortcuts. Software implementations (GLM).
[^26]: Page 126: L1 Regularized Logistic Regression (Eq 4.31, 4.32). Path algorithms.
[^27]: Page 127: Section 4.4.5 Logistic Regression or LDA? Comparison of model forms (Eqs 4.33, 4.34). Conditional vs Full Likelihood. Role of marginal density Pr(X).
[^28]: Page 128: Efficiency comparison LDA vs Logistic Regression. Robustness considerations. Behavior with perfect separation. Logistic regression as safer/more robust.
[^29]: Page 129: Section 4.5 Separating Hyperplanes.
[^30]: Page 130: Linear algebra of hyperplanes.
[^31]: Page 131: Perceptron Learning Algorithm.
[^32]: Page 132: Optimal Separating Hyperplanes (Margin Maximization).
[^33]: Page 133: Lagrange dual for optimal hyperplane (Eq 4.52). Support points.
[^34]: Page 134: Geometric interpretation of optimal hyperplane. Comparison with logistic regression.
[^35]: Page 135: Bibliographic Notes and Exercises.
[^36]: Page 136: Exercises.
[^37]: Page 137: Exercises.

<!-- END -->