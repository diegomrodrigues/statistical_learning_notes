## Comparação entre Regressão Logística e Análise Discriminante Linear: Generalidade e Suposições

### Introdução

No domínio dos métodos lineares para classificação, a **Análise Discriminante Linear (LDA)** e a **Regressão Logística (LR)** emergem como duas abordagens fundamentais, embora distintas, para estimar fronteiras de decisão lineares [^1]. Ambas as técnicas podem levar a funções log-odds ou logits que são lineares nas variáveis preditoras $x$, resultando em fronteiras de decisão hiperplanares [^1, ^6, ^18]. Contudo, a semelhança na forma final das suas funções discriminantes mascara diferenças cruciais nas suas derivações, suposições subjacentes e, consequentemente, na generalidade e robustez dos modelos resultantes. A diferença essencial reside na forma como a função linear é ajustada aos dados de treino [^1]. Este capítulo foca-se na análise comparativa detalhada destas duas metodologias, com ênfase na afirmação de que a regressão logística é mais geral por impor menos restrições sobre a distribuição dos dados, modelando diretamente as probabilidades posteriores $Pr(G|X)$, enquanto a LDA modela a densidade conjunta $Pr(X, G)$ sob suposições específicas sobre as densidades das classes [^20].

### Conceitos Fundamentais

#### Análise Discriminante Linear (LDA)

A LDA aborda o problema de classificação através da teoria de decisão Bayesiana [^2]. O objetivo é modelar as probabilidades posteriores $Pr(G=k|X=x)$, que são necessárias para uma classificação ótima. A LDA alcança isso modelando as densidades condicionais de classe $f_k(x) = Pr(X=x|G=k)$ e as probabilidades a priori das classes $\pi_k = Pr(G=k)$, com $\sum_{k=1}^{K} \pi_k = 1$ [^2]. A regra de Bayes é então aplicada para obter as probabilidades posteriores:
$$ Pr(G=k|X=x) = \frac{f_k(x)\pi_k}{\sum_{l=1}^{K} f_l(x)\pi_l} $$ [^2]
A suposição central da LDA é que cada densidade condicional de classe $f_k(x)$ segue uma distribuição Gaussiana multivariada [^3, ^4]:
$$ f_k(x) = \frac{1}{(2\pi)^{p/2} |\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k)\right) $$ [^4]
Crucialmente, a LDA assume adicionalmente que todas as classes partilham uma **matriz de covariância comum**, i.e., $\Sigma_k = \Sigma$ para todo $k=1, \dots, K$ [^5]. Esta suposição leva a cancelamentos significativos ao calcular o log-ratio das probabilidades posteriores entre duas classes $k$ e $l$:
$$ \log \frac{Pr(G=k|X=x)}{Pr(G=l|X=x)} = \log \frac{f_k(x)}{f_l(x)} + \log \frac{\pi_k}{\pi_l} $$
$$ = \log \frac{\pi_k}{\pi_l} - \frac{1}{2}(\mu_k+\mu_l)^T \Sigma^{-1} (\mu_k-\mu_l) + x^T \Sigma^{-1}(\mu_k-\mu_l) $$ [^6]
Esta equação demonstra que, sob as suposições da LDA, os log-odds posteriores são de facto uma **função linear de $x$** [^6, ^17]. A forma resultante pode ser escrita como $\alpha_{k0} + \alpha_k^T x$ [^17]. As fronteiras de decisão, onde $Pr(G=k|X=x) = Pr(G=l|X=x)$, são, portanto, hiperplanos em $p$ dimensões [^6].

A estimação dos parâmetros na LDA ($\mu_k, \Sigma, \pi_k$) é tipicamente realizada por máxima verossimilhança, baseada na **densidade conjunta** $Pr(X, G=k)$ [^23]:
$$ Pr(X, G=k) = f_k(x) \pi_k = \phi(X; \mu_k, \Sigma) \pi_k $$ [^23]
onde $\phi$ denota a função de densidade Gaussiana. Este procedimento de estimação utiliza a informação completa da distribuição conjunta. Note-se que a densidade marginal $Pr(X)$ implícita neste modelo é uma mistura de Gaussianas:
$$ Pr(X) = \sum_{k=1}^{K} \pi_k \phi(X; \mu_k, \Sigma) $$ [^24]
Esta densidade marginal, embora não utilizada diretamente na classificação após a estimação dos parâmetros, desempenha um papel na estimação através da verossimilhança total [^24].

#### Regressão Logística (LR)

Em contraste com a LDA, o modelo de regressão logística é derivado do desejo de modelar as probabilidades posteriores $Pr(G=k|X=x)$ **diretamente** como funções de $x$, garantindo simultaneamente que estas probabilidades somem um e permaneçam no intervalo $[0, 1]$ [^10]. O modelo assume que os log-odds (ou logits) das probabilidades posteriores são funções lineares de $x$. Para $K$ classes, tomando a classe $K$ como referência, o modelo postula:
$$ \log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \beta_{k0} + \beta_k^T x, \quad \text{para } k=1, \dots, K-1 $$ [^11, ^18]
Esta formulação define diretamente a relação entre os preditores e os log-odds, sem fazer suposições sobre a distribuição de $X$ dentro de cada classe ou sobre a distribuição marginal de $X$. A partir desta definição, as probabilidades posteriores podem ser expressas como:
$$ Pr(G=k|X=x) = \frac{\exp(\beta_{k0} + \beta_k^T x)}{1 + \sum_{l=1}^{K-1} \exp(\beta_{l0} + \beta_l^T x)}, \quad \text{para } k=1, \dots, K-1 $$ [^12]
$$ Pr(G=K|X=x) = \frac{1}{1 + \sum_{l=1}^{K-1} \exp(\beta_{l0} + \beta_l^T x)} $$ [^12]
Estas probabilidades, $p_k(x; \theta)$, onde $\theta$ representa o conjunto completo de parâmetros $\\{\beta_{k0}, \beta_k\\}_{k=1}^{K-1}$, somam um por construção [^12].

A estimação dos parâmetros $\beta_{k0}, \beta_k$ na regressão logística é realizada através da maximização da **verossimilhança condicional** de $G$ dado $X$ [^13]. Assumindo $N$ observações independentes $(x_i, g_i)$, a log-verossimilhança condicional é:
$$ l(\theta) = \sum_{i=1}^{N} \log p_{g_i}(x_i; \theta) $$ [^13]
Esta abordagem de estimação foca-se exclusivamente na relação condicional $Pr(G|X)$ e ignora completamente a distribuição marginal $Pr(X)$ [^22].

> *A regressão logística deixa a densidade marginal de X, $Pr(X)$, como uma função de densidade arbitrária [...] e ajusta os parâmetros de $Pr(G|X)$ maximizando a verossimilhança condicional [...]. Embora $Pr(X)$ seja totalmente ignorado, podemos pensar nesta densidade marginal como sendo estimada de uma forma totalmente não paramétrica e irrestrita, usando a função de distribuição empírica que coloca massa $1/N$ em cada observação $x_i$.* [^22]

#### Comparação Direta: Generalidade e Suposições

A comparação entre LDA e LR revela pontos cruciais sobre suas suposições e generalidade:

1.  **Forma do Modelo:** Ambos os modelos resultam em log-odds posteriores que são lineares em $x$ [^1, ^19]. As expressões $ \log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \alpha_{k0} + \alpha_k^T x $ (derivada da LDA) [^17] e $ \log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \beta_{k0} + \beta_k^T x $ (postulada pela LR) [^18] têm a mesma forma funcional.

2.  **Suposições:** A diferença fundamental reside nas suposições feitas para chegar a esta forma linear. A LDA faz suposições fortes sobre a distribuição dos dados: $X|G=k$ é Gaussiana e a matriz de covariância $\Sigma$ é comum a todas as classes [^5, ^23]. A regressão logística, por outro lado, não faz suposições sobre a distribuição de $X|G=k$ ou sobre a distribuição marginal $Pr(X)$; ela assume diretamente a linearidade dos logits [^20, ^22].

> *O modelo de regressão logística é mais geral, na medida em que faz menos suposições.* [^20]

3.  **Estimação:** A diferença nas suposições reflete-se nos procedimentos de estimação. A LDA maximiza a verossimilhança da *distribuição conjunta* $Pr(X, G)$, utilizando assim informações sobre a estrutura de $X$ (via $\phi(X; \mu_k, \Sigma)$) [^23]. A LR maximiza a verossimilhança *condicional* $Pr(G|X)$, ignorando a estrutura de $Pr(X)$ [^13, ^22]. Os coeficientes lineares $\alpha_k$ (LDA) e $\beta_k$ (LR) são, portanto, estimados de maneiras diferentes, mesmo que a forma do modelo seja a mesma [^19].

4.  **Eficiência vs. Robustez:** As suposições adicionais da LDA (Gaussianidade, covariância comum) podem levar a estimativas dos parâmetros mais eficientes (menor variância) se essas suposições forem verdadeiras ou aproximadamente verdadeiras [^25]. No entanto, se as suposições forem violadas (e.g., distribuições não Gaussianas, presença de outliers), o desempenho da LDA pode degradar-se. A LDA não é robusta a outliers grosseiros, pois estes podem influenciar indevidamente a estimação da média e, especialmente, da matriz de covariância comum [^26]. A regressão logística, fazendo menos suposições, é geralmente considerada uma aposta mais segura e **robusta** [^27]. Ela tende a ser menos afetada por violações das suposições sobre a distribuição dos preditores.

5.  **Dados Separáveis:** Em situações onde os dados são perfeitamente separáveis por um hiperplano, as estimativas de máxima verossimilhança para os parâmetros da regressão logística tendem ao infinito (são indefinidas) [^29, Exercise 4.5]. Os coeficientes da LDA, no entanto, permanecem bem definidos neste cenário, pois a verossimilhança marginal (que envolve $\Sigma$) impede essas degenerescências [^28].

6.  **Desempenho Prático:** Apesar das diferenças teóricas, a experiência prática sugere que os dois modelos frequentemente fornecem resultados muito semelhantes, mesmo quando as suposições da LDA são violadas, como no caso de preditores qualitativos [^28]. A estabilidade das fronteiras de decisão lineares ou quadráticas fornecidas por LDA/QDA, mesmo quando as suposições Gaussianas não se sustentam, pode ser vista como um trade-off viés-variância: aceita-se o viés de uma fronteira simples em troca de uma menor variância na estimação em comparação com métodos não paramétricos mais flexíveis [^9].

### Conclusão

Em suma, enquanto a Análise Discriminante Linear e a Regressão Logística podem ambas produzir classificadores com fronteiras de decisão lineares, elas chegam a este resultado através de caminhos conceptuais e suposicionais distintos. A LDA fundamenta-se em suposições fortes sobre a distribuição conjunta de $X$ e $G$, especificamente que as densidades condicionais de classe são Gaussianas com uma matriz de covariância comum. A Regressão Logística, em contraste, modela diretamente a probabilidade condicional $Pr(G|X)$ assumindo uma forma linear para os logits, sem impor restrições sobre a distribuição dos preditores. Esta abordagem confere à LR uma **maior generalidade e robustez** [^20, ^27], tornando-a uma escolha preferível quando há dúvidas sobre a validade das suposições Gaussianas ou na presença de outliers. No entanto, a LDA pode oferecer estimativas mais eficientes se as suas suposições forem aproximadamente satisfeitas [^25]. A escolha entre os dois métodos pode depender do conhecimento prévio sobre os dados, da robustez desejada e, em alguns casos, da análise comparativa do desempenho em dados específicos, embora na prática suas performances sejam frequentemente similares [^28].

### Referências

[^1]: [Page 102] We discuss two very popular but different methods that result in linear log-odds or logits: linear discriminant analysis and linear logistic regression. Although they differ in their derivation, the essential difference between them is in the way the linear function is fit to the training data.
[^2]: [Page 106] Decision theory for classification (Section 2.4) tells us that we need to know the class posteriors Pr(G|X) for optimal classification. Suppose fk(x) is the class-conditional density of X in class G = k, and let πk be the prior probability of class k, with Σk=1 K πk = 1. A simple application of Bayes theorem gives us Pr(G = k|X = x) = fk(x)πk / Σl=1 K fl(x)πl (4.7).
[^3]: [Page 106] Many techniques are based on models for the class densities: linear and quadratic discriminant analysis use Gaussian densities;
[^4]: [Page 108] Suppose that we model each class density as multivariate Gaussian fk(x) = ... (4.8).
[^5]: [Page 108] Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Σk = Σ ∀k.
[^6]: [Page 108] In comparing two classes k and l, it is sufficient to look at the log-ratio, and we see that log[Pr(G=k|X=x)/Pr(G=l|X=x)] = ... + xT Σ⁻¹(μk - μl), an equation linear in x (4.9). This linear log-odds function implies that the decision boundary between classes k and l ... is linear in x; in p dimensions a hyperplane.
[^7]: [Page 110] Getting back to the general discriminant problem (4.8), if the Σk are not assumed to be equal, then the convenient cancellations in (4.9) do not occur; in particular the pieces quadratic in x remain. We then get quadratic discriminant functions (QDA), δk(x) = ... (4.12).
[^8]: [Page 110] With more than two classes, LDA is not the same as linear regression of the class indicator matrix, and it avoids the masking problems associated with that approach.
[^9]: [Page 111] More likely a reason is that the data can only support simple decision boundaries such as linear or quadratic, and the estimates provided via the Gaussian models are stable. This is a bias variance tradeoff—we can put up with the bias of a linear decision boundary because it can be estimated with much lower variance than more exotic alternatives.
[^10]: [Page 119] The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0, 1].
[^11]: [Page 119] The model has the form log[Pr(G=k|X=x)/Pr(G=K|X=x)] = βk0 + βkT x (4.17).
[^12]: [Page 119] A simple calculation shows that Pr(G=k|X=x) = exp(βk0 + βkT x) / [1 + Σl=1 K-1 exp(βl0 + βlT x)], k=1,...,K-1, and Pr(G=K|X=x) = 1 / [1 + Σl=1 K-1 exp(βl0 + βlT x)] (4.18), and they clearly sum to one.
[^13]: [Page 120] Logistic regression models are usually fit by maximum likelihood, using the conditional likelihood of G given X. Since Pr(G|X) completely specifies the conditional distribution, the multinomial distribution is appropriate. The log-likelihood for N observations is l(θ) = Σi=1 N log pgi(xi; θ) (4.19).
[^14]: [Page 120] To maximize the log-likelihood, we set its derivatives to zero. These score equations are ∂l(β)/∂β = Σi=1 N xi(yi - p(xi; β)) = 0 (4.21).
[^15]: [Page 121] This algorithm is referred to as iteratively reweighted least squares or IRLS...
[^16]: [Page 121] Logistic regression models are used mostly as a data analysis and inference tool...
[^17]: [Page 127] In Section 4.3 we find that the log-posterior odds between class k and K are linear functions of x (4.9): log[Pr(G=k|X=x)/Pr(G=K|X=x)] = ... = αk0 + αkT x (4.33).
[^18]: [Page 127] The linear logistic model (4.17) by construction has linear logits: log[Pr(G=k|X=x)/Pr(G=K|X=x)] = βk0 + βkT x (4.34).
[^19]: [Page 127] It seems that the models are the same. Although they have exactly the same form, the difference lies in the way the linear coefficients are estimated.
[^20]: [Page 127] The logistic regression model is more general, in that it makes less assumptions.
[^21]: [Page 127] We can write the joint density of X and G as Pr(X, G = k) = Pr(X)Pr(G = k|X) (4.35).
[^22]: [Page 127] The logistic regression model leaves the marginal density of X as an arbitrary density function Pr(X), and fits the parameters of Pr(G|X) by maximizing the conditional likelihood [...]. Although Pr(X) is totally ignored, we can think of this marginal density as being estimated in a fully nonparametric and unrestricted fashion, using the empirical distribution function which places mass 1/N at each observation.
[^23]: [Page 127] With LDA we fit the parameters by maximizing the full log-likelihood, based on the joint density Pr(X, G = k) = φ(X; μk, Σ)πk (4.37).
[^24]: [Page 128] However, unlike in the conditional case, the marginal density Pr(X) does play a role here. It is a mixture density Pr(X) = Σk=1 K πk φ(X; μk, Σ) (4.38), which also involves the parameters.
[^25]: [Page 128] What role can this additional component/restriction play? By relying on the additional model assumptions, we have more information about the parameters, and hence can estimate them more efficiently (lower variance).
[^26]: [Page 128] For example, observations far from the decision boundary [...] play a role in estimating the common covariance matrix. This is not all good news, because it also means that LDA is not robust to gross outliers.
[^27]: [Page 128] It is generally felt that logistic regression is a safer, more robust bet than the LDA model, relying on fewer assumptions.
[^28]: [Page 128] It is our experience that the models give very similar results, even when LDA is used inappropriately, such as with qualitative predictors.
[^29]: [Page 134] When a separating hyperplane exists, logistic regression will always find it, since the log-likelihood can be driven to 0 in this case (Exercise 4.5).

<!-- END -->