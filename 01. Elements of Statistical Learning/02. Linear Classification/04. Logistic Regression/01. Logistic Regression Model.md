## Modelo de Regressão Logística para Probabilidades Posteriores de Classe

### Introdução

No contexto dos métodos lineares para classificação, a **regressão logística** (*logistic regression*) emerge como uma abordagem fundamental. Conforme discutido em secções anteriores sobre métodos lineares [^context_intro_ref], o objetivo é modelar a relação entre as variáveis preditoras $x$ e uma resposta categórica $G$. A regressão logística aborda este problema modelando diretamente as **probabilidades posteriores** (*posterior probabilities*) das $K$ classes, $Pr(G=k|X=x)$ [^4]. Uma característica central deste modelo é a sua capacidade de garantir que estas probabilidades estimadas permaneçam no intervalo [0,1] e somem um, restrições essenciais para qualquer modelo probabilístico [^4]. Para alcançar isso, o modelo utiliza funções lineares em $x$, mas aplicadas a uma transformação das probabilidades, nomeadamente a transformação **log-odds** ou **logit** [^4], [^5].

### Conceitos Fundamentais

#### Formulação do Modelo para K Classes

A regressão logística especifica a relação entre as probabilidades posteriores e os preditores $x$ através de $K-1$ transformações logit, refletindo a restrição de que as probabilidades devem somar um [^6]. Tipicamente, uma das classes é escolhida como referência (ou *baseline*). Embora o modelo frequentemente utilize a última classe, $K$, como denominador nas razões de probabilidade (*odds-ratios*), a escolha desta classe de referência é arbitrária, e as estimativas resultantes são equivariantes em relação a essa escolha [^7].

A forma matemática do modelo é dada por [^5]:
$$\
\log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \beta_{k0} + \beta_k^T x, \quad \text{para } k=1, \dots, K-1
$$
Aqui, $\beta_{k0}$ representa o intercepto e $\beta_k$ é o vetor de coeficientes para a classe $k$, relativos à classe $K$. Estas equações definem os log-odds de pertencer à classe $k$ versus a classe $K$ como uma função linear de $x$.

A partir destas relações de log-odds, podemos derivar explicitamente as probabilidades posteriores para cada classe. Uma manipulação algébrica simples leva às seguintes expressões [^8]:
$$\
Pr(G=k|X=x) = \frac{\exp(\beta_{k0} + \beta_k^T x)}{1 + \sum_{l=1}^{K-1} \exp(\beta_{l0} + \beta_l^T x)}, \quad \text{para } k=1, \dots, K-1
$$
e para a classe de referência $K$:
$$\
Pr(G=K|X=x) = \frac{1}{1 + \sum_{l=1}^{K-1} \exp(\beta_{l0} + \beta_l^T x)}
$$
É fácil verificar que estas probabilidades são não-negativas e somam um para qualquer valor de $x$ e dos parâmetros $\theta = \{\beta_{10}, \beta_1^T, \dots, \beta_{(K-1)0}, \beta_{K-1}^T\}$ [^8]. A dependência das probabilidades em todo o conjunto de parâmetros $\theta$ é por vezes denotada como $p_k(x; \theta)$ [^8].

#### Caso Binário (K=2)

O modelo de regressão logística simplifica-se consideravelmente quando existem apenas duas classes ($K=2$) [^8]. Neste cenário, necessitamos apenas de uma função linear para modelar os log-odds da classe 1 em relação à classe 2 (ou vice-versa). A formulação geral reduz-se a:
$$\
\log \frac{Pr(G=1|X=x)}{Pr(G=2|X=x)} = \beta_{0} + \beta^T x
$$
onde $\beta_0$ (anteriormente $\beta_{10}$) e $\beta$ (anteriormente $\beta_1$) são o intercepto e o vetor de coeficientes, respetivamente. Esta transformação é conhecida como **logit**, definida como $\text{logit}(p) = \log[p/(1-p)]$ [^2].

As probabilidades posteriores para as duas classes são então dadas por [^1]:
$$\
Pr(G=1|X=x) = p(x; \beta) = \frac{\exp(\beta_{0} + \beta^T x)}{1 + \exp(\beta_{0} + \beta^T x)}
$$
$$\
Pr(G=2|X=x) = 1 - p(x; \beta) = \frac{1}{1 + \exp(\beta_{0} + \beta^T x)}
$$
Esta forma do modelo é extremamente utilizada, especialmente em aplicações bioestatísticas onde respostas binárias são frequentes [^8]. Exemplos incluem modelar a probabilidade de sobrevivência versus morte de pacientes, a presença ou ausência de doença cardíaca, ou a ocorrência ou não de uma determinada condição [^8].

#### Relação com Fronteiras de Decisão Lineares

Uma consequência direta da linearidade dos log-odds (ou logits) é que as **fronteiras de decisão** (*decision boundaries*) induzidas pela regressão logística são lineares no espaço dos preditores $x$. A fronteira de decisão entre duas classes quaisquer, $k$ e $l$, é o conjunto de pontos $x$ para os quais $Pr(G=k|X=x) = Pr(G=l|X=x)$. Isto equivale a verificar onde os log-odds relativos são iguais, ou seja:
$$\
\log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \log \frac{Pr(G=l|X=x)}{Pr(G=K|X=x)}
$$
Substituindo as expressões do modelo:
$$\
\beta_{k0} + \beta_k^T x = \beta_{l0} + \beta_l^T x
$$
Rearranjando os termos, obtemos:
$$\
(\beta_{k0} - \beta_{l0}) + (\beta_k - \beta_l)^T x = 0
$$
Esta é a equação de um hiperplano (ou um conjunto afim) no espaço $\mathbb{R}^p$ [^context_intro_ref]. No caso binário ($K=2$), a fronteira de decisão ocorre onde os log-odds são zero, $Pr(G=1|X=x) = Pr(G=2|X=x) = 0.5$, o que corresponde diretamente ao hiperplano definido por $\beta_0 + \beta^T x = 0$ [^2]. Assim, apesar de modelar probabilidades de forma não-linear, a regressão logística pertence à classe de métodos que produzem fronteiras de decisão lineares.

### Conclusão

A regressão logística fornece um quadro poderoso e flexível para modelar probabilidades posteriores de classe em problemas de classificação. Ao utilizar transformações **logit** de funções lineares dos preditores, garante que as probabilidades resultantes sejam válidas (entre 0 e 1, somando 1) [^4]. O modelo é especificado em termos de $K-1$ funções logit para problemas de $K$ classes, simplificando-se para uma única função linear no caso binário, amplamente utilizado [^8]. As fronteiras de decisão resultantes são lineares [^2], colocando a regressão logística na família de classificadores lineares.

Comparativamente a outros métodos lineares como a Análise Discriminante Linear (LDA), a regressão logística partilha a forma linear dos logit, mas difere fundamentalmente nas suas suposições e método de estimação [^27], [^28]. Enquanto a LDA assume densidades Gaussianas para cada classe com uma matriz de covariância comum e utiliza a *maximum likelihood* sobre a distribuição conjunta $Pr(X, G)$ [^29], a regressão logística faz menos suposições, modelando diretamente $Pr(G|X)$ através da *conditional maximum likelihood* e não fazendo suposições sobre a distribuição marginal $Pr(X)$ [^28]. Esta característica torna a regressão logística, em geral, mais robusta a desvios das suposições da LDA, sendo frequentemente considerada uma aposta mais segura (*safer bet*), especialmente quando algumas variáveis preditoras são qualitativas ou as distribuições das classes não são Gaussianas [^31].

### Referências

[^1]: Página 102, Equação (4.1).
[^2]: Página 102, Equação (4.2) e texto circundante.
[^3]: Página 107, Tabela 4.1.
[^4]: Página 119, Primeiro parágrafo da Seção 4.4.
[^5]: Página 119, Equação (4.17).
[^6]: Página 119, Texto após Equação (4.17).
[^7]: Página 119, Texto após Equação (4.17).
[^8]: Página 119, Equação (4.18) e texto subsequente.
[^9]: Página 120, Seção 4.4.1, Equação (4.19).
[^10]: Página 120, Equação (4.20).
[^11]: Página 120, Equação (4.21).
[^12]: Página 120, Equação (4.22).
[^13]: Página 121, Equações (4.24), (4.25).
[^14]: Página 121, Equação (4.26).
[^15]: Página 121, Equação (4.27).
[^16]: Página 121, Equação (4.28).
[^17]: Página 121, Texto após Equação (4.28).
[^18]: Página 122, Seção 4.4.2 e Tabela 4.2.
[^19]: Página 124, Discussão da interpretação de coeficientes.
[^20]: Página 124, Seção 4.4.3, Equação (4.29).
[^21]: Página 125, Primeiro e segundo bullet points. Equação (4.30).
[^22]: Página 125, Terceiro bullet point.
[^23]: Página 125, Quarto bullet point.
[^24]: Página 125, Seção 4.4.4, Equação (4.31).
[^25]: Página 126, Discussão e Figura 4.13.
[^26]: Página 127, Seção 4.4.5, Equação (4.33).
[^27]: Página 127, Equação (4.34).
[^28]: Página 127, Discussão sobre estimação e Pr(X). Equação (4.36).
[^29]: Página 127, Equação (4.37).
[^30]: Página 128, Discussão sobre eficiência e robustez.
[^31]: Página 128, Discussão sobre separabilidade e robustez geral.
[^32]: Página 134, Discussão sobre solução de regressão logística em caso de separabilidade.
[^context_intro_ref]: Página 101, Seção 4.1.

<!-- END -->