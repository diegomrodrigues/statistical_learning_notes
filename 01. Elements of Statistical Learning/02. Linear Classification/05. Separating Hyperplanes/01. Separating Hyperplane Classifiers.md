## Classificadores por Hiperplano Separador

### Introdução

Como vimos nas seções anteriores sobre Análise Discriminante Linear (LDA) [^17] e Regressão Logística [^49], ambos os métodos estimam fronteiras de decisão lineares, embora de maneiras ligeiramente diferentes [^62]. Neste segmento final do capítulo, descrevemos uma classe de procedimentos conhecidos como **classificadores por hiperplano separador** [^63]. Estas abordagens constroem fronteiras de decisão lineares que tentam explicitamente separar os dados em classes distintas da melhor forma possível [^63]. Tais métodos fornecem a base para os classificadores de vetores de suporte (support vector classifiers), que serão discutidos em detalhe no Capítulo 12 [^64].

Considere a Figura 4.14 [^66], que ilustra um conjunto de dados bidimensional ($p=2$) com duas classes linearmente separáveis. Existem infinitas retas (hiperplanos em $\mathbb{R}^2$) que podem separar perfeitamente as duas classes; duas delas são mostradas como linhas azuis [^66]. A figura também mostra a solução de mínimos quadrados obtida regredindo a variável resposta $Y \in \{-1, 1\}$ sobre $X$ (com intercepto), representada pela linha laranja [^66]. Como observado anteriormente [^26, ^67], esta fronteira é a mesma encontrada pela LDA no caso de duas classes. No entanto, esta solução de mínimos quadrados comete um erro de classificação num ponto de treino [^66], indicando que nem sempre é a solução ideal para separação. Classificadores que calculam uma combinação linear das features de entrada e retornam o sinal, como a solução de mínimos quadrados dada por $\{x : \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0\}$ [^67], foram denominados **perceptrons** na literatura de engenharia [^68].

### Conceitos Fundamentais

#### Álgebra do Hiperplano

Antes de prosseguirmos, é útil revisar alguns conceitos de álgebra vetorial relacionados a hiperplanos [^70]. A Figura 4.15 [^69] ilustra um hiperplano, ou conjunto afim $L$, em $\mathbb{R}^p$ (mostrado em $\mathbb{R}^2$) definido pela equação linear:
$$ f(x) = \beta_0 + \beta^T x = 0 $$
onde $\beta$ é um vetor em $\mathbb{R}^p$ e $\beta_0$ é um escalar [^70]. Algumas propriedades importantes são:

1.  Para quaisquer dois pontos $x_1$ e $x_2$ pertencentes a $L$, temos $\beta^T(x_1 - x_2) = (\beta_0 + \beta^T x_1) - (\beta_0 + \beta^T x_2) = 0 - 0 = 0$. Portanto, o vetor $\beta$ é ortogonal à superfície do hiperplano. O vetor normal unitário à superfície é $\beta^* = \beta / ||\beta||$ [^71].
2.  Para qualquer ponto $x_0$ pertencente a $L$, temos $\beta^T x_0 = -\beta_0$ [^72].
3.  A distância orientada (signed distance) de um ponto qualquer $x$ ao hiperplano $L$ é dada por:
    $$ \beta^{*T} (x - x_0) = \frac{\beta^T (x - x_0)}{||\beta||} = \frac{\beta^T x - \beta^T x_0}{||\beta||} = \frac{\beta^T x + \beta_0}{||\beta||} = \frac{f(x)}{||\beta||} $$ [^73].
    Portanto, o valor $f(x)$ é proporcional à distância orientada do ponto $x$ ao hiperplano definido por $f(x) = 0$ [^73].

#### 4.5.1 O Algoritmo de Aprendizagem Perceptron de Rosenblatt

O **algoritmo de aprendizagem perceptron**, proposto por Rosenblatt (1958) [^74], tenta encontrar um hiperplano separador minimizando a distância dos pontos mal classificados à fronteira de decisão [^74]. Assumindo que as classes são codificadas como $y_i \in \{-1, 1\}$, um ponto $x_i$ com resposta $y_i = 1$ é mal classificado se $x_i^T \beta + \beta_0 < 0$, e um ponto com $y_i = -1$ é mal classificado se $x_i^T \beta + \beta_0 > 0$ [^75]. Ambos os casos correspondem a $y_i (x_i^T \beta + \beta_0) < 0$. O objetivo é minimizar a função de custo:
$$ D(\beta, \beta_0) = - \sum_{i \in \mathcal{M}} y_i (x_i^T \beta + \beta_0) $$ [^75]
onde $\mathcal{M}$ é o conjunto dos índices dos pontos mal classificados [^75]. Esta quantidade é não negativa e proporcional à soma das distâncias dos pontos mal classificados à fronteira de decisão $f(x) = 0$ [^75].

O algoritmo utiliza **descida de gradiente estocástica (stochastic gradient descent)** para minimizar este critério linear por partes [^76]. Em vez de calcular a soma das contribuições do gradiente para todos os pontos mal classificados e depois dar um passo na direção negativa do gradiente, um passo é dado após visitar cada observação mal classificada [^76]. Os gradientes de $D(\beta, \beta_0)$ em relação a $\beta$ e $\beta_0$, assumindo $\mathcal{M}$ fixo, são:
$$ \frac{\partial D(\beta, \beta_0)}{\partial \beta} = - \sum_{i \in \mathcal{M}} y_i x_i $$ [^77]
$$ \frac{\partial D(\beta, \beta_0)}{\partial \beta_0} = - \sum_{i \in \mathcal{M}} y_i $$ [^77]
Na versão estocástica, se o ponto $(x_i, y_i)$ é mal classificado, os parâmetros são atualizados da seguinte forma:
$$ \begin{pmatrix} \beta \\\\ \beta_0 \end{pmatrix} \leftarrow \begin{pmatrix} \beta \\\\ \beta_0 \end{pmatrix} + \rho \begin{pmatrix} y_i x_i \\\\ y_i \end{pmatrix} $$ [^77]
onde $\rho$ é a taxa de aprendizagem (learning rate), que pode ser tomada como 1 sem perda de generalidade neste caso [^77].

Se as classes forem linearmente separáveis, pode-se mostrar que o algoritmo converge para um hiperplano separador em um número finito de passos (ver Exercício 4.6) [^78]. A Figura 4.14 mostra duas soluções possíveis (linhas azuis) encontradas pelo algoritmo a partir de diferentes pontos de partida aleatórios [^66]. No entanto, este algoritmo apresenta vários problemas, resumidos em Ripley (1996) [^78]:

*   **Não unicidade:** Quando os dados são separáveis, existem muitas soluções, e a solução encontrada depende dos valores iniciais [^78].
*   **Convergência lenta:** O número "finito" de passos pode ser muito grande, especialmente se a margem de separação entre as classes for pequena [^78].
*   **Não convergência:** Quando os dados não são linearmente separáveis, o algoritmo não converge e pode entrar em ciclos, que podem ser longos e difíceis de detectar [^78].

O problema da separabilidade pode, por vezes, ser mitigado transformando o espaço de features original através de funções de base, criando um espaço muito maior [^79]. No entanto, a separação perfeita nem sempre pode ser alcançada (por exemplo, se observações de classes diferentes partilham as mesmas features de entrada) e pode não ser desejável, pois o modelo resultante é provavelmente sobreajustado (overfit) e não generalizará bem [^79].

#### 4.5.2 Hiperplanos Separadores Ótimos

Uma solução mais elegante para o problema da não unicidade do perceptron é adicionar restrições adicionais ao hiperplano separador [^80]. O **hiperplano separador ótimo (optimal separating hyperplane)**, proposto por Vapnik (1996) [^80], não só separa as duas classes, mas também maximiza a distância até ao ponto mais próximo de qualquer uma das classes [^81]. Intuitivamente, maximizar esta *margem* entre as classes nos dados de treino leva a um melhor desempenho de classificação em dados de teste [^82].

Formalmente, consideramos o seguinte problema de otimização:
> **Problema de Otimização do Hiperplano Separador Ótimo (Primal)**
>
> $$ \max_{\beta, \beta_0, ||\beta||=1} M $$
> $$ \text{sujeito a } y_i(x_i^T \beta + \beta_0) \ge M, \quad i=1, \dots, N $$ [^83]

Estas condições garantem que todos os pontos estão a uma distância orientada de pelo menos $M$ da fronteira de decisão definida por $\beta$ e $\beta_0$. Procuramos o maior $M$ e os parâmetros associados $\beta, \beta_0$ [^83]. A restrição $||\beta||=1$ garante que $M$ é a distância geométrica real (ver propriedade 3 da álgebra do hiperplano [^73]). Podemos eliminar esta restrição substituindo as condições por $\frac{1}{||\beta||} y_i(x_i^T \beta + \beta_0) \ge M$ [^84], ou equivalentemente $y_i(x_i^T \beta + \beta_0) \ge M ||\beta||$ [^84]. Como para qualquer solução $(\beta, \beta_0)$, qualquer múltiplo positivo também satisfaz as desigualdades, podemos fixar arbitrariamente $M||\beta|| = 1$. Maximizar $M$ (com $||\beta||=1/M$) é então equivalente a minimizar $||\beta||$, ou $\frac{1}{2}||\beta||^2$. Assim, o problema (4.45) [^83] é equivalente a:

> **Problema de Otimização Equivalente (Quadrático)**
>
> $$ \min_{\beta, \beta_0} \frac{1}{2} ||\beta||^2 $$
> $$ \text{sujeito a } y_i(x_i^T \beta + \beta_0) \ge 1, \quad i=1, \dots, N $$ [^84]

As restrições $y_i(x_i^T \beta + \beta_0) \ge 1$ definem uma "laje" ou **margem (margin)** vazia em torno da fronteira de decisão linear, com espessura total $2/||\beta||$ (medida perpendicularmente ao hiperplano) [^85]. Escolhemos $\beta$ e $\beta_0$ para maximizar esta espessura [^85]. Este é um problema de **otimização convexa** (critério quadrático com restrições de desigualdade linear) [^86].

A solução pode ser encontrada usando a teoria de Lagrange. A função Lagrangiana (primal) a ser minimizada em relação a $\beta$ e $\beta_0$ é:
$$ L_P = \frac{1}{2} ||\beta||^2 - \sum_{i=1}^N \alpha_i [y_i(x_i^T \beta + \beta_0) - 1] $$ [^87]
onde $\alpha_i \ge 0$ são os multiplicadores de Lagrange. Igualando as derivadas em relação a $\beta$ e $\beta_0$ a zero, obtemos:
$$ \beta = \sum_{i=1}^N \alpha_i y_i x_i $$ [^88]
$$ 0 = \sum_{i=1}^N \alpha_i y_i $$ [^89]
Substituindo estas condições de volta em $L_P$, obtemos o problema dual de Wolfe:
> **Problema de Otimização Dual de Wolfe**
>
> $$ \max_{\alpha} L_D = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i^T x_k $$
> $$ \text{sujeito a } \alpha_i \ge 0, \quad \sum_{i=1}^N \alpha_i y_i = 0 $$ [^90]

A solução para $\alpha$ é obtida maximizando $L_D$ no ortante positivo, um problema de otimização convexa mais simples para o qual existe software padrão [^90]. Além disso, a solução deve satisfazer as condições de Karush-Kuhn-Tucker (KKT), que incluem as condições (4.50) [^88], (4.51) [^89], (4.52) ($\alpha_i \ge 0$) [^90], e a condição de **folga complementar (complementary slackness)**:
$$ \alpha_i [y_i(x_i^T \beta + \beta_0) - 1] = 0 \quad \forall i $$ [^91]

Destas condições KKT, podemos inferir propriedades importantes da solução:
*   Se $\alpha_i > 0$, então $y_i(x_i^T \beta + \beta_0) = 1$. Isto significa que o ponto $x_i$ se encontra exatamente na fronteira da margem [^92]. Estes pontos são chamados de **vetores de suporte (support vectors)** [^92].
*   Se $y_i(x_i^T \beta + \beta_0) > 1$, então o ponto $x_i$ está para além da fronteira da margem, e necessariamente $\alpha_i = 0$ [^92].

A equação (4.50) [^88] mostra que o vetor solução $\beta$ é definido como uma combinação linear apenas dos vetores de suporte $x_i$ (aqueles com $\alpha_i > 0$) [^93]. A Figura 4.16 [^97] ilustra o hiperplano separador ótimo para o exemplo de brinquedo; existem três vetores de suporte indicados, que se encontram na fronteira da margem [^97]. O intercepto $\beta_0$ pode ser determinado resolvendo a equação (4.53) [^91] para qualquer vetor de suporte $x_i$ (ou, de forma mais robusta, fazendo a média das soluções obtidas para todos os vetores de suporte) [^94]. A função de decisão final para classificar novas observações $x$ é:
$$ \hat{G}(x) = \text{sign}(x^T \hat{\beta} + \hat{\beta}_0) $$ [^95]

#### Discussão e Comparações

A intuição por trás do hiperplano separador ótimo é que uma margem grande nos dados de treino levará a uma boa separação nos dados de teste [^98]. A descrição da solução em termos de vetores de suporte sugere que o hiperplano ótimo foca mais nos pontos que "contam" (os mais próximos da fronteira), sendo potencialmente mais robusto a erros de especificação do modelo [^99]. Em contraste, a solução LDA depende de todos os dados, mesmo pontos longe da fronteira de decisão [^99]. No entanto, a identificação dos vetores de suporte requer o uso de todos os dados [^99]. Se as classes forem verdadeiramente Gaussianas com covariância comum, então LDA é a regra ótima, e os hiperplanos separadores pagarão um preço por focarem nos dados (potencialmente mais ruidosos) nas fronteiras das classes [^100].

A Figura 4.16 [^97] também inclui a solução de regressão logística para este problema, ajustada por máxima verossimilhança. As duas soluções são muito semelhantes neste caso [^101]. Quando um hiperplano separador existe, a regressão logística encontrará sempre um, pois a log-verossimilhança pode ser levada a zero (ver Exercício 4.5) [^102]. A solução de regressão logística partilha algumas características qualitativas com a solução do hiperplano separador: o vetor de coeficientes é definido por um ajuste de mínimos quadrados ponderado, e os pesos são maiores para pontos próximos da fronteira de decisão [^102].

Quando os dados não são linearmente separáveis, não haverá solução viável para o problema (4.48) [^84], e uma formulação alternativa é necessária [^103]. Como mencionado, pode-se tentar aumentar o espaço de features usando transformações de base, mas isso pode levar a separação artificial e overfitting [^103]. Uma alternativa mais atrativa, conhecida como **máquina de vetores de suporte (support vector machine)**, que permite sobreposição (overlap) mas minimiza uma medida da extensão dessa sobreposição, é discutida no Capítulo 12 [^103].

### Conclusão

Nesta seção, exploramos os classificadores por hiperplano separador, que constroem explicitamente fronteiras lineares para separar classes. O algoritmo Perceptron de Rosenblatt [^74] fornece um método para encontrar *um* hiperplano separador se os dados forem linearmente separáveis, mas sofre de não unicidade e problemas de convergência [^78]. O conceito de **hiperplano separador ótimo** [^80] resolve a questão da unicidade ao encontrar o hiperplano que maximiza a margem entre as classes [^81]. A solução é caracterizada por um subconjunto de pontos de dados, os **vetores de suporte** [^92], que determinam a fronteira de decisão. Este método forma a base para as máquinas de vetores de suporte [^64], uma ferramenta poderosa para classificação, especialmente em casos não separáveis, como veremos no Capítulo 12 [^103].

### Referências

[^1]: Seção 4.5, p. 129.
[^2]: Seção 4.1, p. 101.
[^3]: Seção 4.1, p. 101.
[^4]: Nota de rodapé 1, p. 101.
[^5]: Equação (4.2), p. 102.
[^6]: p. 102.
[^7]: p. 102.
[^8]: p. 102.
[^9]: p. 102.
[^10]: p. 102.
[^11]: Seção 4.2, p. 103.
[^12]: Figura 4.1, p. 103.
[^13]: p. 104.
[^14]: Figura 4.2, p. 105.
[^15]: Figura 4.3, p. 106.
[^16]: p. 106.
[^17]: Seção 4.3, p. 106.
[^18]: Figura 4.4, p. 107.
[^19]: Tabela 4.1, p. 107.
[^20]: Equação (4.8), p. 108.
[^21]: Equação (4.9), p. 108.
[^22]: p. 108.
[^23]: Figura 4.5, p. 109.
[^24]: Equação (4.10), p. 109.
[^25]: p. 109.
[^26]: p. 109.
[^27]: p. 110.
[^28]: p. 110.
[^29]: Equação (4.12), p. 110.
[^30]: p. 110.
[^31]: Figura 4.6, p. 111.
[^32]: p. 111.
[^33]: p. 111.
[^34]: Seção 4.3.1, p. 112.
[^35]: Figura 4.7, p. 112.
[^36]: Seção 4.3.2, p. 113.
[^37]: Seção 4.3.3, p. 113.
[^38]: p. 114.
[^39]: p. 114.
[^40]: Figura 4.8, p. 115.
[^41]: Figura 4.9, p. 116.
[^42]: Equação (4.15), p. 116.
[^43]: p. 116.
[^44]: Figura 4.10, p. 117.
[^45]: p. 117.
[^46]: Figura 4.11, p. 117.
[^47]: p. 117.
[^48]: Figura 4.11, p. 118.
[^49]: Seção 4.4, p. 119.
[^50]: Seção 4.4.1, p. 120.
[^51]: p. 121.
[^52]: Seção 4.4.2, p. 122; Tabela 4.2, p. 122.
[^53]: Figura 4.12, p. 123.
[^54]: p. 124; Tabela 4.3, p. 124.
[^55]: Seção 4.4.3, p. 124-125.
[^56]: Seção 4.4.4, p. 125.
[^57]: p. 126; Figura 4.13, p. 126.
[^58]: Seção 4.4.5, p. 127; Equação (4.33), p. 127.
[^59]: p. 127.
[^60]: p. 128.
[^61]: p. 128.
[^62]: Seção 4.5, p. 129.
[^63]: Seção 4.5, p. 129.
[^64]: Seção 4.5, p. 129.
[^65]: Seção 4.5, p. 129.
[^66]: Figura 4.14, p. 129.
[^67]: p. 129; Equação (4.39), p. 129.
[^68]: p. 129.
[^69]: Figura 4.15, p. 130.
[^70]: p. 130.
[^71]: Propriedade 1, p. 130.
[^72]: Propriedade 2, p. 130.
[^73]: Propriedade 3, p. 130; Equação (4.40), p. 130.
[^74]: Seção 4.5.1, p. 130.
[^75]: Equação (4.41), p. 131.
[^76]: p. 131.
[^77]: Equações (4.42), (4.43), (4.44), p. 131.
[^78]: p. 131.
[^79]: p. 132.
[^80]: Seção 4.5.2, p. 132.
[^81]: p. 132.
[^82]: p. 132.
[^83]: Equação (4.45), p. 132.
[^84]: Equações (4.46), (4.47), (4.48), p. 132.
[^85]: p. 132.
[^86]: p. 132.
[^87]: Equação (4.49), p. 133.
[^88]: Equação (4.50), p. 133.
[^89]: Equação (4.51), p. 133.
[^90]: Equação (4.52), p. 133.
[^91]: Equação (4.53), p. 133.
[^92]: p. 133.
[^93]: p. 133.
[^94]: p. 133.
[^95]: Equação (4.54), p. 133.
[^96]: p. 133.
[^97]: Figura 4.16, p. 134.
[^98]: p. 134.
[^99]: p. 134.
[^100]: p. 134.
[^101]: p. 134.
[^102]: p. 134.
[^103]: p. 134.

<!-- END -->