## O Hiperplano Separador Ótimo

### Introdução

Como explorado na Seção 4.5 [^1], os classificadores baseados em **separating hyperplanes** buscam construir explicitamente fronteiras de decisão lineares que separam os dados em classes distintas da melhor forma possível. Vimos que abordagens como o *Perceptron Learning Algorithm* de Rosenblatt [^5], embora capazes de encontrar um hiperplano separador para dados linearmente separáveis, sofrem de limitações significativas [^7]. Especificamente, quando os dados são separáveis, existe uma infinidade de possíveis hiperplanos separadores [^2], e a solução encontrada pelo algoritmo do *perceptron* depende crucialmente dos valores iniciais, não garantindo uma solução única ou ótima em qualquer sentido particular [^7]. Além disso, a convergência pode ser lenta, e o algoritmo não converge se os dados não forem perfeitamente separáveis [^7].

Em contrapartida, a Seção 4.5.2 introduz uma solução mais "elegante" [^8] para o problema da separabilidade linear: o **optimal separating hyperplane**. Este capítulo aprofunda-se neste conceito, detalhando sua formulação matemática e as propriedades da solução resultante. O objetivo central é encontrar um hiperplano que não apenas separe as classes, mas que o faça maximizando a "margem" entre elas, uma ideia proposta por Vapnik (1996) [^9]. Argumenta-se que esta maximização da margem conduz a um melhor desempenho de classificação em dados de teste [^10]. A análise matemática apresentada aqui é reconhecidamente de um nível mais elevado que as seções anteriores [^33], envolvendo conceitos de otimização convexa e dualidade Lagrangiana.

### Conceitos Fundamentais

#### Definição e Maximização da Margem

O **optimal separating hyperplane** é definido como o hiperplano que separa duas classes de dados e, simultaneamente, maximiza a distância até o ponto mais próximo de qualquer uma das classes [^9]. Esta distância máxima é conhecida como a **margem**. Geometricamente, um hiperplano $L$ é definido por $f(x) = \beta_0 + \beta^T x = 0$. A distância sinalizada de um ponto $x$ a este hiperplano $L$ é dada por $\frac{1}{||\beta||} f(x)$ [^34]. A ideia é encontrar $\beta$ e $\beta_0$ tais que todos os pontos de dados satisfaçam $y_i f(x_i) \ge M$ para a maior margem $M$ possível, onde $y_i \in \{-1, 1\}$ representa o rótulo da classe da observação $x_i$.

> O objetivo é, portanto, maximizar a margem $M$, o que intuitivamente cria uma "zona de segurança" ou um "slab" vazio em torno da fronteira de decisão, cuja espessura é maximizada [^15]. Acredita-se que uma margem maior nos dados de treino corresponda a uma melhor capacidade de generalização para novos dados [^10], [^27].

#### Formulação do Problema de Otimização

A busca pelo hiperplano ótimo pode ser formalizada como um problema de otimização. Inicialmente, consideramos [^11]:
$$
\max_{\beta, \beta_0, ||\beta||=1} M \\
\text{subject to } y_i(\mathbf{x}_i^T \beta + \beta_0) \ge M, \quad i = 1, \dots, N.
$$
As condições garantem que todos os pontos estejam a uma distância sinalizada de pelo menos $M$ da fronteira de decisão definida por $\beta$ e $\beta_0$, e procuramos o maior $M$ [^11]. O constrangimento $||\beta||=1$ pode ser eliminado. Note que a distância sinalizada de $x_i$ à fronteira é $\frac{y_i(\mathbf{x}_i^T \beta + \beta_0)}{||\beta||}$. Portanto, a condição de margem mínima $M$ pode ser reescrita como $\frac{1}{||\beta||} y_i(\mathbf{x}_i^T \beta + \beta_0) \ge M$ [^12], ou equivalentemente, $y_i(\mathbf{x}_i^T \beta + \beta_0) \ge M ||\beta||$ [^13].

Como a escala de $\beta$ e $\beta_0$ pode ser ajustada arbitrariamente (se $(\beta, \beta_0)$ satisfaz as desigualdades, qualquer múltiplo positivo também satisfaz), podemos fixar a margem funcional $M||\beta||$ em 1 [^14]. Maximizar $M$ torna-se então equivalente a minimizar $||\beta||$, ou, por conveniência, $\frac{1}{2}||\beta||^2$. O problema de otimização assume a forma padrão [^14]:
$$
\min_{\beta, \beta_0} \frac{1}{2} ||\beta||^2 \\
\text{subject to } y_i(\mathbf{x}_i^T \beta + \beta_0) \ge 1, \quad i = 1, \dots, N.
$$
Esta formulação representa um **problema de otimização convexa**: o critério a ser minimizado é quadrático e os constrangimentos são lineares (desigualdades lineares) [^16]. Isso garante que, se uma solução existir, ela será única.

#### A Solução via Dualidade Lagrangiana

Para resolver este problema de otimização com constrangimentos, utilizamos a teoria Lagrangiana. A função Lagrangiana (primal) é [^17]:
$$
L_P = \frac{1}{2}||\beta||^2 - \sum_{i=1}^N \alpha_i [y_i(\mathbf{x}_i^T \beta + \beta_0) - 1]
$$
onde $\alpha_i \ge 0$ são os multiplicadores de Lagrange. Minimizamos $L_P$ em relação a $\beta$ e $\beta_0$. Igualando as derivadas a zero, obtemos as seguintes condições [^18]:
$$
\frac{\partial L_P}{\partial \beta} = \beta - \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i = 0 \implies \beta = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i
$$
$$
\frac{\partial L_P}{\partial \beta_0} = - \sum_{i=1}^N \alpha_i y_i = 0 \implies \sum_{i=1}^N \alpha_i y_i = 0
$$
Substituindo estas condições de volta em $L_P$, obtemos a função dual de Wolfe, que deve ser maximizada em relação a $\alpha_i$ [^19]:
$$
L_D = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k \mathbf{x}_i^T \mathbf{x}_k \\
\text{subject to } \alpha_i \ge 0 \text{ and } \sum_{i=1}^N \alpha_i y_i = 0.
$$
Este é um problema de programação quadrática mais simples, frequentemente resolvido usando software padrão [^19]. A solução deve também satisfazer as condições de **Karush-Kuhn-Tucker (KKT)**, que incluem as condições primais ($y_i(\mathbf{x}_i^T \beta + \beta_0) \ge 1$), as condições duais ($\alpha_i \ge 0, \sum \alpha_i y_i = 0$), as derivadas nulas e a condição de **complementary slackness** [^20]:
$$
\alpha_i [y_i(\mathbf{x}_i^T \beta + \beta_0) - 1] = 0 \quad \forall i.
$$

#### Vectores de Suporte (Support Vectors)

As condições KKT revelam uma propriedade fundamental da solução. A condição de *complementary slackness* implica que [^21], [^22]:
*   Se $\alpha_i > 0$, então necessariamente $y_i(\mathbf{x}_i^T \beta + \beta_0) = 1$. Isto significa que o ponto $\mathbf{x}_i$ se encontra exatamente na fronteira da margem (ou *slab*). Estes pontos são chamados **support points** ou **support vectors**.
*   Se $y_i(\mathbf{x}_i^T \beta + \beta_0) > 1$, o ponto $\mathbf{x}_i$ está estritamente fora da margem, e então $\alpha_i$ deve ser zero.

A expressão para $\beta$ derivada da otimização primal, $\beta = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i$, mostra que o vetor normal ao hiperplano ótimo, $\beta$, é uma combinação linear *apenas* dos *support vectors* (aqueles $\mathbf{x}_i$ para os quais $\alpha_i > 0$) [^23]. Todos os outros pontos, mesmo que corretamente classificados mas distantes da fronteira, não influenciam diretamente a definição do hiperplano ótimo (têm $\alpha_i = 0$). A Figura 4.16 ilustra isso para um exemplo de brinquedo, mostrando três *support points* que definem a margem e o hiperplano [^24].

O intercepto $\beta_0$ pode ser determinado usando a condição $y_j(\mathbf{x}_j^T \beta + \beta_0) = 1$ para qualquer *support vector* $\mathbf{x}_j$ (i.e., qualquer $j$ com $\alpha_j > 0$) [^25]. Na prática, é mais robusto calcular $\beta_0$ usando uma média sobre todos os *support vectors*.

#### Classificação e Propriedades

Uma vez que $\beta$ e $\beta_0$ são determinados, a classificação de uma nova observação $\mathbf{x}$ é dada simplesmente pelo sinal da função de decisão [^26]:
$$
G(x) = \text{sign}(\mathbf{x}^T \beta + \beta_0).
$$
A solução baseada em *support vectors* sugere que o *optimal separating hyperplane* pode ser mais robusto a erros de especificação do modelo, pois foca nos pontos mais "difíceis" perto da fronteira [^28]. Em contraste, a solução da *Linear Discriminant Analysis (LDA)*, por exemplo, depende de todos os pontos de dados para estimar as médias e a matriz de covariância [^28]. No entanto, a identificação dos *support vectors* em si requer o uso de todos os dados de treinamento [^28]. Além disso, se as suposições da LDA (classes Gaussianas com covariância comum) forem verdadeiras, a LDA é a solução ótima, e o foco do hiperplano separador nos dados da fronteira (que podem ser mais ruidosos) pode representar uma desvantagem [^29].

É interessante notar a relação com a regressão logística. Quando um hiperplano separador existe, o algoritmo de regressão logística por máxima verossimilhança também encontrará um (embora os coeficientes tendam ao infinito, e a log-verossimilhança a zero) [^31]. A Figura 4.16 mostra que a fronteira encontrada pela regressão logística é frequentemente muito próxima do *optimal separating hyperplane* no caso separável [^30].

> **Limitação:** A formulação apresentada assume que os dados são linearmente separáveis. Quando as classes se sobrepõem, não existe solução viável para o problema de otimização (4.48). Uma extensão, conhecida como *support vector machine*, permite a sobreposição e será discutida no Capítulo 12 [^32].

### Conclusão

O **optimal separating hyperplane** oferece uma solução principiada e única para o problema de encontrar uma fronteira de decisão linear para dados separáveis. Ao maximizar a margem entre as classes [^9], ele não apenas resolve a ambiguidade inerente a outros métodos como o *perceptron* [^7], mas também visa melhorar a generalização para dados não vistos [^10]. A solução é elegantemente caracterizada em termos de um subconjunto dos dados de treinamento, os **support vectors**, que se situam nas bordas da margem e definem o hiperplano [^23]. A formulação como um problema de otimização quadrática convexa [^16] e sua solução via dualidade Lagrangiana [^19] fornecem um quadro matemático robusto. Embora limitado ao caso separável, este método estabelece a base fundamental para as *support vector machines*, uma ferramenta poderosa para classificação em cenários mais complexos [^32].

### Referências

[^1]: Section 4.5 introduces separating hyperplanes as procedures that explicitly construct linear decision boundaries to separate classes as well as possible. (p. 129)
[^2]: Figure 4.14 shows 20 data points in two classes in IR² that can be separated by a linear boundary, illustrating two of the infinitely many possible separating hyperplanes. (p. 129)
[^3]: The least squares solution to the problem, obtained by regressing the -1/1 response Y on X (with intercept), does not do a perfect job in separating the points, and makes one error. This is the same boundary found by LDA, in light of its equivalence with linear regression in the two-class case. (p. 129)
[^4]: Classifiers such as (4.39), that compute a linear combination of the input features and return the sign, were called perceptrons in the engineering literature. (p. 129)
[^5]: Section 4.5.1 discusses Rosenblatt\'s Perceptron Learning Algorithm. (p. 130)
[^6]: The perceptron learning algorithm tries to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary, using stochastic gradient descent on the criterion $D(\beta, \beta_0) = – \sum_{i \in M} y_i(x_i^T \beta + \beta_0)$. (p. 130-131, eq. 4.41, 4.44)
[^7]: Problems with the perceptron algorithm include: non-unique solutions when data are separable (depends on starting values), potentially very large number of steps ("finite" can be large), and non-convergence with cycling when data are not separable. (p. 131)
[^8]: A rather elegant solution to the first problem [non-uniqueness of perceptron solution] is to add additional constraints to the separating hyperplane. (p. 132)
[^9]: The optimal separating hyperplane separates the two classes and maximizes the distance to the closest point from either class (Vapnik, 1996). (p. 132)
[^10]: Not only does this provide a unique solution to the separating hyperplane problem, but by maximizing the margin between the two classes on the training data, this leads to better classification performance on test data. (p. 132)
[^11]: Consider the optimization problem: max M subject to $y_i(\mathbf{x}_i^T \beta + \beta_0) \ge M$ for $i = 1, \dots, N$, and $||\beta||=1$. (p. 132, eq. 4.45)
[^12]: We can get rid of the $||\beta|| = 1$ constraint by replacing the conditions with $\frac{1}{||\beta||} y_i(\mathbf{x}_i^T \beta + \beta_0) \ge M$. (p. 132, eq. 4.46)
[^13]: This is equivalent to $y_i(\mathbf{x}_i^T \beta + \beta_0) \ge M||\beta||$. (p. 132, eq. 4.47)
[^14]: Since for any $\beta$ and $\beta_0$ satisfying these inequalities, any positively scaled multiple satisfies them too, we can arbitrarily set $M||\beta|| = 1$. Thus the problem is equivalent to minimizing $\frac{1}{2}||\beta||^2$ subject to $y_i(\mathbf{x}_i^T \beta + \beta_0) \ge 1$. (p. 132, eq. 4.48)
[^15]: In light of (4.40), the constraints define an empty slab or margin around the linear decision boundary of thickness $1/||\beta||$. Hence we choose $\beta$ and $\beta_0$ to maximize its thickness. (p. 132)
[^16]: This is a convex optimization problem (quadratic criterion with linear inequality constraints). (p. 132-133)
[^17]: The Lagrange (primal) function, to be minimized w.r.t. $\beta$ and $\beta_0$, is $L_P = \frac{1}{2}||\beta||^2 - \sum_{i=1}^N \alpha_i [y_i(\mathbf{x}_i^T \beta + \beta_0) - 1]$. (p. 133, eq. 4.49)
[^18]: Setting the derivatives to zero, we obtain: $\beta = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i$ and $\sum_{i=1}^N \alpha_i y_i = 0$. (p. 133, eq. 4.50, 4.51)
[^19]: Substituting these in (4.49) we obtain the so-called Wolfe dual $L_D = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k \mathbf{x}_i^T \mathbf{x}_k$, subject to $\alpha_i \ge 0$ and $\sum_{i=1}^N \alpha_i y_i = 0$. The solution is obtained by maximizing $L_D$, a simpler convex optimization problem. (p. 133, eq. 4.52)
[^20]: In addition the solution must satisfy the Karush-Kuhn-Tucker conditions, which include (4.50), (4.51), (4.52) and $\alpha_i[y_i(\mathbf{x}_i^T \beta + \beta_0) - 1] = 0 \quad \forall i$. (p. 133, eq. 4.53)
[^21]: From these [KKT conditions] we can see that if $\alpha_i > 0$, then $y_i(\mathbf{x}_i^T \beta + \beta_0) = 1$, or in other words, $\mathbf{x}_i$ is on the boundary of the slab. (p. 133)
[^22]: If $y_i(\mathbf{x}_i^T \beta + \beta_0) > 1$, $\mathbf{x}_i$ is not on the boundary of the slab, and $\alpha_i = 0$. (p. 133)
[^23]: From (4.50) we see that the solution vector $\beta$ is defined in terms of a linear combination of the *support points* $\mathbf{x}_i$—those points defined to be on the boundary of the slab via $\alpha_i > 0$. (p. 133)
[^24]: Figure 4.16 shows the optimal separating hyperplane for our toy example; there are three support points. (p. 133-134)
[^25]: Likewise, $\beta_0$ is obtained by solving (4.53) for any of the support points. (p. 133)
[^26]: The optimal separating hyperplane produces a function $f(x) = \mathbf{x}^T \beta + \beta_0$ for classifying new observations: $G(x) = \text{sign}f(x)$. (p. 133, eq. 4.54)
[^27]: The intuition is that a large margin on the training data will lead to good separation on the test data. (p. 134)
[^28]: The description of the solution in terms of support points seems to suggest that the optimal hyperplane focuses more on the points that count, and is more robust to model misspecification. The LDA solution, on the other hand, depends on all of the data, even points far away from the decision boundary. Note, however, that the identification of these support points required the use of all the data. (p. 134)
[^29]: Of course, if the classes are really Gaussian, then LDA is optimal, and separating hyperplanes will pay a price for focusing on the (noisier) data at the boundaries of the classes. (p. 134)
[^30]: Included in Figure 4.16 is the logistic regression solution to this problem, fit by maximum likelihood. Both solutions are similar in this case. (p. 134)
[^31]: When a separating hyperplane exists, logistic regression will always find it, since the log-likelihood can be driven to 0 in this case (Exercise 4.5). (p. 134)
[^32]: When the data are not separable, there will be no feasible solution to this problem, and an alternative formulation is needed... In Chapter 12 we discuss a more attractive alternative known as the *support vector machine*, which allows for overlap, but minimizes a measure of the extent of this overlap. (p. 134-135)
[^33]: The mathematical level of this section is somewhat higher than that of the previous sections. (p. 129)
[^34]: Review of vector algebra for a hyperplane $f(x) = \beta_0 + \beta^T x = 0$: $\beta^* = \beta/||\beta||$ is the vector normal to the surface; for any point $x_0$ in $L$, $\beta^T x_0 = -\beta_0$; the signed distance of any point $x$ to $L$ is $\beta^{*T} (x - x_0) = \frac{1}{||\beta||}(\beta^T x + \beta_0) = \frac{1}{||f\'(x)||}f(x)$. (p. 130, eq. 4.40)

<!-- END -->