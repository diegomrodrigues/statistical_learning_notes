## O Algoritmo de Aprendizagem Perceptron de Rosenblatt

### Introdução

Nos tópicos anteriores, exploramos métodos como a Análise Discriminante Linear (LDA) [^Page 8-18] e a Regressão Logística [^Page 19-28], que estimam fronteiras de decisão lineares através da modelagem de densidades de classe ou probabilidades posteriores. Em contraste, a abordagem discutida nesta seção, e introduzida na Seção 4.5 [^Page 29], foca em classificadores de **hiperplano separador**. Estes procedimentos constroem fronteiras de decisão lineares que tentam *explicitamente separar os dados em diferentes classes da melhor forma possível* [^Page 29]. O **Perceptron**, proposto por Rosenblatt (1958) [^Page 2], é um algoritmo fundamental e historicamente significativo dentro desta classe de métodos. Conforme mencionado, classificadores que computam uma combinação linear das features de entrada e retornam o sinal foram chamados de *perceptrons* na literatura de engenharia do final dos anos 1950 [^Page 29]. Este capítulo detalhará o algoritmo de aprendizagem do Perceptron, seu objetivo e suas propriedades.

### Conceitos Fundamentais

O objetivo do **Algoritmo de Aprendizagem Perceptron** é encontrar um hiperplano separador para dados que sejam linearmente separáveis [^Page 2]. Consideremos um problema de classificação de duas classes onde as classes são codificadas como $y_i \in \{-1, 1\}$. Um hiperplano (ou conjunto afim $L$) é definido pela equação $f(x) = \beta_0 + \beta^T x = 0$ [^Page 30]. Como visto na Propriedade 3 da página 130, a quantidade $f(x)$ é proporcional à distância sinalizada de um ponto $x$ ao hiperplano $L$ [^Page 30, Prop 3]. A regra de classificação do Perceptron para uma nova observação $x$ é então $G(x) = \text{sign}(\beta_0 + \beta^T x)$.

Uma observação $(x_i, y_i)$ é classificada incorretamente se $y_i (\beta_0 + \beta^T x_i) \le 0$. O algoritmo Perceptron *tenta encontrar um hiperplano separador minimizando a distância dos pontos mal classificados à fronteira de decisão* [^Page 30].

A função objetivo a ser minimizada é definida sobre o conjunto $\mathcal{M}$ dos pontos mal classificados:
$$ D(\beta, \beta_0) = - \sum_{i \in \mathcal{M}} y_i (x_i^T \beta + \beta_0) $$ [^1]
Esta quantidade $D(\beta, \beta_0)$ é não-negativa e é proporcional à soma das distâncias dos pontos mal classificados à fronteira de decisão definida por $\beta_0 + \beta^T x = 0$ [^1].

Para minimizar $D(\beta, \beta_0)$, o algoritmo emprega **stochastic gradient descent** (descida de gradiente estocástica) [^3].

> *Isto significa que, em vez de calcular a soma das contribuições do gradiente de cada observação seguida por um passo na direção do gradiente negativo, um passo é dado após cada observação ser visitada* [^3].

O gradiente de $D$ em relação a $(\beta, \beta_0)$, assumindo $\mathcal{M}$ fixo, é dado por:
$$ \frac{\partial D(\beta, \beta_0)}{\partial \beta} = - \sum_{i \in \mathcal{M}} y_i x_i $$ [^2]
$$ \frac{\partial D(\beta, \beta_0)}{\partial \beta_0} = - \sum_{i \in \mathcal{M}} y_i $$ [^2]

Na abordagem estocástica, o algoritmo visita as observações mal classificadas (por exemplo, ciclicamente) e atualiza os parâmetros $(\beta, \beta_0)$ usando o gradiente referente *apenas* àquela observação mal classificada $i$. A regra de atualização torna-se:
$$ \begin{pmatrix} \beta \\ \beta_0 \end{pmatrix} \leftarrow \begin{pmatrix} \beta \\ \beta_0 \end{pmatrix} - \rho \nabla D_i = \begin{pmatrix} \beta \\ \beta_0 \end{pmatrix} + \rho \begin{pmatrix} y_i x_i \\ y_i \end{pmatrix} $$ [^4]
onde $\nabla D_i = (-y_i x_i, -y_i)^T$ é o gradiente instantâneo para o ponto $i$ mal classificado, e $\rho$ é a **learning rate** (taxa de aprendizado). Neste caso particular, a learning rate $\rho$ pode ser definida como 1 sem perda de generalidade [^5].

Uma propriedade fundamental deste algoritmo é o **Teorema de Convergência do Perceptron**:
> *Se as classes são linearmente separáveis, pode ser demonstrado que o algoritmo converge para um hiperplano separador num número finito de passos* [^6].
Uma prova desta convergência é delineada no Exercício 4.6 [^Ex 4.6]. A Figura 4.14 [^Page 29] ilustra um exemplo onde o algoritmo encontrou dois hiperplanos separadores diferentes, dependendo do ponto de partida aleatório [^7].

Apesar de sua simplicidade e garantia de convergência para casos separáveis, o algoritmo Perceptron possui algumas limitações importantes, sumarizadas em Ripley (1996) [^Page 31]:

1.  **Não-unicidade da Solução:** *Quando os dados são separáveis, existem muitas soluções, e qual delas é encontrada depende dos valores iniciais* [^8]. Isso é visível na Figura 4.14 [^7].
2.  **Tempo de Convergência:** *O número "finito" de passos pode ser muito grande. Quanto menor o gap [a margem de separação entre as classes], maior o tempo necessário para encontrar [uma solução]* [^9].
3.  **Não-separabilidade:** *Quando os dados não são separáveis, o algoritmo não converge e ciclos se desenvolvem. Os ciclos podem ser longos e, portanto, difíceis de detectar* [^10]. Embora transformações de base possam ser usadas para tentar alcançar separabilidade em um espaço aumentado, isso pode levar a overfitting [^Page 32].

### Conclusão

O algoritmo de aprendizagem Perceptron de Rosenblatt representa uma abordagem direta e iterativa para o problema de encontrar um hiperplano que separe duas classes. Utilizando stochastic gradient descent para minimizar uma medida da distância total dos pontos mal classificados à fronteira, ele garante encontrar uma solução em tempo finito se os dados forem linearmente separáveis [^6]. No entanto, a solução encontrada não é única e depende das condições iniciais [^8], e o algoritmo falha em convergir para dados não separáveis [^10]. Estas limitações motivaram o desenvolvimento de métodos subsequentes, como os **Optimal Separating Hyperplanes** discutidos na Seção 4.5.2 [^Page 32], que buscam um hiperplano separador único e mais robusto, maximizando a margem entre as classes.

### Referências

[^1]: Page 131, Eq. (4.41) and surrounding text. *D(β, βο) = – Σi∈M yi(xTi β + βo). The quantity is non-negative and proportional to the distance of the misclassified points to the decision boundary.*
[^2]: Page 131, Eqs. (4.42, 4.43). *∂D/∂β = – Σi∈M yixi, ∂D/∂βo = – Σi∈M yi.*
[^3]: Page 131. *The algorithm in fact uses stochastic gradient descent... This means that rather than computing the sum of the gradient contributions of each observation followed by a step in the negative gradient direction, a step is taken after each observation is visited.*
[^4]: Page 131, Eq. (4.44). *(β, βo)T updated via (β, βo)T ← (β, βo)T + ρ (yi xi, yi)T.*
[^5]: Page 131. *Here ρ is the learning rate, which in this case can be taken to be 1 without loss in generality.*
[^6]: Page 131. *If the classes are linearly separable, it can be shown that the algorithm converges to a separating hyperplane in a finite number of steps (Exercise 4.6).*
[^7]: Page 131. *Figure 4.14 shows two solutions to a toy problem, each started at a different random guess.*
[^8]: Page 131. *When the data are separable, there are many solutions, and which one is found depends on the starting values.*
[^9]: Page 131. *The "finite" number of steps can be very large. The smaller the gap, the longer the time to find it.*
[^10]: Page 131. *When the data are not separable, the algorithm will not converge, and cycles develop. The cycles can be long and therefore hard to detect.*
[^Page 2]: Page 102. *The first is the well-known perceptron model of Rosenblatt (1958), with an algorithm that finds a separating hyperplane in the training data, if one exists.*
[^Page 8-18]: Pages 107-118 (Sections 4.3, 4.3.1-4.3.3). Covers LDA, QDA, RDA, Reduced-Rank LDA.
[^Page 19-28]: Pages 119-128 (Section 4.4, 4.4.1-4.4.5). Covers Logistic Regression.
[^Page 29]: Page 129 (Section 4.5). *Separating Hyperplanes... These procedures construct linear decision boundaries that explicitly try to separate the data... Classifiers such as (4.39)... were called perceptrons... Figure 4.14 shows... two blue separating hyperplanes found by the perceptron learning algorithm...*
[^Page 30]: Page 130 (Section 4.5.1). *f(x) = βo + βT x = 0... property 3... The signed distance... f(x) is proportional to the signed distance... Rosenblatt's Perceptron Learning Algorithm... tries to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary.*
[^Page 30, Prop 3]: Page 130, Property 3. *The signed distance of any point x to L is given by ... (1/||β||) (βT x + βo) = (1/||f'(x)||) f(x).*
[^Page 31]: Page 131. Contains the core description of the Perceptron algorithm, objective, gradient, updates, convergence, and problems.
[^Page 32]: Page 132 (Section 4.5.2). *Optimal Separating Hyperplanes... elegant solution to the first problem [multiple solutions]...*
[^Ex 4.6]: Page 136, Exercise 4.6. *Prove that the perceptron learning algorithm converges to a separating hyperplane in a finite number of steps...*

<!-- END -->