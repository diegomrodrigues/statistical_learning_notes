# Formalização dos Conceitos da Implementação da LDA

## Definições Preliminares

Seja:

- $\mathcal{X} = {(x_i, g_i)}_{i=1}^N$ o conjunto de dados de treinamento, onde $x_i \in \mathbb{R}^p$ e $g_i \in {1,2,...,K}$
- $N_k$ o número de observações pertencentes à classe $k$, tal que $\sum_{k=1}^K N_k = N$
- $\mathcal{X}_k = {x_i \in \mathcal{X} : g_i = k}$ o conjunto de observações da classe $k$

## Estimação dos Parâmetros

1. **Probabilidades a priori**: $$\hat{\pi}_k = \frac{N_k}{N}, \quad k = 1, 2, ..., K$$
2. **Centroides das classes**: $$\hat{\mu}*k = \frac{1}{N_k} \sum*{x_i \in \mathcal{X}_k} x_i, \quad k = 1, 2, ..., K$$
3. **Média global**: $$\bar{x} = \frac{1}{N} \sum_{i=1}^N x_i = \sum_{k=1}^K \frac{N_k}{N} \hat{\mu}_k$$
4. **Matriz de covariância pooled**: $$\hat{\Sigma} = \frac{1}{N-K} \sum_{k=1}^K \sum_{x_i \in \mathcal{X}_k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T$$

## Decomposição Espectral e Transformação

1. **Decomposição espectral**: $$\hat{\Sigma} = UDU^T$$ onde:
   - $U$ é a matriz ortogonal cujas colunas são os autovetores de $\hat{\Sigma}$
   - $D$ é a matriz diagonal contendo os autovalores de $\hat{\Sigma}$
   - $U^T = U^{-1}$ devido à ortogonalidade
2. **Matriz de transformação de sphering**: $$W = D^{-1/2}U^T$$ onde $D^{-1/2}$ é a matriz diagonal com elementos $1/\sqrt{d_j}$ para cada autovalor $d_j$.
3. **Transformação das observações**: $$x_i^* = W(x_i - \bar{x})$$
4. **Transformação dos centroides**: $$\mu_k^* = W(\hat{\mu}_k - \bar{x})$$

## Propriedades Importantes

1. **Propriedade de sphering**: No espaço transformado, a matriz de covariância intraclasse torna-se a matriz identidade: $$\hat{\Sigma}_* = W\hat{\Sigma}W^T = D^{-1/2}U^T \cdot UDU^T \cdot UD^{-1/2} = I_p$$
2. **Distância de Mahalanobis**: A transformação de sphering converte a distância de Mahalanobis em distância Euclidiana: $$d_M(x, \mu_k) = \sqrt{(x - \mu_k)^T\hat{\Sigma}^{-1}(x - \mu_k)} = |x^* - \mu_k^*|_2$$

## Funções Discriminantes e Classificação

1. **Função discriminante no espaço transformado**: $$\delta_k(x^*) = (x^*)^T\mu_k^* - \frac{1}{2}(\mu_k^*)^T\mu_k^* + \log \hat{\pi}_k$$
2. **Regra de classificação**: $$\hat{G}(x) = \arg\max_{k \in {1,...,K}} \delta_k(x^*)$$
3. **Interpretação geométrica**: Equivale a atribuir a observação à classe com a menor distância Euclidiana quadrática ajustada pelas probabilidades a priori: $$\hat{G}(x) = \arg\min_{k \in {1,...,K}} {|x^* - \mu_k^*|_2^2 - 2\log \hat{\pi}_k}$$

## Corretude da Transformação

1. **Teorema**: A classificação no espaço transformado é equivalente à classificação baseada nas funções discriminantes da LDA no espaço original.
2. **Demonstração**:
   - A função discriminante original é $\delta_k(x) = x^T\hat{\Sigma}^{-1}\hat{\mu}_k - \frac{1}{2}\hat{\mu}_k^T\hat{\Sigma}^{-1}\hat{\mu}_k + \log \hat{\pi}_k$
   - Substituindo $x = W^{-1}x^* + \bar{x}$ e $\hat{\mu}_k = W^{-1}\mu_k^* + \bar{x}$
   - Simplificando e removendo termos que não dependem de $k$, obtemos $\delta_k(x^*)$

Esta formalização matemática completa mostra como a implementação baseada em sphering preserva a estrutura probabilística da LDA enquanto simplifica computacionalmente o problema para uma classificação baseada em distâncias no espaço transformado.

--

# Decomposição Espectral na Implementação da LDA

A decomposição espectral (ou eigendecomposition) da matriz de covariância pooled $\hat{\Sigma}$ é um passo fundamental na implementação computacionalmente eficiente da LDA. Vou explicar em detalhes como essa decomposição acontece e por que é importante.

## Processo da Decomposição Espectral

A decomposição espectral expressa $\hat{\Sigma}$ como:

$$\hat{\Sigma} = UDU^T$$

onde:

1. **U** é uma matriz $p \times p$ cujas colunas $u_1, u_2, ..., u_p$ são os autovetores ortonormais de $\hat{\Sigma}$
2. **D** é uma matriz diagonal $p \times p$ contendo os autovalores $d_1, d_2, ..., d_p$ de $\hat{\Sigma}$
3. **$U^T$** é a transposta de $U$, que devido à ortonormalidade é também sua inversa: $U^T = U^{-1}$

### Passos Computacionais

1. **Cálculo da matriz de covariância pooled**: $$\hat{\Sigma} = \frac{1}{N-K} \sum_{k=1}^K \sum_{x_i \in \mathcal{X}_k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T$$
2. **Encontrar os autovalores e autovetores**:
   - Resolver a equação característica: $\det(\hat{\Sigma} - \lambda I) = 0$
   - Para cada autovalor $\lambda_j = d_j$, resolver o sistema homogêneo: $(\hat{\Sigma} - d_j I)u_j = 0$
   - Normalizar cada autovetor: $|u_j| = 1$
   - Organizar os autovalores em ordem decrescente: $d_1 \geq d_2 \geq ... \geq d_p$
   - Construir a matriz $U = [u_1 | u_2 | ... | u_p]$ e a matriz diagonal $D = \text{diag}(d_1, d_2, ..., d_p)$
3. **Verificação**: A decomposição deve satisfazer $\hat{\Sigma}U = UD$, ou equivalentemente, para cada autovetor: $\hat{\Sigma}u_j = d_j u_j$

## Propriedades Importantes da Decomposição

1. **Matriz Semidefinida Positiva**: $\hat{\Sigma}$ é semidefinida positiva por construção, garantindo que todos os autovalores sejam não-negativos: $d_j \geq 0$
2. **Interpretação Geométrica**: Os autovetores $u_j$ representam as direções de variabilidade dos dados, e os autovalores $d_j$ indicam a magnitude dessa variabilidade ao longo de cada direção
3. **Rotação dos Eixos**: $U$ define uma rotação que alinha os eixos do sistema de coordenadas com as direções principais de variabilidade
4. **Diagonalização**: No sistema de coordenadas definido por $U$, a matriz de covariância torna-se diagonal: $U^T\hat{\Sigma}U = D$

## Importância na Implementação da LDA

A decomposição espectral permite a transformação de sphering ($W = D^{-1/2}U^T$) que tem várias vantagens:

1. **Simplificação Computacional**: Transforma o problema da LDA em um problema de distância Euclidiana mais simples
2. **Normalização da Variância**: No espaço transformado, a variância é igual em todas as direções (esférica)
3. **Estabilidade Numérica**: Ajuda a evitar problemas numéricos quando $\hat{\Sigma}$ está próxima da singularidade
4. **Redução de Dimensionalidade**: Facilita a identificação de direções de baixa variância que podem ser descartadas

## Implementação Prática

Na prática, a decomposição espectral é realizada usando algoritmos numéricos eficientes:

```python
# Pseudocódigo para decomposição espectral
def decomposição_espectral(Sigma):
    # Algoritmos como Power Method, QR decomposition, ou SVD
    autovalores, autovetores = algoritmo_decomposição(Sigma)
    
    # Ordenar autovalores e autovetores correspondentes em ordem decrescente
    indices = ordenar_decrescente(autovalores)
    autovalores_ordenados = autovalores[indices]
    autovetores_ordenados = autovetores[:, indices]
    
    # Construir matrizes D e U
    D = matriz_diagonal(autovalores_ordenados)
    U = autovetores_ordenados
    
    return U, D
```

A decomposição espectral transforma fundamentalmente o problema da LDA de um problema de classificação baseado em modelos probabilísticos para uma questão de geometria em um espaço onde a variabilidade dentro das classes é isotrópica (igual em todas as direções).





# Pseudo-código para Decomposição em Valores Singulares (SVD)

Aqui está um pseudo-código que implementa a decomposição SVD de uma matriz A de dimensão m×n do zero:

```
Função SVD(A):
    # Passo 1: Calcular A^T*A (para matrizes de colunas de U)
    ATA = transposta(A) * A
    
    # Passo 2: Calcular A*A^T (para matrizes de colunas de V)
    AAT = A * transposta(A)
    
    # Passo 3: Encontrar autovalores e autovetores de A^T*A
    [eigenvalues_ATA, U] = eigDecomposition(ATA)
    
    # Passo 4: Encontrar autovalores e autovetores de A*A^T
    [eigenvalues_AAT, V] = eigDecomposition(AAT)
    
    # Passo 5: Calcular valores singulares (raízes quadradas dos autovalores)
    sigma = []
    para i de 1 até min(m, n):
        sigma[i] = sqrt(eigenvalues_ATA[i])
    
    # Passo 6: Criar matriz diagonal Sigma com valores singulares
    Sigma = matrizZeros(m, n)
    para i de 1 até min(m, n):
        Sigma[i, i] = sigma[i]
    
    # Passo 7: Garantir o mesmo ordenamento para U, Sigma e V
    # Ordenar em ordem decrescente
    [sigma, indices] = ordenarDecrescente(sigma)
    
    U = reordenarColunas(U, indices)
    V = reordenarColunas(V, indices)
    
    # Passo 8: Garantir que os vetores singulares tenham a orientação correta
    # Se A*u_i ≠ sigma_i*v_i, então mude o sinal de v_i
    para i de 1 até min(m, n):
        u_i = obterColuna(U, i)
        v_i = obterColuna(V, i)
        
        if ||A*u_i - sigma[i]*v_i|| > epsilon:
            V[:, i] = -V[:, i]
    
    retornar U, Sigma, V
```

## Sub-rotina para Decomposição de Autovalores e Autovetores

```
Função eigDecomposition(M):
    # Esta função decompõe uma matriz simétrica M em autovalores e autovetores
    # Existem vários métodos para isso. Aqui usaremos o método da potência com deflação
    # para fins didáticos, embora não seja o mais eficiente

    n = tamanho(M)
    eigenvalues = vetor(n)
    eigenvectors = matrizIdentidade(n)
    
    X = cópiaMatriz(M)
    
    para i de 1 até n:
        # Encontrar o maior autovalor e seu autovetor usando o método da potência
        [eigenvalues[i], eigenvector] = métodoPotência(X)
        
        # Armazenar o autovetor
        eigenvectors[:, i] = eigenvector
        
        # Deflação: remover a componente do autovalor/autovetor encontrado
        X = X - eigenvalues[i] * outer(eigenvector, eigenvector)
    
    # Ordenar em ordem decrescente de magnitude
    [eigenvalues, indices] = ordenarDecrescente(eigenvalues)
    eigenvectors = reordenarColunas(eigenvectors, indices)
    
    retornar eigenvalues, eigenvectors
```

## Sub-rotina para o Método da Potência

```
Função métodoPotência(M, maxIterações=100, tolerância=1e-10):
    n = tamanho(M)
    
    # Inicializar um vetor aleatório
    v = vetorAleatorioUnitário(n)
    
    para itr de 1 até maxIterações:
        # Aplicar a matriz ao vetor
        w = M * v
        
        # Normalizar
        norma = ||w||
        v_novo = w / norma
        
        # Verificar convergência
        se ||v_novo - v|| < tolerância:
            break
            
        v = v_novo
    
    # Estimar o autovalor usando o quociente de Rayleigh
    autovalor = dotProduct(v, M * v)
    
    retornar autovalor, v
```

## Observações importantes

1. Este pseudo-código é didático e não representa a implementação mais eficiente.
2. Algoritmos reais de SVD frequentemente usam métodos mais sofisticados como:
   - Bidiagonalização de Householder seguida de iterações QR
   - Método de Jacobi para matrizes pequenas
   - Algoritmo de Golub-Reinsch
3. Para matrizes de grande porte, frequentemente são usadas implementações numéricas otimizadas disponíveis em bibliotecas como LAPACK.
4. A manipulação dos sinais dos vetores singulares (Passo 8) é importante para garantir que A*u_i = sigma_i*v_i, que é uma propriedade fundamental da SVD.
5. Em uma implementação real, seria recomendável verificar se os eigenvalues de A^TA e AA^T são essencialmente os mesmos (exceto possivelmente por zeros extras), o que deve ser o caso teoricamente.

