## Comparação entre Regressão Logística e Análise Discriminante Linear: Estimação e Suposições

### Introdução

Como explorado anteriormente neste livro, os **métodos lineares para classificação** constituem uma classe fundamental de procedimentos onde as fronteiras de decisão entre as classes são lineares [^2]. Dentro desta classe, a **Regressão Logística (Logistic Regression - LR)** e a **Análise Discriminante Linear (Linear Discriminant Analysis - LDA)** são duas abordagens proeminentes que, apesar de frequentemente levarem a fronteiras de decisão lineares, diferem substancialmente em suas derivações e, crucialmente, na forma como os parâmetros que definem essas fronteiras são estimados [^5], [^6]. Este capítulo foca na comparação detalhada entre LR e LDA, elucidando como ambas resultam em funções lineares para a classificação, mas divergem nos métodos de estimação de coeficientes e nas suposições subjacentes sobre a distribuição dos dados. Investigaremos como a Regressão Logística, ao fazer menos suposições, oferece uma abordagem mais geral [^54], enquanto a LDA, baseada em suposições mais fortes, pode ser mais eficiente sob certas condições [^59].

### Conceitos Fundamentais

#### A Forma Linear Comum

Tanto a LDA quanto a LR podem ser formuladas de modo a expressar a relação entre as classes e os preditores através de funções lineares.

Para a **LDA**, a linearidade surge como consequência de suposições específicas sobre a distribuição dos dados. Assume-se que a densidade condicional da classe $f_k(x) = Pr(X=x|G=k)$ para cada classe $k$ segue uma distribuição Gaussiana multivariada [^13], [^16], com uma **matriz de covariância comum** $\Sigma_k = \Sigma$ para todas as $K$ classes [^17]. Aplicando o teorema de Bayes, a probabilidade a posteriori $Pr(G=k|X=x)$ é dada por $Pr(G=k|X=x) = \frac{f_k(x)\pi_k}{\sum_{l=1}^K f_l(x)\pi_l}$ [^16], onde $\pi_k$ é a probabilidade a priori da classe $k$. Ao comparar duas classes, $k$ e $l$, a LDA examina o logaritmo da razão das probabilidades a posteriori. Devido à suposição de covariância comum, os termos quadráticos em $x$ se cancelam, resultando em uma função linear em $x$ para o log-odds [^18]:
$$ \log \frac{Pr(G=k|X=x)}{Pr(G=l|X=x)} = \log \frac{f_k(x)}{f_l(x)} + \log \frac{\pi_k}{\pi_l} $$
$$ = \log \frac{\pi_k}{\pi_l} - \frac{1}{2}(\mu_k + \mu_l)^T \Sigma^{-1}(\mu_k - \mu_l) + x^T \Sigma^{-1}(\mu_k - \mu_l) $$
Esta equação é explicitamente linear em $x$ [^18]. A fronteira de decisão entre as classes $k$ e $l$, onde as probabilidades a posteriori são iguais, é, portanto, um hiperplano [^19]. De forma equivalente, a regra de decisão pode ser expressa através das **funções discriminantes lineares** $\delta_k(x)$ [^20]:
$$ \delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log \pi_k \quad (4.10) $$
e a classificação é feita para a classe com o maior valor de $\delta_k(x)$ [^20].

Por outro lado, a **Regressão Logística** modela diretamente as probabilidades a posteriori $Pr(G=k|X=x)$ através de funções lineares em $x$ no espaço logit, sem fazer suposições sobre a distribuição de $X$ dentro de cada classe [^29], [^56]. O modelo assume *por construção* que os log-odds (ou logits) são lineares em $x$ [^52]. Para $K$ classes, o modelo é especificado em termos de $K-1$ transformações logit, usando uma classe (por exemplo, a classe $K$) como referência [^30]:
$$ \log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \beta_{k0} + \beta_k^T x, \quad k=1, \dots, K-1 \quad (4.17) $$
A partir desta forma, as probabilidades para cada classe podem ser expressas como [^31]:
$$ Pr(G=k|X=x) = \frac{\exp(\beta_{k0} + \beta_k^T x)}{1 + \sum_{l=1}^{K-1} \exp(\beta_{l0} + \beta_l^T x)}, \quad k=1, \dots, K-1 $$
$$ Pr(G=K|X=x) = \frac{1}{1 + \sum_{l=1}^{K-1} \exp(\beta_{l0} + \beta_l^T x)} \quad (4.18) $$
Estas probabilidades somam um e permanecem no intervalo $[0, 1]$ [^30]. No caso de duas classes ($K=2$), o modelo simplifica para uma única função linear no logit [^4], [^32]:
$$ \log \frac{Pr(G=1|X=x)}{Pr(G=2|X=x)} = \beta_0 + \beta^T x \quad (4.2) $$

> Embora ambos os modelos, LDA (sob a suposição de covariância comum) e Regressão Logística, resultem em log-odds lineares (compare Eq. 4.33 [^51] e Eq. 4.34 [^52]), a diferença fundamental reside na maneira como os coeficientes lineares ($\alpha_{k0}, \alpha_k$ para LDA e $\beta_{k0}, \beta_k$ para LR) são estimados [^53].

#### Diferenças na Estimação dos Coeficientes

A **LDA** estima os parâmetros $(\pi_k, \mu_k, \Sigma)$ maximizando a **verossimilhança completa (full log-likelihood)** baseada na densidade conjunta $Pr(X, G=k)$ [^57]. Assumindo que $Pr(X|G=k)$ é Gaussiana $\phi(X; \mu_k, \Sigma)$ e $Pr(G=k) = \pi_k$, a densidade conjunta é $Pr(X, G=k) = \pi_k \phi(X; \mu_k, \Sigma)$ [^57]. A estimação envolve calcular as estimativas de máxima verossimilhança para as médias $\mu_k$, a matriz de covariância comum $\Sigma$, e as probabilidades a priori $\pi_k$ a partir dos dados de treinamento [^21]:
*   $\hat{\pi}_k = N_k / N$
*   $\hat{\mu}_k = \sum_{g_i=k} x_i / N_k$
*   $\hat{\Sigma} = \sum_{k=1}^K \sum_{g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T / (N-K)$ [^21]
Os coeficientes lineares $\alpha_{k0}$ e $\alpha_k$ da forma log-odds (Eq. 4.33) são então funções dessas estimativas [^57]. É importante notar que a LDA modela explicitamente a densidade marginal $Pr(X) = \sum_{k=1}^K \pi_k \phi(X; \mu_k, \Sigma)$, que é uma mistura de Gaussianas [^58]. Esta modelagem da densidade marginal desempenha um papel na estimação [^58].

A **Regressão Logística**, em contraste, estima os parâmetros $\beta_{k0}$ e $\beta_k$ maximizando a **verossimilhança condicional (conditional log-likelihood)** de $G$ dado $X$ [^33]. A log-verossimilhança condicional para $N$ observações é [^33]:
$$ l(\theta) = \sum_{i=1}^N \log Pr(G=g_i | X=x_i; \theta) \quad (4.19) $$
onde $\theta$ representa o conjunto de todos os parâmetros $\beta$. A LR foca exclusivamente em modelar a relação condicional $Pr(G|X)$ e *ignora completamente a distribuição marginal* $Pr(X)$ [^56]. Pode-se pensar que $Pr(X)$ é estimada de forma não paramétrica usando a distribuição empírica dos dados [^56]. A maximização de $l(\theta)$ não possui uma solução de forma fechada e requer métodos iterativos, como o algoritmo de **Newton-Raphson** [^36], que pode ser visto como um processo de **Mínimos Quadrados Ponderados Iterativamente (Iteratively Reweighted Least Squares - IRLS)** [^37]. As equações de *score* (derivadas da log-verossimilhança) são não lineares nos parâmetros $\beta$ e precisam ser resolvidas numericamente [^35], [^36].

#### Suposições e Generalidade

A diferença fundamental nos métodos de estimação reflete as diferentes suposições subjacentes:

*   **Suposições da LDA:** A derivação padrão da LDA assume que as densidades condicionais de classe $f_k(x)$ são Gaussianas e, crucialmente, que possuem uma matriz de covariância comum $\Sigma$ [^17], [^51]. Estas são suposições fortes sobre a estrutura dos dados [^59].
*   **Suposições da LR:** A LR assume apenas que a transformação logit das probabilidades a posteriori é uma função linear dos preditores $x$ [^30], [^52]. Ela não faz suposições sobre a distribuição marginal $Pr(X)$ ou sobre a forma das densidades condicionais $f_k(x)$ além da implicação na forma dos logits [^56].

> Como resultado, a Regressão Logística é considerada **mais geral** e faz **menos suposições** sobre a distribuição dos dados do que a LDA [^54], [^65].

#### Implicações das Suposições

1.  **Eficiência vs. Robustez:** Ao fazer suposições mais fortes (Gaussianas com $\Sigma$ comum), a LDA utiliza mais informações sobre a estrutura presumida dos dados, incluindo a densidade marginal $Pr(X)$ [^59]. Se essas suposições forem verdadeiras, a LDA pode produzir estimativas dos parâmetros mais eficientes (com menor variância) do que a LR [^59]. Efron (1975) mostrou que ignorar a parte marginal da verossimilhança (como faz a LR) pode levar a uma perda de eficiência assintótica de até 30% na taxa de erro se os dados forem de fato Gaussianos [^60]. No entanto, a força da LDA é também sua fraqueza: se as suposições, particularmente a de normalidade ou de covariância comum, forem violadas, as estimativas da LDA podem ser viesadas e o desempenho do classificador pode degradar. A Regressão Logística, fazendo menos suposições, é geralmente mais **robusta** a desvios dessas condições [^65]. É considerada uma aposta mais segura (*safer bet*) [^65]. Além disso, a LDA não é robusta a outliers grosseiros, pois estes podem influenciar indevidamente a estimação da matriz de covariância comum $\Sigma$ [^62]. A LR, especialmente na formulação IRLS (Eq. 4.27, 4.28), tende a dar menos peso a pontos com probabilidades previstas muito próximas de 0 ou 1, que podem estar longe da fronteira de decisão [^61].

2.  **Dados Separáveis:** Quando os dados de treinamento são perfeitamente separáveis por um hiperplano, as estimativas de máxima verossimilhança para os parâmetros da Regressão Logística não são definidas (tendem ao infinito) [^64], [^70] (ver Exercício 4.5). Isso ocorre porque a verossimilhança condicional pode ser levada arbitrariamente perto do seu valor máximo (que seria 1 para todas as observações) ao aumentar a magnitude dos coeficientes. Em contraste, os coeficientes da LDA permanecem bem definidos mesmo em caso de separabilidade, pois a presença do termo da verossimilhança marginal $Pr(X)$ atua como um regularizador, impedindo essas degenerescências [^64].

3.  **Preditoras Qualitativas:** A LDA assume que os preditores $X$ são quantitativos e seguem uma distribuição Gaussiana. Se alguns componentes de $X$ são variáveis qualitativas (categóricas), a suposição Gaussiana é formalmente violada. A Regressão Logística lida naturalmente com preditores qualitativos (através de codificação apropriada, como variáveis dummy). No entanto, a experiência prática sugere que a LDA frequentemente apresenta desempenho similar à LR mesmo quando usada inadequadamente com preditores qualitativos [^66].

4.  **Desempenho Prático:** Apesar das diferenças teóricas, a experiência sugere que os dois modelos frequentemente fornecem resultados muito similares em aplicações práticas [^66]. A Tabela 4.1 [^15], por exemplo, mostra as taxas de erro para os dados de vogais, onde a LDA (erro de teste 0.56) superou a regressão linear sobre indicadores (0.67) mas foi ligeiramente pior que a LR (0.51) e QDA (0.53). A escolha entre os métodos pode depender do contexto, da familiaridade do analista, e se as suposições da LDA parecem plausíveis ou se a robustez da LR é preferível.

### Conclusão

Em resumo, tanto a Regressão Logística quanto a Análise Discriminante Linear (com covariância comum) são métodos poderosos que resultam em fronteiras de decisão lineares. A LDA chega a essa linearidade como uma consequência de suposições sobre a distribuição Gaussiana dos dados com covariâncias comuns, estimando parâmetros através da maximização da verossimilhança completa da densidade conjunta. A Regressão Logística, por outro lado, postula diretamente uma relação linear entre os preditores e os logits das probabilidades a posteriori, estimando parâmetros através da maximização da verossimilhança condicional, sem fazer suposições sobre a distribuição dos preditores.

A **diferença essencial reside na estimação dos coeficientes e nas suposições subjacentes** [^6], [^53]. A LDA, ao usar mais informações (sob suas suposições), pode ser mais eficiente se essas suposições forem válidas. A LR, fazendo menos suposições, é mais geral e robusta, particularmente quando a suposição Gaussiana é questionável ou na presença de outliers [^54], [^65]. A escolha entre elas envolve um trade-off entre eficiência (potencialmente maior na LDA se as suposições se mantêm) e robustez (geralmente maior na LR). Na prática, embora a LR seja frequentemente considerada a opção mais segura devido à sua maior generalidade [^65], ambos os métodos permanecem ferramentas valiosas e frequentemente produzem resultados comparáveis [^66].

### Referências

[^1]: OCR page 1: "In this chapter we revisit the classification problem and focus on linear methods for classification."
[^2]: OCR page 1: "For an important class of procedures, these decision boundaries are linear; this is what we will mean by linear methods for classification."
[^3]: OCR page 1: "In Chapter 2 we fit linear regression models to the class indicator variables..."
[^4]: OCR page 2: "Here the monotone transformation is the logit transformation: log[p/(1-p)], and in fact we see that log[Pr(G=1|X=x)/Pr(G=2|X=x)] = β0 + βT x."
[^5]: OCR page 2: "We discuss two very popular but different methods that result in linear log-odds or logits: linear discriminant analysis and linear logistic regression."
[^6]: OCR page 2: "Although they differ in their derivation, the essential difference between them is in the way the linear function is fit to the training data."
[^7]: OCR page 2: "We will look at two methods that explicitly look for \'separating hyperplanes.\'"
[^8]: OCR page 3: "4.2 Linear Regression of an Indicator Matrix"
[^9]: OCR page 4: Discussion on rationale and limitations of linear regression on indicator matrix.
[^10]: OCR page 5: "Because of the rigid nature of the regression model, classes can be masked by others. Figure 4.2 illustrates an extreme situation when K = 3."
[^11]: OCR page 6: Figure 4.3 showing masking effect.
[^12]: OCR page 6: "4.3 Linear Discriminant Analysis"
[^13]: OCR page 8: "linear and quadratic discriminant analysis use Gaussian densities;"
[^14]: OCR page 7: Figure 4.4 showing vowel data plot for LDA.
[^15]: OCR page 7: Table 4.1 comparing error rates for Linear regression, LDA, QDA, Logistic regression on vowel data.
[^16]: OCR page 8: Derivation based on Bayes theorem: Pr(G=k|X=x) = fk(x)πk / Σl fl(x)πl.
[^17]: OCR page 8: "Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Σk = Σ ∀k."
[^18]: OCR page 8: Equation (4.9) showing log Pr(G=k|X=x) / Pr(G=l|X=x) = log(πk/πl) - 1/2(μk+μl)TΣ⁻¹(μk-μl) + xTΣ⁻¹(μk-μl), "an equation linear in x."
[^19]: OCR page 8: "This linear log-odds function implies that the decision boundary between classes k and l ... is linear in x; in p dimensions a hyperplane."
[^20]: OCR page 9: Equation (4.10) δk(x) = xTΣ⁻¹μk - 1/2μk TΣ⁻¹μk + log πk.
[^21]: OCR page 9: Parameter estimation for LDA: π̂k, μ̂k, Σ̂.
[^22]: OCR page 9-10: Correspondence between LDA and least squares for K=2, but different intercepts unless N1=N2.
[^23]: OCR page 10: "Since this derivation of the LDA direction via least squares does not use a Gaussian assumption for the features, its applicability extends beyond the realm of Gaussian data. However the derivation of the particular intercept or cut-point given in (4.11) does require Gaussian data."
[^24]: OCR page 10: "With more than two classes, LDA is not the same as linear regression of the class indicator matrix, and it avoids the masking problems associated with that approach"
[^25]: OCR page 10: Introduction of QDA where Σk are not assumed equal.
[^26]: OCR page 11: Performance comparison in STATLOG project.
[^27]: OCR page 11: Bias-variance tradeoff explanation for LDA\'s success.
[^28]: OCR page 12: Section 4.3.1 Regularized Discriminant Analysis.
[^29]: OCR page 19: "4.4 Logistic Regression"
[^30]: OCR page 19: Equation (4.17) defining the logistic model for K classes via K-1 linear logits.
[^31]: OCR page 19: Equation (4.18) showing the probability form Pr(G=k|X=x).
[^32]: OCR page 19: "When K = 2, this model is especially simple, since there is only a single linear function."
[^33]: OCR page 20: "Logistic regression models are usually fit by maximum likelihood, using the conditional likelihood of G given X." Equation (4.19) l(θ) = Σ log pgi(xi; θ).
[^34]: OCR page 20: Equation (4.20) showing the log-likelihood for K=2.
[^35]: OCR page 20: Equation (4.21) showing the score equations ∂l(β)/∂β = Σ xi(yi - p(xi; β)) = 0.
[^36]: OCR page 20: Mention of Newton-Raphson algorithm and Hessian matrix (Eq 4.22).
[^37]: OCR page 21: Equation (4.26) showing the Newton step as a weighted least squares step (IRLS). Equation (4.28) shows the IRLS minimization problem.
[^38]: OCR page 21: Mention of multiclass Newton algorithm and `glmnet` package.
[^39]: OCR page 21: "Logistic regression models are used mostly as a data analysis and inference tool..."
[^40]: OCR page 22: Section 4.4.2 Example: South African Heart Disease.
[^41]: OCR page 22: Discussion of Z scores and Wald test for coefficient significance.
[^42]: OCR page 22: Interpretation issues due to predictor correlation.
[^43]: OCR page 23: Figure 4.12 Scatterplot matrix.
[^44]: OCR page 24: Stepwise logistic regression example.
[^45]: OCR page 24: Interpretation of coefficients in terms of odds ratios.
[^46]: OCR page 25: Connection of LR fitting to Pearson chi-square, asymptotic theory (consistency, normality), Rao/Wald tests.
[^47]: OCR page 25: Section 4.4.4 L1 Regularized Logistic Regression, Equation (4.31).
[^48]: OCR page 26: Solving L1 LR using quadratic approximations / weighted lasso.
[^49]: OCR page 26: Equation (4.32) score equations for L1 LR.
[^50]: OCR page 26: Figure 4.13 L1 regularization paths.
[^51]: OCR page 27: Equation (4.33) repeating the LDA linear log-posterior odds αk0 + αkT x.
[^52]: OCR page 27: Equation (4.34) repeating the LR linear logit βk0 + βkT x. "The linear logistic model (4.17) by construction has linear logits"
[^53]: OCR page 27: "It seems that the models are the same. Although they have exactly the same form, the difference lies in the way the linear coefficients are estimated."
[^54]: OCR page 27: "The logistic regression model is more general, in that it makes less assumptions."
[^55]: OCR page 27: Equation (4.35) Pr(X, G=k) = Pr(X)Pr(G=k|X).
[^56]: OCR page 27: "The logistic regression model leaves the marginal density of X as an arbitrary density function Pr(X), and fits the parameters of Pr(G|X) by maximizing the conditional likelihood... Although Pr(X) is totally ignored, we can think of this marginal density as being estimated in a fully nonparametric and unrestricted fashion, using the empirical distribution function..."
[^57]: OCR page 27: "With LDA we fit the parameters by maximizing the full log-likelihood, based on the joint density Pr(X, G = k) = φ(X; μk, Σ)πk," (Eq 4.37).
[^58]: OCR page 28: Equation (4.38) Pr(X) = Σ πk φ(X; μk, Σ). "the marginal density Pr(X) does play a role here. It is a mixture density which also involves the parameters."
[^59]: OCR page 28: "By relying on the additional model assumptions, we have more information about the parameters, and hence can estimate them more efficiently (lower variance)."
[^60]: OCR page 28: "If in fact the true fk(x) are Gaussian, then in the worst case ignoring this marginal part of the likelihood constitutes a loss of efficiency of about 30% asymptotically in the error rate (Efron, 1975)."
[^61]: OCR page 28: "For example, observations far from the decision boundary (which are down-weighted by logistic regression) play a role in estimating the common covariance matrix."
[^62]: OCR page 28: "This is not all good news, because it also means that LDA is not robust to gross outliers."
[^63]: OCR page 28: "By relying on strong model assumptions, such as here, we can use both types of information [labeled and unlabeled]."
[^64]: OCR page 28: "The marginal likelihood can be thought of as a regularizer... For example, if the data in a two-class logistic regression model can be perfectly separated by a hyperplane, the maximum likelihood estimates of the parameters are undefined (i.e., infinite; see Exercise 4.5). The LDA coefficients for the same data will be well defined, since the marginal likelihood will not permit these degeneracies."
[^65]: OCR page 28: "It is generally felt that logistic regression is a safer, more robust bet than the LDA model, relying on fewer assumptions."
[^66]: OCR page 28: "It is our experience that the models give very similar results, even when LDA is used inappropriately, such as with qualitative predictors."
[^67]: OCR page 34: Figure 4.16 shows logistic regression boundary close to optimal separating hyperplane.
[^68]: OCR page 34: "When a separating hyperplane exists, logistic regression will always find it, since the log-likelihood can be driven to 0 in this case (Exercise 4.5)."
[^69]: OCR page 35: Exercise 4.2 relating LDA and least squares.
[^70]: OCR page 36: Exercise 4.5 asking about LR estimates when data are separable.

<!-- END -->