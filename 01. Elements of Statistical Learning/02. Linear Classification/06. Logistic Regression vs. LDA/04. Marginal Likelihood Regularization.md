## Regularização Implícita via Verossimilhança Marginal em LDA versus Regressão Logística

### Introdução

No âmbito dos métodos lineares para classificação, Linear Discriminant Analysis (LDA) e Regressão Logística (LR) representam duas abordagens fundamentais que, apesar de frequentemente resultarem em fronteiras de decisão lineares, diferem substancialmente em suas derivações e, crucialmente, nos seus procedimentos de estimação de parâmetros [^1]. Como explorado anteriormente (Seção 4.3 [^2, ^3] e Seção 4.4 [^7, ^12]), ambos os modelos podem ser expressos em termos de log-odds lineares em relação às variáveis preditoras $x$. Para LDA, esta linearidade é uma consequência da suposição de densidades de classe Gaussianas com uma matriz de covariância comum $\Sigma$ [^11]. Para a Regressão Logística, a forma linear dos logits é postulada diretamente por construção [^12]. Este capítulo aprofunda a comparação entre LDA e LR, focando especificamente na diferença fundamental em seus métodos de estimação – o uso da verossimilhança condicional em LR versus a verossimilhança conjunta (ou completa) em LDA [^13]. Investigaremos como a incorporação da verossimilhança marginal $Pr(X)$ no processo de estimação do LDA atua como um mecanismo de regularização implícito, notavelmente prevenindo degenerescências nos parâmetros, como estimativas infinitas, que podem ocorrer na Regressão Logística quando os dados são perfeitamente separáveis [^19, ^20, ^21].

### Conceitos Fundamentais

#### Formulação dos Modelos e Fronteiras de Decisão

Relembrando a Seção 4.3, LDA modela a densidade condicional de classe $f_k(x) = Pr(X=x|G=k)$ como uma Gaussiana multivariada, $f_k(x) = \phi(x; \mu_k, \Sigma_k)$ [^2]. No caso particular onde assumimos uma matriz de covariância comum $\Sigma_k = \Sigma$ para todas as classes $k$, a regra de decisão de Bayes leva a funções discriminantes lineares $\delta_k(x)$ ou, equivalentemente, a log-odds lineares entre quaisquer duas classes $k$ e $K$ [^3, ^11]:
$$ \log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \log \frac{\pi_k}{\pi_K} - \frac{1}{2}(\mu_k + \mu_K)^T \Sigma^{-1}(\mu_k - \mu_K) + x^T \Sigma^{-1}(\mu_k - \mu_K) = \alpha_{k0} + \alpha_k^T x $$
onde $\pi_k$ é a probabilidade a priori da classe $k$.

Por outro lado, o modelo de Regressão Logística, detalhado na Seção 4.4, postula diretamente uma forma linear para os log-odds (ou logits) [^7, ^12]:
$$ \log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \beta_{k0} + \beta_k^T x $$
As probabilidades posteriores $Pr(G=k|X=x)$ são então derivadas desta forma linear, garantindo que somem um e permaneçam no intervalo $[0, 1]$ [^7]. Embora a forma funcional resultante para os log-odds seja idêntica em ambos os modelos (Eq. 4.33 [^11] e Eq. 4.34 [^12]), as suposições subjacentes e, mais importante, os métodos de estimação dos coeficientes ($\alpha$ vs $\beta$) diferem significativamente [^13].

#### Estimação de Parâmetros: Verossimilhança Condicional vs. Verossimilhança Conjunta

A divergência crucial entre LR e LDA reside na função de verossimilhança otimizada para estimar os parâmetros.

**Regressão Logística:** Conforme descrito na Seção 4.4.1, os modelos de Regressão Logística são tipicamente ajustados por **máxima verossimilhança condicional** [^8]. A função de log-verossimilhança é baseada na distribuição condicional de $G$ dado $X$, que é apropriadamente modelada como multinomial [^8]:
$$ l(\theta) = \sum_{i=1}^N \log Pr(G=g_i | X=x_i; \theta) $$
onde $\theta$ representa o conjunto de parâmetros $\{\beta_{k0}, \beta_k\}$. Notavelmente, este procedimento ignora completamente a distribuição marginal das variáveis preditoras, $Pr(X)$ [^14]. A otimização é geralmente realizada usando algoritmos iterativos como Iteratively Reweighted Least Squares (IRLS) [^10], que resolvem as equações de score não lineares (Eq. 4.21 [^9]).

**Linear Discriminant Analysis:** Em contraste, a abordagem padrão para LDA envolve a maximização da **verossimilhança completa (ou conjunta)** baseada na densidade conjunta $Pr(X, G=k)$ [^15]. Assumindo as densidades Gaussianas $\phi(X; \mu_k, \Sigma)$ e as probabilidades a priori $\pi_k$, a densidade conjunta é dada por [^15]:
$$ Pr(X, G=k) = Pr(G=k) Pr(X|G=k) = \pi_k \phi(X; \mu_k, \Sigma) $$
A maximização da log-verossimilhança conjunta em relação aos parâmetros $\mu_k, \Sigma, \pi_k$ leva às estimativas padrão apresentadas na Seção 4.3 [^4]: $\hat{\pi}_k = N_k/N$, $\hat{\mu}_k = \sum_{g_i=k} x_i / N_k$, e $\hat{\Sigma}$ como a matriz de covariância pooled dentro das classes. Os coeficientes lineares $\alpha_{k0}, \alpha_k$ da forma logística (Eq. 4.33 [^11]) são então derivados como funções dessas estimativas de parâmetros Gaussianos [^11]. Crucialmente, a otimização da verossimilhança conjunta implicitamente envolve a modelagem da distribuição marginal de $X$, $Pr(X)$. Esta densidade marginal é uma mistura de Gaussianas [^16]:
$$ Pr(X) = \sum_{k=1}^K Pr(G=k) Pr(X|G=k) = \sum_{k=1}^K \pi_k \phi(X; \mu_k, \Sigma) $$
Esta dependência da densidade marginal $Pr(X)$ é a chave para entender as diferenças de comportamento entre LDA e LR.

#### O Papel Regularizador da Verossimilhança Marginal em LDA

A inclusão da verossimilhança marginal $Pr(X)$ no processo de estimação do LDA introduz uma restrição adicional que não está presente na Regressão Logística. Os parâmetros do LDA ($\mu_k, \Sigma, \pi_k$) devem não apenas explicar a separação entre as classes (como capturado por $Pr(G|X)$), mas também devem ser consistentes com a distribuição observada dos próprios dados $X$, conforme modelado pela mistura de Gaussianas $Pr(X)$ (Eq. 4.38 [^16]).

> *A verossimilhança marginal pode ser pensada como um regularizador, exigindo, em certo sentido, que as densidades de classe sejam visíveis a partir dessa visão marginal.* [^19]

Esta exigência de "visibilidade" significa que os parâmetros estimados devem gerar densidades de classe $\phi(X; \mu_k, \Sigma)$ que, quando combinadas com as prioris $\pi_k$, reproduzam razoavelmente a distribuição marginal empírica dos dados. Isso impede que os parâmetros assumam valores extremos que poderiam explicar perfeitamente a separação condicional $Pr(G|X)$ mas que seriam inconsistentes com a estrutura global de $Pr(X)$. Em contraste, a Regressão Logística, ao focar apenas na verossimilhança condicional, não possui essa restrição imposta pela estrutura marginal dos dados $X$ [^14].

#### Degenerescência em Dados Perfeitamente Separáveis

Um cenário onde a diferença entre as abordagens de estimação se torna dramaticamente aparente é quando os dados de treinamento são **perfeitamente separáveis** por um hiperplano.

**Regressão Logística:** Como mencionado no texto e detalhado no Exercício 4.5 [^20, ^25], se os dados de um problema de duas classes podem ser perfeitamente separados por um hiperplano, as estimativas de máxima verossimilhança (condicional) para os parâmetros da Regressão Logística ($\beta_0, \beta$) são indefinidas, tendendo ao infinito [^20]. Isso ocorre porque, para separar perfeitamente os pontos, a função sigmóide precisa se tornar uma função degrau, o que requer que a magnitude dos coeficientes $\beta$ vá para o infinito. A log-verossimilhança condicional pode ser levada arbitrariamente perto de seu valor máximo (zero, neste caso) aumentando continuamente a magnitude dos coeficientes [^23].

**Linear Discriminant Analysis:** No mesmo cenário de dados perfeitamente separáveis, os coeficientes do LDA permanecem bem definidos [^21]. A razão reside precisamente no papel regularizador da verossimilhança marginal. Embora um conjunto de parâmetros com magnitude infinita pudesse, teoricamente, separar perfeitamente as classes (maximizando a parte condicional da verossimilhança), ele seria inconsistente com a modelagem da densidade marginal $Pr(X)$ (Eq. 4.38 [^16]). A necessidade de ajustar a mistura de Gaussianas à distribuição global dos dados $X$ impede que as médias $\mu_k$ se afastem indefinidamente ou que a covariância $\Sigma$ colapse de forma a permitir separação perfeita com densidade zero onde os dados existem.

> *Os coeficientes do LDA para os mesmos dados [perfeitamente separáveis] serão bem definidos, uma vez que a verossimilhança marginal não permitirá essas degenerescências.* [^21]

Essencialmente, a verossimilhança marginal $Pr(X)$ ancora as estimativas dos parâmetros, exigindo que eles expliquem não apenas *como* as classes diferem, mas também *onde* os dados de cada classe estão localizados no espaço de características, de uma forma que seja globalmente consistente com a distribuição marginal observada.

#### Eficiência versus Robustez

A incorporação da informação marginal $Pr(X)$ pelo LDA traz consigo um trade-off. Se as suposições do modelo LDA (Gaussianidade, covariância comum) forem verdadeiras, o LDA utilizará mais informação dos dados do que a Regressão Logística [^17]. Isso pode levar a estimativas de parâmetros mais eficientes (menor variância) [^17]. De fato, Efron (1975) mostrou que ignorar a parte marginal da verossimilhança pode levar a uma perda de eficiência assintótica [^17]. No entanto, essa eficiência aumentada vem ao custo de uma maior sensibilidade às suposições do modelo. Se as densidades de classe não forem Gaussianas ou as covariâncias não forem iguais, as estimativas do LDA podem ser viesadas. A Regressão Logística, fazendo menos suposições (apenas sobre a forma de $Pr(G|X)$), é geralmente considerada mais segura e robusta a desvios das suposições de distribuição [^22]. Além disso, a dependência do LDA da estimativa da matriz de covariância $\Sigma$ usando todos os pontos, incluindo aqueles distantes da fronteira de decisão [^18, ^24], torna-o potencialmente mais sensível a outliers do que a Regressão Logística, que efetivamente dá menos peso a pontos longe da fronteira através da matriz de pesos $W$ no IRLS [^10, ^18].

### Conclusão

A comparação entre Linear Discriminant Analysis e Regressão Logística transcende a mera observação de que ambos podem produzir fronteiras de decisão lineares. A diferença fundamental reside em seus frameworks de estimação: LDA maximiza a verossimilhança conjunta $Pr(X, G)$, incorporando assim a estrutura da densidade marginal $Pr(X)$ [^15, ^16], enquanto a Regressão Logística maximiza a verossimilhança condicional $Pr(G|X)$, ignorando $Pr(X)$ [^8, ^14]. Demonstramos, com base no contexto fornecido, que a dependência do LDA na verossimilhança marginal atua como um **regularizador implícito** [^19]. Esta regularização é particularmente evidente em cenários de dados perfeitamente separáveis, onde impede a degenerescência (parâmetros infinitos) observada na Regressão Logística [^20, ^21]. A exigência de que as densidades de classe estimadas sejam "visíveis" na densidade marginal [^19] ancora os parâmetros do LDA. No entanto, esta característica está ligada às suposições Gaussianas do LDA, resultando num trade-off: maior eficiência estatística se as suposições forem válidas [^17], mas menor robustez a violações dessas suposições e a outliers em comparação com a abordagem mais flexível da Regressão Logística [^22]. A escolha entre LDA e LR, portanto, depende do conhecimento prévio sobre os dados e do equilíbrio desejado entre eficiência e robustez.

### Referências

[^1]: Page 102: "...linear discriminant analysis and linear logistic regression. Although they differ in their derivation, the essential difference between them is in the way the linear function is fit to the training data."
[^2]: Page 108: "Suppose that we model each class density as multivariate Gaussian ... $f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}} e^{-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)}$ (Eq. 4.8)"
[^3]: Page 108: "Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix $\Sigma_k = \Sigma \forall k$. In comparing two classes k and l, it is sufficient to look at the log-ratio... $\log \frac{Pr(G=k|X=x)}{Pr(G=l|X=x)} = \log \frac{f_k(x)}{f_l(x)} + \log \frac{\pi_k}{\pi_l} = \log \frac{\pi_k}{\pi_l} - \frac{1}{2}(\mu_k + \mu_l)^T\Sigma^{-1}(\mu_k - \mu_l) + x^T\Sigma^{-1}(\mu_k - \mu_l)$ (Eq. 4.9), an equation linear in x."
[^4]: Page 109: "In practice we do not know the parameters of the Gaussian distributions, and will need to estimate them using our training data: $\hat{\pi}_k = N_k/N$, $\hat{\mu}_k = \sum_{g_i=k} x_i / N_k$, $\hat{\Sigma} = \sum_{k=1}^K \sum_{g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T / (N-K)$."
[^5]: Page 110: "Since this derivation of the LDA direction via least squares does not use a Gaussian assumption for the features, its applicability extends beyond the realm of Gaussian data. However the derivation of the particular intercept or cut-point given in (4.11) does require Gaussian data."
[^6]: Page 110: "Getting back to the general discriminant problem (4.8), if the $\Sigma_k$ are not assumed to be equal... We then get quadratic discriminant functions (QDA)... $\delta_k(x) = -\frac{1}{2}\log|\Sigma_k| - \frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + \log \pi_k$ (Eq. 4.12)."
[^7]: Page 119: "The logistic regression model... has the form $\log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \beta_{k0} + \beta_k^T x$ (Eq. 4.17)... $Pr(G=k|X=x) = \frac{\exp(\beta_{k0}+\beta_k^T x)}{1+\sum_{l=1}^{K-1}\exp(\beta_{l0}+\beta_l^T x)}$ (Eq. 4.18)."
[^8]: Page 120: "Logistic regression models are usually fit by maximum likelihood, using the conditional likelihood of G given X. Since Pr(G|X) completely specifies the conditional distribution, the multinomial distribution is appropriate. The log-likelihood for N observations is $l(\theta) = \sum_{i=1}^N \log p_{g_i}(x_i; \theta)$ (Eq. 4.19), where $p_k(x_i; \theta) = Pr(G=k|X=x_i; \theta)$."
[^9]: Page 120: "To maximize the log-likelihood, we set its derivatives to zero. These score equations are $\frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^N x_i(y_i - p(x_i; \beta)) = 0$ (Eq. 4.21)." (Shown for two-class case)
[^10]: Page 121: "This algorithm is referred to as iteratively reweighted least squares or IRLS, since each iteration solves the weighted least squares problem: $\beta^{new} \leftarrow \arg \min_{\beta} (z - X\beta)^T W (z - X\beta)$ (Eq. 4.28)."
[^11]: Page 127: "In Section 4.3 we find that the log-posterior odds between class k and K are linear functions of x (4.9): $\log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \alpha_{k0} + \alpha_k^T x$ (Eq. 4.33). This linearity is a consequence of the Gaussian assumption for the class densities, as well as the assumption of a common covariance matrix."
[^12]: Page 127: "The linear logistic model (4.17) by construction has linear logits: $\log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \beta_{k0} + \beta_k^T x$ (Eq. 4.34)."
[^13]: Page 127: "It seems that the models are the same. Although they have exactly the same form, the difference lies in the way the linear coefficients are estimated."
[^14]: Page 127: "The logistic regression model leaves the marginal density of X as an arbitrary density function Pr(X), and fits the parameters of Pr(G|X) by maximizing the conditional likelihood... Although Pr(X) is totally ignored..."
[^15]: Page 127: "With LDA we fit the parameters by maximizing the full log-likelihood, based on the joint density $Pr(X, G=k) = \phi(X; \mu_k, \Sigma)\pi_k$ (Eq. 4.37)."
[^16]: Page 128: "...the marginal density Pr(X) does play a role here. It is a mixture density $Pr(X) = \sum_{k=1}^K \pi_k \phi(X; \mu_k, \Sigma)$ (Eq. 4.38), which also involves the parameters."
[^17]: Page 128: "By relying on the additional model assumptions, we have more information about the parameters, and hence can estimate them more efficiently (lower variance). If in fact the true $f_k(x)$ are Gaussian, then in the worst case ignoring this marginal part of the likelihood constitutes a loss of efficiency..."
[^18]: Page 128: "For example, observations far from the decision boundary (which are down-weighted by logistic regression) play a role in estimating the common covariance matrix [in LDA]."
[^19]: Page 128: "The marginal likelihood can be thought of as a regularizer, requiring in some sense that class densities be visible from this marginal view."
[^20]: Page 128: "For example, if the data in a two-class logistic regression model can be perfectly separated by a hyperplane, the maximum likelihood estimates of the parameters are undefined (i.e., infinite; see Exercise 4.5)."
[^21]: Page 128: "The LDA coefficients for the same data will be well defined, since the marginal likelihood will not permit these degeneracies."
[^22]: Page 128: "In practice these assumptions are never correct... It is generally felt that logistic regression is a safer, more robust bet than the LDA model, relying on fewer assumptions."
[^23]: Page 134: "When a separating hyperplane exists, logistic regression will always find it, since the log-likelihood can be driven to 0 in this case (Exercise 4.5)."
[^24]: Page 134: "The LDA solution, on the other hand, depends on all of the data, even points far away from the decision boundary."
[^25]: Page 136: "Ex. 4.5 Consider a two-class logistic regression problem with x ∈ IR. Characterize the maximum-likelihood estimates of the slope and intercept parameter if the sample xi for the two classes are separated by a point x0 ∈ IR. Generalize this result to (a) x ∈ IRP..."

<!-- END -->