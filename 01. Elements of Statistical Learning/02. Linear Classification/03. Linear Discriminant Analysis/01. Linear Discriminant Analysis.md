## Fundamentos da Análise Discriminante Linear Baseada na Teoria de Decisão Bayesiana

### Introdução

Este capítulo aprofunda-se na **Análise Discriminante Linear (LDA)**, um método fundamental dentro do escopo de **métodos lineares para classificação** [^2]. Como explorado anteriormente, o problema de classificação consiste em atribuir uma observação, representada por um vetor de características $x$, a uma de $K$ classes discretas $\mathcal{G}$. Uma abordagem comum envolve a divisão do espaço de entrada em regiões, onde cada região é associada a uma classe específica [^3]. A natureza das fronteiras entre essas regiões define a complexidade do classificador. LDA pertence à classe de métodos onde essas **fronteiras de decisão são lineares** [^3].

A teoria de decisão para classificação, introduzida no Capítulo 2, estabelece que a classificação ótima, no sentido de minimizar a taxa de erro esperada, requer o conhecimento das **probabilidades a posteriori das classes**, $Pr(G=k|X=x)$ [^1]. Estas probabilidades indicam a chance de uma observação $x$ pertencer à classe $k$. A regra de decisão ótima, então, atribui $x$ à classe com a maior probabilidade a posteriori. O **Teorema de Bayes** fornece a ponte para calcular essas posteriors a partir de quantidades potencialmente mais fáceis de modelar: as **densidades condicionais de classe**, $f_k(x) = f(x|G=k)$, e as **probabilidades a priori das classes**, $\pi_k = Pr(G=k)$ [^1, ^12]. Especificamente, temos:

$$ Pr(G=k|X=x) = \frac{f_k(x)\pi_k}{\sum_{l=1}^K f_l(x)\pi_l} \quad \ $$

A derivação da Análise Discriminante Linear surge de uma modelagem específica para as densidades condicionais de classe, $f_k(x)$. Supõe-se que cada $f_k(x)$ segue uma distribuição **Gaussiana multivariada** [^1, ^12]:

$$ f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)\right) \quad \ $$

onde $p$ é a dimensão do espaço de entrada, $\mu_k$ é o vetor de médias da classe $k$, e $\Sigma_k$ é a matriz de covariância da classe $k$. A LDA introduz uma simplificação crucial ao assumir que todas as classes compartilham uma **matriz de covariância comum**, ou seja, $\Sigma_k = \Sigma$ para todo $k=1, \dots, K$ [^1, ^13]. Como demonstraremos, essa suposição leva diretamente a **funções discriminantes lineares**, $\delta_k(x)$, e, consequentemente, a fronteiras de decisão que são **hiperplanos** no espaço de entrada $p$-dimensional [^1, ^13]. Esta abordagem difere da Regressão Linear de uma Matriz Indicadora (Seção 4.2), notavelmente evitando problemas como o mascaramento de classes observado naquele método [^8, ^11].

### Conceitos Fundamentais

#### Derivação da Função Discriminante Linear

Para classificar uma nova observação $x$, comparamos as probabilidades a posteriori $Pr(G=k|X=x)$ para todas as classes $k$. Como o denominador na Eq 4.7 [^12] é comum a todas as classes, basta comparar os numeradores $f_k(x)\pi_k$. É computacionalmente mais conveniente trabalhar com o logaritmo dessas quantidades, pois o logaritmo é uma função monotônica crescente e transforma produtos em somas. Assim, a regra de decisão é equivalente a atribuir $x$ à classe $k$ que maximiza $\log(f_k(x)\pi_k) = \log f_k(x) + \log \pi_k$.

Considerando a forma Gaussiana de $f_k(x)$ (Eq 4.8 [^12]) e a suposição de matriz de covariância comum $\Sigma_k = \Sigma$ [^13], o termo $\log f_k(x)$ torna-se:

$$ \log f_k(x) = -\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) - \frac{p}{2}\log(2\pi) - \frac{1}{2}\log|\Sigma|\ $$

Expandindo o termo quadrático $(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)$, obtemos:

$$ (x-\mu_k)^T\Sigma^{-1}(x-\mu_k) = x^T\Sigma^{-1}x - 2x^T\Sigma^{-1}\mu_k + \mu_k^T\Sigma^{-1}\mu_k\ $$

Note que o termo $x^T\Sigma^{-1}x$ é quadrático em $x$. No entanto, ao comparar duas classes $k$ e $l$, este termo, assim como os termos constantes $-\frac{p}{2}\log(2\pi)$ e $-\frac{1}{2}\log|\Sigma|$, aparecerá em ambas as expressões $\log(f_k(x)\pi_k)$ e $\log(f_l(x)\pi_l)$ e, portanto, se cancelará na comparação.

Para encontrar a fronteira de decisão entre as classes $k$ e $l$, procuramos o conjunto de pontos $x$ onde as probabilidades (ou seus logaritmos) são iguais: $\log(f_k(x)\pi_k) = \log(f_l(x)\pi_l)$. Usando a expressão para $\log f_k(x)$ e cancelando os termos comuns, a condição de igualdade se torna:

$$ -\frac{1}{2}(-2x^T\Sigma^{-1}\mu_k + \mu_k^T\Sigma^{-1}\mu_k) + \log \pi_k = -\frac{1}{2}(-2x^T\Sigma^{-1}\mu_l + \mu_l^T\Sigma^{-1}\mu_l) + \log \pi_l\ $$

Rearranjando os termos, obtemos:

$$ x^T\Sigma^{-1}(\mu_k - \mu_l) - \frac{1}{2}(\mu_k^T\Sigma^{-1}\mu_k - \mu_l^T\Sigma^{-1}\mu_l) + \log\frac{\pi_k}{\pi_l} = 0\ $$

Esta equação é claramente *linear* em $x$. O termo $x^T\Sigma^{-1}(\mu_k - \mu_l)$ é linear em $x$, e os demais termos são constantes com respeito a $x$. Isso confirma que a fronteira de decisão entre quaisquer duas classes $k$ e $l$, sob as suposições da LDA, é um hiperplano [^13]. A equação completa para o log-ratio das posteriors é dada por:

$$ \log \frac{Pr(G=k|X=x)}{Pr(G=l|X=x)} = \log \frac{f_k(x)}{f_l(x)} + \log \frac{\pi_k}{\pi_l} = \log \frac{\pi_k}{\pi_l} - \frac{1}{2}(\mu_k+\mu_l)^T\Sigma^{-1}(\mu_k-\mu_l) + x^T\Sigma^{-1}(\mu_k-\mu_l) \quad \ $$

A regra de classificação $G(x) = \text{argmax}_k Pr(G=k|X=x)$ é equivalente a $G(x) = \text{argmax}_k [\log f_k(x) + \log \pi_k]$. Ignorando os termos constantes que não dependem de $k$ (como $x^T\Sigma^{-1}x$, $-\frac{p}{2}\log(2\pi)$, $-\frac{1}{2}\log|\Sigma|$), podemos definir a **função discriminante linear** $\delta_k(x)$ como:

$$ \delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log \pi_k \quad \ $$

A regra de classificação torna-se então simplesmente $G(x) = \text{argmax}_k \delta_k(x)$ [^14]. A fronteira entre as classes $k$ e $l$ é o conjunto de $x$ tal que $\delta_k(x) = \delta_l(x)$, que, como vimos, define um hiperplano.

> **Caixa de Destaque: Linearidade das Fronteiras de Decisão em LDA**
> A suposição crucial da LDA é que as densidades condicionais de classe $f_k(x)$ são Gaussianas multivariadas com uma *matriz de covariância comum* $\Sigma_k = \Sigma$. Esta suposição leva ao cancelamento dos termos quadráticos $x^T\Sigma^{-1}x$ ao comparar os logaritmos das densidades ponderadas pelas priors, $\log(f_k(x)\pi_k)$. O resultado são funções discriminantes $\delta_k(x)$ que são lineares em $x$, e, consequentemente, as fronteiras de decisão entre pares de classes são hiperplanos [^1, ^13].

#### Estimação dos Parâmetros

Na prática, os parâmetros das distribuições Gaussianas ($\mu_k$ e $\Sigma$) e as probabilidades a priori ($\pi_k$) são desconhecidos e precisam ser estimados a partir dos dados de treinamento $\{(x_i, g_i)\}_{i=1}^N$, onde $x_i \in \mathbb{R}^p$ e $g_i \in \{1, \dots, K\}$ [^15]. As estimativas padrão são:

*   **Probabilidade a priori:** $\hat{\pi}_k = N_k / N$, onde $N_k$ é o número de observações na classe $k$, e $N = \sum_{k=1}^K N_k$ é o número total de observações [^15].
*   **Vetor de médias da classe:** $\hat{\mu}_k = \frac{1}{N_k} \sum_{i: g_i=k} x_i$ [^15].
*   **Matriz de covariância comum (pooled):** $\hat{\Sigma} = \frac{1}{N-K} \sum_{k=1}^K \sum_{i: g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T$. Esta é uma média ponderada das estimativas de covariância individuais de cada classe, $\hat{\Sigma}_k = \frac{1}{N_k-1} \sum_{i: g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T$, comumente escrita como $\hat{\Sigma} = \sum_{k=1}^K \frac{N_k-1}{N-K} \hat{\Sigma}_k$ [^15]. O denominador $N-K$ é usado para obter um estimador não enviesado.

Substituindo esses estimadores $\hat{\pi}_k, \hat{\mu}_k, \hat{\Sigma}$ nas expressões para $\delta_k(x)$ (Eq 4.10 [^14]), obtemos as funções discriminantes estimadas $\hat{\delta}_k(x)$ e a regra de classificação $\hat{G}(x) = \text{argmax}_k \hat{\delta}_k(x)$.

#### Relação com Regressão Linear por Mínimos Quadrados (Caso K=2)

Existe uma correspondência interessante entre LDA e regressão linear por mínimos quadrados quando há apenas duas classes ($K=2$) [^16]. Suponha que codificamos as classes usando variáveis alvo $y_i$, por exemplo, $+1$ para a classe 2 e $-1$ para a classe 1 (ou outra codificação distinta como $-N/N_1$ e $N/N_2$ [^35]). Se ajustarmos um modelo de regressão linear $f(x) = \beta_0 + \beta^T x$ por mínimos quadrados para prever $y$ a partir de $x$, pode-se mostrar (ver Exercício 4.2 [^35]) que o vetor de coeficientes $\hat{\beta}$ resultante é proporcional à direção discriminante da LDA, $(\hat{\mu}_2 - \hat{\mu}_1)^T \hat{\Sigma}^{-1}$ ou, equivalentemente, $\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1)$ [^17, ^35].

A regra de classificação LDA para duas classes pode ser escrita como classificar para a classe 2 se (reorganizando Eq 4.9 [^13] ou diretamente de Eq 4.11 [^16]):

$$ x^T\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1) > \frac{1}{2}(\hat{\mu}_2+\hat{\mu}_1)^T\hat{\Sigma}^{-1}(\hat{\mu}_2-\hat{\mu}_1) - \log(\hat{\pi}_2/\hat{\pi}_1)\ $$

A regra baseada na regressão por mínimos quadrados seria classificar para a classe 2 se $\hat{f}(x) = \hat{\beta}_0 + \hat{\beta}^T x > c$ para algum limiar $c$ (frequentemente $c=0$ para codificação $\pm 1$). Embora as direções $\hat{\beta}$ e $\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1)$ sejam as mesmas (a menos de um escalar), os interceptos ($\hat{\beta}_0$ e o termo constante na regra LDA) e, portanto, os limiares de decisão, geralmente diferem, a menos que os tamanhos das classes sejam iguais ($N_1 = N_2$) [^17].

É importante notar que a derivação da *direção* discriminante via mínimos quadrados não requer a suposição Gaussiana para os dados $x$ [^18]. No entanto, a derivação do *intercepto* específico (cut-point) dado na Eq 4.11 [^16] *depende* da suposição Gaussiana e das priors estimadas [^18]. Isso sugere que a direção encontrada por regressão pode ser útil mesmo fora do modelo Gaussiano, mas o limiar ótimo pode precisar ser ajustado empiricamente [^18]. Além disso, para $K > 2$ classes, a LDA não é equivalente à regressão linear da matriz indicadora, e a LDA geralmente demonstra melhor desempenho, evitando os problemas de mascaramento associados à regressão [^19].

### Conclusão

A Análise Discriminante Linear (LDA) fornece um método de classificação fundamentado na teoria de decisão Bayesiana, sob as suposições de que as densidades condicionais de classe são Gaussianas multivariadas e compartilham uma matriz de covariância comum [^1, ^13]. Essas suposições resultam em funções discriminantes que são lineares nas variáveis de entrada $x$, levando a fronteiras de decisão que são hiperplanos [^13, ^14]. Apresentamos a derivação dessas funções discriminantes $\delta_k(x)$ e discutimos a estimação dos parâmetros necessários a partir dos dados de treinamento [^15]. Exploramos também a conexão da LDA com a regressão linear por mínimos quadrados no caso de duas classes, destacando a similaridade na direção discriminante, mas diferenças nos interceptos [^16, ^17, ^18]. A LDA representa uma ferramenta poderosa e interpretável para classificação linear, muitas vezes servindo como um benchmark importante e evitando certas patologias de métodos alternativos como a regressão de indicadores [^11, ^19]. As extensões para matrizes de covariância desiguais (Análise Discriminante Quadrática, QDA [^20]) e regularização (Análise Discriminante Regularizada, RDA [^21]), bem como abordagens de redução de dimensionalidade (LDA de Rank Reduzido [^24]), serão abordadas subsequentemente.

### Referências

[^1]: Page 106, Section 4.3: "Decision theory for classification (Section 2.4) tells us that we need to know the class posteriors Pr(G|X) for optimal classification. Suppose fk(x) is the class-conditional density of X in class G = k, and let πk be the prior probability of class k... Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Σk = Σ ∀k... This linear log-odds function implies that the decision boundary between classes k and l...is linear in x; in p dimensions a hyperplane." (Implicitly links LDA to Bayes theory, Gaussian densities, common covariance, and linear boundaries). *Also used for the subtopic description itself.*
[^2]: Page 101, Chapter 4 Title: "Linear Methods for Classification".
[^3]: Page 101, Section 4.1: "...divide the input space into a collection of regions labeled according to the classification... For an important class of procedures, these decision boundaries are linear; this is what we will mean by linear methods for classification."
[^4]: Page 102, Section 4.1: "Here the monotone transformation is the logit transformation: log[p/(1-p)]... The decision boundary is the set of points for which the log-odds are zero, and this is a hyperplane..."
[^5]: Page 102, Section 4.1: "We discuss two very popular but different methods that result in linear log-odds or logits: linear discriminant analysis and linear logistic regression."
[^6]: Page 103, Section 4.2: "We fit a linear regression model to each of the columns of Y simultaneously, and the fit is given by Y = X(XTX)−1XTY. (4.3)"
[^7]: Page 103, Section 4.2: "A new observation with input x is classified as follows: • compute the fitted output f(x)T = (1, xT)B, a K vector; • identify the largest component and classify accordingly: Gˆ(x) = argmaxk∈G fk(x). (4.4)"
[^8]: Page 105, Figure 4.2 Caption: "The left plot shows the boundaries found by linear regression of the indicator response variables. The middle class is completely masked (never dominates)."
[^9]: Page 106, Figure 4.3 Caption: "The effects of masking on linear regression in IR for a three-class problem."
[^10]: Page 107, Table 4.1: Shows error rates for Linear Regression (0.67 test) vs LDA (0.56 test) on vowel data.
[^11]: Page 106, Section 4.2 End: "It seems that masking has hurt in this case. While all the other methods in this chapter are based on linear functions of x as well, they use them in such a way that avoids this masking problem."
[^12]: Page 108, Section 4.3 Equations: Eq (4.7) $Pr(G=k|X=x) = \frac{f_k(x)\pi_k}{\sum_{l=1}^K f_l(x)\pi_l}$, Eq (4.8) $f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}} \exp(-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k))$.
[^13]: Page 108, Section 4.3 Text and Equation: "Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Σk = Σ ∀k... we see that log Pr(G=k|X=x)/Pr(G=l|X=x) = log(πk/πl) - 1/2(μk+μl)TΣ⁻¹(μk-μl) + xTΣ⁻¹(μk-μl), (4.9) an equation linear in x... This linear log-odds function implies that the decision boundary... is linear in x; in p dimensions a hyperplane."
[^14]: Page 109, Section 4.3 Equation and Text: "From (4.9) we see that the linear discriminant functions δk(x) = xTΣ⁻¹μk - 1/2μk^TΣ⁻¹μk + log πk (4.10) are an equivalent description of the decision rule, with G(x) = argmaxk δk(x)."
[^15]: Page 109, Section 4.3 Text: "In practice we do not know the parameters... and will need to estimate them using our training data: • πˆk = Nk/N... • μˆk = Σgi=k xi/Nk; • Σˆ = Σk=1^K Σgi=k(xi - μˆk)(xi - μˆk)T/(N - K)."
[^16]: Page 109, Section 4.3 Equation: "The LDA rule classifies to class 2 if xTΣˆ⁻¹(μˆ2 - μˆ1) > 1/2 μˆ2^TΣˆ⁻¹μˆ2 - 1/2 μˆ1^TΣˆ⁻¹μˆ1 + log(πˆ1/πˆ2) (implicitly from Eq 4.11)" - Note: Eq 4.11 actually shows the inequality rearranged, this text describes the direct comparison. The text refers to Eq (4.5) for least squares.
[^17]: Page 110, Section 4.3 Text: "It is easy to show that the coefficient vector from least squares is proportional to the LDA direction given in (4.11) (Exercise 4.2)... However unless N1 = N2 the intercepts are different and hence the resulting decision rules are different."
[^18]: Page 110, Section 4.3 Text: "Since this derivation of the LDA direction via least squares does not use a Gaussian assumption for the features, its applicability extends beyond the realm of Gaussian data. However the derivation of the particular intercept or cut-point given in (4.11) does require Gaussian data. Thus it makes sense to instead choose the cut-point that empirically minimizes training error..."
[^19]: Page 110, Section 4.3 Text: "With more than two classes, LDA is not the same as linear regression of the class indicator matrix, and it avoids the masking problems associated with that approach..."
[^20]: Page 110, Section 4.3 Text and Equation: "if the Σk are not assumed to be equal... We then get quadratic discriminant functions (QDA), δk(x) = -1/2 log|Σk| - 1/2(x - μk)TΣk⁻¹(x - μk) + log πk. (4.12)"
[^21]: Page 112, Section 4.3.1 Equation: "The regularized covariance matrices have the form Σk(α) = αΣk + (1 - α)Σˆ, (4.13)"
[^22]: Page 112, Section 4.3.1 Equation: "Similar modifications allow Σˆ itself to be shrunk toward the scalar covariance, Σˆ(γ) = γΣˆ + (1 - γ)σˆ²I (4.14)"
[^23]: Page 113, Section 4.3.2 Text: "Sphere the data with respect to the common covariance estimate Σˆ: X* ← D⁻¹/² U^T X, where Σˆ = UDUT... Classify to the closest class centroid in the transformed space..."
[^24]: Page 113, Section 4.3.3 Title: "Reduced-Rank Linear Discriminant Analysis"
[^25]: Page 113, Section 4.3.3 Text: "The K centroids in p-dimensional input space lie in an affine subspace of dimension ≤ K - 1... Thus there is a fundamental dimension reduction in LDA..."
[^26]: Page 114, Section 4.3.3 Text: "Fisher arrived at this decomposition via a different route... He posed the problem: Find the linear combination Z = a^T X such that the between-class variance is maximized relative to the within-class variance."
[^27]: Page 114, Section 4.3.3 Text: "The between-class variance of Z is a^T B a and the within-class variance a^T W a..."
[^28]: Page 116, Section 4.3.3 Text: "...referred to as canonical variates, since an alternative derivation of these results is through a canonical correlation analysis of the indicator response matrix Y on the predictor matrix X." Also Page 119, Section 4.4 Text: "LDA amounts to the regression followed by an eigen-decomposition of YˆT Yˆ."
[^29]: Page 117, Section 4.3.3 Text: "One can show that this is a Gaussian classification rule with the additional restriction that the centroids of the Gaussians lie in a L-dimensional subspace of IRP."
[^30]: Page 117, Section 4.3.3 Text: "Gaussian classification dictates the log πk correction factor in the distance calculation."
[^31]: Page 117, Figure 4.10 Caption: "Training and test error rates for the vowel data, as a function of the dimension of the discriminant subspace."
[^32]: Page 118, Figure 4.11 Caption: "Decision boundaries for the vowel training data, in the two-dimensional subspace spanned by the first two canonical variates."
[^33]: Page 119, Section 4.4 Title: "Logistic Regression"
[^34]: Page 127, Section 4.4.5 Title: "Logistic Regression or LDA?"
[^35]: Page 127, Section 4.4.5 Equations and Text: Compares Eq (4.33) (LDA log-odds) and Eq (4.34) (Logistic Regression log-odds). "Although they have exactly the same form, the difference lies in the way the linear coefficients are estimated." Also Exercise 4.2 on page 135 confirms the LS/LDA direction link.
[^36]: Page 127, Section 4.4.5 Text: "The logistic regression model... fits the parameters of Pr(G|X) by maximizing the conditional likelihood... With LDA we fit the parameters by maximizing the full log-likelihood, based on the joint density Pr(X, G = k) = φ(X; μk, Σ)πk, (4.37)"
[^37]: Page 128, Section 4.4.5 Text: "...unlike in the conditional case, the marginal density Pr(X) does play a role here. It is a mixture density Pr(X) = Σk πk φ(X; μk, Σ), (4.38)"
[^38]: Page 128, Section 4.4.5 Text: "...it also means that LDA is not robust to gross outliers... It is generally felt that logistic regression is a safer, more robust bet than the LDA model, relying on fewer assumptions."
[^39]: Page 129, Section 4.5 Text: "This is the same boundary found by LDA, in light of its equivalence with linear regression in the two-class case (Section 4.3 and Exercise 4.2)."
[^40]: Page 134, Figure 4.16 Caption and Text: Compares optimal separating hyperplane with logistic regression and implicitly LDA. Notes LDA depends on all data, while optimal hyperplane focuses on support points.

<!-- END -->