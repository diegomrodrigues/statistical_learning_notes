## Capítulo 4.3.A: Correspondência entre LDA e Regressão Linear por Mínimos Quadrados para Duas Classes

### Introdução

Como explorado anteriormente na Seção 4.3, a **Linear Discriminant Analysis (LDA)** fornece um método poderoso para classificação, particularmente sob a suposição de densidades de classe Gaussianas com matrizes de covariância comuns [^7], [^8]. Vimos também na Seção 4.2 que a **regressão linear de uma matriz indicadora** pode ser utilizada para classificação, ajustando um modelo linear para cada variável indicadora de classe [^3], [^4]. Este capítulo aprofunda a relação entre estas duas abordagens no cenário específico, mas fundamental, de problemas de classificação com apenas duas classes ($K=2$). Demonstraremos uma correspondência direta e matematicamente elegante entre a LDA e a classificação por **linear least squares (LLS)** [^9], focando tanto na direção do hiperplano de decisão quanto nas implicações para a regra de classificação final.

### Conceitos Fundamentais

#### Revisão da Regra LDA para Duas Classes

Relembrando a derivação da LDA [^8], assumimos que as densidades condicionais de classe $f_k(x)$ para $k=1, 2$ são Gaussianas multivariadas com médias $\mu_1, \mu_2$ e uma matriz de covariância comum $\Sigma$ [^7]. A regra de decisão ótima, baseada no teorema de Bayes, aloca uma nova observação $x$ à classe que maximiza a probabilidade a posteriori $Pr(G=k|X=x)$. Para duas classes, comparar as probabilidades a posteriori é equivalente a analisar o sinal do log-ratio [^8]:
$$ \log \frac{Pr(G=1|X=x)}{Pr(G=2|X=x)} = \log \frac{f_1(x)\pi_1}{f_2(x)\pi_2} = \log \frac{\pi_1}{\pi_2} - \frac{1}{2}(\mu_1+\mu_2)^T\Sigma^{-1}(\mu_1-\mu_2) + x^T\Sigma^{-1}(\mu_1-\mu_2)\ $$
A fronteira de decisão é o conjunto de pontos onde este log-ratio é zero, que é um hiperplano [^8]. A regra de classificação LDA, usando estimativas $\hat{\mu}_1, \hat{\mu}_2, \hat{\Sigma}, \hat{\pi}_1, \hat{\pi}_2$ (onde $\hat{\pi}_k = N_k/N$) a partir dos dados de treino [^9], classifica para a classe 2 se:
$$ x^T\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1) > \frac{1}{2}\hat{\mu}_2^T\hat{\Sigma}^{-1}\hat{\mu}_2 - \frac{1}{2}\hat{\mu}_1^T\hat{\Sigma}^{-1}\hat{\mu}_1 + \log(\hat{\pi}_1/\hat{\pi}_2)\ \qquad (4.11 \text{ adaptado})\ $$
[^10]. O termo chave aqui é o vetor de direção **$\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1)$**, que define a orientação do hiperplano de separação LDA.

#### Revisão da Classificação por Regressão Linear (LLS)

A abordagem de classificação via regressão linear, conforme discutido na Seção 4.2, envolve codificar as classes de resposta $G$ usando variáveis indicadoras $Y_k$ [^3]. Para $K=2$, podemos simplificar usando uma única variável resposta $Y$, codificando as duas classes com valores numéricos distintos. Uma escolha comum é $y_i \in \{+1, -1\}$ [^11] ou $y_i \in \{0, 1\}$ [^20]. Alternativamente, pode-se usar uma codificação como $y_i = -N/N_1$ para a classe 1 e $y_i = N/N_2$ para a classe 2 [^14]. Ajustamos então um modelo de regressão linear, incluindo um intercepto:
$$ \hat{f}(x) = \hat{\beta}_0 + \hat{\beta}^T x\ $$
minimizando a soma dos quadrados dos erros (critério LLS) [^6], [^14]:
$$ \min_{\beta_0, \beta} \sum_{i=1}^N (y_i - \beta_0 - \beta^T x_i)^2 \qquad (4.55)\ $$
A classificação de uma nova observação $x$ é então baseada no sinal de $\hat{f}(x)$ [^29] ou comparando $\hat{f}(x)$ a um limiar, dependendo da codificação.

#### Demonstração da Correspondência (Direção)

Existe uma correspondência notável entre a direção do vetor de coeficientes $\hat{\beta}$ obtido por LLS e a direção do discriminante LDA $\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1)$ no caso de duas classes [^9]. Para demonstrar isso, vamos considerar a codificação de alvos $y_i = -N/N_1$ se $g_i=1$ e $y_i = N/N_2$ se $g_i=2$ [^14].

**Lema 4.3.A.1:** *Sob a codificação de alvos $y_i = -N/N_1$ para a classe 1 e $y_i = N/N_2$ para a classe 2, o vetor de coeficientes $\hat{\beta}$ da regressão linear por mínimos quadrados (excluindo o intercepto) satisfaz a equação:*\
$$ \left( (N-2)\hat{\Sigma} + \frac{N_1 N_2}{N} \Sigma_B \right) \hat{\beta} = N(\hat{\mu}_2 - \hat{\mu}_1) \qquad (4.56)\ $$
*onde $\hat{\Sigma}$ é a estimativa da matriz de covariância within-class pooled, e $\Sigma_B = (\hat{\mu}_2 - \hat{\mu}_1)(\hat{\mu}_2 - \hat{\mu}_1)^T$ é proporcional à matriz de covariância between-class.*\

**Prova:** A minimização do critério LLS (4.55) leva às equações normais. A derivação detalhada (referida no Exercício 4.2b [^14]) mostra que a solução para $\hat{\beta}$ obedece à relação (4.56). A matriz $\hat{\Sigma}$ é a estimativa usual da covariância pooled [^9], e $\Sigma_B$ captura a variância entre as médias das classes na direção $\hat{\mu}_2 - \hat{\mu}_1$. $\blacksquare$

**Corolário 4.3.A.1:** *O vetor de coeficientes $\hat{\beta}$ da regressão por mínimos quadrados é proporcional à direção LDA:*\
$$ \hat{\beta} \propto \hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1) \qquad (4.57 \text{ adaptado})\ $$

**Prova:** Analisando a equação (4.56), notamos que o termo $\Sigma_B \hat{\beta} = (\hat{\mu}_2 - \hat{\mu}_1)(\hat{\mu}_2 - \hat{\mu}_1)^T \hat{\beta}$ é um vetor na direção de $(\hat{\mu}_2 - \hat{\mu}_1)$, multiplicado por um escalar $(\hat{\mu}_2 - \hat{\mu}_1)^T \hat{\beta}$. Portanto, a equação (4.56) pode ser reescrita como:
$$ (N-2)\hat{\Sigma}\hat{\beta} + \left[ \frac{N_1 N_2}{N} (\hat{\mu}_2 - \hat{\mu}_1)^T \hat{\beta} \right] (\hat{\mu}_2 - \hat{\mu}_1) = N(\hat{\mu}_2 - \hat{\mu}_1)\ $$
Isso implica que $\hat{\Sigma}\hat{\beta}$ deve ser uma combinação linear de $\hat{\beta}$ e $(\hat{\mu}_2 - \hat{\mu}_1)$, mas mais diretamente, a estrutura da equação mostra que $\hat{\Sigma}\hat{\beta}$ está na direção de $(\hat{\mu}_2 - \hat{\mu}_1)$ (ou, mais precisamente, $\hat{\beta}$ é uma combinação linear de $\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1)$). O argumento formal [^14] estabelece que $\hat{\beta}$ é de fato proporcional a $\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1)$ [^14]. $\blacksquare$

#### Análise do Intercepto e Regra de Decisão

Embora as *direções* sejam proporcionais, as regras de decisão completas resultantes de LDA e LLS não são idênticas em geral. A regra LDA (4.11) envolve um limiar específico derivado das médias, covariância e priors [^10]. A regra LLS, tipicamente, classifica para a classe 2 se $\hat{f}(x) = \hat{\beta}_0 + \hat{\beta}^T x > c$ para algum limiar $c$ (frequentemente $c=0$ para codificações simétricas como +1/-1 após centrar, ou um valor dependente da codificação).

O intercepto LLS $\hat{\beta}_0$ pode ser encontrado a partir das equações normais (Exercício 4.2e [^14]). Verifica-se que o limiar implícito na regra de classificação LLS (e.g., $\hat{\beta}_0 + \hat{\beta}^T x > 0$) só coincide com o limiar da regra LDA (4.11) se as classes tiverem tamanhos de amostra (e, portanto, priors estimados) iguais, ou seja, $N_1 = N_2$ [^12], [^14]. Se $N_1 \neq N_2$, os interceptos diferem, e as fronteiras de decisão, embora paralelas (mesma direção normal), estarão deslocadas uma em relação à outra [^12].

#### Implicações das Suposições

Uma vantagem significativa da derivação LLS para a *direção* $\hat{\beta}$ é que ela não requer a suposição de que os dados $X$ seguem uma distribuição Gaussiana dentro de cada classe [^13]. A derivação baseia-se apenas na minimização da soma dos quadrados dos erros. Isso sugere que a direção encontrada pela regressão linear pode ser uma direção razoável para separação de classes mesmo quando a suposição Gaussiana da LDA é violada [^13].

No entanto, a derivação do *intercepto* ou *cut-point* ótimo da LDA, como visto na equação (4.11), *depende* explicitamente da suposição Gaussiana e da covariância comum para cancelar os termos quadráticos [^8], [^13]. Portanto, enquanto a direção LLS é robusta à não-Gaussianidade, o limiar LLS padrão (e.g., $\hat{f}(x)=0$) não tem a mesma justificação teórica de otimalidade que o limiar LDA (sob as suposições da LDA).

### Conclusão

Este capítulo estabeleceu a forte ligação entre a Análise Discriminante Linear (LDA) e a regressão linear por mínimos quadrados (LLS) no contexto de classificação binária. Demonstramos matematicamente que o vetor de direção $\hat{\beta}$ resultante da LLS (com codificação apropriada da variável resposta) é proporcional à direção $\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1)$ que define o hiperplano LDA [^9], [^11], [^14]. Esta correspondência na direção é robusta, não dependendo da suposição Gaussiana [^13]. Contudo, as regras de decisão completas, incluindo os interceptos ou limiares, só coincidem se os tamanhos das classes forem iguais ($N_1 = N_2$) [^12]. A derivação do limiar ótimo da LDA depende da suposição Gaussiana, enquanto o limiar LLS padrão carece dessa justificação teórica de otimalidade em geral. Compreender esta relação fornece insights valiosos sobre a natureza dos classificadores lineares e as implicações das suas suposições subjacentes.

### Referências

[^1]: Page 101: Introduction to linear methods for classification.
[^2]: Page 101: Definition of decision boundary between classes k and l using fitted linear models.
[^3]: Page 103: Description of Linear Regression of an Indicator Matrix (Y).
[^4]: Page 103: Classification rule (4.4) based on the largest fitted value.
[^5]: Page 104: "Closest target" view of regression classification (Equation 4.6).
[^6]: Page 104: Least squares criterion (4.5) for fitting the linear model.
[^7]: Page 108: Introduction to LDA assuming Gaussian densities with common covariance $\Sigma$. Equation (4.8).
[^8]: Page 108: Derivation of the linear log-ratio for LDA (Equation 4.9).
[^9]: Page 109: Statement of the simple correspondence between LDA and LLS for two classes, referencing (4.5).
[^10]: Page 109: LDA classification rule for class 2 (Equation 4.11).
[^11]: Page 110: Mention of +1/-1 coding and proportionality of LLS coefficient vector to LDA direction (Exercise 4.2).
[^12]: Page 110: Statement that intercepts differ unless $N_1 = N_2$.
[^13]: Page 110: Discussion on Gaussian assumption: not needed for LLS direction, but needed for LDA cut-point (4.11). Suggestion of empirical cut-point choice.
[^14]: Page 135 (Exercise 4.2): Mathematical details proving the correspondence (direction and intercept differences). Equation (4.55), (4.56), (4.57).
[^20]: Page 120: Mention of 0/1 coding for the two-class logistic regression case.
[^29]: Page 129: Mention of perceptrons returning the sign of a linear combination.

<!-- END -->