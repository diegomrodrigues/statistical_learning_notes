## Computações, Redução de Dimensionalidade e a Perspectiva de Fisher em LDA

### Introdução

Continuando a exploração da **Linear Discriminant Analysis (LDA)**, este capítulo foca em aspetos computacionais, técnicas de redução de dimensionalidade inerentes ao método e a formulação original de Fisher, que oferece uma perspetiva alternativa e poderosa. Assumindo familiaridade com a derivação básica da LDA a partir de densidades Gaussianas com matriz de covariância comum [^8], [^9], aprofundaremos como os cálculos podem ser otimizados, como a dimensionalidade do problema pode ser fundamentalmente reduzida e como a abordagem de Fisher, focada na separação de classes, leva aos mesmos resultados através de um caminho conceitualmente distinto.

### Conceitos Fundamentais

#### Computações para LDA e QDA

A implementação de LDA e **Quadratic Discriminant Analysis (QDA)** envolve cálculos que podem ser significativamente simplificados através da diagonalização das matrizes de covariância relevantes [^1].

Para QDA, onde cada classe $k$ tem sua própria matriz de covariância $\Sigma_k$, a computação das funções discriminantes quadráticas $\delta_k(x)$ [^4], dadas por:
$$delta_k(x) = -\frac{1}{2}\log |\Sigma_k| - \frac{1}{2}(x - \mu_k)^T\Sigma_k^{-1}(x - \mu_k) + \log \pi_k \quad (4.12)$$nbeneficia da decomposição espectral (eigendecomposition) de cada $\Sigma_k = U_k D_k U_k^T$, onde $U_k$ é ortonormal e $D_k$ é uma matriz diagonal de eigenvalues positivos $d_{kl}$ [^2]. Com esta decomposição, os componentes de $\delta_k(x)$ tornam-se:
*   O termo quadrático: $(x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) = [U_k^T (x - \mu_k)]^T D_k^{-1} [U_k^T (x - \mu_k)]$ [^3]. Este cálculo envolve uma transformação de $(x - \mu_k)$ pelas eigenvectors, seguida por um escalonamento inverso pelos eigenvalues e a soma dos quadrados.\n*   O termo do determinante: $\log|\Sigma_k| = \log \left(\prod_l d_{kl}\right) = \sum_l \log d_{kl}$ [^4].

Para LDA, assume-se uma matriz de covariância comum $\Sigma$. Os cálculos são simplificados de forma análoga. Mais importante, a LDA pode ser implementada através de um processo de *sphering* (esferização) dos dados [^5].

> **Implementação da LDA via Sphering:**
> 1.  Calcule a decomposição espectral da estimativa da matriz de covariância comum: $\hat{\Sigma} = U D U^T$.
> 2.  Transforme (sphere) os dados: $X^* \leftarrow D^{-1/2} U^T X$ [^5]. Nesta nova representação $X^*$, a estimativa da covariância comum torna-se a matriz identidade [^6].
> 3.  Classifique uma nova observação $x^*$ (transformada da mesma forma) para o centroide de classe $\mu_k^*$ mais próximo no espaço transformado, levando em conta as probabilidades a priori $\pi_k$ [^7]. A função discriminante no espaço transformado simplifica-se, pois o termo de covariância é a identidade. A regra de classificação baseia-se em minimizar a distância Euclidiana quadrada aos centroides transformados, ajustada pelos logaritmos das priors: $G(x) = \arg\min_k ||x^* - \mu_k^*||^2 - 2 \log \pi_k$.

Esta abordagem de sphering torna a classificação LDA geometricamente intuitiva como uma atribuição ao centroide mais próximo num espaço onde a variância intra-classe é isotrópica.

#### Reduced-Rank Linear Discriminant Analysis (LDA de Rank Reduzido)

A LDA, para além da sua derivação como um classificador Bayes ótimo sob pressupostos Gaussianos, possui uma propriedade intrínseca de redução de dimensionalidade, particularmente útil quando o número de preditores $p$ é grande comparado com o número de classes $K$ [^8], [^10].

Os $K$ centroides das classes, $\mu_1, \dots, \mu_K$, no espaço de entrada $\mathbb{R}^p$ residem num subespaço afim de dimensão no máximo $K-1$ [^9]. Se $p \gg K-1$, isto representa uma redução dimensional considerável [^10]. A intuição para a classificação LDA é que, ao comparar as distâncias de um ponto $x$ aos diferentes centroides $\mu_k$ (após a esferização), as componentes da distância que são ortogonais a este subespaço afim que contém os centroides contribuem igualmente para a distância a cada centroide e, portanto, podem ser ignoradas [^11]. Assim, a classificação pode ser realizada projetando os dados (esferizados) no subespaço $H_{K-1}$ gerado pelos centroides (esferizados) [^12]. A LDA inerentemente realiza uma redução de dimensionalidade para um espaço de dimensão no máximo $K-1$ [^12].

Frequentemente, é desejável encontrar um subespaço de dimensão ainda menor, $L < K-1$, $H_L \subseteq H_{K-1}$, que seja "ótimo" para a LDA [^14]. Fisher propôs um critério de otimalidade: encontrar o subespaço onde os centroides projetados estejam o mais espalhados possível, medido pela sua variância [^15]. Isto equivale a encontrar os subespaços de componentes principais dos próprios centroides (após a transformação apropriada pela covariância intra-classe) [^16]. A Figura 4.4 no contexto original [^17] ilustra um subespaço ótimo de dimensão $L=2$ para dados com $K=11$ classes e $p=10$ dimensões.

A sequência de subespaços ótimos (e as correspondentes **variáveis discriminantes** ou **variáveis canônicas**) é encontrada através dos seguintes passos [^18], [^19]:
1.  Calcule a matriz $K \times p$ dos centroides de classe, M, e a estimativa da matriz de covariância comum intra-classe, W.
2.  Calcule a matriz de covariância intra-classe "esferizada", por exemplo, usando a decomposição espectral $W = V_W D_W V_W^T$, e obtenha $W^{-1/2} = V_W D_W^{-1/2} V_W^T$. Compute os centroides transformados $M^* = M W^{-1/2}$.
3.  Calcule $B^*$, a matriz de covariância dos centroides transformados $M^*$ (representando a covariância inter-classe no espaço esferizado).
4.  Realize a decomposição espectral de $B^* = V^* D_B V^{*T}$. As colunas $v_l^*$ de $V^*$, ordenadas pelos correspondentes eigenvalues em $D_B$ (do maior para o menor), definem as coordenadas dos subespaços ótimos sequenciais [^19].

A $l$-ésima variável discriminante (ou coordenada ótima) é então dada pela projeção dos dados originais $X$:
$$Z_l = v_l^T X, \quad \text{onde } v_l = W^{-1/2} v_l^*$$
[^20]. Estas variáveis $Z_l$ capturam sucessivamente a maior separação entre as médias das classes, relativa à variância intra-classe.

#### A Abordagem de Fisher para LDA

Fisher chegou a esta mesma decomposição por um caminho diferente, sem fazer referência explícita a distribuições Gaussianas [^21]. Ele colocou o problema diretamente em termos de encontrar projeções que maximizem a separação entre classes [^22]:

> *Find the linear combination $Z = a^T X$ such that the between-class variance is maximized relative to the within-class variance.* [^22]

Aqui, a **variância inter-classe (between-class variance)** de $Z$ é a variância das médias de $Z$ para cada classe, e a **variância intra-classe (within-class variance)** é a variância pooled de $Z$ em torno das respetivas médias de classe [^23]. A Figura 4.9 no contexto original [^24] ilustra que a simples direção que une os centroides (maximizando a variância inter-classe bruta) pode não ser ótima devido à sobreposição induzida pela covariância; a abordagem de Fisher leva em conta essa covariância para encontrar uma direção com sobreposição mínima [^24].

Matematicamente, a variância inter-classe de $Z=a^T X$ é $a^T B a$, onde $B$ é a matriz de covariância da matriz de centroides M. A variância intra-classe é $a^T W a$, onde W é a matriz de covariância intra-classe (pooled) definida anteriormente [^25]. É relevante notar que $B + W = T$, onde T é a matriz de covariância total dos dados X, ignorando a informação de classe [^26].

O problema de Fisher consiste, portanto, em maximizar o **quociente de Rayleigh (Rayleigh quotient)** [^27]:
$$max_a \frac{a^T B a}{a^T W a}$$
Isto é equivalente ao problema de otimização com restrição [^28]:
$$max_a a^T B a \quad \text{sujeito a} \quad a^T W a = 1$$
Este é um **problema de valor próprio generalizado (generalized eigenvalue problem)** [^29]. A solução $a_1$ que maximiza o quociente é o eigenvector correspondente ao maior eigenvalue da matriz $W^{-1}B$ [^29]. Pode-se mostrar que este $a_1$ é idêntico ao $v_1$ encontrado através da abordagem de decomposição sequencial descrita anteriormente [^30]. Subsequentemente, pode-se encontrar a próxima direção $a_2$, que maximiza o quociente sujeito a ser "ortogonal em W" a $a_1$ (i.e., $a_2^T W a_1 = 0$), que corresponde a $v_2$, e assim por diante para $a_l = v_l$ [^31].

Estes vetores $a_l$ são referidos como **coordenadas discriminantes (discriminant coordinates)** ou **variáveis canônicas (canonical variates)** [^32]. Uma derivação alternativa destes resultados provém da análise de correlação canônica (Canonical Correlation Analysis - CCA) entre a matriz indicadora de resposta Y e a matriz de preditores X [^33] (discutido na Seção 12.5 do contexto original).

### Conclusão

Este capítulo detalhou aspetos computacionais e de redução de dimensionalidade da LDA, culminando na perspectiva de Fisher. Vimos que:
*   As computações para LDA e QDA podem ser eficientemente realizadas através da diagonalização das matrizes de covariância [^1]. A LDA, em particular, pode ser implementada intuitivamente através da esferização dos dados e classificação para o centroide mais próximo no espaço transformado [^5], [^7], [^34].
*   A LDA inerentemente realiza uma redução de dimensionalidade, confinando os dados relevantes para a classificação a um subespaço de dimensão no máximo $K-1$ gerado pelos centroides [^9], [^12], [^34].
*   Este subespaço pode ser ainda mais decomposto em subespaços ótimos sucessivos em termos de separação de centroides, através de uma sequência de decomposições espectrais [^19], [^34].
*   A abordagem de Fisher, maximizando a razão entre a variância inter-classe e intra-classe [^22], [^27], fornece uma derivação alternativa e conceitualmente poderosa que leva às mesmas coordenadas discriminantes ótimas (variáveis canônicas) [^30], [^32], obtidas como soluções de um problema de valor próprio generalizado [^29].

Estas técnicas não só otimizam os cálculos, mas também fornecem ferramentas valiosas para visualização e compreensão da estrutura de separabilidade de classes nos dados.

### Referências

[^1]: Page 113, Section 4.3.2: "Their computations are simplified by diagonalizing Σ or Σk."\n[^2]: Page 113, Section 4.3.2: "For the latter, suppose we compute the eigen-decomposition for each Σk = UkDkUTk, where Uk is p × p orthonormal, and Dk a diagonal matrix of positive eigenvalues dkl."\n[^3]: Page 113, Section 4.3.2: "(x − μk)T Σ−1k (x − μk) = [UTk (x − μk)]TD−1k [UTk (x − μk)];"\n[^4]: Page 113, Section 4.3.2: "log|Σk| = ∑l log dkl."\n[^5]: Page 113, Section 4.3.2: "Sphere the data with respect to the common covariance estimate Σ: X∗ ← D−1/2UT X, where Σ = UDUT."\n[^6]: Page 113, Section 4.3.2: "The common covariance estimate of X∗ will now be the identity."\n[^7]: Page 113, Section 4.3.2: "Classify to the closest class centroid in the transformed space, modulo the effect of the class prior probabilities πk."\n[^8]: Page 113, Section 4.3.3: "So far we have discussed LDA as a restricted Gaussian classifier. Part of its popularity is due to an additional restriction that allows us to view informative low-dimensional projections of the data."\n[^9]: Page 113, Section 4.3.3: "The K centroids in p-dimensional input space lie in an affine subspace of dimension ≤ K − 1..."\n[^10]: Page 113, Section 4.3.3: "...and if p is much larger than K, this will be a considerable drop in dimension."\n[^11]: Page 113, Section 4.3.3: "Moreover, in locating the closest centroid, we can ignore distances orthogonal to this subspace, since they will contribute equally to each class."\n[^12]: Page 113, Section 4.3.3: "Thus there is a fundamental dimension reduction in LDA, namely, that we need only consider the data in a subspace of dimension at most K − 1."\n[^13]: Page 114: "If K = 3, for instance, this could allow us to view the data in a two-dimensional plot..."\n[^14]: Page 114: "What if K > 3? We might then ask for a L < K − 1 dimensional subspace HL ⊆ HK−1 optimal for LDA in some sense."\n[^15]: Page 114: "Fisher defined optimal to mean that the projected centroids were spread out as much as possible in terms of variance."\n[^16]: Page 114: "This amounts to finding principal component subspaces of the centroids themselves..."\n[^17]: Page 114: "Figure 4.4 shows such an optimal two-dimensional subspace for the vowel data."\n[^18]: Page 114: "In summary then, finding the sequences of optimal subspaces for LDA involves the following steps:"\n[^19]: Page 114: "compute the K × p matrix of class centroids M and the common covariance matrix W (for within-class covariance); compute M∗ = MW−1/2 using the eigen-decomposition of W; compute B∗, the covariance matrix of M∗ (B for between-class covariance), and its eigen-decomposition B∗ = V∗DBV∗T. The columns v∗l of V∗ in sequence from first to last define the coordinates of the optimal subspaces."\n[^20]: Page 114: "Combining all these operations the lth discriminant variable is given by Zl = vTl X with vl = W−1/2v∗l."\n[^21]: Page 114: "Fisher arrived at this decomposition via a different route, without referring to Gaussian distributions at all."\n[^22]: Page 114: "He posed the problem: Find the linear combination Z = aT X such that the between-class variance is maximized relative to the within-class variance."\n[^23]: Page 114: "Again, the between class variance is the variance of the class means of Z, and the within class variance is the pooled variance about the means."\n[^24]: Page 114: "Figure 4.9 shows why this criterion makes sense... By taking the covariance into account as well, a direction with minimum overlap can be found."\n[^25]: Page 114: "The between-class variance of Z is aT Ba and the within-class variance aT Wa, where W is defined earlier, and B is the covariance matrix of the class centroid matrix M."\n[^26]: Page 114: "Note that B + W = T, where T is the total covariance matrix of X, ignoring class information."\n[^27]: Page 116: "Fisher\'s problem therefore amounts to maximizing the Rayleigh quotient, max_a (aT B a) / (aT W a)"\n[^28]: Page 116: "or equivalently max_a aT Ba subject to aT Wa = 1."\n[^29]: Page 116: "This is a generalized eigenvalue problem, with a given by the largest eigenvalue of W−1B."\n[^30]: Page 116: "It is not hard to show (Exercise 4.1) that the optimal a1 is identical to v1 defined above."\n[^31]: Page 116: "Similarly one can find the next direction a2, orthogonal in W to a1, such that aT2 Ba2/aT2 Wa2 is maximized; the solution is a2 = v2, and so on."\n[^32]: Page 116: "The al are referred to as discriminant coordinates... They are also referred to as canonical variates..."\n[^33]: Page 116: "...since an alternative derivation of these results is through a canonical correlation analysis of the indicator response matrix Y on the predictor matrix X."\n[^34]: Page 116: "To summarize the developments so far: Gaussian classification with common covariances leads to linear decision boundaries. Classification can be achieved by sphering the data with respect to W, and classifying to the closest centroid (modulo log πk) in the sphered space. Since only the relative distances to the centroids count, one can confine the data to the subspace spanned by the centroids in the sphered space. This subspace can be further decomposed into successively optimal subspaces in term of centroid separation. This decomposition is identical to the decomposition due to Fisher."

<!-- END -->