## Estimação de Parâmetros na Análise Discriminante Linear Gaussiana

### Introdução

No contexto da Análise Discriminante Linear (LDA), como discutido na Seção 4.3, o objetivo é classificar observações em $K$ classes distintas baseando-se em um vetor de preditores $X$. A teoria de decisão para classificação nos informa que, para uma classificação ótima, necessitamos conhecer as probabilidades a posteriori das classes, $Pr(G=k|X=x)$ [^1]. A LDA aborda este problema modelando a densidade condicional de $X$ para cada classe $G=k$, $f_k(x)$, assumindo que estas seguem uma distribuição Gaussiana multivariada [^4]. Especificamente, LDA impõe a restrição adicional de que todas as classes partilham uma matriz de covariância comum, $\Sigma_k = \Sigma$ para todo $k$ [^5]. Juntamente com as probabilidades a priori $\pi_k$ (onde $\sum_{k=1}^K \pi_k = 1$) [^2], estes parâmetros $(\pi_k, \mu_k, \Sigma)$ definem completamente o modelo. A regra de classificação de Bayes pode então ser expressa através das densidades $f_k(x)$ e das priors $\pi_k$ [^3], resultando em fronteiras de decisão lineares [^7]. As funções discriminantes lineares, $\delta_k(x)$, são derivadas diretamente destes parâmetros [^8]:

$$\
\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log \pi_k
$$\

A classificação de uma nova observação $x$ é então feita atribuindo-a à classe $k$ que maximiza $\delta_k(x)$. Contudo, na vasta maioria das aplicações práticas, os verdadeiros parâmetros das distribuições Gaussianas subjacentes são desconhecidos. Este capítulo foca precisamente na estimação destes parâmetros a partir dos dados de treino disponíveis [^9].

### Estimação dos Parâmetros do Modelo LDA

A derivação teórica da LDA assume o conhecimento das probabilidades a priori $\pi_k$, dos vetores de médias $\mu_k$ para cada classe, e da matriz de covariância comum $\Sigma$. Em cenários práticos, estes devem ser inferidos a partir de um conjunto de dados de treino $\\{(x_i, g_i)\\}_{i=1}^N$, onde $x_i$ é o vetor de preditores para a $i$-ésima observação e $g_i \in \\{1, ..., K\\}$ é a sua classe correspondente. A abordagem padrão consiste em utilizar os estimadores de máxima verossimilhança (MLE) para os parâmetros, derivados sob as suposições Gaussianas do modelo LDA [^20, ^21].

> **Estimadores de Máxima Verossimilhança para LDA:**
> Dado um conjunto de treino com $N$ observações, onde $N_k$ é o número de observações pertencentes à classe $k$ (tal que $\sum_{k=1}^K N_k = N$), os estimadores para os parâmetros da LDA são:
>
> *   **Probabilidade a Priori ($\hat{\pi}_k$):** A proporção amostral de cada classe.
>     $$ \hat{\pi}_k = \frac{N_k}{N} $$
>     [^10]
> *   **Média da Classe ($\hat{\mu}_k$):** O vetor médio amostral para cada classe.
>     $$ \hat{\mu}_k = \frac{1}{N_k} \sum_{i: g_i=k} x_i $$
>     [^11]
> *   **Matriz de Covariância Comum ($\hat{\Sigma}$):** A matriz de covariância *pooled* (agrupada), calculada como uma média ponderada das matrizes de covariância amostrais de cada classe, utilizando um denominador $N-K$ para obter um estimador não enviesado.
>     $$ \hat{\Sigma} = \frac{1}{N-K} \sum_{k=1}^K \sum_{i: g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T $$
>     [^12]

A estimação da matriz de covariância comum $\hat{\Sigma}$ é um passo distintivo da LDA, refletindo a sua suposição fundamental [^5]. Esta matriz *pooled* combina a informação de variabilidade de todas as classes para obter um único estimador de $\Sigma$. Uma vez que estes parâmetros são estimados ($\hat{\pi}_k, \hat{\mu}_k, \hat{\Sigma}$), eles são substituídos nas expressões teóricas das funções discriminantes [^8] ou nos log-odds [^6, ^18] para obter as funções discriminantes *estimadas*:

$$\
\hat{\delta}_k(x) = x^T \hat{\Sigma}^{-1} \hat{\mu}_k - \frac{1}{2} \hat{\mu}_k^T \hat{\Sigma}^{-1} \hat{\mu}_k + \log \hat{\pi}_k
$$\

A regra de classificação prática torna-se então $G(x) = \operatorname{argmax}_k \hat{\delta}_k(x)$. As fronteiras de decisão resultantes são lineares no espaço de $x$, mas são agora baseadas nos parâmetros estimados a partir dos dados, como ilustrado no painel direito da Figura 4.5 [^13].

É importante contrastar esta abordagem de estimação com a da Análise Discriminante Quadrática (QDA). Na QDA, a suposição de uma matriz de covariância comum é relaxada, permitindo que cada classe $k$ tenha a sua própria matriz $\Sigma_k$ [^14]. Consequentemente, na QDA, estimam-se matrizes de covariância separadas para cada classe, $\hat{\Sigma}_k = \frac{1}{N_k-1} \sum_{i: g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T$, em vez de uma única matriz pooled [^15].

A estimação em LDA também difere fundamentalmente daquela usada na regressão logística. Enquanto a LDA estima os parâmetros $(\pi_k, \mu_k, \Sigma)$ maximizando a log-verossimilhança *conjunta* $Pr(X, G=k) = \phi(X; \mu_k, \Sigma)\pi_k$ [^20], a regressão logística estima os parâmetros dos logits lineares $\beta_{k0}, \beta_k$ maximizando a log-verossimilhança *condicional* $Pr(G=k|X=x)$ [^27], sem fazer suposições sobre a distribuição marginal $Pr(X)$. A LDA, ao fazer suposições mais fortes (Gaussianidade, covariância comum), utiliza a informação da distribuição marginal $Pr(X) = \sum_k \pi_k \phi(X; \mu_k, \Sigma)$ [^23] e pode, teoricamente, obter estimadores mais eficientes (menor variância) se as suposições do modelo forem válidas [^24]. No entanto, esta dependência das suposições, particularmente na estimação de $\hat{\Sigma}$, torna a LDA sensível a outliers, pois observações distantes das fronteiras de decisão ainda influenciam a estimação da covariância comum [^25]. A regressão logística, por outro lado, tende a ser mais robusta a desvios das suposições distribucionais.

Finalmente, vale mencionar que existem abordagens de regularização, como a Análise Discriminante Regularizada (RDA), que criam um compromisso entre LDA e QDA, permitindo encolher (*shrink*) as estimativas das covariâncias separadas da QDA em direção à estimativa da covariância comum da LDA [^16, ^17], oferecendo um espectro de modelos intermediários.

### Conclusão

A implementação prática da Análise Discriminante Linear requer a estimação dos parâmetros do modelo — probabilidades a priori $\pi_k$, médias das classes $\mu_k$, e a matriz de covariância comum $\Sigma$ — a partir dos dados de treino, uma vez que os valores populacionais verdadeiros são desconhecidos [^9]. Os estimadores de máxima verossimilhança padrão para estes parâmetros, baseados na suposição de densidades Gaussianas com covariância comum [^4, ^5], são as proporções amostrais para $\hat{\pi}_k$ [^10], as médias amostrais de cada classe para $\hat{\mu}_k$ [^11], e a matriz de covariância *pooled* para $\hat{\Sigma}$ [^12]. A substituição destes estimadores nas funções discriminantes teóricas [^8] permite a construção das fronteiras de decisão lineares estimadas e a classificação de novas observações. Esta abordagem de estimação, baseada na verossimilhança conjunta [^20], distingue a LDA da QDA (que estima covariâncias separadas [^15]) e da regressão logística (que otimiza a verossimilhança condicional [^27]), resultando em diferentes compromissos entre eficiência [^24] e robustez [^25].

### Referências

[^1]: [p. 106] Decision theory for classification (Section 2.4) tells us that we need to know the class posteriors Pr(G|X) for optimal classification.
[^2]: [p. 106] Suppose fk(x) is the class-conditional density of X in class G = k, and let πk be the prior probability of class k, with Σk=1K πk = 1.
[^3]: [p. 108] A simple application of Bayes theorem gives us Pr(G = k|X = x) = fk(x)πk / Σl=1K fl(x)πl (Eq 4.7).
[^4]: [p. 108] Suppose that we model each class density as multivariate Gaussian fk(x) = (1 / ((2π)^(p/2) |Σk|^(1/2))) * exp(-1/2 (x-μk)T Σk⁻¹ (x-μk)) (Eq 4.8).
[^5]: [p. 108] Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Σk = Σ ∀k.
[^6]: [p. 108] In comparing two classes k and l, it is sufficient to look at the log-ratio... log Pr(G=k|X=x)/Pr(G=l|X=x) = log(fk(x)/fl(x)) + log(πk/πl) = ... = log(πk/πl) - 1/2 (μk+μl)T Σ⁻¹ (μk-μl) + xT Σ⁻¹ (μk-μl) (Eq 4.9).
[^7]: [p. 108] This linear log-odds function implies that the decision boundary between classes k and l... is linear in x.
[^8]: [p. 109] From (4.9) we see that the linear discriminant functions δk(x) = xT Σ⁻¹ μk - 1/2 μkT Σ⁻¹ μk + log πk are an equivalent description of the decision rule, with G(x) = argmaxk δk(x) (Eq 4.10).
[^9]: [p. 109] In practice we do not know the parameters of the Gaussian distributions, and will need to estimate them using our training data:
[^10]: [p. 109] • ^πk = Nk/N, where Nk is the number of class-k observations;
[^11]: [p. 109] • ^μk = Σ(gi=k) xi / Nk;
[^12]: [p. 109] • ^Σ = Σk=1^K Σ(gi=k) (xi - ^μk)(xi - ^μk)T / (N - K).
[^13]: [p. 109] Figure 4.5 (right panel) shows the estimated decision boundaries based on a sample of size 30 each from three Gaussian distributions.
[^14]: [p. 110] Getting back to the general discriminant problem (4.8), if the Σk are not assumed to be equal... we then get quadratic discriminant functions (QDA), δk(x) = -1/2 log|Σk| - 1/2 (x-μk)T Σk⁻¹ (x-μk) + log πk (Eq 4.12).
[^15]: [p. 110] The estimates for QDA are similar to those for LDA, except that separate covariance matrices must be estimated for each class.
[^16]: [p. 112] Friedman (1989) proposed a compromise between LDA and QDA, which allows one to shrink the separate covariances of QDA toward a common covariance as in LDA... The regularized covariance matrices have the form Σk(α) = αΣk + (1-α)Σ (Eq 4.13).
[^17]: [p. 112] where Σ is the pooled covariance matrix as used in LDA.
[^18]: [p. 127] In Section 4.3 we find that the log-posterior odds between class k and K are linear functions of x (4.9): log[Pr(G=k|X=x)/Pr(G=K|X=x)] = αk0 + αkT x (Eq 4.33).
[^19]: [p. 127] This linearity is a consequence of the Gaussian assumption for the class densities, as well as the assumption of a common covariance matrix.
[^20]: [p. 127] With LDA we fit the parameters by maximizing the full log-likelihood, based on the joint density Pr(X, G = k) = φ(Χ; μκ, Σ)πκ (Eq 4.37).
[^21]: [p. 128] Standard normal theory leads easily to the estimates ^μk, ^Σ, and ^πk given in Section 4.3.
[^22]: [p. 128] Since the linear parameters of the logistic form (4.33) are functions of the Gaussian parameters, we get their maximum-likelihood estimates by plugging in the corresponding estimates (^μk, ^Σ, ^πk).
[^23]: [p. 128] However, unlike in the conditional case [logistic regression], the marginal density Pr(X) does play a role here. It is a mixture density Pr(X) = Σk=1K πk φ(X; μk, Σ) (Eq 4.38).
[^24]: [p. 128] By relying on the additional model assumptions [Gaussianity, common Σ], we have more information about the parameters, and hence can estimate them more efficiently (lower variance).
[^25]: [p. 128] This is not all good news, because it also means that LDA is not robust to gross outliers.
[^27]: [p. 127] The logistic regression model... fits the parameters of Pr(G|X) by maximizing the conditional likelihood... Although Pr(X) is totally ignored...

<!-- END -->