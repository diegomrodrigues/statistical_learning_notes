## Análise Discriminante Quadrática e Regularizada

### Introdução

Como vimos na Seção 4.3, a **Análise Discriminante Linear (LDA)** baseia-se na suposição de que todas as classes compartilham uma matriz de covariância comum, $\Sigma_k = \Sigma$ para todo $k$ [^26]. Esta suposição crucial leva a funções discriminantes, como a $\delta_k(x)$ definida na equação (4.10) [^27], que são lineares em $x$, resultando em fronteiras de decisão lineares (hiperplanos) entre as classes [^5]. No entanto, a suposição de covariâncias iguais pode ser restritiva em muitas aplicações práticas. Este capítulo explora duas extensões importantes da LDA que relaxam essa suposição: a **Análise Discriminante Quadrática (QDA)**, que permite que cada classe tenha sua própria matriz de covariância, e a **Análise Discriminante Regularizada (RDA)**, que oferece um compromisso entre a LDA e a QDA.

### Análise Discriminante Quadrática (QDA)

A QDA surge quando abandonamos a restrição de que as matrizes de covariância $\Sigma_k$ sejam idênticas entre as classes no modelo geral de densidade Gaussiana multivariada para cada classe $k$ [^1]:

$$nf_k(x) = \frac{1}{(2\pi)^{p/2} |\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k)\right)$$

Ao contrário da derivação da LDA, onde a suposição de $\Sigma_k = \Sigma$ leva ao cancelamento dos termos quadráticos em $x$ ao calcular a log-razão das probabilidades posteriores (como visto na derivação de (4.9)) [^2, ^8], permitir que cada $\Sigma_k$ seja diferente impede esse cancelamento [^3]. Consequentemente, os termos quadráticos em $x$ permanecem na função discriminante.

A função discriminante para a QDA, denotada por $\delta_k(x)$, é obtida diretamente da log-probabilidade posterior (ignorando termos constantes que não dependem de $k$), resultando em [^4]:

> $$n> \delta_k(x) = -\frac{1}{2} \log |\Sigma_k| - \frac{1}{2} (x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k) + \log \pi_k \quad (4.12)\n> $$n

Observa-se claramente a presença do termo quadrático $(x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k)$. A fronteira de decisão entre duas classes quaisquer, $k$ e $l$, é o conjunto de pontos $x$ para os quais $\delta_k(x) = \delta_l(x)$ [^5]. Devido à presença dos termos quadráticos que não se cancelam (pois $\Sigma_k \neq \Sigma_l$), essa fronteira é descrita por uma equação quadrática em $x$ [^5]. Isso implica que as regiões de classificação no espaço de entrada $\mathbb{R}^p$ são separadas por hiperfícies quadráticas (cônicas, elipsoides, paraboloides, hiperboloides). A Figura 4.6 (painel direito) ilustra um exemplo de fronteiras de decisão quadráticas obtidas via QDA [^6, ^13].

**Estimação de Parâmetros:** Na prática, os parâmetros das distribuições Gaussianas são desconhecidos e precisam ser estimados a partir dos dados de treinamento. As estimativas para as médias $\mu_k$ e as probabilidades a priori $\pi_k$ são as mesmas que na LDA [^28]:
*   $\hat{\pi}_k = N_k/N$, onde $N_k$ é o número de observações na classe $k$ e $N$ é o número total de observações.
*   $\hat{\mu}_k = \sum_{g_i=k} x_i / N_k$.

A diferença crucial reside na estimação da covariância: para a QDA, estimamos uma **matriz de covariância separada** para cada classe $k$ [^7]:
*   $\hat{\Sigma}_k = \sum_{g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T / (N_k - 1)$. (Nota: O texto [^9] usa $N-K$ no denominador para a matriz *pooled* $\hat{\Sigma}$ da LDA, mas a prática usual para $\hat{\Sigma}_k$ separadas usa $N_k-1$).

**Discussão:** A flexibilidade adicional da QDA em modelar diferentes estruturas de covariância para cada classe permite capturar fronteiras de decisão mais complexas do que a LDA. No entanto, essa flexibilidade tem um custo. A QDA requer a estimação de $K$ matrizes de covariância distintas. Quando a dimensão $p$ do espaço de entrada é grande, o número de parâmetros a serem estimados para cada $\hat{\Sigma}_k$ ($p(p+1)/2$ parâmetros) pode se tornar muito grande [^8]. O número total de parâmetros a serem estimados para as funções de decisão da QDA (considerando as diferenças $\delta_k(x) - \delta_K(x)$ para uma classe de referência $K$) é da ordem de $(K-1) \times \{p(p+3)/2 + 1\}$ [^9]. Este aumento dramático no número de parâmetros em comparação com a LDA [(K-1) x (p+1) parâmetros, conforme [^9]] significa que a QDA tem uma variância potencialmente muito maior, especialmente com tamanhos de amostra limitados. Isso pode tornar o argumento do trade-off **bias-variance** menos favorável para a QDA em comparação com a LDA, apesar de seu menor bias ao modelar dados não Gaussianos ou com covariâncias desiguais [^11, ^12]. Apesar disso, a QDA demonstrou bom desempenho em várias tarefas de classificação, como no projeto STATLOG, onde esteve entre os três melhores classificadores para 4 dos 22 conjuntos de dados [^10]. A escolha entre LDA e QDA frequentemente envolve avaliar se os dados disponíveis são suficientes para estimar confiavelmente as matrizes de covariância separadas ou se a estrutura mais simples (e estável) da LDA é preferível.

**Computação:** Os cálculos para QDA podem ser simplificados através da diagonalização (eigen-decomposition) de cada matriz de covariância $\hat{\Sigma}_k = U_k D_k U_k^T$ [^24]. Usando essa decomposição, os componentes chave da função discriminante $\delta_k(x)$ podem ser calculados como:
*   $(x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) = [U_k^T(x-\mu_k)]^T D_k^{-1} [U_k^T(x-\mu_k)]$
*   $\log |\Sigma_k| = \sum_{l=1}^p \log d_{kl}$, onde $d_{kl}$ são os eigenvalues na diagonal de $D_k$ [^25].

### Análise Discriminante Regularizada (RDA)

Reconhecendo que a LDA pode ser excessivamente rígida e a QDA excessivamente flexível (e com alta variância), Friedman (1989) propôs a **Análise Discriminante Regularizada (RDA)** como um compromisso entre as duas [^14, ^15]. A ideia central da RDA é "encolher" (*shrink*) as estimativas das matrizes de covariância separadas $\hat{\Sigma}_k$ da QDA em direção à estimativa da matriz de covariância comum $\hat{\Sigma}$ usada na LDA [^16]. Este processo é análogo em espírito à **ridge regression**.

As matrizes de covariância regularizadas na RDA têm a forma [^17]:

> $$n> \hat{\Sigma}_k(\alpha) = \alpha \hat{\Sigma}_k + (1-\alpha) \hat{\Sigma} \quad (4.13)\n> $$n

onde $\hat{\Sigma}_k$ é a matriz de covariância estimada para a classe $k$, $\hat{\Sigma}$ é a matriz de covariância *pooled* estimada como na LDA [^9], e $\alpha$ é um parâmetro de regularização tal que $\alpha \in [0, 1]$ [^18].

Este parâmetro $\alpha$ controla o grau de *shrinkage*:
*   Se $\alpha = 0$, então $\hat{\Sigma}_k(0) = \hat{\Sigma}$, e a RDA se reduz à LDA.
*   Se $\alpha = 1$, então $\hat{\Sigma}_k(1) = \hat{\Sigma}_k$, e a RDA se torna a QDA.
*   Valores de $\alpha$ entre 0 e 1 produzem um **continuum de modelos** que interpolam entre LDA e QDA [^18].

Na prática, o valor ótimo de $\alpha$ não é conhecido a priori e geralmente é escolhido para otimizar o desempenho de classificação em um conjunto de dados de validação ou por meio de **cross-validation** [^19]. A Figura 4.7 demonstra a aplicação da RDA aos dados *vowel*, mostrando como o erro de teste varia com $\alpha$ [^20].

**Extensões:** Friedman (1989) também sugeriu modificações adicionais que permitem que a própria matriz de covariância *pooled* $\hat{\Sigma}$ seja encolhida em direção a uma matriz de covariância escalar, $\hat{\sigma}^2 I$ [^21]:

$$n\hat{\Sigma}(\gamma) = \gamma \hat{\Sigma} + (1-\gamma)\hat{\sigma}^2 I \quad (4.14)$$

onde $\gamma \in [0, 1]$ é outro parâmetro de regularização e $\hat{\sigma}^2 = \text{tr}(\hat{\Sigma})/p$ é a variância média. Substituir $\hat{\Sigma}$ na equação (4.13) por $\hat{\Sigma}(\gamma)$ leva a uma família mais geral de matrizes de covariância regularizadas, $\hat{\Sigma}_k(\alpha, \gamma)$, indexada por dois parâmetros, $\alpha$ e $\gamma$ [^22]. Isso permite um controle ainda mais fino sobre o trade-off bias-variance, encolhendo não apenas em direção a uma estrutura comum, mas também em direção a uma estrutura diagonal (independência condicionada à classe) ou mesmo esférica. Outras versões regularizadas da LDA, particularmente adequadas para dados de alta dimensão como sinais e imagens, são discutidas no Capítulo 12 [^23].

### Conclusão

A Análise Discriminante Quadrática (QDA) e a Análise Discriminante Regularizada (RDA) representam extensões valiosas da LDA. A QDA relaxa a suposição de covariâncias iguais, permitindo fronteiras de decisão quadráticas mais flexíveis, ao custo de um aumento significativo no número de parâmetros e potencial instabilidade. A RDA oferece um meio-termo adaptativo, utilizando a regularização para interpolar entre a simplicidade da LDA e a flexibilidade da QDA. A escolha entre LDA, QDA e RDA depende das características específicas do problema, incluindo a dimensionalidade dos dados, o tamanho da amostra e a evidência de que as estruturas de covariância das classes são de fato diferentes. Essas técnicas, juntamente com a LDA, formam um conjunto fundamental de ferramentas para classificação baseadas em modelos Gaussianos.

### Referências

[^1]: Página 108, Equação (4.8)
[^2]: Página 110, Primeira frase do parágrafo após Eq. (4.11)
[^3]: Página 110, "...in particular the pieces quadratic in x remain."
[^4]: Página 110, Equação (4.12)
[^5]: Página 110, Frase após Eq. (4.12)
[^6]: Página 110, Referência a Figure 4.6
[^7]: Página 110, "...separate covariance matrices must be estimated for each class."
[^8]: Página 110, "When p is large this can mean a dramatic increase in parameters."
[^9]: Página 111, Contagem de parâmetros para QDA
[^10]: Página 111, Menção ao STATLOG project
[^11]: Página 111, Discussão sobre a razão do bom desempenho de LDA/QDA
[^12]: Página 111, "...argument is less believable for QDA, since it can have many parameters itself..."
[^13]: Página 111, Referência a Figure 4.6
[^14]: Página 112, Seção 4.3.1, Friedman (1989)
[^15]: Página 112, "...compromise between LDA and QDA..."
[^16]: Página 112, "...shrink the separate covariances of QDA toward a common covariance as in LDA."
[^17]: Página 112, Equação (4.13)
[^18]: Página 112, "...allows a continuum of models between LDA and QDA..."
[^19]: Página 112, "...chosen based on the performance of the model on validation data, or by cross-validation."
[^20]: Página 112, Referência a Figure 4.7
[^21]: Página 112, Equação (4.14)
[^22]: Página 112, "...leads to a more general family of covariances Σ(α, γ) indexed by a pair of parameters."
[^23]: Página 112, Última frase da seção 4.3.1
[^24]: Página 113, Seção 4.3.2, "...diagonalizing Σ or Σk."
[^25]: Página 113, Seção 4.3.2, Bullet points
[^26]: Página 108, Introdução da LDA
[^27]: Página 109, Equação (4.10)
[^28]: Página 109, Estimativas de parâmetros para LDA

<!-- END -->