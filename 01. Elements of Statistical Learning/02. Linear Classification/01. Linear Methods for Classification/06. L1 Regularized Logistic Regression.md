## Capítulo 4.4.4: Regressão Logística Regularizada L1

### Introdução

Como explorado anteriormente na Seção 4.4, a **regressão logística** modela as probabilidades posteriores das $K$ classes através de funções lineares das variáveis preditoras $x$, garantindo que as probabilidades resultantes somem um e permaneçam no intervalo $[0, 1]$ [^24]. Especificamente, para $K$ classes, o modelo é frequentemente parametrizado em termos de $K-1$ transformações logit (log-odds) em relação a uma classe de referência, tipicamente a classe $K$:
$$ \log \frac{\text{Pr}(G=k|X=x)}{\text{Pr}(G=K|X=x)} = \beta_{k0} + \beta_k^T x, \quad k=1, \dots, K-1 $$
[^24]. A estimação dos parâmetros $\beta_{k0}, \beta_k$ é usualmente realizada por **máxima verossimilhança (maximum likelihood)**, maximizando a log-verossimilhança condicional baseada na distribuição multinomial [^28, ^29]. Para o caso de duas classes (onde $y_i \in \{0, 1\}$), a log-verossimilhança assume a forma conveniente $l(\beta) = \sum_{i=1}^N \{ y_i \beta^T x_i - \log(1 + e^{\beta^T x_i}) \}$ [^31], onde $x_i$ inclui o termo de intercepto.

Em cenários com um grande número de preditores, ou quando se deseja um modelo mais parcimonioso, técnicas de regularização são essenciais. Expandindo o conceito de regularização introduzido com o lasso na Seção 3.4.2 [^56], aplicamos a **penalidade L1** à regressão logística. Este capítulo foca na **Regressão Logística Regularizada L1**, um método que visa maximizar uma versão penalizada da log-verossimilhança para realizar simultaneamente **seleção de variáveis (variable selection)** e **encolhimento (shrinkage)** dos coeficientes [^1, ^56].

### A Verossimilhança Penalizada L1

O objetivo na Regressão Logística Regularizada L1 é encontrar os coeficientes $\beta_0$ (intercepto) e $\beta = (\beta_1, \dots, \beta_p)$ (coeficientes dos preditores) que maximizam a log-verossimilhança penalizada. Para o caso de duas classes, a função objetivo é:

> $$ > \max_{\beta_0, \beta} \left\{ \sum_{i=1}^N \left[ y_i (\beta_0 + \beta^T x_i) - \log(1 + e^{\beta_0 + \beta^T x_i}) \right] - \lambda \sum_{j=1}^p |\beta_j| \right\} > $$
> [^57]

Aqui, o primeiro termo corresponde à log-verossimilhança da regressão logística padrão [^31], e o segundo termo é a **penalidade L1**, ponderada pelo parâmetro de regularização $\lambda \ge 0$. É prática comum não penalizar o coeficiente de intercepto $\beta_0$ e padronizar os preditores $x_j$ para que tenham média zero e variância unitária antes da aplicação da penalidade, garantindo que a penalidade seja aplicada de forma equitativa [^58]. O parâmetro $\lambda$ controla a intensidade da regularização: $\lambda=0$ recupera a regressão logística de máxima verossimilhança padrão, enquanto valores maiores de $\lambda$ aumentam o encolhimento e promovem maior esparsidade no vetor de coeficientes $\beta$.

### Propriedades e Interpretação

A principal vantagem da penalidade L1 é sua capacidade de produzir soluções **esparsas (sparse)**, ou seja, muitos coeficientes $\beta_j$ são estimados exatamente como zero [^56]. Isso efetivamente realiza a seleção de variáveis, resultando em modelos mais interpretáveis e potencialmente mais robustos, especialmente em cenários de alta dimensionalidade ($p \gg N$). Além da seleção, os coeficientes não nulos são encolhidos em direção a zero, o que pode reduzir a variância das estimativas [^56]. A função objetivo [^57] é **côncava** em $(\beta_0, \beta)$, o que garante que um máximo global pode ser encontrado [^59].

### Algoritmos de Otimização

A maximização da função objetivo [^57] pode ser abordada por diferentes métodos de otimização.

1.  **Métodos de Programação Não Linear:** Podem ser utilizados algoritmos genéricos de otimização convexa para resolver o problema [^59].
2.  **Aproximações Quadráticas e Lasso Ponderado:** Uma abordagem eficiente, análoga à utilizada no algoritmo de Newton-Raphson (ou IRLS) para a regressão logística padrão (Seção 4.4.1), envolve aproximações quadráticas da log-verossimilhança. Em cada iteração, a log-verossimilhança penalizada é aproximada por um problema de mínimos quadrados ponderados penalizado com L1 (um **weighted lasso**). Especificamente, utilizando as aproximações quadráticas que fundamentam o passo de Newton [^35, ^37], podemos resolver o problema [^57] por aplicação repetida de um algoritmo de lasso ponderado [^1]. Em cada passo, computamos as probabilidades ajustadas $p$ e a matriz de pesos diagonal $W$ [^38] com base nos coeficientes atuais $\beta^{old}$, formamos a resposta ajustada (ou resposta de trabalho) $z = X\beta^{old} + W^{-1}(y-p)$ [^40], e então resolvemos um problema de lasso ponderado da forma:
    $$     \beta^{new} = \arg \min_{\beta} \frac{1}{2} (z - X\beta)^T W (z - X\beta) + \lambda \sum_{j=1}^p |\beta_j|     $$
    Este subproblema de lasso ponderado pode ser resolvido eficientemente.
3.  **Descida de Coordenadas (Coordinate Descent):** Métodos de descida de coordenadas provaram ser particularmente eficientes para problemas de regressão logística regularizada L1, especialmente para conjuntos de dados grandes em $N$ ou $p$ [^64]. Estes métodos otimizam a função objetivo [^57] iterativamente em relação a um coeficiente de cada vez, mantendo os outros fixos. O pacote R `glmnet` [^43, ^64, ^65] implementa esta abordagem de forma muito eficiente, explorando a esparsidade da matriz de preditores $X$ e computando soluções ao longo de um caminho de valores de $\lambda$.

**Algoritmos de Caminho (Path Algorithms):** Embora algoritmos de caminho como o LARS (Least Angle Regression) sejam eficientes para o lasso padrão, sua adaptação para a regressão logística L1 é mais complexa, pois os perfis dos coeficientes $\beta_j(\lambda)$ são suaves por partes (piecewise smooth) em vez de lineares por partes [^62]. No entanto, progressos foram feitos usando aproximações quadráticas ou métodos de predição-correção, como implementado no pacote R `glmpath` [^63]. A Figura 4.13 [^63] ilustra um exemplo de caminho de regularização L1 para os dados de doença cardíaca da África do Sul (Seção 4.4.2).

### Relação com o Lasso e Equações de Score

A Regressão Logística L1 generaliza o método lasso [^56]. As condições de otimalidade (equações de score) para os coeficientes não nulos $\beta_j$ na solução de [^57] têm uma forma característica. Derivando a função objetivo em relação a $\beta_j$ (para $j=1, \dots, p$) e igualando a zero (considerando a subdiferencial para o termo L1), obtemos:
$$ \sum_{i=1}^N x_{ij} (y_i - p(x_i; \beta_0, \beta)) = \lambda \cdot \text{sign}(\beta_j) \quad \text{se } \beta_j \neq 0 $$
[^60], onde $p(x_i; \beta_0, \beta) = 1 / (1 + e^{-(\beta_0 + \beta^T x_i)})$. Além disso, $|\sum_{i=1}^N x_{ij} (y_i - p(x_i; \beta_0, \beta))| \le \lambda$ se $\beta_j = 0$. Esta condição [^60] generaliza a condição de score para o lasso padrão (equação 3.58 mencionada em [^61]), indicando que a correlação generalizada entre o preditor $j$ e o resíduo $(y-p)$ está no limite $\pm \lambda$ para variáveis ativas.

### Conclusão

A Regressão Logística Regularizada L1 estende o modelo de regressão logística incorporando a penalidade L1, o que induz esparsidade e realiza seleção de variáveis e encolhimento dos coeficientes. Este método é particularmente valioso em contextos de alta dimensionalidade ou quando a interpretabilidade do modelo é prioritária. Através de algoritmos eficientes como a descida de coordenadas ou aproximações quadráticas via lasso ponderado, é possível ajustar esses modelos mesmo para problemas de grande escala. Constitui uma ferramenta poderosa dentro do arsenal de métodos lineares para classificação, oferecendo um equilíbrio entre ajuste aos dados e complexidade do modelo.

### Referências

[^1]: The logit transformation, log[p/(1-p)], is a monotone transformation used in logistic regression, where the log-odds of class membership are modeled as a linear function of the input features. L1 Regularized Logistic Regression maximizes a penalized version of the log-likelihood to achieve variable selection and shrinkage, using the L1 penalty, and can be solved by repeated application of a weighted lasso algorithm. (Prompt)
[^2]: For example, if there are two classes, a popular model for the posterior probabilities is Pr(G = 1|X = x) = exp(β₀ + βᵀx) / (1 + exp(β₀ + βᵀx)), Pr(G = 2|X = x) = 1 / (1 + exp(β₀ + βᵀx)). (p102)
[^3]: Here the monotone transformation is the logit transformation: log[p/(1-p)], and in fact we see that log[Pr(G = 1|X = x) / Pr(G = 2|X = x)] = β₀ + βᵀx. (p102)
[^4]: The decision boundary is the set of points for which the log-odds are zero, and this is a hyperplane defined by {x | β₀ + βᵀx = 0}. (p102)
[^5]: We discuss two very popular but different methods that result in linear log-odds or logits: linear discriminant analysis and linear logistic regression. (p102)
[^6]: This approach can be used with any basis transformation h(X) where h : Rᵖ → R<0xE1><0xB5><0xA9> with q > p... (p102)
[^7]: min_B Σ ||yᵢ - [(1, xᵢᵀ)B]ᵀ||². (p104)
[^8]: There is a serious problem with the regression approach when the number of classes K ≥ 3, especially prevalent when K is large. Because of the rigid nature of the regression model, classes can be masked by others. (p105)
[^9]: TABLE 4.1. Training and test error rates using a variety of linear techniques on the vowel data... Logistic regression 0.22 0.51. (p107)
[^10]: Pr(G = k|X = x) = f_k(x)π_k / Σ_{l=1}^K f_l(x)π_l. (p108)
[^11]: Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Σ_k = Σ ∀k. (p108)
[^12]: log [Pr(G = k|X = x) / Pr(G = l|X = x)] = log(f_k(x)/f_l(x)) + log(π_k/π_l) = log(π_k/π_l) - 1/2 (μ_k + μ_l)ᵀΣ⁻¹(μ_k - μ_l) + xᵀΣ⁻¹(μ_k - μ_l). (p108)
[^13]: δ_k(x) = xᵀΣ⁻¹μ_k - 1/2 μ_kᵀΣ⁻¹μ_k + log π_k. (p109)
[^14]: Getting back to the general discriminant problem (4.8), if the Σk are not assumed to be equal... We then get quadratic discriminant functions (QDA). (p110)
[^15]: δ_k(x) = -1/2 log|Σ_k| - 1/2 (x - μ_k)ᵀΣ_k⁻¹(x - μ_k) + log π_k. (p110)
[^16]: For example, in the STATLOG project (Michie et al., 1994) LDA was among the top three classifiers for 7 of the 22 datasets, QDA among the top three for four datasets... (p111)
[^17]: This is a bias variance tradeoff—we can put up with the bias of a linear decision boundary because it can be estimated with much lower variance than more exotic alternatives. (p111)
[^18]: 4.3.1 Regularized Discriminant Analysis. (p112)
[^19]: Friedman (1989) proposed a compromise between LDA and QDA, which allows one to shrink the separate covariances of QDA toward a common covariance as in LDA... Σ_k(α) = αΣ_k + (1 − α)Σ. (p112)
[^20]: Similar modifications allow Σ itself to be shrunk toward the scalar covariance, Σ(γ) = γΣ + (1 − γ)σ̂²I. (p112)
[^21]: Find the linear combination Z = aᵀX such that the between-class variance is maximized relative to the within-class variance. (p114)
[^22]: Fisher's problem therefore amounts to maximizing the Rayleigh quotient, max_a (aᵀBa / aᵀWa). (p116)
[^23]: Gaussian classification dictates the log π_k correction factor in the distance calculation. (p117)
[^24]: The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x... The model has the form log[Pr(G = k|X = x) / Pr(G = K|X = x)] = β_{k0} + β_kᵀx, k = 1,...,K-1. (p119)
[^25]: Pr(G = k|X = x) = exp(β_{k0} + β_kᵀx) / (1 + Σ_{l=1}^{K-1} exp(β_{l0} + β_lᵀx)), k = 1,...,K-1. (p119)
[^26]: Pr(G = K|X = x) = 1 / (1 + Σ_{l=1}^{K-1} exp(β_{l0} + β_lᵀx)). (p119)
[^27]: When K = 2, this model is especially simple, since there is only a single linear function. It is widely used in biostatistical applications where binary responses (two classes) occur quite frequently. (p119)
[^28]: Logistic regression models are usually fit by maximum likelihood, using the conditional likelihood of G given X. (p120)
[^29]: The log-likelihood for N observations is l(θ) = Σ_{i=1}^N log p_{g_i}(x_i; θ). (p120)
[^30]: We discuss in detail the two-class case... code the two-class g_i via a 0/1 response y_i... The log-likelihood can be written l(β) = Σ_{i=1}^N {y_i log p(x_i; β) + (1 - y_i) log(1 - p(x_i; β))}. (p120)
[^31]: l(β) = Σ_{i=1}^N {y_i βᵀx_i - log(1 + e^{βᵀx_i})}. (p120)
[^32]: These score equations are ∂l(β)/∂β = Σ_{i=1}^N x_i (y_i - p(x_i; β)) = 0. (p120)
[^33]: To solve the score equations (4.21), we use the Newton-Raphson algorithm... (p120)
[^34]: ...which requires the second-derivative or Hessian matrix ∂²l(β)/∂β∂βᵀ = - Σ_{i=1}^N x_i x_iᵀ p(x_i; β) (1 - p(x_i; β)). (p120)
[^35]: Starting with β^{old}, a single Newton update is β^{new} = β^{old} - (∂²l(β)/∂β∂βᵀ)⁻¹ ∂l(β)/∂β. (p120)
[^36]: Let y denote the vector of y_i values, X the N × (p + 1) matrix of x_i values... ∂l(β)/∂β = Xᵀ(y - p). (p121)
[^37]: ∂²l(β)/∂β∂βᵀ = -XᵀWX. (p121)
[^38]: ...p the vector of fitted probabilities with ith element p(x_i; β^{old}) and W a N × N diagonal matrix of weights with ith diagonal element p(x_i; β^{old})(1 - p(x_i; β^{old})). (p121)
[^39]: The Newton step is thus β^{new} = β^{old} + (XᵀWX)⁻¹Xᵀ(y - p) = (XᵀWX)⁻¹XᵀW (Xβ^{old} + W⁻¹(y - p)) = (XᵀWX)⁻¹XᵀWz. (p121)
[^40]: In the second and third line we have re-expressed the Newton step as a weighted least squares step, with the response z = Xβ^{old} + W⁻¹(y - p), sometimes known as the adjusted response. (p121)
[^41]: This algorithm is referred to as iteratively reweighted least squares or IRLS, since each iteration solves the weighted least squares problem: β^{new} ← arg min_β (z - Xβ)ᵀW(z - Xβ). (p121)
[^42]: For the multiclass case (K ≥ 3) the Newton algorithm can also be expressed as an iteratively reweighted least squares algorithm, but with a vector of K-1 responses and a nondiagonal weight matrix per observation. (p121)
[^43]: The R package glmnet (Friedman et al., 2010) can fit very large logistic regression problems efficiently... (p121)
[^44]: Logistic regression models are used mostly as a data analysis and inference tool... (p121)
[^45]: 4.4.2 Example: South African Heart Disease. (p122)
[^46]: This summary includes Z scores for each of the coefficients in the model (coefficients divided by their standard errors); ... Each of these correspond formally to a test of the null hypothesis that the coefficient in question is zero, while all the others are not (also known as the Wald test). (p122)
[^47]: This confusion is a result of the correlation between the set of predictors. (p122)
[^48]: At this stage the analyst might do some model selection... One way to proceed by is to drop the least significant coefficient, and refit the model... A better but more time-consuming strategy is to refit each of the models with one variable removed, and then perform an analysis of deviance... (p124)
[^49]: How does one interpret a coefficient of 0.081... for tobacco...? Thus an increase of 1kg in lifetime tobacco usage accounts for an increase in the odds of coronary heart disease of exp(0.081) = 1.084 or 8.4%. (p124)
[^50]: The maximum-likelihood parameter estimates β̂ satisfy a self-consistency relationship: they are the coefficients of a weighted least squares fit, where the responses are z_i = x_iᵀβ̂ + (y_i - p̂_i) / (p̂_i(1 - p̂_i)). (p124)
[^51]: ...and the weights are w_i = p̂_i(1 - p̂_i)... (p125)
[^52]: The weighted residual sum-of-squares is the familiar Pearson chi-square statistic Σ (y_i - p̂_i)² / (p̂_i(1 - p̂_i)). (p125)
[^53]: Asymptotic likelihood theory says that if the model is correct, then β̂ is consistent... A central limit theorem then shows that the distribution of β̂ converges to N(β, (XᵀWX)⁻¹). (p125)
[^54]: Popular shortcuts are the Rao score test which tests for inclusion of a term, and the Wald test which can be used to test for exclusion of a term. (p125)
[^55]: Software implementations can take advantage of these connections. For example, the generalized linear modeling software in R... GLM (generalized linear model) objects can be treated as linear model objects... (p125)
[^56]: 4.4.4 L1 Regularized Logistic Regression. The L1 penalty used in the lasso (Section 3.4.2) can be used for variable selection and shrinkage with any linear regression model. For logistic regression, we would maximize a penalized version of (4.20). (p125)
[^57]: max_{β₀,β} Σ_{i=1}^N [y_i(β₀ + βᵀx_i) - log(1 + e^{β₀+βᵀx_i})] - λ Σ_{j=1}^p |β_j|. (p125)
[^58]: As with the lasso, we typically do not penalize the intercept term, and standardize the predictors for the penalty to be meaningful. (p125)
[^59]: Criterion (4.31) is concave, and a solution can be found using nonlinear programming methods (Koh et al., 2007, for example). (p126)
[^60]: Interestingly, the score equations [see (4.24)] for the variables with non-zero coefficients have the form x_jᵀ(y - p) = λ · sign(β_j). (p126)
[^61]: ...which generalizes (3.58) in Section 3.4.4; the active variables are tied in their generalized correlation with the residuals. (p126)
[^62]: Path algorithms such as LAR for lasso are more difficult, because the coefficient profiles are piecewise smooth rather than linear. (p126)
[^63]: Figure 4.13 shows the L1 regularization path for the South African heart disease data... This was produced using the R package glmpath (Park and Hastie, 2007)... (p126)
[^64]: Coordinate descent methods (Section 3.8.6) are very efficient for computing the coefficient profiles on a grid of values for λ. The R package glmnet... (p126)
[^65]: (Friedman et al., 2010) can fit coefficient paths for very large logistic regression problems efficiently (large in N or p). Their algorithms can exploit sparsity in the predictor matrix X... (p127)
[^66]: In Section 4.3 we find that the log-posterior odds between class k and K are linear functions of x (4.9): log[Pr(G=k|X=x)/Pr(G=K|X=x)] = ... = α_{k0} + α_kᵀx. (p127)
[^67]: The linear logistic model (4.17) by construction has linear logits: log[Pr(G=k|X=x)/Pr(G=K|X=x)] = β_{k0} + β_kᵀx. (p127)
[^68]: It seems that the models are the same. Although they have exactly the same form, the difference lies in the way the linear coefficients are estimated. (p127)
[^69]: The logistic regression model ... fits the parameters of Pr(G|X) by maximizing the conditional likelihood... (p127)
[^70]: We can write the joint density of X and G as Pr(X, G = k) = Pr(X)Pr(G = k|X). (p127)
[^71]: With LDA we fit the parameters by maximizing the full log-likelihood, based on the joint density Pr(X, G = k) = φ(X; μ_k, Σ)π_k. (p127)
[^72]: Pr(X) = Σ_{k=1}^K π_k φ(X; μ_k, Σ). (p128)
[^73]: By relying on the additional model assumptions [of LDA], we have more information about the parameters, and hence can estimate them more efficiently (lower variance). (p128)
[^74]: ...it also means that LDA is not robust to gross outliers. (p128)
[^75]: The marginal likelihood can be thought of as a regularizer... if the data in a two-class logistic regression model can be perfectly separated by a hyperplane, the maximum likelihood estimates of the parameters are undefined... The LDA coefficients for the same data will be well defined, since the marginal likelihood will not permit these degeneracies. (p128)
[^76]: In practice these assumptions are never correct... It is generally felt that logistic regression is a safer, more robust bet than the LDA model, relying on fewer assumptions. (p128)
[^77]: It is our experience that the models give very similar results, even when LDA is used inappropriately... (p128)

<!-- END -->