## Capítulo 4: Métodos Lineares para Classificação

### 4.3 Linear Discriminant Analysis (LDA)

#### Introdução

Como introduzido anteriormente [1, 3], **Linear Discriminant Analysis (LDA)** é um dos métodos populares que resultam em fronteiras de decisão lineares, ou mais especificamente, em **log-odds** (ou **logits**) lineares. Embora compartilhe essa característica com a regressão logística linear, a LDA difere fundamentalmente em sua derivação e na maneira como a função linear é ajustada aos dados de treinamento [3]. Este capítulo aprofunda a LDA, detalhando sua fundamentação teórica baseada em modelos de densidade de classe. Especificamente, a LDA surge da modelagem da densidade de cada classe, $f_k(x)$, como uma distribuição Gaussiana multivariada, com a suposição crucial de uma matriz de covariância **comum** $\Sigma$ para todas as classes [4, 11]. Esta suposição é a chave para a obtenção de funções discriminantes lineares [4].

#### Derivação da LDA a partir de Densidades Gaussianas

A teoria de decisão para classificação, conforme abordado na Seção 2.4 [8], estabelece que o conhecimento das probabilidades posteriores de classe, $Pr(G=k|X=x)$, é necessário para uma classificação ótima. O teorema de Bayes fornece a ligação entre estas posteriores, as densidades condicionais de classe $f_k(x) = P(X=x|G=k)$ e as probabilidades a priori das classes $\pi_k = P(G=k)$, onde $\sum_{k=1}^{K} \pi_k = 1$ [8, 10]:

$$ Pr(G=k|X=x) = \frac{f_k(x)\pi_k}{\sum_{l=1}^{K} f_l(x)\pi_l} \quad (4.7) [10] $$

A LDA baseia-se na suposição de que cada densidade condicional de classe $f_k(x)$ segue uma distribuição Gaussiana multivariada [11]:

$$ f_k(x) = \frac{1}{(2\pi)^{p/2} |\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k)\right) \quad (4.8) [11] $$

onde $\mu_k$ é o vetor de médias da classe $k$ e $\Sigma_k$ é a matriz de covariância da classe $k$. A característica distintiva da LDA é a suposição adicional de que todas as classes compartilham a mesma matriz de covariância, ou seja, $\Sigma_k = \Sigma$ para todo $k=1, \dots, K$ [11].

Para comparar duas classes, $k$ e $l$, é suficiente analisar a razão logarítmica das suas probabilidades posteriores (ou, equivalentemente, o logaritmo da razão de $f_k(x)\pi_k$ e $f_l(x)\pi_l$, uma vez que o denominador em (4.7) é comum). Sob a suposição de $\Sigma_k = \Sigma_l = \Sigma$, a derivação simplifica consideravelmente [12]:

$$ \begin{aligned} \log \frac{Pr(G=k|X=x)}{Pr(G=l|X=x)} &= \log \frac{f_k(x)\pi_k}{f_l(x)\pi_l} \\\\\ &= \log \frac{f_k(x)}{f_l(x)} + \log \frac{\pi_k}{\pi_l} \\\\\ &= \log f_k(x) - \log f_l(x) + \log \frac{\pi_k}{\pi_l} \\\\\ &= \left(-\frac{1}{2}\log |(2\pi)\Sigma| - \frac{1}{2}(x-\mu_k)^T \Sigma^{-1} (x-\mu_k)\right) \\\\\ &\quad - \left(-\frac{1}{2}\log |(2\pi)\Sigma| - \frac{1}{2}(x-\mu_l)^T \Sigma^{-1} (x-\mu_l)\right) + \log \frac{\pi_k}{\pi_l} \\\\\ &= -\frac{1}{2}(\mu_k^T \Sigma^{-1} \mu_k - 2 x^T \Sigma^{-1} \mu_k) + \frac{1}{2}(\mu_l^T \Sigma^{-1} \mu_l - 2 x^T \Sigma^{-1} \mu_l) + \log \frac{\pi_k}{\pi_l} \\\\\ &\quad \text{(após cancelar os termos } x^T \Sigma^{-1} x \text{ e os termos constantes de logaritmo)} \\\\\ &= x^T \Sigma^{-1}(\mu_k - \mu_l) - \frac{1}{2}(\mu_k^T \Sigma^{-1} \mu_k - \mu_l^T \Sigma^{-1} \mu_l) + \log \frac{\pi_k}{\pi_l} \end{aligned} $$

Reorganizando a expressão, obtemos a forma apresentada na equação (4.9) do contexto [12]:

$$ \log \frac{Pr(G=k|X=x)}{Pr(G=l|X=x)} = \log \frac{\pi_k}{\pi_l} - \frac{1}{2}(\mu_k + \mu_l)^T \Sigma^{-1} (\mu_k - \mu_l) + x^T \Sigma^{-1}(\mu_k - \mu_l) \quad (4.9) [12] $$

> **Destaque:** A suposição crucial de matriz de covariância comum $\Sigma_k = \Sigma$ leva ao cancelamento dos termos quadráticos em $x$ (ou seja, $x^T \Sigma^{-1} x$) na expansão do logaritmo das densidades Gaussianas [12]. Isso resulta em uma função **log-odds** que é *linear* em $x$.

Esta linearidade implica que a fronteira de decisão entre quaisquer duas classes $k$ e $l$, definida pelo conjunto de pontos onde $Pr(G=k|X=x) = Pr(G=l|X=x)$ (ou seja, onde o log-odds é zero), é um hiperplano em $p$ dimensões [13]. Como isso vale para qualquer par de classes, o espaço de entrada $\mathbb{R}^p$ é dividido em regiões de classificação por fronteiras de decisão lineares por partes [1, 13]. A Figura 4.5 (painel esquerdo) ilustra um exemplo idealizado com três classes Gaussianas com a mesma covariância [13, 14]. Note que as fronteiras de decisão não são necessariamente as bissetrizes perpendiculares dos segmentos de reta que unem os centróides das classes, a menos que $\Sigma$ seja esférica ($\sigma^2 I$) e as probabilidades a priori $\pi_k$ sejam iguais [14].

A regra de classificação pode ser expressa de forma equivalente através das **funções discriminantes lineares** $\delta_k(x)$. Ignorando termos constantes que são comuns a todas as classes (como $-\frac{1}{2} x^T \Sigma^{-1} x$), a função discriminante para a classe $k$ é dada por [15]:

$$ \delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log \pi_k \quad (4.10) [15] $$

A regra de classificação é então atribuir $x$ à classe $k$ para a qual $\delta_k(x)$ é máxima: $G(x) = \operatorname{argmax}_k \delta_k(x)$ [15].

#### Estimação dos Parâmetros

Na prática, os parâmetros das distribuições Gaussianas ($\mu_k$, $\Sigma$) e as probabilidades a priori ($\pi_k$) são desconhecidos e precisam ser estimados a partir dos dados de treinamento $\{(x_i, g_i)\}_{i=1}^N$ [16]. As estimativas de máxima verossimilhança padrão são:

*   **Probabilidades a priori:** $\hat{\pi}_k = N_k / N$, onde $N_k$ é o número de observações na classe $k$ [16].
*   **Médias das classes:** $\hat{\mu}_k = \sum_{g_i=k} x_i / N_k$ [16].
*   **Matriz de covariância comum:** $\hat{\Sigma} = \sum_{k=1}^{K} \sum_{g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T / (N - K)$ [16]. (Note que o denominador $N-K$ corresponde a uma estimativa não enviesada).

Estas estimativas são então substituídas nas funções discriminantes $\delta_k(x)$ (Eq. 4.10) para obter a regra de classificação LDA ajustada. A Figura 4.5 (painel direito) mostra as fronteiras de decisão estimadas para uma amostra de dados [14].

#### LDA na Prática e Casos Especiais

##### O Caso de Duas Classes e a Ligação com Mínimos Quadrados

Para o caso de duas classes ($K=2$), existe uma correspondência simples entre a LDA e a classificação por mínimos quadrados lineares [17]. A regra LDA classifica para a classe 2 se $\delta_2(x) > \delta_1(x)$, o que, usando (4.10) e rearranjando, leva à condição [17]:

$$ x^T \hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1) > \frac{1}{2}(\hat{\mu}_2^T \hat{\Sigma}^{-1} \hat{\mu}_2 - \hat{\mu}_1^T \hat{\Sigma}^{-1} \hat{\mu}_1) - \log(\hat{\pi}_2/\hat{\pi}_1) \quad (\text{derivado de } 4.11) [17] $$

Se codificarmos as respostas das duas classes como, por exemplo, $+1$ e $-1$ (ou qualquer codificação distinta, como $-N/N_1$ e $N/N_2$ [42]), pode-se mostrar que o vetor de coeficientes $\hat{\beta}$ obtido pela regressão linear por mínimos quadrados da variável resposta codificada nos preditores $x$ é proporcional à direção LDA $\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1)$ [18, 42, 43].

> **Importante:** Embora a *direção* do vetor de coeficientes da regressão seja proporcional à direção LDA independentemente da suposição Gaussiana [18], o *intercepto* resultante da regressão (e, portanto, a fronteira de decisão final) só coincide com a fronteira LDA se as classes tiverem tamanhos iguais ($N_1 = N_2$) ou se for usada uma codificação específica [18, 43]. A derivação do intercepto específico da LDA (o lado direito da desigualdade acima, derivado de 4.11) *requer* a suposição Gaussiana [18]. Na prática, pode fazer sentido encontrar a direção via regressão e depois escolher um ponto de corte (intercepto) que minimize empiricamente o erro de treinamento [18].

##### LDA vs. Regressão Linear para K > 2 Classes

Com mais de duas classes ($K>2$), a LDA não é equivalente à regressão linear da matriz indicadora de classe [19]. Como ilustrado na Figura 4.2 [6], a abordagem de regressão linear pode sofrer de problemas de *masking*, onde certas classes podem ser completamente dominadas por outras e nunca serem preditas, mesmo que sejam linearmente separáveis [6, 7]. A LDA, por outro lado, evita esses problemas de mascaramento [7, 19], como visto no desempenho superior na Tabela 4.1 para os dados de vogais [9]. A conexão entre LDA e regressão para $K>2$ pode ser estabelecida através do conceito de *optimal scoring* [19].

##### Breve Menção a QDA

Se a suposição de matriz de covariância comum $\Sigma_k = \Sigma$ for relaxada, e permitirmos que cada classe tenha sua própria matriz $\Sigma_k$, obtemos a **Quadratic Discriminant Analysis (QDA)** [20]. Neste caso, os termos quadráticos em $x$ não se cancelam na derivação do log-odds, resultando em funções discriminantes quadráticas $\delta_k(x)$ e fronteiras de decisão quadráticas entre as classes [20]:

$$ \delta_k(x) = -\frac{1}{2} \log |\Sigma_k| - \frac{1}{2} (x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k) + \log \pi_k \quad (4.12) [20] $$

#### Propriedades e Discussão

A LDA tem demonstrado bom desempenho em uma ampla variedade de tarefas de classificação [21]. Por exemplo, no projeto STATLOG, a LDA esteve entre os três melhores classificadores para 7 dos 22 conjuntos de dados [21]. A razão para este sucesso provavelmente não reside no fato de os dados serem aproximadamente Gaussianos com covariâncias iguais, mas sim na capacidade dos dados de suportar apenas fronteiras de decisão simples (lineares ou quadráticas) e na estabilidade das estimativas fornecidas pelos modelos Gaussianos [21]. Este é um exemplo do *tradeoff* viés-variância: aceita-se o viés de uma fronteira de decisão linear (uma suposição forte) porque ela pode ser estimada com variância muito menor do que alternativas mais flexíveis [21].

##### Comparação da Estimação: LDA vs. Regressão Logística

Como mencionado, tanto a LDA (com $\Sigma_k = \Sigma$) quanto a regressão logística linear resultam em log-odds (ou logits) que são funções lineares de $x$ [3, 36]:

*   **LDA (Eq. 4.9/4.33):** $\log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \alpha_{k0} + \alpha_k^T x$ [36]
*   **Regressão Logística (Eq. 4.17/4.34):** $\log \frac{Pr(G=k|X=x)}{Pr(G=K|X=x)} = \beta_{k0} + \beta_k^T x$ [36]

Apesar da forma idêntica, a diferença fundamental reside na forma como os coeficientes lineares ($\alpha_k$ e $\beta_k$) são estimados [3, 37]:

1.  **Regressão Logística:** Ajusta os parâmetros $\beta_k$ maximizando a **verossimilhança condicional** de $G$ dado $X$, $P(G|X)$ [37]. Utiliza a distribuição multinomial apropriada para $G$ condicional a $X$. A distribuição marginal dos preditores $Pr(X)$ é completamente ignorada [37]. Podemos pensar nisso como estimar $Pr(X)$ de forma não paramétrica usando a distribuição empírica [37].
2.  **LDA:** Ajusta os parâmetros $\mu_k$, $\Sigma$ e $\pi_k$ maximizando a **verossimilhança completa (conjunta)** baseada na densidade conjunta $Pr(X, G=k) = f_k(x) \pi_k = \phi(x; \mu_k, \Sigma) \pi_k$ [37]. Os parâmetros $\alpha_k$ da forma logit são então derivados desses parâmetros Gaussianos estimados [38]. A estimação LDA depende explicitamente da densidade marginal $Pr(X) = \sum_{k=1}^K \pi_k \phi(x; \mu_k, \Sigma)$, que é uma mistura de Gaussianas [38].

> **Implicações da Diferença na Estimação:** Ao confiar nas suposições adicionais do modelo Gaussiano (incluindo a forma da densidade marginal $Pr(X)$), a LDA utiliza mais informação sobre a estrutura dos dados [38]. Se as suposições da LDA forem verdadeiras (dados Gaussianos com covariância comum), a estimação da LDA é assintoticamente mais eficiente (menor variância) do que a regressão logística. Ignorar a parte marginal da verossimilhança (como faz a regressão logística) pode levar a uma perda de eficiência de cerca de 30% na taxa de erro assintótica, se as suposições da LDA se mantiverem [38]. No entanto, essa dependência de $Pr(X)$ e da suposição Gaussiana torna a LDA menos robusta a outliers e a desvios do modelo [39]. Observações longe da fronteira de decisão, que têm pouco peso na regressão logística, desempenham um papel na estimação da matriz de covariância comum $\Sigma$ na LDA [39]. Por outro lado, a dependência da verossimilhança marginal pode ser vista como um regularizador: se os dados são perfeitamente separáveis por um hiperplano, as estimativas de máxima verossimilhança para a regressão logística divergem para infinito, enquanto os coeficientes da LDA permanecem bem definidos [40].

Em geral, a regressão logística é considerada uma aposta mais segura e robusta por depender de menos suposições [41]. No entanto, a experiência prática sugere que ambos os modelos frequentemente fornecem resultados muito semelhantes, mesmo quando a LDA é usada de forma inadequada (por exemplo, com preditores qualitativos) [41].

#### Análise de Discriminantes de Rank Reduzido (Reduced-Rank LDA)

A LDA oferece uma forma natural de redução de dimensionalidade [24]. Os $K$ centróides $\hat{\mu}_k$ no espaço de entrada $\mathbb{R}^p$ residem num subespaço afim de dimensão no máximo $K-1$ [24]. Se $p$ for muito maior que $K$, esta é uma redução considerável. Além disso, para classificar um novo ponto $x$ com base na sua proximidade (na métrica induzida por $\Sigma^{-1}$) aos centróides, apenas as distâncias *dentro* deste subespaço importam; distâncias ortogonais a ele contribuem igualmente para todas as classes e podem ser ignoradas [24, 30].

Isso pode ser formalizado através do *esferização* (*sphering*) dos dados: transforme $X^* = \hat{D}^{-1/2} \hat{U}^T X$, onde $\hat{\Sigma} = \hat{U} \hat{D} \hat{U}^T$ é a decomposição espectral de $\hat{\Sigma}$ [23]. No espaço transformado $X^*$, a matriz de covariância comum estimada é a identidade. A classificação LDA torna-se então equivalente a atribuir $X^*$ ao centróide de classe mais próximo no espaço transformado (após correção pelos $\log \hat{\pi}_k$) [23, 30]. A tarefa é então realizada no subespaço de dimensão $\le K-1$ gerado pelos centróides transformados $\hat{\mu}_k^*$ [30].

Podemos ainda procurar por um subespaço de dimensão $L < K-1$ que seja "ótimo" para a LDA [25]. Fisher propôs encontrar a combinação linear $Z = a^T X$ tal que a variância *entre classes* de $Z$ seja maximizada em relação à variância *dentro das classes* de $Z$ [25, 27]. A variância entre classes é a variância das médias das classes projetadas ($a^T \hat{\mu}_k$), e a variância dentro das classes é a variância agrupada em torno dessas médias projetadas [27]. Matematicamente, isso corresponde a maximizar o quociente de Rayleigh [30]:

$$ \max_a \frac{a^T B a}{a^T W a} $$

onde $W = \hat{\Sigma}$ é a matriz de covariância estimada dentro das classes (within-class) e $B = \sum_{k=1}^K \hat{\pi}_k (\hat{\mu}_k - \hat{\mu})(\hat{\mu}_k - \hat{\mu})^T$ é a matriz de covariância entre classes (between-class), com $\hat{\mu}$ sendo a média global [27]. A Figura 4.9 ilustra por que maximizar essa razão (que minimiza a sobreposição) é preferível a simplesmente maximizar a separação dos centróides projetados (que ignora a covariância dentro das classes) [29].

A solução para este problema é dada pelos autovetores $a_l$ correspondentes aos maiores autovalores do problema de autovalor generalizado $B a = \lambda W a$ (ou equivalentemente, $W^{-1} B a = \lambda a$) [30]. Esses vetores $a_l$ definem as direções das **variáveis canônicas** ou **coordenadas discriminantes** [26, 30]. Os $Z_l = a_l^T X$ são as variáveis discriminantes de rank reduzido. A classificação pode então ser realizada usando apenas as primeiras $L$ dessas variáveis [31]. A Figura 4.4 mostra a visão ótima em 2D para os dados das vogais [25], a Figura 4.10 mostra as taxas de erro em função da dimensão $L$ [31], e a Figura 4.11 mostra as fronteiras de decisão no espaço LDA 2D [31, 33]. Existe também uma ligação estreita entre esta análise de rank reduzido e a regressão da matriz indicadora Y, seguida por uma decomposição espectral [32, 34, 44].

#### Conclusão

Linear Discriminant Analysis é um método de classificação clássico derivado da suposição de densidades de classe Gaussianas com uma matriz de covariância comum. Esta suposição fundamental leva a fronteiras de decisão lineares e funções discriminantes lineares. A estimação dos parâmetros é realizada através da maximização da verossimilhança conjunta, contrastando com a abordagem de verossimilhança condicional da regressão logística, embora ambos os métodos produzam formas funcionais lineares para os log-odds. A LDA oferece uma estrutura para redução de dimensionalidade através da análise de discriminantes de rank reduzido baseada no critério de Fisher. Embora suas suposições possam ser restritivas, a LDA frequentemente demonstra bom desempenho devido à estabilidade de suas estimativas e é uma ferramenta valiosa no arsenal de métodos lineares para classificação.

#### Referências

[^1]: Page 101 (p1): Introduces linear methods for classification, decision boundaries, discriminant functions δk(x).
[^2]: Page 102 (p2): Discusses monotone transformations leading to linear boundaries, mentions logit transformation for two classes.
[^3]: Page 102 (p2): Explicitly mentions LDA and linear logistic regression as popular methods yielding linear log-odds, highlighting their different derivations and fitting methods.
[^4]: Page 102 (p2): States LDA arises from modeling each class density as multivariate Gaussian with a *common* covariance matrix, leading to linear discriminant functions.
[^5]: Page 103 (p3): Shows Figure 4.1 comparing LDA and QDA visually (though QDA is discussed later).
[^6]: Page 105 (p5): Shows Figure 4.2 comparing Linear Regression and LDA visually for a 3-class problem, noting LDA separates well while regression fails (masking).
[^7]: Page 106 (p6): Discusses the masking problem in linear regression and mentions LDA avoids it. Refers to Table 4.1.
[^8]: Page 106 (p6): Starts Section 4.3 on Linear Discriminant Analysis. Mentions needing class posteriors Pr(G|X) for optimal classification via Bayes' theorem. Introduces class-conditional density f_k(x) and prior π_k.
[^9]: Page 107 (p7): Shows Figure 4.4 (vowel data) and Table 4.1 comparing error rates (LDA vs. Linear Regression, QDA, Logistic Regression). Highlights LDA's better performance than linear regression on vowel data.
[^10]: Page 108 (p8): Presents Bayes' theorem formula (4.7) relating posterior to f_k(x) and π_k.
[^11]: Page 108 (p8): States LDA assumes multivariate Gaussian densities (Eq. 4.8) with a *common* covariance matrix Σ_k = Σ.
[^12]: Page 108 (p8): Derives the log-ratio for comparing two classes (k and l), showing it simplifies to a linear function of x (Eq. 4.9) due to the common covariance matrix assumption (cancellation of quadratic terms).
[^13]: Page 108 (p8): States this linear log-odds implies linear decision boundaries (hyperplanes). Mentions Figure 4.5 (left panel) showing idealized Gaussian example.
[^14]: Page 109 (p9): Shows Figure 4.5 (left panel: ideal Gaussians, right panel: sample data with fitted LDA boundaries). Notes boundaries are not necessarily perpendicular bisectors unless Σ is spherical and priors are equal.
[^15]: Page 109 (p9): Defines the linear discriminant functions δ_k(x) (Eq. 4.10).
[^16]: Page 109 (p9): Provides formulas for estimating parameters (π_k, μ_k, Σ) from training data. Mentions N_k is the number of class-k observations.
[^17]: Page 109 (p9): Discusses the two-class case and its correspondence with linear least squares (Eq. 4.11).
[^18]: Page 110 (p10): Elaborates on the two-class correspondence with least squares (coding targets as +1/-1). Notes proportionality of coefficient vectors but different intercepts unless N1=N2. States least squares derivation doesn't require Gaussian assumption for the *direction*, but the intercept (cut-point) derivation (4.11) *does*. Suggests empirical cut-point optimization.
[^19]: Page 110 (p10): Contrasts LDA with linear regression for K > 2 classes, stating LDA avoids masking. Mentions connection via optimal scoring (Sec 12.5).
[^20]: Page 110 (p10): Briefly introduces QDA (Quadratic Discriminant Analysis) where Σ_k are *not* assumed equal, resulting in quadratic discriminant functions (Eq. 4.12).
[^21]: Page 111 (p11): Compares LDA and QDA performance (STATLOG project). Discusses the bias-variance tradeoff, suggesting LDA's simplicity and stability are reasons for its success, even if assumptions aren't met. Counts parameters for LDA: (K-1)x(p+1).
[^22]: Page 112 (p12): Introduces Regularized Discriminant Analysis (RDA) as a compromise between LDA and QDA (Eq. 4.13). Shows Figure 4.7 (RDA on vowel data). Mentions shrinking towards scalar covariance (Eq. 4.14).
[^23]: Page 113 (p13): Discusses computations for LDA/QDA via diagonalization/eigen-decomposition. For LDA, mentions sphering the data (X* = D^(-1/2) U^T X) simplifies classification to finding the closest centroid in transformed space (modulo priors).
[^24]: Page 113 (p13): Introduces Reduced-Rank LDA. Notes K centroids lie in an affine subspace of dimension <= K-1. Suggests dimension reduction by projecting onto this subspace H_{K-1}.
[^25]: Page 114 (p14): Discusses finding optimal lower-dimensional subspaces (L < K-1) for LDA, mentioning Fisher's definition (maximizing projected centroid separation relative to within-class variance). Links this to principal components of centroids. Refers to Figure 4.4 (vowel data optimal 2D view) and Figure 4.8 (canonical/discriminant variables).
[^26]: Page 114 (p14): Describes steps to find optimal subspaces: compute centroids M, within-class covariance W; compute M* = MW^(-1/2); compute B* (covariance of M*), find its eigenvectors v*_l. The l-th discriminant variable is Z_l = v_l^T X where v_l = W^(-1/2) v*_l.
[^27]: Page 114 (p14): Presents Fisher's problem formulation: find linear combination Z = a^T X maximizing between-class variance (a^T B a) relative to within-class variance (a^T W a).
[^28]: Page 115 (p15): Shows Figure 4.8 (more canonical variate plots for vowel data).
[^29]: Page 116 (p16): Shows Figure 4.9 illustrating why Fisher's criterion (minimizing overlap) is better than just maximizing centroid separation.
[^30]: Page 116 (p16): States Fisher's problem leads to a generalized eigenvalue problem for W^(-1)B. Solutions a_l are the discriminant coordinates/canonical variates. Summarizes LDA developments (Gaussian -> linear boundaries -> sphering -> subspace projection -> Fisher's optimal subspaces).
[^31]: Page 117 (p17): Discusses rationale for using reduced subspaces for classification (Gaussian model with centroids constrained to subspace). Mentions log π_k correction factor. Shows Figure 4.10 (error rates vs. dimension for vowel data) and Figure 4.11 (decision boundaries in 2D LDA space).
[^32]: Page 117 (p17): Mentions close connection between Fisher's reduced-rank LDA and regression of indicator response matrix.
[^33]: Page 118 (p18): Shows Figure 4.11 (decision boundaries in 2D LDA subspace).
[^34]: Page 119 (p19): Starts Section 4.4 on Logistic Regression. Explicitly links reduced-rank LDA to regression + eigen-decomposition of Y^T Y-hat.
[^35]: Page 119 (p19): Defines the logistic regression model (Eq. 4.17, 4.18).
[^36]: Page 127 (p27): Starts Section 4.4.5 comparing Logistic Regression and LDA. Repeats the LDA log-posterior odds formula (Eq. 4.33) and the logistic regression logit formula (Eq. 4.34), noting they have the *same linear form*.
[^37]: Page 127 (p27): Highlights the key difference: how coefficients are estimated. Logistic regression maximizes conditional likelihood Pr(G|X), ignoring the marginal Pr(X). LDA maximizes full likelihood Pr(X, G) based on joint density (Eq. 4.37), specifically φ(X; μk, Σ)πk.
[^38]: Page 128 (p28): Explains the role of the marginal density Pr(X) (Eq. 4.38) in LDA estimation. States that relying on the Gaussian assumption provides more information, leading to potentially more efficient estimates (lower variance) if the model is correct. Cites Efron (1975) on ~30% efficiency loss for conditional likelihood if Gaussian assumption holds.
[^39]: Page 128 (p28): Discusses implications: LDA uses points far from the boundary (via Σ estimation), making it less robust to outliers than logistic regression. Mentions LDA can use unlabeled data.
[^40]: Page 128 (p28): Discusses the "regularizing" effect of the marginal likelihood in LDA, preventing infinite parameters when data is perfectly separable (unlike logistic regression - links to Exercise 4.5).
[^41]: Page 128 (p28): Concludes that logistic regression is generally considered safer/more robust due to fewer assumptions, although LDA often gives similar results even when LDA is used inappropriately, such as with qualitative predictors.
[^42]: Page 109 (p9) & Page 35 (Ex 4.2): Discusses the link between 2-class LDA and least squares regression on coded (-N/N1, N/N2) targets. The coefficient vector β is proportional to Σ⁻¹(μ₂ - μ₁).
[^43]: Page 110 (p10) & Page 35 (Ex 4.2): Confirms the coefficient proportionality holds for *any* distinct coding, but the intercept (and thus decision rule) only matches LDA if N1=N2 or specific coding is used.
[^44]: Page 119 (p19) & Page 36 (Ex 4.3): States transforming predictors X to Y-hat (fitted values from indicator regression) and then doing LDA on Y-hat is identical to LDA on original X.
[^45]: Page 134 (p34): Notes LDA solution depends on all data, while optimal separating hyperplane focuses on boundary points. If classes are Gaussian, LDA is optimal. Logistic regression boundary is also shown (red line), close to optimal hyperplane.

<!-- END -->