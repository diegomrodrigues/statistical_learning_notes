## Regressão Linear de Matrizes Indicadoras para Classificação

### Introdução

No contexto dos **métodos lineares para classificação**, uma abordagem direta envolve a adaptação de modelos de regressão linear padrão para o problema de classificação [^1]. Como discutido na introdução deste capítulo, pretendemos encontrar fronteiras de decisão lineares que separam as classes no espaço de entrada [^1]. Uma das maneiras de alcançar isso é ajustar modelos de regressão linear diretamente a variáveis indicadoras de classe e, em seguida, classificar novas observações para a classe com o maior valor ajustado [^1]. Esta abordagem se insere na classe de métodos que modelam **funções discriminantes** $\delta_k(x)$ para cada classe e classificam $x$ para a classe com o maior valor para sua função discriminante [^1]. Este capítulo detalha a formulação, as propriedades e as limitações dessa técnica de regressão linear aplicada a uma matriz indicadora.

### Conceitos Fundamentais

#### Formulação e Ajuste do Modelo

Consideremos um problema de classificação com $K$ classes, rotuladas por conveniência como $1, 2, \dots, K$ [^1]. A variável de resposta categórica $G$ é codificada através de um conjunto de $K$ variáveis indicadoras $Y_k$, $k = 1, \dots, K$, onde $Y_k = 1$ se $G = k$ e $Y_k = 0$ caso contrário [^2]. Para um conjunto de treinamento com $N$ instâncias, essas variáveis indicadoras são coletadas em um vetor $Y = (Y_1, \dots, Y_K)^T$ para cada observação, formando uma **matriz de resposta indicadora** $\mathbf{Y}$ de dimensão $N \times K$ [^2]. Esta matriz $\mathbf{Y}$ consiste em zeros e uns, com cada linha contendo exatamente um único 1 [^2].

Ajustamos um modelo de regressão linear a cada coluna de $\mathbf{Y}$ simultaneamente [^2]. Seja $\mathbf{X}$ a matriz do modelo de dimensão $N \times (p+1)$, contendo os $p$ preditores e uma coluna inicial de 1s para o intercepto [^3]. O ajuste do modelo de regressão linear é dado pela equação de mínimos quadrados ordinários para respostas multivariadas:

$$ \hat{\mathbf{Y}} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} $$ [^2]

Isso resulta em uma matriz de coeficientes $\hat{\mathbf{B}}$ de dimensão $(p+1) \times K$, onde cada coluna $k$ contém os coeficientes para a variável de resposta indicadora $y_k$ [^3]:

$$ \hat{\mathbf{B}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} $$ [^3]

Para uma nova observação com vetor de entrada $x$, a classificação é realizada em duas etapas [^4]:
1.  Calcular o vetor de $K$ saídas ajustadas (fitted outputs) $\hat{f}(x)$, onde $\hat{f}(x)^T = (1, x^T)\hat{\mathbf{B}}$ [^4]. O $k$-ésimo componente é $\hat{f}_k(x) = \hat{\beta}_{k0} + \hat{\beta}_k^T x$ [^1].
2.  Identificar o maior componente do vetor $\hat{f}(x)$ e classificar a observação de acordo [^4]:

$$ \hat{G}(x) = \underset{k \in \{1, \dots, K\}}{\operatorname{argmax}} \hat{f}_k(x) $$ [^4]

#### Fronteiras de Decisão

A **fronteira de decisão** entre quaisquer duas classes, digamos $k$ e $l$, é definida como o conjunto de pontos $x$ para os quais os scores ajustados são iguais, ou seja, $\hat{f}_k(x) = \hat{f}_l(x)$ [^1]. Substituindo as formas lineares, temos:

$$ \hat{\beta}_{k0} + \hat{\beta}_k^T x = \hat{\beta}_{l0} + \hat{\beta}_l^T x $$
$$ (\hat{\beta}_{k0} - \hat{\beta}_{l0}) + (\hat{\beta}_k - \hat{\beta}_l)^T x = 0 $$

Este conjunto define um **conjunto afim** ou **hiperplano** no espaço de entrada $\mathbb{R}^p$ [^1]. Como isso se aplica a qualquer par de classes, o espaço de entrada é dividido em regiões de classificação constante, com **fronteiras de decisão hiperplanares por partes** (piecewise hyperplanar decision boundaries) [^1].

#### Justificativa e Propriedades

Uma justificativa formal para esta abordagem é ver a regressão como uma estimativa da esperança condicional [^5]. Para a variável aleatória indicadora $Y_k$, temos $E(Y_k|X=x) = \text{Pr}(G=k|X=x)$ [^5]. Portanto, estimar a esperança condicional de cada $Y_k$ parece ser um objetivo sensato [^5]. A questão crítica, no entanto, é quão boa é a aproximação fornecida pelo modelo de regressão linear, que é bastante rígido [^5].

Uma propriedade interessante é que, desde que haja um intercepto no modelo (coluna de 1s em $\mathbf{X}$), a soma das saídas ajustadas é sempre igual a 1 para qualquer $x$: $\sum_{k=1}^K \hat{f}_k(x) = 1$ [^6]. No entanto, os valores individuais $\hat{f}_k(x)$ não estão restritos ao intervalo $[0, 1]$; eles podem ser negativos ou maiores que 1, especialmente para predições fora do casco convexo (convex hull) dos dados de treinamento [^7]. Embora essas violações não garantam que a abordagem falhe, elas destacam a natureza rígida da regressão linear para esta tarefa [^7]. O uso de expansões de base $h(X)$ dos inputs pode levar a estimativas consistentes das probabilidades posteriores à medida que $N$ cresce [^8].

#### Visão Alternativa: Classificação pelo Alvo Mais Próximo

Uma perspectiva alternativa, mais simplista, é construir vetores alvo (targets) $t_k$ para cada classe, onde $t_k$ é a $k$-ésima coluna da matriz identidade $K \times K$ [^9]. O vetor de resposta $y_i$ para a observação $i$ (a $i$-ésima linha de $\mathbf{Y}$) é igual a $t_k$ se a observação $i$ pertence à classe $k$ ($g_i=k$) [^9]. O modelo linear pode então ser ajustado minimizando a soma das distâncias Euclidianas quadradas dos vetores ajustados aos seus alvos [^9]:

$$ \min_{\mathbf{B}} \sum_{i=1}^N \\| y_i - [(1, x_i^T)\mathbf{B}]^T \\|^2 $$ [^9]

Uma nova observação $x$ é classificada computando seu vetor ajustado $\hat{f}(x)$ e atribuindo-a à classe cujo vetor alvo $t_k$ está mais próximo [^10]:

$$ \hat{G}(x) = \underset{k}{\operatorname{argmin}} \\| \hat{f}(x) - t_k \\|^2 $$ [^10]

> **Nota Crucial:** Esta regra de classificação pelo alvo mais próximo (closest target) é exatamente a mesma que a regra de classificação pelo maior componente ajustado $\hat{f}_k(x)$ (critério 4.4) [^11]. Isso ocorre porque o critério de norma quadrada na equação (4.5) é uma soma de quadrados que se desacopla entre os componentes, permitindo que seja reorganizado como modelos lineares separados para cada elemento [^12]. A equivalência, no entanto, requer que os valores ajustados somem 1 [^11].

#### Limitações: O Problema do Mascaramento (Masking)

Apesar de sua simplicidade, a abordagem de regressão sobre a matriz indicadora apresenta um problema sério quando o número de classes $K$ é maior ou igual a 3 ($K \ge 3$), sendo particularmente prevalente quando $K$ é grande [^13]. Devido à natureza rígida do modelo de regressão, algumas classes podem ser "mascaradas" por outras, significando que suas regiões de decisão são efetivamente eliminadas [^13].

> **Problema de Mascaramento:** Classes podem ser mascaradas de tal forma que os valores ajustados $\hat{f}_k(x)$ para essas classes nunca são os maiores para nenhum $x$, levando a uma taxa de classificação incorreta de 100% para essas classes.

A Figura 4.2 ilustra uma situação extrema com $K=3$ classes em $\mathbb{R}^2$ que são perfeitamente separáveis por fronteiras lineares [^14]. No entanto, a regressão linear falha em identificar a classe intermediária; sua região de decisão é completamente mascarada pelas outras duas [^14]. A Figura 4.3 analisa um cenário unidimensional projetado, mostrando que a linha de regressão ajustada para a classe intermediária ($\hat{f}_2(x)$) é aproximadamente horizontal e seus valores nunca são dominantes [^15]. Observações da classe 2 são, portanto, sempre classificadas como classe 1 ou classe 3 [^15].

Para resolver o mascaramento em exemplos simples como o da Figura 4.3, um ajuste quadrático (ou polinomial de grau superior) poderia funcionar [^15]. No entanto, se tivéssemos $K$ classes alinhadas, poderiam ser necessários termos polinomiais de até grau $K-1$ [^16]. Em um espaço de entrada $p$-dimensional com orientação arbitrária dos centróides, isso poderia exigir a inclusão de termos polinomiais gerais e produtos cruzados de grau total $K-1$, resultando em $O(p^{K-1})$ termos no total para resolver cenários de pior caso [^17].

O problema de mascaramento não é apenas teórico. No exemplo de reconhecimento de vogais (vowel recognition problem) com $K=11$ classes e $p=10$ dimensões, a regressão linear apresenta uma taxa de erro de teste de 67%, enquanto a **Análise Discriminante Linear (LDA)**, um método intimamente relacionado, alcança 56% [^18, ^26]. A Tabela 4.1 mostra que a regressão linear tem desempenho inferior ao LDA, QDA e Regressão Logística neste problema, sugerindo que o mascaramento prejudicou seu desempenho [^18, ^26]. É importante notar que outros métodos baseados em funções lineares de $x$, como LDA e regressão logística, utilizam essas funções de maneira a evitar este problema de mascaramento [^19].

#### Relação com a Análise Discriminante Linear (LDA)

Existe uma correspondência importante entre a regressão linear de indicadores e a LDA, mas ela se aplica principalmente ao caso de duas classes ($K=2$).
Para $K=2$, pode-se mostrar que o vetor de coeficientes $\hat{\beta}$ obtido pela regressão de mínimos quadrados (com codificação de alvos como +1 e -1, ou outra codificação distinta como $-N/N_1$ e $N/N_2$) é proporcional à direção discriminante encontrada pela LDA [^21, ^33]. A derivação via mínimos quadrados não requer a suposição Gaussiana para os features, ampliando sua aplicabilidade teórica para além de dados Gaussianos [^23]. No entanto, a menos que os tamanhos das classes sejam iguais ($N_1 = N_2$), os interceptos dos modelos de regressão e LDA serão diferentes, resultando em regras de decisão distintas [^22]. A derivação do intercepto específico da LDA *requer* a suposição de dados Gaussianos [^23]. A Figura 4.14 mostra que a solução de mínimos quadrados para $K=2$ é a mesma fronteira encontrada pela LDA (neste caso, com $N_1=N_2$, implicitamente), mas ainda assim comete um erro [^31, ^32].

> **Distinção Crucial (K > 2):** Para mais de duas classes ($K>2$), a LDA *não* é equivalente à regressão linear da matriz indicadora [^24]. A LDA evita explicitamente os problemas de mascaramento associados à abordagem de regressão [^24]. Uma correspondência formal entre regressão e LDA para $K>2$ pode ser estabelecida através do conceito de *optimal scoring* [^25]. Além disso, a LDA pode ser vista como uma regressão seguida por uma decomposição de valores próprios (eigen-decomposition) de $\hat{\mathbf{Y}}^T\hat{\mathbf{Y}}$ [^28]. Se os preditores originais $\mathbf{X}$ forem transformados para $\hat{\mathbf{Y}}$, a LDA usando $\hat{\mathbf{Y}}$ é idêntica à LDA no espaço original [^30, ^34].

### Conclusão

A regressão linear de uma matriz indicadora oferece uma abordagem conceitualmente simples para a classificação, estendendo diretamente a estrutura da regressão linear. Ela produz funções discriminantes lineares $\hat{f}_k(x)$ e classifica as observações com base no maior valor ajustado, resultando em fronteiras de decisão lineares por partes [^1, ^4]. A equivalência matemática com a minimização da distância aos vetores alvo fornece uma visão alternativa [^9, ^10, ^11]. No entanto, a principal limitação desta técnica é o fenômeno de **mascaramento** que pode ocorrer quando $K \ge 3$, onde certas classes podem nunca ter o maior score ajustado, levando a um desempenho de classificação pobre [^13, ^14, ^15, ^18]. Embora exista uma conexão direta com a LDA para $K=2$ em termos da direção discriminante [^21, ^32], a abordagem difere significativamente da LDA para $K>2$, sendo esta última geralmente preferível por evitar o problema de mascaramento [^24]. Consequentemente, embora instrutiva, a regressão linear sobre indicadores é frequentemente superada por outros métodos lineares como LDA e regressão logística em problemas práticos de classificação com múltiplas classes.

### Referências

[^1]: Page 101, Introduction section.
[^2]: Page 103, Section 4.2, first paragraph.
[^3]: Page 103, Section 4.2, equation (4.3) and surrounding text.
[^4]: Page 103, Section 4.2, classification rule (4.4).
[^5]: Page 104, First paragraph.
[^6]: Page 104, Second paragraph, first sentence.
[^7]: Page 104, Second paragraph, sentences 2-3.
[^8]: Page 104, Second paragraph, sentences 5-7.
[^9]: Page 104, Third paragraph, equation (4.5).
[^10]: Page 104, Equation (4.6).
[^11]: Page 104, Bullet point below equation (4.6), referencing (4.4) and (4.6). Also Page 105, bullet point.
[^12]: Page 104, Bullet point below equation (4.6).
[^13]: Page 105, First paragraph after bullet points.
[^14]: Page 105, Reference to Figure 4.2.
[^15]: Page 105, Reference to Figure 4.3, left panel description.
[^16]: Page 105, Last paragraph on page, sentences 4-6.
[^17]: Page 106, First paragraph.
[^18]: Page 106, Second paragraph, reference to Figure 4.4 and Table 4.1.
[^19]: Page 106, Second paragraph, last sentence.
[^20]: Page 109, Last paragraph before section 4.3.1.
[^21]: Page 110, First paragraph, sentences 2-3.
[^22]: Page 110, First paragraph, sentence 4.
[^23]: Page 110, Second paragraph.
[^24]: Page 110, Third paragraph, first sentence.
[^25]: Page 110, Third paragraph, second sentence.
[^26]: Page 111, Table 4.1 and its caption referencing page 107.
[^27]: Page 117, Last paragraph.
[^28]: Page 119, First paragraph, first sentence.
[^29]: Page 119, First paragraph, second sentence.
[^30]: Page 119, First paragraph, last sentence.
[^31]: Page 129, Figure 4.14 caption and text below equation (4.39).
[^32]: Page 129, Text below equation (4.39), last sentence.
[^33]: Page 135, Exercise 4.2.
[^34]: Page 136, Exercise 4.3.
<!-- END -->