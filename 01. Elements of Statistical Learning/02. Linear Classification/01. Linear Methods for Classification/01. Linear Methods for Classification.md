## 4.2 Regressão Linear de uma Matriz Indicadora

### Introdução

Em continuidade à nossa exploração de métodos lineares para classificação apresentada na Seção 4.1 [^1], abordaremos agora uma abordagem direta que utiliza a regressão linear. Como vimos, os métodos lineares buscam estabelecer fronteiras de decisão lineares no espaço de entrada [^1]. Uma maneira de alcançar isso é modelar explicitamente funções discriminantes $\delta_k(x)$ para cada classe $k$ e, em seguida, classificar uma nova observação $x$ para a classe que apresenta o maior valor de $\delta_k(x)$ [^1]. A regressão linear de uma matriz indicadora se enquadra nesta categoria de métodos [^1]. Este capítulo detalhará a formulação desta técnica, sua fundamentação teórica, propriedades e limitações inerentes, notadamente o problema de *masking*.

### Conceitos Fundamentais

**Representação por Matriz Indicadora**

A base deste método reside na codificação das categorias de resposta através de variáveis indicadoras. Se a variável de resposta $G$ possui $K$ classes, que por conveniência rotulamos como $1, 2, ..., K$, introduzimos $K$ variáveis indicadoras $Y_k$ [^4]. A variável $Y_k$ assume o valor 1 se a classe observada $G$ for igual a $k$, e 0 caso contrário [^4]. Estas $K$ indicadoras podem ser agrupadas em um vetor $Y = (Y_1, Y_2, ..., Y_K)^T$ [^4]. Para um conjunto de treinamento com $N$ instâncias, estas respostas formam uma matriz indicadora de resposta $\mathbf{Y}$ de dimensão $N \times K$ [^4].

> A matriz $\mathbf{Y}$ é caracterizada por conter apenas zeros e uns, com a restrição de que cada linha deve conter exatamente um único valor 1, indicando a classe daquela observação específica [^4].

**O Modelo de Regressão**

A abordagem consiste em ajustar um modelo de regressão linear para cada coluna da matriz indicadora $\mathbf{Y}$ simultaneamente [^4]. Considerando $\mathbf{X}$ como a matriz do modelo, de dimensão $N \times (p+1)$, contendo os $p$ preditores e uma coluna inicial de 1s para o intercepto [^4], o ajuste do modelo de regressão linear múltipla é dado pela projeção de $\mathbf{Y}$ sobre o espaço coluna de $\mathbf{X}$. Os valores ajustados $\hat{\mathbf{Y}}$ são calculados como:

$$hat{\mathbf{Y}} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$$ [^4]

Esta equação (4.3) fornece as predições para cada variável indicadora para todas as observações de treinamento. O processo resulta em uma matriz de coeficientes $\hat{\mathbf{B}}$ de dimensão $(p+1) \times K$, calculada como:

$$hat{\mathbf{B}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$$ [^4]

Cada coluna $k$ de $\hat{\mathbf{B}}$ contém os coeficientes $(\hat{\beta}_{k0}, \hat{\beta}_k^T)^T$ para o modelo linear da $k$-ésima variável indicadora, $\hat{f}_k(x) = \hat{\beta}_{k0} + \hat{\beta}_k^T x$ [^1, ^4].

**Regra de Classificação**

Para classificar uma nova observação com vetor de preditores $x$, seguimos um procedimento de duas etapas:

1.  Calculamos o vetor de saídas ajustadas (fitted outputs) $\hat{f}(x)$, que é um vetor $K$-dimensional, usando a matriz de coeficientes estimada $\hat{\mathbf{B}}$: $\hat{f}(x)^T = (1, x^T)\hat{\mathbf{B}}$ [^4].
2.  Identificamos o maior componente neste vetor $\hat{f}(x)$ e atribuímos a observação à classe correspondente [^4]. Formalmente, a classe predita $\hat{G}(x)$ é:

$$hat{G}(x) = \underset{k \in \{1, ..., K\}}{\text{argmax}} \hat{f}_k(x)$$ [^4]

Esta regra (4.4) é uma instância da classificação baseada na maximização de uma função discriminante, onde $\hat{f}_k(x)$ atua como $\delta_k(x)$ [^1].

**Fundamentação e Propriedades**

Uma justificativa formal para esta abordagem deriva da interpretação da regressão como uma estimativa da esperança condicional [^5]. Para a variável aleatória indicadora $Y_k$, temos $E(Y_k | X=x) = Pr(G=k | X=x)$ [^5]. Portanto, ajustar uma regressão a cada $Y_k$ parece um objetivo sensato para estimar as probabilidades posteriores [^5]. No entanto, a questão crítica reside na adequação do modelo de regressão linear, que é inerentemente rígido, como uma aproximação para estas esperanças condicionais [^5].

Uma consequência da natureza rígida da regressão linear é que os valores ajustados $\hat{f}_k(x)$ não estão confinados ao intervalo $[0, 1]$, podendo assumir valores negativos ou maiores que 1, especialmente para predições fora do casco convexo (hull) dos dados de treinamento [^5]. Apesar disso, uma propriedade interessante emerge: desde que o modelo de regressão inclua um intercepto (coluna de 1s em $\mathbf{X}$), é simples verificar que a soma dos valores ajustados para uma dada observação $x$ é igual a 1, isto é, $\sum_{k=1}^K \hat{f}_k(x) = 1$ [^5]. Estas violações da natureza probabilística não invalidam necessariamente a abordagem para classificação, e em muitos problemas práticos, ela produz resultados similares a outros métodos lineares padrão [^5].

Um ponto de vista alternativo, mais simplista, é construir vetores alvo (**targets**) $t_k$ para cada classe, onde $t_k$ é a $k$-ésima coluna da matriz identidade $K \times K$ [^5]. O problema de predição torna-se então tentar reproduzir o alvo apropriado para cada observação. Usando a mesma codificação 0/1, o vetor de resposta $y_i$ para a observação $i$ (a $i$-ésima linha de $\mathbf{Y}$) é igual a $t_k$ se a classe verdadeira $g_i$ for $k$ [^5]. O modelo linear pode então ser ajustado minimizando a soma das distâncias Euclidianas quadradas entre os vetores ajustados e seus alvos correspondentes:

$$min_{\mathbf{B}} \sum_{i=1}^N \\|y_i - [(1, x_i^T)\mathbf{B}]^T\\|^2$$ [^5]

Este critério (4.5) é exatamente o critério para regressão linear de múltiplas respostas [^5]. A classificação de uma nova observação $x$ é feita calculando seu vetor ajustado $\hat{f}(x)$ e classificando-o para o alvo $t_k$ mais próximo:

$$hat{G}(x) = \underset{k}{\text{argmin}} \\| \hat{f}(x) - t_k \\|^2$$ [^5]

Esta regra de classificação pelo alvo mais próximo (4.6) é exatamente equivalente à regra do componente máximo ajustado (4.4), contanto que os valores ajustados somem 1, o que é garantido pela presença do intercepto [^5].

**Limitações: O Problema de *Masking***

Apesar de sua simplicidade conceitual, a abordagem de regressão sobre a matriz indicadora sofre de um problema sério, conhecido como **masking**, particularmente quando o número de classes $K$ é maior ou igual a 3, e especialmente prevalente para $K$ grande [^6]. Devido à natureza rígida do modelo de regressão, certas classes podem ser "mascaradas" por outras, significando que seus valores ajustados $\hat{f}_k(x)$ nunca são os maiores para nenhuma região do espaço de entrada [^6].

> A Figura 4.2 [^6] ilustra uma situação extrema com $K=3$. As três classes são perfeitamente separáveis por fronteiras de decisão lineares. No entanto, a regressão linear falha completamente em identificar a classe intermediária, cujas observações são classificadas como pertencentes a uma das outras duas classes [^6].

A Figura 4.3 [^6] demonstra o problema projetando os dados em uma única dimensão relevante. As três linhas de regressão ajustadas às variáveis indicadoras são mostradas. A linha correspondente à classe intermediária é horizontal, e seus valores ajustados nunca dominam os das outras classes [^6]. Consequentemente, nenhuma observação será classificada como pertencente à classe 2 [^6]. Embora uma regressão quadrática pudesse resolver este exemplo específico com $K=3$, um problema similar ocorreria com $K=4$ classes alinhadas, exigindo potencialmente um ajuste cúbico [^6]. Uma regra geral (embora informal) sugere que para $K \ge 3$ classes alinhadas, termos polinomiais de grau até $K-1$ podem ser necessários para resolver tais cenários de pior caso [^6]. No espaço de entrada $p$-dimensional, isso implicaria a necessidade de termos polinomiais gerais e produtos cruzados de grau total $K-1$, resultando em $O(p^{K-1})$ termos no total [^7].

Este problema de *masking* não é apenas uma curiosidade teórica. Ele ocorre naturalmente em cenários com $K$ grande e $p$ pequeno [^7]. A Tabela 4.1 [^8], referente aos dados de reconhecimento de vogais (vowel data, $K=11, p=10$), mostra que a regressão linear apresenta uma taxa de erro no conjunto de teste de 67%, consideravelmente pior que a taxa de 56% obtida pela Análise Discriminante Linear (LDA), um método intimamente relacionado [^7, ^8]. Parece que o *masking* prejudicou o desempenho da regressão linear neste caso [^7]. É importante notar que outros métodos discutidos neste capítulo, embora baseados em funções lineares de $x$, utilizam-nas de forma a evitar este problema de *masking* [^7].

**Relação com LDA (para K=2)**

No caso específico de duas classes ($K=2$), existe uma correspondência direta entre a regressão linear sobre a matriz indicadora (ou qualquer codificação distinta para as duas classes, como +1/-1 ou -N/N1, N/N2) e a Análise Discriminante Linear (LDA) [^10, ^13]. Pode-se demonstrar que o vetor de coeficientes $\hat{\beta}$ obtido por mínimos quadrados é proporcional à direção discriminante encontrada pela LDA [^10, ^13]. No entanto, a menos que os tamanhos das classes sejam iguais ($N_1 = N_2$), os interceptos e, consequentemente, as fronteiras de decisão resultantes serão diferentes [^10]. É crucial ressaltar que para $K > 2$ classes, a regressão da matriz indicadora *não* é equivalente à LDA, e a LDA evita os problemas de *masking* associados à abordagem de regressão [^10]. Uma correspondência mais complexa entre regressão e LDA para $K>2$ pode ser estabelecida através do conceito de *optimal scoring* [^10] ou considerando transformações dos preditores como em [^14].

### Conclusão

A regressão linear de uma matriz indicadora oferece uma abordagem conceitualmente simples para a classificação, enquadrando-se na estratégia de modelar funções discriminantes lineares [^1, ^4]. O método envolve ajustar modelos de regressão linear independentes para variáveis indicadoras 0/1 que representam a afiliação de classe e classificar novas observações com base na maior resposta ajustada [^4]. Embora fundamentada na ideia de estimar probabilidades posteriores [^5], sua principal limitação é a rigidez do modelo linear, que pode levar ao fenômeno de *masking* quando $K \ge 3$, onde classes intermediárias podem ser completamente obscurecidas [^6]. Este problema pode degradar significativamente o desempenho, como evidenciado em exemplos práticos [^7, ^8]. Em contraste, outros métodos lineares como LDA e regressão logística, embora relacionados, são geralmente preferidos por não sofrerem desta limitação específica [^7, ^10]. A conexão com LDA é direta apenas para $K=2$ [^10, ^13], e relações mais gerais para $K>2$ requerem conceitos mais avançados como *optimal scoring* [^10] ou transformações específicas dos dados [^14].

### Referências

[^1]: Page 101: In this chapter we revisit the classification problem and focus on linear methods for classification... For an important class of procedures, these decision boundaries are linear; this is what we will mean by linear methods for classification. There are several different ways in which linear decision boundaries can be found... fit linear regression models to the class indicator variables, and classify to the largest fit... fitted linear model for the kth indicator response variable is fk(x) = ẞko + βx... The decision boundary between class k and l is that set of points for which f(x) = fe(x), that is, the set {x: (ẞko – βeo) + (βκ – βe)Tx = 0}, an affine set or hyperplane... This regression approach is a member of a class of methods that model discriminant functions δκ(x) for each class, and then classify x to the class with the largest value for its discriminant function.
[^2]: Page 102: ... methods that model the posterior probabilities Pr(G = k|X = x) are also in this class... we can expand our variable set X1,..., Xp by including their squares and cross-products... Linear functions in the augmented space map down to quadratic functions in the original space hence linear decision boundaries to quadratic decision boundaries.
[^3]: Page 103: FIGURE 4.1. The left plot shows some data from three classes, with linear decision boundaries found by linear discriminant analysis. The right plot shows quadratic decision boundaries... obtained by finding linear boundaries in the five-dimensional space X1, X2, X1 X2, X1^2, X2^2.
[^4]: Page 103: Here each of the response categories are coded via an indicator variable. Thus if G has K classes, there will be K such indicators Yk, k = 1, ..., K, with Yk = 1 if G = k else 0. These are collected together in a vector Y = (Y1, ..., YK), and the N training instances of these form an N × K indicator response matrix Y. Y is a matrix of 0\'s and 1\'s, with each row having a single 1. We fit a linear regression model to each of the columns of Y simultaneously, and the fit is given by Y = X(XTX)−1XTY. (4.3)... we have a coefficient vector for each response column yk, and hence a (p+1) × K coefficient matrix B = (XX)−¹XTY. Here X is the model matrix with p+1 columns... A new observation with input x is classified as follows: compute the fitted output f(x)T = (1, xT)B, a K vector; identify the largest component and classify accordingly: Ĝ(x) = argmaxkeçfk(x). (4.4)
[^5]: Page 104: One rather formal justification is to view the regression as an estimate of conditional expectation. For the random variable Yk, E(Yk|X = x) = Pr(G = k|X = x)... how good an approximation to conditional expectation is the rather rigid linear regression model?... the f(x) can be negative or greater than 1... It is quite straightforward to verify that keg fk (x) = 1 for any x, as long as there is an intercept in the model... A more simplistic viewpoint is to construct targets te for each class, where tk is the kth column of the K × K identity matrix... fit the linear model by least squares: min B Σ ||yi - [(1,x)B]T||2. (4.5)... classify to the closest target: Ĝ(x) = argmin k || f(x) – tk ||2. (4.6)... The sum-of-squared-norm criterion is exactly the criterion for multiple response linear regression... The closest target classification rule (4.6) is easily seen to be exactly the same as the maximum fitted component criterion (4.4), but does require that the fitted values sum to 1.
[^6]: Page 105: There is a serious problem with the regression approach when the number of classes K > 3, especially prevalent when K is large. Because of the rigid nature of the regression model, classes can be masked by others. Figure 4.2 illustrates an extreme situation when K = 3. The three classes are perfectly separated by linear decision boundaries, yet linear regression misses the middle class completely. In Figure 4.3 we have projected the data onto the line joining the three centroids... The three regression lines (left panel) are included, and we see that the line corresponding to the middle class is horizontal and its fitted values are never dominant! Thus, observations from class 2 are classified either as class 1 or class 3... For this simple example a quadratic rather than linear fit (for the middle class at least) would solve the problem. However, it can be seen that if there were four rather than three classes lined up like this, a quadratic would not come down fast enough, and a cubic would be needed as well. A loose but general rule is that if K > 3 classes are lined up, polynomial terms up to degree K - 1 might be needed to resolve them.
[^7]: Page 106: So in p-dimensional input space, one would need general polynomial terms and cross-products of total degree K − 1, O(pK−1) terms in all, to resolve such worst-case scenarios. The example is extreme, but for large K and small p such maskings naturally occur... Figure 4.4 is a projection of the training data for a vowel recognition problem... Table 4.1; linear regression has an error rate of 67%, while a close relative, linear discriminant analysis, has an error rate of 56%. It seems that masking has hurt in this case. While all the other methods in this chapter are based on linear functions of x as well, they use them in such a way that avoids this masking problem.
[^8]: Page 107: TABLE 4.1. Training and test error rates using a variety of linear techniques on the vowel data... Linear regression 0.48 (Train) 0.67 (Test)... Linear discriminant analysis 0.32 (Train) 0.56 (Test)... We see that linear regression is hurt by masking, increasing the test and training error by over 10%.
[^9]: Page 109: With two classes there is a simple correspondence between linear discriminant analysis and classification by linear least squares, as in (4.5). The LDA rule classifies to class 2 if xT Σ−1(μ2 – μ1) > ... (4.11)
[^10]: Page 110: Suppose we code the targets in the two classes as +1 and -1, respectively. It is easy to show that the coefficient vector from least squares is proportional to the LDA direction given in (4.11) (Exercise 4.2). [In fact, this correspondence occurs for any (distinct) coding of the targets; see Exercise 4.2]. However unless N₁ = N2 the intercepts are different and hence the resulting decision rules are different... With more than two classes, LDA is not the same as linear regression of the class indicator matrix, and it avoids the masking problems associated with that approach... A correspondence between regression and LDA can be established through the notion of optimal scoring, discussed in Section 12.5.
[^11]: Page 117: There is a close connection between Fisher\'s reduced rank discriminant analysis and regression of an indicator response matrix.
[^12]: Page 119: The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x...
[^13]: Page 135: Ex. 4.2 Suppose we have features x ∈ IRP, a two-class response, with class sizes N1, N2, and the target coded as -N/N1, N/N2... (b) Consider minimization of the least squares criterion Σ(yi – βο – βTxi)2... (c) Hence show that ΣBβ is in the direction (μ2 – μ1) and thus β ∝ Σ−1(μ2 – μ1). Therefore the least squares regression coefficient is identical to the LDA coefficient, up to a scalar multiple.
[^14]: Page 136: Ex. 4.3 Suppose we transform the original predictors X to Ý via linear regression. In detail, let Ý = X(XTX)-1XTY = XB, where Y is the indicator response matrix... Show that LDA using Ŷ is identical to LDA in the original space.

<!-- END -->