## Capítulo 4.5: Modelagem Explícita de Fronteiras Lineares como Hiperplanos

### Introdução

Nos métodos lineares para classificação, como explorado neste capítulo [^1], um tema central é a definição de **fronteiras de decisão lineares**. Abordagens como a regressão linear sobre variáveis indicadoras (Seção 4.2) [^2] e a análise discriminante linear (LDA) (Seção 4.3) [^3], embora resultem em fronteiras lineares, o fazem como consequência da modelagem das densidades de classe ou das probabilidades posteriores. Similarmente, a regressão logística (Seção 4.4) [^26] modela as probabilidades posteriores através de funções lineares no espaço logit, o que também define fronteiras de decisão lineares [^27]. Em contraste, uma abordagem mais direta consiste em **modelar explicitamente as fronteiras entre as classes como lineares** [^4]. Isso envolve definir a fronteira de decisão como um **hiperplano**, caracterizado por um **vetor normal** e um **ponto de corte** (*cut-point*) [^4]. Este capítulo foca em métodos que adotam essa abordagem explícita, construindo diretamente classificadores baseados em hiperplanos separadores. Especificamente, examinaremos dois procedimentos fundamentais: o algoritmo **Perceptron** de Rosenblatt [^6] e o conceito de **Hiperplanos Ótimos Separadores** de Vapnik [^13], que formam a base para classificadores de vetores de suporte [^25]. Estes métodos buscam ativamente separar os dados em classes distintas da forma mais eficaz possível dentro da estrutura linear [^25]. O nível matemático desta seção é um pouco mais elevado que o das seções anteriores [^25].

### Fundamentos Geométricos de Hiperplanos

Antes de detalhar os algoritmos, revisaremos brevemente a álgebra vetorial associada a hiperplanos [^5]. Um hiperplano ou conjunto afim $L$ em $\mathbb{R}^p$ é definido pela equação:

$$ f(x) = \beta_0 + \beta^T x = 0 $$

onde $\beta$ é um vetor em $\mathbb{R}^p$ e $\beta_0$ é um escalar [^5]. Algumas propriedades importantes incluem:

1.  Para quaisquer dois pontos $x_1$ e $x_2$ pertencentes a $L$, temos $\beta^T (x_1 - x_2) = (\beta_0 + \beta^T x_1) - (\beta_0 + \beta^T x_2) = 0 - 0 = 0$. Portanto, o vetor $\beta$ é ortogonal a qualquer vetor contido no hiperplano. O vetor **normal** unitário à superfície $L$ é dado por $\beta^* = \beta / ||\beta||$ [^5].
2.  Para qualquer ponto $x_0$ em $L$, temos $\beta^T x_0 = -\beta_0$ [^5].
3.  A **distância sinalizada** (*signed distance*) de um ponto qualquer $x$ ao hiperplano $L$ é dada por:

    $$ \beta^{*T} (x - x_0) = \frac{1}{||\beta||} \beta^T (x - x_0) = \frac{1}{||\beta||} (\beta^T x - \beta^T x_0) = \frac{1}{||\beta||} (\beta^T x + \beta_0) = \frac{1}{||\beta||} f(x) $$ [^5]

    Portanto, $f(x)$ é proporcional à distância sinalizada de $x$ ao hiperplano definido por $f(x) = 0$ [^5].

### O Modelo Perceptron

O **Perceptron** de Rosenblatt (1958) representa um dos primeiros e mais conhecidos algoritmos que buscam explicitamente um hiperplano separador nos dados de treinamento [^4], [^6]. O algoritmo tenta encontrar *um* hiperplano que separe as classes, caso tal hiperplano exista [^4]. Consideramos um problema de duas classes, com respostas $y_i \in \{-1, 1\}$. Um ponto $x_i$ com resposta $y_i=1$ é mal classificado se $f(x_i) = x_i^T \beta + \beta_0 < 0$, e o oposto ocorre para um ponto com $y_i = -1$ [^8]. O objetivo do algoritmo é minimizar a seguinte função de custo para os pontos mal classificados:

$$ D(\beta, \beta_0) = - \sum_{i \in \mathcal{M}} y_i (x_i^T \beta + \beta_0) $$ [^8]

onde $\mathcal{M}$ indexa o conjunto de pontos mal classificados [^8]. Esta quantidade é não negativa e proporcional à soma das distâncias dos pontos mal classificados à fronteira de decisão $f(x) = 0$ [^8].

O algoritmo Perceptron utiliza, na verdade, um método de **descida de gradiente estocástico** (*stochastic gradient descent*) para minimizar este critério linear por partes [^9]. Em vez de calcular a soma das contribuições do gradiente de todas as observações mal classificadas antes de dar um passo, um passo é dado após a visita a *cada* observação mal classificada [^9]. Os pontos mal classificados são visitados em alguma sequência, e os parâmetros $(\beta, \beta_0)$ são atualizados através da regra:

$$ \begin{pmatrix} \beta \\\\ \beta_0 \end{pmatrix} \leftarrow \begin{pmatrix} \beta \\\\ \beta_0 \end{pmatrix} + \rho \begin{pmatrix} y_i x_i \\\\ y_i \end{pmatrix} $$ [^10]

onde $x_i$ é um ponto mal classificado e $\rho$ é a taxa de aprendizado (*learning rate*), que pode ser tomada como 1 sem perda de generalidade neste caso [^10].

Foi demonstrado que, se as classes são linearmente separáveis, o algoritmo converge para um hiperplano separador em um número finito de passos [^11]. A Figura 4.14 no contexto [^25] ilustra duas soluções diferentes encontradas pelo algoritmo Perceptron para um problema de brinquedo, dependendo dos pontos de partida aleatórios [^25]. No entanto, o algoritmo Perceptron apresenta várias limitações significativas, resumidas por Ripley (1996) [^12]:

> *   *Quando os dados são separáveis, existem muitas soluções (hiperplanos separadores), e aquela que é encontrada depende dos valores iniciais.* [^12]
> *   *O número "finito" de passos pode ser muito grande. Quanto menor a separação (gap) entre as classes, mais tempo levará para encontrar.* [^12]
> *   *Quando os dados não são separáveis, o algoritmo não converge e ciclos se desenvolvem. Os ciclos podem ser longos e, portanto, difíceis de detectar.* [^12]

Embora a busca por hiperplanos em espaços de características aumentados (transformações de base) possa mitigar o problema da não separabilidade [^32], isso pode levar a overfitting [^13]. Uma solução mais elegante para o problema da não unicidade da solução em dados separáveis é adicionar restrições adicionais ao hiperplano separador [^13].

### Hiperplanos Ótimos Separadores

O conceito de **Hiperplano Ótimo Separador** (*Optimal Separating Hyperplane*), proposto por Vapnik (1996), aborda diretamente a questão da não unicidade da solução do Perceptron quando os dados são linearmente separáveis [^13]. A ideia central é encontrar o *único* hiperplano que não apenas separa as duas classes, mas também maximiza a **margem**, definida como a distância do hiperplano ao ponto mais próximo de qualquer uma das classes [^13]. Intuitivamente, maximizar a margem nos dados de treinamento leva a um melhor desempenho de classificação em dados de teste [^13].

Formalmente, consideramos o seguinte problema de otimização:

$$ \max_{\beta, \beta_0, ||\beta||=1} M $$
$$ \text{sujeito a } y_i (x_i^T \beta + \beta_0) \ge M, \quad i = 1, \dots, N $$ [^14]

As condições garantem que todos os pontos estejam a uma distância sinalizada de pelo menos $M$ da fronteira de decisão definida por $\beta$ e $\beta_0$ [^14]. Buscamos o maior $M$ possível e os parâmetros associados [^14]. A restrição $||\beta||=1$ pode ser removida substituindo as condições por $\frac{1}{||\beta||} y_i (x_i^T \beta + \beta_0) \ge M$ [^46], o que é equivalente (redefinindo $\beta_0$) a $y_i (x_i^T \beta + \beta_0) \ge M ||\beta||$ [^15]. Como para qualquer $(\beta, \beta_0)$ que satisfaça essas desigualdades, qualquer múltiplo positivamente escalado também as satisfaz, podemos arbitrariamente definir $||\beta|| = 1/M$ [^15]. Maximizar $M$ é então equivalente a minimizar $M^{-2} = ||\beta||^2$. Assim, o problema (4.45) é equivalente a:

$$ \min_{\beta, \beta_0} \frac{1}{2} ||\beta||^2 $$
$$ \text{sujeito a } y_i (x_i^T \beta + \beta_0) \ge 1, \quad i = 1, \dots, N $$ [^15]

À luz da fórmula da distância sinalizada (4.40) [^5], as restrições definem uma "laje" (*slab*) ou margem vazia em torno da fronteira de decisão linear com espessura $1/||\beta||$ [^15]. Escolhemos $\beta$ e $\beta_0$ para maximizar essa espessura [^15]. Este é um problema de otimização convexa (critério quadrático com restrições de desigualdade linear) [^33].

A solução pode ser encontrada usando multiplicadores de Lagrange. A função Lagrangiana (primal) a ser minimizada em relação a $\beta$ e $\beta_0$ é:

$$ L_P = \frac{1}{2} ||\beta||^2 - \sum_{i=1}^N \alpha_i [y_i(x_i^T \beta + \beta_0) - 1] $$ [^16]

onde $\alpha_i \ge 0$. Igualando as derivadas em relação a $\beta$ e $\beta_0$ a zero, obtemos:

$$ \beta = \sum_{i=1}^N \alpha_i y_i x_i $$ [^17]
$$ 0 = \sum_{i=1}^N \alpha_i y_i $$ [^17]

Substituindo essas condições de volta em $L_P$, obtemos o problema dual de Wolfe:

$$ L_D = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i^T x_k $$ [^18]

que deve ser maximizado sujeito a $\alpha_i \ge 0$ e $\sum_{i=1}^N \alpha_i y_i = 0$ [^18]. Além disso, a solução deve satisfazer as condições de Karush-Kuhn-Tucker (KKT), que incluem as condições primais, as condições duais ($\alpha_i \ge 0$) e a condição de folga complementar (*complementary slackness*):

$$ \alpha_i [y_i(x_i^T \beta + \beta_0) - 1] = 0 \quad \forall i $$ [^19]

Destas condições KKT, podemos inferir propriedades importantes da solução:

*   Se $\alpha_i > 0$, então $y_i(x_i^T \beta + \beta_0) = 1$. Isso significa que o ponto $x_i$ está exatamente na fronteira da margem (ou "laje") [^20]. Tais pontos são chamados de **vetores de suporte** (*support vectors*).
*   Se $y_i(x_i^T \beta + \beta_0) > 1$, então o ponto $x_i$ está fora da fronteira da margem, e necessariamente $\alpha_i = 0$ [^20].

Da equação (4.50) [^17], vemos que o vetor solução $\beta$ é definido como uma combinação linear apenas dos **vetores de suporte** $x_i$ (aqueles com $\alpha_i > 0$) [^20]. O intercepto $\beta_0$ pode ser obtido resolvendo a equação (4.53) [^19] para qualquer vetor de suporte (ou, de forma mais robusta, pela média das soluções obtidas a partir de todos os vetores de suporte).

Uma vez que $\beta$ e $\beta_0$ são determinados, a classificação de uma nova observação $x$ é feita calculando $f(x) = x^T \beta + \beta_0$ e usando seu sinal:

$$ G(x) = \text{sign}(f(x)) = \text{sign}(x^T \beta + \beta_0) $$ [^21]

A intuição por trás do hiperplano ótimo separador é que uma margem grande nos dados de treinamento levará a uma boa separação nos dados de teste [^22]. A descrição da solução em termos de vetores de suporte sugere que o hiperplano ótimo foca mais nos pontos que "contam" (próximos à fronteira), tornando-o potencialmente mais robusto a erros de especificação do modelo em comparação com a solução LDA, que depende de todos os dados, mesmo aqueles longe da fronteira de decisão [^23]. No entanto, a identificação dos vetores de suporte requer o uso de todos os dados [^23]. Se as classes forem verdadeiramente Gaussianas com covariância comum, LDA é ótimo, e o hiperplano separador pagará um preço por focar nos dados (potencialmente mais ruidosos) nas fronteiras [^23].

É interessante notar que, quando um hiperplano separador existe, a regressão logística também o encontrará, pois a log-verossimilhança pode ser levada a zero (infinito nos parâmetros $\beta, \beta_0$) [^24]. A solução da regressão logística compartilha algumas características qualitativas com a solução do hiperplano separador, como dar maior peso a pontos próximos da fronteira de decisão [^24].

Quando os dados não são linearmente separáveis, não haverá solução viável para o problema (4.48) [^15], [^34]. Uma formulação alternativa, conhecida como **máquina de vetores de suporte** (*support vector machine*), permite sobreposição (*overlap*) e minimiza uma medida da extensão dessa sobreposição, conforme discutido no Capítulo 12 [^25].

### Conclusão

Este capítulo explorou métodos que modelam explicitamente as fronteiras de decisão entre classes como hiperplanos lineares. Iniciamos com o algoritmo **Perceptron**, um método historicamente importante que encontra *um* hiperplano separador se os dados forem linearmente separáveis, mas sofre de não unicidade e problemas de convergência [^4], [^12]. Em seguida, detalhamos o conceito de **Hiperplanos Ótimos Separadores**, que buscam o hiperplano único que maximiza a margem entre as classes [^13], [^15]. Esta abordagem, formulada como um problema de otimização convexa, leva a uma solução onde o hiperplano é determinado por um subconjunto dos dados de treinamento conhecidos como **vetores de suporte** [^17], [^20]. A maximização da margem oferece uma base teórica e prática para boa generalização [^22]. Esses métodos contrastam com abordagens como LDA e regressão logística, onde as fronteiras lineares surgem implicitamente da modelagem estatística [^2], [^3], [^26]. Os conceitos de hiperplanos separadores e maximização de margem são fundamentais e servem como base para as máquinas de vetores de suporte, uma técnica poderosa para classificação que lida com fronteiras não lineares e dados não separáveis [^25].

### Referências

[^1]: Página 101, Seção 4.1 Introduction.
[^2]: Página 101, Seção 4.1 Introduction. Referência à Seção 4.2.
[^3]: Página 102, Seção 4.1 Introduction. Referência à Seção 4.3 (LDA).
[^4]: Página 102, Seção 4.1 Introduction.
[^5]: Página 130, Propriedades 1, 2, 3 e Figura 4.15.
[^6]: Página 102, Seção 4.1 Introduction; Página 130, início da Seção 4.5.1.
[^7]: Página 110, Discussão sobre codificação +1/-1 para LDA/Regressão; Página 129, menção a resposta -1/1; Página 131, uso de $y_i$ na fórmula de D.
[^8]: Página 131, Equação (4.41) e texto circundante.
[^9]: Página 131, Descrição do algoritmo como stochastic gradient descent.
[^10]: Página 131, Equação (4.44) e texto circundante.
[^11]: Página 131, Menção à convergência em número finito de passos e Exercício 4.6.
[^12]: Página 131, Lista de problemas (Ripley, 1996).
[^13]: Página 102, Seção 4.1 Introduction; Página 132, Seção 4.5.2.
[^14]: Página 132, Equação (4.45).
[^15]: Página 132, Equações (4.47), (4.48) e texto circundante.
[^16]: Página 133, Equação (4.49).
[^17]: Página 133, Equações (4.50), (4.51).
[^18]: Página 133, Equação (4.52) e texto circundante.
[^19]: Página 133, Equação (4.53).
[^20]: Página 133, Discussão das condições KKT e definição de support points/vectors.
[^21]: Página 133, Equação (4.54).
[^22]: Página 134, Primeiro parágrafo.
[^23]: Página 134, Segundo parágrafo.
[^24]: Página 134, Terceiro parágrafo.
[^25]: Página 129, Seção 4.5 Introduction; Página 135, Bibliographic Notes.
[^26]: Página 102, Seção 4.1 Introduction; Página 119, Seção 4.4.
[^27]: Página 102, Equação (4.2) e texto; Página 127, Equação (4.34).
[^32]: Página 132, Primeiro parágrafo.
[^33]: Página 132, Final do parágrafo antes da Eq. 4.49.
[^34]: Página 134, Último parágrafo.
[^46]: Página 132, Equação (4.46).

<!-- END -->