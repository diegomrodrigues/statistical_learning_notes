## Capítulo 4: Métodos Lineares para Classificação

### Seção 4.2: Regressão Linear de uma Matriz Indicadora

#### Introdução

Continuando nossa exploração de métodos lineares para classificação, conforme introduzido na Seção 4.1 [^36], voltamos nossa atenção para uma abordagem que utiliza diretamente o framework da regressão linear. Vimos anteriormente que uma maneira de obter fronteiras de decisão lineares é ajustar modelos de regressão linear às variáveis indicadoras de classe [^37]. Esta seção aprofunda essa técnica, detalhando sua formulação matemática, regras de classificação, justificativas teóricas e, crucialmente, suas limitações inerentes. Faremos referência aos fundamentos da regressão linear discutidos no Capítulo 3 [^41] e posicionaremos essa abordagem em relação a outros métodos lineares que serão abordados posteriormente neste capítulo.

#### Conceitos Fundamentais

**Definição e Formulação**

A ideia central da **regressão linear de uma matriz indicadora** é codificar as $K$ categorias de resposta através de $K$ variáveis indicadoras [^1]. Se a variável de resposta categórica $G$ assume valores em $\mathcal{G} = \{1, 2, ..., K\}$, criamos $K$ indicadores $Y_k$, $k = 1, ..., K$, tais que $Y_k = 1$ se $G = k$ e $Y_k = 0$ caso contrário [^1]. Para um conjunto de $N$ instâncias de treinamento, essas variáveis indicadoras são coletadas em um vetor $Y = (Y_1, ..., Y_K)$ para cada observação [^2]. Empilhando essas $N$ instâncias, formamos uma **matriz de resposta indicadora** $\mathbf{Y}$ de dimensão $N \times K$ [^2]. É importante notar que $\mathbf{Y}$ é uma matriz composta por zeros e uns, onde cada linha possui exatamente um único 1, indicando a classe daquela observação [^3].

**Ajuste do Modelo**

Ajustamos então um modelo de regressão linear a *cada uma* das $K$ colunas da matriz $\mathbf{Y}$ simultaneamente [^4]. Utilizando a notação matricial, onde $\mathbf{X}$ é a matriz do modelo (contendo as $p$ variáveis de entrada e uma coluna de 1s para o intercepto, totalizando $p+1$ colunas) [^7], o ajuste por mínimos quadrados é dado por:

$$mathbf{\hat{Y}} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} \quad (4.3)$$
[^5]

Este processo resulta em uma matriz de coeficientes $\mathbf{\hat{B}}$ de dimensão $(p+1) \times K$, onde cada coluna $k$ contém os coeficientes do modelo de regressão para a variável indicadora $Y_k$ [^6]. A matriz de coeficientes é calculada como $\mathbf{\hat{B}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$ [^6]. Detalhes adicionais sobre o mecanismo da regressão linear podem ser encontrados no Capítulo 3 [^41].

**Regra de Classificação**

Para classificar uma nova observação com vetor de entrada $x$, seguimos dois passos [^8]:

1.  Calcular as saídas ajustadas (fitted outputs), que formam um vetor $K$-dimensional: $\hat{f}(x)^T = (1, x^T)\mathbf{\hat{B}}$ [^8].
2.  Identificar o maior componente desse vetor e classificar a observação na classe correspondente [^9]:

$$hat{G}(x) = \underset{k \in \mathcal{G}}{\text{argmax}} \\; \hat{f}_k(x) \quad (4.4)$$
[^9]

**Justificativas Teóricas**

Existem algumas formas de justificar esta abordagem. Uma delas, de natureza mais formal, é interpretar a regressão como uma estimativa da esperança condicional [^10]. Para a variável aleatória $Y_k$, temos $E(Y_k | \mathbf{X} = x) = Pr(G=k | \mathbf{X} = x)$ [^10]. Portanto, estimar a esperança condicional de cada $Y_k$ parece ser um objetivo sensato para obter as probabilidades posteriores de classe [^10]. A questão crítica, no entanto, reside em quão boa é a aproximação fornecida pelo modelo de regressão linear, que é inerentemente rígido, para essa esperança condicional [^11].

Uma propriedade interessante é que, desde que o modelo inclua um intercepto (coluna de 1s em $\mathbf{X}$), a soma das saídas ajustadas é igual a 1 para qualquer $x$: $\sum_{k \in \mathcal{G}} \hat{f}_k(x) = 1$ [^12]. No entanto, os valores individuais $\hat{f}_k(x)$ não estão restritos ao intervalo $[0, 1]$; eles podem ser negativos ou maiores que 1, e tipicamente alguns o são [^13]. Isso é uma consequência da natureza rígida da regressão linear, especialmente para previsões fora do casco convexo (hull) dos dados de treinamento [^13]. Essas violações, por si só, não garantem que a abordagem falhará, e de fato, em muitos problemas, ela produz resultados similares a métodos lineares mais padronizados para classificação [^14]. Pode-se notar que, se permitirmos regressão linear sobre expansões de base $h(X)$ das entradas, essa abordagem pode levar a estimativas consistentes das probabilidades à medida que o tamanho do conjunto de treinamento $N$ cresce [^15]. Tais abordagens baseadas em expansão de base são discutidas no Capítulo 5 [^15].

Uma perspectiva alternativa e mais simplista é construir vetores alvo (targets) $t_k$ para cada classe, onde $t_k$ é a $k$-ésima coluna da matriz identidade $K \times K$ [^16]. O problema de predição torna-se tentar reproduzir o alvo apropriado para uma observação [^16]. Usando a mesma codificação indicadora, o vetor de resposta $y_i$ (i-ésima linha de $\mathbf{Y}$) para a observação $i$ tem o valor $y_i = t_k$ se $g_i = k$ [^16]. Poderíamos então ajustar o modelo linear por mínimos quadrados, minimizando a soma das distâncias Euclidianas quadradas dos vetores ajustados aos seus alvos [^17]:

$$min_{\mathbf{B}} \sum_{i=1}^N \\| y_i - [(1, x_i^T)\mathbf{B}]^T \\|^2 \quad (4.5)$$
[^17]

> Este critério é uma soma de normas quadradas, que é exatamente o critério para regressão linear com múltiplas respostas (multiple response linear regression), apenas visto de uma forma ligeiramente diferente [^18]. Como uma norma quadrada é uma soma de quadrados, os componentes se desacoplam, permitindo que o problema seja reorganizado como um modelo linear separado para cada elemento [^18].

A classificação de uma nova observação $x$ é feita calculando seu vetor ajustado $\hat{f}(x)$ e classificando para o alvo mais próximo [^19]:

$$hat{G}(x) = \underset{k}{\text{argmin}} \\; \\| \hat{f}(x) - t_k \\|^2 \quad (4.6)$$
[^19]

Esta regra de classificação pelo alvo mais próximo (4.6) é exatamente a mesma que a regra do componente máximo ajustado (4.4), mas requer que os valores ajustados somem 1 [^20].

**Limitações e Problema de Mascaramento (Masking)**

Apesar de sua simplicidade conceitual, a abordagem de regressão sobre matriz indicadora sofre de um problema sério, particularmente quando o número de classes $K$ é maior ou igual a 3, e especialmente prevalente quando $K$ é grande [^21].

> Devido à natureza rígida do modelo de regressão, algumas classes podem ser *mascaradas* (masked) por outras [^22]. Isso significa que as regiões de decisão correspondentes a certas classes podem ser completamente suprimidas, levando a erros de classificação sistemáticos para essas classes.

A Figura 4.2 ilustra uma situação extrema com $K=3$ classes em $\mathbb{R}^2$ [^23]. As três classes são perfeitamente separáveis por fronteiras de decisão lineares, contudo, a regressão linear falha completamente em identificar a classe intermediária [^23]. As observações dessa classe são sempre classificadas como pertencentes a uma das outras duas classes [^24]. A Figura 4.3 demonstra isso projetando os dados em uma única dimensão: a linha de regressão ajustada para a variável indicadora da classe do meio ($Y_2$) nunca assume o valor máximo em comparação com as outras duas linhas de regressão [^24]. Consequentemente, seu valor ajustado $\hat{f}_2(x)$ nunca é dominante, e a classe 2 nunca é predita [^24].

Para exemplos simples como o da Figura 4.3, uma regressão quadrática poderia resolver o problema [^25]. No entanto, se tivéssemos quatro classes alinhadas de forma semelhante, uma quadrática não seria suficiente, e um ajuste cúbico seria necessário [^25]. Uma regra geral, embora informal, é que se $K \ge 3$ classes estiverem alinhadas, termos polinomiais de grau até $K-1$ podem ser necessários para resolver o mascaramento [^25]. Em um espaço de entrada $p$-dimensional, isso implicaria a necessidade de termos polinomiais gerais e produtos cruzados de grau total $K-1$, resultando em $O(p^{K-1})$ termos no total para resolver cenários de pior caso [^26].

O problema de mascaramento não é apenas teórico. Ele ocorre naturalmente para $K$ grande e $p$ pequeno [^27]. Na análise dos dados de vogais (vowel data) apresentada na Tabela 4.1 (com $K=11$ classes e $p=10$ dimensões), a regressão linear obteve uma taxa de erro de teste de 67%, significativamente pior que a Análise Discriminante Linear (LDA), um método intimamente relacionado, que obteve 56% [^28]. Parece que o mascaramento prejudicou o desempenho neste caso [^28]. É importante notar que outros métodos abordados neste capítulo, embora também baseados em funções lineares de $x$, utilizam-nas de forma a evitar este problema de mascaramento [^29].

**Relação com Outros Métodos**

A relação entre a regressão de matriz indicadora e a Análise Discriminante Linear (LDA, Seção 4.3) é sutil. Para o caso de duas classes ($K=2$), existe uma correspondência: pode-se mostrar que o vetor de coeficientes da regressão por mínimos quadrados é proporcional à direção LDA [^30]. No entanto, a menos que os tamanhos das classes sejam iguais ($N_1 = N_2$), os interceptos são diferentes, levando a regras de decisão distintas [^30]. Para mais de duas classes ($K>2$), LDA *não* é o mesmo que a regressão linear da matriz indicadora, e crucialmente, LDA evita os problemas de mascaramento associados a esta última abordagem [^31]. Uma correspondência mais profunda entre regressão e LDA para $K>2$ pode ser estabelecida através da noção de *optimal scoring*, discutida na Seção 12.5 [^32].

Comparando com os classificadores de hiperplanos separadores (Seção 4.5), a solução de mínimos quadrados para $K=2$, como mostrado na Figura 4.14, encontra a *mesma* fronteira que a LDA, mas esta fronteira pode diferir das soluções encontradas por algoritmos como o Perceptron, e geralmente não corresponde ao hiperplano ótimo separador [^33, ^34].

#### Conclusão

A regressão linear de uma matriz indicadora oferece uma abordagem direta para classificação, adaptando a conhecida ferramenta da regressão linear ao problema de prever categorias. Sua formulação envolve a codificação de classes via variáveis indicadoras e o ajuste simultâneo de modelos lineares [^4, ^5]. A classificação é então baseada na maior saída ajustada [^9]. Embora justificada por perspectivas como a estimativa de esperança condicional [^10] ou ajuste a alvos [^16], e possuindo a simplicidade de implementação, esta técnica sofre de uma limitação severa: o fenômeno de **mascaramento** [^21, ^22]. Para $K \ge 3$ classes, a rigidez da regressão linear pode impedir que certas classes sejam corretamente preditas, levando a um desempenho pobre, especialmente em comparação com alternativas como LDA [^28, ^29]. Por esta razão, embora conceitualmente interessante e relacionada a outros métodos, a regressão linear direta de uma matriz indicadora é frequentemente preterida em favor de métodos como LDA ou regressão logística (Seção 4.4) em problemas de classificação com múltiplas classes.

#### Referências

[^1]: Page 103, "Here each of the response categories are coded via an indicator variable. Thus if G has K classes, there will be K such indicators Yk, k = 1, ..., K, with Yk = 1 if G = k else 0."
[^2]: Page 103, "These are collected together in a vector Y = (Y1, ..., YK), and the N training instances of these form an N × K indicator response matrix Y."
[^3]: Page 103, "Y is a matrix of 0’s and 1’s, with each row having a single 1."
[^4]: Page 103, "We fit a linear regression model to each of the columns of Y simultaneously..."
[^5]: Page 103, "...and the fit is given by Y = X(XTX)−1XTY. (4.3)"
[^6]: Page 103, "Note that we have a coefficient vector for each response column yk, and hence a (p+1) × K coefficient matrix B = (XX)−¹XTY."
[^7]: Page 103, "Here X is the model matrix with p+1 columns corresponding to the p inputs, and a leading column of 1’s for the intercept."
[^8]: Page 103, "A new observation with input x is classified as follows: • compute the fitted output f(x)T = (1, xT)B, a K vector;"
[^9]: Page 103, "• identify the largest component and classify accordingly: Ĝ(x) = argmaxkeçfk(x). (4.4)"
[^10]: Page 104, "One rather formal justification is to view the regression as an estimate of conditional expectation. For the random variable Yk, E(Yk|X = x) = Pr(G = k|X = x), so conditional expectation of each of the Yk seems a sensible goal."
[^11]: Page 104, "The real issue is: how good an approximation to conditional expectation is the rather rigid linear regression model?"
[^12]: Page 104, "It is quite straightforward to verify that Σk∈G fk(x) = 1 for any x, as long as there is an intercept in the model (column of 1’s in X)."
[^13]: Page 104, "However, the fk(x) can be negative or greater than 1, and typically some are. This is a consequence of the rigid nature of linear regression, especially if we make predictions outside the hull of the training data."
[^14]: Page 104, "These violations in themselves do not guarantee that this approach will not work, and in fact on many problems it gives similar results to more standard linear methods for classification."
[^15]: Page 104, "If we allow linear regression onto basis expansions h(X) of the inputs, this approach can lead to consistent estimates of the probabilities. As the size of the training set N grows bigger, we adaptively include more basis elements so that linear regression onto these basis functions approaches conditional expectation. We discuss such approaches in Chapter 5."
[^16]: Page 104, "A more simplistic viewpoint is to construct targets tk for each class, where tk is the kth column of the K × K identity matrix. Our prediction problem is to try and reproduce the appropriate target for an observation. With the same coding as before, the response vector yi (ith row of Y) for observation i has the value yi = tk if gi = k."
[^17]: Page 104, "We might then fit the linear model by least squares: min_B Σ_{i=1}^N ||yi - [(1,x_i^T)B]^T||^2. (4.5)"
[^18]: Page 104, "The criterion is a sum-of-squared Euclidean distances of the fitted vectors from their targets. ... The sum-of-squared-norm criterion is exactly the criterion for multiple response linear regression, just viewed slightly differently. Since a squared norm is itself a sum of squares, the components decouple and can be rearranged as a separate linear model for each element."
[^19]: Page 104, "A new observation is classified by computing its fitted vector f(x) and classifying to the closest target: Ĝ(x) = argmin_k || f(x) – tk ||^2. (4.6)"
[^20]: Page 105, "The closest target classification rule (4.6) is easily seen to be exactly the same as the maximum fitted component criterion (4.4), but does require that the fitted values sum to 1."
[^21]: Page 105, "There is a serious problem with the regression approach when the number of classes K ≥ 3, especially prevalent when K is large."
[^22]: Page 105, "Because of the rigid nature of the regression model, classes can be masked by others."
[^23]: Page 105, "Figure 4.2 illustrates an extreme situation when K = 3. The three classes are perfectly separated by linear decision boundaries, yet linear regression misses the middle class completely."
[^24]: Page 105, "In Figure 4.3 we have projected the data onto the line joining the three centroids... The three regression lines (left panel) are included, and we see that the line corresponding to the middle class is horizontal and its fitted values are never dominant! Thus, observations from class 2 are classified either as class 1 or class 3."
[^25]: Page 105, "The right panel uses quadratic regression rather than linear regression. For this simple example a quadratic rather than linear fit (for the middle class at least) would solve the problem. However, it can be seen that if there were four rather than three classes lined up like this, a quadratic would not come down fast enough, and a cubic would be needed as well. A loose but general rule is that if K ≥ 3 classes are lined up, polynomial terms up to degree K − 1 might be needed to resolve them."
[^26]: Page 106, "So in p-dimensional input space, one would need general polynomial terms and cross-products of total degree K − 1, O(p^(K−1)) terms in all, to resolve such worst-case scenarios."
[^27]: Page 106, "The example is extreme, but for large K and small p such maskings naturally occur."
[^28]: Page 106, "As a more realistic illustration, Figure 4.4 is a projection of the training data for a vowel recognition problem... There are K = 11 classes in p = 10 dimensions... The main point here is summarized in Table 4.1; linear regression has an error rate of 67%, while a close relative, linear discriminant analysis, has an error rate of 56%. It seems that masking has hurt in this case." (Table 4.1 is on page 107)
[^29]: Page 106, "While all the other methods in this chapter are based on linear functions of x as well, they use them in such a way that avoids this masking problem."
[^30]: Page 110, "With two classes there is a simple correspondence between linear discriminant analysis and classification by linear least squares, as in (4.5). ... It is easy to show that the coefficient vector from least squares is proportional to the LDA direction given in (4.11) (Exercise 4.2). ... However unless N₁ = N₂ the intercepts are different and hence the resulting decision rules are different."
[^31]: Page 110, "With more than two classes, LDA is not the same as linear regression of the class indicator matrix, and it avoids the masking problems associated with that approach (Hastie et al., 1994)."
[^32]: Page 110, "A correspondence between regression and LDA can be established through the notion of optimal scoring, discussed in Section 12.5."
[^33]: Page 129, "Figure 4.14 shows 20 data points in two classes in IR². ... The orange line is the least squares solution to the problem, obtained by regressing the −1/1 response Y on X (with intercept); the line is given by {x: βο + β₁x₁ + β₂x₂ = 0}. (4.39)"
[^34]: Page 129, "This least squares solution does not do a perfect job in separating the points, and makes one error. This is the same boundary found by LDA, in light of its equivalence with linear regression in the two-class case (Section 4.3 and Exercise 4.2)."
[^35]: Page 101, Chapter Title "4 Linear Methods for Classification".
[^36]: Page 101, "In this chapter we revisit the classification problem and focus on linear methods for classification."
[^37]: Page 101, "In Chapter 2 we fit linear regression models to the class indicator variables, and classify to the largest fit. Suppose there are K classes... and the fitted linear model for the kth indicator response variable is fk(x) = ẞko + βk^T x. The decision boundary between class k and l is... an affine set or hyperplane."
[^38]: Page 101/102, Mentions discriminant functions δk(x) and posterior probabilities Pr(G = k|X = x).
[^39]: Page 102, "We discuss two very popular but different methods that result in linear log-odds or logits: linear discriminant analysis and linear logistic regression."
[^40]: Page 102, "For example, we can expand our variable set... Linear functions in the augmented space map down to quadratic functions in the original space..."
[^41]: Page 103, "Chapter 3 has more details on linear regression."
[^42]: Page 124, "We return to these data in Chapter 5, where we see that some of the variables have nonlinear effects..."
[^43]: Page 102, "...defer treatment of the nonseparable case to Chapter 12."
[^44]: Page 112, "In Chapter 12, we discuss other regularized versions of LDA..."

<!-- END -->