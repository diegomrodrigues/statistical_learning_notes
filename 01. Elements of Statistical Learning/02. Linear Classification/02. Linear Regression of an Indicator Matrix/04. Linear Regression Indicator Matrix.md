## Regressão Linear Múltipla e o Critério da Soma das Normas Quadráticas na Classificação

### Introdução

Como introduzido na Seção 4.2, uma abordagem para classificação com $K$ classes envolve ajustar um modelo de regressão linear a uma matriz de resposta indicadora $\mathbf{Y}$ de dimensão $N \times K$ [^1]. A matriz $\mathbf{Y}$ contém zeros e uns, onde cada linha possui um único 1 indicando a classe da observação correspondente [^1]. O ajuste do modelo de regressão linear às colunas de $\mathbf{Y}$ simultaneamente é dado pela equação matricial $\mathbf{\hat{Y}} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$ (Equação 4.3) [^2], onde $\mathbf{X}$ é a matriz do modelo (incluindo uma coluna de 1s para o intercepto). Este processo resulta em uma matriz de coeficientes $\mathbf{B} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$ de dimensão $(p+1) \times K$, contendo um vetor de coeficientes para cada coluna de resposta (classe) [^3]. A classificação de uma nova observação com vetor de entrada $x$ é então realizada computando o vetor de saídas ajustadas $f(x)^T = (1, x^T)\mathbf{B}$, que é um vetor $K$, e selecionando a classe correspondente ao maior componente: $\hat{G}(x) = \text{argmax}_{k \in \mathcal{G}} f_k(x)$ (Equação 4.4) [^4].

Uma visão alternativa, descrita como mais simplista no texto base, consiste em construir vetores alvo $t_k$ para cada classe, onde $t_k$ representa a $k$-ésima coluna da matriz identidade $K \times K$ [^5]. O modelo linear é então ajustado utilizando o critério de mínimos quadrados, que busca minimizar a soma das distâncias Euclidianas quadráticas entre os vetores de resposta observados $y_i$ (que correspondem aos alvos $t_k$ para a classe $g_i=k$) e os vetores ajustados pelo modelo [^6]:

$$ \min_{\mathbf{B}} \sum_{i=1}^N \\| y_i - [(1, x_i^T)\mathbf{B}]^T \\|^2 \quad (4.5) [^6]\ $$

A regra de classificação associada a esta formulação envolve calcular o vetor ajustado $f(x)$ para uma nova observação e classificá-la para a classe cujo alvo $t_k$ está mais próximo [^7]: $\hat{G}(x) = \text{argmin}_k \\| f(x) - t_k \\|^2$ (Equação 4.6) [^7]. Este capítulo foca na análise detalhada do critério de otimização expresso na Equação (4.5), elucidando sua relação fundamental com a regressão linear de múltiplas respostas e as consequências dessa conexão.

### Conceitos Fundamentais: Decomposição do Critério de Mínimos Quadrados

O critério de otimização apresentado na Equação (4.5) é central para a abordagem de ajuste via vetores alvo. Ele quantifica o erro total como a **soma das normas quadráticas** das diferenças entre os vetores de resposta $y_i$ (os alvos $t_k$) e os vetores de predição $\hat{y}_i^T = [(1, x_i^T)\mathbf{B}]^T$ [^6]. A análise aprofundada deste critério revela uma propriedade fundamental:

> **O critério da soma das normas quadráticas (Eq. 4.5) é exatamente o critério para a regressão linear de múltiplas respostas** [^8].

A justificativa para esta afirmação reside na estrutura matemática da norma Euclidiana quadrática. Para qualquer vetor $v \in \mathbb{R}^K$, sua norma quadrática é definida como $\\|v\\|^2 = \sum_{k=1}^K v_k^2$. Aplicando esta definição ao termo dentro do somatório na Equação (4.5), obtemos:

$$ \\| y_i - \hat{y}_i \\|^2 = \left\\| \begin{pmatrix} y_{i1} \\\\ \vdots \\\\ y_{iK} \end{pmatrix} - \begin{pmatrix} \hat{y}_{i1} \\\\ \vdots \\\\ \hat{y}_{iK} \end{pmatrix} \right\\|^2 = \sum_{k=1}^K (y_{ik} - \hat{y}_{ik})^2\ $$

O critério de minimização (4.5) pode, portanto, ser reescrito como uma soma dupla:

$$ \min_{\mathbf{B}} \sum_{i=1}^N \sum_{k=1}^K (y_{ik} - \hat{y}_{ik})^2\ $$

Lembrando que o vetor de predição $\hat{y}_i^T$ é obtido por $(1, x_i^T)\mathbf{B}$, o $k$-ésimo componente $\hat{y}_{ik}$ é especificamente $\hat{y}_{ik} = (1, x_i^T)\beta_k$, onde $\beta_k$ é o $k$-ésimo vetor coluna da matriz de coeficientes $\mathbf{B}$ [^3]. Substituindo $\hat{y}_{ik}$ na soma dupla e trocando a ordem dos somatórios, temos:

$$ \min_{\mathbf{B}} \sum_{k=1}^K \left( \sum_{i=1}^N (y_{ik} - (1, x_i^T)\beta_k)^2 \right)\ $$

Esta forma reorganizada da função objetivo é crucial. Ela demonstra que a minimização da soma total dos erros quadráticos pode ser decomposta em $K$ problemas de minimização independentes. Cada termo interno $\sum_{i=1}^N (y_{ik} - (1, x_i^T)\beta_k)^2$ corresponde exatamente ao **critério de mínimos quadrados para um modelo de regressão linear padrão com uma única variável resposta**, onde a resposta é a $k$-ésima coluna da matriz indicadora $\mathbf{Y}$ (denotada como $y_k$) [^1], e o vetor de coeficientes a ser estimado é $\beta_k$.

A minimização global é alcançada minimizando cada um desses $K$ termos separadamente, pois a escolha de um vetor de coeficientes $\beta_k$ afeta apenas o $k$-ésimo termo da soma externa e não interfere nos outros termos [^8]. Essa separabilidade é uma consequência direta da estrutura aditiva da norma quadrática.

> *Note que isso só é possível porque não há nada no modelo que vincule as diferentes respostas (colunas de $\mathbf{Y}$) juntas* [^8].

Em essência, ajustar o modelo de regressão linear à matriz indicadora $\mathbf{Y}$ usando o critério da soma das normas quadráticas (4.5) é equivalente a realizar $K$ regressões lineares univariadas independentes, uma para cada variável indicadora $Y_k$, usando os mesmos preditores $\mathbf{X}$. A solução $\beta_k$ de cada regressão univariada forma a $k$-ésima coluna da matriz de coeficientes $\mathbf{B}$. O resultado final, a matriz $\mathbf{B}$ completa, é idêntico àquele obtido pela formulação matricial direta $\mathbf{B} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$ apresentada na Equação (4.3) [^3].

### Conclusão

Este capítulo demonstrou formalmente que o critério da soma das normas quadráticas, aplicado no contexto do ajuste de modelos lineares a vetores alvo para classificação [^6], é matematicamente idêntico ao critério padrão utilizado na regressão linear de múltiplas respostas [^8]. A chave para essa equivalência reside na propriedade de decomposição da norma Euclidiana quadrática, que permite que o problema de otimização global seja dividido em $K$ problemas de regressão linear independentes, um para cada variável indicadora de classe [^8]. Esta compreensão não apenas fornece uma justificativa teórica para a abordagem de mínimos quadrados com vetores alvo, mas também confirma que ela leva à mesma solução de coeficientes $\mathbf{B}$ que a formulação de regressão matricial direta vista na Seção 4.2 [^3]. A ausência de restrições que acoplem as diferentes respostas é a condição que permite essa simplificação e equivalência [^8].

### Referências

[^1]: Page 103, "Here each of the response categories are coded via an indicator variable... These are collected together in a vector Y = (Y1, ..., YK), and the N training instances of these form an N × K indicator response matrix Y. Y is a matrix of 0\'s and 1\'s, with each row having a single 1."
[^2]: Page 103, "We fit a linear regression model to each of the columns of Y simultaneously, and the fit is given by Y = X(XTX)−1XTY. (4.3)"
[^3]: Page 103, "Note that we have a coefficient vector for each response column yk, and hence a (p+1) × K coefficient matrix B = (XX)−¹XTY. Here X is the model matrix with p+1 columns..."
[^4]: Page 103, "A new observation with input x is classified as follows: • compute the fitted output f(x)T = (1, xT)B, a K vector; • identify the largest component and classify accordingly: Ĝ(x) = argmaxkeçfk(x). (4.4)"
[^5]: Page 104, "A more simplistic viewpoint is to construct targets tk for each class, where tk is the kth column of the K × K identity matrix."
[^6]: Page 104, "We might then fit the linear model by least squares: min_B Σ ||yi - [(1,xT)B]T||2. (4.5)" and "The criterion is a sum-of-squared Euclidean distances of the fitted vectors from their targets."
[^7]: Page 104, "A new observation is classified by computing its fitted vector f(x) and classifying to the closest target: Ĝ(x) = argmin_k || f(x) – tk ||2. (4.6)"
[^8]: Page 104, Bullet point: "The sum-of-squared-norm criterion is exactly the criterion for multiple response linear regression... Since a squared norm is itself a sum of squares, the components decouple and can be rearranged as a separate linear model for each element. Note that this is only possible because there is nothing in the model that binds the different responses together."

<!-- END -->