## Regra de Classificação na Regressão Linear de Matriz Indicadora

### Introdução

No presente capítulo, exploramos os métodos lineares para classificação, dando seguimento à discussão iniciada no Capítulo 2 e focando especificamente na abordagem de **Regressão Linear de uma Matriz Indicadora** introduzida na Seção 4.2 [^1]. Como vimos, esta técnica envolve codificar as $K$ categorias de resposta através de $K$ variáveis indicadoras, $Y_k$, onde $Y_k = 1$ se a observação pertence à classe $k$ e $Y_k = 0$ caso contrário [^1]. Estas variáveis são reunidas num vetor $\mathbf{Y} = (Y_1, ..., Y_K)$, e para $N$ observações de treino, formam uma matriz indicadora de resposta $\mathbf{Y}$ de dimensão $N \times K$ [^1]. Um modelo de regressão linear é então ajustado simultaneamente a cada coluna de $\mathbf{Y}$, utilizando uma matriz de modelo $\mathbf{X}$ (que inclui uma coluna de 1s para o *intercept*) [^4], resultando numa matriz de coeficientes $\mathbf{B} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$ [^3]. O objetivo deste capítulo é detalhar o procedimento de classificação para uma nova observação com vetor de entrada $x$, baseado no vetor de saídas ajustadas $\mathbf{f}(x)$, e demonstrar a sua equivalência formal a uma regra de classificação baseada na proximidade a vetores **target** específicos.

### Conceitos Fundamentais

#### Classificação pela Máxima Componente Ajustada

Dada uma nova observação com vetor de características $x$, o primeiro passo no processo de classificação, utilizando o modelo de regressão linear ajustado à matriz indicadora, é calcular o vetor de saídas ajustadas (**fitted output**) [^5]. Este vetor, $\mathbf{f}(x)$, é de dimensão $K$ (correspondente ao número de classes) e é obtido pela seguinte expressão:
$$ \mathbf{f}(x)^T = (1, x^T)\mathbf{B}\ $$
onde $\mathbf{B}$ é a matriz de coeficientes $(p+1) \times K$ estimada previamente [^3, ^5]. Cada componente $f_k(x)$ deste vetor representa o valor ajustado para a $k$-ésima variável indicadora de resposta.

A regra de classificação $\hat{G}(x)$ consiste então em atribuir a observação $x$ à classe que apresenta o maior valor ajustado [^6]. Formalmente:

> **Regra de Classificação (Máxima Componente):**
> $$ > \hat{G}(x) = \operatorname{argmax}_{k \in \mathcal{G}} f_k(x)\ > $$
> onde $\mathcal{G} = \{1, 2, ..., K\}$ é o conjunto de classes [^6].

A justificação formal para esta abordagem pode ser vista como uma tentativa de estimar a esperança condicional $E(Y_k|X=x)$, que é igual à probabilidade posterior $Pr(G=k|X=x)$ [^7]. No entanto, a natureza rígida da regressão linear implica que os valores ajustados $f_k(x)$ não estão confinados ao intervalo [0, 1], podendo ser negativos ou superiores a 1 [^8]. Apesar disso, uma propriedade interessante é que, desde que o modelo inclua um *intercept* (uma coluna de 1s em $\mathbf{X}$), a soma das componentes do vetor ajustado é igual a 1 para qualquer $x$, isto é, $\sum_{k \in \mathcal{G}} f_k(x) = 1$ [^9].

#### Perspectiva da Minimização da Distância ao Alvo

Uma visão alternativa, descrita como mais simplista no contexto original [^10], aborda o problema de predição construindo vetores **target** $\mathbf{t}_k$ para cada classe $k$. O vetor $\mathbf{t}_k$ é definido como a $k$-ésima coluna da matriz identidade $K \times K$ [^10]. Essencialmente, $\mathbf{t}_k$ é um vetor com 1 na $k$-ésima posição e 0 nas restantes.

Nesta perspectiva, o problema de predição é reformulado como uma tentativa de reproduzir o vetor **target** apropriado para cada observação [^11]. Utilizando a mesma codificação de antes, o vetor de resposta $\mathbf{y}_i$ para a $i$-ésima observação de treino é $\mathbf{y}_i = \mathbf{t}_k$ se a classe verdadeira dessa observação, $g_i$, for $k$ [^12]. O ajuste do modelo linear por *least squares* procura então minimizar a soma das distâncias Euclidianas quadradas entre os vetores ajustados e os seus respetivos **targets** [^13]:

$$ \min_{\mathbf{B}} \sum_{i=1}^N ||\mathbf{y}_i - \mathbf{f}(x_i)||^2 = \min_{\mathbf{B}} \sum_{i=1}^N ||\mathbf{y}_i - [(1, \mathbf{x}_i^T)\mathbf{B}]^T||^2\ $$
Este critério é precisamente o critério padrão para a **regressão linear com múltiplas respostas** (*multiple response linear regression*), apenas visto de uma forma ligeiramente diferente [^16]. Como a norma quadrada é uma soma de quadrados, as componentes desacoplam-se, permitindo que o problema seja tratado como modelos lineares separados para cada elemento (coluna de $\mathbf{Y}$) [^16].

A classificação de uma nova observação $x$, sob esta perspectiva, envolve calcular o seu vetor ajustado $\mathbf{f}(x)$ e, em seguida, classificá-lo para a classe $k$ cujo vetor **target** $\mathbf{t}_k$ esteja mais próximo de $\mathbf{f}(x)$ em termos de distância Euclidiana quadrada [^14].

> **Regra de Classificação (Mínima Distância ao Alvo):**
> $$ > \hat{G}(x) = \operatorname{argmin}_{k \in \mathcal{G}} || \mathbf{f}(x) - \mathbf{t}_k ||^2\ > $$
> onde $||\cdot||^2$ denota a norma Euclidiana quadrada [^14].

#### Demonstração da Equivalência

O contexto afirma explicitamente que a regra de classificação baseada na mínima distância ao **target** (equação 4.6 no original [^14]) é "exatamente a mesma" que a regra baseada na máxima componente ajustada (equação 4.4 no original [^6]) [^15]. Vamos demonstrar formalmente esta equivalência.

Queremos provar que:
$$ \operatorname{argmax}_{k \in \mathcal{G}} f_k(x) = \operatorname{argmin}_{k \in \mathcal{G}} || \mathbf{f}(x) - \mathbf{t}_k ||^2\ $$

Expandimos a expressão da distância Euclidiana quadrada:
$$ || \mathbf{f}(x) - \mathbf{t}_k ||^2 = (\mathbf{f}(x) - \mathbf{t}_k)^T (\mathbf{f}(x) - \mathbf{t}_k)\ $$
$$ = \mathbf{f}(x)^T \mathbf{f}(x) - 2 \mathbf{f}(x)^T \mathbf{t}_k + \mathbf{t}_k^T \mathbf{t}_k\ $$
$$ = ||\mathbf{f}(x)||^2 - 2 \mathbf{f}(x)^T \mathbf{t}_k + ||\mathbf{t}_k||^2\ $$
Recordemos que $\mathbf{t}_k$ é o $k$-ésimo vetor da base canónica (a $k$-ésima coluna da matriz identidade $K \times K$) [^10]. Portanto:
1.  O produto escalar $\mathbf{f}(x)^T \mathbf{t}_k$ seleciona a $k$-ésima componente do vetor $\mathbf{f}(x)$, ou seja, $\mathbf{f}(x)^T \mathbf{t}_k = f_k(x)$.
2.  A norma quadrada de $\mathbf{t}_k$ é $1$, pois é um vetor com um 1 e $K-1$ zeros: $||\mathbf{t}_k||^2 = 1$.

Substituindo estes resultados na expansão da norma quadrada:
$$ || \mathbf{f}(x) - \mathbf{t}_k ||^2 = ||\mathbf{f}(x)||^2 - 2 f_k(x) + 1\ $$
Agora, consideramos a tarefa de encontrar o $k$ que minimiza esta expressão:
$$ \operatorname{argmin}_{k \in \mathcal{G}} || \mathbf{f}(x) - \mathbf{t}_k ||^2 = \operatorname{argmin}_{k \in \mathcal{G}} \left( ||\mathbf{f}(x)||^2 - 2 f_k(x) + 1 \right)\ $$
Os termos $||\mathbf{f}(x)||^2$ e $1$ são constantes em relação a $k$. Portanto, minimizar a expressão completa é equivalente a minimizar $-2 f_k(x)$, o que, por sua vez, é equivalente a maximizar $f_k(x)$.
$$ \operatorname{argmin}_{k \in \mathcal{G}} \left( ||\mathbf{f}(x)||^2 - 2 f_k(x) + 1 \right) = \operatorname{argmax}_{k \in \mathcal{G}} f_k(x)\ $$
Isto completa a prova de que as duas regras de classificação são matematicamente equivalentes [^15]. $\blacksquare$

É importante notar que o texto original menciona que a regra do **target** mais próximo (4.6) [^14] requer que os valores ajustados somem 1 [^17]. Contudo, a derivação matemática acima demonstra que a equivalência entre *maximizar a componente ajustada* e *minimizar a distância ao target* é válida independentemente dessa condição. A condição $\sum_k f_k(x) = 1$ [^9], que se verifica quando há um *intercept*, é uma propriedade do ajuste por *least squares* à matriz indicadora, mas não um pré-requisito para a equivalência das *regras de decisão* em si. A equivalência decorre diretamente da álgebra da minimização da distância Euclidiana.

### Conclusão

Demonstramos que a classificação de uma nova observação $x$ no contexto da regressão linear de uma matriz indicadora pode ser vista sob duas perspectivas formalmente equivalentes. A primeira, mais direta, consiste em calcular o vetor de saídas ajustadas $\mathbf{f}(x)$ e selecionar a classe correspondente à maior componente $f_k(x)$ [^6]. A segunda perspectiva envolve definir vetores **target** $\mathbf{t}_k$ para cada classe e selecionar a classe cujo **target** está mais próximo (em distância Euclidiana quadrada) do vetor ajustado $\mathbf{f}(x)$ [^14]. Ambas as regras de decisão emergem naturalmente do processo de ajuste do modelo linear por *least squares* aos dados de treino codificados pela matriz indicadora $\mathbf{Y}$ [^13, ^16]. Embora esta abordagem seja conceitualmente simples e ligada à regressão linear padrão, é importante estar ciente das suas limitações, como o problema de *masking* para $K \ge 3$ classes [^18], que são discutidas noutras secções deste livro.

### Referências

[^1]: Contexto, Página 103, Parágrafo 1 sob Seção 4.2
[^2]: Contexto, Página 103, Equação (4.3)
[^3]: Contexto, Página 103, Parágrafo 2 sob Seção 4.2
[^4]: Contexto, Página 103, Parágrafo 2 sob Seção 4.2
[^5]: Contexto, Página 103, Primeiro bullet point sob Seção 4.2
[^6]: Contexto, Página 103, Segundo bullet point e Equação (4.4)
[^7]: Contexto, Página 104, Parágrafo 1
[^8]: Contexto, Página 104, Parágrafo 2
[^9]: Contexto, Página 104, Parágrafo 2
[^10]: Contexto, Página 104, Parágrafo 3
[^11]: Contexto, Página 104, Parágrafo 3
[^12]: Contexto, Página 104, Parágrafo 3
[^13]: Contexto, Página 104, Equação (4.5)
[^14]: Contexto, Página 104, Equação (4.6)
[^15]: Contexto, Página 104, Parágrafo após Equação (4.6)
[^16]: Contexto, Página 104, Primeiro bullet point após Equação (4.6)
[^17]: Contexto, Página 105, Segundo bullet point (continuação da p. 104)
[^18]: Contexto, Página 105, Parágrafo 1

<!-- END -->