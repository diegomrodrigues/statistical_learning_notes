## Smoothing Splines em Regressão Logística Não Paramétrica

### Introdução
Este capítulo explora a aplicação de **smoothing splines** no contexto de **regressão logística não paramétrica**. Em continuidade com os métodos de expansão de base e regularização discutidos anteriormente [^5], focaremos em como as smoothing splines podem ser utilizadas para estimar a função de regressão logística de forma flexível e regularizada, evitando a necessidade de seleção de nós de maneira explícita. Este método, como veremos, leva a uma solução que é uma spline natural de dimensão finita com nós nos valores únicos de *x*.

### Conceitos Fundamentais
Na regressão logística, o objetivo é modelar a probabilidade condicional de uma variável binária *Y* dado um preditor *X*. Em um contexto não paramétrico, busca-se uma função $f(x)$ que relacione *X* com a probabilidade através da transformação logística [^5]:

$$ \log \frac{Pr(Y=1|X=x)}{Pr(Y=0|X=x)} = f(x). $$

Isso implica que a probabilidade condicional é dada por:

$$ Pr(Y=1|X=x) = \frac{e^{f(x)}}{1+e^{f(x)}}. $$

Para estimar $f(x)$, podemos construir um critério de log-verossimilhança penalizado [^5]:

$$ l(f;\lambda) = \sum_{i=1}^{N} [y_i \log p(x_i) + (1-y_i) \log(1-p(x_i))] - \frac{\lambda}{2} \int \{f''(t)\}^2 dt, $$

onde $p(x_i) = Pr(Y=1|X=x_i)$ e $\lambda$ é o parâmetro de suavização que controla o *trade-off* entre ajuste aos dados e a penalização da curvatura da função.

Argumentos similares aos utilizados na Seção 5.4 [^5] demonstram que a função ótima $f$ que minimiza o critério acima é uma **spline natural de dimensão finita com nós nos valores únicos de *x***. Isso significa que a solução é uma função *piecewise* polinomial de grau três, com nós em cada ponto de dado $x_i$, e que é linear além dos nós extremos.
Este resultado crucial simplifica enormemente o problema de otimização, pois transforma um problema de busca em um espaço de funções infinito-dimensional para um problema de otimização em um espaço de dimensão finita.

Podemos expressar $f(x)$ como uma combinação linear de funções de base $N_j(x)$ [^5]:

$$ f(x) = \sum_{j=1}^{N} N_j(x) \theta_j, $$

onde $N_j(x)$ são as funções de base da spline natural e $\theta_j$ são os coeficientes a serem estimados.

Substituindo esta representação na função de verossimilhança penalizada, obtemos um problema de otimização em termos dos coeficientes $\theta_j$. As derivadas primeira e segunda da função de log-verossimilhança penalizada são dadas por [^5]:

$$ \frac{\partial l(\theta)}{\partial \theta} = N^T(y-p) - \lambda \Omega \theta, $$

$$ \frac{\partial^2 l(\theta)}{\partial \theta \partial \theta^T} = -N^T W N - \lambda \Omega, $$

onde $p$ é o vetor de probabilidades previstas, $W$ é uma matriz diagonal de pesos $p(x_i)(1-p(x_i))$, e $\Omega$ é uma matriz de penalização que quantifica a suavidade da spline.

A solução pode ser encontrada iterativamente usando métodos como o Newton-Raphson [^5]:

$$ \theta^{new} = (N^T W N + \lambda \Omega)^{-1} N^T W (N \theta^{old} + W^{-1}(y-p)). $$

Essa equação pode ser reescrita em termos dos valores ajustados como [^5]:
$$ f^{new} = N(N^T W N + \lambda \Omega)^{-1} N^T W (f^{old} + W^{-1}(y-p)) = S_{\lambda,W} z, $$
onde $S_{\lambda,W}$ é a matriz *smoother* e $z$ é uma resposta de trabalho.

### Conclusão
O uso de smoothing splines em regressão logística não paramétrica oferece uma abordagem flexível e regularizada para modelar relações não lineares entre preditores e probabilidades. A chave desse método é o resultado de que a solução ótima é uma spline natural de dimensão finita, o que simplifica o problema de otimização. A escolha do parâmetro de suavização $\lambda$ é crucial para o desempenho do modelo, e técnicas como validação cruzada podem ser usadas para selecionar um valor apropriado. A conexão com os tópicos anteriores fica evidente no uso de expansões de base e regularização, e a aplicação a um problema específico, como a regressão logística, demonstra a versatilidade desses métodos.

### Referências
[^5]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer science & business media.
<!-- END -->