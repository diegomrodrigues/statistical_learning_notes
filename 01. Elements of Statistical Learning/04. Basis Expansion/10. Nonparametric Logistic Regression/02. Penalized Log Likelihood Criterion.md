## Penalized Log-Likelihood Criterion in Nonparametric Logistic Regression

### Introdução
Este capítulo explora o critério da **penalized log-likelihood** no contexto da **regressão logística não paramétrica**. A regressão logística não paramétrica, conforme apresentado no contexto geral de **Basis Expansions and Regularization** [^5], busca modelar a probabilidade condicional de uma variável categórica (Y) dado um input X, sem assumir uma forma funcional específica para a relação entre X e Y. O critério da penalized log-likelihood, como veremos, introduz um termo de penalidade ao log-likelihood da distribuição binomial, visando controlar a complexidade do modelo e evitar overfitting [^1].

### Conceitos Fundamentais
Em regressão logística paramétrica, o modelo é definido como:
$$\
\log\left(\frac{Pr(Y=1|X=x)}{Pr(Y=0|X=x)}\right) = f(x),
$$
onde $f(x)$ é uma função linear dos inputs [^1]. Na regressão logística não paramétrica, $f(x)$ é uma função mais geral, que não é restrita a uma forma linear [^1]. O objetivo é estimar essa função $f(x)$ de forma flexível, utilizando dados de treinamento [^1].

O critério da **penalized log-likelihood** é construído adicionando um termo de penalidade ao log-likelihood baseado na distribuição binomial [^1]. Especificamente, o critério é definido como:
$$\
l(f; \lambda) = \sum_{i=1}^{N} \left[y_i \log p(x_i) + (1 - y_i) \log (1 - p(x_i))\right] - \lambda \int \{f''(t)\}^2 dt,
$$
onde:
*   $y_i$ é o valor da variável resposta para a i-ésima observação [^1].
*   $p(x_i) = Pr(Y=1|X=x_i)$ é a probabilidade de $Y=1$ dado $X=x_i$, modelada como $p(x) = \frac{e^{f(x)}}{1+e^{f(x)}}$ [^1, 23].
*   $\lambda$ é o parâmetro de regularização que controla a força da penalidade [^1].
*   $\int \{f''(t)\}^2 dt$ é o termo de penalidade, que penaliza a curvatura excessiva da função $f(x)$ [^1].

O primeiro termo na equação acima é o log-likelihood da distribuição binomial, que mede o quão bem o modelo se ajusta aos dados [^23]. O segundo termo é a penalidade, que mede a complexidade do modelo [^1]. O parâmetro $\lambda$ controla o trade-off entre o ajuste aos dados e a complexidade do modelo [^1]. Um valor grande de $\lambda$ impõe uma penalidade forte, resultando em uma função $f(x)$ mais suave, enquanto um valor pequeno de $\lambda$ permite que o modelo se ajuste mais aos dados, potencialmente levando ao overfitting [^1].

Como mencionado na seção 5.4, argumentos similares mostram que a função $f$ ótima é uma spline natural de dimensão finita com nós nos valores únicos de $x$ [^1, 23]. Isso significa que podemos representar $f(x)$ como:
$$\
f(x) = \sum_{j=1}^{N} N_j(x) \theta_j,
$$
onde $N_j(x)$ são funções base spline naturais e $\theta_j$ são os coeficientes [^1, 23].

Para encontrar a função $f(x)$ que minimiza o critério da penalized log-likelihood, precisamos maximizar $l(f; \lambda)$ em relação aos coeficientes $\theta_j$ [^1]. Isso pode ser feito utilizando algoritmos iterativos, como o Newton-Raphson [^1]. As derivadas primeira e segunda de $l(f; \lambda)$ em relação a $\theta$ são dadas por:
$$\
\frac{\partial l(\theta)}{\partial \theta} = N^T (y - p) - \lambda \Omega \theta,
$$
$$\
\frac{\partial^2 l(\theta)}{\partial \theta \partial \theta^T} = -N^T W N - \lambda \Omega,
$$
onde $p$ é o vetor com elementos $p(x_i)$, $W$ é uma matriz diagonal com pesos $p(x_i)(1-p(x_i))$ e $\Omega$ é uma matriz de penalidade [^1]. Utilizando o método de Newton-Raphson, a equação de atualização pode ser escrita como:

$$\
\theta^{new} = (N^T W N + \lambda \Omega)^{-1} N^T W (N \theta^{old} + W^{-1} (y - p)).
$$

Esta equação pode ser expressa em termos dos valores ajustados como:
$$\
f^{new} = N(N^T W N + \lambda \Omega)^{-1} N^T W (f^{old} + W^{-1} (y - p)) = S_{\lambda, W} z,
$$
onde $S_{\lambda, W}$ é o operador smoothing e $z$ é a working response [^1, 23].

### Conclusão
O critério da penalized log-likelihood oferece uma abordagem flexível para modelar a relação entre um input e uma variável resposta categórica em regressão logística não paramétrica [^1]. Ao adicionar um termo de penalidade ao log-likelihood, podemos controlar a complexidade do modelo e evitar o overfitting [^1]. A escolha do parâmetro de regularização $\lambda$ é crucial para obter um bom desempenho [^1]. Métodos como cross-validation podem ser utilizados para selecionar um valor apropriado para $\lambda$ [^1]. A formulação em termos de splines naturais permite uma implementação eficiente utilizando algoritmos iterativos [^1, 23].

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer series in statistics. New York: Springer.
<!-- END -->