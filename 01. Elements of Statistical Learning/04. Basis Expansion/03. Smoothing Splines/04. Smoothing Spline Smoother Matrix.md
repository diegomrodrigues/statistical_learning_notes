## Matrizes de Suavização e Graus de Liberdade Efetivos em Smoothing Splines

### Introdução
Este capítulo aprofunda a análise de **smoothing splines**, explorando o conceito de **matriz de suavização** e sua relação com os **graus de liberdade efetivos**. Como vimos anteriormente [^52], smoothing splines oferecem uma abordagem para ajustar dados que equilibra a proximidade aos dados observados com a suavidade da função resultante. A matriz de suavização desempenha um papel central nessa abordagem, fornecendo uma representação linear da operação de suavização.

### Conceitos Fundamentais
Um **smoothing spline** com um parâmetro de suavização $\lambda$ pré-selecionado é um **suavizador linear** [^1]. Isso significa que os valores ajustados são uma combinação linear dos valores observados $y_i$. O operador linear finito $S_\lambda$ é conhecido como a **matriz de suavização** [^1]. Essa matriz relaciona os valores ajustados $\hat{f}$ aos valores observados $y$ e depende apenas dos $x_i$ e de $\lambda$ [^1]. Podemos expressar essa relação como:

$$ \hat{f} = S_\lambda y $$

A matriz de suavização $S_\lambda$ possui as seguintes propriedades importantes [^1]:

*   É **simétrica**.
*   É **positiva semidefinida**.
*   Possui uma **decomposição em autovalores reais**, permitindo uma parametrização mais intuitiva do smoothing spline.

A dimensão do espaço de projeção, dada por $M = trace(H_\xi)$, onde $H_\xi$ é o *hat matrix* em regressão linear [^15], nos fornece o número de funções de base e, portanto, o número de parâmetros envolvidos no ajuste. Por analogia, definimos os **graus de liberdade efetivos** de um smoothing spline como [^1]:

$$ df_\lambda = trace(S_\lambda) $$

Essa definição nos permite quantificar o grau de suavização imposto pelo smoothing spline. Um valor menor de $df_\lambda$ indica uma maior suavização, enquanto um valor maior indica um ajuste mais próximo aos dados observados.

A representação de **Reinsch** da matriz de suavização é dada por [^16]:

$$ S_\lambda = (I + \lambda K)^{-1} $$

onde $K$ é a **matriz de penalidade** que não depende de $\lambda$ [^16]. A solução $\hat{f} = S_\lambda y$ minimiza [^16]:

$$ \min_{f} (y - f)^T(y - f) + \lambda f^T K f $$

A **decomposição em autovalores** de $S_\lambda$ é dada por [^16]:

$$ S_\lambda = \sum_{k=1}^{N} \rho_k(\lambda) u_k u_k^T $$

onde $u_k$ são os autovetores e $\rho_k(\lambda) = \frac{1}{1 + \lambda d_k}$, sendo $d_k$ os autovalores correspondentes de $K$ [^16].

As propriedades da eigenrepresentação são [^16]:

* Os autovetores não são afetados por mudanças em $\lambda$, e portanto, toda a família de splines de suavização (para uma sequência $x$ particular) indexada por $\lambda$ tem os mesmos autovetores.
* A operação de suavização é feita decompondo $y$ com relação à base completa $\{u_k\}$ e encolhendo diferencialmente as contribuições usando $\rho_k(\lambda)$.

### Conclusão

A matriz de suavização $S_\lambda$ e os graus de liberdade efetivos $df_\lambda$ fornecem ferramentas valiosas para entender e controlar o comportamento dos smoothing splines. A decomposição em autovalores da matriz de suavização revela como o smoothing spline opera, encolhendo diferencialmente as contribuições de diferentes componentes da função. A capacidade de pré-selecionar $\lambda$ com base em um valor desejado de $df_\lambda$ oferece uma maneira intuitiva de controlar o grau de suavização imposto pelo modelo.

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^15]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^16]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
<!-- END -->