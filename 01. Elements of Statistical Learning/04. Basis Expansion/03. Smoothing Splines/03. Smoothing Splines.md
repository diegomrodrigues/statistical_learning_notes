## Solução para Smoothing Splines como Splines Cúbicos Naturais

### Introdução
Este capítulo explora a solução para o problema de **smoothing splines**, detalhando como essa solução se manifesta como um *spline cúbico natural* com nós nos valores únicos das variáveis de entrada [^1]. Além disso, a *penalidade imposta* no processo de suavização se traduz em uma penalidade nos coeficientes do spline, que são ajustados em direção a um ajuste linear [^1].

### Conceitos Fundamentais

O critério para **smoothing splines** é definido em um espaço de função de dimensão infinita. Surpreendentemente, este critério possui um minimizador único, explícito e de dimensão finita, que é um **spline cúbico natural** com nós nos valores únicos de $x_i$, $i = 1, ..., N$ [^1].

A solução para o problema de **smoothing spline** pode ser expressa como [^13]:

$$
f(x) = \sum_{j=1}^{N} N_j(x) \theta_j,
$$

onde $N_j(x)$ são um conjunto de funções de base N-dimensionais para representar esta família de splines naturais (Seção 5.2.1 e Exercício 5.4) [^13]. O critério, portanto, se reduz a:

$$
RSS(\theta, \lambda) = (y - N\theta)^T (y - N\theta) + \lambda \theta^T \Omega_N \theta,
$$

onde ${\{N\}}_{ij} = N_j(x_i)$ e ${\{\Omega_N\}}_{jk} = \int N''_j(t) N''_k(t) dt$ [^13]. A solução é vista facilmente como:

$$
\hat{\theta} = (N^T N + \lambda \Omega_N)^{-1} N^T y,
$$

uma regressão ridge generalizada [^13]. O **smoothing spline** ajustado é dado por:

$$
\hat{f}(x) = \sum_{j=1}^{N} N_j(x) \hat{\theta}_j.
$$

A penalidade imposta nos coeficientes do spline os força a se aproximarem de um ajuste linear [^1]. O parâmetro $\lambda$ controla o *trade-off* entre a proximidade dos dados e a suavidade da função [^13].

**Em relação aos graus de liberdade**: Um **smoothing spline** com $K$ nós é representado por $K$ funções de base [^7]. Pode-se começar com uma base para splines cúbicos e derivar a base reduzida impondo as restrições de contorno [^7].

**Matrizes de Suavização**: Um **smoothing spline** com $\lambda$ pré-selecionado é um exemplo de um *linear smoother* (como em um operador linear) [^15]. Isso porque os parâmetros estimados são uma combinação linear dos $y_i$. Denotando por $f$ o vetor $N$ de valores ajustados $f(x_i)$ nos preditores de treinamento $x_i$, então:

$$
\hat{f} = N(N^TN + \lambda \Omega_N)^{-1}N^Ty = S_{\lambda}y.
$$

Novamente, o ajuste é linear em $y$, e o operador linear finito $S_{\lambda}$ é conhecido como a *matriz smoother* [^15]. Uma consequência dessa linearidade é que a receita para produzir $f$ a partir de $y$ não depende de $y$ em si; $S_{\lambda}$ depende apenas de $x_i$ e $\lambda$ [^15].

Como $S_{\lambda}$ é simétrico (e semidefinido positivo), ele tem uma autodecomposição real [^16]. Antes de prosseguir, é conveniente reescrever $S_{\lambda}$ na forma de Reinsch [^16]:

$$
S_{\lambda} = (I + \lambda K)^{-1},
$$

onde $K$ não depende de $\lambda$ (Exercício 5.9) [^16]. Como $f = S_{\lambda}y$ resolve:

$$
\min_f (y - f)^T(y-f) + \lambda f^TKf,
$$

$K$ é conhecida como a *matriz de penalidade*, e de fato uma forma quadrática em $K$ tem uma representação em termos de uma soma ponderada de segundas diferenças quadráticas (divididas) [^16]. A autodecomposição de $S_{\lambda}$ é [^16]:

$$
S_{\lambda} = \sum_{k=1}^N \rho_k(\lambda) u_k u_k^T,
$$

com:

$$
\rho_k(\lambda) = \frac{1}{1 + \lambda d_k},
$$

e $d_k$ o autovalor correspondente de $K$ [^16].

### Conclusão

A solução de **smoothing spline** oferece uma abordagem elegante para ajustar funções suaves aos dados, equilibrando a proximidade dos dados com a suavidade da função através do parâmetro de regularização $\lambda$ [^1]. A representação como um **spline cúbico natural** com nós nos valores de entrada únicos, juntamente com a interpretação da matriz de suavização, fornece insights valiosos sobre o comportamento e as propriedades dessa técnica [^1].

### Referências
[^1]:  Contexto fornecido.
[^7]: Page 145, 5.2.1 Natural Cubic Splines
[^13]: Page 152, 5.4 Smoothing Splines
[^15]: Page 153, 5.4.1 Degrees of Freedom and Smoother Matrices
[^16]: Page 154, 5. Basis Expansions and Regularization
<!-- END -->