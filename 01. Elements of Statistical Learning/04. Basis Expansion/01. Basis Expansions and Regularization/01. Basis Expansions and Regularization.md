## Estendendo Modelos Lineares com Expansões de Base

### Introdução
Modelos lineares, como regressão linear, análise discriminante linear e regressão logística, são ferramentas fundamentais em *machine learning* devido à sua interpretabilidade e eficiência computacional [^1]. No entanto, sua capacidade de modelar relações complexas é limitada quando a verdadeira função subjacente é não linear e não aditiva [^1]. Para superar essa limitação, as **expansões de base** oferecem uma abordagem para estender esses modelos, permitindo capturar relacionamentos mais intrincados através da transformação das *features* de entrada [^1]. Este capítulo explora os conceitos fundamentais das expansões de base, seus tipos e aplicações, bem como métodos de regularização para controlar a complexidade do modelo.

### Conceitos Fundamentais
A ideia central das expansões de base é **aumentar ou substituir o vetor de *inputs* X por variáveis adicionais que são transformações de X** [^1]. Em seguida, aplicam-se modelos lineares nesse novo espaço de *features* derivadas [^1]. Formalmente, denotamos por $h_m(X): \mathbb{R}^p \rightarrow \mathbb{R}$ a *m*-ésima transformação de X, onde $m = 1, ..., M$ [^1]. O modelo resultante é expresso como uma combinação linear dessas transformações:

$$
f(X) = \sum_{m=1}^{M} \beta_m h_m(X),
$$

onde $\beta_m$ são os coeficientes do modelo [^1]. Essa formulação representa uma **expansão linear em relação às funções de base $h_m(X)$** [^1].

#### Exemplos de Funções de Base
Existem diversas opções para as funções de base $h_m$, cada uma com suas características e aplicações específicas [^2]:

*   **Funções Lineares:** $h_m(X) = X_m$, com $m = 1, ..., p$. Essa escolha recupera o modelo linear original [^2].
*   **Termos Polinomiais:** $h_m(X) = X_j^2$ ou $h_m(X) = X_j X_k$. Permitem aumentar os *inputs* com termos polinomiais para obter expansões de Taylor de ordem superior [^2]. No entanto, o número de variáveis cresce exponencialmente com o grau do polinômio, exigindo $O(p^d)$ termos para um polinômio de grau *d* [^2].
*   **Transformações Não Lineares de Inputs Individuais:** $h_m(X) = \log(X_j)$, $\sqrt{X_j}$, entre outras. Permitem capturar relações não lineares em *features* individuais [^2]. Funções envolvendo múltiplos *inputs*, como $h_m(X) = ||X||$, também são possíveis [^2].
*   **Funções Indicadoras:** $h_m(X) = I(L_m \le X_k < U_m)$, onde $I(\cdot)$ é a função indicadora. Dividem o intervalo de $X_k$ em $M_k$ regiões não sobrepostas, resultando em um modelo com contribuições constantes por partes para $X_k$ [^2].

#### Splines
**Splines** representam uma família útil de funções *piecewise-polynomials* que permitem representações polinomiais locais [^2]. Em vez de ajustar um único polinômio a todo o conjunto de dados, as splines dividem o domínio da variável preditora em intervalos contíguos e ajustam polinômios separados em cada intervalo [^3]. As junções entre esses polinômios, conhecidas como **knots**, são pontos específicos onde a função spline é forçada a se conectar suavemente [^3].

Uma função *piecewise polynomial* $f(X)$ é obtida dividindo o domínio de $X$ em intervalos contíguos e representando $f$ por um polinômio separado em cada intervalo [^3]. A **spline cúbica natural** adiciona restrições adicionais, ou seja, a função é linear além dos *boundary knots* [^7]. Isso libera quatro graus de liberdade (duas restrições em cada região de contorno), que podem ser gastos de forma mais lucrativa, espalhando mais *knots* na região interior [^7].

#### Restrições, Seleção e Regularização
Ao utilizar expansões de base, é crucial controlar a complexidade do modelo para evitar o *overfitting*, especialmente quando o número de funções de base $|D|$ é muito grande em relação ao número de dados [^2]. Existem três abordagens comuns para lidar com essa complexidade [^2]:

1.  **Métodos de Restrição:** Limitam a classe de funções *a priori*. Um exemplo é a **aditividade**, onde o modelo é restrito a uma soma de funções de uma única variável:

    $$
    f(X) = \sum_{j=1}^{p} f_j(X_j) = \sum_{j=1}^{p} \sum_{m=1}^{M_j} \beta_{jm} h_{jm}(X_j).
    $$

    Nesse caso, o tamanho do modelo é limitado pelo número de funções de base $M_j$ usadas para cada componente $f_j$ [^2].
2.  **Métodos de Seleção:** Escaneiam adaptativamente o dicionário de funções de base e incluem apenas aquelas que contribuem significativamente para o ajuste do modelo [^2]. Técnicas de seleção de variáveis e abordagens *stagewise greedy*, como CART, MARS e *boosting*, se enquadram nessa categoria [^2].
3.  **Métodos de Regularização:** Usam todo o dicionário de funções de base, mas restringem os coeficientes [^2]. A regressão de *ridge* é um exemplo simples de regularização, enquanto o *lasso* combina regularização e seleção [^2].

### Conclusão
As expansões de base representam uma ferramenta poderosa para estender modelos lineares e capturar relações complexas nos dados. Ao transformar as *features* de entrada, esses modelos podem se adaptar a funções não lineares e não aditivas, mantendo a interpretabilidade e a eficiência computacional dos modelos lineares. A escolha das funções de base e a aplicação de métodos de regularização são cruciais para controlar a complexidade do modelo e evitar o *overfitting*, garantindo um bom desempenho preditivo e uma generalização adequada para novos dados.

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. New York: Springer. p.139
[^2]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. New York: Springer. p.140
[^3]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. New York: Springer. p.141
[^7]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. New York: Springer. p.145
<!-- END -->