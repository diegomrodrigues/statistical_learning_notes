## Effective Degrees of Freedom in Smoothing Splines

### Introdução
Este capítulo explora em profundidade o conceito de **smoothing splines** como uma técnica de regularização e expansão de base [^1]. Em particular, focaremos na parametrização intuitiva de smoothing splines através dos **effective degrees of freedom** ($df_\lambda$) [^1], que permitem uma comparação consistente entre diferentes smoothers. O conceito de **effective degrees of freedom** surge como uma alternativa mais intuitiva para controlar a complexidade do modelo em comparação com o parâmetro de suavização $\lambda$ [^1]. A relação entre esses conceitos será explorada em detalhe.

### Conceitos Fundamentais

#### Smoothing Splines e Regularização
Como vimos anteriormente [^1], modelos lineares são frequentemente utilizados como aproximações convenientes para funções não lineares. No entanto, para capturar relações mais complexas nos dados, é necessário ir além da linearidade [^1]. Uma abordagem comum é expandir o espaço de entrada através da inclusão de transformações não lineares das variáveis originais [^1].

O **smoothing spline** é um método que evita o problema da seleção de *knots* ao utilizar um conjunto maximal de *knots*. A complexidade do ajuste é controlada através da regularização. Considere o seguinte problema: dentre todas as funções $f(x)$ com duas derivadas contínuas, encontre aquela que minimiza a soma de quadrados residual penalizada [^13]:

$$\
RSS(f, \lambda) = \sum_{i=1}^N \\{y_i - f(x_i)\\}^2 + \lambda \int \\{f''(t)\\}^2 dt,
$$

onde $\lambda$ é um parâmetro de suavização fixo. O primeiro termo mede a proximidade aos dados, enquanto o segundo termo penaliza a curvatura na função. $\lambda$ estabelece um *trade-off* entre os dois [^13].

#### Linearidade do Smoother e a Matriz de Suavização

O **smoothing spline** com $\lambda$ pré-escolhido é um exemplo de um *linear smoother* (ou operador linear) [^15]. Isso ocorre porque os parâmetros estimados em (5.12) [^14] são uma combinação linear dos $y_i$ [^15]. Denotemos por $\hat{f}$ o vetor $N$ de valores ajustados $f(x_i)$ nos preditores de treinamento $x_i$ [^15]. Então:

$$\
\hat{f} = N(N^TN + \lambda \Omega_N)^{-1}N^T y = S_\lambda y,
$$

onde $S_\lambda$ é conhecida como a **matriz de suavização** [^15]. A receita para produzir $\hat{f}$ a partir de $y$ não depende de $y$ em si; $S_\lambda$ depende apenas de $x_i$ e $\lambda$ [^15].

#### Degrees of Freedom Efetivos
A expressão $M = trace(H_\xi)$ fornece a dimensão do espaço de projeção, que é também o número de funções de base [^15]. Por analogia, definimos os **effective degrees of freedom** de um **smoothing spline** como [^15]:

$$\
df_\lambda = trace(S_\lambda),
$$

que é a soma dos elementos diagonais de $S_\lambda$ [^16]. Esta definição permite uma forma mais intuitiva de parametrizar o **smoothing spline**, e de fato muitos outros *smoothers* também [^16]. Por exemplo, na Figura 5.6 [^14] especificamos $df_\lambda = 12$ para cada uma das curvas, e o $\lambda \approx 0.00022$ correspondente foi derivado numericamente resolvendo $trace(S_\lambda) = 12$ [^16].

#### Decomposição em Autovalores e a Forma de Reinsch
Como $S_\lambda$ é simétrica (e positiva semidefinida), ela possui uma autodecomposição real [^16]. Antes de prosseguirmos, é conveniente reescrever $S_\lambda$ na forma de Reinsch [^16]:

$$\
S_\lambda = (I + \lambda K)^{-1},
$$

onde $K$ não depende de $\lambda$ [^16]. $K$ é conhecida como a **matriz de penalidade**, e de fato uma forma quadrática em $K$ tem uma representação em termos de uma soma ponderada de segundas diferenças ao quadrado (divididas) [^16]. A autodecomposição de $S_\lambda$ é [^16]:

$$\
S_\lambda = \sum_{k=1}^N \rho_k(\lambda) u_k u_k^T,
$$

com

$$\
\rho_k(\lambda) = \frac{1}{1 + \lambda d_k},
$$

onde $d_k$ é o autovalor correspondente de $K$ [^16]. A Figura 5.7 (topo) [^17] mostra os resultados da aplicação de um **cubic smoothing spline** a alguns dados de poluição do ar (128 observações). Dois ajustes são dados: um ajuste mais suave correspondente a uma penalidade maior $\lambda$ e um ajuste mais bruto para uma penalidade menor. Os painéis inferiores representam os autovalores (inferior esquerdo) e alguns autovetores (inferior direito) das matrizes de suavização correspondentes [^16].

As principais características da auto-representação são [^16]:

*   Os autovetores não são afetados por mudanças em $\lambda$ e, portanto, toda a família de **smoothing splines** (para uma sequência particular $x$) indexada por $\lambda$ tem os mesmos autovetores [^16].
*   $S_\lambda y = \sum_{k=1}^N u_k \rho_k(\lambda) \langle u_k, y \rangle$ e, portanto, o operador de **smoothing spline** opera decompondo $y$ em relação à base (completa) $\\{u_k\\}$, e encolhendo diferencialmente as contribuições usando $\rho_k(\lambda)$ [^16]. Isso deve ser contrastado com um método de regressão de base, onde os componentes são deixados sozinhos ou encolhidos para zero [^16].
*   A sequência de $u_k$, ordenada por $\rho_k(\lambda)$ decrescente, parece aumentar em complexidade [^16]. De fato, eles têm o comportamento de cruzamento zero de polinômios de grau crescente [^16].

### Conclusão
A utilização de **effective degrees of freedom** ($df_\lambda$) oferece uma maneira mais intuitiva e controlável de parametrizar **smoothing splines**, permitindo a comparação consistente entre diferentes métodos de suavização [^16]. Através da análise da matriz de suavização e sua decomposição em autovalores, compreendemos como o **smoothing spline** opera no espaço de funções, penalizando a complexidade e promovendo a suavidade [^16].

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^13]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^14]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^15]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^16]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^17]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
<!-- END -->