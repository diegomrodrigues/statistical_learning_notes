## Limitações dos Modelos Lineares e Expansão de Base

### Introdução
Modelos lineares, como regressão linear, análise discriminante linear e regressão logística, são ferramentas fundamentais em tarefas de regressão e classificação. No entanto, a suposição de linearidade inerente a esses modelos pode ser uma simplificação excessiva da verdadeira função subjacente $f(X)$ [^1]. Este capítulo explora métodos para superar essas limitações, focando na expansão de base, uma técnica que enriquece o espaço de entrada ao incluir transformações das variáveis originais [^1].

### Conceitos Fundamentais

**Linearidade e suas Limitações:** Modelos lineares pressupõem uma relação linear entre as variáveis de entrada $X$ e a saída $f(X)$. Embora convenientes e fáceis de interpretar, essa suposição raramente se mantém na prática [^1]. Em problemas de regressão, a função $f(X) = E(Y|X)$ geralmente é não linear e não aditiva em $X$ [^1]. Em classificação, uma fronteira de decisão linear Bayes-ótima implica que alguma transformação monotônica de $Pr(Y = 1|X)$ é linear em $X$, o que também é uma aproximação [^1].

**Expansão de Base:** A ideia central da expansão de base é aumentar ou substituir o vetor de entrada $X$ por variáveis adicionais, que são transformações de $X$ [^1]. Essas transformações, denotadas por $h_m(X): \mathbb{R}^p \rightarrow \mathbb{R}$, criam um novo espaço de características derivadas, no qual modelos lineares podem ser aplicados [^1]. O modelo resultante tem a forma:
$$ f(X) = \sum_{m=1}^{M} \beta_m h_m(X), $$
onde $\beta_m$ são os coeficientes a serem estimados e $M$ é o número de transformações [^1]. A beleza desta abordagem reside na sua simplicidade: uma vez determinadas as funções de base $h_m$, o modelo é linear nessas novas variáveis, e o ajuste prossegue como antes [^2].

**Exemplos de Funções de Base:**
1.  **Funções Lineares:** $h_m(X) = X_m$, com $m = 1, ..., p$, recupera o modelo linear original [^2].
2.  **Termos Polinomiais:** $h_m(X) = X_j^2$ ou $h_m(X) = X_jX_k$ permite aumentar as entradas com termos polinomiais para obter expansões de Taylor de ordem superior [^2]. No entanto, o número de variáveis cresce exponencialmente com o grau do polinômio. Um modelo quadrático completo em $p$ variáveis requer $O(p^2)$ termos de produtos cruzados, e um polinômio de grau $d$ requer $O(p^d)$ termos [^2].
3.  **Transformações Não Lineares:** $h_m(X) = log(X_j)$, $\sqrt{X_j}$, ... permitem outras transformações não lineares de entradas únicas [^2]. De forma mais geral, pode-se usar funções similares envolvendo várias entradas, como $h_m(X) = ||X||$ [^2].
4.  **Funções Indicadoras:** $h_m(X) = I(L_m \le X_k < U_m)$, um indicador para uma região de $X_k$. Dividir o intervalo de $X_k$ em $M_k$ regiões não sobrepostas resulta em um modelo com uma contribuição constante por partes para $X_k$ [^2].

**Restrições e Regularização:** Embora a expansão de base aumente a flexibilidade do modelo, ela também pode levar ao *overfitting*, especialmente quando o número de funções de base $M$ é grande [^2]. Para mitigar esse problema, são utilizadas técnicas de restrição ou regularização.

*   **Métodos de Restrição:** Limitam a classe de funções *a priori*. A aditividade é um exemplo onde se assume que o modelo tem a forma:
    $$     f(X) = \sum_{j=1}^{p} f_j(X_j) = \sum_{j=1}^{p} \sum_{m=1}^{M_j} \beta_{jm} h_{jm}(X_j).     $$
    O tamanho do modelo é limitado pelo número de funções de base $M_j$ usadas para cada componente $f_j$ [^2].
*   **Métodos de Seleção:** Escaneiam adaptativamente o dicionário de funções de base e incluem apenas aquelas que contribuem significativamente para o ajuste do modelo [^3]. Técnicas de seleção de variáveis, como as discutidas no Capítulo 3, são úteis aqui [^3]. Abordagens *greedy* como CART, MARS e *boosting* também se enquadram nesta categoria [^3].
*   **Métodos de Regularização:** Utilizam todo o dicionário de funções de base, mas restringem os coeficientes [^3]. Regressão Ridge é um exemplo simples de uma abordagem de regularização, enquanto o *lasso* é um método de regularização e seleção [^3].

### Conclusão

Modelos lineares fornecem uma estrutura simples e interpretável para regressão e classificação, mas podem ser limitados em sua capacidade de capturar relações complexas. A expansão de base oferece uma abordagem flexível para superar essas limitações, transformando as variáveis de entrada em um espaço de características mais rico. No entanto, o aumento da complexidade do modelo exige o uso de técnicas de restrição ou regularização para evitar o *overfitting*. Os métodos de expansão de base fornecem um *framework* poderoso para construir modelos mais precisos e adaptáveis, mantendo a interpretabilidade e o controle sobre a complexidade do modelo.

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer series in statistics. New York: Springer.
[^2]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer series in statistics. New York: Springer.
[^3]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer series in statistics. New York: Springer.
<!-- END -->