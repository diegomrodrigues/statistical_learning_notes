## Capítulo 5.1: Expansões em Bases e Regularização

### Introdução
Como vimos anteriormente, os modelos lineares desempenham um papel fundamental tanto em regressão quanto em classificação. Regressão linear, análise discriminante linear, regressão logística e hiperplanos separadores são todos construídos sobre a base de um modelo linear [^1]. No entanto, é crucial reconhecer que a verdadeira função $f(X)$ raramente é linear em relação a $X$. Em problemas de regressão, $f(X) = E(Y|X)$ exibirá tipicamente um comportamento não linear e não aditivo em relação a $X$. Portanto, representar $f(X)$ por meio de um modelo linear é, na maioria das vezes, uma aproximação conveniente, e por vezes necessária [^1].

Essa aproximação é conveniente devido à facilidade de interpretação inerente aos modelos lineares, que podem ser vistos como a aproximação de Taylor de primeira ordem de $f(X)$ [^1]. A necessidade surge quando o tamanho da amostra $N$ é pequeno e/ou o número de preditores $p$ é grande, onde um modelo linear pode ser o único que pode ser ajustado aos dados sem *overfitting* [^1]. Da mesma forma, em classificação, uma fronteira de decisão linear, idealmente Bayes-ótima, implica que alguma transformação monotônica de $Pr(Y = 1|X)$ é linear em $X$, o que também é uma aproximação [^1].

Este capítulo e o seguinte exploram métodos populares para ir além da linearidade, focando na ideia central de aumentar/substituir o vetor de entradas $X$ com variáveis adicionais que são transformações de $X$ [^1]. Modelos lineares são então empregados neste novo espaço de *features* derivadas.

### Conceitos Fundamentais
As **expansões em bases** envolvem a transformação das *features* de entrada originais usando funções $h_m(X)$ e, em seguida, modelando a saída como uma combinação linear dessas *features* transformadas, representadas por:

$$f(X) = \sum_{m=1}^{M} \beta_m h_m(X) \quad (5.1)$$

onde $h_m(X) : \mathbb{R}^p \rightarrow \mathbb{R}$ denota a *m*-ésima transformação de $X$, com $m = 1, ..., M$ [^1]. Esta abordagem representa uma expansão linear em $X$ [^2]. A beleza desta abordagem reside no fato de que, uma vez que as funções de base $h_m$ são determinadas, os modelos são lineares nessas novas variáveis e o ajuste prossegue como antes [^2].

Exemplos simples e amplamente utilizados das funções $h_m$ incluem:

*   $h_m(X) = X_m$, $m = 1, ..., p$, que recupera o modelo linear original [^2].
*   $h_m(X) = X_j^2$ ou $h_m(X) = X_jX_k$, que permite aumentar as entradas com termos polinomiais para obter expansões de Taylor de ordem superior [^2]. No entanto, o número de variáveis cresce exponencialmente no grau do polinômio. Um modelo quadrático completo em $p$ variáveis requer $O(p^2)$ termos quadrados e de produto cruzado, ou mais geralmente $O(p^d)$ para um polinômio de grau $d$ [^2].
*   $h_m(X) = \log(X_j)$, $\sqrt{X_j}$, ..., que permite outras transformações não lineares de entradas únicas [^2]. Mais geralmente, pode-se usar funções semelhantes envolvendo várias entradas, como $h_m(X) = ||X||$ [^2].
*   $h_m(X) = I(L_m \leq X_k < U_m)$, um indicador para uma região de $X_k$ [^2]. Dividir o intervalo de $X_k$ em $M_k$ regiões não sobrepostas resulta em um modelo com uma contribuição constante por partes para $X_k$ [^2].

Às vezes, o problema em questão exigirá funções de base específicas $h_m$, como logaritmos ou funções de potência. No entanto, muitas vezes usamos as expansões de base como um dispositivo para obter representações mais flexíveis para $f(X)$ [^2]. Polinômios são um exemplo deste último, embora sejam limitados por sua natureza global - ajustar os coeficientes para obter uma forma funcional em uma região pode fazer com que a função se comporte de forma irregular em regiões remotas [^2].

Este capítulo considera famílias mais úteis de polinômios *piecewise* e *splines* que permitem representações polinomiais locais [^2]. Também discutimos as bases de *wavelets*, especialmente úteis para modelar sinais e imagens. Esses métodos produzem um dicionário $D$ consistindo tipicamente de um número muito grande $|D|$ de funções de base, muito mais do que podemos pagar para ajustar aos nossos dados [^2]. Junto com o dicionário, exigimos um método para controlar a complexidade do nosso modelo, usando funções de base do dicionário [^2]. Existem três abordagens comuns [^2]:

*   **Métodos de restrição**, onde decidimos de antemão limitar a classe de funções [^2]. A aditividade é um exemplo, onde assumimos que nosso modelo tem a forma

    $$f(X) = \sum_{j=1}^{p} f_j(X_j) = \sum_{j=1}^{p} \sum_{m=1}^{M_j} \beta_{jm} h_{jm}(X_j) \quad (5.2)$$
    O tamanho do modelo é limitado pelo número de funções de base $M_j$ usadas para cada função componente $f_j$ [^3].
*   **Métodos de seleção**, que examinam adaptativamente o dicionário e incluem apenas as funções de base $h_m$ que contribuem significativamente para o ajuste do modelo [^3]. As técnicas de seleção de variáveis discutidas no Capítulo 3 são úteis aqui. As abordagens *stagewise greedy*, como CART, MARS e *boosting*, também se enquadram nesta categoria [^3].
*   **Métodos de regularização**, onde usamos todo o dicionário, mas restringimos os coeficientes [^3]. A regressão de *ridge* é um exemplo simples de uma abordagem de regularização, enquanto o *lasso* é um método de regularização e seleção [^3].

### Conclusão
As expansões em bases fornecem uma estrutura flexível para modelar relações não lineares, transformando *features* de entrada em um novo espaço onde modelos lineares podem ser aplicados efetivamente [^1, 2]. Controlar a complexidade do modelo é crucial para evitar o *overfitting*, e vários métodos como restrição, seleção e regularização estão disponíveis para este fim [^3]. A escolha da função base $h_m(X)$ e o método de controle de complexidade dependem do problema em questão e das características dos dados.

### Referências
[^1]: Seção 5.1 do documento fornecido.
[^2]: Seção 5.0 e início da Seção 5.1 do documento fornecido.
[^3]: Seção 5.1 do documento fornecido.
<!-- END -->