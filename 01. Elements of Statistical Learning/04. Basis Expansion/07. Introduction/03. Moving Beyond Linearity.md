## Capítulo 5.1: Expansões de Base e Regularização: Indo Além da Linearidade

### Introdução
Como vimos anteriormente, modelos lineares nas *features* de entrada são amplamente utilizados tanto para regressão quanto para classificação [^1]. Regressão linear, análise discriminante linear, regressão logística e hiperplanos separadores dependem de um modelo linear. No entanto, é improvável que a verdadeira função $f(X)$ seja realmente linear em $X$ [^1]. Em problemas de regressão, $f(X) = E(Y|X)$ será tipicamente não linear e não aditiva em $X$, e representar $f(X)$ por um modelo linear é geralmente uma aproximação conveniente, e às vezes necessária [^1]. É conveniente porque um modelo linear é fácil de interpretar e é a aproximação de Taylor de primeira ordem para $f(X)$ [^1]. Às vezes é necessário, porque com $N$ pequeno e/ou $p$ grande, um modelo linear pode ser tudo o que somos capazes de ajustar aos dados sem *overfitting* [^1]. Da mesma forma, na classificação, um limite de decisão linear, ótimo de Bayes, implica que alguma transformação monotônica de $Pr(Y = 1|X)$ é linear em $X$ [^1]. Isso é inevitavelmente uma aproximação [^1]. Este capítulo discute métodos populares para ir além da linearidade. A ideia central é aumentar ou substituir o vetor de entradas $X$ com variáveis adicionais, que são transformações de $X$, e então usar modelos lineares neste novo espaço de *features* de entrada derivadas [^1].

### Conceitos Fundamentais

A essência de superar a linearidade reside em **transformar o vetor de entrada $X$** através de expansões de base [^1]. Denotamos por $h_m(X): \mathbb{R}^p \rightarrow \mathbb{R}$ a *m-ésima* transformação de $X$, onde $m = 1, ..., M$ [^1]. Modelamos então:

$$f(X) = \sum_{m=1}^{M} \beta_m h_m(X) \tag{5.1}$$

Esta formulação representa uma **expansão de base linear** em $X$ [^1]. A beleza desta abordagem reside no fato de que, uma vez determinadas as funções de base $h_m$, os modelos tornam-se lineares nessas novas variáveis, e o ajuste prossegue como antes [^1].

Alguns exemplos simples e amplamente utilizados das funções $h_m$ incluem [^2]:

*   $h_m(X) = X_m$, com $m = 1, ..., p$, que recupera o modelo linear original [^2].
*   $h_m(X) = X_j^2$ ou $h_m(X) = X_j X_k$, que permite aumentar as entradas com termos polinomiais para alcançar expansões de Taylor de ordem superior [^2]. No entanto, o número de variáveis cresce exponencialmente com o grau do polinômio. Um modelo quadrático completo em $p$ variáveis requer $O(p^2)$ termos quadrados e de produtos cruzados, ou mais geralmente $O(p^d)$ para um polinômio de grau $d$ [^2].
*   $h_m(X) = \log(X_j)$, $\sqrt{X_j}$, ..., que permite outras transformações não lineares de entradas únicas [^2]. Mais geralmente, pode-se usar funções semelhantes envolvendo várias entradas, como $h_m(X) = ||X||$ [^2].
*   $h_m(X) = I(L_m \leq X_k < U_m)$, um indicador para uma região de $X_k$ [^2]. Dividir o intervalo de $X_k$ em $M_k$ regiões não sobrepostas resulta em um modelo com uma contribuição constante por partes para $X_k$ [^2].

Por vezes, o problema em questão exigirá funções de base particulares $h_m$, como logaritmos ou funções de potência [^2]. Mais frequentemente, no entanto, usamos as expansões de base como um dispositivo para alcançar representações mais flexíveis para $f(X)$ [^2]. Polinômios são um exemplo do último, embora sejam limitados por sua natureza global – ajustar os coeficientes para alcançar uma forma funcional em uma região pode fazer com que a função se comporte de maneira irregular em regiões remotas [^2]. Este capítulo considera famílias mais úteis de polinômios *piecewise* e *splines* que permitem representações polinomiais locais [^2]. Também discutiremos as bases de *wavelets*, especialmente úteis para modelar sinais e imagens [^2]. Esses métodos produzem um dicionário $D$ consistindo tipicamente de um número muito grande $|D|$ de funções de base, muito mais do que podemos nos dar ao luxo de ajustar aos nossos dados [^2]. Juntamente com o dicionário, precisamos de um método para controlar a complexidade do nosso modelo, usando funções de base do dicionário [^2]. Existem três abordagens comuns [^2]:

*   **Métodos de restrição**, onde decidimos antecipadamente limitar a classe de funções [^2]. A aditividade é um exemplo, onde assumimos que nosso modelo tem a forma

$$f(X) = \sum_{j=1}^{p} f_j(X_j) = \sum_{j=1}^{p}\sum_{m=1}^{M_j} \beta_{jm} h_{jm}(X_j) \tag{5.2}$$

O tamanho do modelo é limitado pelo número de funções de base $M_j$ usadas para cada função componente $f_j$ [^3].

*   **Métodos de seleção**, que examinam adaptativamente o dicionário e incluem apenas as funções de base $h_m$ que contribuem significativamente para o ajuste do modelo [^3]. As técnicas de seleção de variáveis discutidas no Capítulo 3 são úteis aqui. As abordagens gananciosas *stagewise*, como CART, MARS e *boosting*, também se enquadram nesta categoria [^3].

*   **Métodos de regularização**, onde usamos todo o dicionário, mas restringimos os coeficientes [^3]. A regressão de *ridge* é um exemplo simples de uma abordagem de regularização, enquanto o *lasso* é um método de regularização e seleção. Discutiremos estes e métodos mais sofisticados para regularização [^3].

### Conclusão

Este capítulo introduziu o conceito de expansões de base como uma ferramenta fundamental para ir além da linearidade em modelos estatísticos [^1]. Ao transformar o vetor de entrada $X$ em um novo espaço de *features* derivadas, podemos aproveitar modelos lineares para aproximar funções complexas e não lineares [^1]. As diferentes abordagens para construir e regularizar expansões de base, como polinômios, *splines* e *wavelets*, oferecem flexibilidade e controle sobre a complexidade do modelo [^2]. A escolha do método apropriado depende das características do problema em questão e do compromisso desejado entre ajuste e interpretabilidade [^2]. Os métodos de restrição, seleção e regularização fornecem ferramentas adicionais para controlar a complexidade do modelo e evitar o *overfitting*, permitindo-nos construir modelos mais robustos e generalizáveis [^3].

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. New York: Springer.
[^2]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. New York: Springer.
[^3]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. New York: Springer.
<!-- END -->