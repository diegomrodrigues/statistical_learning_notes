{
  "topics": [
    {
      "topic": "Additive Models and Generalized Additive Models (GAMs)",
      "sub_topics": [
        "Supervised learning methods assume structured forms for regression functions to address the curse of dimensionality, balancing model complexity with potential misspecification.",
        "Generalized Additive Models (GAMs) extend linear models by allowing non-linear functions of each predictor, enabling the identification and characterization of non-linear regression effects. GAMs retain additivity, allowing interpretation of the model in a similar way to linear models, but with increased flexibility due to the non-parametric functions. The functions f_j are estimated nonparametrically, often using scatterplot smoothers like cubic smoothing splines or kernel smoothers, to capture nonlinear relationships between predictors and the response variable.",
        "Additive models can be expressed as E(Y|X1, X2, ..., Xp) = a + f1(X1) + f2(X2) + \\u00b7\\u00b7\\u00b7 + fp(Xp), where each f_j is an unspecified smooth function, allowing for flexible modeling of predictor effects.",
        "Additive models use a scatterplot smoother to fit each function, revealing potential nonlinearities in the effect of each predictor, and can mix linear and other parametric forms with nonlinear terms, accommodating qualitative variables and interactions between variables.",
        "Additive models can replace linear models in various settings, including time series decomposition, where Yt = St + Tt + Et, representing seasonal component, trend, and error term respectively.",
        "Additive logistic regression models extend logistic regression by replacing linear terms with more general functional forms, maintaining additivity for interpretability while increasing flexibility. Additive Logistic Regression models classify binary data by relating the mean of the binary response to predictors via a logit link function and unspecified smooth functions.",
        "Additive models can be generalized further to handle more than two classes, using the multilogit formulation, and can be applied to various data analysis tasks, such as predicting email spam. Applying generalized additive models to spam data involves preprocessing steps like log-transformation to handle long-tailed distributions, followed by fitting with cubic smoothing splines, needing expertise in data preparation and model fitting.",
        "Additive models offer a useful extension of linear models, making them more flexible while still retaining much of their interpretability. The familiar tools for modeling and inference in linear models are also available for additive models, but can have limitations for large data-mining applications.",
        "Link functions relate the conditional mean of the response to an additive function of the predictors, with examples including the identity link for Gaussian data, logit/probit links for binomial data, and log link for Poisson data. Generalized additive models can accommodate various types of data through different link functions, such as the identity link for Gaussian data, the logit or probit link for binomial data, and the log link for Poisson count data.",
        "Penalized sum of squares is used as a criterion for fitting additive models, balancing the goodness of fit with the smoothness of the estimated functions, controlled by tuning parameters \\u03bb_j. Model fitting involves minimizing a penalized sum of squares criterion, balancing goodness-of-fit with model complexity using tuning parameters, often leading to additive cubic spline models."
      ]
    },
    {
      "topic": "Fitting Additive Models: The Backfitting Algorithm",
      "sub_topics": [
        "The backfitting algorithm is a modular approach for fitting additive models and their generalizations, using scatterplot smoothers to fit nonlinear effects in a flexible manner. The backfitting algorithm iteratively updates each function estimate by applying a smoothing spline to the residuals, ensuring the functions average zero over the data to address identifiability issues.",
        "A penalized sum of squares criterion can be specified to address this problem, where the tuning parameters are used to control the trade-off between model fit and model complexity.",
        "The minimizer of the penalized sum of squares is an additive cubic spline model where each of the functions is a cubic spline in the component Xj, with knots at each unique value of Xij.",
        "The constant \\u03b1 is not identifiable because we can add or subtract constants to each of the functions fj and adjust \\u03b1 accordingly, the standard convention is to assume that the functions average zero over the data.",
        "The penalized sum of squares criterion (PRSS) is defined as the sum of squared errors plus a penalty term that involves the integral of the squared second derivative of each function f_j, weighted by tuning parameters \\u03bb_j, which control the smoothness of the functions.",
        "Minimizing the PRSS results in an additive cubic spline model, where each function f_j is a cubic spline with knots at the unique values of the predictor X_j.",
        "The backfitting algorithm iteratively updates each function f_j by applying a smoother S_j to the partial residuals (y_i - \\u03b1 - \\u03a3_{k\\u2260j} f_k(x_{ik})), until the functions converge.",
        "The constant \\u03b1 is typically estimated as the average of the response values, and a standard convention is to assume that the functions f_j have zero mean over the data.",
        "Fitting generalized additive models (GAMs) using a scatterplot smoother as a building block, specifically cubic smoothing splines, involves minimizing a penalized sum of squares criterion, which balances model fit and smoothness via tuning parameters \\u03bbj. Efficiently choosing these parameters is crucial for model performance.",
        "Machine rounding errors can cause slippage in the backfitting algorithm, requiring an adjustment step to ensure the functions average to zero over the data, adding a layer of complexity to the implementation.",
        "The backfitting algorithm can accommodate various smoothing operators, including local polynomial regression and kernel methods, providing flexibility in modeling nonlinear effects. The choice of smoothing operator should align with the data characteristics and desired model properties.",
        "Degrees of freedom for each term can be approximated using the trace of the smoother matrix, aiding in model complexity assessment. Careful consideration of degrees of freedom is essential for avoiding overfitting and ensuring model interpretability.",
        "Using the local scoring algorithm for additive logistic regression involves constructing a working target variable and weights iteratively to estimate the functions fj. Convergence is assessed by monitoring the change in these functions, stopping when the change falls below a predefined threshold. Careful selection of the threshold affects model accuracy and computational cost."
      ]
    },
    {
      "topic": "Additive Logistic Regression",
      "sub_topics": [
        "Additive logistic regression models the probability of a binary outcome given predictor variables, using a logit transformation to relate the predictors to the log-odds of the outcome. Additive logistic regression models the log-odds of a binary outcome as an additive function of smooth functions of the predictors, extending the standard logistic regression model.",
        "Model fitting involves a backfitting algorithm within a Newton-Raphson procedure, iteratively reweighting least squares to estimate the functions f1, f2, ..., fp. The functions f_1, f_2, ..., f_p in additive logistic regression are estimated using a backfitting algorithm within a Newton-Raphson procedure, known as iteratively reweighted least squares (IRLS).",
        "The local scoring algorithm computes starting values for the intercept and functions, constructs a working target variable and weights, and fits an additive model to the targets with weights using a weighted backfitting algorithm. The local scoring algorithm is used to iteratively update the estimates of \\u03b1 and f_j, constructing a working target variable z_i and weights w_i based on the current estimates of the probabilities p_i.",
        "The algorithm continues until the change in the functions falls below a pre-specified threshold, ensuring convergence of the estimates.",
        "The additive model fitting in the local scoring algorithm requires a weighted scatterplot smoother, where most smoothing procedures can accept observation weights.",
        "Handling more than two classes requires the multilogit formulation, extending the algorithm but increasing complexity; the VGAM software offers solutions, demanding familiarity with specialized tools.",
        "Applying generalized additive models to spam data involves preprocessing steps like log-transformation to handle long-tailed distributions, followed by fitting with cubic smoothing splines, needing expertise in data preparation and model fitting."
      ]
    },
    {
      "topic": "Tree-Based Methods: Regression and Classification Trees",
      "sub_topics": [
        "Tree-based methods partition the feature space into rectangles, fitting a simple model within each partition, offering interpretability and ease of representation. Tree-based methods partition the feature space into a set of rectangles and fit a simple model in each one. They are conceptually simple yet powerful.",
        "Recursive binary partitioning simplifies the description of regions by splitting the space into two, modeling the response by the mean of Y in each region, and continuing until a stopping rule is applied.",
        "Regression trees are grown by recursively partitioning the data based on splitting variables and points to minimize the sum of squares, with the tree size tuned to balance overfitting and capturing important structure. Regression trees are grown by automatically deciding on the splitting variables and split points to minimize the sum of squares, using a greedy algorithm to find the best binary partition.",
        "The algorithm needs to automatically decide on the splitting variables and split points, and also what topology (shape) the tree should have.",
        "If we adopt as our criterion minimization of the sum of squares \\u2211(yi f(xi))2, it is easy to see that the best \\u00eam is just the average of yi in region Rm",
        "Cost-complexity pruning is used to prune large trees, balancing tree size and goodness of fit to avoid overfitting, with the tuning parameter \\u03b1 governing the tradeoff. Cost-complexity pruning prunes a large tree To by collapsing internal nodes to minimize a cost-complexity criterion that balances tree size and goodness of fit. Estimation of the tuning parameter \\u03b1 is achieved by cross-validation.",
        "Classification trees modify the tree algorithm by using criteria for splitting nodes and pruning the tree based on classification outcomes, such as misclassification error, Gini index, or cross-entropy. For classification trees, node impurity measures such as the Gini index or cross-entropy are used to guide the splitting process. These measures are more sensitive to changes in node probabilities than the misclassification rate, making them more amenable to numerical optimization. Careful choice of the impurity measure impacts model performance and interpretability.",
        "If the target is a classification outcome taking values 1,2,..., K, the only changes needed in the tree algorithm pertain to the criteria for splitting nodes and pruning the tree. For regression we used the squared-error node",
        "When splitting a predictor having q possible unordered values, there are 29-1 1 possible partitions of the q values into two groups, and the computations become prohibitive for large q.",
        "In classification problems, the consequences of misclassifying observations are more serious in some classes than others.",
        "The variable and split-point are chosen to achieve the best fit, then one or both of these regions are split into two more regions, and this process is continued, until some stopping rule is applied.",
        "A key advantage of the recursive binary tree is its interpretability, where the feature space partition is fully described by a single tree, with more than two inputs, partitions are difficult to draw, but the binary tree representation works in the same way.",
        "Growing regression trees involves a greedy algorithm to determine splitting variables and split points by minimizing the sum of squares within resulting regions. Computational efficiency is achieved by quickly determining the best split point for each splitting variable, enabling a feasible search through all inputs.",
        "For classification trees, node impurity measures such as the Gini index or cross-entropy are used to guide the splitting process. These measures are more sensitive to changes in node probabilities than the misclassification rate, making them more amenable to numerical optimization. Careful choice of the impurity measure impacts model performance and interpretability.",
        "When dealing with categorical predictors, ordering predictor classes according to the proportion falling in the outcome class simplifies the splitting process and gives the optimal split in terms of cross-entropy or Gini index. However, categorical predictors with many levels can lead to overfitting and should be avoided.",
        "Surrogate splits exploit correlations between predictors to alleviate the effect of missing data. They provide alternative splits when the primary splitting predictor is missing, minimizing the loss of information due to the missing value."
      ]
    },
    {
      "topic": "MARS: Multivariate Adaptive Regression Splines",
      "sub_topics": [
        "MARS is an adaptive procedure for regression, well-suited for high-dimensional problems, generalizing stepwise linear regression and CART to improve performance in the regression setting.",
        "MARS uses piecewise linear basis functions (x-t)+ and (t-x)+ to model the relationship between predictors and response, forming reflected pairs with knots at each observed value.",
        "The model-building strategy in MARS is like a forward stepwise linear regression, but instead of using the original inputs, we are allowed to use functions from the set C and their products.",
        "MARS uses a generalized cross-validation (GCV) criterion to save on computations, trading off model fit and complexity by penalizing the effective number of parameters.",
        "MARS builds the regression surface parsimoniously, using nonzero components locally where they are needed, and exploits the simple form of piecewise linear functions for computational efficiency.",
        "The forward modeling strategy in MARS is hierarchical, such that multiway products are built up from products involving terms already in the model, using lower-order 'footprints'.",
        "There is one restriction put on the formation of model terms: each input can appear at most once in a product. This prevents the formation of higher-order powers of an input",
        "A useful option in the MARS procedure is to set an upper limit on the order of interaction. For example, one can set a limit of two, allowing pairwise products of piecewise linear functions, but not three- or higher-way products.",
        "The coefficients are estimated by minimizing the residual sum-of-squares, and at each stage we consider as a new basis function pair all products of a function hm in the model set M with one of the reflected pairs in C.",
        "MARS employs a forward stepwise linear regression-like strategy, using piecewise linear basis functions to build a model. The model has the form f(X) = \\u03b2\\u03bf + \\u03a3\\u03b2mhm(X), where each hm(X) is a function in C, or a product of two or more such functions. The coefficients are estimated by minimizing the residual sum-of-squares, i.e., by standard linear regression. The choice of the functions hm(x) is the key to MARS.",
        "The forward modeling strategy in MARS is hierarchical, in the sense that multiway products are built up from products involving terms already in the model. The product that decreases the residual error the most is added into the current model. The process is continued until the model set M contains some preset maximum number of terms. This strategy avoids the search over an exponentially growing space of alternatives.",
        "MARS uses generalized cross-validation (GCV) to select the optimal model size. The value M(\\u03bb) is the effective number of parameters in the model, which accounts both for the number of terms in the models, plus the number of parameters used in selecting the optimal positions of the knots.",
        "Computationally, MARS exploits the simple form of the piecewise linear function. Moving the knot successively one position at a time to the left, the basis functions differ by zero over the left part of the domain, and by a constant over the right part. Hence after each such move we can update the fit in O(1) operations, allowing us to try every knot in only O(N) operations.",
        "A useful option in the MARS procedure is to set an upper limit on the order of interaction. For example, one can set a limit of two, allowing pairwise products of piecewise linear functions, but not three- or higher-way products. This can aid in the interpretation of the final model. An upper limit of one results in an additive model.",
        "Multivariate Adaptive Regression Splines (MARS) uses piecewise linear basis functions (x-t)+ and (t \\u2212 x)+, forming reflected pairs for each input Xj with knots at each observed value xij, requiring efficient algorithms to manage a large number of basis functions.",
        "The model-building strategy is like forward stepwise linear regression, using functions from set C and their products; coefficients are estimated by minimizing the residual sum-of-squares, requiring efficient linear regression techniques.",
        "At each stage, MARS considers as a new basis function pair all products of a function hm in the model set M with one of the reflected pairs in C, requiring efficient search algorithms to identify the term that reduces training error the most.",
        "The backward deletion procedure removes the term whose removal causes the smallest increase in residual squared error, producing an estimated best model fx of each size \\u03bb, requiring cross-validation to estimate the optimal value of \\u03bb.",
        "Generalized cross-validation (GCV) is used to save computational resources, with M(\\u03bb) being the effective number of parameters in the model, accounting for terms and parameters used in selecting knot positions, needing careful selection of penalty parameter c."
      ]
    },
    {
      "topic": "Hierarchical Mixtures of Experts (HME)",
      "sub_topics": [
        "HME can be viewed as a variant of tree-based methods, where the tree splits are not hard decisions but rather soft probabilistic ones, with an observation going left or right with probabilities depending on its input values.",
        "In an HME, a linear (or logistic regression) model is fit in each terminal node, instead of a constant as in CART, and the splits can be multiway, not just binary, and the splits are probabilistic functions of a linear combination of inputs.",
        "The terminal nodes are called experts, and the non-terminal nodes are called gating networks, with each expert providing an opinion (prediction) about the response, and these are combined together by the gating networks.",
        "A two-level HME model consists of a top gating network and expert networks, where the top gating network has the output, gj(x, j), and the expert networks have a similar form.",
        "The EM algorithm is used to estimate the parameters, by maximizing the log-likelihood of the data. The E-step computes the expectations of the latent variables \\u2206j and \\u2206elj given the current values of the parameters. These expectations are then used as observation weights in the M-step of the procedure, to estimate the parameters in the expert networks.",
        "The hierarchical mixtures of experts approach is a promising competitor to CART trees. By using soft splits rather than hard decision rules it can capture situations where the transition from low to high response is gradual. The log-likelihood is a smooth function of the unknown weights and hence is amenable to numerical optimization.",
        "Hierarchical mixtures of experts (HME) uses soft probabilistic splits rather than hard decisions, with observations going left or right with probabilities depending on input values, requiring smooth parameter optimization.",
        "In an HME, a linear (or logistic regression) model is fit in each terminal node, and the splits are probabilistic functions of a linear combination of inputs, with the merits of these choices being unclear.",
        "At the top gating network, the output gj(x, \\u03b3j) = e\\u03b3jTx / \\u03a3k=1Ke\\u03b3kTx represents a soft K-way split, with each gj(x, \\u03b3j) being the probability of assigning an observation with feature vector x to the jth branch; parameters are estimated using the EM algorithm.",
        "Losses may be incorporated into the tree-growing process by using weight Lk,1-k for an observation in class k, with L01 = 5 and L10 = 1, and fitting the same size tree as before (Ta = 17).",
        "The area under the ROC curve is equivalent to the Mann-Whitney U statistic (or Wilcoxon rank-sum test), for the median difference between the prediction scores in the two groups (Hanley and McNeil, 1982)."
      ]
    },
    {
      "topic": "Computational Considerations for Additive Models, Trees, MARS, and HME",
      "sub_topics": [
        "Additive model fitting requires m*p applications of a one-dimensional smoother or regression method, where m is the number of cycles of the backfitting algorithm (usually less than 20) and p is the number of predictors. With cubic smoothing splines, N log N operations are needed for an initial sort and N operations for the spline fit.",
        "Trees require pNlog N operations for an initial sort for each predictor, and typically another pN log N operations for the split computations. If the splits occurred near the edges of the predictor ranges, this number could increase to N^2*p.",
        "MARS requires Nm^2 + pmN operations to add a basis function to a model with m terms already present, from a pool of p predictors. Hence to build an M-term model requires NM^3 + pM^2N computations, which can be quite prohibitive if M is a reasonable fraction of N.",
        "Each of the components of an HME are typically inexpensive to fit at each M-step: Np^2 for the regressions, and Np^2K^2 for a K-class logistic regression. The EM algorithm, however, can take a long time to converge, and so sizable HME models are considered costly to fit."
      ]
    },
    {
      "topic": "Alternative Tree-Building Procedures: ID3, C4.5, C5.0, and Linear Combination Splits",
      "sub_topics": [
        "ID3, C4.5, and C5.0 are alternative tree-building methodologies with rule set derivation, where splitting rules defining terminal nodes are simplified by dropping conditions, requiring algorithms to determine which conditions can be dropped without changing the subset of observations.",
        "Linear combination splits involve optimizing weights aj and split points s to minimize the criterion, potentially improving predictive power but hurting interpretability, with the discreteness of split point search precluding smooth optimization.",
        "Trees have high variance, with small data changes resulting in different splits, making interpretation precarious; bagging reduces variance by averaging many trees, requiring significant computational resources."
      ]
    },
    {
      "topic": "PRIM: Bump Hunting",
      "sub_topics": [
        "Patient Rule Induction Method (PRIM) finds boxes in feature space with high response averages, seeking maxima in the target function through top-down peeling, where the box is compressed along one face, peeling off observations and requiring efficient algorithms to determine the face chosen for compression.",
        "After top-down peeling, PRIM reverses the process, expanding along any edge if such expansion increases the box mean, called pasting; the result is a sequence of boxes with different numbers of observations, requiring cross-validation to choose the optimal box size.",
        "PRIM handles categorical predictors by considering all partitions of the predictor, as in CART, with missing values handled similarly, but PRIM is designed for regression, requiring modifications for classification tasks.",
        "An advantage of PRIM over CART is its patience, as CART fragments data quickly due to binary splits, whereas PRIM peels off a proportion of training points at each stage, requiring careful selection of the proportion to balance exploration and data exhaustion."
      ]
    }
  ]
}