## 9.1.2 Detalhamento do Algoritmo de Local Scoring para Regressão Logística Aditiva

### Introdução
Este capítulo expande a discussão sobre modelos aditivos logísticos, focando especificamente no algoritmo de *local scoring* usado para estimar os parâmetros do modelo [^5]. Este algoritmo, apresentado no Algoritmo 9.2 [^6], é essencial para a implementação prática da regressão logística aditiva, pois fornece um método iterativo para atualizar as estimativas dos parâmetros do modelo. Ele se baseia em conceitos já estabelecidos, como o algoritmo *backfitting* para modelos aditivos [^4] e a estrutura geral de modelos lineares generalizados [^2].

### Conceitos Fundamentais

O algoritmo de *local scoring* para regressão logística aditiva é um procedimento iterativo projetado para estimar os parâmetros $\alpha$ (intercepto) e $f_j$ (funções aditivas para cada preditor $X_j$) no modelo de regressão logística aditiva generalizado [^5, 6]:

$$log \left( \frac{Pr(Y = 1|X)}{Pr(Y = 0|X)} \right) = \alpha + \sum_{j=1}^{p} f_j(X_j)$$

O algoritmo funciona construindo iterativamente uma variável alvo de trabalho $z_i$ e pesos $w_i$ baseados nas estimativas atuais das probabilidades $p_i$ [^5, 6]. O objetivo é maximizar a *log-likelihood* penalizada, utilizando uma rotina *Newton-Raphson* que é reformulada como um algoritmo *IRLS* (iterativamente reponderado por mínimos quadrados) [^5].

**Etapas do Algoritmo**

1.  **Inicialização** [^6]:
    *   Calcula-se um valor inicial para o intercepto $\alpha$ como o *log-odds* da proporção amostral de uns:

    $$alpha = \log \left( \frac{\bar{y}}{1 - \bar{y}} \right), \quad \text{onde } \bar{y} = \text{ave}(y_i)$$

    *   As funções $f_j$ são inicializadas como zero para todos os $j$ e $i$: $f_j = 0, \forall i, j$ [^6].

2.  **Iteração** [^6]:
    *   Define-se $\eta_i = \alpha + \sum_{j} f_j(x_{ij})$ e $\pi_i = 1 / [1 + \exp(-\eta_i)]$ [^6].
    *   Para cada iteração, as seguintes etapas são repetidas até a convergência:
        *   **Construção da Variável Alvo de Trabalho ($z_i$)** [^6]:

        $$z_i = \eta_i + \frac{(y_i - \pi_i)}{\pi_i(1 - \pi_i)}$$

        *   **Construção dos Pesos ($w_i$)** [^6]:

        $$w_i = \pi_i(1 - \pi_i)$$

        *   **Ajuste de um Modelo Aditivo Ponderado** [^6]: Utiliza-se um algoritmo de *backfitting* ponderado para ajustar um modelo aditivo aos alvos $z_i$ com pesos $w_i$. Isso fornece novas estimativas para $\alpha$ e $f_j$: $\hat{\alpha}, \hat{f_j}$.

3.  **Critério de Convergência** [^6]: O algoritmo continua iterando até que a mudança nas funções $f_j$ fique abaixo de um limiar predefinido.

**Detalhes Técnicos**

*   **Variável Alvo de Trabalho ($z_i$)**: A variável alvo de trabalho $z_i$ é uma linearização da função *logit* em torno da estimativa atual $\eta_i$. Ela representa a direção na qual os parâmetros precisam ser ajustados para melhorar o *fit* do modelo [^6].
*   **Pesos ($w_i$)**: Os pesos $w_i$ são derivados da variância da resposta na regressão logística. Eles dão mais peso às observações onde o modelo atual tem mais incerteza (isto é, onde $\pi_i$ está próximo de 0.5) [^6].
*   **Algoritmo Backfitting Ponderado**: O algoritmo *backfitting* ponderado é uma modificação do algoritmo *backfitting* padrão que leva em conta os pesos $w_i$. Isso garante que as funções $f_j$ sejam estimadas de forma a minimizar o erro ponderado [^6, 4].
*   **Smoothers Ponderados**: A etapa (2) do Algoritmo 9.2 [^6] requer um *scatterplot smoother* ponderado. A maioria dos procedimentos de *smoothing* pode aceitar pesos de observação [^6].

**Observações**

*   O algoritmo de *local scoring* é uma instância do algoritmo *IRLS* aplicado a modelos aditivos generalizados [^5].
*   A escolha do *scatterplot smoother* e a definição do critério de convergência podem afetar o desempenho do algoritmo [^6, 4].

### Conclusão

O algoritmo de *local scoring* é uma ferramenta fundamental para ajustar modelos de regressão logística aditiva. Ao iterativamente refinar as estimativas dos parâmetros do modelo, ele permite que o modelo capture relações não lineares entre os preditores e a resposta. O algoritmo é computacionalmente eficiente e pode ser aplicado a conjuntos de dados de tamanho moderado. A referência a Hastie e Tibshirani (1990) [^5] fornece mais detalhes sobre o algoritmo e suas propriedades.

### Referências

[^2]: Page 296: "For two-class classification, recall the logistic regression model for binary data discussed in Section 4.4. We relate the mean of the binary response μ(X) = Pr(Y = 1|X) to the predictors via a linear regression model and the logit link function"
[^4]: Page 298: "Algorithm 9.1 The Backfitting Algorithm for Additive Models."
[^5]: Page 299: "For the logistic regression model and other generalized additive models, the appropriate criterion is a penalized log-likelihood. To maximize it, the backfitting procedure is used in conjunction with a likelihood maximizer. The usual Newton-Raphson routine for maximizing log-likelihoods in generalized linear models can be recast as an IRLS (iteratively reweighted least squares) algorithm."
[^6]: Page 300: "Algorithm 9.2 Local Scoring Algorithm for the Additive Logistic Regression Model."

<!-- END -->