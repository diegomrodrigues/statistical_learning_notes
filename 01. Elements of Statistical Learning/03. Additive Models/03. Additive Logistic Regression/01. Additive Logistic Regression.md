## Additive Logistic Regression: A Detailed Exploration

### Introdução
Este capítulo aprofunda o conceito de **Additive Logistic Regression**, um modelo estatístico poderoso para análise de dados binários. Conforme mencionado anteriormente, a regressão logística aditiva modela a probabilidade de um resultado binário dadas as variáveis preditoras, utilizando uma transformação logit para relacionar os preditores aos log-odds do resultado [^2]. Este modelo estende a regressão logística padrão, representando os log-odds de um resultado binário como uma função aditiva de funções suaves dos preditores [^2]. Este capítulo explora em detalhes a formulação matemática, a interpretação e os métodos de ajuste para modelos de regressão logística aditiva, baseando-se nos conceitos de modelos aditivos generalizados [^1].

### Conceitos Fundamentais
A regressão logística aditiva é uma extensão do modelo linear generalizado (GLM). O GLM relaciona a média da variável resposta, $\mu(X)$, a um preditor linear através de uma função de ligação $g$ [^2]:

$$g[\mu(X)] = \alpha + \beta_1 X_1 + ... + \beta_p X_p$$

Para dados binários, a função de ligação comumente usada é a função logit, resultando no modelo de regressão logística padrão [^2]:

$$log\left(\frac{\mu(X)}{1 - \mu(X)}\right) = \alpha + \beta_1 X_1 + ... + \beta_p X_p$$

Onde $\mu(X) = Pr(Y = 1|X)$ representa a probabilidade de um resultado positivo dado o vetor de preditores $X$ [^2].

A regressão logística aditiva generaliza este modelo substituindo os termos lineares $\beta_j X_j$ por funções suaves não paramétricas $f_j(X_j)$ [^2]:

$$log\left(\frac{\mu(X)}{1 - \mu(X)}\right) = \alpha + f_1(X_1) + ... + f_p(X_p)$$

Onde cada $f_j$ é uma função suave e não especificada [^2]. Essa abordagem permite que o modelo capture relações não lineares entre os preditores e o resultado, mantendo a aditividade para fins de interpretabilidade [^2]. A aditividade implica que o efeito de cada preditor pode ser avaliado independentemente dos outros, embora o efeito possa ser não linear [^2].

**Interpretação dos Componentes**

*   $\alpha$: O termo constante ou intercepto, representando o log-odds base quando todos os preditores são zero [^2].
*   $f_j(X_j)$: Uma função suave que modela o efeito não linear do preditor $X_j$ nos log-odds do resultado [^2]. A forma funcional de $f_j$ não é predefinida, permitindo que o modelo se adapte aos dados [^2].

**Funções de Ligação Clássicas**

Existem outras funções de ligação, como a identidade, utilizada para modelos lineares e aditivos para dados de resposta gaussiana:

*   $g(\mu) = \mu$ [^2]
*   A função probit, utilizada para modelar probabilidades binomiais: $g(\mu) = probit(\mu) = \Phi^{-1}(\mu)$ [^2].
*   A função log, utilizada para modelos log-lineares ou log-aditivos para dados de contagem de Poisson: $g(\mu) = log(\mu)$ [^2].

**Ajuste do Modelo**

As funções $f_j$ são estimadas de forma flexível usando um algoritmo de *backfitting* [^2]. O algoritmo de backfitting é um procedimento iterativo que estima cada função $f_j$ enquanto mantém as outras funções fixas [^4]. O algoritmo pode ser resumido da seguinte forma [^4]:

1.  Inicialize: Defina $\alpha = \frac{1}{N} \sum_{i=1}^{N} y_i$ e $f_j = 0$ para todo $i$ e $j$.
2.  Ciclo: Para $j = 1, 2, ..., p, ..., 1, 2, ..., p, ...$:
    *   $f_j \leftarrow S_j\{y_i - \alpha - \sum_{k \neq j} f_k(X_{ik})\}$
    *   $f_j \leftarrow f_j - \frac{1}{N} \sum_{i=1}^{N} f_j(X_{ij})$
3.  Repita até que as funções $f_j$ mudem menos que um limiar pré-especificado.

Onde $S_j$ é um *scatterplot smoother*, como um *cubic smoothing spline* ou um *kernel smoother* [^2]. A segunda etapa do algoritmo garante que as funções tenham média zero [^4].

Para a regressão logística aditiva, o algoritmo de backfitting é usado em conjunto com um maximizador de verossimilhança [^5]. O procedimento usual de Newton-Raphson para maximizar a verossimilhança em modelos lineares generalizados pode ser reformulado como um algoritmo IRLS (*iteratively reweighted least squares*) [^5]. No modelo aditivo generalizado, a regressão linear ponderada é simplesmente substituída por um algoritmo de *backfitting* ponderado [^5].

**Algoritmo de *Local Scoring* para a Regressão Logística Aditiva**

O algoritmo de *Local Scoring* para a regressão logística aditiva pode ser resumido da seguinte forma [^6]:

1.  Compute os valores iniciais: $\alpha = log[\bar{y}/(1 - \bar{y})]$, onde $\bar{y} = ave(y_i)$, a proporção amostral de uns, e defina $f_j = 0$ para todo $j$ [^6].
2.  Defina $\eta_i = \alpha + \sum_j f_j(x_{ij})$ e $p_i = 1/[1 + exp(-\eta_i)]$ [^6].
3.  Itere:
    *   Construa a variável *target* de trabalho: $z_i = \eta_i + \frac{(y_i - p_i)}{p_i(1 - p_i)}$ [^6].
    *   Construa os pesos: $w_i = p_i(1 - p_i)$ [^6].
    *   Ajuste um modelo aditivo aos *targets* $z_i$ com pesos $w_i$, usando um algoritmo de *backfitting* ponderado. Isso fornece novas estimativas $\hat{\alpha}, \hat{f_j}$ [^6].
4.  Continue a etapa 2 até que a mudança nas funções fique abaixo de um limite pré-especificado [^6].

### Conclusão
A regressão logística aditiva oferece uma abordagem flexível e interpretável para modelar resultados binários. Ao combinar a estrutura aditiva com funções suaves não paramétricas, o modelo pode capturar relações complexas sem sacrificar a interpretabilidade [^2]. O algoritmo de *backfitting* fornece um método computacionalmente eficiente para ajustar o modelo, enquanto o algoritmo de *local scoring* adapta o *backfitting* para a maximização da verossimilhança em um contexto de regressão logística [^6].

### Referências
[^1]: Seção 9.1, "Generalized Additive Models".
[^2]: Seção 9.1, "Generalized Additive Models".
[^4]: Algoritmo 9.1, "The Backfitting Algorithm for Additive Models".
[^5]: Seção 9.1.2, "Example: Additive Logistic Regression".
[^6]: Algoritmo 9.2, "Local Scoring Algorithm for the Additive Logistic Regression Model".
<!-- END -->