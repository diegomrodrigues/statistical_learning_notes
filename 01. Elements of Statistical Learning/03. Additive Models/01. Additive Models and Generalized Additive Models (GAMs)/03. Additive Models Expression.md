## Additive Models: A Flexible Approach to Regression

### Introdução
Este capítulo explora os **Additive Models** (Modelos Aditivos) como uma extensão dos modelos lineares, proporcionando maior flexibilidade na modelagem de relações não lineares entre preditores e a variável resposta [^9, ^295]. Como vimos anteriormente, os modelos lineares tradicionais frequentemente falham em capturar a complexidade das relações em dados reais, onde os efeitos raramente são lineares [^9, ^295]. Em contraste com as técnicas que usam funções de base predefinidas para alcançar não linearidades, os modelos aditivos empregam métodos estatísticos mais automáticos e flexíveis para identificar e caracterizar efeitos de regressão não lineares [^9, ^295]. Este capítulo se aprofundará na estrutura matemática dos modelos aditivos, seus métodos de ajuste e exemplos de aplicação.

### Conceitos Fundamentais

Um modelo aditivo generalizado, no contexto de regressão, assume a forma [^9, ^295]:

$$E(Y|X_1, X_2, ..., X_p) = a + f_1(X_1) + f_2(X_2) + ... + f_p(X_p)$$

onde:
- $Y$ representa a variável resposta.
- $X_1, X_2, ..., X_p$ são os preditores [^9, ^296].
- $a$ é o intercepto.
- $f_j$ são funções suaves e não especificadas ("não paramétricas") [^9, ^296].

[^9, ^296] destaca que, embora a forma não paramétrica das funções $f_j$ torne o modelo mais flexível, a aditividade é retida, permitindo a interpretação do modelo de maneira semelhante aos modelos lineares tradicionais.

Em situações de classificação binária, podemos usar o modelo de regressão logística para dados binários, onde a média da resposta binária $\mu(X) = Pr(Y = 1|X)$ está relacionada aos preditores por meio de um modelo de regressão linear e da função de *link* logit [^9, ^296]:

$$log\left(\frac{\mu(X)}{1 - \mu(X)}\right) = a + \beta_1 X_1 + ... + \beta_p X_p$$

O modelo de regressão logística aditivo substitui cada termo linear por uma forma funcional mais geral [^9, ^296]:

$$log\left(\frac{\mu(X)}{1 - \mu(X)}\right) = a + f_1(X_1) + ... + f_p(X_p)$$

De forma mais geral, a média condicional $\mu(X)$ de uma resposta $Y$ está relacionada a uma função aditiva dos preditores por meio de uma função de *link* $g$ [^9, ^296]:

$$g[\mu(X)] = a + f_1(X_1) + ... + f_p(X_p)$$

Exemplos de funções de *link* clássicas incluem [^9, ^296]:

*   $g(\mu) = \mu$: a função de *link* identidade, usada para modelos lineares e aditivos para dados de resposta Gaussianos [^9, ^296].
*   $g(\mu) = logit(\mu)$ ou $g(\mu) = probit(\mu)$: a função de *link* probit, para modelagem de probabilidades binomiais. A função probit é a função de distribuição cumulativa Gaussiana inversa: $probit(\mu) = \Phi^{-1}(\mu)$ [^9, ^296].
*   $g(\mu) = log(\mu)$: para modelos log-lineares ou log-aditivos para dados de contagem de Poisson [^9, ^296].

Todas as três funções de *link* surgem de modelos de amostragem de família exponencial, que, adicionalmente, incluem as distribuições gama e binomial negativa [^9, ^296]. Essas famílias geram a classe bem conhecida de modelos lineares generalizados, que são todos estendidos da mesma maneira para modelos aditivos generalizados [^9, ^296].

#### Ajustando Modelos Aditivos

As funções $f_j$ são estimadas de forma flexível, usando um algoritmo cujo bloco de construção básico é um *scatterplot smoother* [^9, ^296]. O algoritmo de *backfitting* (retroajuste) é usado para estimar simultaneamente todas as $p$ funções [^9, ^296]. Um critério como a soma penalizada dos quadrados pode ser especificado para este problema [^9, ^297]:

$$PRSS(a, f_1, f_2, ..., f_p) = \sum_{i=1}^{N} \left(Y_i - a - \sum_{j=1}^{p} f_j(X_{ij})\right)^2 + \sum_{j=1}^{p} \lambda_j \int [f\'\'_j(t_j)]^2 dt_j$$

onde $\lambda_j > 0$ são parâmetros de ajuste [^9, ^297]. Pode-se mostrar que o minimizador de (9.7) é um modelo spline cúbico aditivo; cada uma das funções $f_j$ é um spline cúbico no componente $X_j$, com nós em cada um dos valores únicos de $X_{ij}$, $i = 1,..., N$ [^9, ^297]. No entanto, sem mais restrições sobre o modelo, a solução não é única [^9, ^297]. A convenção padrão é assumir que $\frac{1}{N}\sum_{i=1}^{N}f_j(x_{ij}) = 0$ – as funções têm média zero nos dados. Vê-se facilmente que $a = \overline{ave(y_i)}$ neste caso [^9, ^297].

O Algoritmo 9.1, "O Algoritmo de Backfitting para Modelos Aditivos", fornece uma maneira de encontrar a solução [^9, ^298].

**Algoritmo 9.1 O Algoritmo de Backfitting para Modelos Aditivos** [^9, ^298].

1.  Inicialize: $a = \frac{1}{N}\sum_{i=1}^{N}y_i, f_j = 0, \forall i, j$ [^9, ^298].
2.  Ciclo: $j = 1, 2, . . ., p, . . ., 1, 2, ..., p, ...,$ [^9, ^298]
    $$f_j \leftarrow S_j \left[ \{y_i - a - \sum_{k \neq j} f_k(x_{ik}) \}_{i=1}^{N} \right]$$
    $$f_j \leftarrow f_j - \frac{1}{N} \sum_{i=1}^{N} f_j(x_{ij})$$
    até que as funções $f_j$ mudem menos do que um limite pré-especificado [^9, ^298]. $\blacksquare$

O algoritmo de *backfitting* é uma técnica iterativa onde cada função $f_j$ é atualizada sequencialmente, mantendo as outras funções fixas. Isso continua até que as mudanças nas funções $f_j$ se tornem suficientemente pequenas [^9, ^298].

### Conclusão

Os modelos aditivos oferecem uma alternativa flexível aos modelos lineares tradicionais, permitindo a modelagem de relações não lineares sem sacrificar a interpretabilidade. O algoritmo de *backfitting* fornece um meio prático de ajustar esses modelos, e a escolha de funções de *link* apropriadas permite que os modelos aditivos sejam aplicados a uma variedade de tipos de dados de resposta [^9, ^296]. Embora os modelos aditivos tenham limitações, como a dificuldade em capturar interações complexas, eles permanecem uma ferramenta valiosa na análise de dados e modelagem estatística [^9, ^304].

### Referências
[^9]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer science & business media.
[^295]: Ibid, page 295.
[^296]: Ibid, page 296.
[^297]: Ibid, page 297.
[^298]: Ibid, page 298.
[^304]: Ibid, page 304.
<!-- END -->