## Hierarchical Mixtures of Experts: A Promising Competitor to CART Trees

### Introdução
Este capítulo explora o modelo de **Hierarchical Mixtures of Experts (HME)**, apresentando-o como uma alternativa promissora às árvores CART [^35]. O HME, com suas "soft splits" [^37], oferece uma abordagem distinta em relação às "hard decision rules" [^37] das árvores CART, permitindo capturar transições graduais nas respostas. Além disso, a função de log-verossimilhança do HME é suave em relação aos pesos desconhecidos, facilitando a otimização numérica [^37].

### Conceitos Fundamentais

O HME pode ser visto como uma variante dos métodos baseados em árvores [^35]. A principal diferença reside no fato de que as divisões na árvore não são decisões rígidas, mas sim probabilidades suaves [^35]. Em cada nó, uma observação segue para a esquerda ou para a direita com probabilidades que dependem de seus valores de entrada [^35]. Essa característica oferece vantagens computacionais, uma vez que o problema de otimização dos parâmetros resultante é suave, ao contrário da busca discreta por pontos de divisão nas árvores CART [^35]. Além disso, as divisões suaves podem auxiliar na precisão da predição e fornecer uma descrição alternativa dos dados [^35].

#### Arquitetura do Modelo HME
Um modelo HME simples de dois níveis é ilustrado na Figura 9.13 [^36]. Ele pode ser interpretado como uma árvore com divisões suaves em cada nó não terminal [^35]. No entanto, a terminologia utilizada pelos criadores dessa metodologia é diferente: os nós terminais são chamados de **experts**, e os nós não terminais são chamados de **gating networks** [^35]. A ideia é que cada expert forneça uma opinião (previsão) sobre a resposta, e essas opiniões são combinadas pelas gating networks [^35].

#### Formulação Matemática do HME
Formalmente, o HME é um modelo de mistura [^35]. A gating network superior tem a seguinte saída:
$$ng_j(x, \gamma_j) = \frac{e^{\gamma_j^T x}}{\sum_{k=1}^K e^{\gamma_k^T x}}, \quad j = 1, 2, ..., K,$$
onde cada $\gamma_j$ é um vetor de parâmetros desconhecidos [^36]. Isso representa uma divisão suave em K vias (K = 2 na Figura 9.13) [^36]. Cada $g_j(x, \gamma_j)$ é a probabilidade de atribuir uma observação com vetor de características $x$ ao *j*-ésimo ramo [^36]. Observe que, com K = 2 grupos, se tomarmos o coeficiente de um dos elementos de $x$ como $+\infty$, obtemos uma curva logística com inclinação infinita. Nesse caso, as probabilidades de gating são 0 ou 1, correspondendo a uma divisão rígida nessa entrada [^36].
Nos níveis seguintes, as gating networks têm uma forma similar:
$$ng_{l|j}(x, \gamma_{jl}) = \frac{e^{\gamma_{jl}^T x}}{\sum_{k=1}^K e^{\gamma_{jk}^T x}}, \quad l = 1, 2, ..., K.$$
Essa é a probabilidade de atribuição ao *l*-ésimo ramo, dada a atribuição ao *j*-ésimo ramo no nível acima [^37].

Em cada expert (nó terminal), temos um modelo para a variável de resposta da forma:
$$nY \sim Pr(y|x, \theta_{jl}).$$
Isso difere de acordo com o problema [^37].
*   **Regressão:** O modelo de regressão linear Gaussiano é usado, com $\theta_{jl} = (\beta_{jl}, \sigma_{jl}^2)$:
    $$     Y = \beta_{jl}^T x + \epsilon \quad \text{e} \quad \epsilon \sim N(0, \sigma_{jl}^2).\     $$
*   **Classificação:** O modelo de regressão logística linear é usado:
    $$     Pr(Y = 1|x, \theta_{jl}) = \frac{1}{1 + e^{-\theta_{jl}^T x}}.\     $$

Denotando a coleção de todos os parâmetros por $\Psi = \{\gamma_j, \gamma_{jl}, \theta_{jl}\}$, a probabilidade total de que $Y = y$ é:
$$nPr(y|x, \Psi) = \sum_{j=1}^K g_j(x, \gamma_j) \sum_{l=1}^K g_{l|j}(x, \gamma_{jl}) Pr(y|x, \theta_{jl}).$$
Este é um modelo de mistura, com as probabilidades de mistura determinadas pelos modelos de gating network [^37].

#### Estimação de Parâmetros
Para estimar os parâmetros, maximizamos a log-verossimilhança dos dados, $\sum_i \log Pr(y_i|x_i, \Psi)$, sobre os parâmetros em $\Psi$ [^37]. O método mais conveniente para fazer isso é o algoritmo EM [^37]. Definimos variáveis latentes $\Delta_j$, todas as quais são zero, exceto por uma única [^37]. Interpretamos estas como as decisões de ramificação tomadas pela gating network de nível superior [^37]. Similarmente, definimos variáveis latentes $\Delta_{elj}$ para descrever as decisões de gating no segundo nível [^37].

No passo E, o algoritmo EM calcula as expectativas de $\Delta_j$ e $\Delta_{elj}$ dados os valores correntes dos parâmetros [^37]. Estas expectativas são então usadas como pesos de observação no passo M do procedimento, para estimar os parâmetros nas expert networks [^37]. Os parâmetros nos nós internos são estimados por uma versão de regressão logística múltipla [^37]. As expectativas de $\Delta_j$ ou $\Delta_{elj}$ são perfis de probabilidade, e estes são usados como os vetores de resposta para estas regressões logísticas [^37].

### Conclusão
O HME é uma alternativa promissora às árvores CART [^37]. Ao utilizar divisões suaves em vez de regras de decisão rígidas, ele pode capturar situações onde a transição de baixa para alta resposta é gradual [^37]. A log-verossimilhança é uma função suave dos pesos desconhecidos e, portanto, é adequada para otimização numérica [^37]. O modelo é similar ao CART com divisões de combinação linear, mas o último é mais difícil de otimizar [^37].

### Referências
[^35]: Page 295, Capítulo 9, "Additive Models, Trees, and Related Methods"
[^36]: Page 330, Capítulo 9, "Additive Models, Trees, and Related Methods"
[^37]: Page 331, Capítulo 9, "Additive Models, Trees, and Related Methods"
<!-- END -->