## 9.5.1 Two-Level HME Model: Gating and Expert Networks

This chapter expands on methods for supervised learning, moving from generalized additive models and trees to hierarchical mixtures of experts (HME). HME models provide a probabilistic, tree-like approach to regression and classification. This section focuses on understanding the structure and components of a two-level HME model.

### Introduction

As introduced in [^355], the hierarchical mixtures of experts (HME) procedure can be viewed as a variant of tree-based methods. The key innovation lies in replacing "hard" splits with "soft" probabilistic ones, where each observation is routed through the tree with probabilities determined by its input features. This approach enables smooth parameter optimization and provides an alternative way of describing the data [^329].

### Two-Level HME Model

A **two-level HME model** consists of a **top gating network** and **expert networks**. The top gating network determines the initial routing probabilities, while the expert networks provide predictions based on the data they receive.

1.  **Top Gating Network:** The top gating network has an output $g_j(x, \gamma_j)$ [^330], where:

    *   $x$ is the input vector.
    *   $\gamma_j$ is a vector of unknown parameters associated with the $j$-th branch.
    *   $j = 1, 2, ..., K$, where $K$ is the number of branches at the top level.
    *   $g_j(x, \gamma_j)$ represents the probability of assigning an observation with feature vector $x$ to the $j$-th branch [^330]. The gating networks provide a soft K-way split [^330].

    The output of the top gating network is defined as:
    $$     g_j(x, \gamma_j) = \frac{e^{\gamma_j^T x}}{\sum_{k=1}^{K} e^{\gamma_k^T x}}, \quad j = 1, 2, ..., K.     $$
    This equation is analogous to a softmax function, ensuring that the outputs are probabilities that sum to one.
2.  **Expert Networks:** Each expert network resides at a terminal node of the HME "tree" [^329]. The expert networks have a similar form as the gating network, but their role is to provide predictions rather than routing probabilities [^330]. Each expert network provides an *opinion (prediction) about the response* [^329].

### Implications

The use of "soft" splits, governed by probabilities, is a defining characteristic of HME models. This contrasts with the "hard" splits of CART trees, where data is definitively assigned to one branch or another [^329]. If the coefficient of one of the elements of $x$ tends to $+\infty$ for K = 2 groups, then we get a logistic curve with infinite slope and hard split occurs [^330].

### Conclusion

The two-level HME model lays the foundation for more complex hierarchical structures. By combining gating networks for probabilistic routing with expert networks for prediction, HMEs offer a flexible approach to modeling complex data relationships. The architecture of HME models allows for extensions to multiple levels, leading to more intricate models [^329].

### References

[^329]: Section 9.5 Hierarchical Mixtures of Experts
[^330]: Section 9.5 Hierarchical Mixtures of Experts

<!-- END -->