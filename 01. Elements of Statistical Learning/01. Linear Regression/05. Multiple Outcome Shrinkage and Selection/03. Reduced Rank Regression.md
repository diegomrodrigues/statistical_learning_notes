## Reduced-Rank Regression: Combining Regression and Dimensionality Reduction

### Introdução
Este capítulo se aprofunda na técnica de **Reduced-Rank Regression (RRR)**, um método que integra regressão linear com redução de dimensionalidade através da imposição de uma restrição de *rank* na matriz de coeficientes [^42]. Essa restrição reduz o número de parâmetros a serem estimados, melhorando a estabilidade e a interpretabilidade do modelo [^42]. O RRR é particularmente útil em cenários de *multiple outcome shrinkage and selection*, onde o objetivo é prever múltiplos resultados simultaneamente, aproveitando as correlações entre eles para aprimorar a precisão e a eficiência do modelo.

### Conceitos Fundamentais
O RRR realiza uma regressão linear na matriz de respostas agrupadas $Y$, e então mapeia os coeficientes de volta para o espaço de resposta original [^42]. A ideia central é que, ao impor uma restrição de *rank* na matriz de coeficientes $B$, estamos efetivamente reduzindo a dimensionalidade do espaço de resposta, o que pode levar a um modelo mais parcimonioso e generalizável [^42].

Formalmente, considere o modelo de regressão linear multivariada:
$$Y = XB + E$$
onde:
- $Y$ é uma matriz $N \times K$ de respostas, com $N$ observações e $K$ resultados.
- $X$ é uma matriz $N \times (p+1)$ de preditores, incluindo um intercepto.
- $B$ é uma matriz $(p+1) \times K$ de coeficientes.
- $E$ é uma matriz $N \times K$ de erros.

Na regressão de *rank* reduzido, impomos a restrição de que o *rank* da matriz $B$ é igual a $m$, onde $m < \min(p+1, K)$. Isso significa que $B$ pode ser decomposto como o produto de duas matrizes:
$$B = UV^T$$
onde:
- $U$ é uma matriz $(p+1) \times m$.
- $V$ é uma matriz $K \times m$.

O problema de otimização torna-se então:
$$min_{U, V} ||Y - XUV^T||_F^2$$
onde $||\cdot||_F$ denota a norma de Frobenius.

O RRR *borrows strength among responses by truncating the CCA* [^42]. Em outras palavras, o RRR se beneficia da informação compartilhada entre as respostas, truncando a análise de correlação canônica (CCA). A CCA identifica sequências de combinações lineares não correlacionadas $Xv_m$ e $Yu_m$, com $m = 1, \dots, M$, de forma que as correlações $Corr^2(Yu_m, Xv_m)$ são maximizadas sucessivamente [^42].

O modelo de regressão de *rank* reduzido restrito pode ser escrito como [^43]:
$$ \begin{aligned} & \underset{B^{rr}(m)}{\text{argmin}} \sum_{i=1}^N (Y_i - B^T x_i)^T \Sigma^{-1} (Y_i - B^T x_i) \\\\ & \text{rank}(B) = m \end{aligned} $$

[^43] mostra que a solução é dada por uma CCA de $Y$ e $X$:
$$ B^{rr}(m) = B U_m U_m^T $$
onde $U_m$ é a submatriz $K \times m$ de $U$, consistindo das primeiras $m$ colunas, e $U$ é a matriz $K \times M$ de vetores canônicos esquerdos $u_1, u_2, \dots, u_M$. [^43] também mostra que a solução de *rank* reduzido pode ser expressa como:
$$ \hat{Y}^{rr}(m) = X (X^T X)^{-1} X^T Y U_m U_m^T = H Y P_M $$
onde $H$ é o operador de projeção de regressão linear usual, e $P_M$ é o operador de projeção de resposta CCA de *rank* $m$.
<!-- END -->