## 3.2.3 Generalização da Regressão Múltipla a Partir da Regressão Univariada Simples

### Introdução
Este capítulo explora a generalização da regressão múltipla a partir da regressão univariada simples, detalhando o Algoritmo 3.1 e sua representação em forma de decomposição QR [^52]. Este algoritmo fornece uma maneira de entender e calcular os coeficientes na regressão múltipla através de uma série de regressões univariadas, onde cada entrada é ortogonalizada em relação às entradas anteriores.

### Conceitos Fundamentais

O modelo linear geral, apresentado na forma $$f(x) = \beta_0 + \sum_{j=1}^p X_j\beta_j$$ [^44], onde $X = (X_1, X_2, ..., X_p)$ é o vetor de entrada e $\beta$ os coeficientes a serem estimados [^44].
Em uma regressão univariada simples, temos um único preditor, e o coeficiente é calculado diretamente [^52]. No entanto, em regressão múltipla, os preditores podem ser correlacionados, complicando a interpretação direta dos coeficientes [^52]. O Algoritmo 3.1, "Regression by Successive Orthogonalization" [^54], oferece uma solução para este problema, ortogonalizando sucessivamente os preditores.

**Algoritmo 3.1: Regressão por Ortogonalização Sucessiva** [^54]

1.  Inicializar $z_0 = x_0 = 1$ [^54].
2.  Para $j = 1, 2, ..., p$ [^54]:
    *   Regredir $x_j$ em $z_0, z_1, ..., z_{j-1}$ para produzir os coeficientes $\gamma_{\ell j} = (z_\ell, x_j) / (z_\ell, z_\ell)$, para $\ell = 0, ..., j-1$, e o vetor residual $z_j = x_j - \sum_{\ell=0}^{j-1} \gamma_{\ell j} z_\ell$ [^54].
3.  Regredir $y$ no residual $z_p$ para obter a estimativa $\beta_p$ [^54].

A representação do passo 2 do Algoritmo 3.1 em forma de matriz é dada por $X = Z\Gamma$ [^55], onde $Z$ tem as colunas ortogonais $z_j$, e $\Gamma$ é uma matriz triangular superior [^55]. Introduzindo a matriz diagonal $D$ com a $j$-ésima entrada diagonal $D_{jj} = ||z_j||$, a decomposição QR de $X$ é obtida como $X = ZD^{-1}DF = QR$ [^55]. Aqui, $Q = ZD^{-1}$ é uma matriz ortogonal e $R = D\Gamma$ é uma matriz triangular superior [^55].

A decomposição QR fornece uma base ortogonal conveniente para o espaço das colunas de $X$ [^55]. A solução de mínimos quadrados pode então ser expressa como $\beta = R^{-1}Q^Ty$ e os valores ajustados como $\hat{y} = QQ^Ty$ [^55].

**Detalhes Matemáticos**

A ortogonalização sucessiva garante que cada $z_j$ seja ortogonal a todos os $z_i$ para $i < j$ [^54]. Isso simplifica o cálculo dos coeficientes de regressão, pois cada $\beta_j$ pode ser calculado independentemente como $\beta_j = (z_j, y) / (z_j, z_j)$ [^54]. A matriz $\Gamma$ contém os coeficientes $\gamma_{\ell j}$ que descrevem como cada $x_j$ é construído a partir dos $z_i$ [^55].

**Lema 1:** Os vetores $z_j$ são ortogonais entre si.

*Prova:* Por construção, cada $z_j$ é o resultado da remoção das projeções de $x_j$ nos vetores $z_0, z_1, ..., z_{j-1}$ [^54]. Portanto, $z_j$ é ortogonal a cada um desses vetores [^54]. $\blacksquare$

**Corolário 1:** A matriz $Z^TZ$ é uma matriz diagonal com entradas $||z_j||^2$ na diagonal.

A variância de $\beta_p$ é dada por $Var(\beta_p) = \frac{\sigma^2}{||z_p||^2}$ [^55], onde $\sigma^2$ é a variância do erro. Isso mostra que a precisão com que podemos estimar $\beta_p$ depende do comprimento do vetor residual $z_p$ [^55].

### Conclusão

O Algoritmo 3.1 fornece uma maneira eficiente e intuitiva de realizar regressão múltipla, especialmente quando há correlação entre os preditores [^54]. A decomposição QR resultante simplifica o cálculo dos coeficientes e fornece insights sobre a estabilidade e precisão das estimativas [^55]. Este método é particularmente útil em situações onde a interpretação dos coeficientes individuais é crucial, pois a ortogonalização facilita a compreensão da contribuição única de cada preditor [^55].

### Referências
[^44]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^52]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^54]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^55]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
<!-- END -->