## 3.2.3 Dissecando a Regressão Múltipla: Uma Perspectiva Univariada

### Introdução
Este capítulo explora a regressão linear múltipla sob uma nova perspectiva, demonstrando como ela pode ser decomposta e compreendida em termos de regressões univariadas mais simples [^52]. Em particular, examinaremos como a **ortogonalização dos inputs** permite que a regressão múltipla seja interpretada como uma sequência de regressões univariadas, cada uma ajustando o output com relação a um input **ortogonalizado**.

### Conceitos Fundamentais

A regressão linear múltipla, conforme definida em [^52], modela a relação entre um output $Y$ e múltiplos inputs $X_1, X_2, ..., X_p$. A forma geral do modelo é dada por:

$$f(x) = \beta_0 + \sum_{j=1}^{p} X_j\beta_j$$

onde $\beta_j$ são os coeficientes a serem estimados. A abordagem tradicional para estimar esses coeficientes envolve a minimização da soma dos quadrados residuais (RSS):

$$RSS(\beta) = \sum_{i=1}^{N}(Y_i - f(x_i))^2$$

Entretanto, [^52] propõe uma interpretação alternativa através da **ortogonalização dos inputs**. A ideia central é transformar os inputs originais em um conjunto de inputs **ortogonais**, onde a correlação entre cada par de inputs é zero. Quando os inputs são ortogonais, os coeficientes $\beta_j$ podem ser estimados independentemente através de regressões univariadas simples.

O processo de ortogonalização pode ser realizado através do seguinte algoritmo [^53]:

1.  Regredir $x$ em $1$ para produzir o resíduo $z = x - \bar{x}1$, onde $\bar{x}$ é a média de $x$ e $1$ é um vetor de uns.
2.  Regredir $y$ em $z$ para obter o coeficiente $\beta_1$.

Este processo é uma aplicação da regressão linear simples, como definida em [^53]:

$$hat{\beta} = \frac{(x, y)}{(x, x)}$$

onde $(x, y)$ denota o produto interno entre os vetores $x$ e $y$.

De forma mais geral, o Algoritmo 3.1 [^54] descreve a **Regressão por Ortogonalização Sucessiva** para $p$ inputs:

1.  Inicializar $z_0 = x_0 = 1$.
2.  Para $j = 1, 2, ..., p$:
    *   Regredir $x_j$ em $z_0, z_1, ..., z_{j-1}$ para produzir coeficientes $\gamma_{\ell j} = \frac{(z_{\ell}, x_j)}{(z_{\ell}, z_{\ell})}$, $\ell = 0, ..., j-1$ e vetor resíduo $z_j = x_j - \sum_{\ell=0}^{j-1} \gamma_{\ell j} z_{\ell}$.
3.  Regredir $y$ no resíduo $z_p$ para obter a estimativa $\hat{\beta}_p$.

**Lemma:** Os inputs $z_0, ..., z_{j-1}$ no passo 2 do Algoritmo 3.1 são ortogonais.

*Prova:* Por construção, cada $z_j$ é o resíduo da regressão de $x_j$ em $z_0, ..., z_{j-1}$. Portanto, $z_j$ é ortogonal a todos os $z_{\ell}$ para $\ell < j$. $\blacksquare$

**Corolário:** Os coeficientes calculados no passo 2 do Algoritmo 3.1 são também os coeficientes de regressão múltipla [^54].

### Conclusão

Este capítulo demonstrou que a regressão linear múltipla pode ser entendida como uma série de regressões univariadas, desde que os inputs sejam ortogonalizados. A ortogonalização, como implementada no Algoritmo 3.1, permite decompor o problema original em subproblemas mais simples, facilitando a interpretação e o cálculo dos coeficientes. Este insight é particularmente útil em cenários onde a multicolinearidade dificulta a interpretação dos coeficientes na regressão múltipla tradicional.

### Referências
[^52]: Page 43, "Linear Methods for Regression"
[^53]: Page 53, "3.2.3 Multiple Regression from Simple Univariate Regression"
[^54]: Page 54, "Algorithm 3.1 Regression by Successive Orthogonalization."
<!-- END -->