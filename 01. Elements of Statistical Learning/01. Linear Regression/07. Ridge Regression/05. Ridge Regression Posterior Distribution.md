## Ridge Regression como Média ou Moda de uma Distribuição Posterior

### Introdução
Este capítulo aprofunda-se na interpretação bayesiana da **Ridge Regression**, demonstrando como ela pode ser derivada como a média ou a moda de uma distribuição *a posteriori* com uma distribuição *a priori* adequadamente escolhida para os parâmetros. Esta perspectiva oferece uma compreensão mais profunda da **Ridge Regression** como um método de regularização que incorpora crenças *a priori* sobre a magnitude dos coeficientes.

### Conceitos Fundamentais

A **Ridge Regression**, como vimos anteriormente [^61], minimiza a soma dos quadrados residuais penalizada pela norma L2 dos coeficientes:

$$ \hat{\beta}^{\text{ridge}} = \underset{\beta}{\text{argmin}} \left\{ \sum_{i=1}^{N} (y_i - x_i^T \beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\} \qquad (3.41) $$

Onde $\lambda \geq 0$ é o parâmetro de regularização.

**Interpretação Bayesiana:**
A interpretação bayesiana da **Ridge Regression** considera os coeficientes $\beta_j$ como variáveis aleatórias com uma distribuição *a priori*. Especificamente, assumimos que cada $\beta_j$ é distribuído independentemente como $N(0, \tau^2)$ [^1]. Isso implica que, *a priori*, esperamos que os coeficientes sejam próximos de zero, com uma variância $\tau^2$ que controla a força dessa crença.

A densidade *a priori* conjunta dos coeficientes é então:

$$ p(\beta) = \prod_{j=1}^{p} \frac{1}{\sqrt{2\pi\tau^2}} \exp\left(-\frac{\beta_j^2}{2\tau^2}\right) = (2\pi\tau^2)^{-p/2} \exp\left(-\frac{1}{2\tau^2} \sum_{j=1}^{p} \beta_j^2\right) $$

Assumindo que os dados $y_i$ são independentes e normalmente distribuídos em torno de $x_i^T\beta$ com variância $\sigma^2$, a função de verossimilhança é:

$$ p(y|\beta, X) = (2\pi\sigma^2)^{-N/2} \exp\left(-\frac{1}{2\sigma^2} \sum_{i=1}^{N} (y_i - x_i^T \beta)^2\right) $$

A distribuição *a posteriori* de $\beta$ é proporcional ao produto da verossimilhança e da *a priori*:

$$ p(\beta|y, X) \propto p(y|\beta, X) p(\beta) $$

Tomando o logaritmo negativo da densidade *a posteriori*, obtemos (assumindo $\tau^2$ e $\sigma^2$ conhecidos):

$$ -\log p(\beta|y, X) \propto \frac{1}{2\sigma^2} \sum_{i=1}^{N} (y_i - x_i^T \beta)^2 + \frac{1}{2\tau^2} \sum_{j=1}^{p} \beta_j^2 $$

Multiplicando por $2\sigma^2$, obtemos:

$$ \sum_{i=1}^{N} (y_i - x_i^T \beta)^2 + \frac{\sigma^2}{\tau^2} \sum_{j=1}^{p} \beta_j^2 $$

Comparando com a equação (3.41), vemos que minimizar o logaritmo negativo da densidade *a posteriori* é equivalente a minimizar a função objetivo da **Ridge Regression**, com $\lambda = \frac{\sigma^2}{\tau^2}$ [^1].

**Média e Moda:**
Como a distribuição *a priori* e a verossimilhança são Gaussianas, a distribuição *a posteriori* também é Gaussiana. Em distribuições Gaussianas, a média e a moda coincidem. Portanto, a solução da **Ridge Regression** (3.44):

$$ \hat{\beta}^{\text{ridge}} = (X^TX + \lambda I)^{-1}X^Ty $$

é tanto a média quanto a moda da distribuição *a posteriori* de $\beta$ [^44].

**Interpretação de $\lambda$:**
O parâmetro de regularização $\lambda$ controla a relação entre a variância do ruído ($\sigma^2$) e a variância *a priori* dos coeficientes ($\tau^2$). Um valor grande de $\lambda$ (correspondendo a uma pequena $\tau^2$) indica uma forte crença *a priori* de que os coeficientes são próximos de zero, resultando em maior regularização.

**Observações:**
- Esta derivação assume que $\sigma^2$ e $\tau^2$ são conhecidos. Na prática, esses parâmetros são geralmente estimados a partir dos dados, por exemplo, usando métodos de máxima verossimilhança empírica ou abordagens hierárquicas bayesianas.
- A escolha da distribuição *a priori* é crucial. Diferentes distribuições *a priori* levam a diferentes métodos de regularização. Por exemplo, usar uma distribuição de Laplace *a priori* leva ao **Lasso** [^68].

### Conclusão

A interpretação bayesiana da **Ridge Regression** oferece uma perspectiva valiosa sobre o papel da regularização. Ao incorporar uma distribuição *a priori* sobre os coeficientes, a **Ridge Regression** combina informações dos dados com crenças *a priori*, levando a estimativas mais robustas e generalizáveis, especialmente em situações com alta dimensionalidade ou multicolinearidade. Esta interpretação também fornece uma base para comparar a **Ridge Regression** com outros métodos de regularização, como o **Lasso**, que correspondem a diferentes escolhas de distribuições *a priori*.

### Referências
[^1]: Seção 3.4.1
[^61]: Seção 3.4
[^44]: Equação (3.44)
[^68]: Seção 3.4.2
<!-- END -->