## 3.4.1 Ridge Regression: Alívio da Multicolinearidade e Padronização

### Introdução
Este capítulo explora em detalhes a **Ridge Regression**, uma técnica crucial para lidar com problemas de multicolinearidade em modelos de regressão linear [^1]. A Ridge Regression, ao introduzir um viés controlado, oferece uma alternativa robusta ao método dos mínimos quadrados, especialmente quando as variáveis preditoras são altamente correlacionadas. Como mencionado anteriormente no contexto de seleção de modelos [^1], o **trade-off entre viés e variância** é fundamental para a construção de modelos preditivos eficazes. Este capítulo aprofundará esse conceito no contexto específico da Ridge Regression.

### Conceitos Fundamentais
A regressão linear padrão busca minimizar a soma dos quadrados dos resíduos (**RSS**), conforme definido na equação (3.2) [^2]:

$$
RSS(\beta) = \sum_{i=1}^{N} (y_i - f(x_i))^2
$$

onde $y_i$ são os valores observados, $f(x_i)$ são os valores preditos pelo modelo linear, e $\beta$ são os coeficientes do modelo. No entanto, quando as variáveis preditoras são altamente correlacionadas, a matriz **XTX** torna-se quase singular, levando a coeficientes de regressão mal determinados e com alta variância [^3].

A **Ridge Regression** resolve esse problema adicionando um termo de penalidade à função de custo, restringindo o tamanho dos coeficientes [^1]. A função de custo modificada é dada por:

$$
J(\beta) = \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
$$

onde $\lambda \geq 0$ é o **parâmetro de regularização**, que controla a intensidade da penalidade. Quanto maior o valor de $\lambda$, maior a penalidade sobre o tamanho dos coeficientes, resultando em maior *shrinkage* (encolhimento) dos coeficientes em direção a zero [^4]. O termo $\lambda \sum_{j=1}^{p} \beta_j^2$ é conhecido como **penalidade L2** ou *ridge penalty*.

Uma formulação equivalente do problema de Ridge Regression é:

$$
\underset{\beta}{argmin} \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij}\beta_j)^2
$$

$$
\text{subject to} \sum_{j=1}^{p} \beta_j^2 \leq t
$$

onde $t$ é um parâmetro que controla o tamanho máximo permitido para a soma dos quadrados dos coeficientes [^4]. Existe uma correspondência biunívoca entre $\lambda$ e $t$ [^4].

A solução para o problema de Ridge Regression é dada por [^4]:

$$
\hat{\beta}^{ridge} = (X^TX + \lambda I)^{-1}X^Ty
$$

onde $I$ é a matriz identidade $p \times p$. A adição de $\lambda I$ à matriz $X^TX$ garante que a matriz resultante seja não-singular, mesmo que $X^TX$ seja singular, resolvendo o problema de multicolinearidade [^4].

**Importante**: *A Ridge Regression não é equivariante sob o escalonamento das entradas [^1]*. Isso significa que se as variáveis preditoras forem medidas em diferentes escalas, os resultados da Ridge Regression podem ser afetados. Para mitigar esse problema, é prática comum **padronizar as entradas** antes de aplicar a Ridge Regression [^1]. A padronização envolve subtrair a média e dividir pelo desvio padrão de cada variável preditora, garantindo que todas as variáveis tenham média zero e variância um.

#### Derivação Bayesiana
A Ridge Regression também pode ser interpretada como uma solução Bayesiana [^4]. Se assumirmos que os erros têm distribuição normal com média zero e variância $\sigma^2$, e que os coeficientes $\beta_j$ têm uma distribuição normal com média zero e variância $\tau^2$, então a Ridge Regression corresponde à estimativa de máxima a posteriori (MAP) dos coeficientes [^4]. Neste caso, $\lambda = \frac{\sigma^2}{\tau^2}$ [^4].

#### Singular Value Decomposition (SVD)
A análise da **Singular Value Decomposition (SVD)** da matriz de entrada $X$ fornece *insights* valiosos sobre o efeito da Ridge Regression [^4]. A SVD de $X$ é dada por:

$$
X = UDV^T
$$

onde $U$ e $V$ são matrizes ortogonais e $D$ é uma matriz diagonal contendo os valores singulares $d_1 \geq d_2 \geq \dots \geq d_p \geq 0$ de $X$. Usando a SVD, a solução da Ridge Regression pode ser expressa como [^4]:

$$
\hat{\beta}^{ridge} = \sum_{j=1}^{p} u_j \frac{d_j^2}{d_j^2 + \lambda} u_j^T y
$$

onde $u_j$ são as colunas de $U$. Essa expressão revela que a Ridge Regression encolhe as coordenadas de $y$ em relação à base ortonormal $u_j$ pelos fatores $\frac{d_j^2}{d_j^2 + \lambda}$ [^4]. Valores singulares pequenos $d_j$ correspondem a direções de baixa variância em $X$, e essas direções são encolhidas mais fortemente pela Ridge Regression [^4].

### Conclusão

A Ridge Regression oferece uma abordagem eficaz para lidar com a multicolinearidade em modelos de regressão linear [^1]. Ao introduzir um viés controlado, a Ridge Regression reduz a variância dos coeficientes, levando a previsões mais estáveis e generalizáveis [^1]. A padronização das entradas é crucial para garantir que a Ridge Regression seja aplicada de forma justa a todas as variáveis preditoras [^1]. A interpretação Bayesiana e a análise da SVD fornecem *insights* adicionais sobre o funcionamento interno da Ridge Regression e seus efeitos sobre os coeficientes e as direções de variância dos dados [^4].

### Referências
[^1]: Page 43-64, "Linear Methods for Regression"
[^2]: Page 44, equation (3.2)
[^3]: Page 61, section 3.4.1
[^4]: Page 63-64, section 3.4.1
<!-- END -->