## Aprofundando no Algoritmo LAR e sua Relação com o Lasso

### Introdução
Este capítulo visa aprofundar a compreensão do **Least Angle Regression (LAR)**, um algoritmo introduzido por Efron et al. (2004) [^81]. Exploraremos a conexão intrínseca entre o LAR e o **Lasso**, bem como sua relação com outros algoritmos de seleção de variáveis. O LAR pode ser visto como uma versão "democrática" do *forward stepwise regression* [^81], oferecendo uma abordagem alternativa para a construção de modelos lineares.

### Conceitos Fundamentais
#### O Algoritmo LAR
O algoritmo **LAR** opera de forma iterativa, similar ao *forward stepwise regression* [^81].  Inicialmente, todos os coeficientes são definidos como zero, e o algoritmo identifica a variável mais correlacionada com o resíduo atual [^81]. Em vez de ajustar completamente essa variável, o LAR move o coeficiente dessa variável continuamente em direção ao seu valor de mínimos quadrados, diminuindo a correlação com o resíduo [^81]. Quando outra variável "alcança" em termos de correlação com o resíduo, o processo é pausado [^81]. A segunda variável se junta ao conjunto ativo, e seus coeficientes são movidos juntos de forma a manter suas correlações amarradas e decrescentes [^81]. Este processo continua até que todas as variáveis estejam no modelo, terminando no ajuste de mínimos quadrados completo [^81].

O Algoritmo 3.2 [^82] detalha os passos do LAR:
1.  Padronize os preditores para ter média zero e norma unitária. Inicialize o resíduo $r = y - \bar{y}$, e os coeficientes $\beta_1, \beta_2, ..., \beta_p = 0$.
2.  Encontre o preditor $x_j$ mais correlacionado com $r$.
3.  Mova $\beta_j$ de 0 em direção ao seu coeficiente de mínimos quadrados $(x_j, r)$, até que algum outro competidor $x_k$ tenha tanta correlação com o resíduo atual quanto $x_j$.
4.  Mova $\beta_j$ e $\beta_k$ na direção definida pelo seu coeficiente conjunto de mínimos quadrados do resíduo atual em $(x_j, x_k)$, até que algum outro competidor $x_l$ tenha tanta correlação com o resíduo atual.
5.  Continue desta forma até que todos os $p$ preditores tenham sido inseridos. Após min$(N-1, p)$ passos, chegamos à solução de mínimos quadrados completa.

#### A Direção do Passo LAR
A direção do passo LAR é dada por:
$$ \delta_k = (X_A^T X_A)^{-1} X_A^T r_k $$
onde $A_k$ é o conjunto ativo de variáveis no início do *k*-ésimo passo, $X_A$ é a matriz das variáveis em $A_k$ e $r_k$ é o resíduo atual [^82].

#### Conexão com o Lasso
Efron percebeu que o Algoritmo 3.2 era uma implementação de FS0, permitindo que cada preditor empatado tivesse a chance de atualizar seus coeficientes de forma equilibrada, permanecendo empatados em correlação [^88]. A modificação equivale a um ajuste de mínimos quadrados não negativo, mantendo os sinais dos coeficientes os mesmos que os das correlações [^88].

#### Modificação LAR para o Lasso (Algoritmo 3.2b)
A modificação do Algoritmo LAR para implementar o FSo [^88] envolve a resolução do seguinte problema de mínimos quadrados restritos:
$$ \min_b ||r - X_A b||^2 \quad \text{sujeito a} \quad b_j s_j \geq 0, j \in A $$
onde $s_j$ é o sinal de $(x_j, r)$ [^88].

Esta modificação resulta em um ajuste de mínimos quadrados não negativo, mantendo os sinais dos coeficientes os mesmos que os das correlações [^88]. Pode-se mostrar que isso alcança o balanceamento ótimo de "atualizações" infinitesimais para as variáveis empatadas para correlação máxima [^88].

#### Interpretação Geométrica
O nome "least angle" surge de uma interpretação geométrica do processo. O novo vetor de ajuste $u_k$ faz o menor (e igual) ângulo com cada um dos preditores em $A_k$ [^82].

### Conclusão
O algoritmo LAR oferece uma abordagem interessante e eficiente para a seleção de variáveis em modelos lineares. Sua conexão com o Lasso, juntamente com a modificação para implementar o FSo, fornece *insights* valiosos sobre o comportamento desses algoritmos e suas propriedades de otimização. A capacidade de calcular todo o caminho do Lasso com a mesma ordem de computação que um único ajuste de mínimos quadrados torna o LAR uma ferramenta poderosa na modelagem estatística.
### Referências
[^81]: Seção 3.4.4
[^82]: Seção 3.4.4
[^88]: Seção 3.8

<!-- END -->