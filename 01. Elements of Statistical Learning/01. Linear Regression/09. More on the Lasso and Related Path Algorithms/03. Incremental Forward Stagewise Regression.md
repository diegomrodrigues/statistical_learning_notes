## Incremental Forward Stagewise Regression
### Introdução
Este capítulo aprofunda o conceito de **Incremental Forward Stagewise Regression (FSε)**, um algoritmo computacionalmente eficiente para ajustar modelos lineares esparsos. Como uma alternativa aos métodos de seleção de subconjuntos e *shrinkage* discutidos anteriormente [^57, ^61], o FSε oferece uma abordagem iterativa que adiciona pequenos incrementos aos coeficientes dos preditores mais correlacionados com os resíduos atuais [^1]. Este método, similar ao Least Angle Regression (LAR), gera um perfil de coeficientes ao atualizar repetidamente (por uma pequena quantidade ε) o coeficiente da variável mais correlacionada com os resíduos [^1].

### Conceitos Fundamentais
A **Incremental Forward Stagewise Regression (FSε)** é uma técnica ainda mais restrita do que a regressão *forward-stepwise* [^60]. O algoritmo inicia-se com um intercepto igual a $\bar{y}$ e preditores centrados com coeficientes inicialmente todos iguais a 0 [^60]. Em cada passo, o algoritmo identifica a variável mais correlacionada com o resíduo atual [^60]. Em seguida, calcula o coeficiente de regressão linear simples do resíduo na variável escolhida e o adiciona ao coeficiente atual dessa variável [^60].

Diferentemente da regressão *forward-stepwise*, nenhuma das outras variáveis é ajustada quando um termo é adicionado ao modelo [^60]. Como consequência, o *forward stagewise* pode levar muito mais do que *p* passos para alcançar o ajuste de mínimos quadrados [^60]. Historicamente, essa característica levou à sua rejeição como ineficiente [^60]. No entanto, essa "lentidão" pode ser vantajosa em problemas de alta dimensão [^60].

Formalmente, o algoritmo FSε pode ser resumido da seguinte forma [^1]:
1. Inicializar o resíduo $r$ como $y$ e todos os coeficientes $\beta_j$ como 0. Padronizar os preditores para terem média zero e norma unitária.
2. Encontrar o preditor $x_j$ mais correlacionado com o resíduo $r$.
3. Atualizar o coeficiente $\beta_j$ como $\beta_j + \delta_j$, onde $\delta_j = \epsilon \cdot \text{sign}(\langle x_j, r \rangle)$ e $\epsilon > 0$ é um pequeno tamanho de passo. Atualizar o resíduo $r$ como $r - \delta_j x_j$.
4. Repetir os passos 2 e 3 até que os resíduos não estejam correlacionados com nenhum dos preditores.

A escolha do tamanho do passo $\epsilon$ é crucial. Um $\epsilon$ grande pode levar a um ajuste mais rápido, mas pode perder características importantes dos dados. Um $\epsilon$ pequeno, por outro lado, garante um ajuste mais preciso, mas pode aumentar significativamente o tempo de computação [^1].

**Comparação com LAR:**
O FSε é similar ao LAR, mas difere na forma como os coeficientes são atualizados. Enquanto o LAR move os coeficientes das variáveis ativas em uma direção de mínimos quadrados conjunta, o FSε atualiza apenas o coeficiente da variável mais correlacionada com o resíduo em cada passo [^73].

**Infinitesimal Forward Stagewise Regression (FSo):**
Ao tomar o limite de $\epsilon$ tendendo a 0, obtemos o *infinitesimal forward stagewise regression* (FSo) [^87]. Este procedimento desempenha um papel importante em métodos adaptativos não lineares, como o *boosting* (Capítulos 10 e 16) [^87].

### Conclusão
O Incremental Forward Stagewise Regression (FSε) oferece uma abordagem alternativa e computacionalmente eficiente para ajustar modelos lineares esparsos [^1]. Sua natureza iterativa e a capacidade de controlar o tamanho do passo permitem um ajuste fino do modelo, tornando-o adequado para problemas de alta dimensão [^60]. Embora historicamente tenha sido considerado ineficiente em comparação com outros métodos, o FSε tem ressurgido como uma técnica competitiva, especialmente em cenários onde a esparsidade é desejada e a computação é uma preocupação [^60].

### Referências
[^1]: Não há referência correspondente no texto fornecido.
[^57]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^60]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^61]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^73]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^87]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
<!-- END -->