## Multiple Outputs in Linear Regression

### Introdução
Este capítulo explora a generalização dos métodos lineares para regressão com múltiplas saídas, um problema que surge quando se deseja prever múltiplos resultados a partir das mesmas variáveis de entrada. Como veremos, a extensão direta da função de perda univariada leva a soluções que, sob certas condições, separam o problema em regressões lineares independentes para cada saída. No entanto, exploraremos também cenários onde a correlação entre as saídas pode ser explorada para melhorar o desempenho preditivo.

### Conceitos Fundamentais

A regressão linear com múltiplas saídas aborda o problema de prever *K* saídas ($Y_1, Y_2, ..., Y_K$) a partir de *p* entradas ($X_0, X_1, X_2,...,X_p$) [^56]. Assumimos um modelo linear para cada saída:

$$ Y_k = \beta_{0k} + \sum_{j=1}^{p} X_j\beta_{jk} + \epsilon_k = f_k(X) + \epsilon_k, $$

onde $f_k(X)$ é a função de regressão linear para a *k*-ésima saída, e $\epsilon_k$ é o termo de erro associado [^56].

Com *N* casos de treinamento, o modelo pode ser escrito em notação matricial como:

$$ Y = XB + E, $$

onde *Y* é a matriz de respostas *N x K*, *X* é a matriz de entrada *N x (p+1)*, *B* é a matriz de parâmetros *(p+1) x K*, e *E* é a matriz de erros *N x K* [^56].

Uma generalização direta da função de perda da soma dos quadrados dos resíduos (RSS) para o caso multivariado é dada por [^56]:

$$ RSS(B) = \sum_{k=1}^{K} \sum_{i=1}^{N} (Y_{ik} - f_k(X_i))^2 = tr[(Y - XB)^T(Y - XB)], $$

onde $tr[\cdot]$ denota o traço da matriz. Esta função de perda mede a soma dos quadrados dos resíduos para todas as saídas e todas as observações.

Os estimadores de mínimos quadrados para *B* são obtidos minimizando *RSS(B)*. A solução é dada por [^56]:

$$ B = (X^TX)^{-1}X^TY. $$

Essa solução tem a mesma forma que no caso univariado, mas agora *B* é uma matriz. Notavelmente, os coeficientes para a *k*-ésima saída são apenas os estimadores de mínimos quadrados na regressão de $y_k$ em $X_0, X_1, ..., X_p$ [^56]. Isso significa que as múltiplas saídas não afetam as estimativas de mínimos quadrados umas das outras. Em outras palavras, o problema se decompõe em *K* problemas de regressão linear independentes.

**Correlações entre os Erros**

Se os erros $\epsilon = (\epsilon_1, ..., \epsilon_K)$ são correlacionados, pode ser apropriado modificar a função de perda para uma versão multivariada ponderada [^56]. Suponha que $Cov(\epsilon) = \Sigma$, então o critério de mínimos quadrados ponderados multivariado é:

$$ RSS(B; \Sigma) = \sum_{i=1}^{N} (Y_i - f(X_i))^T\Sigma^{-1}(Y_i - f(X_i)), $$

onde $f(X)$ é a função vetor (f1(x),..., fK(x)), e $Y_i$ é o vetor de *K* respostas para a observação *i* [^56].

Surpreendentemente, pode-se mostrar que a solução ainda é dada por [^56]:

$$ B = (X^TX)^{-1}X^TY, $$

ou seja, *K* regressões separadas que ignoram as correlações (Exercício 3.11). No entanto, se os $\Sigma_i$ variam entre as observações, este não é mais o caso, e a solução para *B* não se dissocia mais [^56].

### Conclusão
Este capítulo demonstra que, sob certas condições, a regressão linear com múltiplas saídas pode ser tratada como uma coleção de regressões lineares independentes. No entanto, quando a estrutura de correlação entre as saídas é mais complexa, outras técnicas podem ser necessárias para explorar essas dependências e melhorar o desempenho preditivo.
<!-- END -->