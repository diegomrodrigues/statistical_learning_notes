## Shrinkage and Selection Methods for Multiple Outputs

### Introdução
Como vimos anteriormente [^56], no contexto de **Multiple Outputs**, o modelo linear generaliza-se diretamente para múltiplas saídas através da equação $Y = XB + E$ [^36], onde $Y$ é a matriz de respostas $N \times K$, $X$ é a matriz de entrada $N \times (p+1)$, $B$ é a matriz de parâmetros $(p+1) \times K$ e $E$ é a matriz de erros $N \times K$ [^36]. Surpreendentemente, a solução de mínimos quadrados para $B$ é dada por $B = (X^TX)^{-1}X^TY$ [^39], o que implica que *as estimativas de mínimos quadrados para cada saída são independentes umas das outras* [^39]. Este capítulo explora as implicações desta independência, e como métodos de *shrinkage* e seleção podem ser adaptados e aprimorados para o caso de múltiplas saídas.

### Conceitos Fundamentais

A independência das estimativas de mínimos quadrados para múltiplas saídas [^39] simplifica inicialmente o problema. No entanto, *se os erros $\epsilon$ forem correlacionados, um critério multivariado ponderado pode ser usado* [^56]. Especificamente, se $Cov(\epsilon) = \Sigma$, então o critério a ser minimizado é:

$$RSS(B; \Sigma) = \sum_{i=1}^{N} (y_i - f(x_i))^T \Sigma^{-1} (y_i - f(x_i))$$ [^40]

Este critério surge naturalmente da teoria Gaussiana multivariada [^40]. Contudo, *a solução continua sendo $B = (X^TX)^{-1}X^TY$*, ou seja, $K$ regressões separadas que ignoram as correlações [^56].

A situação muda drasticamente *se as matrizes $\Sigma_i$ variarem entre as observações* [^56]. Neste caso, a solução para $B$ deixa de ser desacoplada. A complexidade adicional requer abordagens que considerem simultaneamente todas as saídas.

**Canonical Correlation Analysis (CCA)**

Uma técnica para lidar com múltiplas saídas é a **Canonical Correlation Analysis (CCA)** [^84], que busca combinações lineares não correlacionadas das entradas $Xv_m$ e das saídas $Yu_m$ de forma a maximizar as correlações:

$$Corr^2(Yu_m, Xv_m)$$ [^84, 3.67]

CCA encontra uma sequência de combinações lineares não correlacionadas de $X$ e $Y$ de forma que as correlações entre as combinações sejam maximizadas sucessivamente [^84].

**Reduced-Rank Regression**

A reduced-rank regression formaliza esta abordagem em termos de um modelo de regressão que agrupa informações [^84]. Dado um modelo com covariância de erro $Cov(\epsilon) = \Sigma$, o objetivo é resolver o seguinte problema de regressão multivariada restrita:

$$ \underset{rank(B)=m}{argmin} \sum_{i=1}^{N} (y_i - B^T x_i)^T \Sigma^{-1} (y_i - B^T x_i) $$ [^85, 3.68]

A solução é dada por uma CCA de $Y$ e $X$:

$$ B^{(m)} = B U_m U_m^T $$ [^85, 3.69]

onde $U_m$ é a submatriz $K \times m$ de $U$ consistindo nas primeiras $m$ colunas e $U$ é a matriz $K \times M$ de vetores canônicos esquerdos $u_1, u_2, ..., u_M$ [^85].

A reduced-rank regression realiza uma regressão linear na matriz de resposta agrupada $YU_m$ e então mapeia os coeficientes (e, portanto, os ajustes) de volta para o espaço de resposta original [^85].

**Curds and Whey (C+W) Estimator**

Breiman e Friedman (1997) exploraram o *shrinkage* das variáveis canônicas entre $X$ e $Y$, uma versão suave da regressão de *rank* reduzido [^85]. A proposta tem a forma:

$$ B^{c+w} = BUA U^{-1} $$ [^85, 3.72]

onde $A$ é uma matriz diagonal de *shrinkage* e as entradas diagonais são:

$$ \lambda_m = \frac{c_m}{c_m + (1 - c_m)} $$ [^85, 3.73]

onde $c_m$ é o m-ésimo coeficiente de correlação canônica [^85].

### Conclusão

Este capítulo detalhou como métodos de *shrinkage* e seleção podem ser estendidos para lidar com **Multiple Outputs**. Embora a solução de mínimos quadrados desacople as estimativas para cada saída, a estrutura de correlação entre as saídas pode ser explorada para melhorar o desempenho. Técnicas como CCA e reduced-rank regression permitem combinar informações entre as saídas, resultando em modelos mais robustos e precisos [^84]. Abordagens que consideram a estrutura de covariância dos erros, especialmente quando esta varia entre observações, oferecem um caminho para aprimorar ainda mais a modelagem de múltiplas saídas [^56].

### Referências
[^36]: Page 56, "Y = XB + E. Here Y is the N × K response matrix, with ik entry Yik, X is the N× (p+1) input matrix, B is the (p + 1) × K matrix of parameters and E is the NXK matrix of errors."
[^39]: Page 56, "Hence the coefficients for the kth outcome are just the least squares es- timates in the regression of yk on X0, X1,...,xp. Multiple outputs do not affect one another\'s least squares estimates."
[^40]: Page 40, "RSS(B; Σ) = ∑i=1N (Yi - f(xi))¯¹ (Yi - f(xi)) arises naturally from multivariate Gaussian theory."
[^56]: Page 56, "Multiple outputs do not affect one another\'s least squares estimates. If the errors ɛ = (€1,...,εκ) in (3.34) are correlated, then it might seem appropriate to modify (3.37) in favor of a multivariate version. Specifically, suppose Cov(ε) = ∑, then the multivariate weighted criterion ... If the Si vary among observations, then this is no longer the case, and the solution for B no longer decouples."
[^84]: Page 84, "Combining responses is at the heart of canonical correlation analysis (CCA), a data reduction technique developed for the multiple output case. Similar to PCA, CCA finds a sequence of uncorrelated linear combinations Xvm, m = 1,..., M of the xj, and a corresponding sequence of uncorrelated linear combinations Yum of the responses yk, such that the correlations Corr²(Yum, Xum) are successively maximized."
[^85]: Page 85, "Reduced-rank regression (Izenman, 1975; van der Merwe and Zidek, 1980) formalizes this approach in terms of a regression model that explicitly pools information."

<!-- END -->