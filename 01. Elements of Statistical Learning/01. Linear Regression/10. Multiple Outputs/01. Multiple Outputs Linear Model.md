## Regressão Linear com Múltiplas Saídas

### Introdução
Este capítulo foca na extensão dos modelos lineares de regressão para lidar com múltiplos *outputs* (variáveis dependentes). Anteriormente, nos concentramos em modelos onde tínhamos um único *output* $Y$ que queríamos prever com base em um conjunto de *inputs* $X_1, X_2, ..., X_p$ [^44]. Agora, expandimos essa estrutura para o caso onde temos múltiplos *outputs* $Y_1, Y_2, ..., Y_k$. Este cenário é comum em diversas aplicações, como modelagem de sistemas complexos com várias variáveis de resposta ou análise de dados multivariados [^56]. O objetivo é modelar a relação entre os *inputs* e cada um dos *outputs* de forma eficiente e, potencialmente, explorar as correlações entre os *outputs* para melhorar a precisão da modelagem.

### Conceitos Fundamentais
Em um cenário com múltiplos *outputs* $Y_1, Y_2, ..., Y_k$, assumimos um modelo linear para cada *output* individualmente [^56]. Isso significa que para cada $Y_k$, temos uma equação da forma:

$$Y_k = X\beta_k + \epsilon_k$$

onde:
*   $Y_k$ é o k-ésimo *output*.
*   $X$ é a matriz de *inputs* (preditores).
*   $\beta_k$ é o vetor de coeficientes para o k-ésimo *output*.
*   $\epsilon_k$ é o termo de erro para o k-ésimo *output*.

Podemos representar esse modelo em notação matricial de forma compacta como:

$$Y = XB + E$$

onde:
*   $Y$ é a matriz de respostas $N \times K$, onde cada coluna representa um *output*. O elemento $Y_{ik}$ representa o valor do k-ésimo *output* para a i-ésima observação [^56].
*   $X$ é a matriz de *inputs* $N \times (p+1)$, como anteriormente [^3, 44].
*   $B$ é a matriz de coeficientes $(p+1) \times K$, onde cada coluna representa os coeficientes para um *output* específico [^56].
*   $E$ é a matriz de erros $N \times K$, onde cada coluna representa os erros para um *output* específico [^56].

#### Estimação dos Coeficientes
Uma generalização direta da função de perda univariada (3.2) [^2, 56] para o caso multivariado é:

$$RSS(B) = \sum_{k=1}^{K} \sum_{i=1}^{N} (Y_{ik} - f_k(X_i))^2 = tr[(Y - XB)^T(Y - XB)]$$

onde $f_k(X_i)$ é a previsão para o k-ésimo *output* na i-ésima observação. A função *tr* denota o traço da matriz.

Os estimadores de mínimos quadrados são da forma:

$$B = (X^TX)^{-1}X^TY$$

Os coeficientes para o k-ésimo *output* são apenas os estimadores de mínimos quadrados na regressão de $y_k$ em $X_0, X_1, ..., X_p$ [^56]. *Múltiplos outputs não afetam os estimadores de mínimos quadrados uns dos outros* [^56].

#### Correlações entre os Erros
Se os erros $\epsilon = (\epsilon_1, ..., \epsilon_k)$ são correlacionados em (3.34) [^56], então pode parecer apropriado modificar (3.37) [^56] em favor de uma versão multivariada. Especificamente, suponha $Cov(\epsilon) = \Sigma$, então o critério multivariado ponderado é:

$$RSS(B; \Sigma) = \sum_{i=1}^{N} (Y_i - f(X_i))^T \Sigma^{-1} (Y_i - f(X_i))$$

onde $f(X)$ é a função vetorial $(f_1(X), ..., f_k(X))$ e $Y_i$ é o vetor de K respostas para a observação i [^56].

No entanto, pode-se mostrar que novamente a solução é dada por (3.39) [^56]; K regressões separadas que ignoram as correlações. Se os $\Sigma_i$ variam entre as observações, então este não é mais o caso e a solução para B não se desvincula mais [^56].

### Conclusão
Este capítulo expandiu a regressão linear para múltiplos *outputs*, mostrando como modelar cada *output* individualmente e como as correlações entre os *outputs* podem ser consideradas. Vimos que, sob certas condições, a estimativa dos coeficientes pode ser feita individualmente para cada *output*, simplificando o processo. No entanto, quando as covariâncias dos erros variam entre as observações, a solução se torna mais complexa e requer métodos que considerem essa dependência [^56]. Este conhecimento é fundamental para aplicações onde múltiplos *outputs* precisam ser modelados de forma eficiente e precisa.

### Referências
[^3]: OCR text from page 1
[^2]: OCR text from page 2
[^56]: OCR text from page 14
[^44]: OCR text from page 44
<!-- END -->