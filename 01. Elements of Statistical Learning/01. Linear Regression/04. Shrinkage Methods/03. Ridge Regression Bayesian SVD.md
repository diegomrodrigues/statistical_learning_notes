## Ridge Regression: Shrinkage via Penalized Least Squares and Bayesian Interpretation

### Introdução
Este capítulo explora o método de **Ridge Regression**, uma técnica de *shrinkage* que aborda problemas de multicolinearidade e *overfitting* em modelos de regressão linear [^61]. Como vimos anteriormente, o método de mínimos quadrados (least squares) pode levar a estimativas de coeficientes com alta variância, especialmente quando as variáveis preditoras são altamente correlacionadas [^61, 51, 47]. Ridge regression introduz um termo de penalidade que reduz a magnitude dos coeficientes, melhorando a estabilidade e a capacidade de generalização do modelo [^61]. Exploraremos a derivação matemática, a interpretação Bayesiana e a análise via decomposição em valores singulares (SVD) para fornecer uma compreensão abrangente da Ridge Regression.

### Conceitos Fundamentais

**Definição da Ridge Regression**

Em contraste com a regressão linear tradicional, a Ridge Regression minimiza uma soma de quadrados residuais penalizada [^61]:

$$ \hat{\beta}^{ridge} = \underset{\beta}{\text{argmin}} \left\{ \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2 \right\} $$

onde:
- $y_i$ são os valores da variável resposta.
- $x_{ij}$ são os valores das variáveis preditoras.
- $\beta_0$ é o intercepto.
- $\beta_j$ são os coeficientes de regressão.
- $\lambda \geq 0$ é o parâmetro de *tuning* que controla a quantidade de *shrinkage* aplicada aos coeficientes [^61]. Quanto maior o valor de $\lambda$, maior a penalidade e menor a magnitude dos coeficientes.

Em forma matricial, a equação acima pode ser escrita como [^64]:

$$ \text{RSS}(\lambda) = (y - X\beta)^T(y - X\beta) + \lambda\beta^T\beta $$

onde:
- $y$ é o vetor de variáveis resposta.
- $X$ é a matriz de desenho.
- $\beta$ é o vetor de coeficientes.

A solução para $\beta$ que minimiza esta expressão é [^64]:

$$ \hat{\beta}^{ridge} = (X^TX + \lambda I)^{-1}X^Ty $$

onde $I$ é a matriz identidade $p \times p$. A adição de $\lambda I$ à matriz $X^TX$ garante que a matriz seja não singular, mesmo que $X^TX$ seja singular, resolvendo o problema de multicolinearidade [^64].

**Interpretação Bayesiana**

A Ridge Regression pode ser interpretada como a média ou a moda de uma distribuição *a posteriori* com uma *prior* Gaussiana nos coeficientes [^64]. Especificamente, se assumirmos que os $y_i$ são normalmente distribuídos em torno de $\beta_0 + x_i^T\beta$ com variância $\sigma^2$, e que os parâmetros $\beta_j$ são cada um distribuído como $N(0, \tau^2)$, independentemente uns dos outros, então o log da densidade *a posteriori* de $\beta$, com $\sigma^2$ e $\tau^2$ assumidos como conhecidos, é igual à expressão entre chaves na equação da Ridge Regression, com $\lambda = \sigma^2/\tau^2$ [^64]. Assim, a estimativa de Ridge é a moda da distribuição *a posteriori*; como a distribuição é Gaussiana, também é a média *a posteriori*.

**Decomposição em Valores Singulares (SVD)**

A decomposição em valores singulares (SVD) da matriz de entrada $X$ fornece *insights* adicionais sobre a Ridge Regression [^64]. Seja a SVD de $X$ dada por:

$$ X = UDV^T $$

onde:
- $U$ é uma matriz ortogonal $N \times p$.
- $D$ é uma matriz diagonal $p \times p$ com valores singulares $d_1 \geq d_2 \geq ... \geq d_p \geq 0$.
- $V$ é uma matriz ortogonal $p \times p$.

Usando a SVD, a solução de Ridge Regression pode ser reescrita como [^66]:

$$ X\hat{\beta}^{ridge} = \sum_{j=1}^p u_j \frac{d_j^2}{d_j^2 + \lambda} u_j^T y $$

onde $u_j$ são as colunas de $U$. Esta equação mostra que a Ridge Regression encolhe as coordenadas de $y$ na base ortonormal $u_j$ [^66]. A quantidade de *shrinkage* é proporcional a $d_j^2/(d_j^2 + \lambda)$. Para valores singulares $d_j$ pequenos, o fator de *shrinkage* é próximo de zero, indicando que as direções em $X$ com pequena variância são mais fortemente penalizadas [^66, 67].

**Não Equivariância sob Escalonamento**

As soluções de Ridge Regression não são *equivariant* sob o escalonamento das entradas [^61]. Isso significa que se escalonarmos uma variável preditora $x_j$ por um fator $c$, o coeficiente correspondente $\beta_j$ não será simplesmente escalonado por $1/c$. Para mitigar esse problema, é comum padronizar as entradas antes de resolver o problema de Ridge Regression [^61]. A padronização geralmente envolve subtrair a média e dividir pelo desvio padrão para cada variável preditora [^50].

**Graus de liberdade efetivos**

Os graus de liberdade efetivos da Ridge Regression são definidos como o traço da matriz *hat* [^68]:

$$ df(\lambda) = \text{tr}(X(X^TX + \lambda I)^{-1}X^T) = \sum_{j=1}^p \frac{d_j^2}{d_j^2 + \lambda} $$

Esta medida quantifica a complexidade do modelo em função de $\lambda$. À medida que $\lambda$ aumenta, $df(\lambda)$ diminui, indicando um modelo mais simples [^68].

### Conclusão

A Ridge Regression oferece uma abordagem eficaz para lidar com multicolinearidade e *overfitting* em modelos de regressão linear [^61]. Ao introduzir uma penalidade nos coeficientes de regressão, a Ridge Regression reduz a variância e melhora a capacidade de generalização do modelo [^61]. A interpretação Bayesiana fornece uma base probabilística para a Ridge Regression, enquanto a análise SVD revela como a Ridge Regression encolhe direções de baixa variância nos dados [^64, 66]. A não equivariância sob o escalonamento destaca a importância da padronização das entradas antes de aplicar a Ridge Regression [^61]. Em resumo, a Ridge Regression é uma ferramenta valiosa no conjunto de ferramentas de modelagem estatística, oferecendo um *trade-off* flexível entre viés e variância.

### Referências
[^61]: Seção 3.4, Shrinkage Methods, página 61.
[^64]: Seção 3.4.1, Ridge Regression, página 64.
[^66]: Seção 3.4.1, Ridge Regression, página 66.
[^67]: Seção 3.4.1, Ridge Regression, página 67.
[^68]: Seção 3.4.1, Ridge Regression, página 68.
[^51]: Seção 3.2.2, The Gauss-Markov Theorem, página 51.
[^47]: Seção 3.2, Linear Regression Models and Least Squares, página 47.
[^50]: Seção 3, Linear Methods for Regression, página 50.
<!-- END -->