## Capítulo 3.4.2: O Lasso

### Introdução

Este capítulo aprofunda o método **Lasso (Least Absolute Shrinkage and Selection Operator)**, um método de *shrinkage* com características únicas no contexto de modelos lineares para regressão [^61]. Diferentemente da regressão de Ridge, o Lasso promove a **esparsidade** da solução, forçando alguns coeficientes a serem exatamente zero, o que efetivamente realiza a seleção de variáveis [^61]. Exploraremos a formulação matemática do Lasso, suas propriedades e diferenças em relação a outros métodos de *shrinkage*, como a regressão de Ridge.

### Conceitos Fundamentais

O Lasso, assim como a regressão de Ridge, busca minimizar a soma dos quadrados dos resíduos, mas introduz uma restrição diferente sobre os coeficientes [^61]. A regressão de Ridge utiliza uma penalidade $L_2$ (soma dos quadrados dos coeficientes), enquanto o Lasso emprega uma penalidade $L_1$ (soma dos valores absolutos dos coeficientes).

A formulação matemática do Lasso é dada por:

$$ \hat{\beta}_{lasso} = \underset{\beta}{\text{argmin}} \left\{ \sum_{i=1}^{N} \left( y_i - \beta_0 - \sum_{j=1}^{p} x_{ij}\beta_j \right)^2 \right\} $$

sujeito a

$$ \sum_{j=1}^{p} |\beta_j| \leq t $$

onde:

*   $y_i$ são os valores observados da variável resposta.
*   $x_{ij}$ são os valores das variáveis preditoras.
*   $\beta_0$ é o intercepto.
*   $\beta_j$ são os coeficientes de regressão.
*   $t$ é um parâmetro de ajuste (tuning parameter) que controla a força da penalidade.

Equivalentemente, podemos escrever o problema do Lasso na forma Lagrangiana:

$$ \hat{\beta}_{lasso} = \underset{\beta}{\text{argmin}} \left\{ \frac{1}{2} \sum_{i=1}^{N} \left( y_i - \beta_0 - \sum_{j=1}^{p} x_{ij}\beta_j \right)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\} $$

onde $\lambda$ é o parâmetro de regularização que controla a intensidade da penalidade $L_1$ [^68]. Existe uma correspondência biunívoca entre $t$ na formulação com restrição e $\lambda$ na formulação Lagrangiana [^62].

**Diferenças em relação à regressão de Ridge:**

1.  ***Natureza da Penalidade:*** A penalidade $L_1$ do Lasso tem a propriedade de "forçar" alguns coeficientes a serem exatamente zero, enquanto a penalidade $L_2$ da regressão de Ridge apenas os encolhe em direção a zero [^61]. Isso torna o Lasso um método de seleção de variáveis, além de um método de *shrinkage*.
2.  ***Não Linearidade:*** A restrição $L_1$ no Lasso torna as soluções não lineares em $y_i$, o que significa que não existe uma expressão de forma fechada para os coeficientes, como na regressão de Ridge [^68].
3.  ***Interpretabilidade:*** Devido à sua capacidade de zerar coeficientes, o Lasso oferece modelos mais interpretáveis, pois identifica as variáveis mais importantes para a predição [^61].
4.  ***Estabilidade:*** O Lasso pode apresentar instabilidade em situações onde as variáveis preditoras são altamente correlacionadas. Pequenas mudanças nos dados podem levar a grandes mudanças no conjunto de variáveis selecionadas.

**Interpretação Geométrica:**

Geometricamente, a restrição $\sum_{j=1}^{p} |\beta_j| \leq t$ define uma região em forma de diamante (ou *rhomboid*) no espaço dos coeficientes [^72]. A solução do Lasso é o ponto onde a superfície de contorno da soma dos quadrados dos resíduos toca essa região. Devido aos "cantos" do diamante, é mais provável que a solução ocorra em um ponto onde alguns coeficientes são exatamente zero. Em contraste, a restrição da regressão de Ridge, $\sum_{j=1}^{p} \beta_j^2 \leq t$, define um disco, que não possui cantos, tornando menos provável que a solução ocorra com coeficientes zerados [^72].

**Least Angle Regression (LAR) e o Lasso:**

O algoritmo **Least Angle Regression (LAR)** está intimamente ligado ao Lasso [^74]. De fato, uma modificação simples do algoritmo LAR permite calcular todo o caminho de soluções do Lasso de forma eficiente [^76]. O LAR começa identificando a variável mais correlacionada com a resposta e move o coeficiente dessa variável em direção à sua estimativa de mínimos quadrados. Quando outra variável se torna igualmente correlacionada com o resíduo, o LAR move os coeficientes das duas variáveis juntas, mantendo suas correlações iguais [^74]. Este processo continua até que todas as variáveis estejam no modelo.

**Graus de Liberdade:**

A definição de graus de liberdade para o Lasso é um tanto sutil. Ao contrário da regressão linear padrão, onde os graus de liberdade são simplesmente o número de parâmetros no modelo, o Lasso impõe uma restrição que torna a contagem direta de parâmetros inadequada. No entanto, pode-se demonstrar que, após a *k*-ésima etapa do procedimento LAR, os graus de liberdade efetivos do vetor de ajuste são exatamente *k* [^78].

### Conclusão

O Lasso é uma ferramenta poderosa para a construção de modelos lineares esparsos e interpretáveis. Sua capacidade de realizar a seleção de variáveis o torna particularmente útil em situações onde há um grande número de preditores e deseja-se identificar os mais relevantes [^61]. Embora apresente algumas limitações, como instabilidade em dados altamente correlacionados, o Lasso continua sendo um método amplamente utilizado e estudado na área de *machine learning* e estatística.

### Referências

[^61]:  *The lasso (Least Absolute Shrinkage and Selection Operator) minimizes the residual sum of squares subject to a constraint on the sum of the absolute values of the coefficients, promoting sparsity by setting some coefficients exactly to zero and effectively performing variable selection.*
[^68]:  *The lasso estimate is defined by minimizing the residual sum of squares subject to a constraint on the sum of the absolute values of the coefficients, and this constraint makes the solutions nonlinear in the yi, and there is no closed form expression as in ridge regression.*
[^61]:  *The lasso (Least Absolute Shrinkage and Selection Operator) minimizes the residual sum of squares subject to a constraint on the sum of the absolute values of the coefficients, promoting sparsity by setting some coefficients exactly to zero and effectively performing variable selection.*
[^68]:  *The lasso estimate is defined by minimizing the residual sum of squares subject to a constraint on the sum of the absolute values of the coefficients, and this constraint makes the solutions nonlinear in the yi, and there is no closed form expression as in ridge regression.*
[^61]:  *The lasso (Least Absolute Shrinkage and Selection Operator) minimizes the residual sum of squares subject to a constraint on the sum of the absolute values of the coefficients, promoting sparsity by setting some coefficients exactly to zero and effectively performing variable selection.*
[^72]: *We can generalize ridge regression and the lasso, and view them as Bayes estimates. Consider the criterion 3 = argmin (vi - 30 - 3)2 + 3, for q ≥ 0. Thinking of |3j|9 as the log-prior density for Bj, these are also the equi-contours of the prior distribution of the parameters. The value q = 0 corre-sponds to variable subset selection, as the penalty simply counts the number of nonzero parameters; q = 1 corresponds to the lasso, while q = 2 to ridge regression. Notice that for q ≤ 1, the prior is not uniform in direction, but concentrates more mass in the coordinate directions. The prior correspond-ing to the q = 1 case is an independent double exponential (or Laplace) distribution for each input, with density (1/2т) ехр(-|3|/т) and τ = 1/λ.*
[^74]: *Least angle regression (LAR) is a relative newcomer (Efron et al., 2004), and can be viewed as a kind of "democratic" version of forward stepwise regression (Section 3.3.2). As we will see, LAR is intimately connected with the lasso, and in fact provides an extremely efficient algorithm for computing the entire lasso path as in Figure 3.10.*
[^76]:  *These observations lead to a simple modification of the LAR algorithm that gives the entire lasso path, which is also piecewise-linear.*
[^78]: *Specifically it can be shown that after the kth step of the LAR procedure, the effective degrees of freedom of the fit vector is exactly k.*
<!-- END -->