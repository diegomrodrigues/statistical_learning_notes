## Shrinkage Methods: Ridge Regression and Lasso

### Introdução
Este capítulo aprofunda os **métodos de *shrinkage***, focando especificamente na **ridge regression** e no **lasso**, e como eles são empregados para reduzir a complexidade de modelos de regressão linear [^61]. Os métodos de *shrinkage* impõem penalidades na magnitude dos coeficientes, aprimorando a acurácia da predição e o desempenho de generalização. Como mencionado, esses métodos produzem resultados mais contínuos, reduzindo a variância sem a alta variabilidade da seleção de subconjuntos.

### Conceitos Fundamentais

#### Regressão Linear e *Shrinkage*
A **regressão linear** assume que a função de regressão $E(Y|X)$ é linear nas entradas $X_1, ..., X_p$ [^43]. Embora modelos lineares tenham sido desenvolvidos antes da era dos computadores, eles ainda são relevantes devido à sua simplicidade e interpretabilidade [^43]. No entanto, a regressão linear pode sofrer de alta variância, especialmente quando o número de preditores $p$ é grande em relação ao número de observações $N$ [^57]. Os métodos de *shrinkage* abordam essa questão ao introduzir um termo de penalidade que restringe o tamanho dos coeficientes [^61].

#### Ridge Regression
A **ridge regression** minimiza a soma dos quadrados residuais penalizada pelo quadrado da norma L2 dos coeficientes [^61]. Formalmente, o objetivo é minimizar:

$$ \beta^{ridge} = \underset{\beta}{\text{argmin}} \left\{ \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\} $$

onde $\lambda \geq 0$ é o parâmetro de complexidade que controla a quantidade de *shrinkage* [^63]. Quanto maior o valor de $\lambda$, maior o *shrinkage* dos coeficientes em direção a zero [^63]. Uma forma equivalente de escrever o problema da *ridge* é:

$$ \beta^{ridge} = \underset{\beta}{\text{argmin}} \left\{ \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2 \right\} \quad \text{sujeito a} \quad \sum_{j=1}^{p} \beta_j^2 \leq t $$

onde $t$ é um parâmetro que limita o tamanho dos coeficientes [^63]. Existe uma correspondência um-para-um entre os parâmetros $\lambda$ na equação anterior e $t$ [^63].

A solução para a ridge regression pode ser expressa em forma fechada como:

$$ \hat{\beta}^{ridge} = (X^TX + \lambda I)^{-1}X^Ty $$

onde $X$ é a matriz de *design*, $y$ é o vetor de resposta e $I$ é a matriz identidade [^64]. A adição de $\lambda I$ à diagonal de $X^TX$ torna a matriz invertível, mesmo que $X^TX$ não seja de *rank* completo [^64].

#### Lasso (Least Absolute Shrinkage and Selection Operator)
O **lasso** é outro método de *shrinkage* que minimiza a soma dos quadrados residuais penalizada pela norma L1 dos coeficientes [^61]. O objetivo é minimizar:

$$ \beta^{lasso} = \underset{\beta}{\text{argmin}} \left\{ \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\} $$

Similarmente à *ridge regression*, o parâmetro $\lambda \geq 0$ controla a quantidade de *shrinkage* [^63]. Uma formulação equivalente é:

$$ \beta^{lasso} = \underset{\beta}{\text{argmin}} \left\{ \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2 \right\} \quad \text{sujeito a} \quad \sum_{j=1}^{p} |\beta_j| \leq t $$

A penalidade L1 tem uma propriedade importante: ela pode forçar alguns coeficientes a serem exatamente zero [^69]. Isso significa que o lasso realiza uma forma de seleção de variáveis, além de *shrinkage* [^69]. Ao contrário da *ridge regression*, o lasso não tem uma solução em forma fechada [^71]. No entanto, algoritmos eficientes estão disponíveis para calcular a solução do lasso, como o algoritmo *Least Angle Regression* (LAR) [^71].

#### Diferenças Chave entre Ridge e Lasso
A principal diferença entre *ridge regression* e *lasso* é o tipo de penalidade usada [^61]. A *ridge regression* usa uma penalidade L2, enquanto o *lasso* usa uma penalidade L1 [^61]. Essa diferença leva a diferentes propriedades:

*   A *ridge regression* encolhe todos os coeficientes em direção a zero, mas raramente os define exatamente como zero [^63].
*   O *lasso* pode definir alguns coeficientes como zero, realizando seleção de variáveis [^69].
*   A *ridge regression* é mais adequada quando muitos preditores têm um pequeno efeito [^63].
*   O *lasso* é mais adequado quando apenas alguns preditores têm um efeito grande [^69].

#### Gauss-Markov Theorem

O **Gauss-Markov Theorem** afirma que os estimadores de mínimos quadrados dos parâmetros $\beta$ têm a menor variância entre todos os estimadores lineares não enviesados [^51]. No entanto, a restrição a estimadores não enviesados nem sempre é uma escolha sábia [^51]. Essa observação nos leva a considerar estimativas enviesadas, como a regressão de *ridge*, mais adiante no capítulo [^51].

#### Discussão: Seleção de Subconjuntos, Ridge Regression e Lasso
Em resumo:

*   **Seleção de subconjuntos**: Mantém apenas um subconjunto das variáveis e elimina o restante do modelo [^57].
*   **Ridge Regression**: Aplica um *shrinkage* proporcional [^71].
*   **Lasso**: Traduz cada coeficiente por um fator constante $\lambda$, truncando em zero (soft thresholding) [^71].

### Conclusão

A *ridge regression* e o *lasso* são ferramentas valiosas para lidar com a complexidade em modelos de regressão linear [^61]. Ao impor penalidades aos coeficientes, esses métodos podem melhorar a precisão da predição e o desempenho da generalização [^61]. A escolha entre *ridge regression* e *lasso* depende das características específicas do conjunto de dados e dos objetivos da análise [^71]. Quando muitos preditores têm um pequeno efeito, a *ridge regression* é mais apropriada, enquanto o *lasso* é mais adequado quando apenas alguns preditores têm um efeito grande [^71].
<!-- END -->