## Least Angle Regression (LAR) e o Lasso

### Introdução
Este capítulo aprofunda o método de **Least Angle Regression (LAR)**, explorando sua relação com o **Lasso** e seu papel dentro dos métodos de *Shrinkage* [^61]. O LAR, como uma versão democrática da regressão stepwise, oferece uma abordagem eficiente para a seleção de variáveis e a estimativa de coeficientes em modelos lineares [^73]. A modificação do algoritmo LAR para obter o caminho completo do Lasso é discutida em detalhes, destacando sua eficiência computacional [^73].

### Conceitos Fundamentais

**Least Angle Regression (LAR)** [^73]:
*   O LAR pode ser visto como uma versão democrática da regressão stepwise forward [^73].
*   Em vez de adicionar variáveis completamente ao modelo, o LAR entra apenas com a parte do preditor que ele merece [^73].
*   O LAR identifica inicialmente a variável mais correlacionada com a resposta [^73].
*   Em vez de ajustar completamente essa variável, o LAR move o coeficiente dessa variável continuamente em direção ao seu valor de mínimos quadrados, o que faz com que sua correlação com o resíduo em evolução diminua em valor absoluto [^73].
*   Assim que outra variável "alcança" em termos de correlação com o resíduo, o processo é pausado [^73].
*   A segunda variável então se junta ao conjunto ativo, e seus coeficientes são movidos juntos de forma a manter suas correlações amarradas e decrescentes [^73].
*   Este processo continua até que todas as variáveis estejam no modelo [^73].

**Relação com o Lasso** [^73]:
*   Uma modificação do algoritmo LAR fornece todo o caminho do Lasso, que também é linear por partes [^73].
*   O algoritmo LAR(lasso) é extremamente eficiente, exigindo a mesma ordem de computação que a de um único ajuste de mínimos quadrados usando os p preditores [^73].
*   No primeiro passo, ele identifica a variável mais correlacionada com a resposta [^73]. Em vez de ajustar completamente essa variável, o LAR move o coeficiente dessa variável continuamente em direção ao seu valor de mínimos quadrados (fazendo com que sua correlação com o resíduo em evolução diminua em valor absoluto) [^73]. Assim que outra variável "alcança" em termos de correlação com o resíduo, o processo é pausado [^73]. A segunda variável então se junta ao conjunto ativo, e seus coeficientes são movidos juntos de forma a manter suas correlações amarradas e decrescentes [^73]. Este processo continua até que todas as variáveis estejam no modelo [^73].

**Algoritmo LAR Modificado para o Lasso (Algoritmo 3.2a) [^76]:**
*   Se um coeficiente não-zero atingir zero, sua variável é removida do conjunto ativo de variáveis, e a direção de mínimos quadrados conjunta atual é recalculada [^76].

**Justificativa Heurística para a Similaridade entre LAR e Lasso [^76]:**
*   O algoritmo LAR é expresso em termos de correlações, mas se as características de entrada forem padronizadas, é equivalente e mais fácil trabalhar com produtos internos [^76].
*   Suponha que $A$ seja o conjunto ativo de variáveis em algum estágio do algoritmo, amarradas em seu produto interno absoluto com os resíduos atuais $y - X\beta$. Podemos expressar isso como [^76]:
    $$x_j^T(y - X\beta) = \gamma \cdot s_j, \forall j \in A$$
    onde $s_j \in \{-1, 1\}$ indica o sinal do produto interno e $\gamma$ é o valor comum [^76].
*   O critério Lasso é [^76]:
    $$R(\beta) = ||y - X\beta||_2^2 + \lambda ||\beta||_1$$
*   Seja $B$ o conjunto ativo de variáveis na solução para um dado valor de $\lambda$. Para essas variáveis, $R(\beta)$ é diferenciável e as condições de estacionaridade dão [^76]:
    $$x_j^T(y - X\beta) = \lambda \cdot \text{sign}(\beta_j), \forall j \in B$$
*   Comparando as duas equações, vemos que elas são idênticas apenas se o sinal de $\beta_j$ corresponder ao sinal do produto interno [^76].

**Interpretação Geométrica** [^29]:
*   A região de restrição para a regressão de Ridge é o disco $β_1^2 + β_2^2 < t$, enquanto para o Lasso é o diamante $|β_1| + |β_2| ≤ t$ [^72].
*   Ambos os métodos encontram o primeiro ponto onde os contornos elípticos atingem a região de restrição [^72].
*   Ao contrário do disco, o diamante tem cantos; se a solução ocorrer em um canto, então ela tem um parâmetro $β_j$ igual a zero [^72].

### Conclusão
O Least Angle Regression oferece uma alternativa eficiente e democraticamente justa à regressão stepwise forward, com uma forte conexão ao método de regularização Lasso. Sua capacidade de calcular o caminho completo do Lasso de forma eficiente o torna uma ferramenta valiosa na seleção de modelos e na análise de dados. As nuances do LAR e sua relação com o Lasso, bem como suas propriedades matemáticas subjacentes, fornecem insights valiosos para a compreensão e aplicação de métodos de encolhimento em uma variedade de contextos estatísticos.
### Referências
[^61]: Page 61, "Shrinkage Methods"
[^73]: Page 73, "Least Angle Regression"
[^76]: Page 76, "Algorithm 3.2a Least Angle Regression: Lasso Modification."
[^29]: Page 29, "FIGURE 3.11. Estimation picture for the lasso (left) and ridge regression (right)."
[^72]: Page 72, "We can generalize ridge regression and the lasso, and view them as Bayes estimates."
<!-- END -->