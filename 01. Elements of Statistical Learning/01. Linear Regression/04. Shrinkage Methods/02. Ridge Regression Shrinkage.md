## Ridge Regression: Regularização por Penalidade L2

### Introdução
Este capítulo explora a **Ridge Regression**, um método de *shrinkage* que visa melhorar a precisão e a estabilidade dos modelos de regressão linear, especialmente em situações de multicolinearidade. A Ridge Regression, ao introduzir uma penalidade L2, equilibra o ajuste aos dados com a complexidade do modelo, resultando em coeficientes mais robustos e generalizáveis. Os métodos lineares são essenciais para entender generalizações não lineares [^3]. A regressão linear assume que a função de regressão E(Y|X) é linear nas entradas X1,..., Xp [^1]. Modelos lineares são simples e fornecem uma descrição adequada de como as entradas afetam a saída [^1].

### Conceitos Fundamentais
A **Ridge Regression** é uma técnica de regularização que adiciona uma penalidade ao **Residual Sum of Squares (RSS)**, com o objetivo de reduzir a magnitude dos coeficientes de regressão [^61]. Essa penalidade é proporcional à soma dos quadrados dos coeficientes, ponderada por um parâmetro de complexidade $\lambda$.

Matematicamente, o objetivo da Ridge Regression é minimizar a seguinte expressão:

$$\
\beta^{ridge} = \underset{\beta}{\operatorname{argmin}} \left\\{ \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\\} \qquad \text{[3.41]}\
$$

Onde:
*   $y_i$ representa o valor da variável resposta para a i-ésima observação.
*   $x_{ij}$ representa o valor da j-ésima variável preditora para a i-ésima observação.
*   $\beta_0$ é o intercepto do modelo.
*   $\beta_j$ é o coeficiente de regressão para a j-ésima variável preditora.
*   $\lambda$ é o parâmetro de complexidade que controla a quantidade de regularização aplicada.

A penalidade $\lambda \sum_{j=1}^{p} \beta_j^2$ força os coeficientes a se aproximarem de zero, reduzindo a sensibilidade do modelo a multicolinearidade [^61].  A multicolinearidade pode levar a coeficientes de regressão mal determinados e com alta variância [^61].

Uma formulação equivalente do problema de Ridge Regression é minimizar o RSS sujeito a uma restrição na soma dos quadrados dos coeficientes:

$$\
\beta^{ridge} = \underset{\beta}{\operatorname{argmin}} \left\\{ \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2 \right\\} \quad \text{subject to} \quad \sum_{j=1}^{p} \beta_j^2 \leq t \qquad \text{[3.42]}\
$$

Onde $t$ é um parâmetro que controla o tamanho máximo permitido para a soma dos quadrados dos coeficientes. Existe uma correspondência biunívoca entre os parâmetros $\lambda$ em (3.41) e $t$ em (3.42) [^61].

A solução para o problema de Ridge Regression pode ser expressa em forma matricial como:

$$\
\beta^{ridge} = (X^TX + \lambda I)^{-1}X^Ty \qquad \text{[3.44]}\
$$

Onde:
*   $X$ é a matriz de design, contendo os valores das variáveis preditoras.
*   $y$ é o vetor de valores da variável resposta.
*   $I$ é a matriz identidade de dimensão $p \times p$.

A adição da constante positiva $\lambda$ à diagonal de $X^TX$ antes da inversão torna o problema não singular, mesmo que $X^TX$ não tenha posto completo [^61].  Isto é particularmente útil quando o número de preditores $p$ é maior do que o número de observações $N$.

**Observações Importantes:**

*   Os coeficientes da Ridge Regression não são *equivariant* sob o escalonamento das entradas [^61]. Portanto, é prática comum padronizar as entradas antes de resolver o problema (3.41) [^61]. A padronização coloca todas as variáveis na mesma escala, evitando que variáveis com maior variância dominem o processo de regularização.
*   O intercepto $\beta_0$ geralmente não é incluído no termo de penalidade [^64]. Penalizar o intercepto tornaria o procedimento dependente da origem escolhida para Y.
*   Após a reparametrização com entradas centradas, a estimativa de $\beta_0$ é dada por $\bar{y} = \frac{1}{N} \sum_{i=1}^{N} y_i$ [^64]. Os coeficientes restantes são estimados por uma regressão de Ridge sem intercepto, utilizando as $x_{ij}$ centradas [^64].

**Derivação Bayesiana:**

A Ridge Regression também pode ser interpretada sob uma perspectiva Bayesiana [^64]. Suponha que os $y_i$ sigam uma distribuição normal $N(\beta_0 + x_i^T\beta, \sigma^2)$ e que os parâmetros $\beta_j$ sejam independentemente distribuídos como $N(0, \tau^2)$.  A densidade *log-posterior* negativa de $\beta$, com $\tau^2$ e $\sigma^2$ assumidos como conhecidos, é igual à expressão entre chaves em (3.41), com $\lambda = \sigma^2 / \tau^2$ [^64]. Assim, a estimativa de Ridge é o modo da distribuição *posterior*; como a distribuição é Gaussiana, também é a média *posterior* [^64].

**Singular Value Decomposition (SVD):**

A **Singular Value Decomposition (SVD)** da matriz de entrada centrada $X$ fornece *insights* adicionais sobre a natureza da Ridge Regression [^64]. A SVD de uma matriz $N \times p$, como $X$, tem a forma:

$$\
X = UDV^T \qquad \text{[3.45]}\
$$

Onde:
*   $U$ e $V$ são matrizes ortogonais $N \times p$ e $p \times p$, respectivamente. As colunas de $U$ abrangem o espaço da coluna de $X$, e as colunas de $V$ abrangem o espaço da linha.
*   $D$ é uma matriz diagonal $p \times p$ com entradas não negativas $d_1 \geq d_2 \geq ... \geq d_p \geq 0$, chamadas de valores singulares de $X$.

Usando a SVD, a solução de Ridge pode ser reescrita como [^66]:

$$\
X\beta^{ridge} = UD(D^2 + \lambda I)^{-1}DU^Ty = \sum_{j=1}^{p} u_j \frac{d_j^2}{d_j^2 + \lambda} u_j^T y \qquad \text{[3.47]}\
$$

Esta equação revela que a Ridge Regression computa as coordenadas de $y$ em relação à base ortonormal $U$ e, em seguida, *shrink* essas coordenadas pelos fatores $\frac{d_j^2}{d_j^2 + \lambda}$.  A quantidade de *shrinkage* é maior para vetores de base com valores singulares menores.

**Graus de Liberdade:**

Uma medida importante da complexidade de um modelo de Ridge Regression é o número de graus de liberdade efetivos, definido como [^68]:

$$\
df(\lambda) = tr(X(X^TX + \lambda I)^{-1}X^T) = \sum_{j=1}^{p} \frac{d_j^2}{d_j^2 + \lambda} \qquad \text{[3.50]}\
$$

Essa função diminui monotonicamente com $\lambda$ [^68]. Quando $\lambda = 0$ (sem regularização), $df(\lambda) = p$, e quando $\lambda \to \infty$, $df(\lambda) \to 0$ [^68].

### Conclusão
A Ridge Regression oferece uma abordagem eficaz para lidar com multicolinearidade e sobreajuste em modelos de regressão linear. Ao introduzir uma penalidade L2, ela *shirnka* os coeficientes, tornando o modelo mais estável e generalizável. A escolha do parâmetro de complexidade $\lambda$ é crucial e pode ser feita utilizando técnicas como validação cruzada [^69]. Métodos de seleção de subconjuntos produzem um modelo que é interpretável e tem possivelmente menor erro de predição do que o modelo completo [^61].

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^3]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^61]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^64]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^66]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^68]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^69]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
<!-- END -->