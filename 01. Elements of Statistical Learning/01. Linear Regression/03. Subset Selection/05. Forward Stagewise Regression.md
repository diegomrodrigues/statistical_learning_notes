## Forward-Stagewise Regression: Uma Abordagem Constrained para Seleção de Subconjuntos

### Introdução
Este capítulo explora o método de **Forward-Stagewise Regression (FS)**, uma técnica alternativa e mais restrita em comparação com a Forward-Stepwise Regression [^59]. Como vimos anteriormente [^57], a seleção de subconjuntos é uma abordagem para modelagem que visa identificar um subconjunto de variáveis preditoras que melhor se ajustam aos dados. O Forward-Stagewise Regression, em particular, oferece uma maneira específica de construir um modelo, adicionando incrementalmente preditores ao modelo, permitindo uma abordagem mais gradual e potencialmente mais estável. O FS é uma técnica de seleção de subconjuntos que, embora historicamente considerada ineficiente, tem demonstrado um desempenho notável em problemas de alta dimensionalidade [^60].

### Conceitos Fundamentais

O Forward-Stagewise Regression (FS) inicia-se de maneira semelhante à Forward-Stepwise Regression [^60], com um **intercepto igual a $\bar{y}$** (a média dos valores de resposta) e preditores centrados com **coeficientes inicialmente todos iguais a 0**. A cada passo, o algoritmo identifica a variável mais correlacionada com o resíduo atual [^60]. Em seguida, calcula o coeficiente de regressão linear simples do resíduo na variável escolhida e o adiciona ao coeficiente atual dessa variável [^60].

A principal diferença entre FS e Forward-Stepwise Regression reside na forma como os coeficientes são atualizados. Enquanto o Forward-Stepwise adiciona a variável com maior contribuição de uma só vez, o FS adiciona apenas um *pequeno incremento* de um preditor a cada passo [^60]. Isso torna o FS uma versão *constrita* do Forward-Stepwise, que pode levar muitos passos para atingir o ajuste de mínimos quadrados [^60].

Formalmente, o algoritmo FS pode ser resumido da seguinte forma:

1.  **Inicialização:**
    -   $\beta_0 = \bar{y}$ (intercepto)
    -   $\beta_j = 0$ para todo $j = 1, ..., p$ (coeficientes iniciais)
    -   $r = y - \bar{y}$ (resíduo inicial)

2.  **Iteração:**
    -   Identificar a variável $x_j$ mais correlacionada com o resíduo $r$:
        $$j^* = \arg \max_j |\langle x_j, r \rangle|$$
    -   Calcular o coeficiente de regressão linear simples do resíduo $r$ em $x_{j^*}$:
        $$delta = \frac{\langle x_{j^*}, r \rangle}{\langle x_{j^*}, x_{j^*} \rangle}$$
    -   Atualizar o coeficiente da variável escolhida:
        $$beta_{j^*} = \beta_{j^*} + \delta$$
    -   Atualizar o resíduo:
        $$r = r - \delta x_{j^*}$$

3.  **Repetição:**
    -   Repetir o passo 2 até que nenhumas das variáveis tenham correlação com o resíduo, ou seja, até atingir o ajuste de mínimos quadrados.

**Observações Importantes:**

*   Ao contrário do Forward-Stepwise, nenhuma das outras variáveis é ajustada quando um termo é adicionado ao modelo [^60].
*   O FS pode levar um número de passos muito maior que $p$ para atingir o ajuste de mínimos quadrados [^60].

### Conclusão
Embora o Forward-Stagewise Regression tenha sido historicamente considerado ineficiente devido ao seu "ajuste lento", ele tem se mostrado promissor em problemas de alta dimensionalidade [^60]. A capacidade de adicionar preditores de forma incremental, sem ajustar os outros, pode levar a modelos mais estáveis e generalizáveis. Em situações onde $N > p$ o FS continuará iterando até que o ajuste de mínimos quadrados seja alcançado. A figura [^59] compara FS com outras técnicas de seleção de subconjuntos demonstrando que leva mais tempo para atingir o erro mínimo.

### Referências
[^59]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^60]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
<!-- END -->