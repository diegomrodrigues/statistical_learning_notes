## Seleção Forward Stepwise

### Introdução
Em continuidade ao tópico de **Subset Selection**, este capítulo aprofunda-se na técnica de seleção *forward stepwise* [^58]. Como alternativa à busca exaustiva de *best-subset regression*, a seleção *forward stepwise* oferece uma abordagem mais eficiente para identificar um subconjunto relevante de preditores em modelos de regressão linear [^57]. Esta técnica, juntamente com a seleção *backward stepwise*, busca um caminho otimizado através do espaço de possíveis subconjuntos de variáveis, equilibrando o poder preditivo com a parcimônia do modelo [^58].

### Conceitos Fundamentais
A seleção *forward stepwise* é um algoritmo iterativo que constrói um modelo de regressão linear sequencialmente [^58]. O processo inicia-se com um modelo nulo, contendo apenas o intercepto [^58]. A cada passo, o algoritmo adiciona ao modelo o preditor que mais melhora o ajuste aos dados, avaliado através de alguma métrica de desempenho, como a soma dos quadrados dos resíduos (RSS) [^58].

O algoritmo pode ser resumido nos seguintes passos:
1. **Inicialização:** Começar com um modelo contendo apenas o intercepto.
2. **Seleção:** Para cada preditor que ainda não está no modelo, calcular a melhoria no ajuste (e.g., redução no RSS) que seria obtida ao adicioná-lo ao modelo.
3. **Adição:** Adicionar ao modelo o preditor que proporciona a maior melhoria no ajuste.
4. **Iteração:** Repetir os passos 2 e 3 até que um critério de parada seja satisfeito. Este critério pode ser um tamanho máximo para o subconjunto ($k$), uma melhoria mínima no ajuste, ou um critério de informação como o AIC [^58].

Assim como o *best-subset regression*, a seleção *forward stepwise* produz uma sequência de modelos indexados por $k$, o tamanho do subconjunto, que deve ser determinado [^58]. A escolha de $k$ envolve um *trade-off* entre viés e variância, juntamente com o desejo de parcimônia [^57]. Técnicas como validação cruzada ou o critério AIC podem ser empregadas para auxiliar na seleção do tamanho do subconjunto [^58].

**Atualização Eficiente com Decomposição QR:**
Uma das vantagens da seleção *forward stepwise* é a possibilidade de utilizar algoritmos de atualização eficientes que exploram a decomposição QR para acelerar o processo de identificação do próximo candidato [^58].

**Caixa de destaque:**

> *Algoritmos de atualização inteligentes podem explorar a decomposição QR para o ajuste atual para estabelecer rapidamente o próximo candidato.* [^58]

A decomposição QR representa a matriz de preditores $X$ como o produto de uma matriz ortogonal $Q$ e uma matriz triangular superior $R$ [^55]:
$$X = QR$$
A cada passo do algoritmo *forward stepwise*, ao invés de recalcular a regressão linear do zero, a decomposição QR do modelo atual pode ser atualizada para refletir a adição do novo preditor. Isso reduz significativamente o custo computacional, especialmente quando o número de preditores é grande.

**Algoritmo ganancioso:**
A seleção *forward stepwise* é um algoritmo *greedy* [^58], o que significa que ele toma a decisão "ótima" a cada passo, sem considerar o impacto a longo prazo dessas decisões. Em outras palavras, uma vez que um preditor é adicionado ao modelo, ele nunca é removido [^58]. Essa característica pode levar a modelos sub-ótimos, uma vez que a importância de um preditor pode mudar à medida que outros preditores são adicionados ao modelo [^58].

### Conclusão
A seleção *forward stepwise* oferece uma alternativa computacionalmente eficiente ao *best-subset regression* para a seleção de variáveis em modelos de regressão linear [^58]. Embora seja um algoritmo *greedy* e possa não encontrar o melhor modelo possível, sua capacidade de explorar a decomposição QR para atualizações eficientes o torna uma ferramenta valiosa em situações com um grande número de preditores [^58]. A escolha do tamanho do subconjunto ($k$) continua sendo um aspecto crucial, exigindo o uso de técnicas de validação ou critérios de informação para equilibrar a complexidade do modelo com seu desempenho preditivo [^57].

### Referências
[^58]: Page 58, "3.3.2 Forward- and Backward-Stepwise Selection"
[^57]: Page 57, "3.3 Subset Selection"
[^55]: Page 55, "3.2 Linear Regression Models and Least Squares"
<!-- END -->