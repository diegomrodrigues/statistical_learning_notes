## O Teorema de Gauss-Markov e a Estimação de Combinações Lineares de Parâmetros

### Introdução
Este capítulo aprofunda o Teorema de Gauss-Markov, um resultado fundamental na teoria da estimação linear. Exploraremos especificamente sua aplicação na estimação de combinações lineares dos parâmetros de um modelo de regressão linear. Como veremos, o teorema garante que, sob certas condições, o estimador de mínimos quadrados (OLS) é o melhor estimador linear não viesado (BLUE) para qualquer combinação linear dos parâmetros [^51]. Este resultado tem implicações significativas para a inferência estatística e a construção de intervalos de confiança, complementando os conceitos introduzidos anteriormente sobre modelos lineares para regressão [^43].

### Conceitos Fundamentais

O Teorema de Gauss-Markov estabelece uma propriedade otimal do estimador de mínimos quadrados em modelos de regressão linear [^51]. Para formalizar o teorema, considere o modelo linear geral:
$$y = X\beta + \epsilon,$$
onde $y$ é o vetor de resposta, $X$ é a matriz de design, $\beta$ é o vetor de parâmetros desconhecidos, e $\epsilon$ é o vetor de erros aleatórios. As premissas cruciais para o teorema são:

1.  **Linearidade:** O modelo é linear nos parâmetros $\beta$. Isto significa que a função de regressão $E(Y|X)$ é linear nas entradas $X_1, \dots, X_p$ [^43, 44].
2.  **Não-viesamento:** Os erros têm média zero, ou seja, $E[\epsilon] = 0$.
3.  **Homocedasticidade:** Os erros têm variância constante, ou seja, $Var(\epsilon_i) = \sigma^2$ para todo $i$.
4.  **Não-correlação:** Os erros são não correlacionados, ou seja, $Cov(\epsilon_i, \epsilon_j) = 0$ para todo $i \neq j$. Em conjunto com a homocedasticidade, isso implica que a matriz de covariância dos erros é $\text{Var}(\epsilon) = \sigma^2 I$, onde $I$ é a matriz identidade.
5.  **$X$ é fixo (não aleatório).**

Sob estas condições, o Teorema de Gauss-Markov afirma que o estimador de mínimos quadrados $\hat{\beta} = (X^TX)^{-1}X^T y$ é o melhor estimador linear não viesado de $\beta$.

Agora, vamos focar na estimação de uma *combinação linear* dos parâmetros, dada por $\theta = a^T \beta$, onde $a$ é um vetor conhecido [^51]. O estimador de mínimos quadrados de $\theta$ é obtido substituindo $\beta$ por $\hat{\beta}$:
$$\hat{\theta} = a^T \hat{\beta} = a^T (X^T X)^{-1} X^T y.$$
É importante notar que, considerando $X$ fixo, $\hat{\theta}$ é uma função linear do vetor de resposta $y$ [^51].

Para verificar que $\hat{\theta}$ é não viesado, calculamos seu valor esperado:
$$E[\hat{\theta}] = E[a^T (X^T X)^{-1} X^T y] = a^T (X^T X)^{-1} X^T E[y].$$
Como $E[y] = X\beta$, temos:
$$E[\hat{\theta}] = a^T (X^T X)^{-1} X^T X\beta = a^T \beta = \theta.$$
Portanto, $\hat{\theta}$ é um estimador não viesado de $\theta$ [^51].

A variância de $\hat{\theta}$ é dada por:
$$\text{Var}(\hat{\theta}) = \text{Var}(a^T (X^T X)^{-1} X^T y) = a^T (X^T X)^{-1} X^T \text{Var}(y) X (X^T X)^{-1} a.$$
Como $\text{Var}(y) = \text{Var}(X\beta + \epsilon) = \text{Var}(\epsilon) = \sigma^2 I$, temos:
$$\text{Var}(\hat{\theta}) = \sigma^2 a^T (X^T X)^{-1} X^T X (X^T X)^{-1} a = \sigma^2 a^T (X^T X)^{-1} a.$$

O Teorema de Gauss-Markov garante que *qualquer outro estimador linear não viesado* de $\theta$, digamos $\tilde{\theta} = c^T y$ (onde $c$ é um vetor constante), terá uma variância maior ou igual a $\text{Var}(\hat{\theta})$ [^51]. Para provar isso, notamos que, como $\tilde{\theta}$ é não viesado, $E[\tilde{\theta}] = E[c^T y] = c^T E[y] = c^T X\beta = a^T \beta$ para todo $\beta$. Isso implica que $c^T X = a^T$.

A variância de $\tilde{\theta}$ é $\text{Var}(\tilde{\theta}) = \text{Var}(c^T y) = c^T \text{Var}(y) c = \sigma^2 c^T c$. Agora, considere a diferença entre as variâncias:
$$\text{Var}(\tilde{\theta}) - \text{Var}(\hat{\theta}) = \sigma^2 (c^T c - a^T (X^T X)^{-1} a).$$
Podemos reescrever $c$ como $c = a (X^T X)^{-1} X^T + (c - a (X^T X)^{-1} X^T)$. Observe que $[c - a (X^T X)^{-1} X^T] X = c^T X - a^T (X^T X)^{-1} X^T X = a^T - a^T = 0$. Então,
$$c^T c = [a (X^T X)^{-1} X^T + (c - a (X^T X)^{-1} X^T)]^T [a (X^T X)^{-1} X^T + (c - a (X^T X)^{-1} X^T)]$$
$$= a (X^T X)^{-1} a + [c - a (X^T X)^{-1} X^T]^T [c - a (X^T X)^{-1} X^T].$$
Como o segundo termo é uma forma quadrática e, portanto, não negativa, temos $c^T c \geq a^T (X^T X)^{-1} a$. Portanto, $\text{Var}(\tilde{\theta}) \geq \text{Var}(\hat{\theta})$, confirmando que $\hat{\theta}$ é o BLUE. $\blacksquare$

**Em resumo:** O Teorema de Gauss-Markov garante que, dentro da classe de estimadores lineares não viesados, o estimador de mínimos quadrados da combinação linear $a^T\beta$ possui a menor variância.

### Conclusão

O Teorema de Gauss-Markov oferece uma justificativa teórica sólida para o uso do estimador de mínimos quadrados em modelos de regressão linear. Ao garantir que o estimador OLS é o BLUE para qualquer combinação linear dos parâmetros, o teorema fornece uma base para inferências estatísticas eficientes e precisas. A compreensão das condições sob as quais o teorema é válido é crucial para a aplicação correta de modelos de regressão linear e para a interpretação de seus resultados. Além disso, a observação de que a restrição a estimadores não viesados nem sempre é a mais sábia [^51] nos motiva a considerar estimadores viesados, como a regressão de *ridge*, que podem ter um erro quadrático médio menor.

### Referências
[^51]: Seção 3.2.2, "The Gauss-Markov Theorem"
[^43]: Seção 3.1, "Introduction"
[^44]: Seção 3.2, "Linear Regression Models and Least Squares"
<!-- END -->