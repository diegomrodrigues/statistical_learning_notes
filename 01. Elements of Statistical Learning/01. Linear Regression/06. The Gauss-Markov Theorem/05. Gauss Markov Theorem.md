## Biased Estimators and the Gauss-Markov Theorem: Trading Bias for Variance

### Introdução
O Teorema de Gauss-Markov estabelece a otimalidade do estimador de mínimos quadrados (OLS) dentro da classe de estimadores lineares não viesados. No entanto, essa restrição de não-viesamento pode ser limitante. Este capítulo explora a possibilidade de obter estimadores com menor erro quadrático médio (MSE) ao permitir um pequeno *bias* em troca de uma redução substancial na *variance* [^52]. Essa é uma consideração crucial, especialmente em situações onde a precisão preditiva é mais importante do que a interpretabilidade ou a garantia de não-viesamento.

### Conceitos Fundamentais

#### O Teorema de Gauss-Markov e suas Implicações
O Teorema de Gauss-Markov afirma que, sob certas condições (linearidade do modelo, erros não correlacionados com variância constante e esperança condicional correta), o estimador de mínimos quadrados ordinários (OLS) é o melhor estimador linear não viesado (BLUE – *Best Linear Unbiased Estimator*) [^51]. Isso significa que, dentro da classe de estimadores lineares não viesados, o OLS possui a menor variância. Formalmente, seja $\hat{\theta} = a^T\hat{\beta}$ o estimador OLS de um parâmetro $\theta = a^T\beta$, onde $\hat{\beta}$ é o vetor de coeficientes OLS. Se $\tilde{\theta} = c^Ty$ é qualquer outro estimador linear não viesado de $\theta$, então:
$$Var(\hat{\theta}) \leq Var(\tilde{\theta})$$
Essa desigualdade destaca a eficiência do estimador OLS dentro de sua classe.

#### Erro Quadrático Médio (MSE)
O erro quadrático médio (MSE) é uma métrica que combina a variância e o bias de um estimador, fornecendo uma medida abrangente de sua qualidade. É definido como:
$$MSE(\hat{\theta}) = E[(\hat{\theta} - \theta)^2] = Var(\hat{\theta}) + [E(\hat{\theta}) - \theta]^2$$
Onde:
*   $Var(\hat{\theta})$ é a variância do estimador
*   $[E(\hat{\theta}) - \theta]^2$ é o quadrado do bias do estimador [^51]

#### Estimadores Viesados: Uma Alternativa Promissora
Embora o Teorema de Gauss-Markov garanta a variância mínima para estimadores lineares não viesados, ele não impede a existência de estimadores viesados com MSE menor. A ideia central é que, em algumas situações, uma pequena quantidade de bias pode levar a uma redução significativa na variância, resultando em um MSE geral menor [^52].
$$MSE(\hat{\theta}_{biased}) < MSE(\hat{\theta}_{OLS})$$
Essa troca entre bias e variância é fundamental na estatística e aprendizado de máquina. Técnicas como *ridge regression* e *lasso* exploram explicitamente essa troca.

#### Ridge Regression
*Ridge regression* é uma técnica que adiciona uma penalidade à soma dos quadrados dos coeficientes no modelo de regressão linear [^61]. O objetivo é minimizar:
$$ \hat{\beta}_{ridge} = \underset{\beta}{\operatorname{argmin}} \left\\{ \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\\} $$
Aqui, $\lambda \geq 0$ é um parâmetro de complexidade que controla a quantidade de *shrinkage*. O estimador *ridge* é viesado, mas pode ter uma variância menor em comparação com o OLS, especialmente quando há multicolinearidade entre os preditores.

#### Lasso Regression
O *lasso* (Least Absolute Shrinkage and Selection Operator) é outra técnica de regularização que também introduz *bias* para reduzir a *variance*. No entanto, em vez de usar uma penalidade $L_2$ (como em *ridge regression*), o *lasso* usa uma penalidade $L_1$ [^68]:
$$ \hat{\beta}_{lasso} = \underset{\beta}{\operatorname{argmin}} \left\\{ \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\\} $$
A penalidade $L_1$ tem a propriedade de forçar alguns coeficientes a serem exatamente zero, realizando seleção de variáveis e, assim, simplificando o modelo.

#### Trade-off entre Bias e Variância
A escolha entre um estimador não viesado (como OLS) e um estimador viesado (como *ridge* ou *lasso*) depende do trade-off entre bias e variância. Um modelo com alta variância é sensível a pequenas mudanças nos dados de treinamento, enquanto um modelo com alto bias faz fortes suposições sobre os dados [^52]. A figura abaixo ilustra esse trade-off:

```
                       Alto Bias
                      (Underfitting)
                            |
                            |
Baixa Variância -------> Ponto Ideal <------- Alta Variância
                            |
                            |
                       Baixo Bias
                      (Overfitting)
```

Em situações onde o número de preditores $p$ é grande em relação ao número de observações $N$ (i.e., $p > N$), ou quando há forte multicolinearidade, os estimadores viesados tendem a ter um desempenho melhor em termos de MSE.

### Conclusão

O Teorema de Gauss-Markov fornece um resultado fundamental sobre a otimalidade do estimador OLS sob certas condições. No entanto, a restrição de não-viesamento pode ser limitante, especialmente em situações complexas. Estimadores viesados, como *ridge regression* e *lasso*, oferecem uma alternativa promissora, trocando um pouco de *bias* por uma redução substancial na *variance*, resultando em um menor erro quadrático médio (MSE). A escolha entre estimadores viesados e não viesados depende do contexto específico do problema e do trade-off entre bias e variância. Técnicas de validação cruzada podem ser usadas para ajustar o parâmetro de regularização $\lambda$ e encontrar o melhor equilíbrio entre bias e variância para um determinado conjunto de dados [^61].

### Referências
[^51]: *Página 51*, *Seção 3.2.2 The Gauss-Markov Theorem*
[^52]: *Página 52*, *The Gauss-Markov theorem implies that the least squares estimator has the smallest mean squared error of all linear estimators with no bias. However, there may well exist a biased estimator with smaller mean squared error, trading a little bias for a larger reduction in variance.*
[^61]: *Página 61*, *Seção 3.4 Shrinkage Methods*
[^68]: *Página 68*, *Seção 3.4.2 The Lasso*

<!-- END -->