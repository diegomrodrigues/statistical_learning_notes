## Capítulo 3.2.2: O Teorema de Gauss-Markov e a Eficiência dos Estimadores de Mínimos Quadrados

### Introdução
Este capítulo explora um dos resultados mais importantes na teoria da regressão linear: o **Teorema de Gauss-Markov**. Este teorema estabelece que, sob certas condições, os estimadores de mínimos quadrados (OLS) são os melhores estimadores lineares não viesados (BLUE). Ou seja, eles têm a menor variância entre todos os estimadores lineares não viesados. Este resultado é fundamental para justificar o uso generalizado dos OLS na análise de dados e modelagem estatística.

### Conceitos Fundamentais

O Teorema de Gauss-Markov se aplica ao modelo linear clássico, que assume que os erros são não correlacionados, têm variância constante ($\sigma^2$) e média zero. Formalmente, o modelo linear é dado por:

$$
y = X\beta + \epsilon
$$

onde:
*   $y$ é o vetor de respostas.
*   $X$ é a matriz de desenho (design matrix).
*   $\beta$ é o vetor de parâmetros desconhecidos.
*   $\epsilon$ é o vetor de erros aleatórios, com $E(\epsilon) = 0$ e $Var(\epsilon) = \sigma^2I$.

O estimador de mínimos quadrados (OLS) de $\beta$ é dado por [^3]:

$$
\hat{\beta} = (X^TX)^{-1}X^Ty
$$

O Teorema de Gauss-Markov afirma que, se tivermos qualquer outro estimador linear não viesado $\tilde{\beta} = Cy$ para $\beta$, então $Var(\hat{\beta}) \leq Var(\tilde{\beta})$ no sentido de que a matriz de diferença $Var(\tilde{\beta}) - Var(\hat{\beta})$ é semidefinida positiva.

**Estimador Linear Não Viesado**
Um estimador $\tilde{\beta}$ é linear se pode ser escrito como uma combinação linear das observações $y_i$. É não viesado se seu valor esperado é igual ao verdadeiro valor do parâmetro, ou seja, $E(\tilde{\beta}) = \beta$.

**Teorema de Gauss-Markov** [^51]
O Teorema de Gauss-Markov afirma que, entre todos os estimadores lineares não viesados de $\beta$, o estimador de mínimos quadrados $\hat{\beta}$ tem a menor variância. Em outras palavras, é o **melhor estimador linear não viesado (BLUE)**.

Para formalizar, considere uma combinação linear dos parâmetros $\theta = a^T\beta$, onde $a$ é um vetor conhecido. O estimador de mínimos quadrados de $\theta$ é dado por [^51]:

$$
\hat{\theta} = a^T\hat{\beta} = a^T(X^TX)^{-1}X^Ty
$$

Como $X$ é considerado fixo, $\hat{\theta}$ é uma função linear $c^Ty$ do vetor de resposta $y$, onde $c = X(X^TX)^{-1}a$. Se assumirmos que o modelo linear está correto, $a^T\hat{\beta}$ é não viesado, uma vez que [^51]:

$$
E(a^T\hat{\beta}) = a^T(X^TX)^{-1}X^TX\beta = a^T\beta
$$

O Teorema de Gauss-Markov estabelece que, se tivermos qualquer outro estimador linear $\tilde{\theta} = c^Ty$ que seja não viesado para $a^T\beta$, ou seja, $E(c^Ty) = a^T\beta$, então [^51]:

$$
Var(a^T\hat{\beta}) \leq Var(c^Ty)
$$

**Prova do Teorema de Gauss-Markov** [^51]
A prova do Teorema de Gauss-Markov envolve mostrar que a variância de qualquer outro estimador linear não viesado é maior ou igual à variância do estimador de mínimos quadrados. Seja $\tilde{\beta} = Cy$ um estimador linear qualquer de $\beta$. Para que $\tilde{\beta}$ seja não viesado, devemos ter $E(\tilde{\beta}) = \beta$. Assim:

$$
E(\tilde{\beta}) = E(Cy) = E(C(X\beta + \epsilon)) = CX\beta
$$

Para que $E(\tilde{\beta}) = \beta$, devemos ter $CX = I$, onde $I$ é a matriz identidade.
A variância de $\tilde{\beta}$ é:

$$
Var(\tilde{\beta}) = Var(Cy) = CVar(y)C^T = C(\sigma^2I)C^T = \sigma^2CC^T
$$

Agora, considere a diferença entre $C$ e $(X^TX)^{-1}X^T$:

$$
D = C - (X^TX)^{-1}X^T
$$

Como $CX = I$, temos:

$$
DX = (C - (X^TX)^{-1}X^T)X = CX - (X^TX)^{-1}X^TX = I - I = 0
$$

Agora, podemos escrever $CC^T$ como:

$$
CC^T = (D + (X^TX)^{-1}X^T)(D + (X^TX)^{-1}X^T)^T
$$
$$
CC^T = DD^T + D(X^T(X^TX)^{-1})^T + (X^TX)^{-1}X^TD^T + (X^TX)^{-1}X^T((X^TX)^{-1}X^T)^T
$$
$$
CC^T = DD^T + D(X(X^TX)^{-1})^T + (X^TX)^{-1}X^TD^T + (X^TX)^{-1}X^TX(X^TX)^{-1}
$$

Como $DX = 0$, temos $X^TD^T = 0$ e $D(X(X^TX)^{-1})^T = 0$. Portanto:

$$
CC^T = DD^T + (X^TX)^{-1}
$$

Assim, a variância de $\tilde{\beta}$ é:

$$
Var(\tilde{\beta}) = \sigma^2CC^T = \sigma^2DD^T + \sigma^2(X^TX)^{-1}
$$

Como $DD^T$ é sempre semidefinida positiva, temos que $Var(\tilde{\beta}) \geq Var(\hat{\beta})$. Portanto, o estimador de mínimos quadrados tem a menor variância entre todos os estimadores lineares não viesados. $\blacksquare$

### Conclusão
O Teorema de Gauss-Markov é um pilar fundamental na econometria e estatística, fornecendo uma justificativa teórica para o uso generalizado dos estimadores de mínimos quadrados. Ele garante que, dentro da classe de estimadores lineares não viesados, o OLS oferece a maior precisão, minimizando a variância das estimativas. No entanto, é crucial lembrar que este teorema depende das premissas do modelo linear clássico. Violações dessas premissas podem levar a estimadores que, embora ainda não viesados, não são mais os mais eficientes. Nesses casos, técnicas alternativas, como mínimos quadrados generalizados (GLS), podem ser mais apropriadas.

### Referências
[^3]: OCR página 3
[^51]: OCR página 9
<!-- END -->