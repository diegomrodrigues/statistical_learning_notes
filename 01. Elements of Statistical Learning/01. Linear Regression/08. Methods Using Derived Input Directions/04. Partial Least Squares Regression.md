## Partial Least Squares: Uma Abordagem Híbrida para Regressão com Redução de Dimensionalidade

### Introdução
Em situações com um grande número de *inputs* frequentemente correlacionados, torna-se crucial empregar métodos que reduzam a dimensionalidade do problema [^79]. As técnicas discutidas até agora envolvem a construção de um número menor de combinações lineares $Z_m$, onde $m = 1, ..., M$, dos *inputs* originais $X_j$. Essas combinações lineares $Z_m$ são então utilizadas no lugar dos *inputs* originais $X_j$ na regressão. O método de Partial Least Squares (PLS) se destaca por sua abordagem única na construção dessas combinações lineares [^80].

### Conceitos Fundamentais
Diferentemente da **Principal Component Regression (PCR)**, que utiliza apenas a matriz de *inputs* $X$ para construir as combinações lineares, o PLS emprega tanto $X$ quanto a variável resposta $y$ nesse processo [^80]. Similarmente à PCR, o PLS não é invariante à escala, portanto, assume-se que cada $x_j$ é padronizado para ter média 0 e variância 1 [^80].

O PLS inicia calculando $\phi_{1j} = \langle x_j, y \rangle$ para cada $j$. A partir disso, o *input* derivado $z_1 = \sum_j \phi_{1j}x_j$ é construído, representando a primeira direção do PLS [^80]. Assim, na construção de cada $z_m$, os *inputs* são ponderados pela força de seu efeito univariado em $y$.

O *outcome* $y$ é regredido em $z_1$, resultando no coeficiente $\theta_1$. Em seguida, os *inputs* $x_1, ..., x_p$ são ortogonalizados em relação a $z_1$. Este processo é continuado até que $M < p$ direções tenham sido obtidas. Desta forma, o PLS produz uma sequência de *inputs* ou direções derivadas e ortogonais $z_1, z_2, ..., z_M$ [^80].

**Algoritmo 3.3 Partial Least Squares**[^81]:
1. Padronize cada $x_j$ para ter média zero e variância um. Defina $\hat{y}^{(0)} = \bar{y}1$, e $x_j^{(0)} = x_j$, $j = 1, ..., p$.
2. Para $m = 1, 2, ..., p$:
   (a) $z_m = \sum_{j=1}^p \phi_{mj} x_j^{(m-1)}$, onde $\phi_{mj} = \langle x_j^{(m-1)}, y \rangle$.
   (b) $\theta_m = \langle z_m, y \rangle / \langle z_m, z_m \rangle$.
   (c) $\hat{y}^{(m)} = \hat{y}^{(m-1)} + \theta_m z_m$.
   (d) Ortogonalize cada $x_j^{(m-1)}$ em relação a $z_m$: $x_j^{(m)} = x_j^{(m-1)} - [\langle z_m, x_j^{(m-1)} \rangle / \langle z_m, z_m \rangle] z_m$, $j = 1, 2, ..., p$.
3. Retorne a sequência de vetores ajustados $\\{\hat{y}^{(m)}\\}$.

Como os $\\{z_l\\}$ são lineares nos $x_j$ originais, $\hat{y}^{(m)} = X\beta_{pls}^{(m)}$. Esses coeficientes lineares podem ser recuperados da sequência de transformações do PLS [^81].

**Importante:** Se construirmos todas as $M = p$ direções, obteremos uma solução equivalente às estimativas usuais de mínimos quadrados; usar $M < p$ direções produz uma regressão reduzida [^1].

### Conclusão
O PLS busca direções que tenham alta variância e alta correlação com a resposta. Análises adicionais revelam que o aspecto da variância tende a dominar, e assim o PLS se comporta muito como a regressão de Ridge e a regressão de componentes principais [^81]. De fato, se a matriz de *input* $X$ for ortogonal, então o PLS encontra as estimativas de mínimos quadrados após $m = 1$ passos [^81]. Passos subsequentes não têm efeito [^81].

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^79]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^80]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^81]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.

<!-- END -->