## Variância e Distribuição do Estimador de Mínimos Quadrados em Regressão Linear

### Introdução
Este capítulo aprofunda a análise do estimador de mínimos quadrados (OLS, *Ordinary Least Squares*) no contexto de modelos de regressão linear, focando especificamente na derivação da sua matriz de variância-covariância e na sua distribuição sob certas condições. Este conhecimento é crucial para a inferência estatística, permitindo a construção de testes de hipóteses e intervalos de confiança para os parâmetros do modelo.

### Conceitos Fundamentais

Em um modelo de regressão linear, assume-se que a função de regressão $E(Y|X)$ é linear nas entradas $X_1, ..., X_p$ [^1], [^2]. O estimador de mínimos quadrados $\hat{\beta}$ é obtido minimizando a soma dos quadrados residuais (RSS, *Residual Sum of Squares*):

$$ RSS(\beta) = \sum_{i=1}^{N} (y_i - f(x_i))^2 = (y - X\beta)^T(y - X\beta) $$

onde $y$ é o vetor de respostas, $X$ é a matriz de desenho (design matrix) com cada linha representando um vetor de entrada, e $\beta$ é o vetor de parâmetros a ser estimado [^3]. A solução única para $\beta$, assumindo que $X$ tem posto coluna completo, é dada por:

$$ \hat{\beta} = (X^TX)^{-1}X^Ty $$

A seguir, exploramos as propriedades estatísticas de $\hat{\beta}$ sob a suposição de que as observações $y_i$ são não correlacionadas e têm variância constante $\sigma^2$, e que os $x_i$ são fixos (não aleatórios) [^5].

**Matriz de Variância-Covariância de $\hat{\beta}$**

Sob as suposições mencionadas, a matriz de variância-covariância de $\hat{\beta}$ pode ser facilmente derivada de (3.6) [^5]. Como $\hat{\beta} = (X^TX)^{-1}X^Ty$ e $y = X\beta + \epsilon$, onde $\epsilon$ é o termo de erro, temos:

$$ \hat{\beta} = (X^TX)^{-1}X^T(X\beta + \epsilon) = \beta + (X^TX)^{-1}X^T\epsilon $$

Portanto,

$$ Var(\hat{\beta}) = Var(\beta + (X^TX)^{-1}X^T\epsilon) = Var((X^TX)^{-1}X^T\epsilon) $$

Como $(X^TX)^{-1}X^T$ é uma matriz constante,

$$ Var(\hat{\beta}) = (X^TX)^{-1}X^T Var(\epsilon) ((X^TX)^{-1}X^T)^T $$

Assumindo que $Var(\epsilon) = \sigma^2I$, onde $I$ é a matriz identidade,

$$ Var(\hat{\beta}) = (X^TX)^{-1}X^T (\sigma^2I) X(X^TX)^{-1} = \sigma^2 (X^TX)^{-1}X^TX(X^TX)^{-1} $$

Finalmente,

$$ Var(\hat{\beta}) = \sigma^2 (X^TX)^{-1} $$ [^5]

Esta matriz de variância-covariância é crucial para a inferência estatística.

**Distribuição do Estimador de Mínimos Quadrados sob Erros Gaussianos**

Além das suposições anteriores, se assumirmos que o termo de erro $\epsilon$ segue uma distribuição normal multivariada com média zero e variância $\sigma^2$, ou seja, $\epsilon \sim N(0, \sigma^2I)$, então o estimador de mínimos quadrados $\hat{\beta}$ também seguirá uma distribuição normal multivariada [^5].

Dado que $\hat{\beta} = \beta + (X^TX)^{-1}X^T\epsilon$ e $\epsilon \sim N(0, \sigma^2I)$, então:

$$ \hat{\beta} \sim N(\beta, (X^TX)^{-1}\sigma^2) $$ [^5]

Isto significa que $\hat{\beta}$ é não-enviesado e sua distribuição é completamente caracterizada pela sua média (o verdadeiro valor de $\beta$) e sua matriz de variância-covariância (derivada acima). Esta propriedade é fundamental para a construção de testes de hipóteses e intervalos de confiança para os parâmetros $\beta_j$.

**Inferência Estatística**

Com a distribuição de $\hat{\beta}$ estabelecida, podemos realizar testes de hipóteses para os parâmetros do modelo. Por exemplo, para testar a hipótese nula de que um coeficiente particular $\beta_j$ é igual a zero, podemos formar a estatística Z (Z-score):

$$ z_j = \frac{\hat{\beta}_j}{\hat{\sigma}_j} $$

onde $\hat{\sigma}_j$ é o erro padrão estimado de $\hat{\beta}_j$, que é a raiz quadrada do j-ésimo elemento diagonal de $(X^TX)^{-1}\hat{\sigma}^2$, e $\hat{\sigma}^2$ é uma estimativa não-enviesada de $\sigma^2$ [^5]:

$$ \hat{\sigma}^2 = \frac{1}{N-p-1} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 $$

Sob a hipótese nula, $z_j$ segue uma distribuição *t* de Student com $N-p-1$ graus de liberdade [^5], [^6]. Para tamanhos de amostra grandes, a distribuição *t* se aproxima da distribuição normal padrão [^6].

Também podemos construir intervalos de confiança para os parâmetros $\beta_j$ usando a distribuição normal ou *t*:

$$ \hat{\beta}_j \pm z^{(1-\alpha/2)} \hat{\sigma}_j $$

onde $z^{(1-\alpha/2)}$ é o quantil $(1-\alpha/2)$-ésimo da distribuição normal padrão [^7].

### Conclusão

A derivação da matriz de variância-covariância e a determinação da distribuição do estimador de mínimos quadrados são passos cruciais para a realização de inferência estatística em modelos de regressão linear. As suposições subjacentes (observações não correlacionadas com variância constante e erros Gaussianos) são importantes e devem ser verificadas na prática. O conhecimento da distribuição de $\hat{\beta}$ permite a construção de testes de hipóteses e intervalos de confiança, fornecendo uma base sólida para a interpretação e validação dos resultados do modelo [^5].

### Referências
[^1]: Page 43, *Linear Methods for Regression*
[^2]: Page 43, *A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,..., Xp.*
[^3]: Page 44, *Typically we have a set of training data (X1,Y1) ... (xn, yn) from which to estimate the parameters β.*
[^4]: Page 45, *β = (XTX)-1X7y.*
[^5]: Page 47, *Var(3) = (XTX)-102.*, *β ~ N(β, (XX)-102).*
[^6]: Page 48, *zj is distributed as tn-p-1 (a t distribution with N – p −1 degrees of freedom)*
[^7]: Page 49, *(β₁ – 2 (1-0), β₁ + z(1-a) vô).*
<!-- END -->