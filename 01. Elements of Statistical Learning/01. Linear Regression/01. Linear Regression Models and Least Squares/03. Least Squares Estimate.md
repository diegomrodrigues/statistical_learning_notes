## Estimativa de Mínimos Quadrados e Regularização em Regressão Linear

### Introdução
Este capítulo aprofunda a **estimativa de mínimos quadrados** (OLS - Ordinary Least Squares) em modelos de regressão linear, abordando cenários onde a matriz de design $X$ não possui posto coluna completo. Como vimos anteriormente [^3], a regressão linear busca modelar a relação entre uma variável dependente $Y$ e um conjunto de variáveis independentes $X_1, ..., X_p$ através de uma função linear [^1]. O método de mínimos quadrados é amplamente utilizado para estimar os parâmetros do modelo, minimizando a soma dos quadrados dos resíduos [^2]. No entanto, a aplicabilidade direta da solução OLS depende crucialmente da **não singularidade da matriz $X^TX$**.

### Conceitos Fundamentais
A **estimativa de mínimos quadrados** para o vetor de coeficientes $\beta$ é dada por:

$$beta = (X^TX)^{-1}X^Ty$$

Esta solução é válida sob a condição de que $X$ tenha posto coluna completo, o que implica que $X^TX$ é **positiva definida** [^3]. A condição de posto completo garante que a matriz $X^TX$ seja invertível, permitindo uma solução única para $\beta$.

#### Deficiência de Posto e Multicolinearidade
A **deficiência de posto** em $X$ ocorre quando as colunas de $X$ não são linearmente independentes [^4]. Isso pode ser causado por:

1.  **Entradas Perfeitamente Correlacionadas:** Duas ou mais variáveis independentes são altamente correlacionadas, tornando redundante a informação que elas fornecem [^4]. Por exemplo, se $x_2 = 3x_1$, as colunas correspondentes em $X$ serão linearmente dependentes.
2.  **Codificação Redundante de Entradas Qualitativas:** Ao codificar variáveis categóricas usando *dummy variables*, pode-se introduzir redundância se não forem tomadas as devidas precauções [^4]. Por exemplo, se uma variável categórica tem cinco níveis, usar cinco *dummy variables* sem remover o intercepto do modelo causa multicolinearidade.

Quando $X$ não tem posto completo, $X^TX$ torna-se **singular** (não invertível), e a solução OLS padrão não pode ser aplicada diretamente [^4]. Matematicamente, isso significa que o determinante de $X^TX$ é zero.

#### Abordagens para Lidar com a Deficiência de Posto
Existem várias maneiras de lidar com a deficiência de posto em $X$ [^4]:

1.  **Recodificação:** Ajustar a codificação das variáveis qualitativas para evitar redundância. Isso pode envolver a combinação de categorias ou a remoção de *dummy variables* redundantes.
2.  **Remoção de Colunas Redundantes:** Identificar e remover colunas de $X$ que são linearmente dependentes de outras colunas.
3.  **Regularização:** Adicionar um termo de penalidade à função de custo OLS, que restringe a magnitude dos coeficientes e torna o problema bem-posto. As técnicas de regularização mais comuns são a *Ridge Regression* e o *Lasso*.

#### Regularização: Ridge Regression
A **Ridge Regression** (ou *Regularização L2*) adiciona um termo de penalidade à soma dos quadrados dos resíduos, proporcional ao quadrado da norma L2 dos coeficientes [^19]:

$$beta_{ridge} = \underset{\beta}{\text{argmin}} \left\{ \sum_{i=1}^N (y_i - x_i^T\beta)^2 + \lambda \sum_{j=1}^p \beta_j^2 \right\}$$

onde $\lambda \geq 0$ é o **parâmetro de regularização**. A solução para a Ridge Regression é:

$$beta_{ridge} = (X^TX + \lambda I)^{-1}X^Ty$$

onde $I$ é a matriz identidade. A adição de $\lambda I$ garante que a matriz $(X^TX + \lambda I)$ seja invertível, mesmo quando $X^TX$ é singular [^22]. O parâmetro $\lambda$ controla a quantidade de regularização; valores maiores de $\lambda$ levam a coeficientes menores e maior viés, mas menor variância [^21].

#### Regularização: Lasso
O **Lasso** (ou *Regularização L1*) adiciona um termo de penalidade à soma dos quadrados dos resíduos, proporcional à norma L1 dos coeficientes [^26]:

$$beta_{lasso} = \underset{\beta}{\text{argmin}} \left\{ \sum_{i=1}^N (y_i - x_i^T\beta)^2 + \lambda \sum_{j=1}^p |\beta_j| \right\}$$

Ao contrário da Ridge Regression, a penalidade L1 do Lasso tem a propriedade de forçar alguns coeficientes a serem exatamente zero, realizando assim a seleção de variáveis [^27]. Isso torna o Lasso útil para identificar as variáveis mais importantes em modelos com muitas *features*. Não há uma solução analítica para o Lasso, e métodos iterativos são usados para encontrar a solução [^27].

#### Gauss-Markov Theorem
O **Teorema de Gauss-Markov** estabelece que, sob certas condições (erros com média zero, variância constante e não correlacionados), a estimativa de mínimos quadrados é o melhor estimador linear não viesado (BLUE - Best Linear Unbiased Estimator) [^51]. No entanto, este teorema se aplica apenas quando $X$ tem posto completo e não considera estimadores viesados. As técnicas de regularização introduzem viés para reduzir a variância, muitas vezes resultando em um erro quadrático médio (MSE - Mean Squared Error) menor do que o OLS [^52].

### Conclusão
Quando a matriz de design $X$ não tem posto coluna completo, a solução OLS padrão não é aplicável. A deficiência de posto pode ser causada por multicolinearidade ou codificação redundante. Nesses casos, técnicas de recodificação, remoção de colunas redundantes ou regularização (Ridge e Lasso) são necessárias para obter uma solução estável e interpretabilizada [^4]. A escolha entre Ridge e Lasso depende do problema específico, com o Lasso sendo preferível quando a seleção de variáveis é importante [^27]. Embora a regularização introduza viés, ela pode reduzir a variância e melhorar o desempenho preditivo do modelo [^52].
### Referências
[^3]:  "...The least squares estimate is given by \u03b2 = (XTX)-1XTy, assuming X has full column rank, and XTX is positive definite; otherwise, regularization techniques are needed..."
[^1]: "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,..., Xp."
[^2]: "The most popular estimation method is least squares, in which we pick the coefficients β = (βο, β1, ..., βp)T to minimize the residual sum of squares"
[^4]: "...Rank deficiencies in X can occur due to perfectly correlated inputs or redundant coding of qualitative inputs, necessitating recoding, dropping redundant columns, or regularization."
[^19]: "Ridge regression shrinks the regression coefficients by imposing a penalty on their size."
[^21]: "Here ≥ 0 is a complexity parameter that controls the amount of shrinkage: the larger the value of A, the greater the amount of shrinkage."
[^22]: "The solution adds a positive constant to the diagonal of XTX before inversion. This makes the problem nonsingular, even if XTX is not of full rank..."
[^26]: "The lasso is a shrinkage method like ridge, with subtle but important dif- ferences. The lasso estimate is defined by..."
[^27]: "Because of the nature of the constraint, making t sufficiently small will cause some of the coefficients to be exactly zero. Thus the lasso does a kind of continuous subset selection..."
[^51]: "One of the most famous results in statistics asserts that the least squares estimates of the parameters β have the smallest variance among all linear unbiased estimates."
[^52]: "The Gauss-Markov theorem implies that the least squares estimator has the smallest mean squared error of all linear estimators with no bias. However, there may well exist a biased estimator with smaller mean squared error."
<!-- END -->