## A Matriz Hat: Projeção Ortogonal e Previsões de Mínimos Quadrados

### Introdução
Este capítulo aprofunda o conceito da **matriz hat** (*hat matrix*)  $H = X(X^TX)^{-1}X^T$ [^4], também conhecida como matriz de projeção (*projection matrix*). Exploraremos como essa matriz computa a projeção ortogonal de *y* no espaço das colunas de *X*, e como os valores ajustados $\hat{y} = X\hat{\beta}$ [^4] representam a projeção de *y* nesse mesmo espaço, fornecendo o vetor de previsões de mínimos quadrados. Este tópico se baseia nos conceitos fundamentais de regressão linear e mínimos quadrados, estabelecendo uma ponte entre a álgebra linear e a estatística [^3].

### Conceitos Fundamentais

A **matriz hat** $H = X(X^TX)^{-1}X^T$ [^4] desempenha um papel crucial na regressão linear. Para compreendê-la, vamos revisitar alguns conceitos-chave:

1.  **Espaço das Colunas de X:** As colunas da matriz *X*, denotadas por $x_0, x_1, ..., x_p$ [^4], com $x_0 = 1$ [^4], formam um subespaço de $\mathbb{R}^N$ [^4]. Este subespaço é conhecido como o espaço das colunas de *X*.

2.  **Projeção Ortogonal:** Dado um vetor *y* em $\mathbb{R}^N$, a projeção ortogonal de *y* no espaço das colunas de *X* é o vetor $\hat{y}$ que minimiza a distância euclidiana $||y - \hat{y}||^2$ [^3].

3.  **Estimativa de Mínimos Quadrados:** A estimativa de mínimos quadrados $\hat{\beta}$ é obtida resolvendo a equação normal $X^T(y - X\hat{\beta}) = 0$ [^3], o que leva a $\hat{\beta} = (X^TX)^{-1}X^Ty$ [^3].

4.  **Valores Ajustados:** Os valores ajustados são dados por $\hat{y} = X\hat{\beta}$ [^4], que, substituindo $\hat{\beta}$, resulta em $\hat{y} = X(X^TX)^{-1}X^Ty$ [^4].

**A Matriz Hat como Projeção Ortogonal**

A matriz *H* transforma o vetor *y* em sua projeção ortogonal $\hat{y}$ no espaço das colunas de *X* [^4]. Ou seja, $\hat{y} = Hy$ [^4]. Isso significa que *H* é uma matriz de projeção.

**Propriedades da Matriz Hat:**

1.  **Idempotência:** Uma matriz de projeção *H* é idempotente, ou seja, $H^2 = H$ [^4].

    *Prova:*
    $$H^2 = (X(X^TX)^{-1}X^T)(X(X^TX)^{-1}X^T) = X(X^TX)^{-1}(X^TX)(X^TX)^{-1}X^T = X(X^TX)^{-1}X^T = H$$
    $\blacksquare$

2.  **Simetria:** Uma matriz de projeção *H* é simétrica, ou seja, $H^T = H$ [^4].

    *Prova:*
    $$H^T = (X(X^TX)^{-1}X^T)^T = (X^T)^T((X^TX)^{-1})^T(X)^T = X((X^TX)^T)^{-1}X^T = X(X^TX)^{-1}X^T = H$$
    $\blacksquare$

3. **Ortogonalidade do Resíduo:** O vetor resíduo $e = y - \hat{y}$ é ortogonal ao espaço das colunas de *X* [^4]. Isso significa que $X^Te = 0$ [^3].

   *Prova:*
    $$e = y - \hat{y} = y - X(X^TX)^{-1}X^Ty = (I - H)y$$
    $$X^Te = X^T(I - H)y = (X^T - X^TH)y = (X^T - X^TX(X^TX)^{-1}X^T)y = (X^T - X^T)y = 0$$
    $\blacksquare$

**Interpretação Geométrica**

Geometricamente, a matriz hat *H* projeta o vetor *y* ortogonalmente no hiperplano formado pelo espaço das colunas de *X* [^4]. O vetor resíduo *e* representa a distância mínima entre *y* e o hiperplano [^4]. A Figura 3.2 [^4] ilustra essa representação geométrica em $\mathbb{R}^N$.

**Rank da Matriz Hat**

Se *X* tem posto coluna completo, então o posto da matriz hat *H* é igual ao número de colunas de *X*, ou seja, $rank(H) = p + 1$ [^3].

### Conclusão

A matriz hat *H* é uma ferramenta fundamental na regressão linear, fornecendo uma maneira direta de calcular a projeção ortogonal de *y* no espaço das colunas de *X* [^4]. Suas propriedades de idempotência e simetria garantem que ela realmente realize uma projeção ortogonal. Além disso, a ortogonalidade do resíduo ao espaço das colunas de *X* confirma que os valores ajustados $\hat{y}$ representam a melhor aproximação linear de *y* [^3]. Entender a matriz hat é essencial para compreender a geometria e as propriedades estatísticas da regressão linear [^4].

### Referências
[^3]: Página 45
[^4]: Página 46
<!-- END -->