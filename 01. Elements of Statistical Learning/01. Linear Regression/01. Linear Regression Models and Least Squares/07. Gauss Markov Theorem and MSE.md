## O Teorema de Gauss-Markov

### Introdução
Este capítulo aprofunda o Teorema de Gauss-Markov, um pilar fundamental na análise de modelos de regressão linear. O teorema estabelece a otimalidade do estimador de mínimos quadrados ordinários (OLS) sob certas condições, garantindo que ele seja o melhor estimador linear não viesado (BLUE) [^51]. Exploraremos as implicações, as condições de validade e as limitações do teorema, bem como suas conexões com outros conceitos importantes na regressão linear, como o erro quadrático médio (MSE) [^51].

### Conceitos Fundamentais

O **Teorema de Gauss-Markov** afirma que, dentro da classe de **estimadores lineares não viesados**, o estimador de **mínimos quadrados ordinários (OLS)** possui a **menor variância** [^51]. Isso significa que, se buscarmos um estimador que seja uma combinação linear das observações e que, em média, acerte o valor verdadeiro do parâmetro, então o estimador OLS será o que fornecerá a menor dispersão em torno desse valor verdadeiro.

Para entender completamente o teorema, é crucial definir os termos-chave:

*   **Estimador Linear:** Um estimador $\hat{\theta}$ é linear se pode ser expresso como uma combinação linear das observações $y_i$:

    $$hat{\theta} = \sum_{i=1}^{N} a_i y_i = \mathbf{a}^T \mathbf{y}$$

    onde $a_i$ são coeficientes constantes e $\mathbf{a}$ é um vetor de coeficientes [^51]. No contexto da regressão linear, o estimador OLS $\hat{\beta} = (X^TX)^{-1}X^T\mathbf{y}$ é linear porque é uma transformação linear do vetor de resposta $\mathbf{y}$ [^3, ^3.6].
*   **Estimador Não Viesado:** Um estimador $\hat{\theta}$ é não viesado se seu valor esperado é igual ao valor verdadeiro do parâmetro $\theta$:

    $$E[\hat{\theta}] = \theta$$

    Em outras palavras, em média, o estimador acerta o valor verdadeiro. Para o estimador OLS, sob a suposição de que o modelo linear está correto (i.e., $E[Y|X] = X\beta$) e que $X$ é fixo (não aleatório), temos [^47, ^3.9]:

    $$E[\hat{\beta}] = E[(X^TX)^{-1}X^T\mathbf{y}] = (X^TX)^{-1}X^TE[\mathbf{y}] = (X^TX)^{-1}X^TX\beta = \beta$$

    Portanto, o estimador OLS é não viesado sob essas condições [^51].
*   **Melhor Estimador Linear Não Viesado (BLUE):** BLUE (*Best Linear Unbiased Estimator*), é o estimador que possui a menor variância dentro da classe de estimadores lineares não viesados [^51]. O Teorema de Gauss-Markov garante que o estimador OLS é BLUE.

**Prova do Teorema de Gauss-Markov:**

Considere um outro estimador linear não viesado $\tilde{\beta} = C\mathbf{y}$, onde $C$ é uma matriz de constantes.  Como $\tilde{\beta}$ é não viesado, temos:

$$E[\tilde{\beta}] = E[C\mathbf{y}] = CX\beta = \beta$$

Isso implica que $CX = I$, onde $I$ é a matriz identidade. A variância de $\tilde{\beta}$ é:

$$Var(\tilde{\beta}) = Var(C\mathbf{y}) = CVar(\mathbf{y})C^T = C(\sigma^2I)C^T = \sigma^2CC^T$$

A variância do estimador OLS é [^47, ^3.8]:

$$Var(\hat{\beta}) = \sigma^2(X^TX)^{-1}$$

Para provar o teorema, precisamos mostrar que a diferença entre as matrizes de covariância de $\tilde{\beta}$ e $\hat{\beta}$ é semi-definida positiva:

$$Var(\tilde{\beta}) - Var(\hat{\beta}) = \sigma^2[CC^T - (X^TX)^{-1}]$$

Como $CX = I$, podemos escrever $C = (X^TX)^{-1}X^T + D$, onde $DX = 0$. Substituindo isso na expressão da variância de $\tilde{\beta}$:

$$Var(\tilde{\beta}) = \sigma^2[( (X^TX)^{-1}X^T + D ) ( (X^TX)^{-1}X^T + D )^T ] = \sigma^2[ (X^TX)^{-1} + DD^T ]$$

Portanto:
$$Var(\tilde{\beta}) - Var(\hat{\beta}) = \sigma^2DD^T$$

Como $DD^T$ é sempre semi-definida positiva, $Var(\tilde{\beta}) - Var(\hat{\beta})$ também é semi-definida positiva, o que implica que [^51]:

$$Var(\hat{\beta}) \leq Var(\tilde{\beta})$$

Isso prova que o estimador OLS tem a menor variância entre todos os estimadores lineares não viesados. $\blacksquare$

**Condições para a Validade do Teorema de Gauss-Markov:**

O Teorema de Gauss-Markov é válido sob as seguintes condições, também conhecidas como as **suposições de Gauss-Markov**:

1.  **Linearidade:** O modelo deve ser linear nos parâmetros [^44, ^3.1]. Isso significa que a relação entre as variáveis independentes e a variável dependente pode ser expressa como uma combinação linear dos parâmetros [^44, ^3.1].
2.  **Exogeneidade Forte:** O valor esperado do erro deve ser zero condicional às variáveis independentes: $E[\epsilon|X] = 0$ [^47, ^3.9]. Isso implica que as variáveis independentes não são correlacionadas com o termo de erro.
3.  **Homocedasticidade:** A variância do erro deve ser constante para todas as observações: $Var(\epsilon_i) = \sigma^2$ para todo $i$ [^47].
4.  **Não Autocorrelação:** Os erros devem ser não correlacionados entre si: $Cov(\epsilon_i, \epsilon_j) = 0$ para todo $i \neq j$ [^47].
5.  **Posto Completo:** A matriz de variáveis independentes $X$ deve ter posto coluna completo [^3]. Isso garante que $(X^TX)^{-1}$ exista.

**Violações das Suposições de Gauss-Markov:**

Se as suposições de Gauss-Markov não forem satisfeitas, o estimador OLS pode não ser BLUE. Por exemplo [^51]:

*   **Heterocedasticidade:** Se a variância do erro não for constante, o estimador OLS ainda será não viesado, mas não será o mais eficiente. Estimadores de mínimos quadrados generalizados (GLS) podem ser usados para lidar com a heterocedasticidade.
*   **Autocorrelação:** Se os erros forem correlacionados, o estimador OLS também não será o mais eficiente. Novamente, estimadores GLS podem ser usados.
*   **Não Linearidade:** Se a relação entre as variáveis não for linear, o estimador OLS será viesado. Nesses casos, pode ser necessário transformar as variáveis ou usar modelos não lineares [^43, ^3.1].
*   **Endogeneidade:** Se as variáveis independentes forem correlacionadas com o termo de erro, o estimador OLS será viesado e inconsistente. Técnicas como variáveis instrumentais podem ser usadas para lidar com a endogeneidade.

**Erro Quadrático Médio (MSE):**

O **erro quadrático médio (MSE)** de um estimador $\hat{\theta}$ ao estimar $\theta$ é definido como [^51]:

$$MSE(\hat{\theta}) = E[(\hat{\theta} - \theta)^2] = Var(\hat{\theta}) + [E(\hat{\theta}) - \theta]^2$$

O MSE é uma medida que combina a variância e o viés de um estimador [^51]. Um estimador com baixo MSE é aquele que tem tanto baixa variância quanto baixo viés. O Teorema de Gauss-Markov garante que, dentro da classe de estimadores lineares não viesados, o estimador OLS tem o menor MSE, uma vez que o viés é zero e a variância é minimizada [^51].

É importante notar que o Teorema de Gauss-Markov se aplica apenas a estimadores lineares não viesados. Em algumas situações, pode ser vantajoso usar um estimador viesado se ele tiver uma variância significativamente menor do que o estimador OLS, resultando em um MSE menor [^51]. Essa é a ideia por trás de técnicas como a regressão de *ridge* e o *lasso* [^52, ^3.4, ^3.4.2].

### Conclusão

O Teorema de Gauss-Markov é um resultado fundamental na teoria da regressão linear. Ele fornece uma justificativa teórica para o uso do estimador OLS, mostrando que ele é o melhor estimador linear não viesado sob certas condições [^51]. No entanto, é importante estar ciente das limitações do teorema e das suposições subjacentes. Quando as suposições de Gauss-Markov não são satisfeitas, outros estimadores podem ser mais apropriados [^51]. Além disso, em algumas situações, pode ser vantajoso usar estimadores viesados se eles tiverem uma variância significativamente menor, resultando em um MSE menor [^51]. Técnicas como a regressão de *ridge* e o *lasso*, exploram esse trade-off entre viés e variância [^52, ^3.4, ^3.4.2].

### Referências
[^3]: 3, Linear Methods for Regression
[^43]: 3.1 Introduction
[^44]: 3.2 Linear Regression Models and Least Squares
[^47]: 3.2 Linear Regression Models and Least Squares
[^51]: 3.2.2 The Gauss-Markov Theorem
[^52]: 3. Linear Methods for Regression
<!-- END -->