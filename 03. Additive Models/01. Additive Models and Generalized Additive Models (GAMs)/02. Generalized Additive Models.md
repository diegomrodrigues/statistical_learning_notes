## Generalized Additive Models (GAMs): A Detailed Exploration

### Introdução
Este capítulo aprofunda o estudo dos **Generalized Additive Models (GAMs)**, uma extensão dos modelos lineares que oferece maior flexibilidade ao permitir funções não lineares de cada preditor [^1]. GAMs são ferramentas poderosas para identificar e caracterizar efeitos de regressão não lineares, mantendo a interpretabilidade dos modelos lineares tradicionais devido à sua natureza aditiva [^1]. Em continuidade ao que foi abordado nos capítulos anteriores sobre técnicas que utilizam funções de base predefinidas para alcançar não linearidades, este capítulo se concentra em métodos estatísticos flexíveis e automáticos que podem ser usados para identificar e caracterizar efeitos de regressão não lineares [^1].

### Conceitos Fundamentais

A forma geral de um GAM em um cenário de regressão é dada por [^1]:
$$
E(Y|X_1, X_2, ..., X_p) = \alpha + f_1(X_1) + f_2(X_2) + ... + f_p(X_p)
$$
onde $Y$ é a variável resposta, $X_1, X_2, ..., X_p$ são os preditores, $\alpha$ é o intercepto, e $f_j$ são funções suaves e não especificadas que descrevem a relação não linear entre cada preditor $X_j$ e a resposta $Y$ [^1].  Essas funções $f_j$ são estimadas de forma não paramétrica, frequentemente utilizando *scatterplot smoothers* como *cubic smoothing splines* ou *kernel smoothers*, para capturar as relações não lineares entre os preditores e a variável resposta [^1].

Em problemas de classificação binária, o GAM pode ser estendido para um modelo de regressão logística aditivo, onde a média da resposta binária $\mu(X) = Pr(Y = 1|X)$ é relacionada aos preditores através de uma função *logit* [^1]:
$$
\log \left( \frac{\mu(X)}{1 - \mu(X)} \right) = \alpha + f_1(X_1) + ... + f_p(X_p)
$$
De forma mais geral, a média condicional $\mu(X)$ de uma resposta $Y$ é relacionada a uma função aditiva dos preditores através de uma função de ligação $g$ [^1]:
$$
g[\mu(X)] = \alpha + f_1(X_1) + ... + f_p(X_p)
$$
Exemplos de funções de ligação clássicas incluem [^1]:
*   $g(\mu) = \mu$, a *identity link*, utilizada para modelos lineares e aditivos para dados de resposta Gaussianos.
*   $g(\mu) = \text{logit}(\mu)$ ou $g(\mu) = \text{probit}(\mu)$, a *probit link function*, para modelagem de probabilidades binomiais.
*   $g(\mu) = \log(\mu)$, para modelos *log-lineares* ou *log-aditivos* para dados de contagem de Poisson.

A estimativa das funções $f_j$ é realizada de maneira flexível, utilizando um algoritmo cujo bloco de construção básico é um *scatterplot smoother* [^2]. O algoritmo de *backfitting* (Algoritmo 9.1) é usado para estimar simultaneamente todas as $p$ funções [^2].

**Algoritmo 9.1: O Algoritmo de Backfitting para Modelos Aditivos** [^4]
1.  Inicialize: $\alpha = \frac{1}{N} \sum_{i=1}^{N} y_i$, $f_j = 0, \forall i, j$.
2.  Ciclo: $j = 1, 2, ..., p, ..., 1, 2, ..., p, ...,$
    $$
    f_j \leftarrow S_j \left[ \{ y_i - \alpha - \sum_{k \neq j} f_k(x_{ik}) \}_{i=1}^{N} \right]
    $$
    $$
    f_j \leftarrow f_j - \frac{1}{N} \sum_{i=1}^{N} f_j(x_{ij})
    $$
    até que as funções $f_j$ mudem menos do que um limiar pré-especificado.

O algoritmo de *backfitting* envolve ajustar iterativamente cada função $f_j$ enquanto mantém as outras funções fixas, até que a convergência seja alcançada [^4].

É importante notar que nem todas as funções $f_j$ precisam ser não lineares [^3]. Podemos facilmente misturar formas lineares e outras formas paramétricas com os termos não lineares, uma necessidade quando algumas das entradas são variáveis qualitativas (fatores) [^3]. Os termos não lineares também não são restritos a efeitos principais; podemos ter componentes não lineares em duas ou mais variáveis ou curvas separadas em $X_j$ para cada nível do fator $X_k$ [^3].

Additive models podem substituir modelos lineares em uma ampla variedade de configurações, por exemplo, uma decomposição aditiva de séries temporais [^3]:
$$
Y_t = S_t + T_t + \epsilon_t
$$
onde $S_t$ é um componente sazonal, $T_t$ é uma tendência e $\epsilon$ é um termo de erro [^3].

Para modelos de regressão logística e outros modelos aditivos generalizados, o critério apropriado é uma *penalized log-likelihood* [^5]. Para maximizá-la, o procedimento de *backfitting* é usado em conjunto com um *likelihood maximizer* [^5]. A rotina usual de Newton-Raphson para maximizar *log-likelihoods* em modelos lineares generalizados pode ser reformulada como um algoritmo IRLS (iteratively reweighted least squares) [^5].

**Algoritmo 9.2: Local Scoring Algorithm for the Additive Logistic Regression Model** [^6]
1.  Compute starting values: $\alpha = \log[\bar{y}/(1 - \bar{y})]$, where $\bar{y} = \text{ave}(y_i)$, the sample proportion of ones, and set $f_j = 0 \ \forall j$.
2.  Define $\eta_i = \alpha + \sum_j f_j(x_{ij})$ and $p_i = 1/[1 + \exp(-\eta_i)]$. Iterate:
    (a) Construct the working target variable
    $$
    z_i = \eta_i + \frac{(y_i - p_i)}{p_i(1 - p_i)}
    $$
    (b) Construct weights $w_i = p_i(1 - p_i)$
    (c) Fit an additive model to the targets $z_i$ with weights $w_i$, using a weighted backfitting algorithm. This gives new estimates $\hat{\alpha}, \hat{f_j}, \forall j$.
3.  Continue step 2. until the change in the functions falls below a pre-specified threshold.

### Conclusão

Os **Generalized Additive Models (GAMs)** oferecem uma extensão valiosa dos modelos lineares, tornando-os mais flexíveis, mantendo grande parte de sua interpretabilidade [^1, 3]. As ferramentas familiares para modelagem e inferência em modelos lineares também estão disponíveis para modelos aditivos [^3]. O procedimento de *backfitting* para ajustar esses modelos é simples e modular, permitindo que se escolha um método de ajuste apropriado para cada variável de entrada [^3]. Como resultado, eles se tornaram amplamente utilizados na comunidade estatística [^3].

No entanto, os modelos aditivos podem ter limitações para aplicações de *data mining* em larga escala [^10]. O algoritmo de *backfitting* ajusta todos os preditores, o que não é viável ou desejável quando um grande número está disponível [^10].

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^2]: Hastie, T., & Tibshirani, R. (1990). *Generalized Additive Models*. Chapman and Hall/CRC.
[^3]: Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). *Classification and Regression Trees*. Wadsworth.
[^4]: Ripley, B. D. (1996). *Pattern Recognition and Neural Networks*. Cambridge University Press.
[^5]: Quinlan, J. R. (1993). *C4.5: Programs for Machine Learning*. Morgan Kaufmann.
[^6]: Friedman, J. H. (1991). Multivariate adaptive regression splines. *The Annals of Statistics*, *19*(1), 1-67.
[^7]: Stone, C. J., Hansen, M. H., Kooperberg, C., & Truong, Y. K. (1997). Polynomial splines and their tensor products in extended linear modeling. *The Annals of Statistics*, *25*(3), 1371-1470.
[^8]: Lin, D. Y., & Zhang, H. (2006). Component selection and smoothing operator via composite likelihood with a diverging number of parameters. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, *68*(5), 791-814.
[^9]: Ravikumar, P., Lafferty, J., Liu, H., & Wasserman, L. (2008). Sparse additive models. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, *71*(5), 1009-1030.
[^10]: Hastie, T., Tibshirani, R., Buja, A. (1989).  Flexible discriminant analysis by optimal scoring. *Journal of the American Statistical Association*, *89*, 1255-1270.

<!-- END -->