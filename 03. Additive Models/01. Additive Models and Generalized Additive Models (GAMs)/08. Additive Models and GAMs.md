## Capítulo 9.1: Modelos Aditivos Generalizados (GAMs): Uma Extensão Flexível dos Modelos Lineares

### Introdução
Este capítulo explora os **Modelos Aditivos Generalizados (GAMs)**, uma extensão dos modelos lineares que oferece maior flexibilidade ao mesmo tempo em que preserva grande parte de sua interpretabilidade [^1]. Os GAMs são particularmente úteis em situações onde os efeitos das variáveis preditoras não são lineares, permitindo uma modelagem mais precisa e adaptável [^1]. Este capítulo abordará a formulação matemática dos GAMs, os métodos de ajuste, e um exemplo prático de aplicação em classificação binária.

### Conceitos Fundamentais

#### 9.1.1 Formulação Matemática dos GAMs
Um modelo aditivo generalizado, no contexto de regressão, assume a seguinte forma [^1]:
$$E(Y|X_1, X_2, ..., X_p) = \alpha + f_1(X_1) + f_2(X_2) + ... + f_p(X_p),$$
onde $Y$ é a variável resposta, $X_1, X_2, ..., X_p$ são as variáveis preditoras, $\alpha$ é o intercepto, e $f_i(X_i)$ são funções *suaves* e não especificadas que descrevem a relação não linear entre cada preditor e a resposta [^2].  A suavidade dessas funções permite que o modelo capture relações complexas sem impor uma forma funcional rígida [^2].

Para classificação binária, o modelo aditivo logístico de regressão é definido como [^2]:

$$log\frac{\mu(X)}{1 - \mu(X)} = \alpha + f_1(X_1) + ... + f_p(X_p),$$
onde $\mu(X) = Pr(Y = 1|X)$ é a probabilidade condicional da resposta binária, e $f_i$ são funções suaves [^2].

De forma mais geral, a média condicional $\mu(X)$ da resposta $Y$ está relacionada a uma função aditiva dos preditores através de uma função de *link* $g$ [^2]:

$$g[\mu(X)] = \alpha + f_1(X_1) + ... + f_p(X_p).$$

Exemplos de funções de *link* clássicas incluem [^2]:

*   $g(\mu) = \mu$ (função identidade) para modelos lineares e aditivos com dados de resposta Gaussianos.
*   $g(\mu) = logit(\mu)$ ou $g(\mu) = probit(\mu)$ para modelagem de probabilidades binomiais.
*   $g(\mu) = log(\mu)$ para modelos log-lineares ou log-aditivos para dados de contagem de Poisson.

Essas funções de *link* derivam de modelos de amostragem de famílias exponenciais, que também incluem distribuições gama e binomial negativa [^2].

#### 9.1.2 Ajuste de Modelos Aditivos

O ajuste dos modelos aditivos envolve estimar as funções $f_j$ de forma flexível. Um método comum utiliza um *scatterplot smoother*, como um *spline* de suavização cúbica [^2]. Dado um conjunto de observações $(x_i, Y_i)$, um critério como a soma penalizada dos quadrados pode ser especificado para este problema [^3]:

$$PRSS(\alpha, f_1, f_2, ..., f_p) = \sum_{i=1}^{N} \Big(Y_i - \alpha - \sum_{j=1}^{p} f_j(x_{ij})\Big)^2 + \sum_{j=1}^{p} \lambda_j \int [f''_j(t_j)]^2 dt_j,$$
onde $\lambda_j > 0$ são parâmetros de ajuste [^3]. Pode-se demonstrar que o minimizador desta expressão resulta num modelo aditivo com *splines* cúbicos, onde cada função $f_j$ é um *spline* cúbico [^3].

**Algoritmo 9.1: Algoritmo de *Backfitting* para Modelos Aditivos** [^4]

1.  **Inicialização**: $\alpha = \frac{1}{N} \sum_{i=1}^{N} y_i$, $f_j = 0$ para todo $i$ e $j$ [^4].
2.  **Ciclo**: Para $j = 1, 2, ..., p, ..., 1, 2, ..., p, ...$ [^4]:

    $$f_j \leftarrow S_j \Big[ \{y_i - \alpha - \sum_{k \neq j} f_k(x_{ik}) \}_{i=1}^{N} \Big],$$

    $$f_j \leftarrow f_j - \frac{1}{N} \sum_{i=1}^{N} f_j(x_{ij}),$$
    onde $S_j$ é o operador de suavização [^4].
3.  Repetir o ciclo até que as funções $f_j$ mudem menos que um limiar pré-especificado [^4].

O algoritmo de *backfitting* ajusta iterativamente cada função $f_j$, mantendo as outras fixas [^4]. A convenção padrão é assumir que as funções têm média zero sobre os dados, o que garante a identificabilidade do modelo [^4].

O mesmo algoritmo pode acomodar outros métodos de ajuste, especificando operadores de suavização $S_j$ apropriados, tais como [^4]:

*   Suavizadores de regressão univariados, como regressão polinomial local e métodos de *kernel*.
*   Operadores de regressão linear, resultando em ajustes polinomiais, ajustes constantes por partes, ajustes de *splines* paramétricos, séries e ajustes de Fourier.
*   Operadores mais complexos, como suavizadores de superfície para interações de segunda ordem ou superior, ou suavizadores periódicos para efeitos sazonais.

**Lemma 9.1:** *Para uma classe ampla de suavizadores lineares $S_j$, o backfitting é equivalente a um algoritmo de Gauss-Seidel para resolver um sistema linear de equações.* [^5]

#### 9.1.3 Regressão Logística Aditiva
Na regressão logística aditiva, o objetivo é modelar a probabilidade de um evento binário [^5]. O modelo tem a forma [^5]:

$$log\frac{Pr(Y = 1|X)}{Pr(Y = 0|X)} = \alpha + f_1(X_1) + ... + f_p(X_p).$$

As funções $f_1, f_2, ..., f_p$ são estimadas por um algoritmo de *backfitting* dentro de um procedimento de Newton-Raphson [^5].

**Algoritmo 9.2: Algoritmo de *Local Scoring* para a Regressão Logística Aditiva** [^6]

1.  **Compute** os valores iniciais: $\alpha = log[\bar{y}/(1-\bar{y})]$, onde $\bar{y}$ é a proporção amostral de uns, e $f_j = 0$ para todo $j$ [^6].
2.  **Defina** $\eta_i = \alpha + \sum_j f_j(x_{ij})$ e $p_i = 1/[1 + exp(-\eta_i)]$ [^6].
3.  **Iterar**:
    a.  Construir a variável alvo de trabalho:
    $$z_i = \eta_i + \frac{(y_i - p_i)}{p_i(1 - p_i)}$$
    b. Construir os pesos: $w_i = p_i(1 - p_i)$
    c. Ajustar um modelo aditivo aos alvos $z_i$ com pesos $w_i$, usando um algoritmo de *backfitting* ponderado. Isso dá novas estimativas $\alpha, f_j, j$ [^6].
4.  Continuar o passo 2 até que a mudança nas funções caia abaixo de um limiar pré-especificado [^6].

**Corolário 9.1:** *O ajuste do modelo aditivo no passo (2) do Algoritmo 9.2 requer um suavizador de scatterplot ponderado.* [^6]

#### 9.1.4 Exemplo: Predição de *Spam* por *Email*

Um exemplo prático é a aplicação de um modelo aditivo generalizado para dados de *spam* por *email* [^6]. O objetivo é classificar mensagens de *email* como *spam* ou não *spam* usando informações sobre o conteúdo do *email* [^6]. A variável resposta é binária, e existem 57 preditores, incluindo a porcentagem de palavras que correspondem a uma determinada palavra, a porcentagem de caracteres que correspondem a um determinado caractere, o comprimento médio de sequências ininterruptas de letras maiúsculas, etc [^6].

Os resultados mostram que o modelo aditivo logístico alcança uma taxa de erro de teste de 5.3%, em comparação com 7.6% para uma regressão logística linear [^7]. A decomposição da contribuição de cada variável em componentes linear e não linear revela informações importantes sobre a relação entre os preditores e a probabilidade de *spam* [^7].

### Conclusão

Os modelos aditivos generalizados oferecem uma extensão flexível dos modelos lineares, permitindo a modelagem de relações não lineares enquanto mantêm a interpretabilidade [^8]. O procedimento de *backfitting* para ajustar esses modelos é simples e modular, permitindo a escolha de um método de ajuste apropriado para cada variável preditora [^8]. No entanto, os modelos aditivos podem ter limitações para aplicações de *data mining* em larga escala, onde o número de preditores é muito grande [^8]. Nesses casos, técnicas como o *boosting* podem ser mais eficazes [^8].

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^2]: Page 296, *The Elements of Statistical Learning*.
[^3]: Page 297, *The Elements of Statistical Learning*.
[^4]: Page 298, *The Elements of Statistical Learning*.
[^5]: Page 299, *The Elements of Statistical Learning*.
[^6]: Page 300, *The Elements of Statistical Learning*.
[^7]: Page 301, *The Elements of Statistical Learning*.
[^8]: Page 304, *The Elements of Statistical Learning*.
<!-- END -->