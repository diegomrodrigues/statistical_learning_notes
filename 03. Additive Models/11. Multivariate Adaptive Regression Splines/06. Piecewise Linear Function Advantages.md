## TÃ­tulo: Modelos Aditivos, Ãrvores e MÃ©todos Relacionados: Vantagens Computacionais da Linearidade por Partes e OtimizaÃ§Ã£o Eficiente em MARS

```mermaid
graph LR
    subgraph "MARS Algorithm Architecture"
        direction TB
        A["Input Data: 'N' Observations"]
        B["Piecewise Linear Basis Functions: '(x-t)_+' and '(t-x)_+'"]
        C["Local Evaluation of Basis Functions"]
        D["Iterative Node Optimization (O(N))"]
        E["Adaptive Model Building"]
        F["Output: MARS Model"]
        A --> B
        B --> C
        C --> D
        D --> E
        E --> F
    end
```

### IntroduÃ§Ã£o

Este capÃ­tulo explora as vantagens computacionais da utilizaÃ§Ã£o de funÃ§Ãµes lineares por partes, ou *piecewise linear functions*, como funÃ§Ãµes de base no algoritmo Multivariate Adaptive Regression Splines (MARS), e como a natureza local dessas funÃ§Ãµes e suas zonas nulas permite que o ajuste a cada nÃ³ seja feito em $O(N)$ operaÃ§Ãµes, onde $N$ Ã© o nÃºmero de observaÃ§Ãµes [^9.1]. A utilizaÃ§Ã£o de funÃ§Ãµes de base com essas caracterÃ­sticas Ã© fundamental para a eficiÃªncia do algoritmo MARS e permite o ajuste de modelos mais complexos em tempo computacional razoÃ¡vel. O objetivo principal Ã© apresentar uma anÃ¡lise detalhada sobre como a estrutura das funÃ§Ãµes de base influencia a complexidade computacional do modelo e como o algoritmo MARS aproveita essa propriedade para otimizar seus parÃ¢metros de forma eficiente. O foco Ã© na base teÃ³rica e nas vantagens prÃ¡ticas de funÃ§Ãµes de base lineares por partes e sua relaÃ§Ã£o com a velocidade e a complexidade dos algoritmos.

### Conceitos Fundamentais

**Conceito 1: FunÃ§Ãµes Lineares por Partes (*Piecewise Linear Functions*)**

FunÃ§Ãµes lineares por partes sÃ£o funÃ§Ãµes definidas por segmentos lineares em diferentes intervalos do domÃ­nio, com cada segmento linear conectado em pontos chamados nÃ³s, ou *knots*. As funÃ§Ãµes de base utilizadas em MARS, $(x-t)_+$ e $(t-x)_+$, sÃ£o funÃ§Ãµes lineares por partes, onde $t$ Ã© um nÃ³, e o Ã­ndice + indica a parte positiva da funÃ§Ã£o, ou seja, zero se o valor dentro dos parenteses for negativo e o valor dentro dos parenteses se o valor for positivo. Essas funÃ§Ãµes sÃ£o definidas por:
$$
(x-t)_+ = \begin{cases}
x-t, & \text{se } x > t\\
0, & \text{se } x \leq t
\end{cases}
$$

e
$$
(t-x)_+ = \begin{cases}
t-x, & \text{se } x < t\\
0, & \text{se } x \geq t
\end{cases}
$$
FunÃ§Ãµes lineares por partes tÃªm a propriedade de serem locais, ou seja, serem nÃ£o nulas apenas em uma regiÃ£o especÃ­fica do domÃ­nio, o que contribui para a sua eficiÃªncia computacional. A combinaÃ§Ã£o dessas funÃ§Ãµes por multiplicaÃ§Ã£o gera novas funÃ§Ãµes com regiÃµes de atividade definidas pela interseÃ§Ã£o das regiÃµes de atividade das funÃ§Ãµes originais. A escolha da funÃ§Ã£o linear por partes e a sua combinaÃ§Ã£o permite representar funÃ§Ãµes com diferentes graus de nÃ£o linearidade e interaÃ§Ãµes.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos considerar um exemplo com um nÃ³ $t=2$. Para a funÃ§Ã£o $(x-2)_+$, se $x=1$, entÃ£o $(1-2)_+ = 0$. Se $x=3$, entÃ£o $(3-2)_+ = 1$. Para a funÃ§Ã£o $(2-x)_+$, se $x=1$, entÃ£o $(2-1)_+ = 1$. Se $x=3$, entÃ£o $(2-3)_+ = 0$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Definindo a funÃ§Ã£o (x-t)_+
> def pos_part_xt(x, t):
>  return np.maximum(x - t, 0)
>
> # Definindo a funÃ§Ã£o (t-x)_+
> def pos_part_tx(x, t):
>  return np.maximum(t - x, 0)
>
> # Criando um range de valores para x
> x = np.linspace(-1, 5, 400)
> t = 2 # Definindo o nÃ³
>
> # Calculando os valores das funÃ§Ãµes
> y1 = pos_part_xt(x, t)
> y2 = pos_part_tx(x, t)
>
> # Plotando as funÃ§Ãµes
> plt.figure(figsize=(8, 6))
> plt.plot(x, y1, label='(x-2)_+')
> plt.plot(x, y2, label='(2-x)_+')
> plt.xlabel('x')
> plt.ylabel('y')
> plt.title('FunÃ§Ãµes Lineares por Partes')
> plt.axvline(x=2, color='r', linestyle='--', label='NÃ³ (t=2)')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Este cÃ³digo Python gera um grÃ¡fico mostrando as funÃ§Ãµes $(x-2)_+$ e $(2-x)_+$. A linha vertical vermelha indica a posiÃ§Ã£o do nÃ³ em $x=2$. Observe como cada funÃ§Ã£o Ã© zero em um lado do nÃ³ e linear no outro. Isso ilustra a localidade das funÃ§Ãµes.

**Lemma 1:** *As funÃ§Ãµes lineares por partes, como as funÃ§Ãµes $(x-t)_+$ e $(t-x)_+$, sÃ£o iguais a zero em certas regiÃµes e sÃ£o definidas por segmentos lineares em outros intervalos, o que permite a construÃ§Ã£o de modelos que se ajustam localmente. A combinaÃ§Ã£o dessas funÃ§Ãµes por multiplicaÃ§Ã£o gera modelos com flexibilidade para representar interaÃ§Ãµes e nÃ£o linearidades*. A natureza local e a forma da funÃ§Ã£o linear por partes Ã© fundamental para a eficiÃªncia e interpretabilidade dos modelos MARS [^9.4].

**Conceito 2: Vantagens Computacionais de FunÃ§Ãµes com Zonas Nulas**
```mermaid
graph LR
    subgraph "Computational Advantages of Null Zones"
        direction TB
        A["Basis Function: 'f(x)'"]
        B["Null Zone: 'f(x) = 0' for some 'x'"]
        C["Local Computation: Only 'x' where 'f(x) != 0' are considered"]
        D["Reduced Computational Complexity"]
        A --> B
        B --> C
        C --> D
    end
```

A presenÃ§a de zonas nulas nas funÃ§Ãµes de base, como as funÃ§Ãµes *spline* lineares por partes utilizadas no MARS, permite que os cÃ¡lculos sejam feitos localmente, ou seja, apenas um subconjunto dos dados precisam ser considerados durante o cÃ¡lculo de uma funÃ§Ã£o. Se uma observaÃ§Ã£o $x_i$ estÃ¡ em uma regiÃ£o onde a funÃ§Ã£o de base $f(x_i)$ Ã© zero, entÃ£o o seu resultado serÃ¡ zero. Essa propriedade de localidade reduz a complexidade computacional dos cÃ¡lculos, e permite que a estimativa dos parÃ¢metros seja feita de forma eficiente. O processo de otimizaÃ§Ã£o pode ser feito utilizando algoritmos que exploram a localidade das funÃ§Ãµes de base para aumentar a sua eficiÃªncia. A escolha das funÃ§Ãµes de base, portanto, influencia a sua complexidade e a eficiÃªncia do seu cÃ¡lculo.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos 1000 observaÃ§Ãµes, $x_1, x_2, \ldots, x_{1000}$, e queremos avaliar a funÃ§Ã£o $(x-5)_+$. Se 600 dessas observaÃ§Ãµes tiverem valores menores ou iguais a 5 (ou seja, $x_i \leq 5$), entÃ£o a funÃ§Ã£o $(x_i - 5)_+$ serÃ¡ zero para essas 600 observaÃ§Ãµes. Portanto, o cÃ¡lculo da funÃ§Ã£o Ã© feito apenas para as 400 observaÃ§Ãµes restantes, onde $x_i > 5$. Isso demonstra como as zonas nulas reduzem o nÃºmero de cÃ¡lculos necessÃ¡rios, aumentando a eficiÃªncia computacional.
>
>  ```python
> import numpy as np
>
> # Exemplo de 1000 observaÃ§Ãµes aleatÃ³rias
> np.random.seed(42)
> x = np.random.rand(1000) * 10 # Valores entre 0 e 10
>
> # Definindo o nÃ³
> t = 5
>
> # Calculando (x-t)_+
> y = np.maximum(x - t, 0)
>
> # Identificando as observaÃ§Ãµes onde a funÃ§Ã£o Ã© zero
> zero_indices = np.where(x <= t)[0]
> non_zero_indices = np.where(x > t)[0]
>
> print(f"NÃºmero de observaÃ§Ãµes com (x-t)_+ igual a zero: {len(zero_indices)}")
> print(f"NÃºmero de observaÃ§Ãµes com (x-t)_+ diferente de zero: {len(non_zero_indices)}")
> ```
>
> Este cÃ³digo Python simula um conjunto de dados e calcula a funÃ§Ã£o $(x-5)_+$. Imprime o nÃºmero de observaÃ§Ãµes para as quais a funÃ§Ã£o Ã© zero e para as quais Ã© diferente de zero, demonstrando o efeito das zonas nulas.

**CorolÃ¡rio 1:** *A presenÃ§a de zonas nulas nas funÃ§Ãµes de base utilizadas em MARS permite que os cÃ¡lculos sejam feitos localmente, e que o custo computacional seja reduzido. A localidade das funÃ§Ãµes de base Ã© uma propriedade fundamental para a construÃ§Ã£o de modelos eficientes*. A combinaÃ§Ã£o de funÃ§Ãµes de base com zonas nulas Ã© uma forma de obter modelos computacionalmente eficientes [^9.4.1].

**Conceito 3: OtimizaÃ§Ã£o Eficiente em MARS: Ajuste de NÃ³s em O(N)**
```mermaid
graph LR
    subgraph "Efficient Optimization in MARS"
        direction TB
        A["Initial Model"]
        B["Evaluate Possible Knot 't'"]
        C["Local Error Reduction Calculation: Only 'x' where '(x-t)_+' or '(t-x)_+' != 0'"]
        D["Select Optimal Knot 't'"]
        E["Update Model with New Basis Function"]
        F["Complexity: O(N) per knot"]
        A --> B
        B --> C
        C --> D
        D --> E
        E --> F
    end
```

Em MARS, a escolha do nÃ³ de cada *spline* Ã© feita atravÃ©s da avaliaÃ§Ã£o de todos os possÃ­veis valores de nÃ³s, e a propriedade de localidade das funÃ§Ãµes de base permite que essa escolha seja feita com apenas $O(N)$ operaÃ§Ãµes. Ao adicionar um nÃ³, as funÃ§Ãµes de base sÃ³ sÃ£o alteradas em uma regiÃ£o especÃ­fica, e os valores para os outros dados nÃ£o sÃ£o afetados. A escolha do melhor nÃ³ Ã© feita atravÃ©s da avaliaÃ§Ã£o do impacto da adiÃ§Ã£o do nÃ³ no erro do modelo, e essa avaliaÃ§Ã£o pode ser feita localmente, o que torna o processo eficiente. A combinaÃ§Ã£o da estrutura aditiva com a propriedade de localidade das funÃ§Ãµes de base permite que o modelo MARS seja construÃ­do de forma eficiente e com menor custo computacional. A utilizaÃ§Ã£o das propriedades das funÃ§Ãµes lineares por partes Ã© fundamental para a eficiÃªncia do modelo.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Imagine que estamos ajustando um modelo MARS e temos 100 observaÃ§Ãµes. Para encontrar o nÃ³ Ã³timo para uma nova funÃ§Ã£o de base, precisamos avaliar o impacto de cada possÃ­vel nÃ³ no erro do modelo. GraÃ§as Ã  localidade, para cada possÃ­vel nÃ³ $t$, sÃ³ precisamos recalcular o erro para as observaÃ§Ãµes $x_i$ onde $(x_i - t)_+$ ou $(t - x_i)_+$ sÃ£o diferentes de zero. Se escolhermos um nÃ³ $t$ tal que apenas 20 observaÃ§Ãµes sÃ£o afetadas, o cÃ¡lculo da reduÃ§Ã£o do erro se concentra apenas nessas 20 observaÃ§Ãµes. Portanto, a complexidade da avaliaÃ§Ã£o de um nÃ³ Ã© proporcional ao nÃºmero de observaÃ§Ãµes afetadas pelo nÃ³, nÃ£o ao nÃºmero total de observaÃ§Ãµes, o que resulta em um custo de $O(N)$ para avaliar todos os nÃ³s.
>
> ```python
> import numpy as np
>
> # Exemplo de 100 observaÃ§Ãµes aleatÃ³rias
> np.random.seed(42)
> x = np.random.rand(100) * 10 # Valores entre 0 e 10
> y = 2 * x + np.random.randn(100) # Resposta com ruÃ­do
>
> # FunÃ§Ã£o para calcular o erro quadrÃ¡tico
> def calculate_sse(y_true, y_pred):
>  return np.sum((y_true - y_pred)**2)
>
> # FunÃ§Ã£o para calcular a reduÃ§Ã£o do erro ao adicionar um nÃ³ t
> def calculate_error_reduction(x, y, t, current_prediction):
>  y_pred_new = current_prediction + 0.5 * np.maximum(x - t, 0) # Exemplo de atualizaÃ§Ã£o
>  sse_old = calculate_sse(y, current_prediction)
>  sse_new = calculate_sse(y, y_pred_new)
>  return sse_old - sse_new
>
> # Avaliando diferentes nÃ³s
> possible_knots = np.linspace(min(x), max(x), num=10) # 10 nÃ³s possÃ­veis
> initial_prediction = np.zeros_like(y) # Inicializa a prediÃ§Ã£o
>
> error_reductions = []
> for t in possible_knots:
>  reduction = calculate_error_reduction(x, y, t, initial_prediction)
>  error_reductions.append(reduction)
>
> best_knot_index = np.argmax(error_reductions)
> best_knot = possible_knots[best_knot_index]
> print(f"Melhor nÃ³ encontrado: {best_knot}")
> print(f"ReduÃ§Ã£o de erro: {error_reductions[best_knot_index]}")
> ```
>
> Este cÃ³digo Python simula um conjunto de dados e avalia a reduÃ§Ã£o do erro para diferentes nÃ³s, demonstrando como a escolha do melhor nÃ³ Ã© feita. A complexidade de calcular a reduÃ§Ã£o do erro para cada nÃ³ Ã© proporcional ao nÃºmero de observaÃ§Ãµes afetadas pelo nÃ³, nÃ£o ao nÃºmero total de observaÃ§Ãµes, o que ilustra a eficiÃªncia do processo.

> âš ï¸ **Nota Importante:** A utilizaÃ§Ã£o de funÃ§Ãµes de base com zonas nulas permite uma otimizaÃ§Ã£o local, onde a atualizaÃ§Ã£o dos parÃ¢metros de cada funÃ§Ã£o Ã© feita apenas nas regiÃµes onde ela Ã© ativa, o que reduz a complexidade computacional e a sua dependÃªncia do nÃºmero total de dados*. O processo de atualizaÃ§Ã£o Ã© feito localmente e de forma eficiente [^9.4.1].

> â— **Ponto de AtenÃ§Ã£o:** A eficiÃªncia do algoritmo MARS estÃ¡ baseada na escolha adequada das funÃ§Ãµes de base e na sua capacidade de explorar a localidade da informaÃ§Ã£o. Outras funÃ§Ãµes, com um domÃ­nio maior, podem aumentar a complexidade computacional da construÃ§Ã£o do modelo. A escolha das funÃ§Ãµes de base influencia o custo computacional [^9.4].

> âœ”ï¸ **Destaque:** A utilizaÃ§Ã£o de funÃ§Ãµes lineares por partes como base em modelos MARS permite que os parÃ¢metros sejam estimados de forma eficiente atravÃ©s de operaÃ§Ãµes locais, e a combinaÃ§Ã£o das funÃ§Ãµes permite modelar relaÃ§Ãµes complexas de maneira mais rÃ¡pida que modelos que nÃ£o exploram a localidade. A escolha da funÃ§Ã£o de base Ã© uma componente importante da construÃ§Ã£o de modelos estatÃ­sticos eficientes [^9.4.1].

### OtimizaÃ§Ã£o Local com FunÃ§Ãµes de Base Lineares por Partes: FormulaÃ§Ã£o MatemÃ¡tica e AnÃ¡lise da Complexidade Computacional

```mermaid
graph LR
    subgraph "Local Optimization with Piecewise Linear Basis Functions"
        direction TB
        A["Basis Functions: '(x-t)_+' or '(t-x)_+'"]
        B["Local Error Reduction: 'Î”SSE'"]
        C["Formula for Î”SSE: 'sum_{x_i in R} (y_i - f_old(x_i))^2 - sum_{x_i in R} (y_i - (f_old(x_i) + Î²*f_new(x_i)))^2'"]
        D["'R' - Region where new basis function is non-zero"]
        E["Optimal Knot Selection based on max(Î”SSE)"]
        F["Iterative Refinement"]
         A --> B
        B --> C
        C --> D
        D --> E
        E --> F
    end
```

O processo de otimizaÃ§Ã£o em Multivariate Adaptive Regression Splines (MARS), utilizando funÃ§Ãµes de base lineares por partes e a sua propriedade de localidade, Ã© dado a seguir:

1.  **DefiniÃ§Ã£o da FunÃ§Ã£o de Base:** O modelo MARS Ã© construÃ­do utilizando uma combinaÃ§Ã£o linear de funÃ§Ãµes de base, onde cada funÃ§Ã£o tem a forma:

$$
(x-t)_+ = \begin{cases}
x-t, & \text{se } x > t\\
0, & \text{se } x \leq t
\end{cases}
$$

e
$$
(t-x)_+ = \begin{cases}
t-x, & \text{se } x < t\\
0, & \text{se } x \geq t
\end{cases}
$$

    onde $t$ Ã© o nÃ³ da funÃ§Ã£o de base.
2.  **CÃ¡lculo da ReduÃ§Ã£o do Erro Local:** Ao adicionar uma funÃ§Ã£o de base, o algoritmo MARS calcula a reduÃ§Ã£o no erro quadrÃ¡tico apenas sobre as observaÃ§Ãµes para as quais a funÃ§Ã£o de base Ã© nÃ£o nula:
$$
\Delta SSE = \sum_{x_i \in R} (y_i - f_{old}(x_i))^2 - \sum_{x_i \in R} (y_i - (f_{old}(x_i) + \beta f_{new}(x_i)))^2
$$
onde $R$ Ã© a regiÃ£o onde a nova funÃ§Ã£o $f_{new}$ Ã© nÃ£o nula, e $f_{old}$ Ã© a funÃ§Ã£o do modelo antes da adiÃ§Ã£o de $f_{new}$. Os outros dados, para os quais $f_{new}$ Ã© zero, nÃ£o sÃ£o considerados no cÃ¡lculo, devido Ã  propriedade de localidade. O coeficiente $\beta$ Ã© estimado localmente atravÃ©s de mÃ­nimos quadrados utilizando apenas as observaÃ§Ãµes em $R$, o que torna o cÃ¡lculo mais eficiente.
3.  **Escolha do NÃ³ Ã“timo:** Ao escolher o melhor nÃ³ $t$ para a funÃ§Ã£o *spline*, o algoritmo MARS avalia o impacto de todos os nÃ³s e, novamente, graÃ§as Ã  localidade, o cÃ¡lculo pode ser feito em $O(N)$ operaÃ§Ãµes. A escolha do melhor nÃ³ Ã© baseada na reduÃ§Ã£o do erro quadrÃ¡tico, e a escolha do nÃ³ Ã© feita de forma iterativa, atÃ© que nÃ£o exista nenhuma reduÃ§Ã£o no erro.

4.  **Processo Iterativo:** O processo de adicionar funÃ§Ãµes de base e escolher os parÃ¢metros Ã© repetido de forma iterativa, utilizando um algoritmo *forward stagewise* e um passo *backward* para controlar a complexidade do modelo.

A localidade das funÃ§Ãµes de base lineares por partes permite que o processo de otimizaÃ§Ã£o seja feito de forma eficiente, pois apenas uma pequena parte dos dados Ã© utilizada em cada passo da iteraÃ§Ã£o. A escolha de parÃ¢metros e nÃ³s, portanto, Ã© computacionalmente eficiente devido Ã  propriedade das funÃ§Ãµes de base, e o nÃºmero de operaÃ§Ãµes por nÃ³ Ã© da ordem de $N$, onde $N$ Ã© o nÃºmero de observaÃ§Ãµes.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos supor que temos um modelo MARS com uma funÃ§Ã£o de base inicial $f_{old}(x) = 2x$ e queremos adicionar uma nova funÃ§Ã£o de base $(x-3)_+$. Temos 10 observaÃ§Ãµes com valores de x: $x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]$ e valores de y correspondentes: $y = [1.5, 3.8, 6.2, 7.9, 10.1, 12.3, 14.1, 16.2, 17.8, 19.9]$.
>
> 1. **FunÃ§Ã£o de base antiga:** $f_{old}(x) = 2x$. As prediÃ§Ãµes com a funÃ§Ã£o antiga serÃ£o: $[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]$.
> 2. **Nova funÃ§Ã£o de base:** $f_{new}(x) = (x-3)_+$. Os valores de $f_{new}(x)$ sÃ£o: $[0, 0, 0, 1, 2, 3, 4, 5, 6, 7]$.
> 3. **RegiÃ£o de atividade R:** A nova funÃ§Ã£o Ã© nÃ£o nula para $x > 3$, entÃ£o $R = [4, 5, 6, 7, 8, 9, 10]$.
> 4. **CÃ¡lculo do erro antes da adiÃ§Ã£o:**
>    $SSE_{old} = \sum_{i=1}^{10} (y_i - f_{old}(x_i))^2 = (1.5-2)^2 + (3.8-4)^2 + (6.2-6)^2 + (7.9-8)^2 + (10.1-10)^2 + (12.3-12)^2 + (14.1-14)^2 + (16.2-16)^2 + (17.8-18)^2 + (19.9-20)^2 = 0.25 + 0.04 + 0.04 + 0.01 + 0.01 + 0.09 + 0.01 + 0.04 + 0.04 + 0.01 = 0.54$
> 5. **EstimaÃ§Ã£o de $\beta$ localmente:** Para estimar $\beta$, usamos apenas as observaÃ§Ãµes em R.
>    Temos os valores de $y$ em R: $[7.9, 10.1, 12.3, 14.1, 16.2, 17.8, 19.9]$ e os valores correspondentes de $f_{new}(x)$: $[1, 2, 3, 4, 5, 6, 7]$.
>   Podemos usar uma regressÃ£o linear simples para encontrar $\beta$, mas aqui, para simplificar, vamos usar a seguinte fÃ³rmula: $\beta = \frac{\sum_{x_i \in R} (y_i - f_{old}(x_i))f_{new}(x_i)}{\sum_{x_i \in R} f_{new}(x_i)^2}$
>  $\beta = \frac{(-0.1)*1 + (0.1)*2 + (0.3)*3 + (0.1)*4 + (0.2)*5 + (-0.2)*6 + (-0.1)*7}{1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2 + 7^2} = \frac{-0.1 + 0.2 + 0.9 + 0.4 + 1.0 - 1.2 - 0.7}{1 + 4 + 9 + 16 + 25 + 36 + 49} = \frac{0.5}{140} \approx 0.00357$
> 6.  **CÃ¡lculo do erro apÃ³s a adiÃ§Ã£o:**
> $f_{new}(x) = 0.00357*(x-3)_+$. As novas prediÃ§Ãµes sÃ£o: $f_{old}(x) + \beta f_{new}(x) = 2x + 0.00357(x-3)_+$
>  As prediÃ§Ãµes para os valores de x sÃ£o: $[2, 4, 6, 8.00357, 10.00714, 12.01071, 14.01428, 16.01785, 18.02142, 20.02499]$
> $SSE_{new} = (1.5-2)^2 + (3.8-4)^2 + (6.2-6)^2 + (7.9-8.00357)^2 + (10.1-10.00714)^2 + (12.3-12.01071)^2 + (14.1-14.01428)^2 + (16.2-16.01785)^2 + (17.8-18.02142)^2 + (19.9-20.02499)^2 = 0.25 + 0.04 + 0.04 + 0.00001274 + 0.008687 + 0.08357 + 0.000204 + 0.03312 + 0.04814 + 0.0625 \approx 0.51$
> 7.  **ReduÃ§Ã£o do erro:** $\Delta SSE = SSE_{old} - SSE_{new} = 0.54 - 0.51 = 0.03$
>
> Este exemplo demonstra como o cÃ¡lculo do erro Ã© feito localmente, utilizando apenas as observaÃ§Ãµes onde a nova funÃ§Ã£o de base Ã© nÃ£o nula (R). O coeficiente $\beta$ Ã© estimado usando apenas essas observaÃ§Ãµes, o que torna o processo mais eficiente. O resultado Ã© uma reduÃ§Ã£o no erro total, mostrando que a adiÃ§Ã£o da nova funÃ§Ã£o de base melhorou o ajuste do modelo.

**Lemma 3:** *A natureza local das funÃ§Ãµes de base lineares por partes permite que a otimizaÃ§Ã£o do modelo MARS seja feita de forma eficiente, e a escolha do nÃ³ e do parÃ¢metro das funÃ§Ãµes de base sejam feitas em $O(N)$ operaÃ§Ãµes. A localidade das funÃ§Ãµes de base Ã© um componente fundamental da eficiÃªncia do algoritmo*. A localidade das funÃ§Ãµes de base torna a otimizaÃ§Ã£o computacionalmente eficiente [^9.4.1].

### InterpretaÃ§Ã£o e ConexÃ£o com MÃ­nimos Quadrados
```mermaid
graph LR
    subgraph "Relationship with Least Squares"
        direction TB
        A["MARS Model with Piecewise Linear Basis Functions"]
        B["Local Least Squares: Î² estimated for each basis function"]
        C["Optimization: Minimize Local 'SSE'"]
        D["Adaptive Node Selection"]
        E["Result: Efficient Model for Non-linearities and Interactions"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```

A otimizaÃ§Ã£o local do algoritmo MARS pode ser interpretada como uma adaptaÃ§Ã£o do mÃ©todo dos mÃ­nimos quadrados (OLS) Ã  estrutura de funÃ§Ãµes de base com zonas nulas, e a escolha do coeficiente $\beta$ para cada funÃ§Ã£o de base Ã© feita atravÃ©s do mÃ©todo dos mÃ­nimos quadrados localmente, considerando as observaÃ§Ãµes onde as funÃ§Ãµes sÃ£o diferentes de zero. O processo de otimizaÃ§Ã£o Ã© feito de forma adaptativa, e a escolha dos nÃ³s e das interaÃ§Ãµes Ã© feita de forma a reduzir o erro do modelo. A combinaÃ§Ã£o de mÃ­nimos quadrados locais e um processo de seleÃ§Ã£o gulosa resulta em um algoritmo eficiente para modelar dados com nÃ£o linearidades e interaÃ§Ãµes.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um exemplo com dados onde a resposta $y$ depende de $x$ de forma nÃ£o linear: $y = 0.5x^2 + \epsilon$, onde $\epsilon$ Ã© um ruÃ­do aleatÃ³rio. Vamos simular dados e aplicar o MARS.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> # SimulaÃ§Ã£o dos dados
> np.random.seed(42)
> x = np.sort(np.random.rand(100) * 10) # 100 valores de x entre 0 e 10
> y = 0.5 * x**2 + np.random.randn(100) * 5 # Resposta com ruÃ­do
>
> # FunÃ§Ã£o para calcular o erro quadrÃ¡tico
> def calculate_sse(y_true, y_pred):
>  return np.sum((y_true - y_pred)**2)
>
> # FunÃ§Ã£o para calcular a reduÃ§Ã£o do erro ao adicionar um nÃ³ t
> def calculate_error_reduction(x, y, t, current_prediction):
>    # RegiÃ£o onde a funÃ§Ã£o Ã© nÃ£o nula
>    active_region_indices = np.where(x > t)[0]
>    if len(active_region_indices) == 0:
>        return 0
>
>    # Extrai os valores de x e y na regiÃ£o ativa
>    x_active = x[active_region_indices]
>    y_active = y[active_region_indices]
>    current_prediction_active = current_prediction[active_region_indices]
>
>    # Calcula os valores da nova funÃ§Ã£o de base
>    new_basis_function = np.maximum(x_active - t, 0)
>
>    # Estima beta usando mÃ­nimos quadrados
>    X = new_basis_function.reshape(-1, 1)
>    model = LinearRegression()
>    model.fit(X, y_active - current_prediction_active)
>    beta = model.coef_[0]
>
>    y_pred_new = current_prediction.copy()
>    y_pred_new[active_region_indices] = current_prediction_active + beta * new_basis_function
>
>    sse_old = calculate_sse(y, current_prediction)
>    sse_new = calculate_sse(y, y_pred_new)
>    return sse_old - sse_new, y_pred_new
>
>
> # InÃ­cio do processo de MARS
> current_prediction = np.zeros_like(y)
>
> knots = []
> error_reductions = []
> for _ in range(3): # Adiciona 3 funÃ§Ãµes de base
>    best_reduction = 0
>    best_knot = None
>    best_new_prediction = None
>    possible_knots = np.linspace(min(x), max(x), num=20) # 20 nÃ³s possÃ­veis
>
>    for t in possible_knots:
>        reduction, y_pred_new = calculate_error_reduction(x, y, t, current_prediction)
>        if reduction > best_reduction:
>            best_reduction = reduction
>            best_knot = t
>            best_new_prediction = y_pred_new
>    if best_knot is not None:
>      knots.append(best_knot)
>      error_reductions.append(best_reduction)
>      current_prediction = best_new_prediction
>
> # Plotando os resultados
> plt.figure(figsize=(10, 6))
> plt.scatter(x, y, label='Dados Originais', color='blue')
> plt.plot(x, current_prediction, label='PrediÃ§Ã£o MARS', color='red')
> for knot in knots:
>   plt.axvline(x=knot, color='gray', linestyle='--', label=f'NÃ³: {knot:.2f}')
> plt.xlabel('x')
> plt.ylabel('y')
> plt.title('Ajuste MARS com nÃ³s')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print(f"NÃ³s selecionados: {knots}")
> ```
>
> Este cÃ³digo Python simula dados nÃ£o lineares e ajusta um modelo MARS iterativamente. A cada iteraÃ§Ã£o, um novo nÃ³ Ã© adicionado usando um mÃ©todo de mÃ­nimos quadrados local, e a reduÃ§Ã£o do erro Ã© calculada. O grÃ¡fico mostra os dados originais, a prediÃ§Ã£o do modelo MARS e as localizaÃ§Ãµes dos nÃ³s adicionados. Isso demonstra como o MARS se adapta localmente aos dados.

### O Papel da RegularizaÃ§Ã£o no Controle da Complexidade dos Modelos
```mermaid
graph LR
    subgraph "Role of Regularization in Controlling Model Complexity"
        direction TB
        A["Model Cost Function: 'SSE'"]
        B["Add Regularization Term: 'Î»||Î²||Â²'"]
        C["Regularized Cost Function: 'SSE + Î»||Î²||Â²'"]
        D["'Î»': Regularization Parameter"]
        E["Result: Reduced Overfitting"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```

A utilizaÃ§Ã£o de regularizaÃ§Ã£o no modelo MARS controla a complexidade do modelo e evita o *overfitting*. O parÃ¢metro de regularizaÃ§Ã£o na funÃ§Ã£o de custo penaliza modelos com muitos parÃ¢metros e funÃ§Ãµes de base, e o ajuste do parÃ¢metro de regularizaÃ§Ã£o Ã© importante para obter modelos com boa capacidade de generalizaÃ§Ã£o. A regularizaÃ§Ã£o permite que o modelo tenha um bom desempenho em dados nÃ£o utilizados no treinamento e com parÃ¢metros que tenham bom significado estatÃ­stico, e sem *overfitting*.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos ilustrar o efeito da regularizaÃ§Ã£o com um exemplo simples. Suponha que temos um modelo MARS com duas funÃ§Ãµes de base: $f(x) = \beta_1 (x-t_1)_+ + \beta_2 (x-t_2)_+$. Sem regularizaÃ§Ã£o, o modelo pode se ajustar perfeitamente aos dados de treinamento, mas pode ter um desempenho ruim em dados novos. Para evitar isso, podemos adicionar um termo de regularizaÃ§Ã£o Ã  funÃ§Ã£o de custo:
>
> $Custo = SSE + \lambda (\beta_1^2 + \beta_2^2)$
>
> Onde SSE Ã© a soma dos erros quadrados e $\lambda$ Ã© o parÃ¢metro de regularizaÃ§Ã£o. Se $\lambda = 0$, nÃ£o hÃ¡ regularizaÃ§Ã£o. Se $\lambda$ Ã© grande, penalizamos modelos com coeficientes grandes.
>
> Vamos supor que, apÃ³s o ajuste do modelo, temos os seguintes resultados:
>
> *   Sem regularizaÃ§Ã£o ($\lambda = 0$): $\beta_1 = 10$, $\beta_2 = -8$. $SSE = 5$.
> *   Com regularizaÃ§Ã£o ($\lambda = 0.5$): $\beta_1 = 5$, $\beta_2 = -4$. $SSE = 8$.
>
> Sem regularizaÃ§Ã£o, o modelo tem um SSE menor, mas coeficientes maiores, o que pode levar ao *overfitting*. Com regularizaÃ§Ã£o, o SSE Ã© maior, mas os coeficientes sÃ£o menores, o que torna o modelo mais simples e com melhor capacidade de generalizaÃ§Ã£o.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import Ridge
> from sklearn.preprocessing import PolynomialFeatures
> from sklearn.pipeline import make_pipeline
>
> # SimulaÃ§Ã£o dos dados
> np.random.seed(42)
> x = np.sort(np.random.rand(100) * 10)
> y = 0.5 * x**2 + np.random.randn(100) * 5
>
> # Definindo os parÃ¢metros de regularizaÃ§Ã£o
> lambdas = [0, 0.1, 1, 10]
>
> # Plotando os resultados
> plt.figure(figsize=(12, 8))
> plt.scatter(x, y, label='Dados Originais', color='blue')
>
> for lambd in lambdas:
>    model = make_pipeline(PolynomialFeatures(degree=10), Ridge(alpha=lambd))
>    model.fit(x.reshape(-1, 1), y)
>    y_pred = model.predict(x.reshape(-1, 1))
>    plt.plot(x, y_pred, label=f'Ridge (Î»={lambd})')
>