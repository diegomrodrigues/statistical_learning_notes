## Hierarchical Mixtures of Experts: Soft Probabilistic Splits

### Introdução
Este capítulo aprofunda o conceito de **Hierarchical Mixtures of Experts (HME)**, uma metodologia que se distingue dos métodos de árvore tradicionais por empregar *divisões probabilísticas suaves* em vez de decisões binárias rígidas [^329]. Em um modelo HME, as observações não são direcionadas deterministicamente para um único ramo da árvore, mas sim distribuídas entre os ramos com probabilidades que dependem dos valores de entrada [^329]. Essa abordagem requer uma otimização suave dos parâmetros, diferenciando-se da busca discreta por pontos de divisão nos métodos baseados em árvores [^329]. O modelo HME pode ser visto como uma extensão dos modelos de mistura, onde as probabilidades de mistura são determinadas por redes de gating e os componentes da mistura são "especialistas" que modelam a relação entre entradas e saídas [^329].

### Conceitos Fundamentais

A principal característica do HME é a utilização de **soft splits** em vez de hard splits. Em métodos tradicionais de árvores, como CART, cada nó de decisão divide o espaço de entrada em regiões disjuntas, com cada observação pertencendo a exatamente uma região [^305]. Em contraste, o HME permite que cada observação contribua para múltiplos nós folha (especialistas), com pesos proporcionais às probabilidades de pertencimento [^329].

Formalmente, em cada nó não terminal, a probabilidade de uma observação $x$ seguir o ramo $j$ é dada por uma função, geralmente uma função softmax, que depende dos valores de entrada [^330]:

$$
g_j(x, \gamma_j) = \frac{e^{\gamma_j^T x}}{\sum_{k=1}^K e^{\gamma_k^T x}}, \quad j = 1, 2, \dots, K
$$

onde:
- $g_j(x, \gamma_j)$ é a probabilidade de a observação $x$ ser atribuída ao ramo $j$.
- $x$ é o vetor de entrada.
- $\gamma_j$ é um vetor de parâmetros desconhecidos associados ao ramo $j$.
- $K$ é o número de ramos no nó.

Essa função, também conhecida como **gating network**, determina a probabilidade de uma observação seguir um determinado ramo da árvore [^329]. Os parâmetros $\gamma_j$ são estimados durante o processo de treinamento para otimizar o desempenho preditivo do modelo.

Nos nós terminais, cada **expert** modela a relação entre as entradas e a saída. A natureza desse modelo depende do tipo de problema [^331]:

*   **Regressão:** Um modelo de regressão linear Gaussiano é usado [^331]:
    $$
    Y = \beta_{je}^T x + \epsilon, \quad \epsilon \sim N(0, \sigma_{je}^2)
    $$
    onde $\beta_{je}$ são os coeficientes de regressão para o expert $e$ no ramo $j$, e $\sigma_{je}^2$ é a variância do erro.

*   **Classificação:** Um modelo de regressão logística linear é usado [^331]:
    $$
    Pr(Y = 1|x, \theta_{je}) = \frac{1}{1 + e^{-\theta_{je}^T x}}
    $$
    onde $\theta_{je}$ são os parâmetros do modelo logístico.

A probabilidade total de uma observação $y$ dado $x$ é uma média ponderada das previsões dos experts, com os pesos dados pelas probabilidades das redes de gating [^330]:

$$
Pr(y|x, \Psi) = \sum_{j=1}^K g_j(x, \gamma_j) \sum_{l=1}^K g_{l|j}(x, \gamma_{jl}) Pr(y|x, \theta_{jl})
$$

onde $\Psi$ representa o conjunto de todos os parâmetros do modelo.

A estimação dos parâmetros $\Psi$ é tipicamente realizada por meio do **algoritmo EM** (Expectation-Maximization) para maximizar a log-verossimilhança dos dados [^331]. O algoritmo EM itera entre dois passos:

1.  **Passo E (Expectation):** Calcula as probabilidades de pertencimento aos ramos da árvore, dadas as estimativas atuais dos parâmetros.
2.  **Passo M (Maximization):** Atualiza as estimativas dos parâmetros para maximizar a log-verossimilhança esperada, dadas as probabilidades de pertencimento calculadas no passo E.

### Conclusão

O HME oferece uma alternativa flexível aos métodos tradicionais baseados em árvores, permitindo *divisões probabilísticas suaves* que podem capturar relações complexas nos dados [^329]. Ao contrário do CART, que faz partições binárias *hard* no espaço de entrada, o HME emprega *soft splits* que permitem que as observações contribuam para múltiplos nós folha [^329]. Essa abordagem resulta em uma função de log-verossimilhança suave, tornando a otimização dos parâmetros mais tratável [^331]. Embora o HME compartilhe semelhanças com o CART com divisões de combinação linear, a otimização deste último pode ser mais difícil [^331]. No entanto, ao contrário do CART, não há métodos conhecidos para encontrar uma boa topologia de árvore para o modelo HME, e a ênfase na pesquisa de HMEs tem sido mais na predição do que na interpretação do modelo final [^332]. Modelos de classes latentes, que compartilham uma relação próxima com o HME, são frequentemente utilizados para interpretar os nós ou classes latentes como grupos de sujeitos exibindo comportamento de resposta semelhante [^332].

### Referências

[^305]: Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). *Classification and regression trees*. Monterey, CA: Wadsworth & Brooks/Cole.
[^329]: Jordan, M. I., & Jacobs, R. A. (1994). Hierarchical mixtures of experts and the EM algorithm. *Neural Computation, 6*(2), 181-214.
[^330]: Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). Adaptive mixtures of local experts. *Neural Computation, 3*(1), 79-87.
[^331]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: Data mining, inference, and prediction*. New York, NY: Springer.
[^332]: Lin, T. H., Turnbull, C. D., Ghosh, D., & McCulloch, R. E. (2000). Bayesian analysis of latent structure in microarray data. *Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68*(5), 701-717.
<!-- END -->