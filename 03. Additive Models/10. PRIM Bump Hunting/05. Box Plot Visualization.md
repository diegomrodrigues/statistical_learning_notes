## T√≠tulo: Modelos Aditivos, √Årvores e M√©todos Relacionados: Visualiza√ß√£o da Constru√ß√£o de *Boxes* e M√©dias em PRIM com Dados Simulados

```mermaid
graph LR
    subgraph "PRIM Algorithm Overview"
    direction TB
        A["Initial Box: All Data"] --> B["Peeling: Compress Box"]
        B --> C{"Calculate Mean"}
        C --> D["Pasting: Expand Box"]
        D --> E{"Calculate Mean"}
        E --> F{"Repeat"}
        F -- "No Improvement" --> G["Final Box"]
        F -- "Improvement" --> B
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora a constru√ß√£o de *boxes* pelo algoritmo PRIM (Patient Rule Induction Method) atrav√©s da utiliza√ß√£o de dados simulados, e como a m√©dia dos dados dentro dos *boxes* √© modificada a cada itera√ß√£o do algoritmo [^9.1]. O PRIM √© um m√©todo que busca regi√µes no espa√ßo de caracter√≠sticas onde a m√©dia da resposta √© alta, e a constru√ß√£o dessas regi√µes (boxes) √© realizada atrav√©s de um processo iterativo de *peeling* e *pasting*. O objetivo principal √© fornecer uma vis√£o pr√°tica sobre o funcionamento do algoritmo PRIM, como a escolha dos par√¢metros de *peeling* e *pasting* influenciam o resultado, e como a visualiza√ß√£o do processo pode ajudar a compreender as etapas da modelagem. O cap√≠tulo tamb√©m aborda como o uso de dados simulados auxilia na compreens√£o dos mecanismos de otimiza√ß√£o do algoritmo PRIM e a sua capacidade de identificar regi√µes com alta m√©dia na vari√°vel resposta.

### Conceitos Fundamentais

**Conceito 1: O Algoritmo PRIM (Patient Rule Induction Method)**

O algoritmo PRIM (Patient Rule Induction Method) √© um m√©todo para a busca de regi√µes (boxes) no espa√ßo de caracter√≠sticas onde a m√©dia da vari√°vel resposta √© alta. O algoritmo come√ßa com um *box* que cont√©m todos os dados e em seguida realiza um processo iterativo que envolve a compress√£o de uma borda do *box* e remo√ß√£o dos pontos que ficam fora do *box*. O *box* resultante √© aquele que mant√©m a maior m√©dia da vari√°vel resposta. Em seguida, o processo de *pasting* busca expandir o *box* para incluir observa√ß√µes que est√£o fora do *box* corrente. O algoritmo PRIM, por isso, busca regi√µes no espa√ßo de caracter√≠sticas com altas m√©dias da resposta, de forma iterativa.

**Lemma 1:** *O algoritmo PRIM busca regi√µes no espa√ßo de caracter√≠sticas onde a m√©dia da vari√°vel resposta √© alta, e o m√©todo utiliza um processo iterativo de compress√£o e expans√£o dos *boxes* para encontrar essas regi√µes*. O objetivo do PRIM √© encontrar regi√µes com alta m√©dia da resposta [^9.3].

> üí° **Exemplo Num√©rico:**
> Vamos considerar um conjunto de dados simulado com duas caracter√≠sticas ($X_1$ e $X_2$) e uma vari√°vel resposta ($Y$). Inicialmente, o *box* cont√©m todos os pontos.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Dados simulados
> np.random.seed(42)
> X1 = np.random.rand(100) * 10
> X2 = np.random.rand(100) * 10
> Y = 2 * (X1 > 5) + 3 * (X2 > 5) + np.random.randn(100)
>
> # Box inicial
> box_x1_min = min(X1)
> box_x1_max = max(X1)
> box_x2_min = min(X2)
> box_x2_max = max(X2)
>
> # C√°lculo da m√©dia inicial
> mean_y_initial = np.mean(Y)
> print(f"M√©dia inicial de Y: {mean_y_initial:.2f}")
>
> plt.figure(figsize=(6, 6))
> plt.scatter(X1, X2, c=Y, cmap='viridis')
> plt.xlabel("X1")
> plt.ylabel("X2")
> plt.title("Dados Simulados e Box Inicial")
> plt.show()
> ```
>
> Neste exemplo, a m√©dia inicial de Y √© calculada, e o gr√°fico mostra os dados e o *box* inicial que abrange todos os pontos.

**Conceito 2: Processo de *Peeling* e Compress√£o do *Box***

O processo de *peeling* (remo√ß√£o) envolve a compress√£o de uma borda do *box* por uma pequena fra√ß√£o dos dados, onde os pontos que s√£o removidos s√£o os que est√£o fora do *box* comprimido. O processo de compress√£o √© feito na dire√ß√£o do preditor que resulta na maior m√©dia da resposta nos dados restantes. O processo √© iterativo, onde em cada passo a borda do *box* √© comprimida, e a m√©dia dos dados dentro do *box* √© recalculada. A compress√£o do *box* resulta na remo√ß√£o de pontos que n√£o contribuem para o aumento da m√©dia da resposta, de acordo com o crit√©rio estabelecido pelo algoritmo. A escolha do passo da compress√£o, e de qual borda √© comprimida, s√£o passos importantes no processo de modelagem com o PRIM.

**Corol√°rio 1:** *O processo de *peeling* remove as observa√ß√µes com baixa m√©dia na resposta, e comprime o *box* para uma regi√£o mais homog√™nea com maior m√©dia. A escolha da borda e o passo da compress√£o definem o comportamento do algoritmo*. O processo de *peeling* √© guiado pela maximiza√ß√£o da m√©dia da resposta [^9.3.1].

```mermaid
graph TB
    subgraph "Peeling Process"
      direction TB
        A["Initial Box"] --> B["Compress Border"]
        B --> C["Remove Points Outside Box"]
        C --> D{"Calculate New Mean"}
        D --> E["Resulting Box"]
    end
```

> üí° **Exemplo Num√©rico:**
> Continuando o exemplo anterior, vamos realizar uma etapa de *peeling* comprimindo a borda de $X_1$ em 10%.
>
> ```python
> # Par√¢metro de peeling
> peeling_fraction = 0.1
>
> # Peeling na borda X1
> box_x1_min_peeled = box_x1_min + (box_x1_max - box_x1_min) * peeling_fraction
>
> # Pontos dentro do novo box
> indices_inside_box = (X1 >= box_x1_min_peeled) & (X1 <= box_x1_max) & (X2 >= box_x2_min) & (X2 <= box_x2_max)
>
> # C√°lculo da nova m√©dia
> mean_y_peeled = np.mean(Y[indices_inside_box])
>
> print(f"M√©dia de Y ap√≥s peeling em X1: {mean_y_peeled:.2f}")
>
> plt.figure(figsize=(6, 6))
> plt.scatter(X1[indices_inside_box], X2[indices_inside_box], c=Y[indices_inside_box], cmap='viridis', label='Dentro do Box')
> plt.scatter(X1[~indices_inside_box], X2[~indices_inside_box], c='gray', marker='x', label='Fora do Box')
> plt.xlabel("X1")
> plt.ylabel("X2")
> plt.title("Peeling em X1")
> plt.legend()
> plt.show()
>
> ```
>
> Neste exemplo, o *box* √© comprimido em $X_1$, e a m√©dia √© recalculada. Os pontos removidos s√£o marcados com 'x'. A m√©dia dentro do box aumenta ap√≥s o peeling.

**Conceito 3: Processo de *Pasting* e Expans√£o do *Box***

O processo de *pasting* (expans√£o) √© uma etapa que busca expandir o *box* e incluir observa√ß√µes que foram removidas em etapas anteriores, utilizando como guia, a manuten√ß√£o da alta m√©dia. Ap√≥s a etapa de compress√£o, o algoritmo avalia todas as bordas do *box*, e expande aquelas onde a m√©dia aumenta. O processo √© feito em cada borda do *box*, at√© que nenhuma expans√£o cause aumento da m√©dia, ou at√© que um crit√©rio de parada seja atingido. O processo de *pasting*, ao permitir a expans√£o do *box* de acordo com a m√©dia, compensa o processo de compress√£o e permite que o *box* se expanda para uma regi√£o com alta m√©dia.

> ‚ö†Ô∏è **Nota Importante:** Os processos de *peeling* e *pasting* s√£o iterativos e permitem que o algoritmo PRIM encontre regi√µes no espa√ßo de caracter√≠sticas com altas m√©dias da resposta atrav√©s de compress√µes e expans√µes do *box*. A combina√ß√£o dos dois processos √© o componente principal do algoritmo PRIM [^9.3.1].

> ‚ùó **Ponto de Aten√ß√£o:** A escolha dos par√¢metros de *peeling* e *pasting*, como o tamanho do passo da compress√£o e da expans√£o, influencia diretamente no tamanho dos *boxes*, na m√©dia e no n√∫mero de observa√ß√µes em cada box. O ajuste desses par√¢metros √© importante para a qualidade do resultado final do algoritmo PRIM [^9.3.1].

> ‚úîÔ∏è **Destaque:** A combina√ß√£o das abordagens de *peeling* e *pasting* √© uma forma eficaz para a constru√ß√£o de *boxes* com alta m√©dia e que modelam regi√µes espec√≠ficas do espa√ßo de caracter√≠sticas [^9.3].

```mermaid
graph TB
    subgraph "Pasting Process"
        direction TB
        A["Compressed Box"] --> B["Evaluate Box Edges"]
        B --> C{"Expand Border Where Mean Increases"}
         C --> D{"Calculate New Mean"}
        D --> E["Resulting Box"]
    end
```

> üí° **Exemplo Num√©rico:**
> Agora, vamos realizar um passo de *pasting* na borda de $X_1$.
>
> ```python
> # Par√¢metro de pasting
> pasting_fraction = 0.05
>
> # Expans√£o na borda X1
> box_x1_min_pasted = box_x1_min_peeled - (box_x1_max - box_x1_min) * pasting_fraction
>
> # Pontos dentro do novo box
> indices_inside_box_pasted = (X1 >= box_x1_min_pasted) & (X1 <= box_x1_max) & (X2 >= box_x2_min) & (X2 <= box_x2_max)
>
> # C√°lculo da nova m√©dia
> mean_y_pasted = np.mean(Y[indices_inside_box_pasted])
>
> print(f"M√©dia de Y ap√≥s pasting em X1: {mean_y_pasted:.2f}")
>
> plt.figure(figsize=(6, 6))
> plt.scatter(X1[indices_inside_box_pasted], X2[indices_inside_box_pasted], c=Y[indices_inside_box_pasted], cmap='viridis', label='Dentro do Box')
> plt.scatter(X1[~indices_inside_box_pasted], X2[~indices_inside_box_pasted], c='gray', marker='x', label='Fora do Box')
> plt.xlabel("X1")
> plt.ylabel("X2")
> plt.title("Pasting em X1")
> plt.legend()
> plt.show()
> ```
>
> Neste exemplo, o *box* √© expandido em $X_1$, e a m√©dia √© recalculada. A m√©dia pode aumentar ou diminuir dependendo dos dados inclu√≠dos. Este processo √© repetido para todas as bordas e iterativamente.

### Visualiza√ß√£o da Constru√ß√£o de Boxes com Dados Simulados: Peeling, Pasting e Varia√ß√£o da M√©dia

```mermaid
graph LR
    subgraph "PRIM Iterative Process"
        direction TB
         A["Simulated Data (X1, X2, Y)"] --> B["Initial Box: All Data"]
        B --> C["Peeling: Compress Box Border"]
        C --> D["Pasting: Expand Box Border"]
         D --> E{"Calculate Mean Y"}
        E --> F{"Repeat Until Convergence"}
         F --> G["Final Box with High Mean"]
         F --> C
    end
```

Para ilustrar o funcionamento do algoritmo PRIM, √© √∫til a utiliza√ß√£o de dados simulados em duas dimens√µes, onde a vari√°vel resposta $Y$ assume valores altos em uma regi√£o espec√≠fica do espa√ßo de caracter√≠sticas, e um valor mais baixo fora dessa regi√£o. O processo de visualiza√ß√£o envolve:

1.  **Dados Simulados:** Dados simulados em duas dimens√µes ($X_1$ e $X_2$), onde a resposta $Y$ assume um valor alto em uma regi√£o espec√≠fica do espa√ßo. A simula√ß√£o pode ser utilizada com dados com diferentes n√≠veis de ru√≠do e diferentes tipos de padr√µes.
2.  **Visualiza√ß√£o dos Dados:** Os dados simulados s√£o visualizados em um gr√°fico de dispers√£o, onde a cor de cada ponto corresponde ao valor da resposta $Y$, ou a outra m√©trica desejada.
3.  **Visualiza√ß√£o do *Box*:** O *box* inicial corresponde a todos os dados, e a cada itera√ß√£o, o processo de compress√£o do *box* √© visualizado no gr√°fico, de forma a observar como o *box* se adapta aos dados, e como os dados com baixa resposta v√£o sendo removidos.
4.  **Processo de *Peeling*:** A etapa de compress√£o do *box* √© feita atrav√©s da compress√£o de uma borda, onde a quantidade comprimida e a borda escolhida s√£o definidas por par√¢metros de escolha. Os pontos que s√£o removidos a cada etapa s√£o mostrados no gr√°fico, com um indicador visual da sua remo√ß√£o.
5.  **Processo de *Pasting*:** Ap√≥s a compress√£o, a etapa de expans√£o √© feita, e √© mostrado como o *box* se expande para incluir pontos que levam ao aumento da m√©dia, e como essa expans√£o permite explorar regi√µes com alta m√©dia. O *pasting*, por ser uma estrat√©gia gulosa, busca aumentar a m√©dia e tamb√©m expandir o *box*.
6.  **Visualiza√ß√£o da M√©dia:** A m√©dia da vari√°vel resposta dentro do *box* √© calculada em cada itera√ß√£o e mostrada no gr√°fico, demonstrando como a m√©dia evolui em fun√ß√£o das decis√µes de compress√£o e expans√£o. O processo √© iterativo, at√© que nenhum aumento na m√©dia ou outros crit√©rios de parada sejam atingidos.

A visualiza√ß√£o do processo iterativo do algoritmo PRIM, com compress√£o e expans√£o do *box*, e como a m√©dia da vari√°vel resposta varia durante o processo, oferece uma compreens√£o sobre como o PRIM funciona e como a escolha dos par√¢metros de *peeling* e *pasting* influencia no modelo final.

**Lemma 4:** *A visualiza√ß√£o do processo de constru√ß√£o dos *boxes* no algoritmo PRIM, utilizando dados simulados, permite que as diferentes etapas do algoritmo sejam entendidas de forma intuitiva, e como a combina√ß√£o dos processos de compress√£o e expans√£o do *box*, buscam encontrar regi√µes no espa√ßo de caracter√≠sticas onde a m√©dia da vari√°vel resposta √© alta*. O uso de visualiza√ß√µes facilita a compreens√£o do comportamento de algoritmos de aprendizado supervisionado [^9.3].

### A Varia√ß√£o da M√©dia Dentro do Box e o Impacto da Escolha dos Par√¢metros

A m√©dia da vari√°vel resposta dentro do *box* muda a cada itera√ß√£o, e a forma como essa m√©dia varia depende da escolha dos par√¢metros de *peeling* e *pasting*. Um par√¢metro de *peeling* muito alto pode remover muitos dados de uma vez, e um par√¢metro de *peeling* muito baixo pode gerar um processo de compress√£o muito lento. Um par√¢metro de *pasting* muito pequeno pode fazer com que o *box* n√£o se expanda para √°reas com alta m√©dia, e um par√¢metro de *pasting* muito grande pode levar a expans√µes desnecess√°rias. A escolha dos par√¢metros de compress√£o e expans√£o do *box* deve considerar a natureza dos dados, e a forma como a m√©dia da vari√°vel resposta varia ao longo do espa√ßo de caracter√≠sticas. A escolha adequada dos par√¢metros √© importante para o resultado final do algoritmo PRIM, e a valida√ß√£o cruzada pode ser utilizada para guiar a escolha dos melhores valores dos par√¢metros.

### Rela√ß√£o com Outros M√©todos: √Årvores de Decis√£o, GAMs e MARS

Embora o PRIM seja diferente em sua abordagem de constru√ß√£o dos *boxes* comparado a √°rvores de decis√£o e MARS, os tr√™s m√©todos buscam encontrar rela√ß√µes entre os preditores e a resposta, com a ideia de particionar o espa√ßo dos dados. √Årvores de decis√£o utilizam divis√µes bin√°rias para criar regi√µes no espa√ßo, GAMs utilizam fun√ß√µes n√£o param√©tricas para modelar rela√ß√µes n√£o lineares, e MARS utiliza fun√ß√µes *spline* lineares por partes. Enquanto √°rvores de decis√£o focam em decis√µes locais, GAMs utilizam uma modelagem global atrav√©s das fun√ß√µes n√£o param√©tricas, e MARS utiliza uma combina√ß√£o de ambos. Cada modelo tem suas vantagens e desvantagens, e a sua escolha depende da natureza dos dados, e dos objetivos da modelagem. O PRIM oferece uma abordagem alternativa para encontrar regi√µes espec√≠ficas do espa√ßo de caracter√≠sticas que podem auxiliar a constru√ß√£o e interpreta√ß√£o dos modelos.

```mermaid
graph LR
    subgraph "Model Comparison"
        direction LR
        A["PRIM"] --> B["Finds Boxes with High Mean"]
        C["Decision Trees"] --> D["Binary Partitions"]
        E["GAMs"] --> F["Non-parametric Functions"]
        G["MARS"] --> H["Piecewise Linear Splines"]
    end
```

### Perguntas Te√≥ricas Avan√ßadas: Como a natureza do suavizador utilizado no *peeling* e *pasting* afeta a converg√™ncia, a forma final do *box* e a estabilidade do algoritmo PRIM, e como essa escolha se relaciona com a fun√ß√£o de custo?

**Resposta:**

A natureza do suavizador utilizado nas etapas de *peeling* e *pasting* no algoritmo PRIM tem um impacto significativo na sua converg√™ncia, na forma final do *box* e na estabilidade do algoritmo, e como esta escolha se relaciona com a fun√ß√£o de custo, e o seu comportamento.

Durante o processo de *peeling*, o suavizador √© utilizado para decidir quais pontos devem ser removidos, e como a borda do *box* √© comprimida. Um suavizador mais flex√≠vel pode resultar em um *box* com contornos mais irregulares, que se adaptam aos dados de treino com mais precis√£o, mas com maior risco de *overfitting* e menor capacidade de generaliza√ß√£o. Um suavizador mais r√≠gido resulta em um *box* com contornos mais suaves, que t√™m maior estabilidade, e tendem a capturar os padr√µes de forma mais geral. A escolha do suavizador, portanto, afeta a forma como o *box* se ajusta aos dados e a capacidade do modelo de representar as informa√ß√µes.

During the *pasting* process, the role of the smoother is to define how the *box* edges are expanded, and the need to include a given point. A more flexible smoother can lead to a box with more irregular contours, which may include points with a lower mean. A more rigid smoother, on the other hand, leads to a *box* with smoother and more homogeneous contours. The choice of the smoother in the *pasting* step is crucial to control the shape of the final *box* and ensure that it is a good representative of the high mean region.

O uso de um suavizador adequado tamb√©m afeta a converg√™ncia do algoritmo. Suavizadores mais flex√≠veis podem fazer com que o algoritmo PRIM demore mais para convergir, pois a escolha dos par√¢metros de *peeling* e *pasting* se torna mais sens√≠vel. Suavizadores mais r√≠gidos podem levar a um algoritmo que converge mais rapidamente, mas com uma perda na qualidade do ajuste. A escolha do suavizador adequado deve considerar a natureza dos dados e o *trade-off* entre flexibilidade, converg√™ncia e capacidade de generaliza√ß√£o.

A escolha do suavizador, portanto, tem um impacto direto no comportamento do algoritmo PRIM. No entanto, a escolha do suavizador, por si s√≥, n√£o define a fun√ß√£o de custo, mas sim a forma como o algoritmo se comporta durante a otimiza√ß√£o e como ele encontra as regi√µes no espa√ßo de caracter√≠sticas. A fun√ß√£o de custo do PRIM √© definida implicitamente pelo seu objetivo de encontrar regi√µes com alta m√©dia. A rela√ß√£o com outros modelos, como GAMs, que utilizam uma fun√ß√£o de custo explicita para controlar a flexibilidade e o ajuste dos dados, √© importante para entender a diferen√ßa nas abordagens de cada um dos m√©todos.

```mermaid
graph LR
    subgraph "Smoother Influence"
        direction TB
        A["Smoother in Peeling"] --> B["Shape of Box"]
        A --> C["Convergence"]
        B --> D["Flexibility"]
        B --> E["Generalization"]
        C --> D
         C --> E
         F["Smoother in Pasting"] --> G["Box Shape"]
         G --> D
         G --> E

    end
```

**Lemma 5:** *A escolha do suavizador utilizado nas etapas de peeling e pasting influencia a forma do *box* resultante, a converg√™ncia do algoritmo e a sua capacidade de generaliza√ß√£o. Suavizadores mais flex√≠veis levam a *boxes* mais complexos, e suavizadores mais r√≠gidos levam a *boxes* mais suaves. A escolha adequada do suavizador √© importante para o bom desempenho do algoritmo PRIM* [^9.3.1].

**Corol√°rio 5:** *A combina√ß√£o das etapas de peeling e pasting, com a escolha do suavizador apropriado, influencia o comportamento do algoritmo PRIM e a sua capacidade de encontrar regi√µes no espa√ßo de caracter√≠sticas com alta m√©dia. A natureza do suavizador tem um grande impacto no processo de otimiza√ß√£o e na forma como o algoritmo se adapta a diferentes tipos de dados* [^9.3].

> ‚ö†Ô∏è **Ponto Crucial**: A escolha do suavizador √© um componente fundamental do algoritmo PRIM, e sua natureza tem um impacto direto na forma dos *boxes* resultantes, na sua converg√™ncia e na sua capacidade de aproxima√ß√£o da fun√ß√£o resposta. A escolha do suavizador e dos par√¢metros de *peeling* e *pasting* deve ser feita com cuidado para garantir que os modelos sejam robustos, com alta capacidade de generaliza√ß√£o e uma boa interpreta√ß√£o das regi√µes com altas m√©dias [^9.3].

### Conclus√£o

Este cap√≠tulo explorou a constru√ß√£o de *boxes* e a varia√ß√£o da m√©dia utilizando o algoritmo PRIM com dados simulados, mostrando como o processo de compress√£o e expans√£o, guiados pelo crit√©rio de alta m√©dia, s√£o utilizados para encontrar regi√µes espec√≠ficas no espa√ßo de caracter√≠sticas. A discuss√£o detalhou a import√¢ncia dos par√¢metros de *peeling* e *pasting* no comportamento do algoritmo e na sua rela√ß√£o com outros modelos estat√≠sticos. A compreens√£o das propriedades do algoritmo PRIM permite a sua utiliza√ß√£o de forma eficaz em problemas de busca de regi√µes espec√≠ficas no espa√ßo de caracter√≠sticas, e sua rela√ß√£o com o m√©todo da m√°xima verossimilhan√ßa.

### Footnotes

[^4.1]: "In this chapter we begin our discussion of some specific methods for super-vised learning. These techniques each assume a (different) structured form for the unknown regression function, and by doing so they finesse the curse of dimensionality. Of course, they pay the possible price of misspecifying the model, and so in each case there is a tradeoff that has to be made." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^4.2]: "Regression models play an important role in many data analyses, providing prediction and classification rules, and data analytic tools for understand-ing the importance of different inputs." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^4.3]: "In this section we describe a modular algorithm for fitting additive models and their generalizations. The building block is the scatterplot smoother for fitting nonlinear effects in a flexible way. For concreteness we use as our scatterplot smoother the cubic smoothing spline described in Chapter 5." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^4.3.1]: "The additive model has the form $Y = \alpha + \sum_{j=1}^p f_j(X_j) + \epsilon$, where the error term $\epsilon$ has mean zero." * (Trecho de "Additive Models, Trees, and Related Methods")*

[^4.3.2]: "Given observations $x_i, y_i$, a criterion like the penalized sum of squares (5.9) of Section 5.4 can be specified for this problem, $PRSS(\alpha, f_1, f_2,\ldots, f_p) = \sum_i^N (y_i - \alpha - \sum_j^p f_j(x_{ij}))^2 + \sum_j^p \lambda_j \int (f_j''(t_j))^2 dt_j$" * (Trecho de "Additive Models, Trees, and Related Methods")*

[^4.3.3]: "where the $\lambda_j > 0$ are tuning parameters. It can be shown that the minimizer of (9.7) is an additive cubic spline model; each of the functions $f_j$ is a cubic spline in the component $X_j$, with knots at each of the unique values of $x_{ij}$, $i = 1,\ldots, N$." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^4.4]: "For two-class classification, recall the logistic regression model for binary data discussed in Section 4.4. We relate the mean of the binary response $\mu(X) = Pr(Y = 1|X)$ to the predictors via a linear regression model and the logit link function: $log(\mu(X)/(1 ‚Äì \mu(X)) = \alpha + \beta_1 X_1 + \ldots + \beta_pX_p$." * (Trecho de "Additive Models, Trees, and Related Methods")*

[^4.4.1]: "The additive logistic regression model replaces each linear term by a more general functional form: $log(\mu(X)/(1 ‚Äì \mu(X))) = \alpha + f_1(X_1) + \cdots + f_p(X_p)$, where again each $f_j$ is an unspecified smooth function." * (Trecho de "Additive Models, Trees, and Related Methods")*

[^4.4.2]: "While the non-parametric form for the functions $f_j$ makes the model more flexible, the additivity is retained and allows us to interpret the model in much the same way as before. The additive logistic regression model is an example of a generalized additive model." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^4.4.3]: "In general, the conditional mean $\mu(X)$ of a response $Y$ is related to an additive function of the predictors via a link function $g$: $g[\mu(X)] = \alpha + f_1(X_1) + \cdots + f_p(X_p)$." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^4.4.4]: "Examples of classical link functions are the following: $g(\mu) = \mu$ is the identity link, used for linear and additive models for Gaussian response data." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^4.4.5]: "$g(\mu) = logit(\mu)$ as above, or $g(\mu) = probit(\mu)$, the probit link function, for modeling binomial probabilities. The probit function is the inverse Gaussian cumulative distribution function: $probit(\mu) = \Phi^{-1}(\mu)$." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^4.5]: "All three of these arise from exponential family sampling models, which in addition include the gamma and negative-binomial distributions. These families generate the well-known class of generalized linear models, which are all extended in the same way to generalized additive models." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^4.5.1]: "The functions $f_j$ are estimated in a flexible manner, using an algorithm whose basic building block is a scatterplot smoother. The estimated func-tion $f_j$ can then reveal possible nonlinearities in the effect of $X_j$. Not all of the functions $f_j$ need to be nonlinear." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^4.5.2]: "We can easily mix in linear and other parametric forms with the nonlinear terms, a necessity when some of the inputs are qualitative variables (factors)." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^9.1]: "In this chapter we begin our discussion of some specific methods for super-vised learning. These techniques each assume a (different) structured form for the unknown regression function, and by doing so they finesse the curse of dimensionality. Of course, they pay the possible price of misspecifying the model, and so in each case there is a tradeoff that has to be made. We describe five related techniques: generalized additive models, trees, multivariate adaptive regression splines, the patient rule induction method, and hierarchical mixtures of experts." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^9.3]: "Tree-based methods partition the feature space into box-shaped regions, to try to make the response averages in each box as differ-ent as possible. The splitting rules defining the boxes are related to each through a binary tree, facilitating their interpretation." *(Trecho de "Additive Models, Trees, and Related Methods")*

[^9.3.1]: "The patient rule induction method (PRIM) also finds boxes in the feature space, but seeks boxes in which the response average is high. Hence it looks for maxima in the target function, an exercise known as bump hunting. (If minima rather than maxima are desired, one simply works with the negative response values.) PRIM also differs from tree-based partitioning methods in that the box definitions are not described by a binary tree. This makes interpretation of the collection of rules more difficult; however, by removing the binary tree constraint, the individual rules are often simpler. The main box construction method in PRIM works from the top down, starting with a box containing all of the data. The box is compressed along one face by a small amount, and the observations then falling outside the box are peeled off. The face chosen for compression is the one resulting in the largest box mean, after the compression is performed. Then the process is repeated, stopping when the current box contains some minimum number of data points." *(Trecho de "Additive Models, Trees, and Related Methods")*
