## Encontrando o Melhor Modelo para Cada Tamanho de Subconjunto k

```mermaid
graph LR
    A[Conjunto de Predictores] --> B{Sele√ß√£o de Subconjuntos};
    B --> C[Modelos para cada k];
    C --> D{Avalia√ß√£o (RSS)};
    D --> E[Sele√ß√£o do melhor modelo para cada k];
    E --> F[Curvas de Erro (RSS vs k)];
    F --> G{Sele√ß√£o do k √≥timo (AIC, BIC)};
    G --> H[Modelo Final];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Em problemas de sele√ß√£o de modelos, um dos principais desafios √© determinar qual subconjunto de preditores resulta no melhor ajuste, considerando tanto a precis√£o da predi√ß√£o quanto a interpretabilidade do modelo. Um dos m√©todos para abordar este problema √© avaliar todos os modelos para cada tamanho de subconjunto $k$, e ent√£o, selecionar o melhor modelo, para cada valor de $k$ [^1]. Esta se√ß√£o explora detalhadamente esse processo, incluindo o algoritmo para encontrar os melhores modelos, os crit√©rios para sua avalia√ß√£o e as dificuldades inerentes √† escolha do subconjunto ideal.

### Defini√ß√£o do Problema de Sele√ß√£o de Melhor Subconjunto

O objetivo da sele√ß√£o de melhor subconjunto √© encontrar, para cada tamanho de subconjunto $k$, o modelo que minimiza a soma dos quadrados dos res√≠duos (RSS) [^3]. Ou seja, para cada $k \in \{0, 1, 2, ..., p\}$, onde $p$ √© o n√∫mero total de preditores, buscamos o modelo com $k$ preditores que resulta no menor RSS poss√≠vel. Este processo gera uma sequ√™ncia de modelos, cada um com um n√∫mero diferente de preditores [^4].

**Conceito 1: Espa√ßo de Modelos**
Em um problema de sele√ß√£o de modelos, o espa√ßo de modelos √© o conjunto de todos os poss√≠veis modelos que podem ser constru√≠dos a partir de um conjunto de preditores. Para um conjunto de $p$ preditores, o n√∫mero total de modelos poss√≠veis √© $2^p$, o que torna invi√°vel a avalia√ß√£o de todos os modelos para valores de $p$ n√£o muito pequenos [^5].

> üí° **Exemplo Num√©rico:**
> Imagine que temos um conjunto de $p=3$ preditores: $X_1$, $X_2$ e $X_3$. Os subconjuntos poss√≠veis s√£o:
> - $k=0$: {} (modelo com apenas o intercepto)
> - $k=1$: {$X_1$}, {$X_2$}, {$X_3$}
> - $k=2$: {$X_1$, $X_2$}, {$X_1$, $X_3$}, {$X_2$, $X_3$}
> - $k=3$: {$X_1$, $X_2$, $X_3$}
>
> O n√∫mero total de modelos √© $2^3 = 8$. Para cada valor de $k$, devemos encontrar o modelo com o menor RSS.

**Lemma 1:** *O n√∫mero total de subconjuntos de um conjunto com $p$ elementos √© $2^p$. Em sele√ß√£o de melhor subconjunto, o algoritmo examina os modelos dentro de cada um desses subconjuntos* [^6].

**Prova do Lemma 1:**
Para um conjunto de $p$ elementos, cada um dos elementos pode ou n√£o ser inclu√≠do em um subconjunto. Assim, para cada elemento, temos duas op√ß√µes (incluir ou n√£o incluir), resultando em $2 \times 2 \times \ldots \times 2 = 2^p$ subconjuntos poss√≠veis. $\blacksquare$

**Conceito 2: Busca Exaustiva vs. Busca Restrita**
Em geral, encontrar o melhor modelo para cada tamanho $k$ implica em uma busca exaustiva dentro de cada subconjunto. Essa busca exaustiva √© computacionalmente invi√°vel para grandes valores de $p$. Abordagens alternativas, como m√©todos *stepwise*, realizam buscas restritas nos modelos. *O algoritmo de sele√ß√£o do melhor subconjunto √© uma forma de busca exaustiva*. [^7].

```mermaid
graph LR
    A[Conjunto de p Predictores] --> B{k = 0};
    B --> C[Subconjunto k=0];
    A --> D{k = 1};
    D --> E[Subconjuntos k=1];
    A --> F{k = 2};
     F --> G[Subconjuntos k=2];
    A --> H{k = p};
    H --> I[Subconjuntos k=p];
    C --> J{Ajustar Modelos e Calcular RSS};
    E --> J
    G --> J
    I --> J
    J --> K[Melhor Modelo para cada k];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style K fill:#ccf,stroke:#333,stroke-width:2px
```

### Algoritmo para Encontrar o Melhor Subconjunto para Cada k

O algoritmo de sele√ß√£o de melhor subconjunto segue os seguintes passos:

1. **Definir k:** Inicializar a busca com $k = 0$.

2. **Gerar Subconjuntos:** Para cada valor de $k$, gere todos os subconjuntos poss√≠veis de $k$ preditores [^8].

3. **Ajustar Modelos:** Para cada subconjunto de preditores, ajuste um modelo de regress√£o linear e calcule o RSS correspondente [^9].

4. **Selecionar o Melhor:** Selecione o modelo com o menor RSS para cada valor de $k$. O resultado √© um modelo de melhor ajuste para cada tamanho $k$.

5. **Incrementar k:** Incrementar o valor de $k$ e repetir as etapas de 2 a 4 at√© atingir $k=p$.

> üí° **Exemplo Num√©rico:**
> Suponha que temos 3 preditores ($p=3$) e um conjunto de dados com 10 observa√ß√µes.
>
> **k = 0:**
> - Modelo: $y = \beta_0$
> - Ajustamos o modelo e calculamos $RSS_0$.
>
> **k = 1:**
> - Modelos: $y = \beta_0 + \beta_1 x_1$, $y = \beta_0 + \beta_2 x_2$, $y = \beta_0 + \beta_3 x_3$
> - Ajustamos cada modelo e calculamos $RSS_1^1$, $RSS_1^2$ e $RSS_1^3$
> - Selecionamos o modelo com o menor RSS, por exemplo $RSS_1 = min(RSS_1^1, RSS_1^2, RSS_1^3)$
>
> **k = 2:**
> - Modelos: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2$, $y = \beta_0 + \beta_1 x_1 + \beta_3 x_3$, $y = \beta_0 + \beta_2 x_2 + \beta_3 x_3$
> - Ajustamos cada modelo e calculamos $RSS_2^1$, $RSS_2^2$ e $RSS_2^3$
> - Selecionamos o modelo com o menor RSS, por exemplo $RSS_2 = min(RSS_2^1, RSS_2^2, RSS_2^3)$
>
> **k = 3:**
> - Modelo: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3$
> - Ajustamos o modelo e calculamos $RSS_3$.
>
> Ao final, temos o melhor modelo para cada valor de $k$ e seus respectivos RSS.

**Lemma 2:** *O algoritmo de sele√ß√£o de melhor subconjunto garante que, para cada tamanho de subconjunto k, o modelo selecionado seja o de menor RSS dentre todos os modelos poss√≠veis com k preditores* [^10].

**Prova do Lemma 2:** O algoritmo examina todos os poss√≠veis modelos com k preditores, calcula o RSS para cada modelo, e armazena o menor RSS e as vari√°veis que resultaram no menor valor. Portanto, para cada k, o modelo encontrado √© o que tem o menor RSS. $\blacksquare$

### Uso do Algoritmo *Leaps and Bounds*

A busca exaustiva por todos os modelos poss√≠veis pode ser computacionalmente invi√°vel para um grande n√∫mero de preditores. O algoritmo **leaps and bounds** (Furnival and Wilson, 1974) √© um procedimento eficiente que evita a avalia√ß√£o de todos os modelos poss√≠veis e torna a sele√ß√£o de melhor subconjunto vi√°vel para um n√∫mero de preditores maior do que o que seria poss√≠vel utilizando uma for√ßa bruta de todos os modelos [^11]. O algoritmo *leaps and bounds* utiliza as rela√ß√µes entre os valores do RSS em diferentes modelos para direcionar a busca e eliminar modelos que n√£o podem ser os melhores para um dado valor de $k$, acelerando o processo e melhorando a efici√™ncia [^12].

### Interpreta√ß√£o e Uso das Curvas de Erro

Ap√≥s encontrar o melhor modelo para cada valor de $k$, podemos plotar o RSS (ou outras medidas de erro, como o AIC ou BIC) contra o tamanho do subconjunto $k$ [^13]. O resultado √© uma curva de erro, onde cada ponto representa o melhor modelo para um dado n√∫mero de preditores.
-  **Necessidade de crit√©rios para escolha de k:** Note que, a curva de erro geralmente apresenta um comportamento monotonicamente decrescente, o que impossibilita o uso da mesma para determinar o melhor tamanho de subconjunto. √â necess√°rio, portanto, utilizar um crit√©rio de avalia√ß√£o como AIC, BIC ou valida√ß√£o cruzada para escolher o valor √≥timo de $k$ [^14].

> üí° **Exemplo Num√©rico:**
> Suponha que, ap√≥s aplicar o algoritmo de melhor subconjunto, obtivemos os seguintes RSS para cada valor de $k$:
>
> | k   | Melhor Modelo                                         | RSS   |
> |-----|-------------------------------------------------------|-------|
> | 0   | $y = \beta_0$                                        | 150   |
> | 1   | $y = \beta_0 + \beta_1 x_2$                           | 100   |
> | 2   | $y = \beta_0 + \beta_1 x_2 + \beta_3 x_3$             | 80    |
> | 3   | $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3$ | 75    |
>
> Plotando RSS versus k, observamos uma curva decrescente. No entanto, n√£o podemos escolher o modelo com $k=3$ apenas por ter o menor RSS. Precisamos usar AIC/BIC ou valida√ß√£o cruzada para balancear complexidade e ajuste.

**Corol√°rio 1:** *A curva de erro em fun√ß√£o do tamanho do subconjunto k, ao ter um comportamento monotonicamente decrescente, n√£o oferece uma resposta direta para a sele√ß√£o do tamanho √≥timo do subconjunto, tornando o uso de crit√©rios de avalia√ß√£o importantes para a escolha de um modelo entre os modelos de melhor ajuste para cada valor de k* [^15].

### Desafios e Considera√ß√µes na Sele√ß√£o de Modelos

Apesar de o algoritmo de sele√ß√£o do melhor subconjunto encontrar o modelo de menor RSS para cada tamanho $k$, a escolha do valor ideal de $k$ (isto √©, o n√∫mero ideal de preditores para o modelo final) ainda √© um problema a ser resolvido. Algumas das dificuldades e considera√ß√µes incluem [^16]:
1. **Tradeoff Bias-Vari√¢ncia:** Como explicitado em se√ß√µes anteriores deste cap√≠tulo, √© preciso equilibrar o tradeoff bias-vari√¢ncia ao escolher o tamanho do subconjunto. Modelos com poucos preditores (pequeno $k$) tendem a ter alto bias, mas baixa vari√¢ncia, enquanto modelos com muitos preditores (grande $k$) tendem a ter baixo bias, mas alta vari√¢ncia [^17].
2. **Complexidade do Modelo:** A complexidade do modelo aumenta com o n√∫mero de preditores, o que pode dificultar a interpreta√ß√£o e levar a overfitting. Por isso a necessidade de considerar crit√©rios que equilibrem precis√£o e complexidade.
3. **Custos Computacionais:** O n√∫mero de modelos aumenta exponencialmente com o n√∫mero de preditores, e mesmo com algoritmos otimizados como *leaps and bounds*, a busca pelo melhor subconjunto para grandes conjuntos de dados pode ser computacionalmente intensiva.
4. **Interpretabilidade:** Modelos com um n√∫mero moderado de preditores podem oferecer um melhor equil√≠brio entre precis√£o e interpretabilidade [^18].

### Pergunta Te√≥rica Avan√ßada: Como o Crit√©rio AIC se Relaciona com a Sele√ß√£o de Melhor Subconjunto e como ele tenta equilibrar o Tradeoff Bias-Vari√¢ncia?

**Resposta:**

O Crit√©rio de Informa√ß√£o de Akaike (AIC) √© um crit√©rio de sele√ß√£o de modelos que pode ser utilizado para determinar qual √© o valor √≥timo de $k$ na sele√ß√£o de melhor subconjunto. O AIC oferece uma maneira de avaliar a adequa√ß√£o de um modelo, levando em considera√ß√£o tanto a sua precis√£o quanto a sua complexidade [^19]. O AIC √© definido como:

$$ AIC = -2\log(L) + 2p $$

Onde:
-   $L$ √© a verossimilhan√ßa maximizada do modelo
-   $p$ √© o n√∫mero de par√¢metros no modelo (incluindo o intercepto).

No contexto de regress√£o linear, o termo log-verossimilhan√ßa reflete o ajuste do modelo aos dados, enquanto o termo de penalidade $2p$ penaliza a complexidade do modelo [^20].  *O AIC recompensa a precis√£o do modelo, mas penaliza modelos com um grande n√∫mero de par√¢metros*. Assim, o AIC procura um balan√ßo entre a bondade do ajuste e a parcim√¥nia do modelo. O modelo que minimiza o AIC √© considerado o melhor modelo [^21].

> üí° **Exemplo Num√©rico:**
> Considerando os modelos do exemplo anterior, suponha que tenhamos as seguintes log-verossimilhan√ßas e AICs:
>
> | k   | Melhor Modelo                                         | RSS   | Log-Verossimilhan√ßa (L) | p | AIC   |
> |-----|-------------------------------------------------------|-------|-----------------------|---|-------|
> | 0   | $y = \beta_0$                                        | 150   | -40                    | 1 | -78   |
> | 1   | $y = \beta_0 + \beta_1 x_2$                           | 100   | -30                    | 2 | -56   |
> | 2   | $y = \beta_0 + \beta_1 x_2 + \beta_3 x_3$             | 80    | -25                    | 3 | -44   |
> | 3   | $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3$ | 75    | -20                    | 4 | -32   |
>
> *Calculando o AIC:*
> - Para k=0: $AIC = -2*(-40) + 2*1 = 80 + 2 = 82$
> - Para k=1: $AIC = -2*(-30) + 2*2 = 60 + 4 = 64$
> - Para k=2: $AIC = -2*(-25) + 2*3 = 50 + 6 = 56$
> - Para k=3: $AIC = -2*(-20) + 2*4 = 40 + 8 = 48$
>
> Os valores de AIC aqui foram constru√≠dos de forma a ilustrar a ideia de que um modelo com muitos par√¢metros pode ter o menor RSS, mas o AIC pode penalizar esse modelo em favor de um modelo mais simples.
> Note que quanto maior a verossimilhan√ßa (menor o valor negativo), melhor o ajuste do modelo, mas o AIC penaliza o modelo por ter muitos par√¢metros. O modelo com o menor AIC √© o modelo com $k=3$.

A rela√ß√£o entre o AIC e a sele√ß√£o de melhor subconjunto surge da necessidade de selecionar um modelo entre uma lista de melhores modelos encontrados para cada subconjunto de tamanho k.  O AIC √© usado para avaliar modelos de diferentes subconjuntos, e o modelo que apresentar o menor valor de AIC √© aquele que melhor concilia a qualidade do ajuste com a complexidade do modelo, aproximando a melhor solu√ß√£o para o trade-off bias-vari√¢ncia [^22]. *Ao adicionar um termo de penalidade, o AIC impede a escolha de modelos demasiadamente complexos que podem se ajustar perfeitamente aos dados de treinamento mas generalizar mal para novos dados*, o que √© consistente com a ideia de controle da vari√¢ncia. Por esse motivo, o AIC n√£o escolhe necessariamente o modelo que tem o menor RSS no treino, e sim um modelo que balanceia adequadamente essa medida de erro com a complexidade [^23].

```mermaid
graph LR
    A[Modelos para cada k] --> B{C√°lculo do AIC};
    B --> C[Sele√ß√£o do Modelo com menor AIC];
    C --> D[Modelo Final];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

### Conclus√£o

Encontrar o melhor modelo para cada tamanho de subconjunto $k$ √© uma etapa fundamental no processo de sele√ß√£o de modelos [^24]. O algoritmo de sele√ß√£o de melhor subconjunto, auxiliado por m√©todos como *leaps and bounds*, garante que para cada valor de $k$ seja escolhido o modelo com menor RSS, mas n√£o responde a pergunta sobre qual valor de $k$ √© ideal. M√©tricas como o AIC e t√©cnicas de valida√ß√£o cruzada fornecem crit√©rios importantes para a escolha do melhor tamanho de subconjunto, levando em considera√ß√£o o tradeoff bias-vari√¢ncia, a interpretabilidade e a complexidade do modelo [^25]. A compreens√£o deste processo e de suas nuances √© crucial para a constru√ß√£o de modelos de regress√£o lineares robustos e eficazes [^26].

### Refer√™ncias

[^1]: "Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them." *(Trecho de Linear Methods for Regression)*
[^2]: "They are simple and often provide an adequate and interpretable description of how the inputs affect the output." *(Trecho de Linear Methods for Regression)*
[^3]: "In this chapter we describe linear methods for regression..." *(Trecho de Linear Methods for Regression)*
[^4]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^5]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Regression Models and Least Squares)*
[^6]: "The linear model has the form f(x) = Œ≤0 + Œ£j=1 pXjŒ≤j." *(Trecho de Linear Regression Models and Least Squares)*
[^7]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their population." *(Trecho de Linear Regression Models and Least Squares)*
[^8]: "Even if the xi's were not drawn randomly, the criterion is still valid if the yi's are conditionally independent given the inputs xi." *(Trecho de Linear Regression Models and Least Squares)*
[^9]: "Figure 3.1 illustrates the geometry of least-squares fitting in the IRp+1-dimensional space occupied by the pairs (X, Y)." *(Trecho de Linear Regression Models and Least Squares)*
[^10]: "Note that (3.2) makes no assumptions about the validity of model (3.1); it simply finds the best linear fit to the data." *(Trecho de Linear Regression Models and Least Squares)*
[^11]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Regression Models and Least Squares)*
[^12]: "How do we minimize (3.2)? Denote by X the N x (p + 1) matrix with each row an input vector (with a 1 in the first position), and similarly let y be the N-vector of outputs in the training set." *(Trecho de Linear Regression Models and Least Squares)*
[^13]: "Then we can write the residual sum-of-squares as RSS(Œ≤) = (y - XŒ≤)T(y - XŒ≤)." *(Trecho de Linear Regression Models and Least Squares)*
[^14]: "This is a quadratic function in the p + 1 parameters. Differentiating with respect to Œ≤ we obtain" *(Trecho de Linear Regression Models and Least Squares)*
[^15]: "Assuming (for the moment) that X has full column rank, and hence XTX is positive definite, we set the first derivative to zero XTY - XTXŒ≤ = 0." *(Trecho de Linear Regression Models and Least Squares)*
[^16]: "To obtain the unique solution Œ≤ = (XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^17]: "The predicted values at an input vector x0 are given by f(x0) = (1 x0)TŒ≤; the fitted values at the training inputs are ≈∑ = XŒ≤ = X(XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^18]: "The matrix H = X(XTX)-1XT appearing in equation (3.7) is sometimes called the ‚Äúhat‚Äù matrix because it puts the hat on y." *(Trecho de Linear Regression Models and Least Squares)*
[^19]: "Figure 3.2 shows a different geometrical representation of the least squares estimate, this time in IRN." *(Trecho de Linear Regression Models and Least Squares)*
[^20]: "We denote the column vectors of X by x0, x1,..., xp, with x0 = 1. For much of what follows, this first column is treated like any other. These vectors span a subspace of IRN, also referred to as the column space of X." *(Trecho de Linear Regression Models and Least Squares)*
[^21]: "We minimize RSS(Œ≤) = ||y - XŒ≤||2 by choosing Œ≤ so that the residual vector y - ≈∑ is orthogonal to this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^22]: "This orthogonality is expressed in (3.5), and the resulting estimate ≈∑ is hence the orthogonal pro- jection of y onto this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^23]: "The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix." *(Trecho de Linear Regression Models and Least Squares)*
[^24]: "The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion." *(Trecho de Linear Regression Models and Least Squares)*
[^25]: "There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X." *(Trecho de Linear Regression Models and Least Squares)*
[^26]: "Up to now we have made minimal assumptions about the true distribution of the data." *(Trecho de Linear Regression Models and Least Squares)*
