## Procedimento *Leaps and Bounds*: Otimiza√ß√£o na Sele√ß√£o de Melhor Subconjunto

```mermaid
graph LR
    A[In√≠cio] --> B{N√∫mero de Preditoes > 0?};
    B -- Sim --> C[Calcular RSS para todos os modelos de tamanho 1];
    C --> D{Melhor RSS para k=1?};
    D -- Sim --> E["Armazenar melhor RSS e Preditor(es)"];
    D -- N√£o --> C;
    E --> F{k < p?};
    F -- Sim --> G[Incrementar k];
    G --> H[Gerar candidatos baseados no melhor modelo de k-1];
    H --> I[Calcular RSS dos candidatos];
    I --> J{Melhor RSS para k encontrado?};
    J -- Sim --> K["Armazenar melhor RSS e Preditor(es)"];
     J -- N√£o --> I;
    K --> F;
    F -- N√£o --> L[Fim];
    B -- N√£o --> L;
   
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style L fill:#f9f,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

A busca exaustiva pelo melhor subconjunto de preditores em modelos de regress√£o linear pode se tornar computacionalmente invi√°vel √† medida que o n√∫mero de preditores aumenta. O **procedimento *leaps and bounds***, proposto por Furnival e Wilson (1974), surge como uma alternativa eficiente para realizar a sele√ß√£o de melhor subconjunto, evitando a avalia√ß√£o de todos os modelos poss√≠veis e explorando rela√ß√µes entre as somas de quadrados dos res√≠duos (RSS) de diferentes modelos para direcionar a busca [^1]. Nesta se√ß√£o, exploraremos os fundamentos e o funcionamento do procedimento *leaps and bounds*.

### Fundamentos do Procedimento *Leaps and Bounds*

O procedimento *leaps and bounds* √© um algoritmo de busca que explora a natureza hier√°rquica do problema de sele√ß√£o de melhor subconjunto [^3]. Ele utiliza o fato de que se um subconjunto de $k$ preditores n√£o √© o melhor modelo com $k$ preditores, ent√£o nenhum subconjunto com $k+1$ preditores que contenha aquele subconjunto pode ser o melhor para seu tamanho [^4]. Em outras palavras, o algoritmo busca um modelo melhorando a lista atual de modelos √≥timos, utilizando informa√ß√µes sobre os modelos j√° avaliados para realizar uma busca mais eficiente no espa√ßo de modelos [^5].

**Conceito 1: Hierarquia de Modelos**

O espa√ßo de modelos em sele√ß√£o de melhor subconjunto pode ser organizado de forma hier√°rquica, onde os modelos com menos preditores s√£o "submodelos" de modelos com mais preditores [^6]. Essa hierarquia estabelece que o RSS de um modelo nunca pode ser menor que o RSS de qualquer um de seus submodelos.

> üí° **Exemplo Num√©rico:**
> Considere um modelo com 2 preditores, $X_1$ e $X_2$, que resulta em um RSS de 150. Se adicionarmos um terceiro preditor, $X_3$, ao modelo, o novo RSS ser√° necessariamente igual ou menor que 150, digamos 120. Se o modelo com apenas $X_1$ tivesse um RSS de 200, sabemos que o modelo com $X_1$, $X_2$ e $X_3$ n√£o poderia ter um RSS maior que 200. Isso ilustra como a hierarquia de modelos imp√µe limites no RSS de modelos maiores.

**Lemma 1:** *Se um modelo com $k$ preditores n√£o √© o melhor para tamanho $k$, ent√£o nenhum modelo com $k+1$ preditores que contenha o subconjunto de preditores daquele modelo pode ser o melhor para tamanho $k+1$* [^7].

**Prova do Lemma 1:**
Considere um modelo A com k preditores e um modelo B com k+1 preditores que inclui os preditores de A e um preditor a mais. Sendo que A n√£o √© o melhor modelo com k preditores. Como o RSS √© uma fun√ß√£o que decresce com a inclus√£o de mais preditores, o RSS de B √© necessariamente menor ou igual ao RSS de A. Se A n√£o √© o melhor, ent√£o nenhum subconjunto com seus preditores pode ser o melhor para o tamanho k+1. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Suponha que, para $k=2$, o melhor modelo encontrado seja aquele com os preditores $X_1$ e $X_3$, com um RSS de 100. Se o modelo com preditores $X_1$ e $X_2$ tivesse um RSS de 150, sabemos que nenhum modelo com $k=3$ que inclua os preditores $X_1$ e $X_2$ (como $X_1, X_2, X_3$ ou $X_1, X_2, X_4$) poder√° ser o melhor modelo para $k=3$. Isso porque o modelo com $X_1$ e $X_2$ j√° se mostrou pior do que o melhor modelo com $k=2$.

**Conceito 2: *Bounds* Inferiores**

A ideia chave do *leaps and bounds* √© usar *bounds* (limites) inferiores para descartar modelos que n√£o podem ser os melhores para cada tamanho $k$. Os *bounds* s√£o obtidos a partir do RSS de submodelos j√° avaliados, de forma que se o RSS de um submodelo j√° √© maior que o melhor RSS encontrado para modelos com o mesmo ou mais preditores, aquele modelo √© descartado. Isso evita a necessidade de calcular o RSS de todos os modelos [^8].

```mermaid
graph LR
    A["Modelo com k preditores"] --> B{RSS > Melhor RSS para k?};
    B -- Sim --> C[Descartar Modelo];
    B -- N√£o --> D[Manter Modelo para compara√ß√£o];
    
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#fcc,stroke:#333,stroke-width:2px
    style D fill:#cfc,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
> Imaginemos que, para um modelo com 2 preditores, o melhor RSS encontrado at√© agora seja 80. Se um modelo com apenas o preditor $X_1$ apresentar um RSS de 95, ent√£o qualquer modelo com 2 preditores que inclua $X_1$ n√£o poder√° ser melhor que o melhor modelo atual (RSS=80). Isso significa que n√£o precisamos avaliar outros modelos que contenham o preditor $X_1$ para o tamanho $k=2$, pois o *bound* inferior (95) j√° √© maior que o melhor RSS para $k=2$ (80).

### Funcionamento do Algoritmo *Leaps and Bounds*

O algoritmo *leaps and bounds* utiliza a propriedade hier√°rquica dos modelos e os *bounds* inferiores para otimizar a busca pelo melhor modelo para cada tamanho $k$, evitando a avalia√ß√£o exaustiva do espa√ßo de modelos. O funcionamento geral do algoritmo envolve as seguintes etapas [^9]:

1. **Inicializa√ß√£o:** Inicie com um modelo vazio (sem preditores), com RSS igual √† soma dos quadrados da vari√°vel resposta ($RSS_{0}$). Armazene $RSS_{0}$ como o melhor RSS para $k=0$ (menor valor de RSS poss√≠vel para $k=0$).

> üí° **Exemplo Num√©rico:**
> Se a vari√°vel resposta $y$ tiver os valores [5, 8, 10], ent√£o a m√©dia de $y$ √© $\bar{y} = \frac{5+8+10}{3} = 7.67$. O $RSS_0$ (quando n√£o h√° preditores) √© a soma dos quadrados da diferen√ßa entre cada valor de $y$ e a m√©dia: $RSS_0 = (5-7.67)^2 + (8-7.67)^2 + (10-7.67)^2 = 2.67^2 + 0.33^2 + 2.33^2 = 7.13 + 0.11 + 5.43 = 12.67$. Este valor √© o nosso melhor RSS para k=0.

2. **Itera√ß√£o:** Para cada tamanho de modelo $k = 1, 2, \ldots, p$, repita os seguintes passos:
    - **Gerar Candidatos:** A partir da lista de melhores modelos para tamanho $k-1$, gera-se modelos candidatos adicionando um novo preditor.
    - **Avaliar Candidatos:**
      -   Calcule o RSS para cada candidato.
      -   Se o RSS de algum modelo candidato for maior do que o melhor RSS armazenado para modelos de tamanho $k$, este candidato e todos os seus superconjuntos s√£o descartados. Isso utiliza os *bounds* inferiores para podar a busca.
      -   Se o RSS de um modelo candidato √© menor do que o melhor RSS armazenado para modelos de tamanho $k$, armazene o novo RSS como o melhor modelo encontrado para tamanho $k$, e armazene tamb√©m as vari√°veis correspondentes.
```mermaid
sequenceDiagram
    participant Leaps and Bounds
    participant Modelos Candidatos
    participant Melhor Modelo k
    Leaps and Bounds ->> Melhor Modelo k: Inicializa com RSS_0
    loop para k = 1 at√© p
        Leaps and Bounds ->> Modelos Candidatos: Gera modelos com base no melhor modelo k-1
        loop Para cada Modelo Candidato
            Leaps and Bounds ->> Modelos Candidatos: Calcula RSS
            Modelos Candidatos -->> Leaps and Bounds: Retorna RSS
            Leaps and Bounds -->> Melhor Modelo k: RSS < Melhor RSS para k?
            alt RSS < Melhor RSS para k
               Leaps and Bounds ->> Melhor Modelo k: Atualiza melhor modelo e RSS
            else RSS >= Melhor RSS para k
               Leaps and Bounds ->> Modelos Candidatos: Descartar o modelo candidato
            end
        end
    end
    Leaps and Bounds ->> Melhor Modelo k: Fim do processo, retorna melhor modelo para cada k
```
> üí° **Exemplo Num√©rico:**
> Suponha que temos 3 preditores ($X_1$, $X_2$, $X_3$) e estamos na itera√ß√£o para $k=1$.
> -   **Inicializa√ß√£o:** O melhor RSS para $k=0$ √© $RSS_0=12.67$.
> -   **Gera√ß√£o de Candidatos:** Os candidatos para $k=1$ s√£o os modelos com $X_1$, $X_2$ e $X_3$ separadamente.
> -   **Avalia√ß√£o de Candidatos:**
>    -   Digamos que o modelo com $X_1$ tenha um RSS de 10, o modelo com $X_2$ tenha um RSS de 8 e o modelo com $X_3$ tenha um RSS de 15.
>    -   O melhor RSS encontrado para $k=1$ ser√° 8 (modelo com $X_2$).
>    -   Agora, para $k=2$, os candidatos ser√£o formados a partir do modelo com $X_2$, adicionando um dos preditores restantes, $X_1$ ou $X_3$.
>    -   Continuamos o processo para $k=2$, $k=3$, etc.

3.  **Finaliza√ß√£o:** Ao final do processo, obtemos o melhor modelo para cada tamanho $k$, com o respectivo RSS m√≠nimo [^10].

**Lemma 2:** *O algoritmo *leaps and bounds* garante encontrar o melhor modelo (isto √©, o modelo com menor RSS) para cada tamanho $k$ sem a necessidade de avaliar todos os modelos poss√≠veis, atrav√©s do uso de *bounds* inferiores* [^11].

**Prova do Lemma 2:**
A prova se baseia na propriedade da hierarquia dos modelos e no Lemma 1. Dado que o melhor modelo para cada $k$ √© constru√≠do a partir da avalia√ß√£o dos modelos resultantes da inclus√£o de um novo preditor nos modelos √≥timos do passo anterior, o modelo que resulta da melhoria do modelo pr√©-existente √© garantidamente um dos melhores para aquele k. O algoritmo descarta modelos atrav√©s de *bounds*, eliminando aqueles que, com certeza, n√£o resultar√£o em modelos √≥timos no final. Isso garante a identifica√ß√£o do melhor modelo sem precisar analisar todos os poss√≠veis subconjuntos. $\blacksquare$

### Efici√™ncia do Algoritmo *Leaps and Bounds*

O algoritmo *leaps and bounds* √© uma estrat√©gia eficiente para encontrar o melhor subconjunto de preditores para cada tamanho $k$ porque evita a busca exaustiva pelo espa√ßo de modelos. Em vez de avaliar todos os $2^p$ modelos poss√≠veis, o algoritmo avalia somente um n√∫mero limitado de modelos por meio do uso de *bounds* inferiores e da an√°lise hier√°rquica dos modelos.  *Isso torna a aplica√ß√£o da sele√ß√£o de melhor subconjunto vi√°vel para um n√∫mero maior de preditores do que seria poss√≠vel com uma busca exaustiva por for√ßa bruta* [^12].

> üí° **Exemplo Num√©rico:**
> Se tivermos 10 preditores, uma busca exaustiva avaliaria $2^{10} = 1024$ modelos. O *leaps and bounds*, ao usar os *bounds* inferiores, pode reduzir drasticamente o n√∫mero de modelos que precisam ser avaliados. Se, por exemplo, ao avaliar os modelos com 3 preditores, o algoritmo descobrir que um modelo com $X_1$, $X_2$ e $X_3$ tem um RSS alto, n√£o precisar√° avaliar nenhum modelo com 4 preditores que inclua $X_1$, $X_2$ e $X_3$, economizando tempo de computa√ß√£o.

### Implementa√ß√£o e Detalhes Pr√°ticos

A implementa√ß√£o do algoritmo *leaps and bounds* pode ser complexa, pois envolve opera√ß√µes de manipula√ß√£o de conjuntos de preditores, c√°lculo de RSS e compara√ß√£o de *bounds*, mas ela √© significativamente mais eficiente do que uma busca por for√ßa bruta [^13]. Existem varia√ß√µes do algoritmo que implementam diferentes estrat√©gias para atualizar os *bounds* e para construir os conjuntos candidatos de forma mais eficiente [^14].
O algoritmo, em sua forma original, pode ser implementado usando recurs√£o e otimiza√ß√µes que utilizem a decomposi√ß√£o QR para a computa√ß√£o eficiente do RSS de cada subconjunto [^15].

### Interpreta√ß√£o e Uso dos Resultados do *Leaps and Bounds*

O resultado do algoritmo *leaps and bounds* √© um conjunto de modelos, um para cada tamanho de subconjunto $k$, que minimiza o RSS. Esses modelos podem ser comparados e avaliados usando crit√©rios como AIC, BIC ou valida√ß√£o cruzada [^16].
-   **Interpreta√ß√£o:** O resultado do leaps and bounds √© uma sequ√™ncia de modelos, sendo que cada um desses modelos √© o que tem o melhor ajuste (menor RSS) para um n√∫mero espec√≠fico de preditores.
-   **Utiliza√ß√£o:** Com as informa√ß√µes de todos os melhores modelos encontrados, crit√©rios como AIC ou valida√ß√£o cruzada s√£o usados para escolher o melhor modelo considerando o tradeoff bias-vari√¢ncia.

> üí° **Exemplo Num√©rico:**
> Ap√≥s executar o *leaps and bounds*, podemos ter a seguinte tabela de melhores modelos para cada tamanho $k$:
>
> | $k$ | Preditor(es) | RSS   |
> |-----|--------------|-------|
> | 1   | $X_2$        | 8     |
> | 2   | $X_2, X_4$   | 5     |
> | 3   | $X_2, X_4, X_6$ | 3   |
> | 4   | $X_2, X_4, X_6, X_9$ | 2.5 |
>
> Agora, podemos calcular o AIC ou BIC para cada um desses modelos e escolher aquele que oferece o melhor compromisso entre ajuste e complexidade. Por exemplo, se o modelo com $X_2$ e $X_4$ tiver o menor AIC, ele ser√° o modelo selecionado. Alternativamente, podemos usar valida√ß√£o cruzada para avaliar o desempenho preditivo de cada um dos modelos e escolher o que apresentar o melhor desempenho.

**Corol√°rio 2:** *O *leaps and bounds*, embora encontre o melhor modelo para cada tamanho de subconjunto, n√£o oferece uma forma direta para a sele√ß√£o de qual modelo, entre todos os encontrados, ser√° o modelo final*. √â necess√°rio, portanto, o uso de crit√©rios de avalia√ß√£o que levem em considera√ß√£o a qualidade do ajuste e a complexidade do modelo, e a ideia do trade-off bias-vari√¢ncia [^17].

### Desafios e Limita√ß√µes do *Leaps and Bounds*

Apesar de sua efici√™ncia, o *leaps and bounds* apresenta algumas limita√ß√µes e desafios [^18]:
1.  **Complexidade Computacional:** Apesar de mais eficiente que a busca exaustiva, o algoritmo ainda pode ser computacionalmente custoso para grandes conjuntos de dados com muitos preditores, o que limita seu uso.
2.  **Escolha do Modelo Final:** O algoritmo n√£o oferece uma solu√ß√£o direta para a escolha do modelo final, sendo necess√°rio recorrer a crit√©rios externos (como AIC, BIC, valida√ß√£o cruzada) para fazer a sele√ß√£o do modelo.
3.  **Limita√ß√µes em Cen√°rios de Alta Dimensionalidade:** Em situa√ß√µes onde o n√∫mero de preditores se aproxima ou excede o n√∫mero de observa√ß√µes, a busca por todos os subconjuntos pode continuar sendo inst√°vel. Nesses casos m√©todos que imp√µe sparsity podem ser mais adequados.

### Pergunta Te√≥rica Avan√ßada: Como o *Leaps and Bounds* se Compara a M√©todos de Sele√ß√£o *Stepwise* e Como a Escolha entre Esses M√©todos Afeta o Tradeoff Bias-Vari√¢ncia em Problemas de Sele√ß√£o de Modelos?

**Resposta:**

O procedimento *leaps and bounds* e os m√©todos de sele√ß√£o *stepwise* (forward, backward) s√£o abordagens para sele√ß√£o de modelos com diferen√ßas significativas no seu funcionamento e impacto sobre o tradeoff bias-vari√¢ncia [^19].
O *leaps and bounds* √© um procedimento de *busca exaustiva restrita* que encontra o melhor modelo para cada tamanho de subconjunto de preditores, utilizando os *bounds* inferiores para acelerar o processo e evitar a avalia√ß√£o de todas as combina√ß√µes de modelos [^20]. Por outro lado, os m√©todos *stepwise* realizam uma *busca gulosa* atrav√©s do espa√ßo de modelos, adicionando (forward) ou removendo (backward) preditores sequencialmente, baseado no impacto local sobre o RSS. M√©todos *stepwise* n√£o garantem encontrar o melhor modelo para cada n√∫mero de preditores, mas buscam uma solu√ß√£o razo√°vel em um tempo de computa√ß√£o menor, o que os torna mais computacionalmente vi√°veis em cen√°rios de alta dimensionalidade.

*A principal diferen√ßa entre esses m√©todos reside na sua abordagem de busca e no impacto sobre o tradeoff bias-vari√¢ncia* [^21]. O *leaps and bounds*, ao explorar todo o espa√ßo de modelos poss√≠veis (limitado a um dado tamanho de subconjunto), pode levar a um modelo com menor bias para um determinado n√∫mero de preditores. No entanto, devido ao grande n√∫mero de modelos explorados, ele tamb√©m pode aumentar a vari√¢ncia, ou seja, o overfitting, e o tempo de computa√ß√£o. M√©todos stepwise, devido √† busca mais restrita, podem apresentar um bias ligeiramente maior, mas tipicamente menor vari√¢ncia.
*Em termos de interpretabilidade, o m√©todo *leaps and bounds* garante que, para cada tamanho de modelo k, o melhor modelo √© encontrado, mas, como os m√©todos *stepwise*, n√£o apresenta uma forma clara de como escolher entre esses modelos*. Os m√©todos *stepwise*, ao adicionar ou remover preditores sequencialmente,  podem tornar a interpreta√ß√£o mais intuitiva do processo de inclus√£o e exclus√£o de vari√°veis [^22]. Em cen√°rios com muitos preditores e recursos computacionais limitados, os m√©todos *stepwise* s√£o frequentemente mais vi√°veis, mas quando a qualidade da solu√ß√£o √© cr√≠tica e o custo computacional n√£o √© uma grande restri√ß√£o, o *leaps and bounds* pode ser mais adequado [^23].
A escolha entre os dois tipos de m√©todos depende, portanto, da complexidade do problema, da disponibilidade de recursos computacionais e do trade-off desejado entre o bias, a vari√¢ncia e a interpretabilidade do modelo.
```mermaid
graph LR
    A[Leaps and Bounds] --> B[Busca Exaustiva Restrita];
    B --> C[Garante Melhor Modelo para cada k];
    C --> D[Maior Custo Computacional];
    D --> E[Maior Risco de Overfitting];
    A --> F[Resultados Interpret√°veis para cada k];

    G[Stepwise] --> H[Busca Gulosa];
    H --> I[N√£o garante melhor modelo para cada k];
    I --> J[Menor Custo Computacional];
     J --> K[Menor Risco de Overfitting];
    G --> L[Interpreta√ß√£o Intuitiva do Processo];
    
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style G fill:#cfc,stroke:#333,stroke-width:2px
```
> üí° **Exemplo Num√©rico:**
> Considere um problema com 15 preditores. O *leaps and bounds* buscaria o melhor modelo para $k=1$, $k=2$, ..., $k=15$. Em contraste, um m√©todo *forward stepwise* poderia come√ßar com um modelo vazio e adicionar um preditor de cada vez at√© que um crit√©rio de parada seja atingido. Um m√©todo *backward stepwise* come√ßaria com todos os 15 preditores e removeria um por vez. O *leaps and bounds* pode encontrar um modelo com $k=5$ que seja melhor do que o encontrado pelos m√©todos *stepwise* com 5 preditores, mas o custo computacional √© maior. A escolha do melhor m√©todo depende dos recursos computacionais dispon√≠veis e da import√¢ncia de encontrar o melhor modelo poss√≠vel para cada tamanho de subconjunto.

### Conclus√£o

O procedimento *leaps and bounds* √© uma ferramenta essencial para a sele√ß√£o de modelos em regress√£o linear, oferecendo uma forma eficiente de encontrar o melhor subconjunto de preditores para cada tamanho k [^24]. Ao utilizar *bounds* inferiores e explorar a hierarquia dos modelos, este algoritmo evita a necessidade de avaliar todas as combina√ß√µes poss√≠veis, tornando a busca mais eficiente e vi√°vel para problemas complexos. A compreens√£o de seu funcionamento e de suas limita√ß√µes √© crucial para a escolha das melhores estrat√©gias de modelagem em diversas √°reas [^25].

### Refer√™ncias
[^1]: "Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them." *(Trecho de Linear Methods for Regression)*
[^2]: "They are simple and often provide an adequate and interpretable description of how the inputs affect the output." *(Trecho de Linear Methods for Regression)*
[^3]: "In this chapter we describe linear methods for regression..." *(Trecho de Linear Methods for Regression)*
[^4]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^5]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Regression Models and Least Squares)*
[^6]: "The linear model has the form f(x) = Œ≤0 + Œ£j=1 pXjŒ≤j." *(Trecho de Linear Regression Models and Least Squares)*
[^7]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their population." *(Trecho de Linear Regression Models and Least Squares)*
[^8]: "Even if the xi's were not drawn randomly, the criterion is still valid if the yi's are conditionally independent given the inputs xi." *(Trecho de Linear Regression Models and Least Squares)*
[^9]: "Figure 3.1 illustrates the geometry of least-squares fitting in the IRp+1-dimensional space occupied by the pairs (X, Y)." *(Trecho de Linear Regression Models and Least Squares)*
[^10]: "Note that (3.2) makes no assumptions about the validity of model (3.1); it simply finds the best linear fit to the data." *(Trecho de Linear Regression Models and Least Squares)*
[^11]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Regression Models and Least Squares)*
[^12]: "How do we minimize (3.2)? Denote by X the N x (p + 1) matrix with each row an input vector (with a 1 in the first position), and similarly let y be the N-vector of outputs in the training set." *(Trecho de Linear Regression Models and Least Squares)*
[^13]: "Then we can write the residual sum-of-squares as RSS(Œ≤) = (y - XŒ≤)T(y - XŒ≤)." *(Trecho de Linear Regression Models and Least Squares)*
[^14]: "This is a quadratic function in the p + 1 parameters. Differentiating with respect to Œ≤ we obtain" *(Trecho de Linear Regression Models and Least Squares)*
[^15]: "Assuming (for the moment) that X has full column rank, and hence XTX is positive definite, we set the first derivative to zero XTY - XTXŒ≤ = 0." *(Trecho de Linear Regression Models and Least Squares)*
[^16]: "To obtain the unique solution Œ≤ = (XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^17]: "The predicted values at an input vector x0 are given by f(x0) = (1 x0)TŒ≤; the fitted values at the training inputs are ≈∑ = XŒ≤ = X(XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^18]: "The matrix H = X(XTX)-1XT appearing in equation (3.7) is sometimes called the ‚Äúhat‚Äù matrix because it puts the hat on y." *(Trecho de Linear Regression Models and Least Squares)*
[^19]: "Figure 3.2 shows a different geometrical representation of the least squares estimate, this time in IRN." *(Trecho de Linear Regression Models and Least Squares)*
[^20]: "We denote the column vectors of X by x0, x1,..., xp, with x0 = 1. For much of what follows, this first column is treated like any other. These vectors span a subspace of IRN, also referred to as the column space of X." *(Trecho de Linear Regression Models and Least Squares)*
[^21]: "We minimize RSS(Œ≤) = ||y - XŒ≤||2 by choosing Œ≤ so that the residual vector y - ≈∑ is orthogonal to this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^22]: "This orthogonality is expressed in (3.5), and the resulting estimate ≈∑ is hence the orthogonal pro- jection of y onto this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^23]: "The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix." *(Trecho de Linear Regression Models and Least Squares)*
[^24]: "The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion." *(Trecho de Linear Regression Models and Least Squares)*
[^25]: "There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X." *(Trecho de Linear Regression Models and Least Squares)*
[^26]: "Up to now we have made minimal assumptions about the true distribution of the data." *(Trecho de Linear Regression Models and Least Squares)*
