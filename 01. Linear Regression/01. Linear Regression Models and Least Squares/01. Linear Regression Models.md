## Modelos de Regressão Linear e Mínimos Quadrados: Uma Análise Detalhada

### Introdução
Este capítulo se dedica ao estudo dos **modelos de regressão linear** e ao método dos **mínimos quadrados**, um dos mais populares para a estimação dos parâmetros nesses modelos. Os modelos lineares, apesar de terem sido amplamente desenvolvidos na era pré-computacional da estatística, continuam relevantes devido à sua simplicidade, interpretabilidade e, em certos contextos, superioridade preditiva em relação a modelos não lineares mais complexos [^1]. Abordaremos a formulação do modelo, as diferentes fontes das variáveis de entrada, a estimação dos coeficientes por mínimos quadrados, a interpretação geométrica do método, o teorema de Gauss-Markov e as extensões para múltiplos resultados.

### Conceitos Fundamentais
**Modelo de Regressão Linear:**
Um modelo de regressão linear assume que a função de regressão $E(Y|X)$ é linear nas variáveis de entrada $X_1, ..., X_p$ [^1]. Matematicamente, o modelo é expresso como:
$$f(X) = \beta_0 + \sum_{j=1}^{p} X_j\beta_j$$
onde:
- $Y$ é a variável de resposta (ou saída)
- $X = (X_1, X_2, ..., X_p)^T$ é o vetor de variáveis preditoras (ou entradas)
- $\beta_0$ é o intercepto (ou termo constante)
- $\beta_j$ são os coeficientes (ou parâmetros) associados a cada preditor $X_j$
[^2].

É importante notar que o modelo é *linear nos parâmetros* $\beta_j$, e não necessariamente nas variáveis de entrada $X_j$. Isso significa que podemos aplicar transformações nas variáveis de entrada (como logaritmos, raízes quadradas, exponenciações ou interações entre variáveis) e ainda assim manter a linearidade do modelo [^2].

**Fontes das Variáveis de Entrada:**
As variáveis preditoras $X_j$ podem ter diferentes origens [^2]:
- **Entradas Quantitativas:** Variáveis numéricas diretamente mensuráveis.
- **Transformações de Entradas Quantitativas:** Aplicação de funções matemáticas nas variáveis quantitativas (e.g., $\log(X)$, $\sqrt{X}$, $X^2$).
- **Expansões de Base:** Criação de novas variáveis a partir das originais, como em regressão polinomial (e.g., $X_2 = X_1^2$, $X_3 = X_1^3$).
- **Codificação "Dummy" de Entradas Qualitativas:** Representação de variáveis categóricas por meio de variáveis binárias (0 ou 1) para cada nível da categoria. Por exemplo, se $G$ é um fator de cinco níveis, podemos criar variáveis $X_j = I(G = j)$, onde $I$ é a função indicadora.
- **Interações entre Variáveis:** Criação de novas variáveis que representam a interação entre duas ou mais variáveis (e.g., $X_3 = X_1 \cdot X_2$).

**Mínimos Quadrados:**
O método dos **mínimos quadrados** é a técnica mais popular para estimar os parâmetros $\beta$ do modelo de regressão linear. Dado um conjunto de dados de treinamento $(x_1, y_1), ..., (x_N, y_N)$, o objetivo é encontrar os coeficientes $\beta = (\beta_0, \beta_1, ..., \beta_p)^T$ que minimizem a soma dos quadrados dos resíduos (RSS) [^2]:
$$RSS(\beta) = \sum_{i=1}^{N} (y_i - f(x_i))^2 = \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij}\beta_j)^2$$
Sob o ponto de vista estatístico, esse critério é razoável se as observações de treinamento $(x_i, y_i)$ representarem sorteios aleatórios independentes de sua população. Mesmo que os $x_i$ não sejam sorteados aleatoriamente, o critério ainda é válido se os $y_i$ forem condicionalmente independentes, dados os inputs $x_i$ [^2].

**Solução dos Mínimos Quadrados:**
Para encontrar os valores de $\beta$ que minimizam o RSS, podemos usar cálculo diferencial. Definindo $X$ como a matriz $N \times (p+1)$ onde cada linha é um vetor de entrada (com um 1 na primeira posição para o intercepto) e $y$ como o vetor $N \times 1$ de saídas, o RSS pode ser escrito como [^3]:
$$RSS(\beta) = (y - X\beta)^T (y - X\beta)$$
Derivando o RSS em relação a $\beta$ e igualando a zero, obtemos a solução dos mínimos quadrados [^3]:
$$\hat{\beta} = (X^TX)^{-1}X^Ty$$
Essa solução é única se a matriz $X$ tiver posto coluna completo (ou seja, as colunas de $X$ são linearmente independentes), o que implica que a matriz $X^TX$ é positiva definida e, portanto, invertível [^3].

**Interpretação Geométrica:**
A solução dos mínimos quadrados tem uma interpretação geométrica interessante. Os vetores coluna de $X$ (denotados por $x_0, x_1, ..., x_p$, onde $x_0 = 1$) geram um subespaço em $\mathbb{R}^N$, também conhecido como o espaço coluna de $X$. A solução dos mínimos quadrados $\hat{y} = X\hat{\beta}$ representa a projeção ortogonal do vetor de resposta $y$ nesse subespaço. O vetor resíduo $y - \hat{y}$ é ortogonal ao espaço coluna de $X$ [^4].

**Caso de Posto Não Completo:**
Se as colunas de $X$ não forem linearmente independentes (ou seja, $X$ não tem posto coluna completo), a matriz $X^TX$ é singular e a solução dos mínimos quadrados não é única. Isso ocorre, por exemplo, se duas variáveis de entrada forem perfeitamente correlacionadas. No entanto, os valores ajustados $\hat{y} = X\hat{\beta}$ ainda representam a projeção de $y$ no espaço coluna de $X$, e essa projeção é única, mesmo que existam múltiplas maneiras de expressá-la em termos dos vetores coluna de $X$ [^4]. Na prática, a não singularidade de $X^TX$ pode ser resolvida removendo colunas redundantes ou usando técnicas de regularização que serão discutidas em capítulos subsequentes [^4].

**Inferência Estatística:**
Para realizar inferência estatística sobre os parâmetros $\beta$, precisamos fazer algumas suposições sobre a distribuição dos dados. Assumimos que as observações $y_i$ são não correlacionadas, têm variância constante $\sigma^2$ e que os preditores $x_i$ são fixos (não aleatórios). Sob essas condições, a matriz de variância-covariância dos estimadores dos mínimos quadrados é dada por [^5]:
$$Var(\hat{\beta}) = (X^TX)^{-1}\sigma^2$$
Tipicamente, a variância $\sigma^2$ é desconhecida e precisa ser estimada a partir dos dados. Um estimador não viesado para $\sigma^2$ é dado por [^5]:
$$\hat{\sigma}^2 = \frac{1}{N-p-1} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

Para construir testes de hipóteses e intervalos de confiança, é comum assumir que os desvios de $Y$ em torno de sua esperança são aditivos e Gaussianos. Ou seja [^5]:
$$Y = E(Y|X_1, ..., X_p) + \epsilon = \beta_0 + \sum_{j=1}^{p} X_j\beta_j + \epsilon$$
onde $\epsilon \sim N(0, \sigma^2)$. Sob essa suposição, os estimadores dos mínimos quadrados têm distribuição normal multivariada [^5]:
$$\hat{\beta} \sim N(\beta, (X^TX)^{-1}\sigma^2)$$
Além disso, o estimador da variância $\hat{\sigma}^2$ segue uma distribuição qui-quadrado [^5]:
$$\frac{(N-p-1)\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{N-p-1}$$
$\hat{\beta}$ e $\hat{\sigma}^2$ são estatisticamente independentes. Essas propriedades distribucionais são usadas para construir testes de hipóteses e intervalos de confiança para os parâmetros $\beta_j$ [^5].

**Teste de Hipóteses para Coeficientes Individuais:**
Para testar a hipótese nula de que um coeficiente particular $\beta_j$ é igual a zero, formamos o coeficiente padronizado ou Z-score [^6]:
$$z_j = \frac{\hat{\beta}_j}{\hat{\sigma}\sqrt{v_j}}$$
onde $v_j$ é o j-ésimo elemento diagonal de $(X^TX)^{-1}$. Sob a hipótese nula, $z_j$ segue uma distribuição t de Student com $N-p-1$ graus de liberdade. Se o tamanho da amostra for grande, podemos aproximar a distribuição t pela distribuição normal padrão [^6].

**Teste de Hipóteses para Grupos de Coeficientes:**
Para testar a significância de grupos de coeficientes simultaneamente, usamos a estatística F [^6]:
$$F = \frac{(RSS_0 - RSS_1)/(p_1 - p_0)}{RSS_1/(N-p_1 - 1)}$$
onde $RSS_1$ é a soma dos quadrados dos resíduos do modelo maior com $p_1 + 1$ parâmetros, e $RSS_0$ é a soma dos quadrados dos resíduos do modelo menor (aninhado) com $p_0 + 1$ parâmetros. Sob a hipótese nula de que o modelo menor é correto, a estatística F segue uma distribuição F com $p_1 - p_0$ e $N - p_1 - 1$ graus de liberdade [^6].

**Intervalos de Confiança:**
Podemos isolar $\beta_j$ para obter um intervalo de confiança de $1-2\alpha$ para $\beta_j$ [^7]:
$$(\hat{\beta}_j - z^{(1-\alpha)}\hat{\sigma}\sqrt{v_j}, \hat{\beta}_j + z^{(1-\alpha)}\hat{\sigma}\sqrt{v_j})$$
onde $z^{(1-\alpha)}$ é o percentil $1-\alpha$ da distribuição normal.

**Teorema de Gauss-Markov:**
O **Teorema de Gauss-Markov** estabelece que, sob as suposições de linearidade, não viés e variância constante dos erros, os estimadores dos mínimos quadrados são os melhores estimadores lineares não viesados (BLUE - Best Linear Unbiased Estimators). Em outras palavras, eles têm a menor variância entre todos os estimadores lineares não viesados.
Para estimar qualquer combinação linear dos parâmetros $\theta = \alpha^T\beta$, o estimador de mínimos quadrados de $\alpha^T\beta$ é [^9]:
$$\hat{\theta} = \alpha^T\hat{\beta} = \alpha^T(X^TX)^{-1}X^Ty$$
Considerando $X$ como fixo, essa é uma função linear do vetor de resposta $y$. Assumindo que o modelo linear está correto, $\alpha^T\hat{\beta}$ é não tendencioso, pois [^9]:
$$E(\alpha^T\hat{\beta}) = \alpha^T\beta$$
O Teorema de Gauss-Markov afirma que se tivermos qualquer outro estimador linear $\tilde{\theta} = c^Ty$ que não seja tendencioso para $\alpha^T\beta$, ou seja, $E(c^Ty) = \alpha^T\beta$, então [^9]:
$$Var(\alpha^T\hat{\beta}) \leq Var(c^Ty)$$
Em termos mais simples, o Teorema de Gauss-Markov garante que, dentro da classe de estimadores lineares não viesados, o estimador de mínimos quadrados é o mais eficiente (ou seja, tem a menor variância). No entanto, é importante notar que pode existir um estimador viesado com um erro quadrático médio (MSE) menor do que o estimador de mínimos quadrados [^10].

### Conclusão
Este capítulo forneceu uma visão abrangente dos modelos de regressão linear e do método dos mínimos quadrados. Discutimos a formulação do modelo, as diferentes fontes das variáveis de entrada, a derivação e interpretação da solução dos mínimos quadrados, as suposições necessárias para inferência estatística e o Teorema de Gauss-Markov. Os modelos de regressão linear formam a base para muitas outras técnicas estatísticas e são uma ferramenta essencial na análise de dados.
<!-- END -->