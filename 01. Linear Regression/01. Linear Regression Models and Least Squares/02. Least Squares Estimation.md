## Estimativa de Mínimos Quadrados em Regressão Linear

### Introdução
Este capítulo explora o método de **mínimos quadrados** para estimar os parâmetros $\beta$ em modelos de regressão linear [^44]. Como introduzido anteriormente [^44], o objetivo é modelar a relação entre um vetor de entrada $X$ e uma saída real $Y$ através de uma função linear [^43]. Este método, amplamente utilizado, busca minimizar a soma dos quadrados dos resíduos (RSS), que representa a falta de ajuste entre os valores previstos e observados [^45].

### Conceitos Fundamentais

O método de **mínimos quadrados** estima os parâmetros $\beta$ minimizando a soma dos quadrados dos resíduos (RSS) [^45]. A **Residual Sum of Squares (RSS)** quantifica a discrepância entre os valores observados e os valores previstos pelo modelo [^45]. Matematicamente, o RSS é definido como:

$$\
RSS(\beta) = (y - X\beta)^T (y - X\beta)
$$

onde:
*  $y$ é o vetor de valores observados da variável resposta.
*  $X$ é a matriz de desenho (design matrix) contendo os valores das variáveis preditoras.
*  $\beta$ é o vetor de parâmetros a serem estimados.

A equação acima representa uma função quadrática em relação a $\beta$ [^45]. Para encontrar o valor de $\beta$ que minimiza o RSS, derivamos a função em relação a $\beta$ e igualamos a zero [^45]. Este procedimento resulta nas **equações normais**:

$$\
X^T X \beta = X^T y
$$

A solução para as equações normais, que fornece a estimativa de mínimos quadrados para $\beta$, é dada por:

$$\
\hat{\beta} = (X^T X)^{-1} X^T y
$$

Esta solução é única se a matriz $X^T X$ for invertível, o que ocorre quando a matriz $X$ tem posto coluna completo [^45]. A condição de posto completo garante que as colunas de $X$ são linearmente independentes, evitando multicolinearidade perfeita [^45].

**Interpretação Geométrica:**

A solução de mínimos quadrados pode ser interpretada geometricamente como a projeção ortogonal do vetor de resposta $y$ no espaço coluna da matriz $X$ [^46]. O vetor de resíduos $y - X\hat{\beta}$ é ortogonal ao espaço coluna de $X$, garantindo que $\hat{\beta}$ minimize a distância entre $y$ e o espaço coluna de $X$ [^46].

**Considerações Estatísticas:**

Do ponto de vista estatístico, o critério de mínimos quadrados é razoável se as observações de treinamento $(x_i, y_i)$ representarem amostras aleatórias independentes de sua população [^44]. Mesmo que os $x_i$ não sejam amostrados aleatoriamente, o critério permanece válido se os $y_i$ forem condicionalmente independentes, dados os inputs $x_i$ [^44].

**Caso de Não Posto Completo:**

Se as colunas de $X$ não forem linearmente independentes (ou seja, $X$ não tem posto coluna completo), então $X^T X$ é singular e os coeficientes de mínimos quadrados $\beta$ não são definidos de forma única [^46]. No entanto, os valores ajustados $\hat{y} = X\beta$ ainda são a projeção de $y$ no espaço coluna de $X$ [^46]; existe apenas mais de uma maneira de expressar essa projeção em termos dos vetores coluna de $X$ [^46].

### Conclusão

O método de mínimos quadrados fornece uma abordagem sistemática e intuitiva para estimar os parâmetros em modelos de regressão linear [^45]. Ao minimizar a soma dos quadrados dos resíduos, o método busca encontrar o melhor ajuste linear para os dados observados [^45]. A solução obtida através das equações normais possui uma interpretação geométrica clara como uma projeção ortogonal [^46]. É importante considerar as condições estatísticas sob as quais o método é mais apropriado e lidar com o caso de não posto completo através de técnicas de regularização ou reformulação do modelo [^45, 46].

### Referências
[^43]: Page 43, "Linear Methods for Regression"
[^44]: Page 44, "Linear Regression Models and Least Squares"
[^45]: Page 45, "3.2 Linear Regression Models and Least Squares"
[^46]: Page 46, "3. Linear Methods for Regression"
<!-- END -->