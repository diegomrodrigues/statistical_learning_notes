## M√©todos de Sele√ß√£o Sequencial: Adi√ß√£o (Forward) e Remo√ß√£o (Backward) de Preditores

```mermaid
flowchart TD
    subgraph "Sele√ß√£o Forward"
    A[Come√ßar com modelo vazio] --> B{Adicionar melhor preditor};
    B --> C{Crit√©rio de parada atingido?};
    C -- Sim --> E[Modelo final];
    C -- N√£o --> B;
    end
    subgraph "Sele√ß√£o Backward"
    F[Come√ßar com todos os preditores] --> G{Remover pior preditor};
    G --> H{Crit√©rio de parada atingido?};
    H -- Sim --> I[Modelo final];
    H -- N√£o --> G;
    end
    
    E --> K[Comparar resultados];
    I --> K;
    
```

### Introdu√ß√£o

Os m√©todos de sele√ß√£o de modelos sequenciais, que incluem a **sele√ß√£o forward** e a **sele√ß√£o backward**, s√£o abordagens pr√°ticas para lidar com o desafio de selecionar um subconjunto de preditores relevantes em modelos de regress√£o linear [^1]. Ao contr√°rio da sele√ß√£o de melhor subconjunto, que busca exaustivamente o espa√ßo de modelos, as abordagens *stepwise* constroem modelos de maneira sequencial, adicionando ou removendo preditores de forma iterativa [^2]. Nesta se√ß√£o, exploraremos o funcionamento, as vantagens, as desvantagens e as particularidades desses dois m√©todos de sele√ß√£o.

### Sele√ß√£o Forward: Adi√ß√£o Sequencial de Preditores

A **sele√ß√£o forward** √© uma abordagem *bottom-up*, em que o modelo come√ßa sem preditores e adiciona sequencialmente aqueles que mais melhoram o ajuste [^4]. As etapas principais do algoritmo de sele√ß√£o forward s√£o:

1.  **Inicializa√ß√£o:** Comece com um modelo contendo apenas o intercepto (ou um modelo vazio, caso o intercepto n√£o esteja inclu√≠do) [^5].
2.  **Itera√ß√£o:** Em cada passo, adicione ao modelo o preditor que mais reduz o RSS (ou que mais melhora algum crit√©rio de avalia√ß√£o) entre os preditores restantes [^6].
3.  **Crit√©rio de Parada:** Continue a adicionar preditores at√© que algum crit√©rio de parada seja satisfeito (e.g. um n√∫mero m√°ximo de preditores, um limite na melhoria do RSS, um limite para AIC ou BIC) [^7].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um conjunto de dados com uma vari√°vel resposta $y$ e tr√™s preditores $x_1$, $x_2$ e $x_3$. O objetivo √© usar sele√ß√£o forward para encontrar o melhor subconjunto de preditores.
>
> **Dataset:**
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
> from sklearn.metrics import mean_squared_error
>
> data = {
>     'y': [5, 8, 10, 14, 17, 20, 22, 25],
>     'x1': [1, 2, 3, 4, 5, 6, 7, 8],
>     'x2': [2, 1, 4, 3, 6, 5, 8, 7],
>     'x3': [3, 4, 1, 2, 7, 8, 5, 6]
> }
> df = pd.DataFrame(data)
> y = df['y']
> ```
>
> **Passo 1: Inicializa√ß√£o**
>
>   - Modelo inicial: Apenas o intercepto (RSS = Soma dos quadrados totais (SST)).
>   - $RSS_0 = \sum_{i=1}^{N} (y_i - \bar{y})^2 = \sum_{i=1}^{8} (y_i - 15.125)^2 = 276.875$
>
> **Passo 2: Itera√ß√£o 1**
>
>   - Avaliar cada preditor individualmente:
>
>     - Modelo com $x_1$:
>       ```python
>       X1 = df[['x1']]
>       model1 = LinearRegression().fit(X1, y)
>       y_pred1 = model1.predict(X1)
>       rss_x1 = mean_squared_error(y, y_pred1) * len(y)
>       print(f"RSS com x1: {rss_x1:.3f}")
>       ```
>       $RSS_{x1} = 25.0$
>
>     - Modelo com $x_2$:
>       ```python
>       X2 = df[['x2']]
>       model2 = LinearRegression().fit(X2, y)
>       y_pred2 = model2.predict(X2)
>       rss_x2 = mean_squared_error(y, y_pred2) * len(y)
>       print(f"RSS com x2: {rss_x2:.3f}")
>       ```
>       $RSS_{x2} = 32.0$
>
>     - Modelo com $x_3$:
>       ```python
>       X3 = df[['x3']]
>       model3 = LinearRegression().fit(X3, y)
>       y_pred3 = model3.predict(X3)
>       rss_x3 = mean_squared_error(y, y_pred3) * len(y)
>       print(f"RSS com x3: {rss_x3:.3f}")
>       ```
>       $RSS_{x3} = 108.0$
>
>   - Selecionar $x_1$ (menor RSS).
>
> **Passo 3: Itera√ß√£o 2**
>
>   - Avaliar cada preditor restante, adicionado ao modelo com $x_1$:
>
>     - Modelo com $x_1$ e $x_2$:
>       ```python
>       X12 = df[['x1', 'x2']]
>       model12 = LinearRegression().fit(X12, y)
>       y_pred12 = model12.predict(X12)
>       rss_x12 = mean_squared_error(y, y_pred12) * len(y)
>       print(f"RSS com x1 e x2: {rss_x12:.3f}")
>       ```
>       $RSS_{x1,x2} = 1.714$
>
>     - Modelo com $x_1$ e $x_3$:
>       ```python
>       X13 = df[['x1', 'x3']]
>       model13 = LinearRegression().fit(X13, y)
>       y_pred13 = model13.predict(X13)
>       rss_x13 = mean_squared_error(y, y_pred13) * len(y)
>       print(f"RSS com x1 e x3: {rss_x13:.3f}")
>       ```
>       $RSS_{x1,x3} = 23.143$
>   - Selecionar $x_2$ (menor RSS quando adicionado a $x_1$).
>
> **Passo 4: Itera√ß√£o 3**
>
>   - Avaliar o preditor restante ($x_3$) adicionado ao modelo com $x_1$ e $x_2$:
>     - Modelo com $x_1$, $x_2$ e $x_3$:
>       ```python
>       X123 = df[['x1', 'x2', 'x3']]
>       model123 = LinearRegression().fit(X123, y)
>       y_pred123 = model123.predict(X123)
>       rss_x123 = mean_squared_error(y, y_pred123) * len(y)
>       print(f"RSS com x1, x2 e x3: {rss_x123:.3f}")
>       ```
>       $RSS_{x1,x2,x3} = 1.515$
>
> **Crit√©rio de Parada:**
>
>   - Suponha que o crit√©rio de parada seja que a redu√ß√£o do RSS seja menor que 1.0. Como a redu√ß√£o de $RSS_{x1,x2}$ para $RSS_{x1,x2,x3}$ √© de apenas $0.199$, o algoritmo para.
>
> **Resultado Final:**
>
>   - O modelo selecionado pela sele√ß√£o forward √© o modelo com $x_1$ e $x_2$.
>
>   - A ordem de adi√ß√£o dos preditores foi $x_1$, depois $x_2$.

**Conceito 1: Busca Gulosa**

A sele√ß√£o forward √© um exemplo de busca gulosa, onde em cada etapa se toma a decis√£o que parece mais vantajosa no momento, sem levar em considera√ß√£o os efeitos futuros das decis√µes tomadas [^8]. Essa caracter√≠stica faz com que a sele√ß√£o forward possa n√£o encontrar o modelo globalmente √≥timo [^9].

**Lemma 1:** *A sele√ß√£o forward, por ser uma busca gulosa, n√£o garante que o conjunto de preditores selecionados a cada etapa corresponda ao subconjunto que melhor se ajusta aos dados para aquele tamanho de subconjunto*.

**Prova do Lemma 1:**
A sele√ß√£o forward adiciona o preditor que mais reduz o RSS a cada etapa. Ao fazer isso, ele n√£o avalia a melhor combina√ß√£o de preditores a cada etapa, mas utiliza como base o melhor modelo constru√≠do na etapa anterior.  √â poss√≠vel que um preditor que tenha uma pequena contribui√ß√£o para o RSS em uma determinada etapa seja uma parte crucial para um modelo √≥timo com uma combina√ß√£o maior de preditores. $\blacksquare$

**Conceito 2: Crit√©rios de Escolha do Preditores**
A escolha do preditor que ser√° adicionado em cada passo √© realizada utilizando algum crit√©rio de avalia√ß√£o, sendo a redu√ß√£o do RSS um dos mais comuns, mas tamb√©m √© poss√≠vel o uso de crit√©rios como AIC ou BIC, onde modelos complexos s√£o penalizados [^11]. O uso de crit√©rios que penalizam a complexidade do modelo ajuda a evitar o overfitting.

### Sele√ß√£o Backward: Remo√ß√£o Sequencial de Preditores

A **sele√ß√£o backward**, por sua vez, √© uma abordagem *top-down*, que come√ßa com o modelo contendo todos os preditores e remove sequencialmente os preditores menos relevantes [^12]. As etapas principais do algoritmo de sele√ß√£o backward s√£o:
1.  **Inicializa√ß√£o:** Comece com o modelo contendo todos os preditores (todos os $p$) [^13].
2.  **Itera√ß√£o:** Em cada passo, remova do modelo o preditor que menos reduz o RSS (ou que menos piora o crit√©rio de avalia√ß√£o) entre os preditores restantes [^14].
3.  **Crit√©rio de Parada:** Continue a remover preditores at√© que algum crit√©rio de parada seja satisfeito (e.g. um n√∫mero m√≠nimo de preditores, um limite na piora do RSS, um limite para AIC ou BIC) [^15].

```mermaid
sequenceDiagram
    participant Modelo Inicial
    participant Itera√ß√£o
    participant Crit√©rio de Parada
    
    Modelo Inicial->>Itera√ß√£o: Iniciar com todos os preditores
    loop Para cada preditor
        Itera√ß√£o->>Itera√ß√£o: Avaliar remo√ß√£o do preditor
    end
    Itera√ß√£o->>Itera√ß√£o: Remover preditor menos relevante
    Itera√ß√£o->>Crit√©rio de Parada: Verificar crit√©rio de parada
    Crit√©rio de Parada-- Sim -->> Itera√ß√£o: Modelo final
     Crit√©rio de Parada-- N√£o -->> Itera√ß√£o: Nova itera√ß√£o
```

> üí° **Exemplo Num√©rico:**
>
> Usando o mesmo conjunto de dados do exemplo anterior, vamos aplicar a sele√ß√£o backward.
>
> **Passo 1: Inicializa√ß√£o**
>
>   - Modelo inicial: Todos os preditores $x_1, x_2, x_3$.
>   - $RSS_{x1,x2,x3} = 1.515$ (calculado no exemplo forward).
>
> **Passo 2: Itera√ß√£o 1**
>
>   - Avaliar a remo√ß√£o de cada preditor individualmente:
>
>     - Remover $x_1$: Modelo com $x_2$ e $x_3$:
>     ```python
>     X23 = df[['x2', 'x3']]
>     model23 = LinearRegression().fit(X23, y)
>     y_pred23 = model23.predict(X23)
>     rss_x23 = mean_squared_error(y, y_pred23) * len(y)
>     print(f"RSS com x2 e x3: {rss_x23:.3f}")
>     ```
>       $RSS_{x2,x3} = 24.857$
>
>     - Remover $x_2$: Modelo com $x_1$ e $x_3$:
>       $RSS_{x1,x3} = 23.143$ (calculado no exemplo forward).
>
>     - Remover $x_3$: Modelo com $x_1$ e $x_2$:
>       $RSS_{x1,x2} = 1.714$ (calculado no exemplo forward).
>
>   - Remover $x_3$ (menor aumento no RSS).
>
> **Passo 3: Itera√ß√£o 2**
>
>   - Avaliar a remo√ß√£o de cada preditor restante:
>
>     - Remover $x_1$: Modelo com apenas $x_2$:
>       $RSS_{x2} = 32$ (calculado no exemplo forward).
>
>     - Remover $x_2$: Modelo com apenas $x_1$:
>       $RSS_{x1} = 25$ (calculado no exemplo forward).
>
>   - Remover $x_1$ (menor aumento no RSS).
>
> **Passo 4: Itera√ß√£o 3**
>
>   - Modelo final: Apenas $x_2$.
>
> **Crit√©rio de Parada:**
>
>   - Suponha que o crit√©rio de parada seja um n√∫mero m√≠nimo de preditores igual a 2.
>
> **Resultado Final:**
>
>   - O modelo selecionado pela sele√ß√£o backward √© o modelo com $x_1$ e $x_2$.
>   - A ordem de remo√ß√£o dos preditores foi $x_3$, depois $x_1$.
>
> **Compara√ß√£o dos Resultados:**
>
>   - Neste exemplo, ambos os m√©todos, forward e backward, selecionaram o modelo com $x_1$ e $x_2$. No entanto, a ordem de sele√ß√£o foi diferente. Isso demonstra como as buscas gulosas podem levar a resultados diferentes dependendo da abordagem.
>
> | M√©todo | Preditoes Selecionados | RSS  |
> |--------|-----------------------|------|
> | Forward| $x_1$, $x_2$           | 1.714|
> | Backward| $x_1$, $x_2$          | 1.714|

**Conceito 3: Busca Gulosa e Multicolinearidade**

Assim como a sele√ß√£o forward, a sele√ß√£o backward tamb√©m √© uma busca gulosa e, portanto, n√£o garante a obten√ß√£o do melhor modelo poss√≠vel. A sele√ß√£o backward, por come√ßar com todos os preditores, pode ser especialmente adequada para casos de alta colinearidade, j√° que o modelo inicial √© capaz de considerar as intera√ß√µes entre todas as vari√°veis [^16].

**Lemma 2:** *A sele√ß√£o backward, tamb√©m por ser uma busca gulosa, n√£o garante encontrar o modelo com menor RSS para o dado n√∫mero de preditores*. Modelos removidos em uma dada etapa podem ser cruciais em modelos posteriores [^17].

**Prova do Lemma 2:**
O algoritmo backward escolhe a vari√°vel para remover do modelo atual com base na sua menor contribui√ß√£o na fun√ß√£o de perda naquele momento.  Em outras palavras, ao remover um preditor com base em seu impacto local no RSS, ele n√£o leva em considera√ß√£o que este preditor poderia ser importante em combina√ß√µes com outros preditores removidos posteriormente.  $\blacksquare$

**Conceito 4: Crit√©rios de Remo√ß√£o de Preditores**

O crit√©rio mais usado √© remover o preditor que tem o menor efeito na fun√ß√£o de perda, o que muitas vezes corresponde ao preditor com o menor valor absoluto do escore Z (ou uma estat√≠stica equivalente) [^18]. Assim como na adi√ß√£o, crit√©rios mais sofisticados como AIC e BIC podem ser utilizados para garantir que se escolha um modelo balanceando complexidade e ajuste.

### Diferen√ßas e Similaridades entre Forward e Backward
```mermaid
flowchart TD
    A[Sele√ß√£o Forward] --> B(Abordagem: "Bottom-up");
    A --> C(Aplicabilidade: "Pode ser usada com p > n");
    A --> D(Custo: "Menor custo computacional");
    E[Sele√ß√£o Backward] --> F(Abordagem: "Top-down");
    E --> G(Aplicabilidade: "Requer n > p");
     E --> H(Custo: "Maior custo computacional");
     B --> I[Compara√ß√£o];
     C --> I;
     D --> I;
     F --> I;
     G --> I;
     H --> I;
     I --> J[Resultados e Propriedades]
```

Embora ambas as abordagens busquem selecionar um subconjunto de preditores, elas diferem em sua forma de abordagem e, portanto, em seus resultados e propriedades [^19]:
1. **Abordagem:** A sele√ß√£o forward come√ßa com um modelo simples e adiciona preditores, enquanto a sele√ß√£o backward come√ßa com um modelo complexo e remove preditores.
2. **Aplicabilidade:** A sele√ß√£o forward pode ser aplicada mesmo quando o n√∫mero de preditores √© maior do que o n√∫mero de observa√ß√µes, enquanto a sele√ß√£o backward s√≥ pode ser utilizada quando h√° mais observa√ß√µes do que preditores [^20].
3. **Custo Computacional:** A sele√ß√£o backward requer a avalia√ß√£o de um n√∫mero de modelos maior do que a sele√ß√£o forward, pois ela come√ßa com todos os preditores.

**Corol√°rio 1:** *A escolha entre sele√ß√£o forward e backward depende da estrutura do problema*. Em problemas de alta dimensionalidade, a sele√ß√£o forward pode ser mais adequada por ter um custo computacional mais baixo e por permitir iniciar a busca com um modelo simples, enquanto que a sele√ß√£o backward √© mais adequada quando a multicolinearidade √© forte, mas exige que o n√∫mero de observa√ß√µes seja maior que o de preditores [^21].

### Impacto dos M√©todos Sequenciais no Tradeoff Bias-Vari√¢ncia
```mermaid
flowchart TD
    A[Sele√ß√£o Forward] --> B{Inicia com alto bias e baixa vari√¢ncia};
    B --> C{Adiciona preditores at√© bias aceit√°vel};
    C --> D{Aumenta vari√¢ncia};
    E[Sele√ß√£o Backward] --> F{Inicia com baixo bias e alta vari√¢ncia};
    F --> G{Remove preditores at√© balanceamento};
    G --> H{Pode remover preditores importantes};
    D--> I[Tradeoff Bias-Vari√¢ncia];
    H-->I;
```

Tanto a sele√ß√£o forward quanto a sele√ß√£o backward impactam o tradeoff bias-vari√¢ncia de maneiras distintas [^22]:

1.  **Sele√ß√£o Forward:** A sele√ß√£o forward tende a iniciar com alto bias e baixa vari√¢ncia, adicionando preditores at√© que o bias seja aceit√°vel e, com isso, a vari√¢ncia aumente. A escolha do crit√©rio de parada pode levar a resultados diferentes, com escolhas que consideram crit√©rios com penalidade pela complexidade levando a um menor bias na predi√ß√£o [^23].
2. **Sele√ß√£o Backward:** A sele√ß√£o backward tende a iniciar com baixo bias, mas potencialmente alta vari√¢ncia, e remove preditores at√© que um balanceamento mais aceit√°vel entre bias e vari√¢ncia seja alcan√ßado. *Seu resultado √© influenciado pela escolha da m√©trica para decidir qual preditor remover e pode levar √† remo√ß√£o de preditores que seriam importantes em outros modelos* [^24].

A escolha entre os dois m√©todos depende do cen√°rio espec√≠fico e das prioridades do modelo (e.g. bias ou vari√¢ncia, complexidade ou interpretabilidade) [^25].

### Vantagens e Desvantagens dos M√©todos Sequenciais

*As vantagens dos m√©todos sequenciais incluem a simplicidade da implementa√ß√£o e o menor custo computacional em rela√ß√£o ao m√©todo de melhor subconjunto* [^26]. No entanto, esses m√©todos n√£o garantem que encontremos o melhor subconjunto de preditores, uma vez que realizam uma busca gulosa que n√£o necessariamente considera os efeitos das decis√µes futuras [^27].
-   **Forward:** √â adequado quando o n√∫mero de preditores √© grande e o custo computacional √© uma preocupa√ß√£o, mas pode levar a resultados sub-√≥timos quando o n√∫mero de preditores √© alto.
-   **Backward:** √â adequado para casos de forte colinearidade e quando o n√∫mero de observa√ß√µes √© maior que o de preditores, mas pode remover preditores importantes se avaliados apenas localmente [^28].

### Pergunta Te√≥rica Avan√ßada: Em que Medida as Escolhas de Crit√©rios de Avalia√ß√£o (RSS, AIC, BIC) nos M√©todos *Stepwise* Influenciam o Balanceamento entre Bias e Vari√¢ncia e a Sele√ß√£o Final do Modelo?

**Resposta:**
As escolhas dos crit√©rios de avalia√ß√£o (RSS, AIC, BIC) nos m√©todos *stepwise* t√™m um impacto significativo no balanceamento entre bias e vari√¢ncia e, consequentemente, no modelo final selecionado [^29].

- **RSS (Residual Sum of Squares):** Ao utilizar apenas a redu√ß√£o do RSS como crit√©rio de avalia√ß√£o, os m√©todos *stepwise* tendem a adicionar ou remover preditores que minimizam o erro de ajuste nos dados de treinamento, o que √© o oposto da ideia de parcim√¥nia do modelo.  Isso pode levar a modelos complexos e inst√°veis, que podem se ajustar bem aos dados de treino, mas generalizar mal, resultando em baixo bias e alta vari√¢ncia. A utiliza√ß√£o direta do RSS tende a superestimar a import√¢ncia de preditores espec√≠ficos nos dados do treino, e pode piorar o desempenho preditivo em dados n√£o vistos [^30].

-   **AIC (Akaike Information Criterion):** O AIC adiciona um termo de penalidade √† verossimilhan√ßa do modelo que aumenta com a complexidade do modelo. Dessa forma, ao escolher o modelo com menor AIC, o m√©todo tenta equilibrar a bondade do ajuste com a parcim√¥nia, o que resulta em modelos mais simples, com maior bias e menor vari√¢ncia [^31]. A escolha por este crit√©rio busca evitar o overfitting nos dados de treino e buscar um modelo que generaliza melhor.

-   **BIC (Bayesian Information Criterion):** O BIC tamb√©m penaliza a complexidade do modelo, mas de forma mais forte do que o AIC, e portanto ele tende a selecionar modelos ainda mais simples, com menos preditores [^32]. O BIC pode ser prefer√≠vel quando a interpretabilidade do modelo √© uma prioridade, ou quando se busca modelos mais simples para melhor generaliza√ß√£o, mesmo ao custo de uma perda pequena no ajuste aos dados [^33].
```mermaid
flowchart TD
    A[RSS] --> B(Minimiza erro nos dados de treino);
    B --> C(Modelos complexos e inst√°veis);
        C --> D(Baixo Bias, Alta Vari√¢ncia);

    E[AIC] --> F(Equilibra ajuste e parcim√¥nia);
    F --> G(Modelos mais simples);
    G --> H(Maior Bias, Menor Vari√¢ncia);

    I[BIC] --> J(Penaliza complexidade mais forte);
    J --> K(Modelos ainda mais simples);
      K --> L(Alta interpretabilidade);
    D-->M[Impacto no Tradeoff Bias-Vari√¢ncia];
    H-->M;
     L-->M;
```

A escolha entre esses crit√©rios afeta diretamente o tradeoff bias-vari√¢ncia e o modelo final. Modelos selecionados com base apenas no RSS tendem a ser menos generaliz√°veis e apresentar maior vari√¢ncia. Ao introduzir crit√©rios de penalidade (AIC e BIC), os m√©todos *stepwise* tendem a convergir para modelos com um melhor equil√≠brio entre bias e vari√¢ncia e potencialmente melhor performance preditiva em dados n√£o vistos.  *O uso de crit√©rios como AIC e BIC pode levar a um modelo com um custo computacional menor e um melhor equil√≠brio entre qualidade de ajuste e complexidade* [^34].

### Conclus√£o

A sele√ß√£o sequencial de preditores, com os m√©todos forward e backward, oferece formas pr√°ticas de lidar com o problema de sele√ß√£o de modelos em regress√£o linear [^35]. Embora ambas as abordagens sejam buscas gulosas e n√£o garantam a obten√ß√£o do melhor modelo global, cada uma tem vantagens e desvantagens distintas que dependem das caracter√≠sticas dos dados e dos objetivos do analista. A escolha adequada entre esses m√©todos e o uso de crit√©rios de avalia√ß√£o apropriados s√£o essenciais para a constru√ß√£o de modelos de regress√£o linear robustos e interpret√°veis [^36].

### Refer√™ncias

[^1]: "Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them." *(Trecho de Linear Methods for Regression)*
[^2]: "They are simple and often provide an adequate and interpretable description of how the inputs affect the output." *(Trecho de Linear Methods for Regression)*
[^3]: "In this chapter we describe linear methods for regression..." *(Trecho de Linear Methods for Regression)*
[^4]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^5]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Regression Models and Least Squares)*
[^6]: "The linear model has the form f(x) = Œ≤0 + Œ£j=1 pXjŒ≤j." *(Trecho de Linear Regression Models and Least Squares)*
[^7]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their population." *(Trecho de Linear Regression Models and Least Squares)*
[^8]: "Even if the xi's were not drawn randomly, the criterion is still valid if the yi's are conditionally independent given the inputs xi." *(Trecho de Linear Regression Models and Least Squares)*
[^9]: "Figure 3.1 illustrates the geometry of least-squares fitting in the IRp+1-dimensional space occupied by the pairs (X, Y)." *(Trecho de Linear Regression Models and Least Squares)*
[^10]: "Note that (3.2) makes no assumptions about the validity of model (3.1); it simply finds the best linear fit to the data." *(Trecho de Linear Regression Models and Least Squares)*
[^11]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Regression Models and Least Squares)*
[^12]: "How do we minimize (3.2)? Denote by X the N x (p + 1) matrix with each row an input vector (with a 1 in the first position), and similarly let y be the N-vector of outputs in the training set." *(Trecho de Linear Regression Models and Least Squares)*
[^13]: "Then we can write the residual sum-of-squares as RSS(Œ≤) = (y - XŒ≤)T(y - XŒ≤)." *(Trecho de Linear Regression Models and Least Squares)*
[^14]: "This is a quadratic function in the p + 1 parameters. Differentiating with respect to Œ≤ we obtain" *(Trecho de Linear Regression Models and Least Squares)*
[^15]: "Assuming (for the moment) that X has full column rank, and hence XTX is positive definite, we set the first derivative to zero XTY - XTXŒ≤ = 0." *(Trecho de Linear Regression Models and Least Squares)*
[^16]: "To obtain the unique solution Œ≤ = (XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^17]: "The predicted values at an input vector x0 are given by f(x0) = (1 x0)TŒ≤; the fitted values at the training inputs are ≈∑ = XŒ≤ = X(XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^18]: "The matrix H = X(XTX)-1XT appearing in equation (3.7) is sometimes called the ‚Äúhat‚Äù matrix because it puts the hat on y." *(Trecho de Linear Regression Models and Least Squares)*
[^19]: "Figure 3.2 shows a different geometrical representation of the least squares estimate, this time in IRN." *(Trecho de Linear Regression Models and Least Squares)*
[^20]: "We denote the column vectors of X by x0, x1,..., xp, with x0 = 1. For much of what follows, this first column is treated like any other. These vectors span a subspace of IRN, also referred to as the column space of X." *(Trecho de Linear Regression Models and Least Squares)*
[^21]: "We minimize RSS(Œ≤) = ||y - XŒ≤||2 by choosing Œ≤ so that the residual vector y - ≈∑ is orthogonal to this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^22]: "This orthogonality is expressed in (3.5), and the resulting estimate ≈∑ is hence the orthogonal pro- jection of y onto this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^23]: "The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix." *(Trecho de Linear Regression Models and Least Squares)*
[^24]: "The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion." *(Trecho de Linear Regression Models and Least Squares)*
[^25]: "There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X." *(Trecho de Linear Regression Models and Least Squares)*
[^26]: "Up to now we have made minimal assumptions about the true distribution of the data." *(Trecho de Linear Regression Models and Least Squares)*
[^27]: "In order to pin down the sampling properties of Œ≤, we now assume that the observations yi are uncorrelated and have constant variance œÉ¬≤, and that the xi are fixed (non random)." *(Trecho de Linear Regression Models and Least Squares)*
[^28]: "The variance-covariance matrix of the least squares parameter estimates is easily derived from (3.6) and is given by Var(Œ≤) = (XTX)-1œÉ2." *(Trecho de Linear Regression Models and Least Squares)*
[^29]: "Typically one estimates the variance œÉ¬≤ by √¥¬≤ = (1/(N-p-1)) Œ£i(Yi-≈∂i)¬≤." *(Trecho de Linear Regression Models and Least Squares)*
[^30]: "To test the hypothesis that a particular coefficient Œ≤j = 0, we form the standardized coefficient or Z-score Zj = Œ≤j /√¥‚àövj, where vj is the jth diagonal element of (XTX)-1." *(Trecho de Linear Regression Models and Least Squares)*
[^31]: "Under the null hypothesis that Œ≤j = 0, zj is distributed as tN-p-1 (a t distribution with N ‚Äì p ‚Äì 1 degrees of freedom)" *(Trecho de Linear Regression Models and Least Squares)*
[^32]: "Often we need to test for the significance of groups of coefficients simultaneously." *(Trecho de Linear Regression Models and Least Squares)*
[^33]: "For example, to test if a categorical variable with k levels can be excluded from a model, we need to test whether the coefficients of the dummy variables used to represent the levels can all be set to zero." *(Trecho de Linear Regression Models and Least Squares)*
[^34]: "Here we use the F statistic, F = ((RSS0 - RSS1)/(p1 - p0))/(RSS1/(N-p1 - 1))" *(Trecho de Linear Regression Models and Least Squares)*
[^35]: "where RSS1 is the residual sum-of-squares for the least squares fit of the bigger model with p1 + 1 parameters, and RSS0 the same for the nested smaller model with p0 + 1 parameters, having p1 - p0 parameters constrained to be zero." *(Trecho de Linear Regression Models and Least Squares)*
[^36]: "The F statistic measures the change in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of œÉ¬≤." *(Trecho de Linear Regression Models and Least Squares)*
