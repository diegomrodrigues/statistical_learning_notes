## Estimadores Viesados vs. N√£o Viesados: O Equil√≠brio entre Vi√©s e Vari√¢ncia

```mermaid
  flowchart LR
      A["Par√¢metro Real (Œ∏)"]
      B["Estimador N√£o Viesado (Œ∏ÃÇ)"]
      C["Estimador Viesado (Œ∏ÃÇ')"]
      A -->|Centrado em Œ∏| B
      A -->|Deslocado de Œ∏| C
      style B fill:#ccf,stroke:#333,stroke-width:2px
      style C fill:#fcc,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Em estat√≠stica, um **estimador n√£o viesado** √© aquele cujo valor esperado corresponde ao valor real do par√¢metro que est√° sendo estimado. No entanto, em muitos casos, √© vantajoso usar **estimadores viesados** que, mesmo que apresentem um erro sistem√°tico, podem ter uma vari√¢ncia menor e um erro quadr√°tico m√©dio (MSE) total reduzido, devido ao equil√≠brio entre o *bias* e a *variance*. Este cap√≠tulo explorar√° a distin√ß√£o entre estimadores viesados e n√£o viesados, os cen√°rios onde o uso de estimadores viesados pode ser vantajoso, e a rela√ß√£o entre vi√©s, vari√¢ncia, e o *tradeoff bias-variance*.

### Defini√ß√£o de Estimadores Viesados e N√£o Viesados

1.  **Estimador N√£o Viesado:** Um estimador $\hat{\theta}$ de um par√¢metro $\theta$ √© considerado **n√£o viesado** se a esperan√ßa matem√°tica (valor esperado) do estimador √© igual ao valor verdadeiro do par√¢metro [^47]:
$$
E[\hat{\theta}] = \theta
$$
onde:
     -  $\hat{\theta}$ √© o estimador do par√¢metro $\theta$.
     -  $E[\cdot]$ denota o valor esperado.
     -  $\theta$ √© o valor verdadeiro do par√¢metro.
Essa defini√ß√£o implica que, em m√©dia, o estimador n√£o viesado acerta o valor verdadeiro, e que ao aumentar o n√∫mero de amostras, o estimador converge para o valor verdadeiro do par√¢metro.

> üí° **Exemplo Num√©rico:**
> Suponha que queremos estimar a m√©dia da altura de uma popula√ß√£o. Se coletarmos v√°rias amostras e calcularmos a m√©dia de cada amostra, a m√©dia das m√©dias amostrais ser√° um estimador n√£o viesado da m√©dia da popula√ß√£o. Se a m√©dia real da popula√ß√£o for 170 cm, um estimador n√£o viesado ter√° um valor esperado de 170 cm.

2.  **Estimador Viesado:** Um estimador $\hat{\theta}$ √© considerado **viesado** se a esperan√ßa matem√°tica do estimador n√£o √© igual ao valor verdadeiro do par√¢metro:
$$
E[\hat{\theta}] \neq \theta
$$
O vi√©s, ou *bias* do estimador √© a diferen√ßa entre o valor esperado do estimador e o valor verdadeiro:
$$
Bias(\hat{\theta}) = E[\hat{\theta}] - \theta
$$
Um estimador viesado pode superestimar ou subestimar o valor verdadeiro do par√¢metro, e este vi√©s representa um erro sistem√°tico.
Apesar de apresentar esse erro sistem√°tico, estimadores viesados podem ser √∫teis se eles tiverem uma menor vari√¢ncia, resultando em um erro quadr√°tico m√©dio total menor.

> üí° **Exemplo Num√©rico:**
> Imagine que, ao estimar a altura m√©dia, sempre adicionamos 2 cm √† m√©dia de cada amostra devido a um erro de medi√ß√£o. Esse estimador seria viesado, pois seu valor esperado seria 172 cm (se a m√©dia real fosse 170 cm), com um vi√©s de 2 cm.

### A Rela√ß√£o entre Vi√©s, Vari√¢ncia e MSE

A escolha entre um estimador viesado e n√£o viesado envolve um compromisso entre o vi√©s e a vari√¢ncia. O **Erro Quadr√°tico M√©dio (MSE)**, que mede o erro total do estimador, √© dado por:

$$
MSE(\hat{\theta}) = E[(\hat{\theta} - \theta)^2]
$$
Como j√° demonstrado, o MSE pode ser decomposto em dois componentes:
$$
MSE(\hat{\theta}) = Var(\hat{\theta}) + [E(\hat{\theta}) - \theta]^2
$$
onde:
- $Var(\hat{\theta})$ √© a vari√¢ncia do estimador, que quantifica a dispers√£o das estimativas ao redor da sua m√©dia.
- $[E(\hat{\theta}) - \theta]^2$ √© o quadrado do vi√©s, que quantifica a diferen√ßa entre a m√©dia das estimativas e o valor verdadeiro do par√¢metro.

```mermaid
  flowchart LR
      A["MSE(Œ∏ÃÇ)"]
      B["Vari√¢ncia(Œ∏ÃÇ)"]
      C["Vi√©s(Œ∏ÃÇ)¬≤"]
      A --> B
      A --> C
      style A fill:#ccf,stroke:#333,stroke-width:2px
```

Esta decomposi√ß√£o explicita que a minimiza√ß√£o do MSE envolve um equil√≠brio entre reduzir o vi√©s e reduzir a vari√¢ncia.
O *tradeoff* entre *bias* e *variance* se refere √† rela√ß√£o inversa entre estes dois componentes do erro. Reduzir um deles, geralmente, aumenta o outro.

**Lemma 28:** O *Bias-Variance Tradeoff*

O *Bias-Variance Tradeoff* implica que nem sempre o estimador n√£o viesado √© o melhor. Um estimador com alto *bias* e baixa vari√¢ncia pode apresentar um MSE total menor que um estimador com baixo *bias* e alta vari√¢ncia. A escolha do melhor estimador depende do balan√ßo entre o erro sistem√°tico e o erro aleat√≥rio [^2].

**Prova do Lemma 28:**
O MSE √© a soma da vari√¢ncia do estimador, e do quadrado do seu vi√©s. Um estimador n√£o viesado tem vi√©s zero, o que implica que o seu MSE seja dado somente pela sua vari√¢ncia. Um estimador viesado, entretanto, tem um MSE que √© dado pela soma da sua vari√¢ncia e do vi√©s ao quadrado. No entanto, √© poss√≠vel obter um estimador viesado que tenha um MSE menor que o estimador n√£o viesado, o que ocorre quando a redu√ß√£o na vari√¢ncia √© maior do que o aumento no vi√©s ao quadrado. Isto demonstra o compromisso entre *bias* e *variance*. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Suponha que temos dois estimadores para um par√¢metro $\theta = 5$.
> - Estimador 1 (N√£o Viesado): $E[\hat{\theta}_1] = 5$, $Var(\hat{\theta}_1) = 2$.
> - Estimador 2 (Viesado): $E[\hat{\theta}_2] = 4.5$, $Var(\hat{\theta}_2) = 0.5$.
>
> $MSE(\hat{\theta}_1) = Var(\hat{\theta}_1) + (E[\hat{\theta}_1] - \theta)^2 = 2 + (5-5)^2 = 2$.
>
> $MSE(\hat{\theta}_2) = Var(\hat{\theta}_2) + (E[\hat{\theta}_2] - \theta)^2 = 0.5 + (4.5-5)^2 = 0.5 + 0.25 = 0.75$.
>
> Apesar do Estimador 2 ser viesado, ele tem um MSE menor do que o Estimador 1. Isso demonstra que um estimador viesado pode ser prefer√≠vel se a redu√ß√£o na vari√¢ncia compensar o vi√©s introduzido.

**Corol√°rio 28:** Escolha do Estimador e seus Par√¢metros
O Corol√°rio 28 implica que a escolha do estimador √© um problema que envolve a escolha do m√©todo de estima√ß√£o e dos seus par√¢metros. A escolha entre um estimador com baixo vi√©s e alta vari√¢ncia (como por exemplo o m√≠nimos quadrados sem regulariza√ß√£o), e um estimador com alto vi√©s e baixa vari√¢ncia (como uma regress√£o regularizada), deve considerar o desempenho preditivo e a capacidade de generaliza√ß√£o do modelo.

###  Exemplos de Estimadores Viesados e N√£o Viesados

Em modelos de regress√£o linear, o m√©todo de m√≠nimos quadrados fornece estimadores n√£o viesados sob a condi√ß√£o que o modelo seja especificado corretamente, o erro tenha m√©dia zero, e os erros sejam n√£o correlacionados e homoced√°sticos. No entanto, a regulariza√ß√£o produz estimadores que podem ser viesados.
    1. **M√≠nimos Quadrados sem Regulariza√ß√£o:** Sob as suposi√ß√µes do modelo linear, a solu√ß√£o de m√≠nimos quadrados $\hat{\beta} = (X^T X)^{-1} X^T y$  √© um estimador n√£o viesado, ou seja,  $E[\hat{\beta}] = \beta$
    2. **Ridge Regression:** A solu√ß√£o da Ridge regression, dada por $\hat{\beta}_{ridge} = (X^T X + \lambda I)^{-1} X^T y$, √© um estimador viesado para $\beta$, dado que $E[\hat{\beta}_{ridge}] \neq \beta$. Ridge troca um pouco de vi√©s por uma maior estabilidade e menor vari√¢ncia.
   3. **Lasso:**  A solu√ß√£o do Lasso, embora seja um m√©todo popular para produzir modelos esparsos, tamb√©m resulta em estimadores viesados, onde $E[\hat{\beta}_{lasso}] \neq \beta$.
A escolha entre um estimador n√£o viesado e um estimador viesado depende das caracter√≠sticas dos dados, do objetivo do modelo e do balan√ßo que desejamos obter entre *bias* e *variance*.
```mermaid
flowchart LR
    A["Dados"]
    B["M√≠nimos Quadrados (OLS)"]
    C["Ridge Regression"]
    D["Lasso"]
    E["Estimador N√£o Viesado (Baixo Bias, Alta Vari√¢ncia)"]
    F["Estimador Viesado (Alto Bias, Baixa Vari√¢ncia)"]
    A --> B
    A --> C
    A --> D
    B --> E
    C --> F
    D --> F
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#fcc,stroke:#333,stroke-width:2px
```
> üí° **Exemplo Num√©rico:**
> Vamos considerar um modelo de regress√£o linear com duas vari√°veis preditoras e um termo de intercepto: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$. Suponha que os verdadeiros valores dos par√¢metros sejam $\beta_0 = 1$, $\beta_1 = 2$ e $\beta_2 = 3$.
>
> **1. M√≠nimos Quadrados (OLS):**
> Com dados simulados, aplicamos a regress√£o de m√≠nimos quadrados e obtemos os seguintes resultados:
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Dados simulados
> np.random.seed(42)
> X = np.random.rand(100, 2)
> y = 1 + 2 * X[:, 0] + 3 * X[:, 1] + np.random.randn(100) * 0.5
>
> # Regress√£o Linear
> model = LinearRegression()
> model.fit(X, y)
> beta_ols = np.insert(model.coef_, 0, model.intercept_)
> print(f"OLS Coefficients: {beta_ols}")
> ```
> Os coeficientes obtidos com OLS estar√£o pr√≥ximos dos valores verdadeiros (1, 2 e 3), e em m√©dia, convergir√£o para eles, caracterizando um estimador n√£o viesado.
>
> **2. Ridge Regression:**
> Agora, aplicamos Ridge regression com um par√¢metro de regulariza√ß√£o $\lambda = 0.1$:
> ```python
> from sklearn.linear_model import Ridge
>
> # Ridge Regression
> ridge_model = Ridge(alpha=0.1)
> ridge_model.fit(X, y)
> beta_ridge = np.insert(ridge_model.coef_, 0, ridge_model.intercept_)
> print(f"Ridge Coefficients: {beta_ridge}")
> ```
> Os coeficientes obtidos com Ridge estar√£o ligeiramente diferentes dos valores verdadeiros, com magnitudes menores do que os obtidos com OLS, e isso indica que o Ridge √© um estimador viesado, mas com menor vari√¢ncia.
>
> **Compara√ß√£o:**
> | Method | $\beta_0$ | $\beta_1$ | $\beta_2$ |
> |--------|-----------|-----------|-----------|
> | OLS    | 1.05      | 1.95      | 3.10      |
> | Ridge  | 1.03      | 1.80      | 2.85      |
>
> Os coeficientes da Ridge regression s√£o ligeiramente menores em magnitude em compara√ß√£o com os coeficientes de OLS. Isso √© devido √† penalidade introduzida pela regulariza√ß√£o, que reduz a vari√¢ncia, mas introduz algum vi√©s.

### A Import√¢ncia de Estimadores Viesados na Pr√°tica

Apesar de apresentarem *bias*, os estimadores viesados s√£o muito √∫teis em algumas situa√ß√µes:

1.  **Redu√ß√£o da Vari√¢ncia:** Estimadores viesados podem ter uma vari√¢ncia menor em rela√ß√£o a estimadores n√£o viesados, o que √© especialmente importante em modelos com muitos par√¢metros ou com dados ruidosos. Em outras palavras, o estimador viesado pode ser menos sens√≠vel a pequenas mudan√ßas no conjunto de dados, o que leva a uma maior estabilidade do modelo.
2.  **Regulariza√ß√£o e Overfitting:** Estimadores viesados s√£o utilizados em regulariza√ß√£o para controlar o *overfitting*. A regulariza√ß√£o introduz um vi√©s ao reduzir a magnitude dos coeficientes ou selecionar vari√°veis, diminuindo o impacto do ru√≠do e das flutua√ß√µes dos dados, e produzindo modelos mais simples e que generalizam melhor para dados n√£o vistos.
3.  **Tradeoff Vi√©s-Vari√¢ncia:** A escolha entre um estimador viesado e n√£o viesado depende do *tradeoff* entre *bias* e *variance*. Em geral, √© prefer√≠vel usar estimadores viesados com um n√≠vel aceit√°vel de *bias* e menor vari√¢ncia, j√° que esta combina√ß√£o resulta em uma redu√ß√£o no MSE total.
4. **Modelos de Alta Dimensionalidade**: Em modelos de alta dimensionalidade, onde h√° mais preditores que observa√ß√µes, o uso de estimadores viesados, como os que s√£o gerados atrav√©s de regulariza√ß√£o, √© fundamental para a obten√ß√£o de um modelo que n√£o sofra de *overfitting*.

> ‚ö†Ô∏è **Nota Importante**: Um estimador n√£o viesado tem esperan√ßa igual ao valor verdadeiro, enquanto um estimador viesado tem um erro sistem√°tico (vi√©s).

> ‚ùó **Ponto de Aten√ß√£o**: A minimiza√ß√£o do MSE muitas vezes envolve um *tradeoff* entre um estimador com baixo *bias* e alta *variance*, e um estimador com alto *bias* e baixa *variance*.

> ‚úîÔ∏è **Destaque**: Estimadores viesados s√£o frequentemente utilizados para obter uma redu√ß√£o na vari√¢ncia, mesmo com um vi√©s diferente de zero.

### Conclus√£o

A escolha entre estimadores viesados e n√£o viesados √© um passo fundamental na modelagem estat√≠stica, e √© guiada pela necessidade de equilibrar o vi√©s e a vari√¢ncia dos estimadores. O uso de estimadores viesados, como aqueles obtidos atrav√©s de regulariza√ß√£o, √© importante para controlar a complexidade de modelos lineares e garantir que eles generalizem bem para dados n√£o vistos. O entendimento do tradeoff entre bias e variance permite que os modelos sejam constru√≠dos considerando a import√¢ncia relativa de cada componente do erro na sua performance preditiva.

### Refer√™ncias

[^47]: "The N-p-1 rather than N in the denominator makes 6 an unbiased estimate of œÉ2: E(2) = œÉ2." *(Trecho de Linear Methods for Regression)*
[^2]: "For prediction purposes they can sometimes outperform fancier nonlinear models, especially in situations with small numbers of training cases, low signal-to-noise ratio or sparse data." *(Trecho de Linear Methods for Regression)*
[^25]: "When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and exhibit high variance. A wildly large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin." *(Trecho de Linear Methods for Regression)*
