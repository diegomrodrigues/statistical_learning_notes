## Ridge Regression: Shrinkage and Penalized Least Squares

### Introdução
Este capítulo aprofunda o conceito de **Ridge Regression**, um método crucial para lidar com a multicolinearidade e o *overfitting* em modelos de regressão linear [^19]. Diferentemente do método de mínimos quadrados, que pode levar a estimativas de coeficientes com alta variância, a Ridge Regression introduz um termo de penalidade que força os coeficientes a serem menores, resultando em um modelo mais estável e generalizável [^61]. Este capítulo explorará a formulação matemática da Ridge Regression, suas propriedades estatísticas e sua relação com outros métodos de regularização.

### Conceitos Fundamentais

A **Ridge Regression** é uma técnica de regularização que adiciona um termo de penalidade à soma dos quadrados dos resíduos (RSS). O objetivo é minimizar uma combinação do ajuste do modelo aos dados e do tamanho dos coeficientes [^61]. A formulação matemática da Ridge Regression é dada por:

$$\beta_{ridge} = \underset{\beta}{\operatorname{argmin}} \left\\{ \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\\}$$

onde:

*   $y_i$ é a *i*-ésima observação da variável resposta.
*   $x_{ij}$ é o valor da *j*-ésima variável preditora para a *i*-ésima observação.
*   $\beta_0$ é o *intercept*.
*   $\beta_j$ é o coeficiente da *j*-ésima variável preditora.
*   $\lambda \geq 0$ é o parâmetro de complexidade que controla a quantidade de *shrinkage* [^61].

O termo $\lambda \sum_{j=1}^{p} \beta_j^2$ é a **penalidade L2** (ou *Ridge penalty*), que penaliza a magnitude dos coeficientes. Quando $\lambda = 0$, a Ridge Regression se reduz à regressão linear de mínimos quadrados. À medida que $\lambda$ aumenta, a penalidade se torna mais forte, forçando os coeficientes a se aproximarem de zero [^61].

É importante notar que o *intercept* $\beta_0$ geralmente não é incluído no termo de penalidade [^64]. Isso ocorre porque a penalização do *intercept* tornaria o procedimento dependente da origem escolhida para a variável resposta $Y$. Para evitar isso, os dados são geralmente centrados antes de aplicar a Ridge Regression, subtraindo a média de cada variável preditora [^64].

Em notação matricial, o critério a ser minimizado pode ser escrito como:

$$RSS(\lambda) = (y - X\beta)^T(y - X\beta) + \lambda\beta^T\beta$$

onde:

*   $y$ é o vetor de observações da variável resposta.
*   $X$ é a matriz de variáveis preditoras.
*   $\beta$ é o vetor de coeficientes.

A solução para este problema de minimização é dada por:

$$\hat{\beta}^{ridge} = (X^TX + \lambda I)^{-1}X^Ty$$

onde $I$ é a matriz identidade $p \times p$ [^64]. A adição de $\lambda I$ à matriz $X^TX$ garante que a matriz seja invertível, mesmo que $X^TX$ seja singular, o que ocorre quando as variáveis preditoras são altamente correlacionadas (multicolinearidade) [^64].

**Interpretação Geométrica:**

A Ridge Regression pode ser interpretada geometricamente como a projeção ortogonal de $y$ no espaço coluna de $X$, com uma restrição adicional no tamanho dos coeficientes [^46]. A restrição $\sum_{j=1}^{p} \beta_j^2 \leq t$ define uma região circular no espaço dos coeficientes, e a solução da Ridge Regression é o ponto dentro desta região que minimiza a soma dos quadrados dos resíduos [^61].

**Singular Value Decomposition (SVD):**

A *Singular Value Decomposition* (SVD) da matriz de entrada centrada $X$ fornece *insights* adicionais sobre a natureza da Ridge Regression [^64]. A SVD de $X$ é dada por:

$$X = UDV^T$$

onde:

*   $U$ e $V$ são matrizes ortogonais ($N \times p$ e $p \times p$, respectivamente).
*   $D$ é uma matriz diagonal $p \times p$ com entradas não negativas $d_1 \geq d_2 \geq ... \geq d_p \geq 0$, chamadas de *singular values* de $X$ [^66].

Usando a SVD, a solução da Ridge Regression pode ser expressa como:

$$\hat{\beta}^{ridge} = V D (D^2 + \lambda I)^{-1} U^T y$$

Esta expressão mostra que a Ridge Regression calcula as coordenadas de $y$ em relação à base ortonormal $U$ e, em seguida, encolhe essas coordenadas pelos fatores $d_j^2 / (d_j^2 + \lambda)$. As direções com menores valores singulares são mais encolhidas, correspondendo às direções de menor variância nos dados [^66].

**Graus de Liberdade Efetivos:**

Os graus de liberdade efetivos da Ridge Regression são dados por:

$$df(\lambda) = tr(X(X^TX + \lambda I)^{-1}X^T) = \sum_{j=1}^{p} \frac{d_j^2}{d_j^2 + \lambda}$$

Esta é uma função monotonicamente decrescente de $\lambda$, variando de $p$ quando $\lambda = 0$ (nenhuma regularização) a 0 quando $\lambda \to \infty$ (regularização máxima) [^68].

### Conclusão
A Ridge Regression oferece uma abordagem eficaz para mitigar os problemas de multicolinearidade e *overfitting* em modelos de regressão linear. Ao introduzir um termo de penalidade que encolhe os coeficientes, a Ridge Regression pode melhorar a estabilidade e a generalização do modelo. A escolha apropriada do parâmetro de regularização $\lambda$ é crucial para obter o melhor desempenho preditivo. Métodos como a validação cruzada podem ser usados para selecionar um valor ideal para $\lambda$ [^61].

<!-- END -->