## Multiple Outcome Shrinkage and Selection: Simultaneous Modeling

### Introdução
Este capítulo aprofunda o estudo dos métodos de *shrinkage* e seleção, expandindo-os para o cenário de múltiplos resultados [^84]. Como vimos anteriormente [^56], a regressão linear multivariada trata de prever múltiplos outputs $Y_1, Y_2, ..., Y_K$ a partir de um conjunto de inputs $X_0, X_1, X_2, ..., X_p$. A extensão para *shrinkage* e seleção em modelos com múltiplos outputs permite modelar simultaneamente múltiplos resultados relacionados, oferecendo flexibilidade na aplicação de métodos de seleção e *shrinkage* individualmente para cada outcome ou simultaneamente para todos eles.

### Conceitos Fundamentais
Em modelos com múltiplos outputs, podemos aplicar métodos de seleção e *shrinkage* de duas maneiras principais [^84]:
1.  **Individualmente a cada outcome:** Nesta abordagem, aplicamos uma técnica univariada a cada coluna da matriz de outcomes $Y$. Por exemplo, poderíamos aplicar a fórmula de *ridge regression* (3.44) [^22] a cada uma das $K$ colunas da matriz $Y$, permitindo que cada outcome tenha um parâmetro $\lambda$ diferente. Isso resulta na estimativa de $k$ parâmetros de regularização distintos, $\lambda_1, ..., \lambda_k$. Esta abordagem permite que diferentes outcomes tenham diferentes graus de regularização.
2.  **Simultaneamente a todos os outcomes:** Alternativamente, podemos aplicar a regularização a todos os outcomes utilizando o mesmo valor de $\lambda$. Isso implica que todos os $k$ outputs são usados para estimar um único parâmetro de regularização $\lambda$.

A escolha entre essas abordagens depende da estrutura dos dados e dos objetivos da modelagem. Se os outcomes são altamente correlacionados, pode ser vantajoso usar uma abordagem simultânea para explorar essas correlações e melhorar a eficiência da estimativa [^84].

**Canonical Correlation Analysis (CCA)**
A Análise de Correlação Canônica (CCA) é uma técnica de redução de dimensionalidade que combina respostas, sendo uma ferramenta valiosa para o caso de múltiplos outputs [^84]. A CCA busca encontrar sequências de combinações lineares não correlacionadas $Xv_m$ dos inputs $X_j$ e combinações lineares não correlacionadas correspondentes $Yu_m$ das respostas $y_k$, de forma que as correlações entre elas sejam maximizadas. Matematicamente, o objetivo é maximizar:
$$Corr^2(Yu_m, Xv_m)$$
[^84]
A CCA encontra no máximo $M = min(K, p)$ direções [^84]. As variáveis de resposta canônicas iniciais são aquelas combinações lineares melhor preditas pelos $X_j$. Em contrapartida, as variáveis canônicas finais são mal preditas pelos $X_j$ e podem ser descartadas. A solução para CCA é computada por meio do Generalized Singular Value Decomposition (SVD) da matriz de covariância cruzada amostral $Y^TX/N$, assumindo que $Y$ e $X$ estejam centrados [^84].

**Reduced-Rank Regression**
A *Reduced-Rank Regression* formaliza essa abordagem em termos de um modelo de regressão que explicitamente combina informações [^84]. Dado uma matriz de covariância de erros $Cov(\epsilon) = \Sigma$, resolvemos o seguinte problema de regressão multivariada restrita:
$$argmin_B \sum_{i=1}^N (Y_i - B^T x_i)^T \Sigma^{-1} (Y_i - B^T x_i)$$
[^85]
com $rank(B) = m$.

Substituindo $\Sigma$ pela estimativa $Y^TY/N$, a solução é dada por uma CCA de $Y$ e $X$:
$$B^{(m)} = BU_mU_m^T$$
[^85]

Aqui, $U_m$ é a submatriz $K \times m$ de $U$, consistindo das primeiras $m$ colunas, e $U$ é a matriz $K \times M$ de vetores canônicos à esquerda $u_1, u_2, ..., u_M$. A solução pode ser escrita como:
$$B^{(M)} = (X^TX)^{-1}X^T(YU_m)U_m^T$$
[^85]

Isso significa que a *Reduced-Rank Regression* realiza uma regressão linear na matriz de resposta agrupada $YU_m$ e então mapeia os coeficientes (e, portanto, os ajustes) de volta para o espaço de resposta original [^85]. Os ajustes de *Reduced-Rank* são dados por:
$$\hat{Y}^{rr(m)} = X(X^TX)^{-1}X^TYU_mU_m^T = HYP_m$$
[^85]
onde $H$ é o operador de projeção de regressão linear usual, e $P_m$ é o operador de projeção de resposta CCA de rank-$m$ [^85].

**Curds and Whey (c+w) Shrinkage**

Breiman e Friedman (1997) exploraram com sucesso o *shrinkage* das variáveis canônicas entre $X$ e $Y$, uma versão suave da regressão de rank reduzido [^85]. A proposta tem a forma:
$$B^{c+w} = BU \Lambda U^{-1}$$
[^85]
onde $\Lambda$ é uma matriz diagonal de *shrinkage*, baseada na predição ótima no *population setting*, mostram que $\Lambda$ tem entradas diagonais
$$\Lambda_m = \frac{c_m}{c_m + (1-c_m)}, m = 1, ..., M,$$
onde $c_m$ é o $m$-ésimo coeficiente de correlação canônica [^85].

### Conclusão
Em resumo, a extensão dos métodos de *shrinkage* e seleção para múltiplos outcomes oferece uma abordagem flexível e poderosa para modelar dados complexos. A escolha entre aplicar métodos univariados individualmente ou abordagens multivariadas que exploram as correlações entre os outcomes depende dos objetivos específicos da modelagem e da estrutura dos dados. Técnicas como CCA e *Reduced-Rank Regression*, junto com as estratégias de *shrinkage*, fornecem um conjunto de ferramentas valiosas para lidar com a complexidade e a dimensionalidade encontradas em problemas de múltiplos outcomes [^84].

### Referências
[^56]: Seção 3.2.4, "Multiple Outputs", *The Elements of Statistical Learning*.
[^84]: Seção 3.7, "Multiple Outcome Shrinkage and Selection", *The Elements of Statistical Learning*.
[^22]: Seção 3.4.1, "Ridge Regression", *The Elements of Statistical Learning*.
[^85]: Pág 85, *The Elements of Statistical Learning*.
<!-- END -->