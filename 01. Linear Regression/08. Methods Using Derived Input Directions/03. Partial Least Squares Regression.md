## Partial Least Squares (PLS)

### Introdução
Este capítulo explora o método de **Partial Least Squares (PLS)** como uma técnica para construir modelos de regressão linear em situações onde os preditores são altamente correlacionados [^1]. PLS se destaca por otimizar as direções que possuem alta variância e alta correlação com a variável resposta, diferentemente da **Principal Components Regression (PCR)**, que se concentra apenas na alta variância [^1]. Em continuidade aos métodos que utilizam direções derivadas dos inputs, PLS oferece uma abordagem alternativa para lidar com multicolinearidade e predição.

### Conceitos Fundamentais
**Partial Least Squares (PLS)** é uma técnica de regressão que constrói um conjunto de combinações lineares dos inputs que estão relacionados à variável resposta [^1]. Essa característica torna PLS particularmente útil quando os preditores são altamente correlacionados e o objetivo principal é predizer a resposta [^1].

A diferença fundamental entre PLS e PCR reside em como as direções são construídas. Enquanto PCR foca apenas em encontrar direções de alta variância nos preditores, PLS busca direções que simultaneamente maximizem a variância dos preditores e sua correlação com a variável resposta *y* [^1]. Em outras palavras, PLS utiliza *y* (além de *X*) para a construção das combinações lineares dos inputs [^1].

No exemplo do câncer de próstata, a validação cruzada escolheu *M = 2* direções PLS na Figura 3.7, resultando no modelo apresentado na coluna mais à direita da Tabela 3.3 [^1].

Matematicamente, o algoritmo PLS pode ser resumido nos seguintes passos (conforme detalhado no Algoritmo 3.3 [^1]):
1. **Padronização:** Padronize cada preditor $x_j$ para ter média zero e variância um [^1].
2. **Iteração:** Para *m = 1, 2, ..., p*:\n    *   Calcule $z_m = \sum_{j=1}^{p} \phi_{mj} x_j^{(m-1)}$, onde $\phi_{mj} = (x_j^{(m-1)}, y)$ [^1]. Isso significa que os inputs são ponderados pela força de seu efeito univariado em *y* [^1].\n    *   Calcule $\theta_m = (z_m, y) / (z_m, z_m)$ [^1].\n    *   Calcule $\hat{y}^{(m)} = \hat{y}^{(m-1)} + \theta_m z_m$ [^1].\n    *   Ortogonalize cada $x_j^{(m-1)}$ com respeito a $z_m$: $x_j^{(m)} = x_j^{(m-1)} - [(z_m, x_j^{(m-1)}) / (z_m, z_m)] z_m$ [^1].\n3. **Output:** A sequência de vetores fitted $\\{\hat{y}^{(m)}\\}$ [^1].

É importante notar que os coeficientes lineares $\beta_{pls}^{(m)}$ podem ser recuperados da sequência de transformações PLS, uma vez que os $\\{z_l\\}$ são lineares nos inputs originais $x_j$ e $\hat{y}^{(m)} = X \beta_{pls}^{(m)}$ [^1].

PLS busca direções que possuem alta variância e alta correlação com a resposta [^1]. Especificamente, a *m*-ésima direção PLS $\delta_m$ resolve o seguinte problema de otimização [^1]:

$$\
\max_{\alpha} Corr^2(y, X\alpha) Var(X\alpha)\
$$
sujeito a $||\alpha|| = 1$, $\alpha^T S v_l = 0$, $l = 1, ..., m-1$, onde $S$ é a matriz de covariância amostral dos $x_j$ [^1].

### Conclusão
PLS oferece uma alternativa valiosa para a regressão em cenários com multicolinearidade. Ao contrário de PCR, que ignora a relação com a variável resposta na construção das componentes, PLS incorpora essa informação, buscando direções que são tanto de alta variância quanto altamente correlacionadas com *y* [^1]. Embora PLS possa se comportar de maneira semelhante à Ridge Regression e PCR, a capacidade de inflar algumas das direções de maior variância pode torná-lo um pouco instável [^1]. A validação cruzada é crucial para determinar o número ideal de direções PLS a serem incluídas no modelo, equilibrando bias e variância [^1].

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.

<!-- END -->