## Principal Component Regression (PCR)

### Introdução
Este capítulo explora métodos que utilizam **direções de entrada derivadas** no contexto de modelos de regressão linear. Especificamente, focaremos na **Principal Component Regression (PCR)**, uma técnica que combina **Principal Component Analysis (PCA)** com **regressão linear** [^79]. A PCR é particularmente útil em situações onde temos um grande número de entradas, frequentemente altamente correlacionadas, e busca-se reduzir a dimensionalidade dos dados mantendo a maior parte da informação relevante para a predição. Veremos como a PCR se relaciona com outros métodos de regularização, como a **ridge regression**, e discutiremos suas vantagens e desvantagens.

### Conceitos Fundamentais
A Principal Component Regression (PCR) é um método que, inicialmente, aplica a **Principal Component Analysis (PCA)** às variáveis de entrada para criar um conjunto de **componentes principais não correlacionados** [^79]. Em seguida, utiliza um subconjunto desses componentes como *preditores* em um modelo de regressão linear.

Formalmente, a PCR forma colunas de entrada derivadas $z_m = Xu_m$, baseadas nos componentes principais, e então regride $y$ nesses $z_m$ para algum $M < p$ [^79]. Aqui, $X$ representa a matriz de entrada, $u_m$ é o vetor correspondente ao $m$-ésimo componente principal, e $p$ é o número total de variáveis de entrada.

A regressão resultante pode ser expressa como:
$$\
\hat{y}^{(M)} = \bar{y} + \sum_{m=1}^{M} \hat{\theta}_m z_m,
$$
onde $\hat{\theta}_m = \frac{(z_m, y)}{(z_m, z_m)}$ [^80]. Como os $z_m$ são cada um combinações lineares das variáveis originais $x_j$, podemos expressar a solução (3.61) em termos de coeficientes de $x_j$ (Exercício 3.13):

$$\hat{\beta}^{pcr}(M) = \sum_{m=1}^{M} \hat{\theta}_m u_m \text{[^80]}$$

**Similaridade com Ridge Regression:**
*PCR é muito similar à ridge regression: ambas operam via os componentes principais da matriz de entrada; ridge regression encolhe os coeficientes dos componentes principais* [^79].

A PCR e a ridge regression compartilham a característica de operar através dos componentes principais da matriz de entrada [^79]. No entanto, elas diferem na maneira como tratam esses componentes. Enquanto a ridge regression *encolhe* os coeficientes dos componentes principais, a PCR *descarta* os componentes menos relevantes, utilizando apenas os $M$ componentes principais mais importantes. Ridge regression encolhe os coeficientes dos componentes principais (Figura 3.17), encolhendo mais dependendo do tamanho do autovalor correspondente [^80].

**Scaling das Entradas:**
Assim como na ridge regression, os componentes principais dependem do *scaling* das entradas. Portanto, é típico padronizar as entradas antes de aplicar a PCR [^80].

**Seleção de M:**
A escolha do número de componentes principais $M$ é crucial na PCR. Se $M = p$, recuperamos as estimativas de mínimos quadrados usuais, já que as colunas de $Z = UD$ abrangem o espaço das colunas de $X$ [^80]. Para $M < p$, obtemos uma regressão reduzida. Uma abordagem comum para selecionar $M$ é usar *validação cruzada* [^80].

**Interpretação via Singular Value Decomposition (SVD):**
A Singular Value Decomposition (SVD) da matriz de entrada centrada $X$ fornece insights adicionais sobre a natureza da PCR [^80]. A SVD de $X$ tem a forma $X = UDV^T$, onde $U$ e $V$ são matrizes ortogonais e $D$ é uma matriz diagonal contendo os valores singulares de $X$. Os vetores $v_j$ (colunas de $V$) são chamados de **componentes principais** (ou direções de Karhunen-Loeve) de $X$ [^80]. A primeira direção do componente principal $v_1$ tem a propriedade de que $z_1 = Xv_1$ tem a maior variância amostral entre todas as combinações lineares normalizadas das colunas de $X$ [^80]. Esta variância amostral pode ser vista como:
$$Var(z_1) = Var(Xv_1) = \frac{d_1^2}{N} \text{[^80]}$$

Os valores singulares pequenos $d_j$ correspondem a direções no espaço da coluna de $X$ que têm pequena variância, e a regressão de crista encolhe mais essas direções [^80].

**Algoritmo:**
A PCR pode ser implementada através do seguinte algoritmo:
1. Aplicar PCA à matriz de entrada $X$ para obter os componentes principais $z_m$.
2. Selecionar o número de componentes principais $M$ a serem utilizados na regressão.
3. Regredir a variável resposta $y$ nos $M$ componentes principais selecionados.

### Conclusão
A Principal Component Regression (PCR) oferece uma abordagem eficaz para lidar com problemas de regressão que envolvem um grande número de preditores correlacionados [^79]. Ao combinar a PCA com a regressão linear, a PCR reduz a dimensionalidade dos dados, mantendo a maior parte da informação relevante para a predição. Embora seja semelhante à ridge regression, a PCR difere na maneira como trata os componentes principais, *descartando* os menos relevantes em vez de *encolher* seus coeficientes [^79]. A escolha do número de componentes principais $M$ é crucial e pode ser feita através de técnicas como a validação cruzada [^80]. Em resumo, a PCR é uma ferramenta valiosa no arsenal de técnicas de regressão, especialmente em cenários de alta dimensionalidade e multicolinearidade.

### Referências
[^79]: Trecho do contexto fornecido sobre Principal Component Regression (PCR).
[^80]: Trechos do contexto fornecido sobre componentes principais, scaling das entradas e seleção de M na PCR.
<!-- END -->