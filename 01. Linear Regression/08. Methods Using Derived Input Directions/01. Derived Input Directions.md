## Métodos Utilizando Direções de Entrada Derivadas

### Introdução
Em situações com um grande número de entradas, frequentemente altamente correlacionadas, torna-se crucial reduzir a dimensionalidade do espaço de entrada para construir modelos de regressão mais eficientes e interpretáveis. Os métodos que utilizam **direções de entrada derivadas** [^79] abordam esse problema criando um conjunto menor de combinações lineares das entradas originais. Estas combinações lineares, ou **direções derivadas**, são então usadas no lugar das entradas originais para construir o modelo de regressão. Este capítulo explora os métodos que se enquadram nessa categoria, focando em como as combinações lineares são construídas e como elas impactam o desempenho do modelo.

### Conceitos Fundamentais

**Direções de Entrada Derivadas** [^79]: A ideia central desses métodos é transformar o espaço de entrada original em um novo espaço de menor dimensão, onde as novas variáveis (as direções derivadas) são combinações lineares das variáveis originais. Essa transformação visa capturar a maior parte da informação relevante contida nas variáveis originais, eliminando redundâncias e ruídos.

Existem diversas maneiras de construir essas combinações lineares, dando origem a diferentes métodos. Dois dos mais comuns são:

1.  **Regressão por Componentes Principais (PCR - *Principal Components Regression*)** [^80]:
    *   Utiliza as **componentes principais** [^80] como as direções derivadas.
    *   As componentes principais são obtidas através da **decomposição em valores singulares (SVD - *Singular Value Decomposition*)** [^80] da matriz de entrada $X$.
    *   As componentes principais são combinações lineares das variáveis originais que capturam a maior parte da variância dos dados.
    *   A regressão é então realizada sobre um subconjunto das primeiras *M* componentes principais.
    *   $$ \hat{y}_{(M)} = \bar{y} + \sum_{m=1}^{M} \hat{\theta}_m z_m, $$ onde $z_m$ são as componentes principais e $\hat{\theta}_m$ são os coeficientes de regressão [^81].
    *   A escolha de *M* afeta o desempenho do modelo. Um valor muito pequeno pode levar a um modelo subajustado (*underfitting*), enquanto um valor muito grande pode levar a um modelo sobreajustado (*overfitting*).
    *   Como as componentes principais dependem do *scaling* das entradas, é comum **padronizar** [^80] as variáveis antes de aplicar a PCR.

2.  **Mínimos Quadrados Parciais (PLS - *Partial Least Squares*)** [^81]:
    *   Similar à PCR, mas utiliza a variável resposta *y* [^81] no processo de construção das direções derivadas.
    *   As direções derivadas são construídas de forma a maximizar a **covariância** [^81] entre as combinações lineares das entradas e a variável resposta.
    *   $$ \max_a \text{Corr}^2(y, Xa) \text{Var}(Xa) \text{ sujeito a } ||a|| = 1, a^T S v_l = 0, l = 1, ..., m-1, $$ onde $S$ é a matriz de covariância amostral das $x_j$ [^81].
    *   Algoritmo iterativo que constrói as direções derivadas sequencialmente [^81].
    *   As direções derivadas são ortogonais entre si.
    *   A regressão é então realizada sobre um subconjunto das direções derivadas.
    *   A escolha do número de direções derivadas afeta o desempenho do modelo.

**Comparação entre PCR e PLS** [^82]:

*   Ambos os métodos reduzem a dimensionalidade do espaço de entrada através da construção de direções derivadas.
*   PCR utiliza apenas as entradas *X* para construir as direções derivadas, enquanto PLS utiliza tanto *X* quanto *y*.
*   PLS tende a ser mais eficiente em situações onde a relação entre as entradas e a resposta é complexa, uma vez que a informação sobre a resposta é utilizada na construção das direções derivadas.
*   Em situações onde a relação entre as entradas e a resposta é linear e as entradas são altamente correlacionadas, PCR pode ser uma escolha mais adequada.
*   Em geral, PLS tende a se comportar de forma semelhante à regressão de *ridge* [^82].

**Vantagens dos Métodos Utilizando Direções de Entrada Derivadas:**

*   **Redução da dimensionalidade:** Diminui o número de variáveis no modelo, o que pode levar a modelos mais simples e fáceis de interpretar.
*   **Lidar com multicolinearidade:** As direções derivadas são construídas de forma a serem não correlacionadas, o que mitiga o problema da multicolinearidade.
*   **Melhora na precisão da previsão:** Em algumas situações, a redução da dimensionalidade e a eliminação de ruídos podem levar a modelos com melhor desempenho preditivo.

**Desvantagens dos Métodos Utilizando Direções de Entrada Derivadas:**

*   **Perda de interpretabilidade:** As direções derivadas são combinações lineares das variáveis originais, o que pode dificultar a interpretação dos coeficientes do modelo.
*   **Escolha do número de direções derivadas:** A escolha do número de direções derivadas a serem utilizadas no modelo é um problema crucial que pode afetar significativamente o desempenho do modelo.
*   **Dependência do *scaling* das entradas:** A maioria dos métodos utilizando direções de entrada derivadas é sensível ao *scaling* das entradas, o que requer a padronização das variáveis antes da aplicação do método.

### Conclusão

Os métodos utilizando direções de entrada derivadas representam uma abordagem eficaz para lidar com problemas de regressão em situações com um grande número de entradas correlacionadas. Ao reduzir a dimensionalidade do espaço de entrada e mitigar o problema da multicolinearidade, esses métodos podem levar a modelos mais simples, interpretáveis e com melhor desempenho preditivo. A escolha do método mais adequado (PCR, PLS, etc.) e a seleção do número ideal de direções derivadas dependem das características específicas do problema em questão. A análise das componentes principais e a validação cruzada são ferramentas úteis para guiar essas escolhas.

### Referências
[^79]: "Methods using derived input directions reduce the dimensionality of the input space by creating a smaller set of linear combinations of the original inputs, which are then used in place of the original predictors to build a regression model."
[^80]: Seção 3.5.1
[^81]: Seção 3.5.2
[^82]: Seção 3.6
<!-- END -->