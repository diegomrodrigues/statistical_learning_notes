## An√°lise Comparativa de M√©todos de Regress√£o com Dois Predictores Correlacionados: Perfis de Coeficientes, Shrinkage e Sele√ß√£o

```mermaid
graph LR
    subgraph "Regression Methods Comparison"
    direction TB
        A["Input: Correlated Predictors (X1, X2)"]
        B["Linear Regression"]
        C["Ridge Regression"]
        D["Lasso Regression"]
        E["PCR"]
        F["PLS"]
        G["Best Subset"]
        H["Output: Predicted Y"]
        A --> B & C & D & E & F & G
        B & C & D & E & F & G --> H
    end
```

### Introdu√ß√£o
Neste cap√≠tulo, exploramos em profundidade as nuances de diversos modelos lineares para regress√£o, com um foco especial na an√°lise comparativa de seus comportamentos quando aplicados a cen√°rios com dois preditores correlacionados [^3.1]. Compreender as diferen√ßas entre esses m√©todos √© crucial para tomar decis√µes informadas no desenvolvimento de modelos estat√≠sticos e de *machine learning* robustos e interpret√°veis. Em particular, vamos analisar os perfis de coeficientes, o comportamento de shrinkage e sele√ß√£o de vari√°veis de cada um dos m√©todos.

### Conceitos Fundamentais
Para uma compreens√£o clara das abordagens comparativas, √© essencial revisarmos alguns conceitos fundamentais [^3.1], [^3.2].

**Conceito 1: Problema de Classifica√ß√£o e Modelos Lineares**
O problema de classifica√ß√£o envolve a atribui√ß√£o de uma classe a um dado conjunto de entradas. Modelos lineares s√£o amplamente utilizados por sua simplicidade e interpretabilidade, assumindo que a fun√ß√£o de regress√£o $E(Y|X)$ √© linear nas entradas X [^3.1]. Em situa√ß√µes com poucos dados de treinamento, modelos lineares podem superar modelos n√£o-lineares, ressaltando sua import√¢ncia mesmo na era da computa√ß√£o moderna [^3.1].

> üí° **Exemplo Num√©rico:** Suponha que queremos classificar emails como "spam" ou "n√£o spam" com base em duas vari√°veis: n√∫mero de palavras comumente encontradas em spam ($X_1$) e n√∫mero de links ($X_2$). Um modelo linear poderia assumir que a probabilidade de ser spam √© linearmente relacionada com $X_1$ e $X_2$. Se temos poucos emails para treinar, modelos lineares podem generalizar melhor do que modelos n√£o-lineares.

**Lemma 1:** A regress√£o linear, quando aplicada a uma matriz de indicadores (dummy coding), pode ser vista como uma forma de discrimina√ß√£o linear, onde a previs√£o de uma vari√°vel resposta indicadora de classe corresponde a uma decis√£o de classe. Esta equival√™ncia √© observada quando a fun√ß√£o discriminante linear (LDA) √© derivada sob certas suposi√ß√µes de classes gaussianas com covari√¢ncias iguais [^4.2], [^4.3].

**Conceito 2: Linear Discriminant Analysis (LDA)**
A **Linear Discriminant Analysis (LDA)** √© um m√©todo cl√°ssico de classifica√ß√£o linear que assume que os dados de cada classe s√£o origin√°rios de uma distribui√ß√£o Gaussiana com a mesma matriz de covari√¢ncia [^4.3]. A **fronteira de decis√£o** √© obtida projetando os dados em um subespa√ßo que maximiza a separa√ß√£o entre as classes [^4.3.1]. A LDA √© um m√©todo fundamental, mas possui limita√ß√µes em casos onde essas suposi√ß√µes n√£o se sustentam [^4.3.3].

```mermaid
graph LR
    subgraph "LDA Conceptual Model"
        direction TB
        A["Input Data X"]
        B["Assume Gaussian Distributions per Class"]
        C["Equal Covariance Matrices Œ£"]
        D["Project Data onto Subspace"]
        E["Maximize Class Separation"]
        F["Output: Decision Boundary"]
        A --> B
        B --> C
        C --> D
        D --> E
        E --> F
    end
```

> üí° **Exemplo Num√©rico:** Imagine classificar flores em duas esp√©cies, A e B, com base em duas medidas: comprimento da s√©pala ($X_1$) e largura da p√©tala ($X_2$). LDA assume que as medidas de cada esp√©cie seguem uma distribui√ß√£o gaussiana com a mesma vari√¢ncia para ambas as esp√©cies, tentando encontrar um eixo que maximiza a separa√ß√£o entre os grupos.

**Corol√°rio 1:**  A fun√ß√£o discriminante da LDA pode ser expressa como uma proje√ß√£o linear dos dados originais em um subespa√ßo de dimens√£o reduzida. Essa proje√ß√£o √© definida por autovetores da matriz de covari√¢ncia entre classes, maximizando a separabilidade [^4.3.1].

**Conceito 3: Logistic Regression**
A **Logistic Regression** modela a probabilidade de pertencimento a uma classe atrav√©s da fun√ß√£o *logit* (log-odds) [^4.4]. O modelo estima par√¢metros maximizando a verossimilhan√ßa dos dados observados, o que a torna uma abordagem robusta para classifica√ß√£o [^4.4.1], [^4.4.2]. Ao contr√°rio da LDA, a regress√£o log√≠stica n√£o assume normalidade nas distribui√ß√µes das vari√°veis preditoras [^4.4.4].

> üí° **Exemplo Num√©rico:** No mesmo exemplo das flores, a regress√£o log√≠stica modelaria a probabilidade de uma flor pertencer √† esp√©cie A, usando a fun√ß√£o sigm√≥ide para garantir que a probabilidade esteja entre 0 e 1, sem fazer a suposi√ß√£o de normalidade. A fun√ß√£o log√≠stica √© dada por:
> $$P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2)}}$$
> onde Y=1 representa a classe A, $X_1$ √© o comprimento da s√©pala, $X_2$ a largura da p√©tala e $\beta_0, \beta_1, \beta_2$ os coeficientes a serem estimados via m√°xima verossimilhan√ßa.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o
```mermaid
graph LR
    subgraph "Linear Regression for Classification"
        direction TB
        A["Classes as Indicator Variables"]
        B["Apply Linear Regression"]
        C["Estimate Coefficients via Least Squares"]
        D["Decision Rule based on Predicted Values"]
        A --> B
        B --> C
        C --> D
        subgraph "Limitations"
          E["May Produce Predictions outside [0, 1]"]
          D-->E
        end
    end
```

A regress√£o linear pode ser utilizada para fins de classifica√ß√£o, codificando as classes como vari√°veis indicadoras, estimando coeficientes via m√≠nimos quadrados e aplicando uma regra de decis√£o para classificar os dados. Este m√©todo √© simples, mas pode apresentar limita√ß√µes em casos onde a rela√ß√£o entre as vari√°veis preditoras e a classe n√£o √© estritamente linear [^4.2].

**Lemma 2:** Em determinadas condi√ß√µes, as fronteiras de decis√£o obtidas atrav√©s da regress√£o linear de vari√°veis indicadoras podem ser equivalentes √†s fronteiras geradas por discriminantes lineares, especialmente quando as classes podem ser razoavelmente separadas por hiperplanos. Essa equival√™ncia √© not√°vel em cen√°rios onde as vari√°veis s√£o bem comportadas e a vari√¢ncia residual √© baixa [^4.2].

> üí° **Exemplo Num√©rico:** Consideremos duas classes, A e B, onde A √© codificada como 1 e B como 0. Se a regress√£o linear prediz um valor acima de 0.5, classificamos como A, caso contr√°rio como B. Se as classes estiverem bem separadas e os preditores tiverem baixa variabilidade, essa abordagem pode produzir resultados semelhantes √† LDA.

**Corol√°rio 2:** Esta equival√™ncia permite utilizar m√©todos de regress√£o linear, muitas vezes mais computacionalmente eficientes, para gerar solu√ß√µes semelhantes √†quelas encontradas por abordagens discriminantes, desde que as condi√ß√µes de linearidade e separabilidade sejam minimamente atendidas [^4.2].

*Limita√ß√µes e Compara√ß√µes*: A regress√£o linear aplicada a vari√°veis indicadoras pode produzir previs√µes fora do intervalo [0,1], o que n√£o √© ideal para classificar probabilidades. Nestes casos, a regress√£o log√≠stica pode ser mais adequada, pois o *logit* garante que as probabilidades fiquem entre 0 e 1 [^4.4].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Regularization Techniques"
      direction TB
        A["L1 Regularization (Lasso)"]
        B["L2 Regularization (Ridge)"]
        C["Elastic Net (L1+L2)"]
        D["Promotes Sparsity"]
        E["Reduces Coefficient Magnitude"]
        F["Balances Sparsity & Stability"]
        A --> D
        B --> E
        C --> F
        subgraph "Goals"
            G["Overfitting Reduction"]
            H["Improve Interpretability"]
            D & E & F --> G & H
        end
    end
```

A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o t√©cnicas essenciais para lidar com *overfitting* e melhorar a interpretabilidade dos modelos de classifica√ß√£o. A regulariza√ß√£o L1 (lasso) promove esparsidade nos coeficientes, selecionando automaticamente as vari√°veis mais relevantes [^4.4.4]. A regulariza√ß√£o L2 (ridge) diminui a magnitude dos coeficientes, melhorando a estabilidade do modelo [^4.5]. A combina√ß√£o das duas, *Elastic Net*, equilibra esparsidade e estabilidade, oferecendo uma abordagem mais vers√°til para o controle do vi√©s e vari√¢ncia [^4.5.1].

> üí° **Exemplo Num√©rico:** Suponha que temos um problema de classifica√ß√£o com duas vari√°veis preditoras, $X_1$ e $X_2$, e a resposta $Y$. Em um modelo de regress√£o log√≠stica regularizado com Lasso (L1), a fun√ß√£o de custo que minimizamos √©:
> $$ -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1-y_i) \log(1-p_i)] + \lambda (|\beta_1| + |\beta_2|)$$
> onde $p_i$ √© a probabilidade prevista para a i-√©sima observa√ß√£o e $\lambda$ √© o par√¢metro de regulariza√ß√£o. Se $\lambda$ for suficientemente alto, o modelo pode for√ßar um dos coeficientes (por exemplo, $\beta_2$) para zero, efetivamente selecionando $X_1$ como o preditor mais importante.

**Lemma 3:** A penaliza√ß√£o L1 na regress√£o log√≠stica conduz a coeficientes esparsos, pois a otimiza√ß√£o da fun√ß√£o de custo tende a zerar os coeficientes menos importantes, direcionando o modelo para uma solu√ß√£o mais simples e interpret√°vel [^4.4.4].

**Prova do Lemma 3:** A prova deste lemma envolve o exame da fun√ß√£o de custo penalizada, demonstrando como a componente n√£o-diferenci√°vel da penaliza√ß√£o L1 for√ßa alguns coeficientes a zero na solu√ß√£o √≥tima [^4.4.3]. $\blacksquare$

**Corol√°rio 3:** Como consequ√™ncia do lemma anterior, a utiliza√ß√£o da penaliza√ß√£o L1 em modelos classificat√≥rios resulta em modelos mais interpret√°veis, facilitando a identifica√ß√£o das vari√°veis preditoras mais relevantes para a decis√£o de classe [^4.4.5].

*Nota Crucial*: A regulariza√ß√£o *Elastic Net*, como um h√≠brido de L1 e L2, pode ser √∫til para lidar com conjuntos de dados onde h√° alta correla√ß√£o entre os preditores, pois ela combina a capacidade de selecionar vari√°veis da L1 com a estabilidade da L2 [^4.5].

### Separating Hyperplanes e Perceptrons
O conceito de hiperplanos separadores surge da ideia de maximizar a margem entre as classes [^4.5.2]. A busca pelo hiperplano √≥timo leva a um problema de otimiza√ß√£o onde os vetores de suporte (pontos mais pr√≥ximos da fronteira de decis√£o) definem a solu√ß√£o. O Perceptron de Rosenblatt, por sua vez, √© um algoritmo iterativo capaz de encontrar hiperplanos separadores para dados linearmente separ√°veis [^4.5.1].

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?
**Resposta:** Sob a hip√≥tese de que as classes s√£o gaussianas com a mesma matriz de covari√¢ncia, a LDA e a Regra de Decis√£o Bayesiana levam √† mesma fronteira de decis√£o. No entanto, a LDA √© um m√©todo de estima√ß√£o que n√£o assume conhecimento pr√©vio sobre as distribui√ß√µes de probabilidade das classes, enquanto a decis√£o Bayesiana √© um m√©todo de decis√£o √≥timo, dada a probabilidade de pertencimento √† classe [^4.3].

```mermaid
graph LR
    subgraph "LDA vs. Bayesian Decision Rule"
      direction TB
        A["Assumes Gaussian Classes"]
        B["Equal Covariance Matrices"]
        C["LDA: Estimation Method"]
        D["Bayesian: Optimal Decision"]
        E["LDA: No prior on class probabilities"]
        F["Bayesian: Uses class prior probabilities"]
        A-->B
        B-->C
        B-->D
        C-->E
        D-->F
       subgraph "Equivalent Decision Boundary"
         G["Same decision boundary"]
         C&D-->G
       end
    end
```

**Lemma 4:** Formalmente, pode-se demonstrar a equival√™ncia da fronteira de decis√£o obtida pela LDA e pela Regra de Decis√£o Bayesiana ao assumir distribui√ß√µes gaussianas com covari√¢ncias iguais para cada classe. Esta demonstra√ß√£o envolve derivar analiticamente as fun√ß√µes discriminantes de ambos os m√©todos e mostrar sua equival√™ncia [^4.3], [^4.3.3].

> üí° **Exemplo Num√©rico:**  Se cada classe seguir uma distribui√ß√£o Gaussiana com m√©dia $\mu_k$ e a mesma matriz de covari√¢ncia $\Sigma$, a regra de decis√£o Bayesiana para classificar um novo ponto *x* atribui *x* √† classe *k* que maximiza a probabilidade condicional $P(x|k)P(k)$. Se $P(x|k)$ √© uma Gaussiana, a regra de decis√£o torna-se linear, similar √† LDA. A decis√£o Bayesiana √© expressa como:
> $$ \text{argmax}_k \left\{ -\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) + \ln P(k) \right\} $$
> Quando as covari√¢ncias s√£o iguais, esta express√£o simplifica-se para uma fun√ß√£o linear em *x*, comprovando a equival√™ncia com a LDA.

**Corol√°rio 4:** Ao relaxarmos a suposi√ß√£o de covari√¢ncias iguais, a fronteira de decis√£o obtida pela Regra de Decis√£o Bayesiana n√£o √© mais linear, mas sim quadr√°tica (QDA), demonstrando a sensibilidade das decis√µes de classifica√ß√£o a estas suposi√ß√µes [^4.3].

*Ponto Crucial*: A escolha entre assumir ou n√£o covari√¢ncias iguais tem implica√ß√µes diretas na forma da fronteira de decis√£o, com a LDA levando a decis√µes lineares e a QDA a decis√µes quadr√°ticas [^4.3.1].

### An√°lise Comparativa dos M√©todos: Perfis de Coeficientes
A Figura 3.18, conforme citada no contexto, demonstra o comportamento dos coeficientes quando variamos os par√¢metros de ajuste (tuning parameters) em m√©todos como Ridge, Lasso, PCR, PLS e Best Subset. A an√°lise comparativa atrav√©s dos perfis de coeficientes √© fundamental para a compreens√£o das diferen√ßas de comportamento entre esses m√©todos [^3.6].

#### Ridge Regression
A Ridge Regression aplica um shrinkage cont√≠nuo nos coeficientes, fazendo com que eles se aproximem uns dos outros e de zero √† medida que o par√¢metro de regulariza√ß√£o aumenta. Este m√©todo tende a "espalhar" a influ√™ncia dos preditores, sem eliminar nenhum completamente [^3.4.1].
```mermaid
graph LR
    subgraph "Ridge Regression"
      direction TB
        A["Input: Data X, Y"]
        B["Cost Function:  RSS + Œª||Œ≤||¬≤"]
        C["Continuous Shrinkage of Coefficients"]
        D["Coefficients Approach Zero"]
        E["No Variable Elimination"]
        A --> B
        B --> C
        C --> D
        C --> E
    end
```

> üí° **Exemplo Num√©rico:** Considere um problema de regress√£o linear com dois preditores correlacionados, $X_1$ e $X_2$, e uma resposta $Y$. A equa√ß√£o do modelo √©:
> $$Y = \beta_0 + \beta_1X_1 + \beta_2X_2$$
> Na Ridge regression, a fun√ß√£o de custo a minimizar √©:
> $$ \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} - \beta_2x_{i2})^2 + \lambda(\beta_1^2 + \beta_2^2) $$
> Se $\lambda = 0$, temos a regress√£o linear comum. Se $\lambda$ aumenta, os coeficientes $\beta_1$ e $\beta_2$ se aproximam de 0 de forma suave, mas geralmente ambos s√£o diferentes de zero.  Por exemplo, com $\lambda=0.1$, poder√≠amos obter $\beta_1 = 0.6$ e $\beta_2=0.7$, e com $\lambda=1$ obter√≠amos $\beta_1 = 0.3$ e $\beta_2=0.35$, mostrando a redu√ß√£o progressiva.

#### Lasso
O Lasso, em contraste, promove um shrinkage mais agressivo, capaz de zerar coeficientes de preditores menos relevantes, gerando modelos mais esparsos [^3.4.2]. A natureza n√£o linear da penalidade L1 permite sele√ß√µes mais seletivas, eliminando alguns preditores do modelo.

```mermaid
graph LR
    subgraph "Lasso Regression"
      direction TB
        A["Input: Data X, Y"]
        B["Cost Function: RSS + Œª||Œ≤||‚ÇÅ"]
        C["Aggressive Shrinkage of Coefficients"]
        D["Coefficients set to Zero"]
        E["Variable Selection"]
        A --> B
        B --> C
        C --> D
        C --> E
    end
```

> üí° **Exemplo Num√©rico:**  No mesmo problema de regress√£o, se usarmos Lasso, a fun√ß√£o de custo √©:
> $$ \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} - \beta_2x_{i2})^2 + \lambda(|\beta_1| + |\beta_2|) $$
> Aqui, √† medida que $\lambda$ aumenta, um dos coeficientes pode ir para zero. Por exemplo, com $\lambda=0.1$, poder√≠amos obter $\beta_1 = 0.6$ e $\beta_2=0.7$, mas com $\lambda=1$ poder√≠amos ter $\beta_1 = 0.5$ e $\beta_2 = 0$, indicando que o preditor $X_2$ foi efetivamente removido do modelo.

#### Principal Components Regression (PCR) e Partial Least Squares (PLS)
PCR e PLS utilizam abordagens de redu√ß√£o de dimens√£o, onde os dados s√£o projetados em um subespa√ßo de menor dimens√£o atrav√©s de componentes principais ou vari√°veis latentes. A figura 3.17 demonstra como o PCR trunca os coeficientes de componentes menos relevantes, enquanto o PLS faz um balanceamento entre as covari√¢ncias dos dados e da resposta [^3.5.1], [^3.5.2].

```mermaid
graph LR
    subgraph "PCR and PLS"
        direction TB
        A["Input: Data X, Y"]
        B["PCA (for PCR) or Latent Variables (for PLS)"]
        C["Dimensionality Reduction"]
        D["Truncates Coefficients (PCR)"]
        E["Balances Covariance (PLS)"]
        A --> B
        B --> C
        C-->D
        C-->E

    end
```

> üí° **Exemplo Num√©rico:** Em PCR, primeiro realizamos a an√°lise de componentes principais nos preditores $X_1$ e $X_2$. Digamos que o primeiro componente principal $Z_1$ capture a maior parte da vari√¢ncia de $X_1$ e $X_2$. Usamos ent√£o $Z_1$ para ajustar a regress√£o linear com $Y$. Se o segundo componente principal $Z_2$ n√£o tiver rela√ß√£o com $Y$, o modelo PCR ignora este componente, reduzindo a dimensionalidade e potencialmente melhorando a generaliza√ß√£o.
> ```python
> import numpy as np
> from sklearn.decomposition import PCA
> from sklearn.linear_model import LinearRegression
>
> # Simula√ß√£o de dados
> X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])
> y = np.array([3, 3.5, 13, 16, 2, 20])
>
> # PCR
> pca = PCA(n_components=1)
> X_reduced = pca.fit_transform(X)
> model_pcr = LinearRegression().fit(X_reduced, y)
> print("Coeficiente PCR:", model_pcr.coef_)
>
> #Regress√£o sem PCR
> model_reg = LinearRegression().fit(X, y)
> print("Coeficiente Regress√£o:", model_reg.coef_)
> ```
> No PLS, o foco √© na covari√¢ncia entre os preditores e a vari√°vel resposta, o que pode gerar uma representa√ß√£o mais eficiente para o modelo de regress√£o.

#### Best Subset
Best Subset √© um m√©todo de sele√ß√£o de vari√°veis que avalia todas as combina√ß√µes poss√≠veis de preditores e seleciona a que gera o menor erro, resultando em um comportamento descont√≠nuo (discreto) dos coeficientes [^3.3.1]. Esse m√©todo costuma "overshoot" e depois retroceder para a solu√ß√£o ideal.

```mermaid
graph LR
    subgraph "Best Subset Selection"
       direction TB
        A["Input: Data X, Y"]
        B["Evaluate all combinations of Predictors"]
        C["Select Subset with minimum error"]
        D["Discontinuous Coefficient Behavior"]
       A --> B
       B --> C
       C --> D
    end
```

> üí° **Exemplo Num√©rico:** Se tivermos dois preditores $X_1$ e $X_2$, o m√©todo Best Subset avaliaria modelos com apenas $X_1$, apenas $X_2$ e com ambos. Ele selecionaria o modelo que minimiza o erro. N√£o haver√° um caminho gradual de coeficientes como no Ridge ou Lasso. Se com um subconjunto espec√≠fico ($X_1$) obtivermos um modelo com menor erro, este ser√° selecionado, e a inclus√£o de $X_2$ ser√° avaliada de forma discreta.

**Observa√ß√µes Importantes:**
* Quando h√° alta correla√ß√£o entre os preditores, como ilustrado na figura 3.18 (œÅ=0.5), o Ridge tende a diminuir a magnitude dos coeficientes de forma conjunta, enquanto o Lasso pode selecionar um e eliminar outro, resultando em modelos com mais esparsidade.
* Em situa√ß√µes de correla√ß√£o negativa (œÅ=-0.5), os m√©todos tendem a se comportar de maneira mais similar, devido a menor ambiguidade na rela√ß√£o entre os preditores.
* Os m√©todos de redu√ß√£o de dimens√£o (PCR e PLS) se assemelham ao Ridge no sentido de que ambos tentam reduzir o impacto de varia√ß√µes indesejadas, mas o fazem de forma diferente.

### Conclus√£o
Neste cap√≠tulo, analisamos em detalhe o comportamento de diversos modelos lineares para regress√£o, especialmente em cen√°rios com dois preditores correlacionados. Os perfis de coeficientes, o *shrinkage* e a sele√ß√£o de vari√°veis foram pontos de foco na an√°lise comparativa. Cada um dos m√©todos demonstra comportamentos √∫nicos e com vantagens e desvantagens espec√≠ficas, dependendo da natureza dos dados e objetivos do problema. A escolha entre esses modelos deve ser feita com base em uma compreens√£o profunda de seus fundamentos te√≥ricos e das caracter√≠sticas do problema a ser resolvido. A figura 3.18 evidencia visualmente o comportamento de cada m√©todo, facilitando a compreens√£o da influ√™ncia da correla√ß√£o entre preditores. A an√°lise te√≥rica e emp√≠rica nos mostra como as decis√µes na constru√ß√£o do modelo podem impactar a qualidade das previs√µes e a interpretabilidade dos resultados.

### Footnotes
[^3.1]: "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,..., Xp. Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them." *(Trecho de Linear Methods for Regression)*
[^3.2]: "As introduced in Chapter 2, we have an input vector XT = (X1, X2, ..., Xp), and want to predict a real-valued output Y. The linear regression model has the form f(x) = Œ≤Œø + Œ£XjŒ≤j. " *(Trecho de Linear Methods for Regression)*
[^4.2]: "In this chapter we describe linear methods for regression, while in the next chapter we discuss linear methods for classification." *(Trecho de Linear Methods for Regression)*
[^4.3]: "Linear Discriminant Analysis (LDA) is a classical approach for classification that assumes Gaussian distributions for each class with equal covariance matrices." *(Trecho de Linear Methods for Classification)*
[^4.3.1]: "LDA finds a separating hyperplane by projecting the data onto a subspace that maximizes the separation between classes." *(Trecho de Linear Methods for Classification)*
[^4.3.3]: "LDA makes assumptions about the data distribution, which can be limiting in cases where these assumptions are violated." *(Trecho de Linear Methods for Classification)*
[^4.4]: "Logistic Regression models the probability of class membership through the logit function." *(Trecho de Linear Methods for Classification)*
[^4.4.1]: "The model parameters are estimated by maximizing the likelihood of observed data." *(Trecho de Linear Methods for Classification)*
[^4.4.2]: "Logistic regression is a robust approach for classification, even when data is not perfectly balanced." *(Trecho de Linear Methods for Classification)*
[^4.4.3]: "The optimization in logistic regression involves the analysis of the likelihood function and its derivatives." *(Trecho de Linear Methods for Classification)*
[^4.4.4]: "Logistic regression does not require assumptions of normal distributions in predictor variables." *(Trecho de Linear Methods for Classification)*
[^4.4.5]: "The logit link function ensures that probabilities lie within the interval [0, 1]." *(Trecho de Linear Methods for Classification)*
[^4.5]: "Regularization techniques, such as L1 and L2, are important for addressing overfitting and improving model stability." *(Trecho de Linear Methods for Classification)*
[^4.5.1]: "L1 regularization (lasso) leads to sparse models by automatically selecting the most relevant variables." *(Trecho de Linear Methods for Classification)*
[^4.5.2]: "Separating hyperplanes are found by maximizing the margin between classes, a concept central to several classification methods." *(Trecho de Linear Methods for Classification)*
[^3.4.1]: "Ridge regression shrinks the coefficients toward zero, but does not generally eliminate predictors from the model." *(Trecho de Linear Methods for Regression)*
[^3.4.2]: "The lasso, on the other hand, performs variable selection by forcing some coefficients to be exactly zero." *(Trecho de Linear Methods for Regression)*
[^3.5.1]: "Principal component regression uses principal components to reduce dimensionality, which affects the model coefficients." *(Trecho de Linear Methods for Regression)*
[^3.5.2]: "Partial least squares similarly reduces dimensionality, but takes into account the response variable during the process." *(Trecho de Linear Methods for Regression)*
[^3.3.1]: "Best subset selection evaluates all possible predictor subsets to determine the model that provides the lowest error." *(Trecho de Linear Methods for Regression)*
[^3.6]: "Comparison of the coefficient profiles reveals how these methods deal with correlated predictors." *(Trecho de Linear Methods for Regression)*
