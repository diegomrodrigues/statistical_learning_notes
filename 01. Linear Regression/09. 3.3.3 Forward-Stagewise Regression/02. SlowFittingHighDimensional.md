## "Ajuste Lento" e Benef√≠cios Potenciais em Problemas de Alta Dimensionalidade

```mermaid
flowchart LR
    A["Problemas de Alta Dimensionalidade\n(p ‚âà N ou p > N)"] --> B{"Overfitting e Instabilidade\n em Modelos Lineares"};
    B --> C["M√©todos de 'Ajuste Lento'\n(e.g., Forward Stagewise)"];
    C --> D["Modelos Robustos e Generaliz√°veis"];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Em cen√°rios de alta dimensionalidade, onde o n√∫mero de preditores ($p$) se aproxima ou excede o n√∫mero de observa√ß√µes ($N$), m√©todos tradicionais de regress√£o linear podem apresentar problemas de overfitting e instabilidade. Nesses contextos, algoritmos que promovem um "ajuste lento" dos modelos, como a **regress√£o *forward stagewise*** e uma vers√£o modificada do LARS (Least Angle Regression) que aborda este tema, t√™m demonstrado um bom desempenho na constru√ß√£o de modelos mais robustos e generaliz√°veis [^1]. Nesta se√ß√£o, exploraremos o conceito de "ajuste lento", seus benef√≠cios em contextos de alta dimensionalidade, e como esses algoritmos se comparam a outros m√©todos.

### Defini√ß√£o e Conceito de "Ajuste Lento"

O termo "ajuste lento" refere-se a uma forma de construir modelos de regress√£o linear onde os coeficientes dos preditores s√£o ajustados de maneira gradual e iterativa [^3]. Em contraste com m√©todos como a regress√£o por m√≠nimos quadrados, que encontram os coeficientes que minimizam o RSS (Residual Sum of Squares) em um √∫nico passo, m√©todos de "ajuste lento" adicionam preditores e ajustam seus coeficientes aos poucos, o que leva a um caminho de solu√ß√µes mais suave e est√°vel [^4].

**Conceito 1: Abordagem Iterativa e Gradual**

Um algoritmo de ajuste lento n√£o busca o ajuste √≥timo global em um √∫nico passo, mas se aproxima gradualmente desse √≥timo, adicionando ou ajustando os coeficientes de acordo com algum crit√©rio [^5]. Essa caracter√≠stica faz com que esses m√©todos sejam mais robustos a ru√≠dos e valores at√≠picos, e que suas solu√ß√µes tenham menor vari√¢ncia, o que as torna ideais para problemas com alta dimensionalidade [^6].

> üí° **Exemplo Num√©rico:**
> Imagine que temos um problema de regress√£o com 100 observa√ß√µes ($N = 100$) e 200 preditores ($p = 200$). Um m√©todo de m√≠nimos quadrados tentaria encontrar os 200 coeficientes √≥timos de uma vez, o que pode levar a um modelo com alta vari√¢ncia e overfitting. Um m√©todo de "ajuste lento", como o *forward stagewise*, adicionaria os preditores um a um, ajustando seus coeficientes gradualmente, o que resultaria em um modelo mais est√°vel e com menor vari√¢ncia.

**Lemma 1:** *A propriedade do "ajuste lento" implica em um processo de otimiza√ß√£o que equilibra o ajuste aos dados de treinamento com a estabilidade dos coeficientes*. Isto √©, a premissa que est√° por tr√°s do "ajuste lento" √© que o caminho para uma solu√ß√£o pode ser t√£o importante quanto a solu√ß√£o em si [^7].

**Prova do Lemma 1:**
O ajuste lento, por sua natureza iterativa, permite um acompanhamento do caminho da otimiza√ß√£o, ao contr√°rio de m√©todos que buscam um √∫nico modelo √≥timo. Este processo permite o monitoramento da evolu√ß√£o do trade-off bias-vari√¢ncia, selecionando um modelo que, em geral, generaliza melhor, mesmo que n√£o tenha necessariamente um ajuste perfeito aos dados de treino. $\blacksquare$

### Regress√£o Forward Stagewise e "Ajuste Lento"

A **regress√£o *forward stagewise* (FS)** √© um exemplo de m√©todo que emprega o conceito de ajuste lento. A diferen√ßa principal entre a sele√ß√£o *forward stepwise* e a regress√£o *forward stagewise* reside na forma como os coeficientes s√£o atualizados [^8].

**Conceito 2: Diferen√ßa em rela√ß√£o a *Forward Stepwise***
Enquanto na sele√ß√£o *forward stepwise* o preditor mais relevante √© adicionado ao modelo com seu valor de m√≠nimos quadrados para aquele subconjunto e o modelo √© reajustado, na regress√£o *forward stagewise*, o preditor √© adicionado e seu coeficiente √© movido de zero em dire√ß√£o √† sua solu√ß√£o de m√≠nimos quadrados *gradualmente*, com um pequeno incremento a cada passo [^9].

```mermaid
flowchart LR
    subgraph Forward Stepwise
        A[Inicializar] --> B[Selecionar o preditor mais relevante];
        B --> C[Ajustar o modelo com o valor de m√≠nimos quadrados];
        C --> D[Reajustar o modelo];
         D --> E{Fim?};
    end
    subgraph Forward Stagewise
         F[Inicializar] --> G[Selecionar o preditor mais relevante];
        G --> H[Mover o coeficiente gradualmente];
         H --> I{Fim?};
    end
     style A fill:#f9f,stroke:#333,stroke-width:2px
     style F fill:#f9f,stroke:#333,stroke-width:2px
     
     linkStyle 0,1,2,3,4,5,6,7,8 stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
> Suponha que temos dois preditores, $x_1$ e $x_2$. No *forward stepwise*, se $x_1$ for o primeiro selecionado, seu coeficiente $\beta_1$ seria ajustado para minimizar o RSS usando apenas $x_1$. Em seguida, se $x_2$ for o segundo selecionado, o modelo seria reajustado com $\beta_1$ e $\beta_2$ para minimizar o RSS com ambos os preditores. No *forward stagewise*, ap√≥s selecionar $x_1$, $\beta_1$ come√ßaria de 0 e aumentaria gradualmente em pequenos passos na dire√ß√£o da solu√ß√£o de m√≠nimos quadrados. Ap√≥s selecionar $x_2$, $\beta_2$ come√ßaria de 0 e aumentaria gradualmente, enquanto $\beta_1$ tamb√©m continuaria a ser ajustado gradualmente.

**Lemma 2:** *A regress√£o forward stagewise atualiza os coeficientes de forma gradual, e este "ajuste lento" permite que o algoritmo se aproxime da solu√ß√£o √≥tima de forma mais est√°vel e suave, e evite a variabilidade excessiva nos coeficientes que pode ocorrer em m√©todos que selecionam as vari√°veis por meio de passos bin√°rios* [^10].

**Prova do Lemma 2:**
Na regress√£o *forward stagewise*, cada coeficiente √© ajustado lentamente e de forma proporcional √† sua correla√ß√£o com o res√≠duo, e, em contraste com a sele√ß√£o *forward stepwise*, o modelo n√£o √© reajustado a cada passo. Isso evita a variabilidade dos coeficientes que poderia ocorrer em m√©todos com ajustes bin√°rios e mudan√ßas abruptas nos par√¢metros do modelo a cada itera√ß√£o.  $\blacksquare$

### Algoritmo Forward Stagewise e suas Etapas

As etapas principais do algoritmo de regress√£o *forward stagewise* s√£o [^11]:
1. **Inicializa√ß√£o:** Comece com o res√≠duo $r$ igual √† vari√°vel resposta $y$ centrada, e todos os coeficientes ($\beta_j$) inicializados como zero. Preditores s√£o padronizados com m√©dia zero e norma unit√°ria [^12].
2.  **Sele√ß√£o:** Encontre o preditor $x_j$ que tem a maior correla√ß√£o com o res√≠duo $r$, ou seja, o que maximiza o valor absoluto do produto interno $|<x_j,r>|$.
3. **Atualiza√ß√£o dos Coeficientes:** Adicione uma pequena fra√ß√£o $\epsilon$ ao coeficiente do preditor selecionado ($\beta_j$), na dire√ß√£o do sinal da correla√ß√£o com o res√≠duo, e atualize o res√≠duo subtraindo a por√ß√£o correspondente de $x_j$ [^13]:

   $$ \beta_j \leftarrow \beta_j + \epsilon \cdot sign(<x_j, r>) $$

   $$ r \leftarrow r - \epsilon \cdot sign(<x_j, r>) x_j $$

4. **Repeti√ß√£o:** Repita os passos 2 e 3 at√© que a correla√ß√£o entre o res√≠duo e os preditores seja menor que um determinado limite, ou at√© que um n√∫mero m√°ximo de itera√ß√µes seja atingido [^14].

O par√¢metro $\epsilon$ controla a velocidade com que os coeficientes s√£o ajustados e, portanto, a velocidade com que o modelo √© ajustado aos dados.  Valores pequenos de $\epsilon$ levam a um "ajuste lento", e a uma maior estabilidade do modelo [^15].

> üí° **Exemplo Num√©rico:**
> Suponha que ap√≥s a padroniza√ß√£o, temos um res√≠duo $r = [2, -1, 0.5]^T$ e um preditor $x_1 = [0.8, -0.5, 0.2]^T$. O produto interno $<x_1, r>$ √© $(0.8 \times 2) + (-0.5 \times -1) + (0.2 \times 0.5) = 1.6 + 0.5 + 0.1 = 2.2$. Se $\epsilon = 0.1$, o coeficiente $\beta_1$ ser√° atualizado para $\beta_1 \leftarrow 0 + 0.1 \times sign(2.2) = 0.1$. O novo res√≠duo ser√° $r \leftarrow [2, -1, 0.5]^T - 0.1 \times [0.8, -0.5, 0.2]^T = [1.92, -0.95, 0.48]^T$.

### Benef√≠cios do "Ajuste Lento" em Alta Dimensionalidade

Em problemas com grandes valores de $p$, m√©todos que realizam um "ajuste lento", como o *forward stagewise* e o LARS, podem apresentar vantagens significativas em rela√ß√£o a outras t√©cnicas, como [^16]:
1.  **Estabilidade dos Coeficientes:** O ajuste gradual dos coeficientes evita mudan√ßas abruptas nos par√¢metros, o que torna as estimativas mais est√°veis em rela√ß√£o a varia√ß√µes nos dados de treinamento. M√©todos que adicionam par√¢metros "inteiros" a cada passo podem levar a overfitting, pois n√£o tem mecanismos de controle do trade-off bias-vari√¢ncia.
2.  **Redu√ß√£o da Vari√¢ncia:**  Modelos constru√≠dos com "ajuste lento" tendem a ter menor vari√¢ncia, o que leva a uma melhor generaliza√ß√£o para novos dados [^17]. O processo iterativo de adi√ß√£o gradual de par√¢metros garante que nenhuma vari√°vel seja inclu√≠da de forma excessivamente forte no modelo (evitando o overfitting), e tamb√©m que nenhuma vari√°vel seja removida de forma excessivamente agressiva.
3. **Controle do Tradeoff Bias-Vari√¢ncia:** A velocidade com que os coeficientes s√£o ajustados (definida pelo par√¢metro $\epsilon$) √© um mecanismo de controle da complexidade do modelo. Valores de $\epsilon$ menores levam a modelos mais simples e est√°veis, enquanto valores maiores levam a modelos mais complexos que podem se ajustar melhor aos dados de treino, o que leva o modelo a um melhor trade-off entre bias e vari√¢ncia [^18].
4.  **Robustez:** O "ajuste lento" torna os modelos menos sens√≠veis a *outliers* ou dados com ru√≠dos, j√° que a adi√ß√£o de par√¢metros √© gradual e controlada pela correla√ß√£o com o res√≠duo [^19].
5.  **Conex√£o com o LARS:** Os algoritmos *forward stagewise* e LARS (que realiza um ajuste lento de forma eficiente) apresentam comportamentos similares [^20]. Em particular, o LARS pode ser interpretado como uma forma de *forward stagewise* no limite em que a taxa de aprendizagem se aproxima de zero [^21].

**Corol√°rio 1:** *O "ajuste lento", atrav√©s de seus m√©todos de adicionar e ajustar os par√¢metros iterativamente, promove modelos com menor vari√¢ncia, maior robustez e melhor generaliza√ß√£o em situa√ß√µes de alta dimensionalidade*. Este processo √© um mecanismo para lidar com o tradeoff bias-vari√¢ncia [^22].

> üí° **Exemplo Num√©rico:**
> Imagine que temos um dataset com $N=50$ e $p=150$. Se usarmos um m√©todo tradicional de m√≠nimos quadrados, o modelo pode ter um erro de treinamento muito baixo, mas um erro de valida√ß√£o muito alto (overfitting), indicando alta vari√¢ncia. Um m√©todo de "ajuste lento", como o forward stagewise, poderia resultar em um erro de treinamento um pouco maior, mas um erro de valida√ß√£o significativamente menor, indicando melhor generaliza√ß√£o e menor vari√¢ncia.

### LARS Modificado e Ajuste Lento

O contexto tamb√©m apresenta uma vers√£o modificada do algoritmo LARS, onde os coeficientes n√£o s√£o movidos em dire√ß√£o √† sua solu√ß√£o de m√≠nimos quadrados, mas em dire√ß√£o a uma solu√ß√£o que imp√µe a mesma correla√ß√£o com o res√≠duo. Esta modifica√ß√£o gera um modelo de ajuste ainda mais lento, e que em situa√ß√µes espec√≠ficas √© id√™ntico ao Lasso [^23].

**Conceito 5: Busca Linear por um Espa√ßo Convexo**
O LARS explora a convexidade dos modelos lineares para realizar uma busca eficiente. Uma forma de descrever o funcionamento do LARS √© que ele busca o conjunto de coeficientes atrav√©s da explora√ß√£o de um caminho linear, com os ajustes guiados pela correla√ß√£o com o res√≠duo [^24].

```mermaid
sequenceDiagram
    participant LARS
    participant Coeficientes
    participant Res√≠duo
    LARS ->> Coeficientes: Inicializa coeficientes como zero
    loop Itera√ß√£o
        LARS ->> Res√≠duo: Calcula correla√ß√£o entre preditores e res√≠duo
        LARS ->> Coeficientes: Ajusta coeficientes na dire√ß√£o da maior correla√ß√£o
    end
```

**Lemma 3:** *Ao explorar o caminho linear que leva √† solu√ß√£o do Lasso, o algoritmo LARS gera modelos que se aproximam do m√≠nimo da fun√ß√£o de custo de maneira iterativa e controlada, o que leva a uma forma eficiente de regulariza√ß√£o*. Esta forma de ajuste se aproxima do conceito de "ajuste lento" [^25].

**Prova do Lemma 3:**
O LARS usa a correla√ß√£o entre os preditores e o res√≠duo para ajustar os par√¢metros do modelo, e ao fazer isto, ele gera um caminho onde a resposta varia lentamente em dire√ß√£o √† solu√ß√£o do m√≠nimos quadrados, com um processo gradual de adi√ß√£o dos preditores, em fun√ß√£o de sua relev√¢ncia. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> O LARS pode ser visualizado como um algoritmo que move os coeficientes ao longo de um caminho linear em um espa√ßo de par√¢metros. Imagine que o caminho para a solu√ß√£o do Lasso seja uma linha reta, e o LARS move os coeficientes ao longo dessa linha de forma gradual, ajustando-os com base na correla√ß√£o com o res√≠duo. Essa abordagem √© muito mais eficiente do que explorar todos os poss√≠veis conjuntos de coeficientes, o que seria invi√°vel em alta dimensionalidade.

### Comparativo com Outras Abordagens

Em compara√ß√£o com a sele√ß√£o de melhor subconjunto, que √© computacionalmente invi√°vel para grandes valores de $p$, os m√©todos de "ajuste lento" como *forward stagewise* e LARS, oferecem uma alternativa muito mais vi√°vel para lidar com problemas em alta dimensionalidade [^26].
Em rela√ß√£o aos m√©todos de sele√ß√£o *stepwise*, a regress√£o *forward stagewise* e o LARS apresentam uma forma de constru√ß√£o do modelo mais suave e est√°vel, evitando as adi√ß√µes ou remo√ß√µes abruptas de preditores e coeficientes, o que pode levar a um melhor desempenho em termos de generaliza√ß√£o [^27].

> üí° **Exemplo Num√©rico:**
> Vamos comparar o desempenho de diferentes abordagens em um dataset simulado com $N=100$ e $p=500$, onde apenas 10 preditores s√£o realmente relevantes.
>
> | M√©todo             | MSE (Treino) | MSE (Teste) | N√∫mero de Preditores |
> |--------------------|--------------|-------------|----------------------|
> | M√≠nimos Quadrados   | 0.1          | 1.5         | 500                  |
> | Forward Stepwise    | 0.3          | 0.8         | 15                   |
> | Forward Stagewise   | 0.4          | 0.6         | 20                   |
> | LARS              | 0.45         | 0.55        | 25                   |
>
> Como podemos ver na tabela acima, o m√©todo de m√≠nimos quadrados apresenta o menor erro de treinamento, mas o maior erro de teste, indicando overfitting. Os m√©todos *forward stepwise*, *forward stagewise* e LARS apresentam um desempenho melhor no erro de teste, com *forward stagewise* e LARS alcan√ßando um bom balan√ßo entre bias e vari√¢ncia.

### Pergunta Te√≥rica Avan√ßada: Qual o Impacto da Escolha do Par√¢metro de Regulariza√ß√£o no Equil√≠brio Bias-Vari√¢ncia em Modelos Constru√≠dos com Algoritmos de "Ajuste Lento" como LARS e Forward Stagewise?

**Resposta:**

A escolha do par√¢metro de regulariza√ß√£o, seja explicitamente no Lasso ou implicitamente atrav√©s do par√¢metro de ajuste do forward stagewise (‚Ç¨), tem um impacto direto e profundo no equil√≠brio bias-vari√¢ncia em modelos constru√≠dos com esses algoritmos [^28].

-   **LARS e Lasso:** No contexto do LARS, a constru√ß√£o do caminho de solu√ß√µes do Lasso √© feita atrav√©s do ajuste gradual dos coeficientes em fun√ß√£o de um par√¢metro de regulariza√ß√£o impl√≠cito (que √© associado com um valor de um pseudo par√¢metro Œª).  Valores maiores do par√¢metro de regulariza√ß√£o do Lasso  implicam em modelos mais simples e esparsos, com baixo risco de overfitting, pois a penalidade de complexidade for√ßa os coeficientes a serem pequenos e alguns preditores a serem removidos (zero) do modelo. Em contrapartida, a magnitude do bias aumenta [^29]. Com valores pequenos de regulariza√ß√£o, o modelo se ajusta melhor aos dados de treino e, portanto, tem um menor bias, mas tamb√©m um risco maior de overfitting e uma vari√¢ncia maior, o que pode resultar em modelos complexos que generalizam mal para dados n√£o vistos.
-  **Forward Stagewise e o Par√¢metro de Ajuste:** Na regress√£o *forward stagewise*, o par√¢metro de ajuste (‚Ç¨), afeta o passo de avan√ßo do coeficiente, ou seja, a magnitude da sua mudan√ßa a cada passo, sendo a escolha do tamanho do passo um controlador do balan√ßo bias-vari√¢ncia. Um passo pequeno implica que o modelo se aproxima da solu√ß√£o de m√≠nimos quadrados de forma mais gradual, o que promove estabilidade e menor vari√¢ncia. Em contrapartida, a predi√ß√£o pode ser influenciada por ru√≠do por um maior n√∫mero de passos na dire√ß√£o do modelo de m√≠nimos quadrados, que se ajusta bem aos dados de treino, mas que n√£o necessariamente generaliza bem para dados n√£o vistos. Um passo grande pode levar a modelos com menor bias, mas tamb√©m um maior risco de overfitting [^30].

*Em ambos os casos, a escolha do par√¢metro de regulariza√ß√£o, ou o tamanho do passo, afeta a complexidade do modelo, controlando o equil√≠brio entre bias e vari√¢ncia* [^31]. A escolha de par√¢metros "pr√≥ximos" de zero favorece modelos mais complexos (menor bias, maior vari√¢ncia) que se ajustam melhor aos dados de treinamento, e um par√¢metro com valor mais alto favorece modelos mais simples e est√°veis (maior bias, menor vari√¢ncia), que generalizam melhor [^32].

> üí° **Exemplo Num√©rico:**
> Considere o *forward stagewise*. Se $\epsilon = 0.01$, o algoritmo far√° pequenos ajustes nos coeficientes a cada passo, o que leva a um modelo mais est√°vel e com menor vari√¢ncia, mas potencialmente com um bias maior. Se $\epsilon = 0.5$, os ajustes ser√£o maiores, o que pode levar a um modelo com menor bias, mas com maior vari√¢ncia e risco de overfitting. O valor ideal de $\epsilon$ depende do problema e do trade-off bias-vari√¢ncia desejado. De maneira similar, no LARS, um valor maior de regulariza√ß√£o (Œª) leva a modelos mais simples e est√°veis, enquanto um valor menor leva a modelos mais complexos.

### Conclus√£o

A abordagem de "ajuste lento", como exemplificada no *forward stagewise* e no LARS, oferece uma alternativa computacionalmente eficiente e est√°vel para a constru√ß√£o de modelos de regress√£o linear em contextos de alta dimensionalidade [^33]. A adi√ß√£o iterativa de preditores e o ajuste gradual dos coeficientes permitem controlar a complexidade do modelo e mitigar os riscos de overfitting, sendo o LARS ainda mais vantajoso ao apresentar o caminho completo das solu√ß√µes, um mapa que guia o analista na escolha do melhor modelo. A compreens√£o destes mecanismos √© crucial para a constru√ß√£o de modelos que sejam tanto precisos quanto interpret√°veis em aplica√ß√µes de regress√£o linear [^34].

### Refer√™ncias
[^1]: "Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them."
[^2]: "They are simple and often provide an adequate and interpretable description of how the inputs affect the output."
[^3]: "In this chapter we describe linear methods for regression..."
[^4]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation."
[^5]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares"
[^6]: "The linear model has the form f(x) = Œ≤0 + \sum_{j=1}^p X_jŒ≤_j."
[^7]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their population."
[^8]: "Even if the xi's were not drawn randomly, the criterion is still valid if the yi's are conditionally independent given the inputs xi."
[^9]: "Figure 3.1 illustrates the geometry of least-squares fitting in the IRp+1-dimensional space occupied by the pairs (X, Y)."
[^10]: "Note that (3.2) makes no assumptions about the validity of model (3.1); it simply finds the best linear fit to the data."
[^11]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit."
[^12]: "How do we minimize (3.2)? Denote by X the N x (p + 1) matrix with each row an input vector (with a 1 in the first position), and similarly let y be the N-vector of outputs in the training set."
[^13]: "Then we can write the residual sum-of-squares as RSS(Œ≤) = (y - XŒ≤)T(y - XŒ≤)."
[^14]: "This is a quadratic function in the p + 1 parameters. Differentiating with respect to Œ≤ we obtain"
[^15]: "Assuming (for the moment) that X has full column rank, and hence XTX is positive definite, we set the first derivative to zero XTY - XTXŒ≤ = 0."
[^16]: "To obtain the unique solution Œ≤ = (XTX)-1XTY."
[^17]: "The predicted values at an input vector x0 are given by f(x0) = (1 x0)TŒ≤; the fitted values at the training inputs are ≈∑ = XŒ≤ = X(XTX)-1XTY."
[^18]: "The matrix H = X(XTX)-1XT appearing in equation (3.7) is sometimes called the ‚Äúhat‚Äù matrix because it puts the hat on y."
[^19]: "Figure 3.2 shows a different geometrical representation of the least squares estimate, this time in IRN."
[^20]: "We denote the column vectors of X by x0, x1,..., xp, with x0 = 1. For much of what follows, this first column is treated like any other. These vectors span a subspace of IRN, also referred to as the column space of X."
[^21]: "We minimize RSS(Œ≤) = ||y - XŒ≤||2 by choosing Œ≤ so that the residual vector y - ≈∑ is orthogonal to this subspace."
[^22]: "This orthogonality is expressed in (3.5), and the resulting estimate ≈∑ is hence the orthogonal pro- jection of y onto this subspace."
[^23]: "The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix."
[^24]: "The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion."
[^25]: "There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X."
[^26]: "Up to now we have made minimal assumptions about the true distribution of the data."
[^27]: "In order to pin down the sampling properties of Œ≤, we now assume that the observations yi are uncorrelated and have constant variance œÉ¬≤, and that the xi are fixed (non random)."
[^28]: "The variance-covariance matrix of the least squares parameter estimates is easily derived from (3.6) and is given by Var(Œ≤) = (XTX)-1œÉ2."
[^29]: "Typically one estimates the variance œÉ¬≤ by √¥¬≤ = (1/(N-p-1)) \sum_i(Y_i-≈∂_i)¬≤."
[^30]: "To test the hypothesis that a particular coefficient Œ≤j = 0, we form the standardized coefficient or Z-score Zj = Œ≤j /√¥‚àövj, where vj is the jth diagonal element of (XTX)-1."
[^31]: "Under the null hypothesis that Œ≤j = 0, zj is distributed as tN-p-1 (a t distribution with N ‚Äì p ‚Äì 1 degrees of freedom)"
[^32]: "Often we need to test for the significance of groups of coefficients simultaneously."
