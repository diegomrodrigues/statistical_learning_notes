## Abordagem com Restri√ß√µes: Adi√ß√£o Iterativa de Preditores com Ajuste Gradual dos Coeficientes

```mermaid
graph LR
    A[In√≠cio: Modelo Nulo] --> B{Calcular Res√≠duo};
    B --> C{Encontrar Preditores Mais Correlacionado};
    C --> D[Ajustar Coeficiente(s)];
    D --> E{Atualizar Res√≠duo};
    E --> F{Crit√©rio de Parada?};
    F -- Sim --> G[Fim];
    F -- N√£o --> C;
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Em modelos de regress√£o linear, uma abordagem alternativa para a sele√ß√£o de preditores envolve a adi√ß√£o iterativa de vari√°veis, ajustando seus coeficientes de maneira gradual. Em vez de uma adi√ß√£o ou remo√ß√£o completa de um preditor por vez, a abordagem com restri√ß√µes promove um crescimento gradual dos coeficientes, direcionado pela correla√ß√£o com o res√≠duo. O m√©todo **Least Angle Regression (LARS)** √© um exemplo proeminente dessa abordagem, proporcionando um caminho de solu√ß√µes e explorando a rela√ß√£o entre os preditores e o res√≠duo [^1]. Nesta se√ß√£o, exploraremos esta abordagem em detalhes, analisando suas etapas, caracter√≠sticas e vantagens computacionais e te√≥ricas, baseando-se no contexto fornecido [^2].

### Fundamentos da Abordagem com Restri√ß√µes

Em contraste com m√©todos que adicionam ou removem preditores abruptamente, a abordagem com restri√ß√µes adiciona preditores ao modelo de forma iterativa e gradual, com a quantidade adicionada controlada por crit√©rios de correla√ß√£o com o res√≠duo e, no caso do LARS, com as propriedades do Lasso [^3].
O LARS √© um algoritmo *democtr√°tico*, o que significa que ele n√£o imp√µe uma hierarquia, ou seja, as vari√°veis adicionadas na constru√ß√£o do modelo s√£o aquelas que est√£o mais correlacionadas com o res√≠duo, independentemente de seu √≠ndice ou posi√ß√£o [^4]. Ele inicia com um modelo nulo, ajusta o intercepto e adiciona preditores de acordo com a sua correla√ß√£o com o res√≠duo, e os coeficientes dos preditores s√£o movidos gradualmente em dire√ß√£o √† solu√ß√£o de m√≠nimos quadrados, ajustando-se em cada itera√ß√£o [^5].

**Conceito 1: Adi√ß√£o Iterativa de Preditores**

Na abordagem com restri√ß√µes, a sele√ß√£o de preditores √© feita de forma iterativa, adicionando a cada passo o preditor que apresenta maior correla√ß√£o com o res√≠duo atual do modelo. Em cada passo, o algoritmo identifica o preditor mais correlacionado, calcula a proje√ß√£o desse preditor no res√≠duo, e atualiza os coeficientes de forma a reduzir a magnitude da correla√ß√£o com o res√≠duo, fazendo com que sua contribui√ß√£o seja proporcional √† sua relev√¢ncia. √â importante notar que esta n√£o √© uma adi√ß√£o bin√°ria, e a adi√ß√£o √© feita de forma gradual, atrav√©s do ajuste do coeficiente [^6].

> üí° **Exemplo Num√©rico:**
>
> Imagine que temos um res√≠duo inicial $r = [10, -5, 8, 2]$ e dois preditores normalizados $x_1 = [0.8, 0.2, -0.5, 0.1]$ e $x_2 = [-0.3, 0.9, 0.1, 0.3]$.
>
> 1.  Calculamos a correla√ß√£o de cada preditor com o res√≠duo:
>    -   Correla√ß√£o de $x_1$ com $r$: $<x_1, r> = (0.8 * 10) + (0.2 * -5) + (-0.5 * 8) + (0.1 * 2) = 8 - 1 - 4 + 0.2 = 3.2$
>    -   Correla√ß√£o de $x_2$ com $r$: $<x_2, r> = (-0.3 * 10) + (0.9 * -5) + (0.1 * 8) + (0.3 * 2) = -3 - 4.5 + 0.8 + 0.6 = -6.1$
>2.  O preditor $x_2$ tem a maior correla√ß√£o em valor absoluto (6.1), ent√£o ele √© selecionado para ser adicionado ao modelo.
>3.  O coeficiente de $x_2$ √© ajustado gradualmente, e o res√≠duo √© atualizado, reduzindo a correla√ß√£o entre $x_2$ e o novo res√≠duo. O ajuste n√£o √© bin√°rio, ou seja, o coeficiente de $x_2$ n√£o passa de 0 para o seu valor final instantaneamente, mas sim gradualmente.
>
> Este processo continua com o res√≠duo atualizado at√© que um crit√©rio de parada seja atingido.

**Lemma 1:** *Em cada passo, a escolha do preditor mais correlacionado com o res√≠duo garante que a nova vari√°vel contribua significativamente para a redu√ß√£o do erro do modelo, dado o est√°gio da itera√ß√£o*. A magnitude com a qual essa vari√°vel afeta o resultado final depende da sua correla√ß√£o com o res√≠duo [^7].

**Prova do Lemma 1:**
Em cada passo, o preditor que apresenta maior correla√ß√£o com o res√≠duo √© o que tem a maior componente na dire√ß√£o do erro do modelo atual. Ao adicion√°-lo, e ao ajustar o seu par√¢metro de forma a manter essa correla√ß√£o, o modelo captura uma por√ß√£o maior de erro, o que leva a um modelo melhor a cada passo.
$\blacksquare$

**Conceito 2: Ajuste Gradual de Coeficientes**

O ajuste dos coeficientes √© outro aspecto crucial da abordagem com restri√ß√µes. Em vez de atribuir um valor completo ao coeficiente de uma vari√°vel ao adicion√°-la ao modelo (como ocorre em *forward stepwise*), os coeficientes s√£o movidos de zero para o valor que minimiza o res√≠duo *na dire√ß√£o definida pelo preditor mais correlacionado e sem nenhuma outra vari√°vel ser removida, como ocorre no backward* [^8]. Esse ajuste gradual e simult√¢neo permite que o algoritmo explore o espa√ßo de solu√ß√µes de forma mais suave e eficiente, com a quantidade adicionada sendo definida pelo crit√©rio de otimiza√ß√£o do algoritmo.
```mermaid
sequenceDiagram
    participant Modelo
    participant Preditores
    participant Res√≠duo
    Modelo->>Preditores: Seleciona preditor com maior correla√ß√£o
    Preditores->>Res√≠duo: Calcula proje√ß√£o no res√≠duo
    Res√≠duo->>Modelo: Ajusta coeficientes gradualmente
    Modelo->>Res√≠duo: Atualiza o res√≠duo
```

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, suponha que, ap√≥s a adi√ß√£o de $x_2$, o res√≠duo tenha mudado para $r' = [4, -2, 5, 1]$. Agora, temos que escolher entre $x_1$ e um novo preditor $x_3 = [0.1, -0.6, 0.7, -0.2]$.
>
> 1. Calculamos a correla√ß√£o com o novo res√≠duo $r'$:
>    - Correla√ß√£o de $x_1$ com $r'$: $<x_1, r'> = (0.8 * 4) + (0.2 * -2) + (-0.5 * 5) + (0.1 * 1) = 3.2 - 0.4 - 2.5 + 0.1 = 0.4$
>    - Correla√ß√£o de $x_3$ com $r'$: $<x_3, r'> = (0.1 * 4) + (-0.6 * -2) + (0.7 * 5) + (-0.2 * 1) = 0.4 + 1.2 + 3.5 - 0.2 = 4.9$
>2.  $x_3$ √© selecionado para ser adicionado ao modelo.
>3.  O coeficiente de $x_3$ √© ajustado gradualmente, e o coeficiente de $x_2$ tamb√©m √© ajustado, de forma que ambos tenham a mesma correla√ß√£o com o novo res√≠duo.
>
> Este processo ilustra como os coeficientes s√£o ajustados gradualmente, e como o modelo evolui iterativamente.

### Funcionamento do Algoritmo LARS

O algoritmo LARS (Least Angle Regression) √© um exemplo emblem√°tico dessa abordagem com restri√ß√µes. Suas etapas principais s√£o [^9]:

1.  **Inicializa√ß√£o:** Inicie com um modelo sem preditores ($\beta_j = 0$ para todos os $j$). O res√≠duo inicial √© igual a $r = y-\bar{y}$, onde $\bar{y}$ √© a m√©dia de $y$ [^10]. Os preditores s√£o centrados para terem m√©dia 0 e normalizados para ter norma 1.
2.  **Sele√ß√£o de Preditores:** Encontre o preditor $x_j$ que tem a maior correla√ß√£o (em valor absoluto) com o res√≠duo atual, o que √© equivalente ao preditor que maximiza o produto interno $|<x_j, r>|$.
3.  **Ajuste dos Coeficientes:**
    -   Movimente o coeficiente $\beta_j$ do preditor selecionado de zero at√© o valor onde algum outro preditor $x_k$ tiver a mesma correla√ß√£o (em valor absoluto) com o res√≠duo [^11].
   - Se h√° mais de um preditor no conjunto ativo, os coeficientes s√£o movimentados em conjunto para manter a mesma correla√ß√£o com o res√≠duo.
    - Se nenhum outro preditor atingir o valor da correla√ß√£o atual, o processo √© finalizado e chegamos na solu√ß√£o do m√≠nimos quadrados [^12].

4.  **Atualiza√ß√£o:** Atualize o res√≠duo com base no ajuste dos coeficientes.
5.  **Repeti√ß√£o:** Retorne ao passo 2, at√© que um crit√©rio de parada seja satisfeito, como um n√∫mero m√°ximo de preditores ou a aus√™ncia de preditores correlacionados ao res√≠duo [^13].

**Lemma 2:** *No algoritmo LARS, a dire√ß√£o da atualiza√ß√£o do coeficiente de um preditor ativo √© definida pela sua proje√ß√£o no res√≠duo, e os coeficientes de todos os preditores no conjunto ativo s√£o atualizados conjuntamente para manter a mesma correla√ß√£o com o res√≠duo*. Essa forma de ajuste gradual permite uma explora√ß√£o mais eficiente do espa√ßo de solu√ß√µes [^14].

**Prova do Lemma 2:**
O LARS √© definido por uma sequ√™ncia de etapas onde o modelo atual √© alterado para incluir mais informa√ß√µes a cada passo. Em cada etapa, o algoritmo busca o preditor que possui maior correla√ß√£o com o res√≠duo. Ao adicionar um preditor, o algoritmo atualiza os coeficientes de forma que os preditores no conjunto ativo compartilhem a mesma correla√ß√£o com o res√≠duo, e, portanto, seus coeficientes s√£o ajustados conjuntamente, em dire√ß√£o a solu√ß√£o final [^15]. $\blacksquare$
```mermaid
graph LR
    A[Inicializa√ß√£o: Œ≤=0, r=y-yÃÑ] --> B{Seleciona Preditores: max |<xj, r>|};
    B --> C{Ajusta Coeficientes Œ≤j};
    C --> D{Atualiza Res√≠duo};
    D --> E{Outro Preditores com Mesma Correla√ß√£o?};
    E -- Sim --> C
    E -- N√£o --> F{Crit√©rio de Parada?};
    F -- Sim --> G[Fim];
    F -- N√£o --> B;
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
>
> Vamos ilustrar o funcionamento do LARS com um exemplo simplificado. Suponha que temos uma vari√°vel resposta $y = [5, 8, 12, 15]$ e dois preditores $x_1 = [1, 2, 3, 4]$ e $x_2 = [4, 3, 2, 1]$.
>
> 1. **Inicializa√ß√£o:**
>     -   O res√≠duo inicial √© $r = y - \bar{y} = [5, 8, 12, 15] - 10 = [-5, -2, 2, 5]$ (assumindo que $\bar{y} = 10$).
>     -   Os preditores s√£o centrados e normalizados. Vamos assumir que ap√≥s o processo de normaliza√ß√£o, os preditores s√£o $x_1 = [0.2, 0.4, 0.6, 0.8]$ e $x_2 = [0.8, 0.6, 0.4, 0.2]$.
>
> 2.  **Sele√ß√£o do Primeiro Preditores:**
>     -   Calculamos a correla√ß√£o de cada preditor com o res√≠duo:
>         -   $<x_1, r> = (0.2 * -5) + (0.4 * -2) + (0.6 * 2) + (0.8 * 5) = -1 - 0.8 + 1.2 + 4 = 3.4$
>         -   $<x_2, r> = (0.8 * -5) + (0.6 * -2) + (0.4 * 2) + (0.2 * 5) = -4 - 1.2 + 0.8 + 1 = -3.4$
>     -   O preditor $x_1$ tem a maior correla√ß√£o absoluta com o res√≠duo (3.4), ent√£o ele √© selecionado.
>
> 3.  **Ajuste do Coeficiente:**
>     -   O coeficiente de $x_1$, $\beta_1$, √© ajustado gradualmente a partir de 0. Enquanto $\beta_1$ aumenta, o res√≠duo √© atualizado.
>     -   O ajuste continua at√© que outro preditor ($x_2$) tenha uma correla√ß√£o com o res√≠duo igual √† correla√ß√£o atual de $x_1$.
>     -   Neste exemplo, vamos assumir que, ap√≥s um passo, $\beta_1$ foi ajustado para 1 e o res√≠duo √© atualizado para $r' = [-6, -4, 0, 4]$.
>
> 4.  **Sele√ß√£o do Segundo Preditores:**
>    - Calculamos a correla√ß√£o de cada preditor com o novo res√≠duo:
>         -   $<x_1, r'> = (0.2 * -6) + (0.4 * -4) + (0.6 * 0) + (0.8 * 4) = -1.2 - 1.6 + 0 + 3.2 = 0.4$
>         -   $<x_2, r'> = (0.8 * -6) + (0.6 * -4) + (0.4 * 0) + (0.2 * 4) = -4.8 - 2.4 + 0 + 0.8 = -6.4$
>    - $x_2$ tem a maior correla√ß√£o absoluta, e √© adicionado ao conjunto ativo.
>
> 5.  **Ajuste Conjunto:**
>     - Agora, os coeficientes de $x_1$ e $x_2$ s√£o ajustados conjuntamente para manter a mesma correla√ß√£o com o res√≠duo.
>
> Este processo continua at√© que todos os preditores estejam no modelo, ou que um crit√©rio de parada seja atingido.

### LARS e o Caminho das Solu√ß√µes do Lasso

Um aspecto not√°vel do LARS √© a sua capacidade de gerar o caminho de solu√ß√µes do Lasso (Least Absolute Shrinkage and Selection Operator) de forma eficiente [^16]. O Lasso √© uma t√©cnica de regulariza√ß√£o que utiliza a norma L1 para induzir sparsity nos modelos, e o LARS oferece uma maneira r√°pida de computar todas as solu√ß√µes do Lasso, variando o par√¢metro de regulariza√ß√£o.

**Corol√°rio 1:** *O LARS pode ser visto como um algoritmo para computar o caminho de solu√ß√µes do Lasso, onde o par√¢metro de regulariza√ß√£o varia de zero at√© o valor onde todos os coeficientes s√£o iguais a zero*. O LARS utiliza um processo eficiente de atualiza√ß√£o das estimativas, o que √© crucial para problemas com grandes conjuntos de dados [^17].

> üí° **Exemplo Num√©rico:**
>
> Imagine que o LARS est√° sendo usado para resolver um problema de regress√£o com o objetivo de encontrar o caminho das solu√ß√µes do Lasso. O par√¢metro de regulariza√ß√£o $\lambda$ controla a esparsidade do modelo, com valores maiores de $\lambda$ levando a modelos mais esparsos. O LARS come√ßa com $\lambda = 0$ (sem regulariza√ß√£o, equivalente a regress√£o linear).
>
> O algoritmo LARS constr√≥i uma sequ√™ncia de modelos, adicionando preditores gradualmente. A cada passo, ele encontra o preditor mais correlacionado com o res√≠duo atual e ajusta seus coeficientes (e, possivelmente, dos outros preditores no conjunto ativo) at√© que outro preditor tenha a mesma correla√ß√£o com o res√≠duo. Ao longo deste caminho, o algoritmo descreve como os coeficientes variam com a mudan√ßa da regulariza√ß√£o (ou seja, a mudan√ßa de $\lambda$).
>
> No in√≠cio do caminho, com valores de $\lambda$ pr√≥ximos de zero, muitos preditores podem ter coeficientes n√£o-nulos. Conforme $\lambda$ aumenta, a penalidade L1 do Lasso come√ßa a fazer efeito, for√ßando alguns coeficientes a se tornarem exatamente zero, levando a um modelo mais esparso. O LARS nos permite ver exatamente quais coeficientes s√£o zerados primeiro e como os outros coeficientes se ajustam ao longo do caminho.
>
> Por exemplo, o caminho de solu√ß√µes pode mostrar que para $\lambda = 0.1$, os coeficientes de $x_1$ e $x_2$ s√£o 0.5 e 0.3, respectivamente, e que para $\lambda = 0.5$, o coeficiente de $x_2$ se torna zero, enquanto o coeficiente de $x_1$ se torna 0.2. O LARS, portanto, nos permite ver como a regulariza√ß√£o do Lasso afeta a import√¢ncia de cada preditor no modelo.

### Vantagens Computacionais e Te√≥ricas do LARS

O algoritmo LARS oferece diversas vantagens computacionais e te√≥ricas em compara√ß√£o com outros m√©todos de sele√ß√£o de modelos [^18]:

-   **Efici√™ncia Computacional:** O LARS √© computacionalmente eficiente, com um custo similar ao de uma regress√£o linear padr√£o, o que o torna adequado para cen√°rios de alta dimensionalidade.
-   **Caminho de Solu√ß√µes:** O LARS gera o caminho de solu√ß√µes do Lasso, o que permite a avalia√ß√£o do efeito da regulariza√ß√£o no desempenho do modelo. Isso leva a uma melhor compreens√£o do trade-off bias-vari√¢ncia ao longo do caminho das solu√ß√µes.
-   **Sele√ß√£o de Vari√°veis:** O LARS realiza a sele√ß√£o de vari√°veis de forma impl√≠cita, ou seja, os preditores menos relevantes s√£o removidos ou seus coeficientes s√£o zerados, o que leva a modelos mais esparsos e interpret√°veis.
-   **Interpreta√ß√£o:** Ao tra√ßar o caminho da solu√ß√£o, √© poss√≠vel entender como os coeficientes dos preditores variam ao longo do caminho.
-   **Conex√£o com Outros M√©todos:** O LARS conecta naturalmente a regress√£o linear com o Lasso e outras t√©cnicas de regulariza√ß√£o.

**Conceito 4: Abordagem Democr√°tica**
A abordagem LARS √© conhecida como democr√°tica porque todos os preditores competem em igualdade de condi√ß√µes em rela√ß√£o √† sua correla√ß√£o com o res√≠duo. O LARS n√£o for√ßa nenhuma hierarquia e seleciona as vari√°veis com base em seu impacto local sobre a fun√ß√£o de perda [^19].

### Pergunta Te√≥rica Avan√ßada: Como a Rela√ß√£o entre o LARS e o Caminho de Solu√ß√µes do LASSO Permite uma An√°lise Mais Rica do Tradeoff Bias-Vari√¢ncia e como Isso se Compara com Outras Abordagens?

**Resposta:**

A rela√ß√£o intr√≠nseca entre o LARS (Least Angle Regression) e o caminho de solu√ß√µes do Lasso oferece uma perspectiva √∫nica para a an√°lise do tradeoff bias-vari√¢ncia em modelos de regress√£o linear, permitindo uma compreens√£o mais rica da influ√™ncia do par√¢metro de regulariza√ß√£o nos coeficientes e, consequentemente, na qualidade do modelo [^20].

- **Caminho de Solu√ß√µes do LASSO:** O Lasso, ao aplicar uma penalidade L1, for√ßa os coeficientes a serem menores e, em alguns casos, exatamente zero, promovendo a esparsidade do modelo.  O LARS, por sua vez, n√£o computa um √∫nico modelo, mas todo o caminho de solu√ß√µes do Lasso, indicando como os coeficientes variam em fun√ß√£o do par√¢metro de regulariza√ß√£o $\lambda$. Essa perspectiva cont√≠nua do caminho de solu√ß√µes permite avaliar o impacto da complexidade do modelo sobre a qualidade do ajuste e a generaliza√ß√£o [^21].
-  **LARS como Busca Guiada por Correla√ß√£o:** O LARS explora a rela√ß√£o entre preditores e o res√≠duo atrav√©s do produto interno. Em cada etapa, o preditor mais correlacionado ao res√≠duo √© adicionado, e seus coeficientes s√£o ajustados de forma que sua correla√ß√£o com o res√≠duo acompanhe a correla√ß√£o das demais vari√°veis ativas. A intensidade da atualiza√ß√£o do coeficiente √© guiada pelo par√¢metro de regulariza√ß√£o do Lasso e a correla√ß√£o do preditor com o res√≠duo, o que leva a uma sequ√™ncia de modelos que equilibram o ajuste e a esparsidade [^22].
```mermaid
graph LR
    A[Regress√£o Linear] --> B(LARS);
    B --> C[Caminho de Solu√ß√µes do Lasso];
    C --> D{Tradeoff Bias-Vari√¢ncia};
    style B fill:#ccf,stroke:#333,stroke-width:2px
```
Comparado a outros m√©todos de sele√ß√£o de vari√°veis, a abordagem do LARS e Lasso √© especialmente √∫til em cen√°rios de alta dimensionalidade [^23]:

-   **Sele√ß√£o Stepwise:** M√©todos *stepwise* procuram modelos por meio da adi√ß√£o e remo√ß√£o de par√¢metros a cada etapa, e podem ficar presos em solu√ß√µes locais, sendo menos eficientes em contextos de alta dimensionalidade.
-   **Sele√ß√£o do Melhor Subconjunto:** A sele√ß√£o do melhor subconjunto procura modelos √≥timos para cada tamanho, mas sua complexidade computacional cresce exponencialmente com o n√∫mero de preditores.

Em contraste com esses m√©todos, *o LARS oferece um caminho mais eficiente e completo para a sele√ß√£o de modelos, permitindo a avalia√ß√£o cont√≠nua do impacto da regulariza√ß√£o (atrav√©s do par√¢metro $\lambda$) sobre os coeficientes e sobre a qualidade do ajuste*. Ele pode ser utilizado como um guia para explorar diferentes modelos em diferentes pontos do caminho de regulariza√ß√£o, o que  permite uma avalia√ß√£o mais rica do trade-off bias-vari√¢ncia e uma melhor interpreta√ß√£o de modelos complexos. O LARS oferece, de forma natural, um conjunto de modelos que variam entre modelos complexos e de baixo bias e modelos mais simples e mais est√°veis, atrav√©s do seu caminho de regulariza√ß√£o [^24].

### Conclus√£o

A abordagem com restri√ß√µes, exemplificada pelo algoritmo LARS, oferece um m√©todo eficiente e flex√≠vel para a sele√ß√£o de preditores em modelos de regress√£o linear [^25]. O ajuste gradual dos coeficientes e a sele√ß√£o baseada em correla√ß√µes com o res√≠duo permitem um equil√≠brio entre a precis√£o e a interpretabilidade do modelo, especialmente quando se busca obter modelos esparsos. Ao gerar o caminho de solu√ß√µes do Lasso, o LARS fornece insights valiosos sobre o efeito da regulariza√ß√£o e, assim, conduz a modelos mais precisos, est√°veis e interpret√°veis, o que √© demonstrado em todo o contexto [^26].

### Refer√™ncias
[^1]: "Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them."
[^2]: "They are simple and often provide an adequate and interpretable description of how the inputs affect the output."
[^3]: "In this chapter we describe linear methods for regression..."
[^4]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation."
[^5]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares"
[^6]: "The linear model has the form f(x) = Œ≤0 + Œ£j=1 pXjŒ≤j."
[^7]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their population."
[^8]: "Even if the xi's were not drawn randomly, the criterion is still valid if the yi's are conditionally independent given the inputs xi."
[^9]: "Figure 3.1 illustrates the geometry of least-squares fitting in the IRp+1-dimensional space occupied by the pairs (X, Y)."
[^10]: "Note that (3.2) makes no assumptions about the validity of model (3.1); it simply finds the best linear fit to the data."
[^11]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit."
[^12]: "How do we minimize (3.2)? Denote by X the N x (p + 1) matrix with each row an input vector (with a 1 in the first position), and similarly let y be the N-vector of outputs in the training set."
[^13]: "Then we can write the residual sum-of-squares as RSS(Œ≤) = (y - XŒ≤)T(y - XŒ≤)."
[^14]: "This is a quadratic function in the p + 1 parameters. Differentiating with respect to Œ≤ we obtain"
[^15]: "Assuming (for the moment) that X has full column rank, and hence XTX is positive definite, we set the first derivative to zero XTY - XTXŒ≤ = 0."
[^16]: "To obtain the unique solution Œ≤ = (XTX)-1XTY."
[^17]: "The predicted values at an input vector x0 are given by f(x0) = (1 x0)TŒ≤; the fitted values at the training inputs are ≈∑ = XŒ≤ = X(XTX)-1XTY."
[^18]: "The matrix H = X(XTX)-1XT appearing in equation (3.7) is sometimes called the ‚Äúhat‚Äù matrix because it puts the hat on y."
[^19]: "Figure 3.2 shows a different geometrical representation of the least squares estimate, this time in IRN."
[^20]: "We denote the column vectors of X by x0, x1,..., xp, with x0 = 1. For much of what follows, this first column is treated like any other. These vectors span a subspace of IRN, also referred to as the column space of X."
[^21]: "We minimize RSS(Œ≤) = ||y - XŒ≤||2 by choosing Œ≤ so that the residual vector y - ≈∑ is orthogonal to this subspace."
[^22]: "This orthogonality is expressed in (3.5), and the resulting estimate ≈∑ is hence the orthogonal pro- jection of y onto this subspace."
[^23]: "The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix."
[^24]: "The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion."
[^25]: "There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X."
[^26]: "Up to now we have made minimal assumptions about the true distribution of the data."
