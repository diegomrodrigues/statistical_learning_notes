## Regress√£o em Componentes Ortogonais Derivados e Conex√£o com Ridge Regression
<imagem: Um mapa mental que conecte os conceitos de regress√£o linear, componentes principais, ridge regression e partial least squares, destacando as rela√ß√µes entre eles e como eles lidam com multicolinearidade.>

### Introdu√ß√£o

O problema de classifica√ß√£o e regress√£o muitas vezes envolve conjuntos de dados com um grande n√∫mero de preditores, que podem ser altamente correlacionados. Esta condi√ß√£o, conhecida como multicolinearidade, pode levar a modelos inst√°veis e dif√≠ceis de interpretar [^4.1], [^4.2].  Em resposta, m√©todos de aprendizado estat√≠stico que derivam componentes ortogonais dos preditores originais foram desenvolvidos, tais como **Principal Component Regression (PCR)** e **Partial Least Squares (PLS)**. Esses m√©todos visam reduzir a dimensionalidade dos dados e simplificar o modelo, projetando os preditores originais em um novo espa√ßo, onde os componentes s√£o linearmente independentes [^4.1], [^4.2], [^4.3], [^4.5]. Al√©m disso, esses m√©todos possuem uma forte conex√£o com **Ridge Regression**, um m√©todo de regulariza√ß√£o que tamb√©m visa mitigar os efeitos da multicolinearidade. Este cap√≠tulo explora profundamente estas t√©cnicas e suas inter-rela√ß√µes.

### Conceitos Fundamentais

#### Conceito 1: Problema de Classifica√ß√£o e Regress√£o com Multicolinearidade
Em problemas de classifica√ß√£o e regress√£o, frequentemente nos deparamos com conjuntos de dados onde as vari√°veis preditoras, $X_1, X_2, ..., X_p$, s√£o correlacionadas entre si. Essa multicolinearidade pode levar a diversos problemas [^4.1], [^4.2]:
*   **Instabilidade dos coeficientes:** Pequenas mudan√ßas nos dados podem causar grandes varia√ß√µes nos coeficientes do modelo de regress√£o linear, dificultando a interpreta√ß√£o dos resultados.
*   **Alta vari√¢ncia:** Estimativas dos coeficientes podem apresentar alta vari√¢ncia, levando a previs√µes menos precisas e generaliz√°veis para novos dados.
*   **Dificuldade de interpreta√ß√£o:** Modelos com multicolinearidade podem ser dif√≠ceis de interpretar, pois os coeficientes individuais podem n√£o refletir o verdadeiro impacto de cada preditor na vari√°vel resposta.
M√©todos lineares, como a regress√£o linear, podem ser inadequados para lidar com esse tipo de problema [^4.1].  A ideia principal por tr√°s da regress√£o em componentes derivados √© criar novas vari√°veis que sejam linearmente independentes (ortogonais), combinando as vari√°veis originais, e assim contornar a multicolinearidade e simplificar o modelo [^4.2], [^4.3]. M√©todos de regulariza√ß√£o como a Ridge Regression tamb√©m s√£o usados para tratar a multicolinearidade, penalizando o tamanho dos coeficientes.

```mermaid
graph LR
    subgraph "Multicollinearity Impact"
        direction TB
        A["Multicollinearity"] --> B["Unstable Coefficients"]
        A --> C["High Variance"]
        A --> D["Difficult Interpretation"]
        B --> E["Poor Prediction"]
        C --> E
        D --> F["Reduced Model Clarity"]
    end
```

> üí° **Exemplo Num√©rico:** Imagine que temos um conjunto de dados com duas vari√°veis preditoras, $X_1$ (tamanho do motor em litros) e $X_2$ (pot√™ncia do motor em cavalos), que s√£o altamente correlacionadas, e uma vari√°vel resposta $Y$ (consumo de combust√≠vel em km/l). Se ajustarmos um modelo de regress√£o linear, os coeficientes podem ser inst√°veis. Pequenas varia√ß√µes na amostra podem gerar grandes mudan√ßas nos coeficientes estimados para $X_1$ e $X_2$. Por exemplo, poder√≠amos obter $\hat{\beta}_1 = 0.5$ e $\hat{\beta}_2 = -0.2$, onde a interpreta√ß√£o seria confusa, visto que ambas as vari√°veis est√£o relacionadas √† performance do motor. A multicolinearidade dificulta a interpreta√ß√£o dos coeficientes individuais.

##### Lemma 1
Em um modelo de regress√£o linear, a vari√¢ncia dos coeficientes $\hat{\beta}$ √© dada por $Var(\hat{\beta}) = \sigma^2(X^T X)^{-1}$, onde $X$ √© a matriz de desenho e $\sigma^2$ √© a vari√¢ncia do erro. A presen√ßa de multicolinearidade implica que a matriz $X^T X$ √© mal condicionada (pr√≥xima de singular), o que leva a grandes elementos diagonais em $(X^T X)^{-1}$, e, consequentemente, a alta vari√¢ncia em $\hat{\beta}$. [^4.3]
$$ \blacksquare $$

```mermaid
graph TB
    subgraph "Variance of Coefficients"
        direction TB
        A["Variance of Coefficients: Var(Œ≤ÃÇ)"]
        B["Formula: œÉ¬≤(X·µÄX)‚Åª¬π"]
        C["X·µÄX: Design Matrix"]
        D["œÉ¬≤: Error Variance"]
        E["Multicollinearity Impact"]
        F["X·µÄX is ill-conditioned"]
        G["Large diagonal elements in (X·µÄX)‚Åª¬π"]
        H["High Variance in Œ≤ÃÇ"]
        A --> B
        B --> C
        B --> D
        C --> E
        E --> F
        F --> G
        G --> H
    end
```

> üí° **Exemplo Num√©rico:** Suponha que temos uma matriz de desenho $X$ com duas colunas (al√©m da coluna de 1's para o intercepto):
> ```python
> import numpy as np
> X = np.array([[1, 2, 3], [1, 2.1, 3.1], [1, 2.2, 3.2], [1, 5, 6], [1, 5.1, 6.1], [1, 5.2, 6.2]])
> ```
> Observe que as colunas 2 e 3 s√£o altamente correlacionadas. Vamos calcular $(X^T X)^{-1}$:
> ```python
> XtX_inv = np.linalg.inv(X.T @ X)
> print(XtX_inv)
> ```
> O resultado ser√°:
> ```
> [[ 2.75000000e+01 -1.50000000e+01 -4.85000000e+00]
> [-1.50000000e+01  8.16666667e+00  2.60000000e+00]
> [-4.85000000e+00  2.60000000e+00  8.50000000e-01]]
> ```
> Note que os elementos diagonais (relacionados √† vari√¢ncia dos coeficientes) s√£o relativamente grandes, indicando alta vari√¢ncia devido √† multicolinearidade. Se as colunas fossem ortogonais, a matriz $(X^T X)$ seria diagonal, e a inversa tamb√©m, com elementos menores na diagonal.

#### Conceito 2: Principal Component Regression (PCR)

A **Principal Component Regression (PCR)** √© uma t√©cnica que combina **an√°lise de componentes principais (PCA)** com regress√£o linear [^4.5]. O processo envolve duas etapas principais:
1.  **PCA:** Primeiro, a PCA √© aplicada para derivar um conjunto de componentes principais (PCs) a partir da matriz de preditores $X$. Os PCs s√£o combina√ß√µes lineares ortogonais das vari√°veis originais, ordenadas por sua vari√¢ncia, ou seja, a primeira PC captura a maior variabilidade nos dados, a segunda PC captura a segunda maior variabilidade, e assim por diante.  Os PCs s√£o calculados usando a decomposi√ß√£o em valores singulares (SVD) de $X$, de acordo com $X = UDV^T$, onde $U$ √© uma matriz ortogonal com as coordenadas das proje√ß√µes de $X$ no espa√ßo dos PCs, $V$ s√£o os autovetores da matriz de covari√¢ncia $X^T X$ (tamb√©m conhecidos como as dire√ß√µes dos componentes principais), e $D$ √© uma matriz diagonal com os valores singulares [^4.5]. As colunas de $U$ s√£o denotadas por $u_m$, para $m=1,\ldots,p$. As componentes principais s√£o definidas como $z_m = X u_m$
2.  **Regress√£o Linear:** Em seguida, as primeiras $M$ componentes principais (onde $M < p$) s√£o usadas como preditores em um modelo de regress√£o linear para prever a vari√°vel resposta $y$. O modelo de regress√£o PCR √© dado por:
    $$ \hat{y}_{PCR} = \hat{\beta_0} + \sum_{m=1}^{M} \hat{\theta}_m z_m  $$
     onde $\hat{\theta}_m = (z_m,y) / (z_m,z_m)$ e $(z_m,y)$ s√£o os produtos internos.
A PCR reduz o problema da multicolinearidade, pois os componentes principais s√£o ortogonais entre si e, portanto, n√£o correlacionados [^4.5], o que leva a uma solu√ß√£o de m√≠nimos quadrados com boa estabilidade e menos vari√¢ncia do que a regress√£o linear com os preditores originais.

```mermaid
graph LR
    subgraph "Principal Component Regression (PCR)"
        direction TB
        A["Input Predictors: X"] --> B["PCA: X = UDV·µÄ"]
        B --> C["Principal Components (PCs): Z = XU"]
         C --> D["Select Top M PCs: Z_M"]
         D --> E["Linear Regression: ≈∑ = Œ≤‚ÇÄ + Œ£Œ∏_m * z_m"]
        F["Response Variable: y"]
        F --> E
    end
```

> üí° **Exemplo Num√©rico:** Vamos usar a mesma matriz $X$ anterior, mas agora com valores centrados na m√©dia:
> ```python
> from sklearn.decomposition import PCA
> from sklearn.preprocessing import StandardScaler
>
> X = np.array([[2, 3], [2.1, 3.1], [2.2, 3.2], [5, 6], [5.1, 6.1], [5.2, 6.2]])
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)
> pca = PCA(n_components=2)
> pca.fit(X_scaled)
> Z = pca.transform(X_scaled)
> print("Componentes principais Z:\n", Z)
> print("Autovetores:\n", pca.components_)
> ```
> As componentes principais $Z$ s√£o combina√ß√µes lineares das vari√°veis originais, e s√£o ortogonais entre si. Por exemplo, a primeira componente principal (primeira coluna de Z) captura a maior parte da varia√ß√£o em X, enquanto a segunda captura a varia√ß√£o residual. O autovetores mostram as dire√ß√µes que definem os componentes principais no espa√ßo original de $X$.
> Se a vari√°vel resposta for $y = [4, 4.2, 4.4, 7, 7.2, 7.4]$, podemos agora usar $Z$ como preditores no modelo:
> ```python
> y = np.array([4, 4.2, 4.4, 7, 7.2, 7.4])
> theta_1 = np.dot(Z[:,0], y) / np.dot(Z[:,0], Z[:,0])
> theta_2 = np.dot(Z[:,1], y) / np.dot(Z[:,1], Z[:,1])
> print(f"theta_1: {theta_1:.3f}")
> print(f"theta_2: {theta_2:.3f}")
> beta0 = np.mean(y) - theta_1*np.mean(Z[:,0]) - theta_2*np.mean(Z[:,1])
> y_hat = beta0 + theta_1*Z[:,0] + theta_2*Z[:,1]
> print("y_hat:", y_hat)
> ```
> Note que o modelo agora usa como preditores as vari√°veis $Z_1$ e $Z_2$, que s√£o ortogonais (n√£o correlacionadas), removendo o problema da multicolinearidade. Al√©m disso, se usarmos apenas a primeira componente $Z_1$ ($M=1$), podemos simplificar o modelo.

##### Corol√°rio 1

A vari√¢ncia dos coeficientes da regress√£o PCR √© menor que a vari√¢ncia dos coeficientes da regress√£o linear padr√£o, pois a matriz $Z^T Z$, onde $Z$ √© a matriz dos componentes principais, √© diagonal e, portanto, bem condicionada. Al√©m disso, ao selecionar um subconjunto das componentes principais com maior vari√¢ncia ($M < p$), a PCR tamb√©m reduz a complexidade do modelo, mitigando o problema de overfitting. [^4.5]

```mermaid
graph TB
    subgraph "PCR Variance Reduction"
      direction TB
      A["PCR Coefficient Variance"]
      B["Z·µÄZ (PCs are orthogonal)"]
      C["Diagonal Z·µÄZ, well-conditioned"]
      D["Reduced Variance of Coefficients"]
      E["Selecting M < p PCs"]
      F["Reduced Model Complexity"]
      G["Mitigates Overfitting"]
      A --> B
      B --> C
      C --> D
      D --> E
       E --> F
       F --> G
    end
```

#### Conceito 3: Partial Least Squares (PLS)

A **Partial Least Squares (PLS)** √© outra t√©cnica que deriva componentes ortogonais dos preditores originais, mas, ao contr√°rio da PCR, ela usa informa√ß√µes sobre a vari√°vel resposta $y$ para construir essas componentes [^4.5.2]. O algoritmo PLS opera de forma iterativa, construindo novos componentes que maximizem a covari√¢ncia entre os preditores e a resposta. O procedimento PLS pode ser resumido nos seguintes passos [^4.5.2]:
1.  **Inicializa√ß√£o:** Calcula-se a covari√¢ncia entre cada preditor original $x_j$ e a vari√°vel resposta $y$.
2.  **Constru√ß√£o da primeira componente:** Combina-se as vari√°veis originais $x_j$ usando as covari√¢ncias calculadas na etapa anterior como pesos, criando uma nova vari√°vel ou componente $z_1$.
     $$ z_1 = \sum_{j=1}^p \phi_{1j} x_j $$ onde $\phi_{1j} = (x_j,y)$
3.  **Regress√£o e Ortogonaliza√ß√£o:** A vari√°vel resposta $y$ √© regredida em $z_1$ para obter um coeficiente $\theta_1$, e em seguida os preditores originais $x_j$ s√£o ortogonalizados com rela√ß√£o a $z_1$.
     $$ x_j^{(1)} = x_j - \frac{(z_1, x_j)}{(z_1, z_1)} z_1$$
4.  **Itera√ß√£o:** O processo √© repetido com os preditores ortogonalizados para obter uma nova componente $z_2$, e assim por diante, at√© que $M$ componentes tenham sido constru√≠das.
5.  **Regress√£o:** Finalmente, a regress√£o √© feita em $y$ com as componentes $z_m$.
      $$ \hat{y}_{PLS} =  \hat{\beta_0} + \sum_{m=1}^{M} \hat{\theta}_m z_m $$

A PLS tem como objetivo encontrar um conjunto de componentes que sejam relevantes tanto para a descri√ß√£o dos preditores quanto para a previs√£o da vari√°vel resposta.  PLS pode ser vantajoso em rela√ß√£o a PCR em situa√ß√µes em que nem todas as vari√°veis preditoras s√£o altamente relevantes para a resposta [^4.5.2].

```mermaid
graph LR
    subgraph "Partial Least Squares (PLS)"
        direction TB
        A["Predictors X, Response y"] --> B["Covariance: œÜ_1j = (x_j, y)"]
        B --> C["Component 1: z‚ÇÅ = Œ£œÜ_1j * x_j"]
        C --> D["Regression: y on z‚ÇÅ"]
        D --> E["Orthogonalization: x_j^(1) = x_j - (z‚ÇÅ, x_j)/(z‚ÇÅ, z‚ÇÅ) * z‚ÇÅ"]
        E --> F["Iterate to get z‚ÇÇ, z‚ÇÉ...z_M"]
        F --> G["Final Regression: ≈∑ = Œ≤‚ÇÄ + Œ£Œ∏_m * z_m"]

    end
```

> üí° **Exemplo Num√©rico:** Usando os mesmos dados $X$ e $y$ anteriores, vamos ilustrar a primeira itera√ß√£o da PLS. Primeiro, calcula-se a covari√¢ncia entre cada coluna de $X$ e $y$:
> ```python
> X = np.array([[2, 3], [2.1, 3.1], [2.2, 3.2], [5, 6], [5.1, 6.1], [5.2, 6.2]])
> y = np.array([4, 4.2, 4.4, 7, 7.2, 7.4])
> phi_11 = np.dot(X[:,0], y)
> phi_12 = np.dot(X[:,1], y)
> print(f"phi_11 (cov entre X1 e y): {phi_11:.2f}")
> print(f"phi_12 (cov entre X2 e y): {phi_12:.2f}")
> ```
> A primeira componente $z_1$ ser√°:
> ```python
> z1 = phi_11 * X[:,0] + phi_12 * X[:,1]
> print("z1:", z1)
> ```
> Em seguida, a vari√°vel resposta $y$ √© regredida em $z_1$:
> ```python
> theta_1 = np.dot(z1, y) / np.dot(z1, z1)
> print(f"theta_1: {theta_1:.3f}")
> ```
> E os preditores originais s√£o ortogonalizados em rela√ß√£o a $z_1$:
> ```python
> z1 = z1.reshape(-1,1)
> x1_ort = X[:,0] - np.dot(z1.flatten(), X[:,0]) / np.dot(z1.flatten(), z1.flatten()) * z1.flatten()
> x2_ort = X[:,1] - np.dot(z1.flatten(), X[:,1]) / np.dot(z1.flatten(), z1.flatten()) * z1.flatten()
> print("x1_ort:", x1_ort)
> print("x2_ort:", x2_ort)
> ```
> O algoritmo PLS continuaria iterando para obter mais componentes, mas o exemplo mostra que, ao contr√°rio do PCR, PLS usa informa√ß√µes de $y$ para guiar a constru√ß√£o das componentes.

> ‚ö†Ô∏è **Nota Importante**: A escolha de $M$ (n√∫mero de componentes a serem retidas) √© crucial tanto na PCR quanto na PLS e √© geralmente feita atrav√©s de t√©cnicas de valida√ß√£o cruzada, como discutido em [^3.3.4] e [^7.10].

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

A regress√£o linear em matriz de indicadores pode ser usada como uma abordagem para problemas de classifica√ß√£o, mas com algumas limita√ß√µes. Nesta abordagem, cada classe √© codificada como uma vari√°vel indicadora (dummy variable), que √© igual a 1 quando uma observa√ß√£o pertence a essa classe e 0 caso contr√°rio. O modelo de regress√£o linear √© ent√£o ajustado para prever a probabilidade de pertencer a cada classe. O modelo tem a seguinte forma:
$$ f(x) = \beta_0 + X\beta $$
onde $f(x)$ √© um vetor de probabilidades de pertencer a cada classe para uma determinada observa√ß√£o $x$. A matriz de design $X$ inclui uma coluna de 1‚Äôs para intercepto e as colunas com as vari√°veis preditoras.

Embora essa abordagem possa parecer simples e direta, ela pode ter problemas [^4.2]:
*   **Interpreta√ß√£o das previs√µes:** As previs√µes do modelo linear podem n√£o se restringir ao intervalo [0,1], o que pode dificultar a interpreta√ß√£o das probabilidades.
*   **Masking problem**: Quando uma vari√°vel pode discriminar as classes de forma igual e oposta (por exemplo, para tr√™s classes, uma vari√°vel sendo alta para a primeira classe e baixa para as outras duas, e outra vari√°vel sendo baixa para a primeira e alta para as outras duas), isso pode levar a erros de regress√£o linear (masking problem).
*   **Sensibilidade a outliers:** A regress√£o de m√≠nimos quadrados √© sens√≠vel a outliers, o que pode afetar a precis√£o da classifica√ß√£o.
*   **Suposi√ß√£o de linearidade**: Sup√µe que as fronteiras de decis√£o entre as classes s√£o lineares, o que nem sempre √© verdadeiro [^4.1].

```mermaid
graph LR
    subgraph "Linear Regression for Classification - Challenges"
        direction TB
        A["Linear Regression as Classifier"] --> B["Predictions not in [0,1]"]
        A --> C["Masking Problem"]
        A --> D["Sensitivity to Outliers"]
        A --> E["Linear Decision Boundary Assumption"]
        B --> F["Difficult Probability Interpretation"]
        C --> G["Classification Errors"]
        D --> H["Reduced Classification Accuracy"]
        E --> I["Not Suitable for Non-Linear Boundaries"]

    end
```

> üí° **Exemplo Num√©rico:** Imagine que temos um problema de classifica√ß√£o com tr√™s classes (A, B, C) e duas vari√°veis preditoras $X_1$ e $X_2$. Codificamos as classes usando vari√°veis indicadoras (dummies). Por exemplo:
> - Classe A: $Y_A = [1, 0, 0]$
> - Classe B: $Y_B = [0, 1, 0]$
> - Classe C: $Y_C = [0, 0, 1]$
>
>  Para cada observa√ß√£o, teremos um vetor de probabilidades preditas para cada classe. Se uma observa√ß√£o tem o vetor predito $\hat{f}(x) = [0.2, 0.7, 0.1]$, a classe mais prov√°vel seria B, embora a regress√£o linear possa predizer valores negativos ou maiores que 1, o que n√£o s√£o probabilidades v√°lidas. Al√©m disso, se as classes n√£o forem linearmente separ√°veis, a regress√£o linear ter√° dificuldade em construir um bom modelo.

> ‚ùó **Ponto de Aten√ß√£o**: M√©todos como a regress√£o log√≠stica s√£o projetados especificamente para problemas de classifica√ß√£o e podem oferecer estimativas de probabilidade mais est√°veis e melhor performance em rela√ß√£o √† regress√£o linear em matriz de indicadores, conforme abordado em [^4.4].

##### Lemma 2

Em um problema de classifica√ß√£o com duas classes, a proje√ß√£o dos dados no hiperplano de decis√£o gerado por regress√£o linear sobre matriz de indicadores √© equivalente (em termos de ordem relativa de dist√¢ncia) √† proje√ß√£o obtida pela An√°lise Discriminante Linear (LDA), quando a matriz de covari√¢ncia √© assumida igual entre as classes. A diferen√ßa fundamental reside na interpreta√ß√£o da sa√≠da: enquanto a regress√£o linear tenta estimar probabilidades (com poss√≠veis resultados fora do intervalo $[0,1]$), a LDA projeta as amostras em um eixo de decis√£o.  [^4.3] e [^4.2].
$$ \blacksquare $$

```mermaid
graph LR
    subgraph "Linear Regression vs LDA in Classification"
        direction LR
        A["Linear Regression on Indicator Matrix"] --> B["Data Projection on Decision Hyperplane"]
         C["Linear Discriminant Analysis (LDA)"] --> D["Data Projection on Decision Axis"]
        B <--> D
        E["Assumption: Equal Covariance"]
        E --> B
        E --> D
        B --> F["Tries to estimate Probabilities (may be outside [0,1])"]
        D --> G["Projects samples on a decision axis"]
    end
```

> üí° **Exemplo Num√©rico**: Considere duas classes, com alguns pontos em 2 dimens√µes: Classe 1: $X_1 = [[1, 2], [1.5, 1.8], [1.2, 2.5]]$ e Classe 2: $X_2 = [[3, 4], [3.5, 3.8], [3.2, 4.5]]$. Se realizarmos uma regress√£o linear sobre uma matriz de indicadores $y=[0,0,0,1,1,1]$ e, depois, aplicarmos LDA, a proje√ß√£o dos dados sobre o hiperplano de decis√£o resultante ser√° a mesma em ambos os m√©todos. A diferen√ßa √© que a regress√£o linear tentar√° produzir um vetor de probabilidades para a classe (que pode estar fora do intervalo [0,1]), enquanto LDA projeta os dados em um eixo de decis√£o para maximizar a separa√ß√£o entre as classes.

##### Corol√°rio 2
Quando se busca a fronteira de decis√£o linear √≥tima e n√£o as estimativas de probabilidades, a regress√£o em matriz de indicadores e a an√°lise discriminante linear produzem resultados similares em termos de classifica√ß√£o e em muitos casos, a regress√£o linear pode ser suficiente. [^4.3].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

A sele√ß√£o de vari√°veis e regulariza√ß√£o s√£o t√©cnicas importantes em modelos de classifica√ß√£o, especialmente quando se trabalha com dados de alta dimensionalidade [^4.5]. A sele√ß√£o de vari√°veis visa identificar e reter apenas as vari√°veis mais relevantes para a previs√£o, enquanto a regulariza√ß√£o adiciona um termo de penalidade √† fun√ß√£o de custo do modelo para evitar overfitting e melhorar a generaliza√ß√£o.

**Regulariza√ß√£o L1 (Lasso):**
A regulariza√ß√£o L1 adiciona uma penalidade proporcional √† soma dos valores absolutos dos coeficientes.  Isso tende a for√ßar alguns coeficientes a exatamente zero, realizando implicitamente a sele√ß√£o de vari√°veis e levando a modelos esparsos. No contexto da regress√£o log√≠stica, a fun√ß√£o de custo com regulariza√ß√£o L1 se torna:
$$ L(\beta) + \lambda \sum_{j=1}^p |\beta_j|$$
onde $L(\beta)$ √© a fun√ß√£o de log-verossimilhan√ßa da regress√£o log√≠stica e  $\lambda$ √© o par√¢metro de regulariza√ß√£o, que controla a intensidade da penalidade.

```mermaid
graph LR
    subgraph "L1 Regularization (Lasso)"
        direction TB
        A["Cost Function L(Œ≤)"]
        B["L1 Penalty: Œª Œ£|Œ≤_j|"]
        C["Combined Objective: L(Œ≤) + Œª Œ£|Œ≤_j|"]
         A --> C
        B --> C
        D["Forces some coefficients to zero"]
        E["Implicit Variable Selection"]
        C --> D
        D --> E
    end
```

> üí° **Exemplo Num√©rico:** Digamos que temos um modelo de regress√£o log√≠stica com 5 preditores e $\lambda = 0.5$. Ap√≥s a otimiza√ß√£o, com regulariza√ß√£o L1 (Lasso), os coeficientes estimados poderiam ser: $\hat{\beta} = [0.8, 0, -0.3, 0, 1.2]$. Observe que os coeficientes $\beta_2$ e $\beta_4$ foram for√ßados a zero. Assim, apenas os preditores $X_1$, $X_3$, e $X_5$ seriam usados no modelo final, efetuando a sele√ß√£o de vari√°veis.

**Regulariza√ß√£o L2 (Ridge):**
A regulariza√ß√£o L2 adiciona uma penalidade proporcional √† soma dos quadrados dos coeficientes. Isso tende a encolher os coeficientes em dire√ß√£o a zero, mas geralmente n√£o os for√ßa a ser exatamente zero, o que significa que a regulariza√ß√£o L2 n√£o realiza explicitamente a sele√ß√£o de vari√°veis. No contexto da regress√£o log√≠stica, a fun√ß√£o de custo com regulariza√ß√£o L2 √©:
$$ L(\beta) + \lambda \sum_{j=1}^p \beta_j^2$$

```mermaid
graph LR
    subgraph "L2 Regularization (Ridge)"
        direction TB
        A["Cost Function L(Œ≤)"]
        B["L2 Penalty: Œª Œ£Œ≤_j¬≤"]
        C["Combined Objective: L(Œ≤) + Œª Œ£Œ≤_j¬≤"]
         A --> C
         B --> C
        D["Shrinks Coefficients towards zero"]
        E["Does not perform explicit variable selection"]
         C --> D
         D --> E
    end
```

> üí° **Exemplo Num√©rico:** Usando o mesmo exemplo anterior, com $\lambda = 0.5$, ap√≥s a otimiza√ß√£o com regulariza√ß√£o L2 (Ridge), os coeficientes estimados poderiam ser: $\hat{\beta} = [0.5, 0.1, -0.2, 0.05, 0.9]$. Observe que os coeficientes foram reduzidos, mas nenhum deles foi for√ßado a zero. Isso significa que todos os preditores ainda est√£o no modelo, mas com menor peso.

**Regulariza√ß√£o El√°stica:**
Combina as penalidades L1 e L2 para aproveitar as vantagens de ambos os tipos de regulariza√ß√£o:
$$ L(\beta) + \lambda \left( \alpha \sum_{j=1}^p |\beta_j| + (1-\alpha) \sum_{j=1}^p \beta_j^2\right) $$
onde $\alpha$ √© um par√¢metro que controla o equil√≠brio entre as penalidades L1 e L2 [^4.5].

```mermaid
graph LR
    subgraph "Elastic Net Regularization"
        direction TB
        A["Cost Function L(Œ≤)"]
        B["L1 Penalty: Œ±Œª Œ£|Œ≤_j|"]
        C["L2 Penalty: (1-Œ±)Œª Œ£Œ≤_j¬≤"]
        D["Combined Objective: L(Œ≤) + Œª(Œ± Œ£|Œ≤_j| + (1-Œ±) Œ£Œ≤_j¬≤)"]
         A --> D
         B --> D
         C --> D
        E["Combines L1 and L2 benefits"]
        D --> E
    end
```

> üí° **Exemplo Num√©rico:** Com $\lambda = 0.5$ e $\alpha = 0.5$, a regulariza√ß√£o el√°stica com os mesmos dados pode gerar $\hat{\beta} = [0.6, 0, -0.1, 0.02, 1.0]$. A penaliza√ß√£o el√°stica combina L1 e L2, tentando gerar esparsidade como o Lasso e ao mesmo tempo encolhendo os coeficientes como Ridge.

##### Lemma 3
A regulariza√ß√£o L1 em classifica√ß√£o log√≠stica leva a coeficientes esparsos, porque a penalidade da norma L1 tem como caracter√≠stica uma "n√£o-diferenciabilidade" na origem, o que provoca que otimizadores como o gradiente descendente encontrem solu√ß√µes nas quais os coeficientes s√£o exatamente 0, ao inv√©s de simplesmente pr√≥ximos de zero. [^4.4.4]
$$ \blacksquare $$

```mermaid
graph LR
    subgraph "L1 Regularization Sparsity"
        direction TB
         A["L1 Penalty: Œª Œ£|Œ≤_j|"]
        B["Non-differentiability at Œ≤_j = 0"]
        C["Optimization algorithms (e.g., gradient descent)"]
        D["Find solutions where coefficients are exactly zero"]
         A --> B
        B --> C
         C --> D
        E["Leads to sparse models"]
         D --> E
    end
```

##### Prova do Lemma 3

A fun√ß√£o de custo da regress√£o log√≠stica com penaliza√ß√£o L1 √©:
$$ - \sum_{i=1}^{n} \left( y_i \log(p_i) + (1-y_i) \log(1-p_i) \right) + \lambda \sum_{j=1}^p |\beta_j| $$
onde $p_i$ √© a probabilidade estimada para a amostra $i$. A parte que corresponde √† penalidade da norma L1, $\lambda \sum_{j=1}^p |\beta_j|$, √© diferenci√°vel em todos os pontos exceto em $\beta_j = 0$. No ponto onde $\beta_j = 0$, essa fun√ß√£o tem uma quina. A import√¢ncia dessa quina se d√° na deriva√ß√£o do gradiente da fun√ß√£o de custo, onde os coeficientes n√£o-nulos s√£o penalizados linearmente com o fator $\lambda$ ao passo que coeficientes j√° nulos n√£o mudam o valor da fun√ß√£o quando se movem da origem. Portanto, o algoritmo de otimiza√ß√£o tender√° a atingir pontos onde parte das componentes do vetor de coeficientes √© exatamente 0, o que gera esparsidade. [^4.4.4].
$$ \blacksquare $$
##### Corol√°rio 3
A esparsidade induzida pela regulariza√ß√£o L1 leva a modelos mais interpret√°veis, pois apenas os preditores mais relevantes s√£o retidos.  No contexto da regress√£o log√≠stica, a regulariza√ß√£o L1 pode simplificar o modelo, reduzindo o n√∫mero de par√¢metros a serem estimados e, consequentemente, diminuindo o risco de overfitting, e ao mesmo tempo melhorando a interpretabilidade. [^4.4.5]

> ‚ö†Ô∏è **Ponto Crucial**: A regulariza√ß√£o n√£o apenas reduz a vari√¢ncia dos coeficientes, mas tamb√©m pode melhorar a precis√£o de modelos de classifica√ß√£o, especialmente em conjuntos de dados de alta dimensionalidade, ao controlar a complexidade do modelo. [^4.5]

### Separating Hyperplanes e Perceptrons

O conceito de **Separating Hyperplanes** √© fundamental em problemas de classifica√ß√£o linear [^4.5.2]. Em um problema com duas classes, um hiperplano separador √© uma superf√≠cie linear que divide o espa√ßo de entrada em duas regi√µes, correspondendo √†s duas classes. O objetivo √© encontrar o hiperplano que melhor separe as classes, ou seja, que maximize a margem de separa√ß√£o (a dist√¢ncia entre o hiperplano e os pontos mais pr√≥ximos de cada classe).

A formula√ß√£o do problema de otimiza√ß√£o para encontrar o hiperplano separador envolve a maximiza√ß√£o da margem sujeita a restri√ß√µes de que todos os pontos de treinamento estejam corretamente classificados [^4.5.2]. Este problema pode ser resolvido usando t√©cnicas de programa√ß√£o quadr√°tica. No dual de Wolfe, a solu√ß√£o √© expressa em termos de uma combina√ß√£o linear dos pontos de suporte (pontos de treinamento mais pr√≥ximos do hiperplano) [^4.5.2].
A formula√ß√£o matem√°tica do problema pode ser descrita da seguinte maneira:
Minimizar:
$$ \frac{1}{2} ||w||^2 $$
sujeito a:
$$ y_i (w^T x_i + b) \geq 1 $$, para todo $i = 1, \ldots, N$
onde:
* $w$ √© o vetor normal ao hiperplano,
* $b$ √© o bias, que define o deslocamento do hiperplano em rela√ß√£o √† origem,
* $x_i$ s√£o as observa√ß√µes,
* $y_i$ √© a etiqueta da classe (1 ou -1).

```mermaid
graph LR
    subgraph "Separating Hyperplane"
        direction TB
        A["Goal: Linear Surface to Separate Classes"]
        B["Maximize Separation Margin"]
        C["Optimization Problem: Minimize 1/2||w||¬≤"]
        D["Constraint: y·µ¢(w·µÄx·µ¢ + b) ‚â• 1"]
        E["w: Normal Vector to Hyperplane"]
        F["b: Bias term"]
        C --> B
        B --> A
        C --> D
        D --> E
        D --> F

    end
```

O **Perceptron** √© um algoritmo de aprendizado que busca encontrar um hiperplano separador [^4.5.1]. Ele opera de forma iterativa, ajustando o hiperplano com base nos erros de classifica√ß√£o observados nos pontos de treinamento. O Perceptron atualiza os coeficientes sempre que encontra uma observa√ß√£o classificada incorretamente, movendo o hiperplano de forma a corrigir o erro.

> üí° **Exemplo Num√©rico:** Imagine duas classes em 2D, Classe 1 = [(1,1), (2,1), (1,2)] e Classe 2 = [(3,3), (4,3), (3,4)]. As etiquetas seriam $y = [-1,-1,-1, 1,1,1]$. Inicializamos $w = [0.1, 0.2