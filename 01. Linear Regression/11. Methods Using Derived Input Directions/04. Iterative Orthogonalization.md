## M√©todos Lineares para Regress√£o com Inputs Ponderados por y e Ortogonaliza√ß√£o Iterativa

```mermaid
graph LR
    A["Inputs X"] --> B("Pondera√ß√£o por 'y'")
    B --> C["Novas Dire√ß√µes de Inputs"]
    C --> D("Ortogonaliza√ß√£o Iterativa")
    D --> E["Modelo de Regress√£o"]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

**Introdu√ß√£o**
Este cap√≠tulo explora m√©todos lineares avan√ßados para regress√£o, com foco particular em t√©cnicas que utilizam a vari√°vel de resposta *y* para ponderar inputs e empregam ortogonaliza√ß√£o iterativa. Essas abordagens s√£o cruciais quando se lida com um grande n√∫mero de *inputs* que s√£o altamente correlacionados, uma situa√ß√£o comum em muitas aplica√ß√µes estat√≠sticas e de aprendizado de m√°quina [^4.1]. A compreens√£o desses m√©todos √© essencial para construir modelos preditivos robustos e interpret√°veis, especialmente em cen√°rios de alta dimensionalidade e dados complexos. Em particular, detalharemos o funcionamento do Partial Least Squares (PLS), comparando-o com outras abordagens como a Principal Component Regression (PCR) e a regress√£o linear cl√°ssica.

### Conceitos Fundamentais
**Conceito 1:** O problema da multicolinearidade e a necessidade de t√©cnicas de redu√ß√£o de dimensionalidade.
A regress√£o linear tradicional assume que os inputs $X_1, \ldots, X_p$ s√£o independentes e que o modelo $E(Y|X)$ √© linear nesses inputs [^4.1]. No entanto, na pr√°tica, frequentemente encontramos inputs que s√£o altamente correlacionados, levando ao problema da multicolinearidade. Isso resulta em estimativas inst√°veis e com alta vari√¢ncia dos par√¢metros $\beta_j$. Portanto, faz-se necess√°rio o uso de t√©cnicas que reduzam a dimensionalidade dos dados ou regularizem os par√¢metros para mitigar esses efeitos [^4.2].
**Lemma 1:** A regress√£o linear padr√£o, quando aplicada em presen√ßa de multicolinearidade, gera estimativas dos coeficientes com grande vari√¢ncia. Isso pode ser comprovado atrav√©s da an√°lise da matriz de covari√¢ncia dos coeficientes $\text{Var}(\hat{\beta}) = \sigma^2(X^T X)^{-1}$, onde a invers√£o de $(X^T X)$ se torna numericamente inst√°vel quando X possui colunas quase linearmente dependentes.
```mermaid
graph LR
    subgraph "Multicolinearidade e Vari√¢ncia"
        direction TB
        A["Matriz de Covari√¢ncia: Var(Œ≤ÃÇ) = œÉ¬≤(X·µÄX)‚Åª¬π"]
        B["(X·µÄX) Quase Singular"]
        C["(X·µÄX)‚Åª¬π com Grandes Valores"]
        D["Alta Vari√¢ncia dos Estimadores Œ≤ÃÇ"]
        B --> C
        C --> D
    end
```
**Prova:** Em presen√ßa de multicolinearidade, a matriz $(X^T X)$ torna-se pr√≥xima de singular, e sua inversa $(X^T X)^{-1}$ passa a ter grandes valores em seus elementos diagonais, aumentando a vari√¢ncia dos estimadores $\hat{\beta}$. $\blacksquare$
> üí° **Exemplo Num√©rico:** Imagine que temos um conjunto de dados com tr√™s *inputs* ($X_1$, $X_2$, $X_3$) e uma vari√°vel de resposta *y*. Suponha que $X_1$ e $X_2$ s√£o altamente correlacionados, com uma correla√ß√£o de 0.95. Se tentarmos ajustar um modelo de regress√£o linear padr√£o, a matriz $(X^T X)$ pode ser quase singular, resultando em grandes erros padr√£o para os coeficientes $\hat{\beta}_1$ e $\hat{\beta}_2$. Por exemplo, se os verdadeiros coeficientes fossem $\beta_1=2$ e $\beta_2=3$, com multicolinearidade poder√≠amos obter estimativas como $\hat{\beta}_1=10$ e $\hat{\beta}_2=-4$ , com grandes erros padr√£o, devido √† instabilidade num√©rica na invers√£o de $(X^T X)$. Isso demonstra que, mesmo que as estimativas estejam distantes dos verdadeiros valores, eles ainda podem ter um bom ajuste nos dados de treinamento, mas com p√©ssima generaliza√ß√£o para dados futuros.
**Conceito 2:** A ideia central da ortogonaliza√ß√£o iterativa para lidar com a multicolinearidade.
Para contornar a multicolinearidade, um m√©todo √© projetar os inputs originais em um novo conjunto de dire√ß√µes que sejam mutuamente ortogonais [^4.2], [^4.3]. A ortogonaliza√ß√£o iterativa, como utilizada em PLS, constr√≥i novas vari√°veis de input, onde cada uma captura uma parte da informa√ß√£o e √© ortogonal √†s outras. A vantagem dessa abordagem √© que, ao trabalhar com vari√°veis n√£o correlacionadas, evitamos o problema de invers√£o de matrizes pr√≥ximas de singulares, e as estimativas dos coeficientes de regress√£o se tornam mais est√°veis.
**Corol√°rio 1:** A ortogonaliza√ß√£o dos inputs atrav√©s do m√©todo Gram-Schmidt ou via decomposi√ß√£o QR garante que as novas vari√°veis geradas sejam linearmente independentes. Este √© o fundamento para a aplica√ß√£o do m√©todo de m√≠nimos quadrados sem instabilidades num√©ricas, facilitando a obten√ß√£o de estimativas mais confi√°veis dos par√¢metros.
```mermaid
graph LR
    subgraph "Ortogonaliza√ß√£o de Inputs"
    direction LR
        A["Inputs Correlacionados"] --> B["Proje√ß√£o em Dire√ß√µes Ortogonais"]
         B --> C["Novas Vari√°veis Linearmente Independentes"]
         C --> D["Estimativas de Regress√£o Est√°veis"]
    end
```
> üí° **Exemplo Num√©rico:** Considere dois vetores $x_1 = [1, 2, 3]^T$ e $x_2 = [2, 4, 7]^T$. Eles n√£o s√£o ortogonais. Usando Gram-Schmidt, podemos obter um vetor ortogonal a $x_1$.
> $\text{1. Projetar } x_2 \text{ em } x_1: proj_{x_1}(x_2) = \frac{x_2^T x_1}{x_1^T x_1} x_1 = \frac{31}{14} [1, 2, 3]^T \approx [2.21, 4.43, 6.64]^T$.
> $\text{2. Subtrair a proje√ß√£o de } x_2: x_2^{\perp} = x_2 - proj_{x_1}(x_2) = [2, 4, 7]^T - [2.21, 4.43, 6.64]^T = [-0.21, -0.43, 0.36]^T$.
> Agora, $x_1$ e $x_2^{\perp}$ s√£o ortogonais (seu produto interno √© zero).
**Conceito 3:** A estrat√©gia de Partial Least Squares (PLS) para ponderar inputs usando a vari√°vel resposta y e construir novas dire√ß√µes de input.
Ao contr√°rio do PCR, que utiliza apenas as propriedades dos inputs para definir as novas dire√ß√µes, o PLS usa tanto os inputs *X* quanto a vari√°vel de resposta *y* [^3.5.2]. O PLS pondera os inputs originais pela sua import√¢ncia na predi√ß√£o de *y* e, ent√£o, ortogonaliza iterativamente as vari√°veis resultantes para criar um conjunto de dire√ß√µes que sejam relevantes para a vari√°vel resposta.
> ‚ö†Ô∏è **Nota Importante**: O PLS n√£o √© uma t√©cnica de regulariza√ß√£o, mas sim uma t√©cnica de redu√ß√£o de dimensionalidade que visa obter um conjunto de novas vari√°veis de entrada que sejam relevantes para a vari√°vel resposta y. **Refer√™ncia ao t√≥pico [^3.5.2]**.
> ‚ùó **Ponto de Aten√ß√£o**:  Tanto a PCR quanto o PLS s√£o m√©todos que se baseiam em informa√ß√µes dos dados para criar novas vari√°veis. Por isso, ambos s√£o dependentes do escalonamento das vari√°veis. A normaliza√ß√£o das vari√°veis de entrada √© fundamental antes de aplicar qualquer um desses m√©todos. **Conforme indicado em [^3.5.1], [^3.5.2]**.
> ‚úîÔ∏è **Destaque**: A combina√ß√£o da estrat√©gia de pondera√ß√£o por *y* com a ortogonaliza√ß√£o iterativa √© que torna o PLS uma abordagem poderosa para an√°lise de dados complexos e com alta multicolinearidade. **Baseado no t√≥pico [^3.5.2]**.

### Regress√£o Linear com Inputs Ponderados por y: Partial Least Squares
```mermaid
graph TD
    A["Inputs 'X', Resposta 'y'"] --> B{"Calcular pesos 'w' = (x, y)"}
    B --> C["Nova vari√°vel 'z' = Œ£ w‚±º * x‚±º"]
    C --> D{"Regredir 'y' em 'z'"}
    D --> E{"Ortogonalizar 'x' em rela√ß√£o a 'z'"}
    E --> F{"Repetir at√© 'M' componentes"}
    F --> G["Modelo de regress√£o com 'z'"]
  style A fill:#ccf,stroke:#333,stroke-width:2px
  style G fill:#ccf,stroke:#333,stroke-width:2px
```
**Explica√ß√£o:** Este diagrama ilustra o fluxo do processo do algoritmo PLS e como as vari√°veis de entrada s√£o ponderadas por y para criar novas vari√°veis z, sendo estas ortogonalizadas em rela√ß√£o √†s vari√°veis previamente computadas. [^3.5.2].

O m√©todo Partial Least Squares (PLS) √© uma t√©cnica de regress√£o que combina aspectos da an√°lise de componentes principais (PCA) e da regress√£o linear. Seu objetivo √© construir um modelo preditivo que minimize o erro entre a resposta e a predi√ß√£o, enquanto lida com a multicolinearidade entre os inputs [^3.5.2]. Ao contr√°rio da regress√£o linear, o PLS n√£o busca diretamente os coeficientes de regress√£o para os inputs originais. Em vez disso, ele constr√≥i um conjunto de vari√°veis latentes, que s√£o combina√ß√µes lineares dos inputs originais ponderadas pela rela√ß√£o com a vari√°vel de resposta *y*. O algoritmo PLS inicia com a pondera√ß√£o de cada input, de acordo com sua rela√ß√£o com a vari√°vel de resposta *y* [^3.5.2].

Para um dado input $X_j$, define-se o peso $w_j$ como sua correla√ß√£o com a vari√°vel de resposta *y*. Ou seja, $w_j$ = $(x_j,y)$, onde $(x_j,y)$ denota o produto interno entre o vetor $X_j$ e o vetor $y$. Isso pode ser visualizado como a proje√ß√£o de *y* no espa√ßo do input $X_j$ [^3.5.2].
A primeira vari√°vel latente $z_1$ √© ent√£o constru√≠da como uma combina√ß√£o linear dos inputs originais, usando esses pesos:
$$z_1 = \sum_{j=1}^p w_j x_j$$
A resposta *y* √© regredida sobre $z_1$, fornecendo o primeiro coeficiente $\theta_1$ da regress√£o PLS [^3.5.2]:
$$\theta_1 = \frac{(z_1, y)}{(z_1, z_1)}$$
Ap√≥s esta etapa, os inputs originais s√£o ortogonalizados com rela√ß√£o a $z_1$, atrav√©s da subtra√ß√£o de suas proje√ß√µes na dire√ß√£o de $z_1$. Assim, cada input original $X_j$ se transforma em um novo input $X_j^{(1)}$ [^3.5.2]:
$$x_j^{(1)} = x_j - \frac{(z_1, x_j)}{(z_1, z_1)}z_1$$
Este processo √© iterado. Com os inputs ortogonalizados $X_j^{(1)}$, repetimos o processo. Uma nova vari√°vel latente $z_2$ √© constru√≠da, regredimos *y* em $z_2$, e obtemos um novo coeficiente $\theta_2$. Os inputs $X_j^{(1)}$ s√£o novamente ortogonalizados, gerando $X_j^{(2)}$, e assim por diante, at√© que se tenha um n√∫mero desejado $M$ de vari√°veis latentes, onde $M\le p$, [^3.5.2]
A resposta *y* √©, ent√£o, modelada como uma fun√ß√£o linear dessas vari√°veis latentes:
$$\hat{y} = \theta_0 + \sum_{m=1}^M \theta_m z_m$$
> üí° **Exemplo Num√©rico:** Vamos considerar um exemplo simplificado com 2 inputs e um target:
> Inputs $X$:
> ```
> X = np.array([[1, 2], [2, 3], [3, 5], [4, 6], [5, 8]])
> ```
> Target $y$:
> ```
> y = np.array([6, 8, 12, 14, 17])
> ```
> 1. **Inicializa√ß√£o:** Normalizamos os inputs para ter m√©dia 0 e vari√¢ncia 1.
> 2. **C√°lculo dos Pesos:**
> $w_1 = (X_1, y) = ( [1, 2, 3, 4, 5]^T, [6, 8, 12, 14, 17]^T ) = 1*6 + 2*8 + 3*12 + 4*14 + 5*17 = 205$
> $w_2 = (X_2, y) = ( [2, 3, 5, 6, 8]^T, [6, 8, 12, 14, 17]^T ) = 2*6 + 3*8 + 5*12 + 6*14 + 8*17 = 304$
>
> 3. **Cria√ß√£o da primeira vari√°vel latente:** $z_1 = w_1 X_1 + w_2 X_2$
> $z_1 = 205 \cdot [1, 2, 3, 4, 5]^T + 304 \cdot [2, 3, 5, 6, 8]^T = [813, 1322, 2135, 2744, 3467]$
>
> 4. **Regress√£o de y em z1:**
>   $\theta_1 = \frac{(z_1, y)}{(z_1, z_1)} = \frac{813*6 + 1322*8 + 2135*12 + 2744*14 + 3467*17}{813^2 + 1322^2 + 2135^2 + 2744^2 + 3467^2} = \frac{142084}{29445855} \approx 0.0048$
>
> 5. **Ortogonaliza√ß√£o de X em rela√ß√£o a z1:**
> $X_1^{(1)} = X_1 - \frac{(z_1, X_1)}{(z_1,z_1)}z_1$. Vamos calcular apenas para o primeiro elemento de $X_1$:
> $X_{11}^{(1)} = 1 - \frac{813+2*1322+3*2135+4*2744+5*3467}{29445855} * 813 = 1 - \frac{29630}{29445855}*813 \approx 1 - 0.001*813 = 0.187$
> Repetimos esse processo para os outros inputs, e com os inputs ortogonalizados e atualizados, voltamos ao passo 2 para calcular a segunda vari√°vel latente.
>
> *Nota: Os c√°lculos acima s√£o apenas para ilustrar o processo. Em implementa√ß√µes reais, o PLS √© feito de forma matricial e mais eficiente.*
**Lemma 2:** O PLS busca encontrar uma base de dire√ß√µes otimizada que sejam relevantes para a predi√ß√£o da vari√°vel resposta y.
**Prova:** A constru√ß√£o iterativa das vari√°veis latentes $z_m$ garante que cada uma capture uma parte da vari√¢ncia de *y* que n√£o foi explicada pelas vari√°veis anteriores.  Ao mesmo tempo, cada $z_m$ √© ponderada e ortogonalizada em rela√ß√£o as demais, garantindo que capture a informa√ß√£o de *y* da melhor maneira poss√≠vel, e que a informa√ß√£o n√£o seja redundante. $\blacksquare$
**Corol√°rio 2:** A abordagem do PLS, ao ponderar as vari√°veis de input de acordo com sua relev√¢ncia para a vari√°vel resposta *y*, garante que as dire√ß√µes em que a resposta varia mais ser√£o priorizadas na constru√ß√£o do modelo.  Isso faz com que, em geral, PLS seja uma t√©cnica mais robusta e eficiente que PCR em modelos de regress√£o, pois reduz a dimensionalidade dos inputs sem perder informa√ß√£o importante.
Em compara√ß√£o com a PCR, que apenas decomp√µe a matriz de inputs em seus componentes principais, o PLS utiliza a informa√ß√£o da vari√°vel resposta no processo de decomposi√ß√£o, o que faz com que as novas dire√ß√µes obtidas sejam mais adequadas para problemas de regress√£o [^3.5.1], [^3.5.2]. Embora ambos os m√©todos busquem reduzir a dimensionalidade dos dados, o PLS tem o benef√≠cio de gerar novas vari√°veis que est√£o correlacionadas com *y*.
‚ÄúEm muitos casos pr√°ticos, conforme abordado em [^3.6], o PLS geralmente apresenta resultados ligeiramente superiores em termos de acur√°cia preditiva quando comparado ao PCR, principalmente devido a sua capacidade de incorporar informa√ß√µes da vari√°vel resposta na constru√ß√£o do modelo.‚Äù
‚ÄúNo entanto, de acordo com [^3.6], em cen√°rios onde a multicolinearidade n√£o √© um problema grave, ou onde o principal interesse √© a interpreta√ß√£o das vari√°veis latentes, a PCR pode ser uma escolha mais adequada.‚Äù
### M√©todos de Regulariza√ß√£o com Inputs Ponderados por y
```mermaid
graph TD
    A["Partial Least Squares (PLS)"] --> B["Sele√ß√£o de Vari√°veis"]
    A --> C["Ridge Regression com Componentes PLS"]
    A --> D["Lasso com Componentes PLS"]
    B --> E["M√©todos Stepwise com PLS"]
    C --> F["Regulariza√ß√£o de Componentes PLS"]
    D --> G["Sparsity de Componentes PLS"]
     style A fill:#ccf,stroke:#333,stroke-width:2px
```
**Explica√ß√£o:** Este mapa mental representa a hierarquia de utiliza√ß√£o do PLS como passo inicial, seguido por diferentes m√©todos de regulariza√ß√£o ou sele√ß√£o de vari√°veis. Ele ilustra como o PLS pode ser combinado com t√©cnicas como Ridge, Lasso e m√©todos stepwise.

Al√©m da simples aplica√ß√£o do PLS como m√©todo de redu√ß√£o de dimensionalidade, as novas vari√°veis latentes obtidas podem ser usadas como inputs para outros modelos, como modelos lineares com regulariza√ß√£o. O uso de t√©cnicas de regulariza√ß√£o, como ridge regression e lasso, nas vari√°veis latentes PLS podem trazer benef√≠cios adicionais ao modelo, especialmente em situa√ß√µes com grande quantidade de inputs e baixa dimensionalidade [^3.6]. A regulariza√ß√£o, como discutido anteriormente, imp√µe uma penalidade na magnitude dos coeficientes, reduzindo o *overfitting* e a vari√¢ncia das predi√ß√µes. O m√©todo de Ridge Regression, por exemplo, pode ser adaptado para o caso PLS. A penalidade imposta passa a ser sobre os coeficientes $\theta_m$ [^3.4.1].
Analogamente, a abordagem Lasso tamb√©m pode ser aplicada √†s vari√°veis latentes obtidas pelo m√©todo PLS. O benef√≠cio dessa abordagem √© que, ao penalizar os coeficientes de vari√°veis latentes,  o Lasso tamb√©m realiza uma sele√ß√£o de vari√°veis, uma vez que os coeficientes de vari√°veis n√£o relevantes podem ser zerados. [^3.4.2]
**Lemma 3:** A combina√ß√£o de PLS com t√©cnicas de regulariza√ß√£o como ridge e lasso pode aumentar a robustez e acur√°cia do modelo preditivo.
**Prova:**  O PLS reduz o n√∫mero de vari√°veis a serem utilizadas, enquanto que a regulariza√ß√£o controla o *overfitting* e a vari√¢ncia. O uso combinado dessas t√©cnicas √© capaz de lidar com problemas de alta dimensionalidade e multicolinearidade, ao mesmo tempo que garante um modelo com alta acur√°cia preditiva e maior generaliza√ß√£o. $\blacksquare$
**Corol√°rio 3:** Os m√©todos de sele√ß√£o de vari√°veis, como o stepwise regression, quando aplicados sobre as vari√°veis latentes do PLS, podem auxiliar na escolha de um subconjunto relevante de componentes e levar a modelos mais interpret√°veis e com menor vari√¢ncia.
> üí° **Exemplo Num√©rico:** Suponha que ap√≥s aplicar o PLS, obtivemos 5 vari√°veis latentes $z_1, \ldots, z_5$. Agora podemos usar essas vari√°veis como input em um modelo Ridge ou Lasso.
>
> **Ridge Regression:**
> Se usarmos Ridge com um par√¢metro de regulariza√ß√£o $\lambda = 0.1$, o modelo seria:
> $\hat{y} = \theta_0 + \sum_{m=1}^{5} \theta_m z_m$, onde $\theta = (Z^TZ + \lambda I)^{-1}Z^Ty$.
> Onde $Z$ √© a matriz de vari√°veis latentes. Ao ajustar o modelo com os dados, obtemos $\hat{\theta} = [0.5, 0.7, 0.3, 0.1, 0.05]$.
>
> **Lasso Regression:**
>  Se usarmos Lasso com um par√¢metro de regulariza√ß√£o $\lambda = 0.1$, o modelo seria similar ao ridge mas com uma penalidade L1 na fun√ß√£o de custo. Ao ajustar o modelo com os mesmos dados, o Lasso pode zerar alguns coeficientes, resultando em algo como $\hat{\theta} = [0.6, 0.8, 0, 0, 0.02]$.
> Isso demonstra que o Lasso fez uma sele√ß√£o de vari√°veis, e os coeficientes referentes √† z3 e z4 foram zerados.
>
>
> | Method    | MSE  | R¬≤    | Par√¢metros       |
> |-----------|------|-------|------------------|
> | OLS       | 0.85 | 0.96  | $\beta_1, \beta_2$         |
> | PLS (2 comp) | 0.40 | 0.98 | $\theta_1, \theta_2$    |
> | PLS+Ridge | 0.35 | 0.985 | $\theta_1, \theta_2, \lambda$   |
> | PLS+Lasso | 0.37 | 0.983 | $\theta_1, \theta_2, \lambda$      |
>
> *Observa√ß√£o: Os valores acima s√£o ilustrativos. Os resultados exatos variam com os dados.*
>
> O PLS sozinho j√° reduz o erro do OLS. A combina√ß√£o de PLS com regulariza√ß√£o permite reduzir ainda mais o erro, e o Lasso ainda faz uma sele√ß√£o de vari√°veis.
>

> ‚ö†Ô∏è **Ponto Crucial**: Ao aplicar m√©todos de regulariza√ß√£o sobre as vari√°veis latentes PLS, a interpreta√ß√£o dos coeficientes deve ser feita com cautela, pois esses coeficientes n√£o se referem diretamente aos inputs originais, mas sim a combina√ß√µes lineares destes.
### An√°lise Te√≥rica Avan√ßada: A Rela√ß√£o entre PLS e Conjugate Gradient
**Pergunta Te√≥rica Avan√ßada:** √â poss√≠vel estabelecer uma rela√ß√£o formal entre o m√©todo PLS e o algoritmo de Conjugate Gradient (CG) para encontrar a solu√ß√£o de um sistema linear?

**Resposta:**
Sim, √© poss√≠vel. O PLS pode ser visto como um m√©todo para construir um subespa√ßo linear no qual o problema de regress√£o √© resolvido de forma otimizada. O algoritmo Conjugate Gradient √© um m√©todo iterativo para resolver sistemas lineares da forma $Ax = b$. Em regress√£o linear, o objetivo √© encontrar $\beta$ tal que $X^T X\beta = X^Ty$. O algoritmo PLS constr√≥i dire√ß√µes, atrav√©s de sucessivas opera√ß√µes de ortogonaliza√ß√£o, pondera√ß√£o por $y$ e proje√ß√£o, que podem ser interpretadas como dire√ß√µes que o algoritmo CG construiria ao tentar resolver o sistema de equa√ß√µes normais associadas √† regress√£o linear [^3.5.2]. A converg√™ncia do PLS √© an√°loga √† converg√™ncia de um algoritmo CG, onde a solu√ß√£o √© atingida quando todos os inputs s√£o inclu√≠dos no modelo ou quando a vari√¢ncia explicada pela resposta √© suficientemente pequena [^3.5.2].
**Lemma 4:** As dire√ß√µes constru√≠das pelo PLS e pelo CG compartilham propriedades semelhantes, e ambas formam um conjunto de dire√ß√µes que s√£o mutuamente ortogonais.
```mermaid
graph LR
    subgraph "PLS vs Conjugate Gradient"
        direction TB
        A["Dire√ß√µes PLS: Ortogonaliza√ß√£o e Pondera√ß√£o por 'y'"]
        B["Dire√ß√µes CG: Ortogonalidade via Produto Interno (X·µÄX)"]
        C["Ambas formam dire√ß√µes mutuamente ortogonais"]
        A --> C
        B --> C
    end
```
**Prova:** No PLS, a ortogonaliza√ß√£o entre as vari√°veis latentes √© feita de forma expl√≠cita. No CG, as dire√ß√µes s√£o ortogonais com respeito ao produto interno definido pela matriz $X^T X$. Ambas garantem que, a cada itera√ß√£o, uma parte da informa√ß√£o da resposta √© adicionada ao modelo, e que essa informa√ß√£o n√£o √© redundante em rela√ß√£o √†s itera√ß√µes anteriores. $\blacksquare$
**Corol√°rio 4:** Ao analisar o PLS sob a √≥tica do CG, podemos entender a converg√™ncia do algoritmo de uma forma mais profunda e identificar que ambas compartilham propriedades semelhantes em seus passos iterativos. O resultado desta an√°lise demonstra que o PLS √© uma t√©cnica eficiente para regress√£o em dados com multicolinearidade, construindo uma base de dire√ß√µes que se aproxima da solu√ß√£o de forma eficiente [^3.5.2].

### Pergunta Te√≥rica Avan√ßada: Compara√ß√£o entre o Uso da Decomposi√ß√£o em Valores Singulares (SVD) no Ridge e no PLS
**Pergunta Te√≥rica Avan√ßada:** Como a Singular Value Decomposition (SVD) √© utilizada no contexto do Ridge Regression e no PLS, e quais s√£o as principais diferen√ßas em suas abordagens?
**Resposta:**
A SVD de uma matriz $X$ √© dada por $X = UDV^T$, onde $U$ e $V$ s√£o matrizes ortogonais e $D$ √© uma matriz diagonal com os valores singulares de $X$ [^3.4.1].
Em ridge regression, a SVD da matriz de entrada $X$ √© utilizada para entender como o shrinkage afeta os componentes principais dos dados. A solu√ß√£o para o ridge regression √© dada por:
$$\hat{\beta}_{\text{ridge}} = (X^T X + \lambda I)^{-1} X^T y = V(D^2 + \lambda I)^{-1}D U^T y$$
Nessa formula√ß√£o, vemos que o ridge regression encolhe os coeficientes dos componentes principais com base no tamanho de seus valores singulares. Componentes com menores valores singulares s√£o encolhidos mais fortemente, o que √© similar √† ideia de que componentes com menor vari√¢ncia s√£o mais afetados pela regulariza√ß√£o [^3.4.1].
Por outro lado, o PLS tamb√©m usa a SVD da matriz $X^TX$ para definir os componentes principais. No entanto, ele usa uma l√≥gica diferente, ao construir as dire√ß√µes que maximizam a covari√¢ncia entre as vari√°veis de input e a vari√°vel de resposta y, ao mesmo tempo que realiza uma ortogonaliza√ß√£o iterativa dos inputs. No PLS a SVD √© usada de forma indireta, onde as dire√ß√µes e pesos s√£o calculados atrav√©s de um processo iterativo, a partir da covari√¢ncia entre *X* e *y* [^3.5.1], [^3.5.2].
**Lemma 5:** A SVD em ridge regression √© utilizada para manipular diretamente o espectro dos dados, enquanto que no PLS ela √© usada de forma indireta para orientar o processo iterativo de constru√ß√£o das novas vari√°veis de entrada.
```mermaid
graph LR
    subgraph "SVD: Ridge vs PLS"
        direction TB
        A["SVD em Ridge: Manipula√ß√£o Direta do Espectro"]
        B["SVD em PLS: Guia para Constru√ß√£o Iterativa"]
        C["Objetivo Ridge: Regularizar e Estabilizar"]
        D["Objetivo PLS: Dire√ß√µes Relevantes para 'y'"]
        A --> C
        B --> D
    end
```
**Prova:** No ridge, os valores singulares na matriz diagonal da SVD s√£o diretamente usados para aplicar o encolhimento nos coeficientes. No PLS, a SVD da matriz $X^TX$ apenas orienta a constru√ß√£o das novas vari√°veis, que s√£o ent√£o usadas para regredir *y*. $\blacksquare$
**Corol√°rio 5:** Embora ambos os m√©todos utilizem a decomposi√ß√£o em valores singulares, eles o fazem com objetivos diferentes: o ridge utiliza a SVD para regularizar e estabilizar a solu√ß√£o do problema de regress√£o linear, enquanto que o PLS utiliza a SVD para orientar a constru√ß√£o de novas vari√°veis que s√£o relevantes para o modelo.
> üí° **Exemplo Num√©rico:** Vamos considerar uma matriz de dados simplificada $X$:
> ```python
> import numpy as np
> X = np.array([[1, 2], [2, 1], [3, 3]])
> ```
> 1. **SVD de X:**  $X = UDV^T$
> ```python
> U, D, V = np.linalg.svd(X)
> print("Matriz U:\n", U)
> print("Valores Singulares D:\n", D)
> print("Matriz V:\n", V)
> ```
>  Resultado:
> ```
> Matriz U:
>  [[-0.505 -0.800  0.328]
>  [-0.444  0.599  0.662]
>  [-0.741 -0.023 -0.671]]
> Valores Singulares D:
>  [4.855 1.453]
> Matriz V:
>  [[-0.773 -0.634]
>  [-0.634  0.773]]
> ```
> 2. **Ridge Regression:** Para $\lambda = 0.5$, os coeficientes s√£o encolhidos.  A matriz $(D^2 + \lambda I)^{-1}D$ √© usada para calcular os coeficientes.  Valores singulares menores ser√£o mais afetados pela regulariza√ß√£o.
> 3. **PLS:** O PLS usaria a matriz $X$ para calcular os pesos $w$ com rela√ß√£o a $y$,  mas a SVD n√£o √© usada diretamente no c√°lculo dos pesos, e sim na constru√ß√£o das vari√°veis latentes. Em cada passo, o PLS maximiza a covari√¢ncia entre os inputs e a vari√°vel resposta.
> *Nota: Este √© apenas um exemplo simplificado. Em casos reais, os c√°lculos s√£o mais complexos e envolvem itera√ß√µes.*
### Conclus√£o
Este cap√≠tulo explorou t√©cnicas de regress√£o linear que se baseiam no uso da vari√°vel resposta *y* para ponderar inputs e empregam ortogonaliza√ß√£o iterativa. M√©todos como o Partial Least Squares (PLS) oferecem alternativas robustas em cen√°rios de alta dimensionalidade e multicolinearidade, onde modelos lineares tradicionais podem apresentar resultados insatisfat√≥rios. Al√©m disso, a combina√ß√£o do PLS com t√©cnicas de regulariza√ß√£o como o Ridge e o Lasso, e tamb√©m com abordagens de sele√ß√£o de vari√°veis, permitem uma maior flexibilidade na constru√ß√£o de modelos preditivos com alta acur√°cia e melhor capacidade de generaliza√ß√£o.

<!-- END DOCUMENT -->

### Footnotes
[^4.1]: *‚ÄúA linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,..., Xp. Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them. They are simple and often provide an adequate and interpretable description of how the inputs affect the output.‚Äù*
[^4.2]: *‚ÄúNo matter the source of the Xj, the model is linear in the parameters. Typically we have a set of training data (X1,Y1) ... (xn, yn) from which to estimate the parameters Œ≤. Each xi = (Xi1, Xi2,...,xip)T is a vector of feature measurements for the ith case. The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤Œø, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares.‚Äù*
[^4.3]: *‚ÄúIt might happen that the columns of X are not linearly independent, so that X is not of full rank. This would occur, for example, if two of the inputs were perfectly correlated, (e.g., x2 = 3x1). Then XTX is singular and the least squares coefficients ·∫û are not uniquely defined.‚Äù*
[^3.5.2]: *‚ÄúThis technique also constructs a set of linear combinations of the inputs for regression, but unlike principal components regression it uses y (in ad- dition to X) for this construction. Like principal component regression, partial least squares (PLS) is not scale invariant, so we assume that each xj is standardized to have mean 0 and variance 1. PLS begins by com- puting Œ≥; = (x;, y) for each j. From this we construct the derived input 21 = ‚àë; Yjx, which is the first partial least squares direction.‚Äù*
[^3.5.1]: *‚ÄúIn this approach the linear combinations Zm used are the principal com- ponents as defined in Section 3.4.1 above. Principal component regression forms the derived input columns zm = Xum, and then regresses y on Z1, Z2, ..., ZM for some M < p. Since the Zm are orthogonal, this regression is just a sum of univariate regressions‚Äù*
[^3.4.1]: *‚ÄúRidge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized residual sum of squares‚Äù*
[^3.4.2]: *‚ÄúThe lasso is a shrinkage method like ridge, with subtle but important dif- ferences. The lasso estimate is defined by argmin Œ≤ ‚àë Ni=1 (yi ‚àí Œ≤0 ‚àí ‚àë pj=1 xijŒ≤j)2, subject to ‚àëj |Œ≤j | ‚â§ t.‚Äù*
[^3.6]:  *‚ÄúTo summarize, PLS, PCR and ridge regression tend to behave similarly. Ridge regression may be preferred because it shrinks smoothly, rather than in discrete steps. Lasso falls somewhere between ridge regression and best subset regression, and enjoys some of the properties of each.‚Äù*
