## Partial Least Squares (PLS): Constructing Inputs Based on Response and Predictors

```mermaid
graph LR
    subgraph "PLS vs. PCA vs. Linear Regression"
        direction LR
        A["Predictors (X)"] --> B["PCA: Uses only X"];
        A --> C["PLS: Uses X and Y"];
        A --> D["Linear Regression: Directly uses X"];
        E["Response (Y)"] --> C;
        B --> F["Outputs: Directions based on X variance"];
        C --> G["Outputs: Directions maximizing covariance(X,Y)"];
        D --> H["Outputs: Prediction based on X"];
        F --> I["Prediction using Z (PCA)"];
        G --> J["Prediction using Z (PLS)"];
    end
```

### Introdu√ß√£o

Neste cap√≠tulo, abordamos uma variedade de m√©todos para lidar com problemas de regress√£o envolvendo um grande n√∫mero de preditores, frequentemente correlacionados. O Partial Least Squares (PLS) destaca-se como uma t√©cnica que, diferentemente do PCA, utiliza informa√ß√µes tanto dos preditores ($X$) quanto da resposta ($Y$) para construir dire√ß√µes que maximizem a vari√¢ncia e a correla√ß√£o, visando otimizar a predi√ß√£o [^3.5]. O PLS √© particularmente √∫til em cen√°rios onde a resposta est√° fortemente relacionada aos preditores, oferecendo uma alternativa eficaz √† regress√£o linear tradicional e ao PCA.

### Conceitos Fundamentais

**Conceito 1: A Necessidade de Derivar Inputs em Problemas de Alta Dimensionalidade**

Em problemas de regress√£o com um n√∫mero elevado de preditores (alta dimensionalidade), os m√©todos tradicionais de regress√£o linear podem apresentar desafios devido √† multicolinearidade e √† alta vari√¢ncia das estimativas dos par√¢metros. Nesses cen√°rios, torna-se vantajoso derivar um conjunto reduzido de novas vari√°veis (inputs) que capturem a maior parte da informa√ß√£o relevante para prever a resposta [^3.5].

**Lemma 1:** *A multicolinearidade nos preditores ($X$) leva √† instabilidade e alta vari√¢ncia nas estimativas dos par√¢metros de modelos de regress√£o linear*. Este lemma destaca a motiva√ß√£o para a necessidade de t√©cnicas como o PLS, que visam reduzir a dimensionalidade e mitigar tais problemas, utilizando uma combina√ß√£o de informa√ß√µes dos preditores e da resposta [^3.5.2].

> üí° **Exemplo Num√©rico:**
>
> Imagine um cen√°rio de regress√£o com 10 preditores ($x_1, x_2, ..., x_{10}$), onde $x_1$ e $x_2$ s√£o altamente correlacionados (por exemplo, correla√ß√£o de 0.95). A matriz de correla√ß√£o dos preditores ($X^TX$) ter√° elementos fora da diagonal significativamente grandes, indicando multicolinearidade. Em uma regress√£o linear, a matriz $(X^TX)^{-1}$ ter√° valores muito grandes, amplificando o ru√≠do e levando a coeficientes $\beta$ inst√°veis e com alta vari√¢ncia. O PLS tenta resolver isso construindo combina√ß√µes lineares dos preditores que minimizam essa instabilidade.
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> # Simula√ß√£o de dados com multicolinearidade
> np.random.seed(42)
> n_samples = 100
> X = np.random.rand(n_samples, 10)
> X[:, 1] = 0.95 * X[:, 0] + 0.05 * np.random.rand(n_samples)  # x1 e x2 correlacionados
> y = 2 * X[:, 0] + 3 * X[:, 3] + np.random.randn(n_samples)
>
> # Regress√£o linear padr√£o
> model = LinearRegression()
> model.fit(X, y)
> print(f"Coeficientes da regress√£o linear: {model.coef_}")
> # Coeficientes podem ser inst√°veis e grandes devido √† multicolinearidade
>
> # O PLS tenta gerar novas features que minimizem essa instabilidade
> # (o c√≥digo para PLS ser√° mostrado em exemplos posteriores)
> ```
>
> Este exemplo mostra como a multicolinearidade pode afetar os coeficientes de um modelo de regress√£o linear, justificando a necessidade de m√©todos como o PLS.

**Conceito 2: Deriva√ß√£o de Dire√ß√µes em PLS**

O PLS constr√≥i um conjunto de dire√ß√µes latentes ($Z_m$), que s√£o combina√ß√µes lineares dos preditores originais ($X$), com o objetivo de maximizar a covari√¢ncia entre as dire√ß√µes e a resposta ($Y$). O processo √© iterativo, onde as dire√ß√µes subsequentes ($Z_2, Z_3,...$) s√£o constru√≠das de forma ortogonal √†s anteriores [^3.5.2]. Essa abordagem contrasta com o PCA, que se concentra apenas na variabilidade dos preditores, sem considerar a rela√ß√£o com a resposta [^3.5.1].

```mermaid
graph LR
    subgraph "PLS Direction Derivation"
        direction TB
        A["Predictors (X)"]
        B["Response (Y)"]
        C["Covariance Calculation: cov(X,Y)"]
        D["Latent Direction Z_1"]
        E["Orthogonalization of X with respect to Z_1"]
        F["Latent Direction Z_2"]
        G["Iterate to get Z_m"]
        A --> C
        B --> C
        C --> D
        D --> E
        E --> F
        F --> G
     end
```

**Corol√°rio 1:** *A maximiza√ß√£o da covari√¢ncia entre as dire√ß√µes latentes ($Z_m$) e a resposta ($Y$) garante que o PLS capture as dire√ß√µes mais relevantes para a predi√ß√£o, levando a um modelo mais parcimonioso e com maior poder preditivo*. Esse resultado √© essencial para justificar o uso do PLS em compara√ß√£o com m√©todos que ignoram a rela√ß√£o entre preditores e resposta.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos dois preditores, $x_1$ e $x_2$, e uma resposta $y$. No PLS, a primeira dire√ß√£o latente $z_1$ ser√° constru√≠da de forma que a covari√¢ncia entre $z_1$ e $y$ seja maximizada. Se $x_1$ tem uma forte rela√ß√£o com $y$, enquanto $x_2$ n√£o, o PLS dar√° um peso maior a $x_1$ na forma√ß√£o de $z_1$. Em contraste, o PCA criaria dire√ß√µes baseadas apenas na vari√¢ncia de $x_1$ e $x_2$, sem considerar a rela√ß√£o com $y$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.decomposition import PCA
> from sklearn.cross_decomposition import PLSRegression
>
> # Dados simulados
> np.random.seed(42)
> n_samples = 100
> x1 = np.random.rand(n_samples)
> x2 = 0.2 * x1 + 0.8 * np.random.rand(n_samples) # x2 correlacionado fracamente com x1
> y = 3 * x1 + 0.1 * np.random.randn(n_samples)
> X = np.column_stack((x1, x2))
>
> # PLS
> pls = PLSRegression(n_components=1)
> pls.fit(X, y)
> z_pls = pls.transform(X)
>
> # PCA
> pca = PCA(n_components=1)
> pca.fit(X)
> z_pca = pca.transform(X)
>
> # Plot
> plt.figure(figsize=(10, 5))
> plt.subplot(1, 2, 1)
> plt.scatter(z_pls, y, label='PLS')
> plt.xlabel('z_pls')
> plt.ylabel('y')
> plt.title('PLS: z1 vs y')
>
> plt.subplot(1, 2, 2)
> plt.scatter(z_pca, y, label='PCA')
> plt.xlabel('z_pca')
> plt.ylabel('y')
> plt.title('PCA: z1 vs y')
> plt.legend()
> plt.tight_layout()
> plt.show()
>
> # PLS maximiza a rela√ß√£o entre z1 e y, PCA maximiza a vari√¢ncia em x
> # Ao analisar os gr√°ficos, nota-se que a rela√ß√£o entre z_pls e y √© mais evidente que entre z_pca e y
> ```
>
> O c√≥digo acima ilustra como a dire√ß√£o latente gerada pelo PLS ($z_{pls}$) est√° mais correlacionada com a resposta $y$ do que a dire√ß√£o gerada pelo PCA ($z_{pca}$).

**Conceito 3: Processo Iterativo do PLS**

O PLS realiza uma s√©rie de etapas para construir as dire√ß√µes latentes, conforme descrito em [^3.5.2] e detalhado no Algoritmo 3.3. Primeiro, computa-se a covari√¢ncia entre cada preditor ($x_j$) e a resposta ($y$). Em seguida, a primeira dire√ß√£o latente ($z_1$) √© criada como uma combina√ß√£o linear dos preditores, ponderada pelas covari√¢ncias. Subsequentemente, a resposta √© regredida em $z_1$, e os preditores s√£o ortogonalizados em rela√ß√£o a $z_1$, preparando o caminho para a pr√≥xima itera√ß√£o.

### Regress√£o com Partial Least Squares (PLS)

```mermaid
graph LR
 subgraph "PLS Regression Flow"
    direction TB
    A["Predictors (X)"]
    B["Response (Y)"]
    C["Calculate Covariance: cov(X,Y)"]
    D["Construct Latent Directions (Z) based on cov(X,Y)"]
    E["Regress Y on Z"]
    A --> C
    B --> C
    C --> D
    D --> E
 end
```

O PLS aborda o desafio da alta dimensionalidade e da multicolinearidade derivando novas vari√°veis (dire√ß√µes) que s√£o constru√≠das utilizando informa√ß√µes tanto dos preditores quanto da resposta. Cada dire√ß√£o $z_m$ √© uma combina√ß√£o linear dos preditores originais $x_j$, e o processo de deriva√ß√£o √© projetado para maximizar a covari√¢ncia entre as dire√ß√µes e a resposta [^3.5.2].

**Lemma 2:** *A escolha dos pesos na combina√ß√£o linear dos preditores, para formar as dire√ß√µes latentes, √© guiada pela rela√ß√£o com a resposta ($Y$)*. Diferentemente do PCA, que constr√≥i dire√ß√µes apenas com base na varia√ß√£o dos preditores, o PLS pondera os preditores de forma a maximizar a covari√¢ncia com a resposta, resultando em dire√ß√µes que s√£o mais relevantes para a predi√ß√£o [^3.5.2].

**Prova do Lemma 2:** O PLS inicia calculando a covari√¢ncia entre cada preditor $x_j$ e a resposta $y$, denotada por $\phi_{mj} = (x_j,y)$ (onde $(.,.)$ representa o produto interno). Esses valores s√£o ent√£o usados para formar a primeira dire√ß√£o latente $z_1$, como $z_1 = \sum_j \phi_{1j}x_j$. Esta pondera√ß√£o garante que os preditores mais correlacionados com a resposta tenham maior peso na forma√ß√£o da dire√ß√£o latente. A cada itera√ß√£o $m$, as covari√¢ncias s√£o calculadas novamente e um processo similar √© executado para cada dire√ß√£o, assegurando que as dire√ß√µes s√£o constru√≠das tendo como objetivo maximizar a covari√¢ncia com a resposta. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simplificado com 3 preditores ($x_1, x_2, x_3$) e uma resposta $y$. Suponha que ap√≥s centralizar as vari√°veis temos as seguintes covari√¢ncias com a resposta:
>
> $\phi_{11} = \text{cov}(x_1, y) = 0.8$
> $\phi_{12} = \text{cov}(x_2, y) = 0.2$
> $\phi_{13} = \text{cov}(x_3, y) = 0.5$
>
> A primeira dire√ß√£o latente $z_1$ seria calculada como:
>
> $z_1 = 0.8x_1 + 0.2x_2 + 0.5x_3$
>
> Note que o preditor $x_1$, que tem a maior covari√¢ncia com $y$, tem o maior peso na forma√ß√£o de $z_1$.
>
> ```python
> import numpy as np
> from sklearn.cross_decomposition import PLSRegression
>
> # Simula√ß√£o de dados
> np.random.seed(42)
> n_samples = 100
> X = np.random.rand(n_samples, 3)
> y = 0.8 * X[:, 0] + 0.2 * X[:, 1] + 0.5 * X[:, 2] + 0.1* np.random.randn(n_samples)
>
> # PLS com 1 componente
> pls = PLSRegression(n_components=1)
> pls.fit(X, y)
>
> # Coeficientes dos preditores na 1¬™ dire√ß√£o latente
> weights = pls.x_weights_
> print(f"Pesos dos preditores em z1: {weights}")
> # Os pesos refletem as covari√¢ncias com a resposta (apesar de estarem normalizados)
>
> # As dire√ß√µes latentes s√£o constru√≠das com base nos pesos e nos preditores
> z1 = pls.transform(X)
> # O modelo PLS, nesse caso, usa apenas z1 para prever y
> y_pred_pls = pls.predict(X)
>
> # O resultado demonstra como os preditores s√£o combinados para construir as dire√ß√µes latentes.
> ```

**Corol√°rio 2:** *No caso especial em que os preditores ($X$) s√£o ortogonais, o PLS reduz-se √† regress√£o linear padr√£o*. Este corol√°rio ressalta a natureza geral do PLS e sua capacidade de lidar com casos especiais em que os preditores n√£o s√£o correlacionados. Quando os preditores s√£o ortogonais, as dire√ß√µes latentes geradas pelo PLS se tornam equivalentes aos preditores originais, e o processo de regress√£o √© id√™ntico ao da regress√£o linear.

**Implica√ß√µes e Limita√ß√µes:** Embora o PLS utilize informa√ß√µes da resposta para derivar as dire√ß√µes, este processo pode levar a um modelo de regress√£o mais complexo, uma vez que cada nova dire√ß√£o $z_m$ √© uma combina√ß√£o de todos os preditores originais. Em situa√ß√µes onde os preditores s√£o mais importantes na predi√ß√£o, outros m√©todos de sele√ß√£o e regulariza√ß√£o (como discutidos em [^3.3] e [^3.4]) podem ser mais apropriados.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em PLS

```mermaid
graph LR
    subgraph "PLS and Regularization"
        direction TB
        A["PLS: Reduction via covariance"]
        B["PCA: Reduction via variance"]
        C["Regularization (Ridge, Lasso)"]
        D["PLS as a compromise"]
        E["Reduces overfit and improves generalisation"]
        A --> D
        B --> D
        C --> E
        D --> E
    end
```

Assim como em outros m√©todos de regress√£o, o PLS pode se beneficiar de t√©cnicas de sele√ß√£o de vari√°veis e regulariza√ß√£o para evitar overfitting e melhorar a generaliza√ß√£o do modelo. Embora o pr√≥prio PLS realize uma redu√ß√£o de dimensionalidade, a aplica√ß√£o conjunta com regulariza√ß√£o pode aumentar a estabilidade e a interpretabilidade dos resultados.

**Lemma 3:** *A inclus√£o de termos de penaliza√ß√£o (como em Ridge ou Lasso) no PLS pode levar a um modelo mais est√°vel e com melhor desempenho em dados de teste*. Por exemplo, a combina√ß√£o do PLS com regulariza√ß√£o L1 (Lasso) pode realizar a sele√ß√£o de vari√°veis em conjunto com a deriva√ß√£o das dire√ß√µes latentes, resultando em um modelo mais parcimonioso e robusto. Isso combina os benef√≠cios da redu√ß√£o de dimensionalidade com a sele√ß√£o de vari√°veis relevantes para a predi√ß√£o [^3.4.1] e [^3.4.2].

**Prova do Lemma 3:** A regulariza√ß√£o em PLS pode ser implementada diretamente na fun√ß√£o de custo do modelo, introduzindo termos de penaliza√ß√£o que controlam a magnitude dos coeficientes da regress√£o ou a complexidade do modelo. Por exemplo, a penaliza√ß√£o Ridge pode ser implementada adicionando ao custo do modelo um termo proporcional √† soma dos quadrados dos coeficientes das dire√ß√µes latentes ($z_m$), similar ao descrito em (3.41). Este termo de penaliza√ß√£o restringe a complexidade do modelo, prevenindo overfitting, e pode levar a melhorias de desempenho em dados de teste [^3.4.1]. A penaliza√ß√£o L1 (Lasso) pode ser introduzida de forma an√°loga, resultando em um modelo com maior esparsidade. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um cen√°rio onde, ap√≥s aplicar o PLS, obtemos as dire√ß√µes latentes ($z_1, z_2$) e os correspondentes coeficientes de regress√£o ($\beta_1, \beta_2$). Em um PLS padr√£o, o modelo de regress√£o seria:
>
> $\hat{y} = \beta_0 + \beta_1z_1 + \beta_2z_2$
>
> Para regulariza√ß√£o Ridge, adicionamos um termo de penaliza√ß√£o aos coeficientes:
>
> $\text{Custo} = \sum_{i=1}^n(y_i - \hat{y}_i)^2 + \lambda(\beta_1^2 + \beta_2^2)$
>
> Onde $\lambda$ √© o par√¢metro de regulariza√ß√£o. Um valor maior de $\lambda$ ir√° encolher os coeficientes ($\beta_1$ e $\beta_2$), prevenindo overfitting.
>
> ```python
> import numpy as np
> from sklearn.linear_model import Ridge
> from sklearn.cross_decomposition import PLSRegression
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import mean_squared_error
>
> # Simula√ß√£o de dados com ru√≠do e alta dimens√£o
> np.random.seed(42)
> n_samples = 100
> n_features = 20
> X = np.random.rand(n_samples, n_features)
> y = 2 * X[:, 0] - 3 * X[:, 5] + np.random.randn(n_samples)
>
> # Divide em treino e teste
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
>
> # PLS com 2 componentes
> pls = PLSRegression(n_components=2)
> pls.fit(X_train, y_train)
> y_pred_pls = pls.predict(X_test)
> mse_pls = mean_squared_error(y_test, y_pred_pls)
> print(f"MSE do PLS: {mse_pls}")
>
> # PLS com Ridge regularization
> Z_train = pls.transform(X_train)
> Z_test = pls.transform(X_test)
>
> ridge = Ridge(alpha=0.5)  # Regulariza√ß√£o Ridge com lambda=0.5
> ridge.fit(Z_train, y_train)
> y_pred_ridge = ridge.predict(Z_test)
> mse_ridge = mean_squared_error(y_test, y_pred_ridge)
> print(f"MSE do PLS com Ridge: {mse_ridge}")
> # Em muitos casos, o MSE com regulariza√ß√£o √© menor.
>
> # A regulariza√ß√£o √© particularmente √∫til para evitar overfitting
> ```

**Corol√°rio 3:** *A combina√ß√£o do PLS com m√©todos de sele√ß√£o de vari√°veis, como o forward stepwise, pode ser utilizada para selecionar um subconjunto de dire√ß√µes latentes que capture a maior parte da informa√ß√£o preditiva*. A inclus√£o de m√©todos de sele√ß√£o de vari√°veis ao PLS permite a escolha de um n√∫mero menor de dire√ß√µes latentes, melhorando a interpretabilidade e reduzindo o risco de overfitting.

> ‚ö†Ô∏è **Ponto Crucial:** A sele√ß√£o de um n√∫mero apropriado de dire√ß√µes latentes √© crucial para o desempenho do PLS, e √© tipicamente determinada por t√©cnicas de valida√ß√£o cruzada ou outros crit√©rios de sele√ß√£o de modelos [^3.3.1].

> ‚ùó **Ponto de Aten√ß√£o:** Embora o PLS possa ser implementado iterativamente (Algoritmo 3.3), uma implementa√ß√£o matricial √© poss√≠vel, o que torna mais eficiente a combina√ß√£o com m√©todos de regulariza√ß√£o e sele√ß√£o de vari√°veis [^3.5.2].

### Compara√ß√£o com Principal Component Regression (PCR)

O Principal Component Regression (PCR) √© uma t√©cnica similar ao PLS, que tamb√©m utiliza dire√ß√µes latentes derivadas a partir dos preditores originais. No entanto, o PCR constr√≥i as dire√ß√µes utilizando apenas a vari√¢ncia dos preditores, ignorando a resposta, diferentemente do PLS. Como resultado, as dire√ß√µes do PCR podem n√£o ser as mais relevantes para a predi√ß√£o [^3.5.1].

**Pergunta Te√≥rica Avan√ßada:** Quais s√£o as condi√ß√µes em que PCR e PLS levam √†s mesmas dire√ß√µes?

**Resposta:** As dire√ß√µes do PCR e do PLS ser√£o similares quando a resposta ($Y$) estiver fortemente correlacionada com as dire√ß√µes de maior vari√¢ncia dos preditores ($X$). Nesses casos, as dire√ß√µes que maximizam a vari√¢ncia tamb√©m ser√£o as que maximizam a covari√¢ncia com a resposta.

**Lemma 4:** *O PCR √© equivalente ao PLS quando a resposta ($Y$) √© uma combina√ß√£o linear das dire√ß√µes principais com maior vari√¢ncia nos preditores ($X$)*. Essa equival√™ncia surge quando as dire√ß√µes que melhor descrevem a vari√¢ncia dos preditores tamb√©m s√£o as mais relevantes para explicar a resposta.

**Prova do Lemma 4:** O PCR cria as dire√ß√µes latentes usando a decomposi√ß√£o espectral da matriz de covari√¢ncia dos preditores ($X^TX$). As dire√ß√µes s√£o definidas pelos autovetores da matriz de covari√¢ncia, que correspondem aos componentes principais dos preditores. Quando a resposta ($Y$) √© uma combina√ß√£o linear destes componentes principais, o PLS, que considera as rela√ß√µes entre preditores e resposta, chega √†s mesmas dire√ß√µes que o PCR. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Imagine que os preditores ($X$) s√£o constru√≠dos de forma que a primeira componente principal (PC1) seja fortemente correlacionada com a resposta ($Y$). Tanto o PCR quanto o PLS dar√£o maior import√¢ncia a essa primeira componente na regress√£o. No entanto, se $Y$ tamb√©m tiver alguma rela√ß√£o com a segunda componente principal (PC2), mas de maneira diferente da vari√¢ncia dos preditores, o PLS ajustar√° os pesos de forma a capturar melhor essa rela√ß√£o, enquanto o PCR ainda dar√° maior import√¢ncia √† PC1, mesmo que PC2 tenha uma rela√ß√£o mais forte com Y.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.decomposition import PCA
> from sklearn.cross_decomposition import PLSRegression
> from sklearn.linear_model import LinearRegression
> from sklearn.metrics import mean_squared_error
> from sklearn.model_selection import train_test_split
>
> # Simula√ß√£o de dados
> np.random.seed(42)
> n_samples = 100
> X = np.random.rand(n_samples, 3)
>
> # Componentes principais (PC1 e PC2)
> pc1 = 2 * X[:, 0] + 1 * X[:, 1] + 0.5 * X[:,2]
> pc2 = -1 * X[:, 0] + 2 * X[:, 1] + 1 * X[:,2]
> y = 0.8 * pc1 + 0.2 * pc2 + 0.1* np.random.randn(n_samples)
> X = np.column_stack((X[:,0], X[:,1], X[:,2])) # Retorna X para formato original
>
> X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)
>
> # PCR
> pca = PCA(n_components=2)
> pca.fit(X_train)
> Z_pca_train = pca.transform(X_train)
> Z_pca_test = pca.transform(X_test)
>
> pcr_model = LinearRegression()
> pcr_model.fit(Z_pca_train, y_train)
> y_pred_pcr = pcr_model.predict(Z_pca_test)
> mse_pcr = mean_squared_error(y_test, y_pred_pcr)
>
> # PLS
> pls = PLSRegression(n_components=2)
> pls.fit(X_train, y_train)
> y_pred_pls = pls.predict(X_test)
> mse_pls = mean_squared_error(y_test, y_pred_pls)
>
> print(f"MSE do PCR: {mse_pcr}")
> print(f"MSE do PLS: {mse_pls}")
>
> # Os resultados mostram que PLS pode ter um erro menor nesse caso espec√≠fico,
> # pois considera a rela√ß√£o de y com as componentes, e n√£o apenas a vari√¢ncia
>
> plt.figure(figsize=(12,5))
> plt.subplot(1,2,1)
> plt.scatter(y_test, y_pred_pcr, label=f'PCR - MSE: {mse_pcr:.2f}')
> plt.xlabel('Valor real')
> plt.ylabel('Valor predito')
> plt.title('PCR - Predi√ß√µes')
> plt.legend()
>
> plt.subplot(1,2,2)
> plt.scatter(y_test, y_pred_pls, label=f'PLS - MSE: {mse_pls:.2f}')
> plt.xlabel('Valor real')
> plt.ylabel('Valor predito')
> plt.title('PLS - Predi√ß√µes')
> plt.legend()
> plt.tight_layout()
> plt.show()
> ```
>
> Este exemplo simula dados onde a resposta ($y$) depende tanto da primeira quanto da segunda componentes principais dos preditores ($X$). O PLS demonstra um erro quadr√°tico m√©dio menor do que o PCR, evidenciando a vantagem de usar a informa√ß√£o da resposta na constru√ß√£o das dire√ß√µes latentes.

```mermaid
graph LR
 subgraph "PCR vs. PLS"
  direction TB
  A["Predictors (X)"]
  B["Response (Y)"]
  C["PCA: Directions from X variance"]
  D["PLS: Directions from covariance(X,Y)"]
  E["PCR: Regression on PCA Directions"]
  F["PLS: Regression on PLS Directions"]
  A --> C
  A --> D
  B --> D
  C --> E
  D --> F
 end
```

**Corol√°rio 4:** *O PLS geralmente supera o PCR quando a resposta ($Y$) n√£o est√° perfeitamente alinhada com as dire√ß√µes de maior vari√¢ncia dos preditores ($X$)*. Nesses casos, o PLS, ao maximizar a covari√¢ncia entre dire√ß√µes e resposta, captura as rela√ß√µes mais relevantes para a predi√ß√£o, enquanto o PCR se concentra na vari√¢ncia dos preditores, independentemente de sua rela√ß√£o com a resposta.

> ‚úîÔ∏è **Destaque:** A escolha entre PLS e PCR depende da natureza dos dados e da rela√ß√£o entre preditores e resposta. O PLS tende a ser mais robusto quando a resposta est√° linearmente relacionada com os preditores, enquanto o PCR pode ser mais adequado quando a resposta √© influenciada por dire√ß√µes que n√£o s√£o as mais relevantes em termos de varia√ß√£o dos preditores.

### Conclus√£o

O PLS apresenta uma abordagem eficaz para problemas de regress√£o com muitos preditores, especialmente quando esses s√£o altamente correlacionados. Sua capacidade de utilizar informa√ß√µes tanto dos preditores quanto da resposta na constru√ß√£o das dire√ß√µes latentes permite que o modelo capture as rela√ß√µes mais relevantes para a predi√ß√£o. No entanto, a aplica√ß√£o de m√©todos de sele√ß√£o de vari√°veis e regulariza√ß√£o pode ser necess√°ria para otimizar a performance e a interpretabilidade do modelo. O PLS se destaca como uma alternativa ao PCA, especialmente quando a resposta est√° fortemente relacionada aos preditores, mostrando-se um importante m√©todo no arsenal de modelos de aprendizado estat√≠stico para regress√£o.
<!-- END DOCUMENT -->
[^3.5]: "In many situations we have a large number of inputs, often very correlated. The methods in this section produce a small number of linear combinations $Z_m$, $m = 1, \ldots, M$ of the original inputs $X_j$, and the $Z_m$ are then used in place of the $X_j$ as inputs in the regression. The methods differ in how the linear combinations are constructed." *(Trecho de <Nome do Documento>)*
[^3.5.1]: "In this approach the linear combinations $Z_m$ used are the principal components as defined in Section 3.4.1 above. Principal component regression forms the derived input columns $z_m = X u_m$, and then regresses $y$ on $Z_1, Z_2, \ldots, Z_M$ for some $M < p$. Since the $z_m$ are orthogonal, this regression is just a sum of univariate regressions: ..." *(Trecho de <Nome do Documento>)*
[^3.5.2]: "This technique also constructs a set of linear combinations of the inputs for regression, but unlike principal components regression it uses $y$ (in addition to $X$) for this construction. Like principal component regression, partial least squares (PLS) is not scale invariant, so we assume that each $x_j$ is standardized to have mean 0 and variance 1. PLS begins by computing $\phi_{1j} = (x_j,y)$ for each $j$. From this we construct the derived input $z_1 = \sum_j \phi_{1j}x_j$, which is the first partial least squares direction. Hence in the construction of each $z_m$, the inputs are weighted by the strength of their univariate effect on $y$. The outcome $y$ is regressed on $z_1$ giving coefficient $\theta_1$, and then we orthogonalize $x_1,\ldots,x_p$ with respect to $z_1$. We continue this process, until $M < p$ directions have been obtained. In this manner, partial least squares produces a sequence of derived, orthogonal inputs or directions $z_1,z_2,\ldots,z_M$. As with principal-component regression, if we were to construct all $M = p$ directions, we would get back a solution equivalent to the usual least squares estimates; using $M < p$ directions produces a reduced regression. The procedure is described fully in Algorithm 3.3." *(Trecho de <Nome do Documento>)*
[^3.3]: "In this section we describe a number of approaches to variable subset selection with linear regression. In later sections we discuss shrinkage and hybrid approaches for controlling variance, as well as other dimension-reduction strategies. These all fall under the general heading model selection. Model selection is not restricted to linear models; Chapter 7 covers this topic in some detail." *(Trecho de <Nome do Documento>)*
[^3.4]: "By retaining a subset of the predictors and discarding the rest, subset selection produces a model that is interpretable and has possibly lower prediction error than the full model. However, because it is a discrete process‚Äî variables are either retained or discarded‚Äîit often exhibits high variance, and so doesn't reduce the prediction error of the full model. Shrinkage methods are more continuous, and don't suffer as much from high variability." *(Trecho de <Nome do Documento>)*
[^3.4.1]: "Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized residual sum of squares,..." *(Trecho de <Nome do Documento>)*
[^3.4.2]: "The lasso is a shrinkage method like ridge, with subtle but important differences. The lasso estimate is defined by ..." *(Trecho de <Nome do Documento>)*
[^3.3.1]: "Best subset regression finds for each $k \in \{0,1, 2, \ldots, p\}$ the subset of size $k$ that gives smallest residual sum of squares (3.2). An efficient algorithm‚Äî the leaps and bounds procedure (Furnival and Wilson, 1974)‚Äîmakes this feasible for $p$ as large as 30 or 40. Figure 3.5 shows all the subset models for the prostate cancer example. The lower boundary represents the models that are eligible for selection by the best-subsets approach. Note that the best subset of size 2, for example, need not include the variable that was in the best subset of size 1 (for this example all the subsets are nested). The best-subset curve (red lower boundary in Figure 3.5) is necessarily decreasing, so cannot be used to select the subset size $k$. The question of how to choose $k$ involves the tradeoff between bias and variance, along with the more subjective desire for parsimony. There are a number of criteria that one may use; typically we choose the smallest model that minimizes an estimate of the expected prediction error." *(Trecho de <Nome do Documento>)*
