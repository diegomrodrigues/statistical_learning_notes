## Principal Components Regression (PCR) e Regress√£o Truncada: Uma Abordagem de Redu√ß√£o de Dimensionalidade para Regress√£o Linear

<imagem: Mapa mental abrangente que ilustra a rela√ß√£o entre PCA, regress√£o linear, e a regress√£o truncada. O mapa deve mostrar as etapas do PCA, a transforma√ß√£o dos dados, a sele√ß√£o das componentes principais e como essas componentes s√£o usadas em uma regress√£o linear para criar um modelo de regress√£o truncada.>

### Introdu√ß√£o
Em problemas de regress√£o, √© comum nos depararmos com conjuntos de dados caracterizados por um grande n√∫mero de preditores, muitas vezes altamente correlacionados. Este cen√°rio pode levar a instabilidades nos modelos de regress√£o linear, al√©m de dificultar a interpreta√ß√£o dos resultados. O **Principal Components Regression (PCR)** surge como uma alternativa eficaz para contornar esses problemas, utilizando as **componentes principais (PCs)** como novos preditores em um modelo de regress√£o linear. O PCR, ao aplicar uma transforma√ß√£o de redu√ß√£o de dimensionalidade nos dados, n√£o somente mitiga os problemas de multicolinearidade como tamb√©m potencialmente aprimora a capacidade preditiva do modelo [^3.5.1].

Este cap√≠tulo explorar√° em detalhes a t√©cnica de PCR, abordando a sua formula√ß√£o matem√°tica, os seus fundamentos te√≥ricos, as suas vantagens e limita√ß√µes, e a sua rela√ß√£o com outras abordagens de redu√ß√£o de dimensionalidade, como a *Partial Least Squares* (PLS). Al√©m disso, discutiremos a **regress√£o truncada**, que √© a aplica√ß√£o da regress√£o linear utilizando apenas um subconjunto das componentes principais, o que permite um ajuste mais parcimonioso e com menos ru√≠do, em especial em cen√°rios de alta dimensionalidade e com amostras limitadas.

### Conceitos Fundamentais

**Conceito 1: Componentes Principais (PCs)**
As **componentes principais (PCs)** s√£o combina√ß√µes lineares das vari√°veis originais de um conjunto de dados, constru√≠das de forma a capturar a m√°xima vari√¢ncia poss√≠vel. O processo de c√°lculo das PCs √© baseado na **An√°lise de Componentes Principais (PCA)**, uma t√©cnica de redu√ß√£o de dimensionalidade que busca identificar os eixos principais de varia√ß√£o nos dados. A PCA transforma as vari√°veis originais em um novo conjunto de vari√°veis, as PCs, que s√£o ortogonais (n√£o correlacionadas) entre si.  As primeiras PCs capturam a maior parte da vari√¢ncia total dos dados, enquanto as √∫ltimas representam a menor parte, muitas vezes associadas ao ru√≠do [^3.4.1].  
Matematicamente, as PCs s√£o obtidas atrav√©s da decomposi√ß√£o espectral (eigen decomposition) da matriz de covari√¢ncia ou da matriz de correla√ß√£o dos dados. Dada uma matriz de dados $X$ de dimens√£o $N \times p$, a PCA resulta em uma matriz de transforma√ß√£o $V$ de dimens√£o $p \times p$, cujas colunas s√£o os **autovetores (eigenvectors)** da matriz de covari√¢ncia $S = \frac{X^TX}{N}$.  A j-√©sima componente principal ($Z_j$) √© dada por $Z_j = Xv_j$, onde $v_j$ √© o j-√©simo autovetor [^3.4.1].

```mermaid
graph LR
    subgraph "PCA Decomposition"
        direction TB
        A["Data Matrix X (N x p)"] --> B["Covariance Matrix S = (X^T X) / N"]
        B --> C["Eigen Decomposition of S"]
        C --> D["Eigenvectors V (p x p)"]
        D --> E["Principal Components Z_j = Xv_j"]
    end
```

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma matriz de dados $X$ com 3 amostras e 2 vari√°veis (p=2), onde:
>
> $$
> X = \begin{bmatrix}
> 1 & 2 \\
> 3 & 4 \\
> 5 & 6
> \end{bmatrix}
> $$
>
> 1. **Calculando a Matriz de Covari√¢ncia (S):**
>    Primeiro, centralizamos os dados subtraindo a m√©dia de cada coluna. A m√©dia da primeira coluna √© $(1+3+5)/3=3$, e a da segunda coluna √© $(2+4+6)/3=4$.  A matriz centralizada $X_c$ √©:
>
>    $$
> X_c = \begin{bmatrix}
> -2 & -2 \\
> 0 & 0 \\
> 2 & 2
> \end{bmatrix}
>    $$
>    A matriz de covari√¢ncia $S$ √©:
>    $$
> S = \frac{1}{N} X_c^T X_c = \frac{1}{3} \begin{bmatrix}
> -2 & 0 & 2 \\
> -2 & 0 & 2
> \end{bmatrix} \begin{bmatrix}
> -2 & -2 \\
> 0 & 0 \\
> 2 & 2
> \end{bmatrix} = \frac{1}{3} \begin{bmatrix}
> 8 & 8 \\
> 8 & 8
> \end{bmatrix} = \begin{bmatrix}
> 8/3 & 8/3 \\
> 8/3 & 8/3
> \end{bmatrix}
>    $$
>
> 2. **Calculando os Autovetores:**
>    Os autovetores (v1 e v2) de S s√£o os vetores que satisfazem $Sv = \lambda v$ onde $\lambda$ s√£o os autovalores.  Neste caso, os autovetores normalizados ser√£o aproximadamente:
>    $v_1 = \begin{bmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{bmatrix} \approx \begin{bmatrix} 0.707 \\ 0.707 \end{bmatrix}$
>    $v_2 = \begin{bmatrix} -1/\sqrt{2} \\ 1/\sqrt{2} \end{bmatrix} \approx \begin{bmatrix} -0.707 \\ 0.707 \end{bmatrix}$
>
> 3. **Calculando as Componentes Principais (Z):**
>    As componentes principais s√£o dadas por $Z = X_c V$, onde V √© a matriz formada pelos autovetores.
>
>    $$
> V = \begin{bmatrix}
> 0.707 & -0.707 \\
> 0.707 & 0.707
> \end{bmatrix}
>    $$
>  
>    $$
> Z = X_c V  =  \begin{bmatrix}
> -2 & -2 \\
> 0 & 0 \\
> 2 & 2
> \end{bmatrix} \begin{bmatrix}
> 0.707 & -0.707 \\
> 0.707 & 0.707
> \end{bmatrix} = \begin{bmatrix}
> -2.828 & 0 \\
> 0 & 0 \\
> 2.828 & 0
> \end{bmatrix}
>    $$
>
>  A primeira componente principal $Z_1$ captura a maior vari√¢ncia, enquanto a segunda componente $Z_2$ tem vari√¢ncia zero neste exemplo espec√≠fico.

**Lemma 1:** _As componentes principais formam um conjunto ortogonal, ou seja, n√£o s√£o correlacionadas entre si._  
**Prova:**  As componentes principais $Z_i$ e $Z_j$ s√£o dadas por $Z_i = Xv_i$ e $Z_j = Xv_j$, onde $v_i$ e $v_j$ s√£o autovetores da matriz de covari√¢ncia $S = \frac{X^TX}{N}$. Como $S$ √© uma matriz sim√©trica, seus autovetores correspondentes a autovalores distintos s√£o ortogonais, i.e., $v_i^T v_j = 0$ para $i \neq j$. Portanto, a covari√¢ncia entre $Z_i$ e $Z_j$ √© dada por $Cov(Z_i, Z_j) = \frac{1}{N} (Xv_i)^T (Xv_j) = \frac{1}{N} v_i^T X^T X v_j = v_i^T S v_j$. Se os autovetores forem ortogonais, ent√£o $v_i^T S v_j = v_i^T (\lambda_j v_j) = \lambda_j v_i^T v_j = 0$, mostrando que $Z_i$ e $Z_j$ n√£o s√£o correlacionadas.  $\blacksquare$

**Conceito 2: Principal Components Regression (PCR)**
O **PCR** √© uma t√©cnica que combina PCA com regress√£o linear. Em vez de usar as vari√°veis originais, o PCR utiliza as componentes principais como preditores. Primeiro, aplicamos a PCA no conjunto de preditores $X$ para obter as PCs ($Z_1, Z_2, \dots, Z_p$).  Em seguida, um modelo de regress√£o linear √© ajustado usando as primeiras $M$ componentes principais ($Z_1, Z_2, \dots, Z_M$) como preditores, onde $M < p$.  O modelo resultante √© dado por:
$$ \hat{y} = \beta_0 + \beta_1 Z_1 + \beta_2 Z_2 + \dots + \beta_M Z_M $$
onde $ \hat{y} $ √© o valor previsto da vari√°vel resposta, $\beta_0$ √© o intercepto, e $\beta_1, \beta_2, \dots, \beta_M$ s√£o os coeficientes de regress√£o estimados.
Essa abordagem reduz a dimensionalidade do problema de regress√£o, focando em um conjunto menor de vari√°veis que capturam a maior parte da variabilidade dos dados.  Al√©m disso, a ortogonalidade das PCs elimina problemas de multicolinearidade [^3.5.1].

```mermaid
graph LR
    subgraph "Principal Components Regression (PCR)"
        direction TB
        A["Original Predictors X"] --> B["PCA Transformation to Z"]
        B --> C["Selected Principal Components Z_1 ... Z_M"]
        C --> D["Linear Regression: yÃÇ = Œ≤_0 + Œ£Œ≤_j Z_j"]
    end
```

> üí° **Exemplo Num√©rico:**
>
> Continuando com o exemplo anterior, vamos supor que temos uma vari√°vel resposta $y$:
>
> $$
> y = \begin{bmatrix}
> 3 \\
> 5 \\
> 7
> \end{bmatrix}
> $$
>
> E que decidimos usar apenas a primeira componente principal $Z_1$ (M=1) para prever $y$.
>
> A nossa matriz $Z_M$ ser√°:
> $$
> Z_M = \begin{bmatrix}
> -2.828 \\
> 0 \\
> 2.828
> \end{bmatrix}
> $$
>
> Adicionamos uma coluna de 1's para o termo de intercepto:
>
> $$
> Z_M' = \begin{bmatrix}
> 1 & -2.828 \\
> 1 & 0 \\
> 1 & 2.828
> \end{bmatrix}
> $$
> O modelo de regress√£o linear √© $\hat{y} = Z_M' \beta$, onde $\beta = (Z_M'^T Z_M')^{-1}Z_M'^T y$.
>
> Calculando $(Z_M'^T Z_M')$:
> $$
> Z_M'^T Z_M' = \begin{bmatrix}
> 1 & 1 & 1 \\
> -2.828 & 0 & 2.828
> \end{bmatrix} \begin{bmatrix}
> 1 & -2.828 \\
> 1 & 0 \\
> 1 & 2.828
> \end{bmatrix} = \begin{bmatrix}
> 3 & 0 \\
> 0 & 16
> \end{bmatrix}
> $$
> Calculando a inversa:
> $$
> (Z_M'^T Z_M')^{-1} = \begin{bmatrix}
> 1/3 & 0 \\
> 0 & 1/16
> \end{bmatrix}
> $$
> Calculando $Z_M'^T y$:
> $$
> Z_M'^T y = \begin{bmatrix}
> 1 & 1 & 1 \\
> -2.828 & 0 & 2.828
> \end{bmatrix} \begin{bmatrix}
> 3 \\ 5 \\ 7
> \end{bmatrix} = \begin{bmatrix}
> 15 \\
> 11.312
> \end{bmatrix}
> $$
> Finalmente, calculamos os coeficientes $\beta$:
>
> $$
> \beta = \begin{bmatrix}
> 1/3 & 0 \\
> 0 & 1/16
> \end{bmatrix} \begin{bmatrix}
> 15 \\
> 11.312
> \end{bmatrix} =  \begin{bmatrix}
> 5 \\
> 0.707
> \end{bmatrix}
> $$
>
> Portanto, o modelo de regress√£o linear √© $\hat{y} = 5 + 0.707 Z_1$. O intercepto √© 5, e o coeficiente associado √† primeira componente principal √© 0.707.

**Corol√°rio 1:**  _O modelo de PCR pode ser expresso em termos das vari√°veis originais._
**Prova:** Como cada PC $Z_j$ √© uma combina√ß√£o linear das vari√°veis originais $X_k$, podemos escrever  $Z_j = \sum_{k=1}^p v_{kj} X_k$, onde $v_{kj}$ √© o elemento da j-√©sima coluna e k-√©sima linha da matriz $V$ de autovetores.  Substituindo essa express√£o na equa√ß√£o do modelo de PCR, obtemos:
$$ \hat{y} = \beta_0 + \sum_{j=1}^M \beta_j Z_j = \beta_0 + \sum_{j=1}^M \beta_j \sum_{k=1}^p v_{kj} X_k =  \beta_0 + \sum_{k=1}^p \left(\sum_{j=1}^M \beta_j v_{kj}\right) X_k = \beta_0 + \sum_{k=1}^p \gamma_k X_k, $$
onde $\gamma_k = \sum_{j=1}^M \beta_j v_{kj}$.  Isso demonstra que, apesar de o ajuste ser feito com as PCs, o modelo resultante ainda pode ser expresso em termos das vari√°veis originais. $\blacksquare$

**Conceito 3: Regress√£o Truncada**
A **regress√£o truncada** √© a aplica√ß√£o do PCR em que apenas um n√∫mero limitado de componentes principais (M < p) √© selecionado para a constru√ß√£o do modelo. Esta sele√ß√£o √© crucial e baseia-se no princ√≠pio de que as primeiras componentes principais ret√™m a maior parte da informa√ß√£o relevante sobre os dados, enquanto as √∫ltimas componentes principais representam, principalmente, ru√≠do.  A escolha de $M$ envolve um *trade-off*: um valor muito baixo de $M$ pode levar a um modelo subajustado (underfitting), enquanto um valor muito alto pode incluir ru√≠do e aumentar a vari√¢ncia do modelo [^3.5.1]. T√©cnicas de valida√ß√£o cruzada s√£o geralmente utilizadas para escolher o valor √≥timo de M, minimizando o erro de previs√£o e equilibrando o vi√©s e a vari√¢ncia do modelo.

```mermaid
graph LR
    subgraph "Truncated Regression"
    direction TB
        A["All Principal Components Z_1... Z_p"] --> B["Selection of First M Components Z_1...Z_M"]
        B --> C["Linear Regression using Z_1...Z_M"]
    end
```

> ‚ö†Ô∏è **Nota Importante**: A escolha do n√∫mero de componentes principais (M) √© crucial para o desempenho do PCR. A valida√ß√£o cruzada √© uma abordagem comum para selecionar um valor apropriado para M [^3.5.1].
> ‚ùó **Ponto de Aten√ß√£o**: Ao truncar as componentes principais, o modelo resultante pode n√£o ser invariante a transforma√ß√µes de escala nas vari√°veis originais. √â recomendado padronizar as vari√°veis antes de aplicar a PCA. [^3.4.1]
> ‚úîÔ∏è **Destaque**: O PCR √© uma abordagem de redu√ß√£o de dimensionalidade que aborda multicolinearidade e, quando truncada, pode levar a modelos mais simples e parcimoniosos [^3.5.1].

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

<imagem: Diagrama mostrando o fluxo de trabalho da PCR, desde a padroniza√ß√£o dos dados at√© a regress√£o linear usando as componentes principais truncadas. O diagrama deve mostrar as etapas de PCA, a sele√ß√£o das PCs, a regress√£o linear com as PCs selecionadas e como as previs√µes s√£o feitas usando o modelo final.>

```mermaid
flowchart TD
    A[Dados de Entrada] --> B{Padroniza√ß√£o dos Dados}
    B --> C[An√°lise de Componentes Principais (PCA)]
    C --> D{Sele√ß√£o das M Principais Componentes}
    D --> E[Regress√£o Linear com Componentes Principais]
    E --> F{Previs√£o}
```

**Explica√ß√£o:** Este diagrama descreve o fluxo de trabalho do PCR, desde o pr√©-processamento dos dados at√© a realiza√ß√£o das previs√µes usando o modelo de regress√£o linear ajustado com as componentes principais selecionadas.

A aplica√ß√£o da regress√£o linear em matrizes de indicadores, conforme explorado em outras se√ß√µes, pode ser diretamente combinada com o PCR. Em vez de usar as vari√°veis originais na regress√£o de indicadores, podemos aplicar o PCA e usar as componentes principais para criar uma vers√£o truncada da regress√£o linear. As limita√ß√µes, como a extrapola√ß√£o fora do intervalo [0, 1] discutidas em outros contextos [^4.4], permanecem relevantes aqui, mas s√£o mitigadas pela redu√ß√£o de dimensionalidade proporcionada pelo PCA. Por exemplo, se o objetivo prim√°rio for a constru√ß√£o de fronteiras de decis√£o lineares, e os dados apresentarem alta dimensionalidade, a regress√£o truncada via PCR pode ser uma alternativa interessante √† regress√£o de indicadores com as vari√°veis originais [^3.5.1].

A combina√ß√£o de PCA com regress√£o de indicadores pode, em alguns casos, resolver o "masking problem", abordado anteriormente [^4.3], ao trabalhar com componentes principais ortogonais que capturam padr√µes mais robustos nos dados. A rela√ß√£o com o *Linear Discriminant Analysis* (LDA), tamb√©m discutida anteriormente [^4.3], √© evidente, uma vez que ambas as t√©cnicas buscam reduzir a dimensionalidade dos dados, embora com objetivos distintos. A PCA busca preservar a m√°xima vari√¢ncia dos dados, enquanto a LDA busca maximizar a separa√ß√£o entre classes [^3.5.1].

**Lemma 2:**  _A regress√£o truncada, utilizando as primeiras M componentes principais, reduz a vari√¢ncia do modelo em compara√ß√£o com a regress√£o linear com todas as vari√°veis originais._
**Prova:** Seja $\hat{y}_{LS} = X\hat{\beta}_{LS}$ o ajuste da regress√£o linear com todas as vari√°veis originais, e $\hat{y}_{PCR} = Z_M\hat{\beta}_{PCR}$ o ajuste da regress√£o truncada usando as primeiras M componentes principais, onde $Z_M$ √© a matriz de scores das componentes principais. A vari√¢ncia dos coeficientes estimados na regress√£o linear √© dada por $Var(\hat{\beta}_{LS}) = (X^TX)^{-1}\sigma^2$, onde $\sigma^2$ √© a vari√¢ncia do erro. Na PCR, a vari√¢ncia dos coeficientes estimados √© dada por  $Var(\hat{\beta}_{PCR}) = (Z_M^T Z_M)^{-1} \sigma^2$. Como as PCs s√£o ortogonais, $Var(\hat{\beta}_{PCR}) = \Lambda_M^{-1} \sigma^2$, onde $\Lambda_M$ √© a matriz diagonal de autovalores associados a $Z_M$. Ao truncar as componentes principais, removemos os autovalores menores, que est√£o associados √† maior vari√¢ncia do erro. Portanto, a vari√¢ncia dos coeficientes estimados, e consequentemente, a vari√¢ncia das previs√µes do modelo, √© reduzida. $\blacksquare$

```mermaid
graph LR
    subgraph "Variance Reduction in Truncated Regression"
        direction TB
        A["Full Linear Regression: Var(Œ≤ÃÇ_LS) = (X^TX)‚Åª¬πœÉ¬≤"]
        B["PCR with M Components: Var(Œ≤ÃÇ_PCR) = (Z_M^T Z_M)‚Åª¬πœÉ¬≤"]
        C["Orthogonality of PCs: Var(Œ≤ÃÇ_PCR) = Œõ_M‚Åª¬πœÉ¬≤"]
        D["Truncation Reduces Variance"]
        A --> D
        B --> C
        C --> D

    end
```

> üí° **Exemplo Num√©rico:**
>
> Vamos ilustrar o efeito da redu√ß√£o de vari√¢ncia com um exemplo. Suponha que temos os seguintes dados simulados com 10 amostras e 5 preditores, e uma vari√°vel resposta y. Vamos comparar a regress√£o linear com todas as vari√°veis originais (OLS) com o PCR usando apenas 2 componentes principais.
> ```python
> import numpy as np
> from sklearn.decomposition import PCA
> from sklearn.linear_model import LinearRegression
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import mean_squared_error, r2_score
> import pandas as pd
>
> np.random.seed(42)
> n_samples = 100
> n_features = 5
>
> X = np.random.rand(n_samples, n_features)
> true_coef = np.array([2, -1, 3, -2, 1])
> y = np.dot(X, true_coef) + np.random.normal(0, 0.5, n_samples)
>
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # Regress√£o Linear OLS
> ols_model = LinearRegression()
> ols_model.fit(X_train, y_train)
> ols_y_pred = ols_model.predict(X_test)
> ols_mse = mean_squared_error(y_test, ols_y_pred)
> ols_r2 = r2_score(y_test, ols_y_pred)
>
>
> # PCA + Regress√£o (PCR)
> pca = PCA(n_components=2)
> X_train_pca = pca.fit_transform(X_train)
> X_test_pca = pca.transform(X_test)
>
> pcr_model = LinearRegression()
> pcr_model.fit(X_train_pca, y_train)
> pcr_y_pred = pcr_model.predict(X_test_pca)
> pcr_mse = mean_squared_error(y_test, pcr_y_pred)
> pcr_r2 = r2_score(y_test, pcr_y_pred)
>
> results = pd.DataFrame({
>    'Method': ['OLS', 'PCR'],
>    'MSE': [ols_mse, pcr_mse],
>    'R¬≤': [ols_r2, pcr_r2],
>    'Parameters': [len(ols_model.coef_), len(pcr_model.coef_)]
> })
> print(results)
>
> ```
> Este c√≥digo simula dados, realiza a regress√£o linear com todas as vari√°veis e com o PCR. Observe a sa√≠da, com resultados como:
>
> ```
>   Method       MSE        R¬≤  Parameters
> 0    OLS  0.198931  0.978552           5
> 1    PCR  0.240805  0.973994           2
> ```
>
> O modelo PCR, utilizando apenas 2 componentes principais (2 par√¢metros), tem um erro quadr√°tico m√©dio (MSE) ligeiramente maior, mas ainda mant√©m um R¬≤ alto e uma redu√ß√£o significativa na complexidade do modelo. Isso exemplifica o Lemma 2: a redu√ß√£o da dimensionalidade pode levar a um aumento na vari√¢ncia do erro, mas geralmente resulta em um modelo mais generaliz√°vel.

**Corol√°rio 2:** _A escolha do n√∫mero M de componentes principais na regress√£o truncada influencia o tradeoff entre vi√©s e vari√¢ncia do modelo._
**Prova:**  Um valor baixo de M leva a um modelo com alta vari√¢ncia e baixo vi√©s, uma vez que poucas componentes principais s√£o usadas, simplificando o modelo e, potencialmente, perdendo informa√ß√µes relevantes. Por outro lado, um valor alto de M resulta em um modelo com baixa vari√¢ncia, mas com alto vi√©s, pois ru√≠do e variabilidade esp√∫ria das √∫ltimas componentes principais s√£o inclu√≠das. T√©cnicas como valida√ß√£o cruzada permitem ajustar o valor de M para minimizar o erro de previs√£o, equilibrando o tradeoff entre vi√©s e vari√¢ncia [^3.5.1].

```mermaid
graph LR
    subgraph "Bias-Variance Tradeoff in PCR"
        direction TB
        A["Low M: High Variance, Low Bias"]
        B["High M: Low Variance, High Bias"]
        C["Optimal M via Cross-Validation"]
        A --> C
        B --> C
    end
```

Em alguns cen√°rios, a regress√£o log√≠stica, conforme discutido anteriormente [^4.4], pode fornecer estimativas de probabilidade mais est√°veis. No entanto, se o objetivo prim√°rio for a constru√ß√£o de fronteiras de decis√£o lineares, a combina√ß√£o de regress√£o de indicadores com PCA ou PCR pode ser suficiente e vantajosa em rela√ß√£o √† regress√£o com as vari√°veis originais. A escolha da t√©cnica apropriada depende das caracter√≠sticas do problema espec√≠fico, tais como a dimens√£o dos dados, a multicolinearidade dos preditores e o objetivo final da modelagem [^3.5.1].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

<imagem: Mapa mental que relaciona PCR com t√©cnicas de regulariza√ß√£o. O mapa deve mostrar como a redu√ß√£o de dimensionalidade via PCR pode atuar como um m√©todo de regulariza√ß√£o impl√≠cito, destacando a rela√ß√£o com penaliza√ß√µes L1/L2. Apresentar fluxos de trabalho mostrando como o uso de PCR pode preceder ou substituir t√©cnicas de regulariza√ß√£o em regress√£o log√≠stica ou em outros modelos de classifica√ß√£o linear.>

Na pr√°tica, a sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o cruciais para construir modelos robustos e generaliz√°veis. Em problemas de alta dimensionalidade, a combina√ß√£o de PCR com m√©todos de regulariza√ß√£o pode ser bastante eficaz [^3.5].  Enquanto o PCR reduz o n√∫mero de preditores atrav√©s da combina√ß√£o das vari√°veis originais em um n√∫mero menor de componentes principais n√£o correlacionadas, t√©cnicas de regulariza√ß√£o, como a penaliza√ß√£o L1 (Lasso) e L2 (Ridge) podem ser aplicadas para evitar *overfitting* e tornar o modelo mais est√°vel e generaliz√°vel [^3.4.1].  

```mermaid
graph LR
    subgraph "PCR and Regularization"
        direction TB
        A["Principal Component Regression (PCR)"] --> B["Regularization Techniques (L1/L2)"]
        B --> C["L1 Regularization (Lasso)"]
        B --> D["L2 Regularization (Ridge)"]
        C & D --> E["Improved Model Stability & Generalization"]
    end
```

A aplica√ß√£o da penaliza√ß√£o L1, como visto anteriormente em [^4.4.4], leva a coeficientes esparsos, o que pode ser √∫til para a sele√ß√£o de componentes principais mais relevantes. Analogamente, a penaliza√ß√£o L2 pode estabilizar os coeficientes e tornar o modelo mais robusto em cen√°rios onde h√° colinearidade entre as componentes principais. A combina√ß√£o de regulariza√ß√£o L1 e L2 (Elastic Net), como abordado em [^4.5], pode ser utilizada para aproveitar os benef√≠cios de ambos os m√©todos, balanceando a sele√ß√£o de vari√°veis com a estabiliza√ß√£o dos coeficientes [^3.4.1].

**Lemma 3:** _A aplica√ß√£o da penaliza√ß√£o L1 em modelos de regress√£o linear que utilizam as componentes principais como preditores leva a coeficientes esparsos nos pesos das componentes principais, selecionando as mais relevantes._
**Prova:** Dado um modelo de regress√£o linear, $y = \beta_0 + \sum_{j=1}^M Z_j \beta_j + \epsilon$, onde $Z_j$ s√£o as componentes principais, a aplica√ß√£o da penaliza√ß√£o L1 introduz um termo adicional na fun√ß√£o objetivo: $\text{argmin}_\beta \frac{1}{2} ||y - \beta_0 - \sum_{j=1}^M Z_j \beta_j||^2 + \lambda \sum_{j=1}^M |\beta_j|$. A minimiza√ß√£o desta fun√ß√£o leva a que alguns coeficientes $\beta_j$ sejam exatamente iguais a zero para valores suficientemente altos de $\lambda$.  Isso implica que apenas as componentes principais com coeficientes n√£o nulos ser√£o utilizadas no modelo final, realizando uma sele√ß√£o de componentes principais. $\blacksquare$

```mermaid
graph LR
    subgraph "L1 Regularization in PCR"
        direction TB
        A["Regression Model: y = Œ≤‚ÇÄ + Œ£Œ≤_j Z_j"] --> B["L1 Penalty: Œª Œ£|Œ≤_j|"]
        B --> C["Objective: argmin(Loss + Penalty)"]
        C --> D["Sparse Coefficients Œ≤_j"]
        D --> E["Selection of Relevant PCs"]
    end
```

**Prova do Lemma 3:** (referenciando tamb√©m [^4.4.3]): A prova pode ser desenvolvida utilizando conceitos de otimiza√ß√£o. O termo de penaliza√ß√£o L1, $\lambda \sum_{j=1}^M |\beta_j|$, adiciona uma penalidade linear √† fun√ß√£o de custo. Como a fun√ß√£o de custo √© convexa, a solu√ß√£o √≥tima pode ocorrer em pontos onde algumas das derivadas parciais em rela√ß√£o a  $\beta_j$ sejam zero.  A fun√ß√£o de custo para a regress√£o com penaliza√ß√£o L1 √© n√£o-diferenci√°vel em $\beta_j=0$, mas a otimiza√ß√£o pode ser feita via subgradientes. Para coeficientes suficientemente pequenos, a aplica√ß√£o da penaliza√ß√£o L1 leva a que seus coeficientes sejam exatamente zero, dado que o subgradiente em torno de $\beta_j=0$ ter√° um componente da norma da penaliza√ß√£o que √© sempre diferente de zero.  $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o exemplo anterior e aplicar a regulariza√ß√£o L1 (Lasso) na regress√£o com componentes principais, para exemplificar a esparsidade nos coeficientes.
> ```python
> import numpy as np
> from sklearn.decomposition import PCA
> from sklearn.linear_model import Lasso
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import mean_squared_error, r2_score
> import pandas as pd
>
> np.random.seed(42)
> n_samples = 100
> n_features = 5
>
> X = np.random.rand(n_samples, n_features)
> true_coef = np.array([2, -1, 3, -2, 1])
> y = np.dot(X, true_coef) + np.random.normal(0, 0.5, n_samples)
>
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # PCA + Regress√£o (PCR) + Lasso
> pca = PCA(n_components=4)
> X_train_pca = pca.fit_transform(X_train)
> X_test_pca = pca.transform(X_test)
>
> lasso_model = Lasso(alpha=0.1)
> lasso_model.fit(X_train_pca, y_train)
> lasso_y_pred = lasso_model.predict(X_test_pca)
> lasso_mse = mean_squared_error(y_test, lasso_y_pred)
> lasso_r2 = r2_score(y_test, lasso_y_pred)
>
> print(f"Lasso coefficients (PCR): {lasso_model.coef_}")
>
> results = pd.DataFrame({
>    'Method': ['Lasso (PCR)'],
>    'MSE': [lasso_mse],
>    'R¬≤': [lasso_r2],
>    'Parameters': [len(lasso_model.coef_)]
> })
> print(results)
>
> ```
> Executando o c√≥digo acima, um resultado como este deve aparecer:
>
> ```
> Lasso coefficients (PCR): [ 1.45662567  0.          0.01382742 -0.        ]
>   Method       MSE        R¬≤  Parameters
> 0  Lasso (PCR)  0.204213  0.977954           4
> ```
> Podemos ver que dois dos coeficientes s√£o exatamente zero. A penaliza√ß√£o L1 for√ßou o modelo a usar somente as componentes principais mais relevantes, exemplificando o Lemma 3.

**Corol√°rio 3:** _A interpretabilidade dos modelos de regress√£o truncada pode ser melhorada utilizando a penaliza√ß√£o L1 em conjunto com o PCR._
**Prova:** A esparsidade induzida pela penaliza√ß√£o L1, como demonstrado no Lemma 3, leva a que apenas um subconjunto das componentes principais seja utilizado na constru√ß√£o do modelo. A interpreta√ß√£o desse subconjunto de componentes pode ser feita analisando os pesos das componentes principais nas vari√°veis originais, revelando quais vari√°veis originais contribuem mais para o modelo.  Al√©m disso, a redu√ß√£o do n√∫mero de componentes principais facilita a visualiza√ß√£o e a compreens√£o da rela√ß√£o entre preditores e resposta [^3.4.1].

```mermaid
graph LR
    subgraph "Improved Interpretability with L1 + PCR"
        direction TB
        A["L1 Regularization induces Sparsity"] --> B["Subset of Relevant PCs Selected"]
        B --> C["Analyze Loadings of Selected PCs"]
        C --> D["Improved Understanding of Feature Importance"]
    end
```

Em resumo, t√©cnicas como Elastic Net, conforme discutido em [^4.5], quando aplicadas a modelos que utilizam componentes principais como preditores, permitem balancear entre a sele√ß√£o de componentes mais relevantes (L1) e a estabiliza√ß√£o dos coeficientes (L2), al√©m de mitigar os problemas de multicolinearidade. A aplica√ß√£o de m√©todos de regulariza√ß√£o em modelos de regress√£o com componentes principais (PCR), quando comparada √† regress√£o linear com regulariza√ß√£o nas vari√°veis originais, tamb√©m pode levar a um modelo mais parcimonioso, al√©m de mitigar poss√≠veis problemas de instabilidade e melhorando a interpretabilidade [^3.4.1], [^4.4.4], [^4.4.5].

### Separating Hyperplanes e Perceptrons

Em rela√ß√£o a *separating hyperplanes* e Perceptrons [^4.5.1], [^4.5.2], √© importante notar que estas t√©cnicas de classifica√ß√£o geralmente operam no espa√ßo das vari√°veis originais. No entanto, a aplica√ß√£o de PCR como um passo de pr√©-processamento pode simplificar a representa√ß√£o dos dados, transformando-os em um espa√ßo de menor dimens√£o onde a separa√ß√£o linear se torna mais f√°cil. Ao reduzir a dimensionalidade dos