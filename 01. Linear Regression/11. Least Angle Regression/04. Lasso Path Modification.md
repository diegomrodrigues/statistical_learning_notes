## Modificação do Algoritmo LAR para o Caminho Completo do Lasso

### Introdução
Este capítulo aprofunda-se na relação entre o algoritmo Least Angle Regression (**LAR**) e o caminho de soluções do **Lasso**. Como mencionado no tópico, uma simples modificação do algoritmo LAR permite gerar todo o caminho do Lasso, que é também *piecewise-linear* [^76]. Exploraremos os detalhes desta modificação, suas implicações teóricas e práticas, e o porquê desta relação ser fundamental no contexto de métodos de regressão linear.

### Conceitos Fundamentais
O algoritmo LAR, conforme descrito por Efron et al. (2004) [^74], é um procedimento iterativo para construir um modelo de regressão linear. Ele começa com todos os coeficientes zerados e, em cada passo, identifica a variável mais correlacionada com o resíduo atual. Em vez de incluir a variável completamente no modelo, como no *forward stepwise regression*, o LAR move o coeficiente da variável selecionada continuamente em direção ao seu valor de mínimos quadrados, até que outra variável alcance o mesmo nível de correlação com o resíduo. Este processo continua até que todas as variáveis estejam no modelo ou até que se atinja uma solução de mínimos quadrados com resíduo zero.

O Lasso, por outro lado, é um método de regularização que minimiza a soma dos quadrados dos resíduos sujeito a uma restrição na norma L1 dos coeficientes [^68]. Esta restrição, dada por $\sum_{j=1}^{p} |\beta_j| \leq t$, força alguns coeficientes a serem exatamente zero, realizando assim uma seleção de variáveis. A formulação Lagrangiana equivalente do problema Lasso é dada por:
$$
\hat{\beta}^{lasso} = \underset{\beta}{\text{argmin}} \left\{ \frac{1}{2} \sum_{i=1}^{N} \left(Y_i - \beta_0 - \sum_{j=1}^{p} X_{ij}\beta_j\right)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
$$
onde $\lambda$ é o parâmetro de regularização [^68].

A conexão entre LAR e Lasso reside na observação de que ambos os algoritmos geram soluções *sparse*, e que o caminho de soluções do Lasso (ou seja, o conjunto de soluções para diferentes valores de $\lambda$) é *piecewise-linear* [^76]. Isso significa que os coeficientes do Lasso mudam linearmente em função de $\lambda$ em determinados intervalos.

**A Modificação Chave:**
A modificação do algoritmo LAR que permite gerar o caminho completo do Lasso envolve monitorar quando um coeficiente não-nulo atinge o valor zero. No algoritmo LAR original, uma vez que uma variável entra no conjunto ativo, ela permanece lá até o final. No entanto, no Lasso, um coeficiente pode ser forçado a zero devido à penalidade L1. Assim, a modificação consiste em:

**Algoritmo 3.2a Least Angle Regression: Lasso Modification [^76].**
4a. Se um coeficiente não-zero atinge zero, remova sua variável do conjunto ativo de variáveis e recompute a direção conjunta de mínimos quadrados.

Este passo adicional garante que o algoritmo LAR siga o caminho correto do Lasso, permitindo que variáveis saiam do modelo quando sua penalidade L1 se torna maior do que sua contribuição para a redução do erro [^76].

**Justificativa Heurística:**
Para entender porque esta modificação funciona, considere as condições de otimalidade do Lasso [^76]. Para uma variável ativa $j$, a condição é:
$$
X_j^T(y - X\beta) = \lambda \cdot \text{sign}(\beta_j)
$$
onde $X_j$ é a j-ésima coluna da matriz de design $X$, $y$ é o vetor de resposta, $\beta$ é o vetor de coeficientes, e $\lambda$ é o parâmetro de regularização. Esta condição afirma que a correlação entre a variável $X_j$ e o resíduo $(y - X\beta)$ deve ser igual a $\lambda$ em magnitude, e ter o mesmo sinal que o coeficiente $\beta_j$.

No algoritmo LAR, as variáveis são adicionadas ao conjunto ativo quando sua correlação com o resíduo atinge um certo limiar [^74]. A modificação do Lasso garante que, se um coeficiente atinge zero, a variável correspondente é removida do conjunto ativo, mantendo as condições de otimalidade do Lasso.

### Conclusão
A modificação do algoritmo LAR para gerar o caminho completo do Lasso é uma demonstração elegante de como um algoritmo simples pode ser adaptado para resolver um problema mais complexo [^76]. Esta conexão não apenas fornece um método eficiente para calcular as soluções do Lasso, mas também oferece *insights* valiosos sobre a relação entre seleção de variáveis e regularização em modelos lineares. A natureza *piecewise-linear* do caminho do Lasso, garantida por esta modificação, facilita a interpretação e a análise das soluções obtidas.

### Referências
[^74]: Efron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least angle regression. *Annals of statistics*, *32*(2), 407-499.
[^76]: Hastie, T., Tibshirani, R., & Wainwright, M. (2015). *Statistical learning with sparsity: the lasso and generalizations*. CRC press.

<!-- END -->