## Exploração Detalhada do Algoritmo de Regressão de Ângulo Mínimo (LAR)

### Introdução
Este capítulo aprofunda o Algoritmo 3.2, que detalha a Regressão de Ângulo Mínimo (LAR), conforme mencionado em [^1]. O LAR é um método de seleção de variáveis que constrói um modelo de regressão iterativamente, adicionando preditores ao modelo em etapas, de forma semelhante à regressão stepwise, mas com uma abordagem mais "democrática". Este capítulo visa fornecer uma compreensão completa do algoritmo LAR, incluindo sua motivação, funcionamento interno e propriedades importantes.

### Conceitos Fundamentais

O algoritmo LAR pode ser visto como uma versão "democrática" da regressão stepwise forward [^1]. Em vez de adicionar uma variável de cada vez e ajustar o modelo completamente, o LAR adiciona apenas "o quanto" uma variável merece, conforme determinado por sua correlação com o resíduo atual.

**Funcionamento do Algoritmo LAR:**

1. **Inicialização:** O algoritmo começa com todos os coeficientes definidos como zero e o resíduo igual à resposta $y$ [^1]. Os preditores são padronizados para ter média zero e norma unitária [^1].
2. **Identificação do Preditores Mais Correlacionado:** Em cada etapa, o algoritmo identifica o preditor $x_j$ que tem a maior correlação absoluta com o resíduo atual $r$ [^1].
3. **Movimento na Direção do Mínimos Quadrados:** Em vez de ajustar completamente o preditor $x_j$, o coeficiente $\beta_j$ é movido de 0 em direção ao seu coeficiente de mínimos quadrados $(x_j, r)$ até que outro competidor $x_k$ tenha tanta correlação com o resíduo atual quanto $x_j$ [^1].
4. **Movimento Conjunto:** Os coeficientes $\beta_j$ e $\beta_k$ são então movidos na direção definida por seu coeficiente conjunto de mínimos quadrados do resíduo atual em $(x_j, x_k)$, até que outro competidor $x_l$ tenha tanta correlação com o resíduo atual [^1].
5. **Continuação:** Este processo continua até que todos os $p$ preditores tenham sido inseridos. Após min(N-1, p) passos, chegamos à solução de mínimos quadrados completa [^1].

**Detalhes Matemáticos:**

Seja $A_k$ o conjunto ativo de variáveis no início da *k*-ésima etapa, e seja $\beta_{A_k}$ o vetor de coeficientes para essas variáveis nesta etapa. Haverá $k-1$ valores diferentes de zero e o recém-inserido será zero. Se $r_k = y - X_A \beta_{A_k}$ é o resíduo atual, então a direção para esta etapa é dada por [^1]:
$$\
\delta_k = (X_A^T X_A)^{-1} X_A^T r_k
$$
O perfil do coeficiente então evolui como $\beta_A(\alpha) = \beta_{A_k} + \alpha \cdot \delta_k$ [^1].

**Verificação da Direção:**

O Exercício 3.23 [^1] verifica que as direções escolhidas desta forma fazem o que é afirmado: manter as correlações amarradas e decrescentes. Isso significa que, à medida que o algoritmo avança, as correlações absolutas dos preditores no conjunto ativo com o resíduo atual permanecem iguais e diminuem monotonicamente [^1].

**Interpretação Geométrica:**

O nome "ângulo mínimo" surge de uma interpretação geométrica desse processo; $u_k$ faz o menor (e igual) ângulo com cada um dos preditores em $A_k$ (Exercício 3.24) [^1].

### Conclusão

O algoritmo LAR oferece uma abordagem sistemática para selecionar variáveis em modelos de regressão linear. Sua capacidade de controlar o ângulo entre os preditores ativos e o resíduo garante que o modelo seja construído de forma parcimoniosa e estável. A conexão com o lasso, explorada em seções posteriores, destaca ainda mais a importância e a versatilidade do LAR no campo da modelagem estatística.

### Referências
[^1]: The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (2009) Trevor Hastie, Robert Tibshirani, Jerome Friedman

<!-- END -->