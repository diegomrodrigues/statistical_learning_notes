## The Dantzig Selector: An Alternative Approach to Lasso for Sparse Regression

<imagem: Um mapa mental mostrando a rela√ß√£o entre Lasso, Dantzig Selector e outros m√©todos de regulariza√ß√£o, com √™nfase nas diferen√ßas de suas fun√ß√µes objetivo e propriedades de esparsidade. Incluir caixas de texto com explica√ß√µes concisas das caracter√≠sticas de cada m√©todo.>

### Introdu√ß√£o

No contexto da regress√£o linear com um grande n√∫mero de preditores, m√©todos de regulariza√ß√£o como o Lasso se tornaram ferramentas essenciais para lidar com overfitting e para promover a esparsidade nas solu√ß√µes. O **Lasso**, como abordado em [^4.2], [^4.5.1], minimiza uma fun√ß√£o de custo que inclui a soma dos erros quadrados e uma penalidade L1 sobre os coeficientes, induzindo solu√ß√µes esparsas onde alguns coeficientes s√£o exatamente zero. Contudo, uma alternativa, o **Dantzig Selector (DS)**, introduzido por Candes e Tao (2007), oferece uma abordagem diferente ao problema, com foco na minimiza√ß√£o de uma norma diferente dos res√≠duos. Este cap√≠tulo explora o Dantzig Selector como uma alternativa ao Lasso, destacando suas diferen√ßas, propriedades te√≥ricas, e implica√ß√µes pr√°ticas.

### Conceitos Fundamentais

**Conceito 1: O Problema de Classifica√ß√£o e Modelos Lineares**
O problema da classifica√ß√£o, como discutido em [^4.1] e [^4.2], busca construir uma fronteira de decis√£o que separa diferentes classes de dados. M√©todos lineares, como a regress√£o linear aplicada a matrizes indicadoras [^4.2], fornecem solu√ß√µes simples e computacionalmente eficientes, embora com limita√ß√µes no que tange √† sua capacidade de capturar rela√ß√µes n√£o-lineares. A esparsidade, onde alguns coeficientes de regress√£o s√£o exatamente zero, pode levar a uma maior interpretabilidade do modelo e a um melhor desempenho em situa√ß√µes de alta dimensionalidade.

> üí° **Exemplo Num√©rico:** Imagine um problema de classifica√ß√£o de emails em "spam" (classe 1) ou "n√£o spam" (classe 0), onde cada email √© representado por um vetor de caracter√≠sticas (por exemplo, frequ√™ncia de certas palavras). Podemos criar uma matriz de indicadores Y onde a primeira coluna indica se o email √© spam (1) ou n√£o (0) e tentar aplicar regress√£o linear para criar um classificador linear. Se tivermos 1000 emails e 100 caracter√≠sticas, teremos uma matriz X de 1000x100 e um vetor Y de 1000x1. O modelo buscar√° um vetor de coeficientes $\beta$ que minimiza o erro quadr√°tico m√©dio da predi√ß√£o da classe. A esparsidade aqui significa que apenas algumas palavras (caracter√≠sticas) ser√£o relevantes para classificar emails como spam ou n√£o spam, e os coeficientes correspondentes a outras palavras ser√£o zero ou pr√≥ximos de zero.
 
**Lemma 1:** *Em cen√°rios onde a matriz de covari√¢ncia das classes √© conhecida, a regress√£o linear sobre uma matriz indicadora pode se equivaler a uma an√°lise discriminante linear (LDA) [^4.3].* Contudo, a regress√£o linear pode levar a extrapola√ß√µes fora do intervalo [0,1] para estimativas de probabilidade, conforme apontado em [^4.4], enquanto LDA fornece estimativas mais est√°veis, como discutido em [^4.4.1].

**Conceito 2: Linear Discriminant Analysis (LDA)**
A LDA [^4.3] √© um m√©todo cl√°ssico de classifica√ß√£o que assume distribui√ß√µes gaussianas para cada classe e busca projetar os dados em um subespa√ßo de menor dimens√£o que maximiza a separa√ß√£o entre as classes. A fun√ß√£o discriminante linear em LDA [^4.3.1] define uma fronteira de decis√£o linear, sendo a dire√ß√£o dessa fronteira determinada pelos par√¢metros do modelo, incluindo a m√©dia e a covari√¢ncia das classes. A LDA tem forte liga√ß√£o com a regress√£o linear e o conceito de proje√ß√£o sobre hiperplanos, sendo discutido tamb√©m como a an√°lise discriminante regularizada pode ser uma extens√£o do m√©todo [^4.3.3].

```mermaid
graph LR
    subgraph "LDA Components"
        direction TB
        A["Assumes Gaussian distributions for each class"]
        B["Projects data to lower dimension"]
        C["Maximizes separation between classes"]
        D["Defines a linear decision boundary"]
        A --> B
        B --> C
        C --> D
    end
```

**Corol√°rio 1:** *Quando as covari√¢ncias de classe s√£o iguais, LDA produz fronteiras de decis√£o lineares [^4.3.1] e pode ser vista como um caso particular da regress√£o de indicadores, conforme o Lemma 1.* Sob certas condi√ß√µes, os resultados de LDA e regress√£o log√≠stica, como abordado em [^4.4], convergem, destacando as similaridades de ambas as abordagens [^4.5].

**Conceito 3: Regress√£o Log√≠stica e Maximiza√ß√£o da Verossimilhan√ßa**
A regress√£o log√≠stica [^4.4] utiliza uma fun√ß√£o log√≠stica para modelar a probabilidade de pertin√™ncia a uma classe. O modelo √© ajustado maximizando a verossimilhan√ßa dos dados. O logit [^4.4.1] √© uma transforma√ß√£o da probabilidade em um modelo linear, tornando-o adequado para aplicar m√©todos de otimiza√ß√£o. A regress√£o log√≠stica tamb√©m aborda a quest√£o de separabilidade de hiperplanos, como discutido em [^4.5.2]. Em casos com classes desbalanceadas [^4.4.2], o modelo pode ser penalizado para favorecer a classe minorit√°ria.

```mermaid
graph LR
    subgraph "Logistic Regression"
        direction TB
        A["Uses logistic function for probability"]
        B["Model fitted by maximizing likelihood"]
        C["Logit transforms probability into linear model"]
        D["Addresses hyperplane separability"]
        E["Penalized for imbalanced classes"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

<imagem: Um diagrama de fluxo que ilustra o processo de regress√£o de indicadores para classifica√ß√£o, incluindo etapas como codifica√ß√£o de classes, ajuste do modelo linear via m√≠nimos quadrados, e aplica√ß√£o da regra de decis√£o. Incluir uma breve explica√ß√£o em cada etapa, enfatizando como as estimativas dos coeficientes s√£o usadas para realizar a classifica√ß√£o.>

Como mencionado em [^4.2], a regress√£o linear pode ser utilizada para classifica√ß√£o por meio de uma matriz de indicadores, onde cada coluna representa uma classe. No entanto, essa abordagem tem limita√ß√µes, pois o objetivo da regress√£o linear √© minimizar o erro quadr√°tico m√©dio, enquanto a classifica√ß√£o requer predi√ß√µes precisas das classes.

A regress√£o linear aplicada a matrizes de indicadores minimiza a soma dos quadrados dos res√≠duos (RSS), como mostrado em [^4.2]:

$$ RSS(\beta) = \sum_{i=1}^{N} (y_i - f(x_i))^2 $$

onde $f(x)$ √© o modelo linear e $y_i$ √© o valor observado, que pode ser uma codifica√ß√£o da classe.

> üí° **Exemplo Num√©rico:** Suponha que temos 5 amostras com duas classes (0 e 1) e uma √∫nica caracter√≠stica. Os dados s√£o:
>
> | Amostra | Caracter√≠stica (x) | Classe (y) |
> |---|---|---|
> | 1 | 1 | 0 |
> | 2 | 2 | 0 |
> | 3 | 3 | 1 |
> | 4 | 4 | 1 |
> | 5 | 5 | 1 |
>
> A matriz X √©:
>
> ```
> [[1, 1],
>  [1, 2],
>  [1, 3],
>  [1, 4],
>  [1, 5]]
> ```
> onde a primeira coluna √© o intercepto.
>
> O vetor y √©:
>
> ```
> [[0],
>  [0],
>  [1],
>  [1],
>  [1]]
> ```
>
> Aplicando regress√£o linear, o objetivo √© encontrar $\beta$ ($\beta_0$ e $\beta_1$) que minimiza  $\sum_{i=1}^{5} (y_i - (\beta_0 + \beta_1 x_i))^2$. Usando numpy para calcular os coeficientes:
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])
> y = np.array([0, 0, 1, 1, 1])
>
> model = LinearRegression()
> model.fit(X, y)
> beta0 = model.intercept_
> beta1 = model.coef_[1]
>
> print(f"Beta0: {beta0:.2f}")
> print(f"Beta1: {beta1:.2f}")
> ```
> Isso nos dar√°  $\beta_0 \approx -0.6$ e $\beta_1 \approx 0.4$. A fronteira de decis√£o seria dada por $\beta_0 + \beta_1 x = 0.5$, que implica que a fronteira de decis√£o √© aproximadamente $x = 2.75$.  Valores abaixo de 2.75 seriam classificados como 0, e acima como 1.

```mermaid
graph LR
    subgraph "Linear Regression for Classification"
        direction TB
        A["Minimizes RSS:  ‚àë(yi - f(xi))¬≤"]
        B["f(x) is a linear model"]
        C["yi is class encoding"]
        D["Finds coefficients Œ≤ to minimize error"]
        A --> B
        B --> C
        C --> D
    end
```

**Lemma 2:** *Sob a hip√≥tese de que as classes s√£o separ√°veis por um hiperplano, a regress√£o linear com matriz indicadora encontra coeficientes que definem tal hiperplano [^4.2], [^4.5.2].* No entanto, essa solu√ß√£o pode n√£o ser √∫nica e pode sofrer de instabilidade quando h√° multicolinearidade nas caracter√≠sticas, uma discuss√£o mencionada em [^4.3].

**Corol√°rio 2:** *A proje√ß√£o dos pontos em um hiperplano de decis√£o derivado de regress√£o linear, sob certas condi√ß√µes, se assemelha √† proje√ß√£o obtida via LDA [^4.3].* Essa equival√™ncia destaca a interconex√£o entre diferentes m√©todos de classifica√ß√£o linear, como em [^4.3.3], e refor√ßa que o objetivo primordial √© encontrar o melhor hiperplano de decis√£o [^4.5.2].

A regress√£o de indicadores √© uma abordagem direta, mas pode ser menos eficiente em termos de estimativas de probabilidade se comparada √† regress√£o log√≠stica, conforme discutido em [^4.4]. A regress√£o linear pode ter problemas para gerar probabilidades bem calibradas, dado que seu output n√£o √© naturalmente restrito ao intervalo [0,1], diferente da regress√£o log√≠stica onde o output √© mapeado nesse intervalo atrav√©s da fun√ß√£o sigm√≥ide [^4.4.1].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

<imagem: Diagrama mostrando um fluxo de trabalho para sele√ß√£o de vari√°veis em modelos lineares, comparando regulariza√ß√£o L1 e L2 em regress√£o log√≠stica, com exemplos matem√°ticos das fun√ß√µes de custo e como estas levam a solu√ß√µes esparsas e est√°veis. Incluir equa√ß√µes matem√°ticas relevantes com explica√ß√µes detalhadas.>

A sele√ß√£o de vari√°veis √© crucial para lidar com modelos de alta dimensionalidade, onde o n√∫mero de preditores √© grande. A regulariza√ß√£o, como discutido em [^4.4.4], [^4.5], √© uma t√©cnica chave para controlar a complexidade do modelo, evitando o overfitting e promovendo a esparsidade.

Regulariza√ß√£o L1 (Lasso) [^4.4.4] adiciona uma penalidade proporcional √† soma dos valores absolutos dos coeficientes, resultando em coeficientes exatamente iguais a zero:

$$ Cost(\beta) = - \sum_i y_i \log(\sigma(x_i^T \beta)) + (1 - y_i) \log(1 - \sigma(x_i^T \beta)) + \lambda \sum_{j=1}^p |\beta_j| $$

onde $\sigma$ √© a fun√ß√£o log√≠stica, e $\lambda$ √© o par√¢metro de regulariza√ß√£o.

Regulariza√ß√£o L2 (Ridge) adiciona uma penalidade proporcional √† soma dos quadrados dos coeficientes, encolhendo seus valores, mas raramente levando-os a zero. Essa regulariza√ß√£o √© discutida em detalhes em [^4.5.1]. A penaliza√ß√£o L2 √© geralmente expressa como:

$$ Cost(\beta) = - \sum_i y_i \log(\sigma(x_i^T \beta)) + (1 - y_i) \log(1 - \sigma(x_i^T \beta)) + \lambda \sum_{j=1}^p \beta_j^2 $$

O elastic net [^4.5] combina as penaliza√ß√µes L1 e L2, aproveitando a esparsidade do Lasso e a estabilidade do Ridge.

> üí° **Exemplo Num√©rico:**  Vamos considerar um problema de regress√£o log√≠stica com duas caracter√≠sticas ($x_1$ e $x_2$) e classe bin√°ria (y). A fun√ß√£o de custo log√≠stica com penaliza√ß√£o L1 (Lasso) seria:
>
> $$ Cost(\beta) = - \sum_i y_i \log(\sigma(x_{i1}\beta_1 + x_{i2}\beta_2 + \beta_0)) + (1 - y_i) \log(1 - \sigma(x_{i1}\beta_1 + x_{i2}\beta_2 + \beta_0)) + \lambda (|\beta_1| + |\beta_2|) $$
>
> Onde $\sigma(z) = \frac{1}{1 + e^{-z}}$. Suponha que temos 100 amostras e, ap√≥s a otimiza√ß√£o, obtemos os seguintes resultados para diferentes valores de $\lambda$:
>
> | $\lambda$    | $\beta_0$    | $\beta_1$    | $\beta_2$    |
> |------|-------|-------|-------|
> | 0.01 | 0.5   | 1.2   | -0.8  |
> | 0.1  | 0.3   | 0.7   | -0.2  |
> | 1    | 0.1   | 0.0   | -0.0  |
>
> Quando $\lambda$=0.01, ambos os coeficientes s√£o n√£o-nulos, indicando que ambas as caracter√≠sticas contribuem para o modelo. Conforme $\lambda$ aumenta, os coeficientes s√£o encolhidos. Quando $\lambda$=1, ambos os coeficientes s√£o zero, indicando que o modelo s√≥ usar√° o intercepto e a classifica√ß√£o seria feita com base nele, n√£o usando as caracter√≠sticas $x_1$ e $x_2$.
>
> Para o caso da penaliza√ß√£o L2 (Ridge), as mudan√ßas nos coeficientes s√£o similares, mas tendem a zero sem nunca atingi-lo, por exemplo:
>
> | $\lambda$   | $\beta_0$    | $\beta_1$   | $\beta_2$    |
> |-----|-------|------|-------|
> | 0.01| 0.49 | 1.1  | -0.75 |
> | 0.1 | 0.45 | 0.8  | -0.30 |
> | 1   | 0.1  | 0.2  | -0.08 |
>
>Aqui, os coeficientes sempre permanecem diferentes de zero.

```mermaid
graph LR
    subgraph "Regularization Methods"
        direction TB
        A["L1 Regularization (Lasso)"]
        B["Cost(Œ≤) = -Œ£yi log(œÉ(xiTŒ≤)) + (1 - yi) log(1 - œÉ(xiTŒ≤)) + ŒªŒ£|Œ≤j|"]
        C["Induces sparsity"]
        D["L2 Regularization (Ridge)"]
        E["Cost(Œ≤) = -Œ£yi log(œÉ(xiTŒ≤)) + (1 - yi) log(1 - œÉ(xiTŒ≤)) + ŒªŒ£Œ≤j¬≤"]
        F["Shrinks coefficients, rarely zero"]
        G["Elastic Net"]
        H["Combines L1 and L2"]
        A --> B
        B --> C
        D --> E
        E --> F
        G --> H
        H --> C & F
    end
```

**Lemma 3:** *A penaliza√ß√£o L1 na regress√£o log√≠stica induz solu√ß√µes esparsas devido √† natureza n√£o-diferenci√°vel da norma L1 em zero, enquanto a penaliza√ß√£o L2 induz solu√ß√µes que tendem para zero, mas raramente se tornam zero [^4.4.4], [^4.5.1].* A prova desse lemma envolve a an√°lise das condi√ß√µes de otimalidade para cada tipo de penaliza√ß√£o, e √© detalhado em [^4.4.3] e [^4.4.5].

**Prova do Lemma 3:**
A condi√ß√£o de otimalidade para a regress√£o log√≠stica com penalidade L1, sem intercepto, √© dada por:

$$  \nabla J(\beta) + \lambda \partial  || \beta ||_1 = 0 $$

onde $\partial || \beta ||_1$ √© o subgradiente da norma L1.
Em componentes:
$$  \nabla J(\beta_j) + \lambda \text{sign}(\beta_j) = 0, $$
se $\beta_j \neq 0$ e
$$  |\nabla J(\beta_j)| \leq \lambda $$
se $\beta_j = 0$.
Isso significa que para que um coeficiente seja n√£o-zero, a magnitude do seu gradiente deve ser exatamente $\lambda$, e para que um coeficiente seja zero, a magnitude do seu gradiente deve ser no m√°ximo $\lambda$. Essa condi√ß√£o de igualdade n√£o ocorre com frequ√™ncia para coeficientes n√£o-nulos, levando a esparsidade.
Para o caso da penaliza√ß√£o L2 a condi√ß√£o de otimalidade para um $\beta_j$ √©
$$ \nabla J(\beta_j) + 2 \lambda \beta_j = 0  $$
ou
$$ \beta_j = - \frac{1}{2\lambda} \nabla J(\beta_j) $$
Portanto, os coeficientes tendem a zero, mas n√£o necessariamente se tornam zero. $\blacksquare$

**Corol√°rio 3:** *Modelos com regulariza√ß√£o L1 s√£o mais interpret√°veis devido √† esparsidade dos coeficientes, facilitando a identifica√ß√£o de caracter√≠sticas mais relevantes [^4.4.5].* Essa propriedade √© fundamental em √°reas como a biologia e economia onde a interpretabilidade √© t√£o importante quanto a performance preditiva, como destacado em [^4.5].

### Separating Hyperplanes e Perceptrons

<imagem: Um diagrama explicando a ideia de hiperplanos separadores e a converg√™ncia do Perceptron. Ilustrar graficamente como o Perceptron ajusta seus pesos iterativamente para encontrar um hiperplano que separa as classes de dados linearmente separ√°veis. Incluir exemplos de como diferentes ajustes de peso podem levar a diferentes hiperplanos, e quando o algoritmo para de iterar por j√° ter convergido.>

O conceito de **hiperplanos separadores**, como abordado em [^4.5.2], est√° intimamente ligado √† ideia de encontrar uma fronteira de decis√£o linear que maximize a margem entre diferentes classes. Um hiperplano separador √≥timo busca maximizar essa margem, o que pode ser formulado como um problema de otimiza√ß√£o. Os pontos de suporte s√£o aqueles mais pr√≥ximos do hiperplano e que influenciam diretamente sua posi√ß√£o.

A formula√ß√£o matem√°tica do problema de hiperplanos separadores envolve a otimiza√ß√£o de uma fun√ß√£o de custo com restri√ß√µes que garantem a separa√ß√£o das classes, conforme descrito em [^4.5.2].

O **Perceptron**, como descrito em [^4.5.1], √© um algoritmo de aprendizagem supervisionada capaz de encontrar um hiperplano separador linear para dados linearmente separ√°veis. O algoritmo itera sobre os dados, ajustando os pesos do modelo at√© que todos os pontos sejam corretamente classificados. Sob certas condi√ß√µes, o Perceptron √© garantidamente convergente, como abordado em [^4.5.1].

```mermaid
graph LR
    subgraph "Separating Hyperplanes and Perceptron"
        direction TB
        A["Hyperplane maximizes margin between classes"]
        B["Support vectors influence hyperplane position"]
        C["Perceptron: supervised algorithm"]
        D["Finds separating hyperplane for linearly separable data"]
         E["Iteratively adjusts weights"]
         F["Guaranteed convergence under conditions"]
         A --> B
         C --> D
         D --> E
         E --> F
    end
```

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre o Lasso e o Dantzig Selector (DS)?

**Resposta:**
O Lasso e o DS s√£o m√©todos de regulariza√ß√£o para modelos lineares, ambos buscando solu√ß√µes esparsas, mas com abordagens diferentes. O Lasso, como discutido em [^4.4.4], minimiza a soma dos erros quadrados com uma penalidade L1 sobre os coeficientes. O DS, por outro lado, minimiza a norma L1 dos coeficientes, sujeito a uma restri√ß√£o na norma L‚àû do produto da matriz de design transposta com o res√≠duo, conforme discutido em [^3.8.3].

Enquanto o Lasso, busca um balan√ßo entre a minimiza√ß√£o do erro e a complexidade do modelo, como discutido em [^3.2.2], o DS se concentra em encontrar coeficientes que minimizem sua norma L1, garantindo que o erro residual esteja bem controlado.

**Lemma 4:** *A solu√ß√£o do DS pode ser encontrada como um problema de programa√ß√£o linear [^3.8.3], enquanto a solu√ß√£o do Lasso geralmente requer m√©todos de otimiza√ß√£o n√£o lineares [^3.4.2].*

**Corol√°rio 4:** *Em certas condi√ß√µes, o DS pode identificar corretamente os preditores relevantes com alta probabilidade [^3.8.5], uma propriedade que o torna atraente em cen√°rios de alta dimensionalidade.* A escolha entre os dois m√©todos depende do cen√°rio espec√≠fico e dos objetivos de modelagem, com o Lasso sendo um m√©todo mais popular devido a sua facilidade de implementa√ß√£o e vasta literatura.

> ‚ö†Ô∏è **Ponto Crucial**: Embora ambos Lasso e Dantzig Selector promovam esparsidade, eles diferem na forma como penalizam a complexidade do modelo e garantem que a condi√ß√£o de otimalidade seja satisfeita, como discutido em [^3.8.3] e [^3.8.5].

A principal diferen√ßa reside na fun√ß√£o objetivo: enquanto o Lasso visa minimizar a soma dos quadrados dos res√≠duos com uma penalidade L1 na norma dos coeficientes, o Dantzig Selector busca minimizar a norma L1 dos coeficientes sujeito a uma restri√ß√£o na norma L‚àû do gradiente do erro. Essa diferen√ßa fundamental leva a propriedades diferentes para os dois modelos. O Lasso tende a fornecer estimativas com maior esparsidade, ou seja, tende a zerar um maior n√∫mero de coeficientes, enquanto o Dantzig Selector pode ser mais eficiente em identificar os preditores corretos com alta probabilidade em cen√°rios espec√≠ficos, como discutido em [^3.8.5] e [^3.8.3].

```mermaid
graph LR
    subgraph "Lasso vs Dantzig Selector"
        direction TB
        A["Lasso: Minimizes RSS + L1 penalty on coefficients"]
        B["Dantzig Selector: Minimizes L1 norm of coefficients, subject to constraint on L‚àû norm of residual gradient"]
        C["Lasso balances error and model complexity"]
         D["Dantzig Selector minimizes L1 norm, controls residual error"]
         A --> C
        B --> D
    end
```

### Dantzig Selector: Formula√ß√£o Detalhada

O Dantzig Selector (DS) pode ser definido pela seguinte otimiza√ß√£o [^3.8.3]:

$$ \min_{\beta} ||\beta||_1 \quad \text{sujeito a} \quad ||X^T (y - X\beta)||_\infty \leq s $$

onde:

*   $||\beta||_1$ √© a norma L1 dos coeficientes $\beta$.
*   $X$ √© a matriz de design.
*   $y$ √© o vetor de resposta.
*   $||X^T (y - X\beta)||_\infty$ √© a norma L‚àû do produto da matriz de design transposta com o res√≠duo.
*   $s$ √© um par√¢metro que controla a esparsidade.

> üí° **Exemplo Num√©rico:** Para demonstrar o DS, considere um problema de regress√£o com 5 amostras e 3 preditores (incluindo o intercepto):
>
> Matriz X:
> ```
> [[1, 2, 3],
> [1, 4, 5],
> [1, 6, 7],
> [1, 8, 9],
> [1, 10, 11]]
> ```
> Vetor y:
> ```
> [8, 12, 16, 20, 24]
> ```
> Vamos supor que o valor de s seja 2. O problema do DS √© encontrar $\beta$ que minimize $||\beta||_1$  sujeito a $||X^T (y - X\beta)||_\infty \leq 2$.
> A solu√ß√£o deste problema pode ser encontrada usando um solver de programa√ß√£o linear, mas para fins ilustrativos, vamos assumir que ap√≥s a otimiza√ß√£o encontramos: $\beta \approx [0, 1, 1]$.
>
> Podemos calcular o res√≠duo: $r = y - X\beta =  [8, 12, 16, 20, 24] - [[1, 2, 3], [1, 4, 5], [1, 6, 7], [1, 8, 9], [1, 10, 11]] \cdot [0, 1, 1]^T = [8, 12, 16, 20, 24] - [5, 9, 13, 17, 21] = [3, 3, 3, 3, 3]$.
>
> Agora vamos calcular $||X^Tr||_\infty$ :
>
> $$X^T r = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 2 & 4 & 6 & 8 & 10 \\ 3 & 5 & 7 & 9 & 11 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \\ 3 \\ 3 \\ 3 \end{bmatrix} = \begin{bmatrix} 15 \\ 90 \\ 135 \end{bmatrix}$$
>
> $$ ||X^Tr||_\infty = \max(15, 90, 135) = 135$$
>
> O res√≠duo foi calculado usando uma solu√ß√£o que n√£o considera a restri√ß√£o. Vamos supor que a solu√ß√£o √≥tima considerando a restri√ß√£o √© $\beta \approx [1, 1.8, 0.2]$.
>
> Ent√£o:
>
> $r = y - X\beta =  [8, 12, 16, 20, 24] - [[1, 2, 3], [1, 4, 5], [1, 6, 7], [1, 8, 9], [1, 10, 11]] \cdot [1, 1.8, 0.2]^T = [8, 12, 16, 20, 24] - [5.2, 8.8, 12.4, 16, 19.6] = [2.8, 3.2, 3.6, 4, 4.4]$
>
> $$X^T r = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ 2 & 4 & 6 & 8 & 10 \\ 3 & 5 & 7 & 9 & 11 \end{bmatrix} \begin{bmatrix} 2.8 \\ 3.2 \\ 3.6 \\ 4 \\ 4.4 \end{bmatrix} = \begin{bmatrix} 18 \\ 107.2 \\ 154 \end{bmatrix}$$
>
> $||X^Tr||_\infty = \max(18, 107.2, 154) = 154$
>
> A norma infinito de $X^Tr$ √© maior que $s=2$. Portanto $\beta=[1, 1.8, 0.2]$ n√£o √© uma solu√ß√£o v√°lida.  Este exemplo √© para ilustrar o c√°lculo e o entendimento da formula√ß√£o do Dantzig Selector e n√£o necessariamente para resolver o problema.
>
> O Dantzig Selector buscaria a solu√ß√£o que minimiza a norma L1 de $\beta$ sob essa restri√ß√£o.  Na pr√°tica, a solu√ß√£o para o Dantzig Selector seria encontrada usando um resolvedor de programa√ß√£o linear.

```mermaid
graph LR
    subgraph "Dantzig Selector Formulation"
        direction TB
        A["Objective: minimize ||Œ≤||‚ÇÅ"]
         B["Subject to constraint: ||X·µÄ(y - XŒ≤)||‚àû ‚â§ s"]
        C["||Œ≤||‚ÇÅ: L1 norm of coefficients"]
        D["X: design matrix"]
        E["y: response vector"]
        F["||X·µÄ(y - XŒ≤)||‚àû: L‚àû norm of design matrix transpose times the residual"]
        G["s: sparsity parameter"]
         A --> B
         B --> C
        B --> D
        B --> E
         B --> F
        B --> G
    end
```

Essa formula√ß√£o difere do Lasso, pois n√£o penaliza diretamente a soma dos erros quadrados, mas sim a norma L1 dos coeficientes, enquanto o erro √© controlado atrav√©s de uma restri√ß√£o na norma L‚àû do produto da matriz de design transposta com o res√≠duo.

### Conclus√£o

O Dantzig Selector oferece uma alternativa promissora ao Lasso para modelagem em contextos de regress√£o com dados de alta dimensionalidade. Sua abordagem focada na norma L1 dos coeficientes e na restri√ß√£o da norma L‚àû do res√≠duo leva a um equil√≠brio diferente entre a adequa√ß√£o aos dados e a esparsidade da solu√ß√£o. Enquanto o Lasso √© amplamente utilizado e compreendido, o Dantzig Selector possui propriedades te√≥ricas interessantes, especialmente em situa√ß√µes onde a recupera√ß√£o de um modelo esparso √© primordial, como mencionado em [^3.8.3] e [^3.8.5]. Explorar as diferen√ßas e nuances entre esses m√©todos de regulariza√ß√£o √© essencial para a modelagem estat√≠stica e aprendizado de m√°quina avan√ßados, utilizando os conceitos e fundamentos fornecidos ao longo deste cap√≠tulo.

### Footnotes
[^4.1]: "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,..., Xp." *(Trecho de <Linear Methods for Regression>)*
[^4.2]: "In this chapter we describe linear methods for regression, while in the next chapter we discuss linear methods for classification." *(Trecho de <Linear Methods for Regression>)*
[^4.3]: "There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X." *(Trecho de <Linear Methods for Regression>)*
[^4.3.1]: "We denote the column vectors of X by x0, x1,..., xp, with x0 = 1. For much of what follows, this first column is treated like any other." *(Trecho de <Linear Methods for Regression>)*
[^4.3.2]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de <Linear Methods for Regression>)*
[^4.3.3]: "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,..., Xp" *(Trecho de <Linear Methods for Regression>)*
[^4.4]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their popu- lation." *(Trecho de <Linear Methods for Regression>)*
[^4.4.1]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de <Linear Methods for Regression>)*
[^4.4.2]: "Typically we have a set of training data (x1, y1) ... (xn, yn) from which to estimate the parameters Œ≤." *(Trecho de <Linear Methods for Regression>)*
[^4.4.3]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their population." *(Trecho de <Linear Methods for Regression>)*
[^4.4.4]: "Even if the xi's were not drawn randomly, the criterion is still valid if the yi's are conditionally independent given the inputs xi" *(Trecho de <Linear Methods for Regression>)*
[^4.4.5]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de <Linear Methods for Regression>)*
[^4.5]: "No matter the source of the Xj, the model is linear in the parameters." *(Trecho de <Linear Methods for Regression>)*
[^4.5.1]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de <Linear Methods for Regression>)*
[^4.5.2]: "These generalizations are sometimes called basis-function methods, and are discussed in Chapter 5" *(Trecho de <Linear Methods for Regression>)*
