## Singular Value Decomposition (SVD) e Shrinkage em Modelos de Regress√£o Linear

```mermaid
graph LR
    subgraph "SVD Decomposition"
        direction LR
        A["Matrix X (N x p)"] --> B["U (N x p) Orthogonal Matrix"]
        A --> C["D (p x p) Diagonal Matrix"]
        A --> D["V^T (p x p) Orthogonal Matrix Transposed"]
        B & C & D --> E["X = U D V^T"]
    end
    subgraph "SVD Components"
        direction TB
         E --> F["U: Left Singular Vectors, spanning column space of X"]
         E --> G["D: Singular Values (d1 >= d2 >= ... >= dp >= 0)"]
         E --> H["V: Right Singular Vectors, spanning row space of X"]
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora em detalhes a **Singular Value Decomposition (SVD)** e seu papel crucial na interpreta√ß√£o de m√©todos de regulariza√ß√£o, com √™nfase em **shrinkage methods** em regress√£o linear [^3.4]. A SVD n√£o apenas oferece uma forma alternativa de expressar a matriz de dados, mas tamb√©m revela informa√ß√µes valiosas sobre a estrutura de depend√™ncia entre as vari√°veis preditoras. Ao conectar a SVD com t√©cnicas de **principal component analysis (PCA)** e **ridge regression**, podemos obter uma compreens√£o profunda de como esses m√©todos afetam os coeficientes e as previs√µes em modelos de regress√£o linear.

### Conceitos Fundamentais

**Conceito 1:** **SVD e sua Decomposi√ß√£o**
A Singular Value Decomposition (SVD) de uma matriz $X$ de dimens√£o $N \times p$ √© dada por:
$$X = U D V^T$$
onde $U$ √© uma matriz $N \times p$ ortogonal, $V$ √© uma matriz $p \times p$ ortogonal, e $D$ √© uma matriz diagonal $p \times p$ com entradas n√£o negativas $d_1 \geq d_2 \geq \dots \geq d_p \geq 0$, chamados de **valores singulares** de $X$ [^3.4.1]. Os valores singulares quantificam a ‚Äúimport√¢ncia‚Äù ou ‚Äúmagnitude‚Äù das dire√ß√µes correspondentes em $X$. Se algum $d_j = 0$, a matriz X √© singular.
> üí° **Exemplo Num√©rico:**
> Vamos considerar uma matriz $X$ com 3 amostras e 2 vari√°veis preditoras:
> ```python
> import numpy as np
> from numpy.linalg import svd
> X = np.array([[1, 2], [3, 4], [5, 6]])
> U, D, VT = svd(X)
> print("Matriz U:\n", U)
> print("Valores Singulares D:\n", D)
> print("Matriz VT:\n", VT)
> ```
> A SVD decomp√µe $X$ em $U$, $D$ e $V^T$. Os valores singulares $D$ (diagonal da matriz D) indicam a import√¢ncia de cada dire√ß√£o. Neste exemplo, $d_1 \approx 9.52$ e $d_2 \approx 0.51$, indicando que a primeira componente principal (correspondente a $d_1$) captura a maior parte da variabilidade nos dados. A matriz U cont√©m os autovetores de $XX^T$ e V os autovetores de $X^TX$

**Lemma 1:** *A matriz $X^TX$ pode ser decomposta como $V D^2 V^T$, onde os vetores $v_j$ (colunas de $V$) s√£o os autovetores de $X^TX$ e $d_j^2$ s√£o seus autovalores correspondentes* [^3.4.1].
Essa decomposi√ß√£o conecta a SVD √† an√°lise de autovalores, um conceito central em **Principal Component Analysis (PCA)**.
```mermaid
graph LR
    subgraph "Lemma 1: X^TX Decomposition"
        direction LR
        A["X^T X"] --> B["V (p x p) Eigenvectors of X^TX"]
        A --> C["D^2 (p x p) Squared Singular Values"]
        A --> D["V^T (p x p) Transposed Eigenvectors"]
        B & C & D --> E["X^TX = V D^2 V^T"]
    end
```
> üí° **Exemplo Num√©rico:**
> Usando a matriz $X$ do exemplo anterior, podemos verificar este Lemma:
> ```python
> XT_X = X.T @ X
> print("X^T * X:\n", XT_X)
> D_squared = np.diag(D**2)
> print("Matriz D^2:\n",D_squared)
> V = VT.T
> result = V @ D_squared @ VT
> print("V * D^2 * VT:\n", result)
> print("V:\n", V)
> ```
> Calculando $X^TX$ e $VD^2V^T$, podemos verificar que os resultados s√£o iguais (a menos de erros de arredondamento). As colunas de $V$ s√£o os autovetores de $X^TX$, e os elementos diagonais de $D^2$ s√£o os autovalores correspondentes.

**Conceito 2:** **Principal Components e sua Rela√ß√£o com SVD**
As colunas da matriz $V$, denotadas por $v_j$, s√£o os **principal components** de $X$ ou **Karhunen-Loeve directions** [^3.4.1]. A componente principal $z_j = Xv_j = u_jd_j$, onde $u_j$ s√£o as colunas de $U$, corresponde a uma combina√ß√£o linear das vari√°veis originais de $X$ que captura a m√°xima vari√¢ncia poss√≠vel dos dados. Cada componente $z_j$ √© ortogonal √†s outras, e suas vari√¢ncias s√£o proporcionais aos valores singulares $d_j^2/N$. A primeira componente $z_1$ captura a maior variabilidade, seguida pela segunda $z_2$, e assim por diante [^3.4.1].
```mermaid
graph LR
    subgraph "Principal Components"
       direction LR
       A["X (N x p)"] --> B["vj (p x 1) Principal Components (columns of V)"]
       B --> C["zj = Xvj"]
       C --> D["zj = uj * dj"]
    end
     subgraph "PCA Properties"
        direction TB
        D --> E["zj are orthogonal"]
        D --> F["Variance of zj proportional to dj^2/N"]
    end
```
> ‚ö†Ô∏è **Nota Importante**: A SVD fornece uma base ortogonal para o espa√ßo coluna de X, que simplifica a an√°lise e a visualiza√ß√£o de dados de alta dimens√£o.
> üí° **Exemplo Num√©rico:**
> Usando o exemplo anterior, calculamos as componentes principais $z_j$:
> ```python
> z1 = X @ V[:, 0]
> z2 = X @ V[:, 1]
> print("Componente Principal z1:\n", z1)
> print("Componente Principal z2:\n", z2)
> ```
> A componente $z_1$ √© a dire√ß√£o que captura a maior variabilidade dos dados em $X$. As componentes $z_1$ e $z_2$ s√£o ortogonais e as vari√¢ncias s√£o proporcionais a $d_1^2/N$ e $d_2^2/N$.

**Corol√°rio 1:** *A vari√¢ncia de $z_j$ √© dada por $d_j^2/N$, onde $N$ √© o n√∫mero de amostras. Componentes com valores singulares maiores correspondem a dire√ß√µes em $X$ com maior variabilidade, enquanto valores singulares pequenos indicam dire√ß√µes com menor variabilidade* [^3.4.1].
> üí° **Exemplo Num√©rico:**
> Para a matriz $X$ anterior (N=3), a vari√¢ncia da primeira componente principal √© $d_1^2/3 \approx 9.52^2/3 \approx 30.25$ e a da segunda √© $d_2^2/3 \approx 0.51^2/3 \approx 0.0867$. A primeira componente tem uma vari√¢ncia muito maior que a segunda.
> ```python
> N = X.shape[0]
> variance_z1 = D[0]**2 / N
> variance_z2 = D[1]**2 / N
> print("Vari√¢ncia da componente z1:", variance_z1)
> print("Vari√¢ncia da componente z2:", variance_z2)
> ```

**Conceito 3:** **Shrinkage via SVD**
A SVD fornece uma base para entender como o **shrinkage** funciona em m√©todos como **ridge regression** [^3.4.1].  As solu√ß√µes de **least squares (LS)** podem ser expressas como:
$$ \hat{y}_{ls} = X(X^TX)^{-1}X^Ty = U U^T y $$
ou seja, $\hat{y}_{ls}$ √© obtida atrav√©s de uma proje√ß√£o ortogonal de $y$ sobre o espa√ßo coluna de $X$, usando a base ortonormal $U$. As solu√ß√µes da **ridge regression** s√£o obtidas como:
$$ \hat{y}_{ridge} = X(X^TX + \lambda I)^{-1}X^Ty = U D(D^2 + \lambda I)^{-1} D U^T y$$
Em resumo, a **ridge regression** usa a mesma base ortonormal $U$ para projetar os dados, mas reduz os componentes de $y$ ao longo das dire√ß√µes de componentes principais por um fator $d_j^2/(d_j^2 + \lambda)$, onde $\lambda$ √© o par√¢metro de regulariza√ß√£o [^3.4.1]. Este fator reduz o impacto das dire√ß√µes de menor vari√¢ncia nos dados.
```mermaid
graph LR
    subgraph "Least Squares Projection"
        direction LR
        A["y"] --> B["U (N x p)"]
        B --> C["U^T (p x N)"]
         C --> D["y_hat_ls = UU^Ty"]
    end
    subgraph "Ridge Regression Projection"
       direction LR
       E["y"] --> F["U (N x p)"]
       F --> G["D (p x p)"]
       G --> H["(D^2 + lambda*I)^-1"]
       H --> I["D (p x p)"]
       I --> J["U^T (p x N)"]
       J --> K["y_hat_ridge = UD(D^2 + lambda*I)^-1DU^Ty"]
    end
      subgraph "Ridge Shrinkage"
        direction TB
        K --> L["Shrinkage factor: dj^2/(dj^2 + lambda)"]
        L --> M["Reduces impact of directions with small singular values"]
      end
```
> üí° **Exemplo Num√©rico:**
> Vamos gerar um vetor de resposta $y$ para o exemplo anterior e comparar as predi√ß√µes do OLS com a Ridge:
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression, Ridge
> y = np.array([5, 12, 19])
> # Least Squares
> model_ls = LinearRegression()
> model_ls.fit(X, y)
> y_hat_ls = model_ls.predict(X)
> print("Predi√ß√µes Least Squares:\n", y_hat_ls)
> # Ridge Regression
> lambda_ridge = 1
> model_ridge = Ridge(alpha=lambda_ridge, fit_intercept = False) # fit_intercept = False para simplificar o exemplo.
> model_ridge.fit(X, y)
> y_hat_ridge = model_ridge.predict(X)
> print("Predi√ß√µes Ridge Regression (Œª=1):\n", y_hat_ridge)
> ```
> As predi√ß√µes de Ridge s√£o afetadas pelo par√¢metro $\lambda$ e sofrem um 'encolhimento' em compara√ß√£o com as predi√ß√µes de least squares. As predi√ß√µes $y_{ridge}$ tendem a ser menores em magnitude, especialmente nas dire√ß√µes de menor vari√¢ncia.
>

> ‚ùó **Ponto de Aten√ß√£o**:  O **shrinkage** da **ridge regression** n√£o apenas reduz o tamanho dos coeficientes, mas tamb√©m os torna menos sens√≠veis ao ru√≠do nas vari√°veis de entrada.

### Regress√£o Linear e M√≠nimos Quadrados

A regress√£o linear busca minimizar a soma dos quadrados dos res√≠duos (RSS), ou seja:

$$ RSS(\beta) = \sum_{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 $$

Onde os coeficientes $\beta$ s√£o estimados atrav√©s dos m√≠nimos quadrados, cuja solu√ß√£o pode ser expressa como:

$$\hat{\beta} = (X^TX)^{-1}X^Ty$$

Em termos da SVD de $X = UDV^T$, a solu√ß√£o de m√≠nimos quadrados pode ser reescrita como:

$$\hat{\beta} = V D^{-1}U^Ty$$

onde $U^Ty$ s√£o as coordenadas de $y$ no espa√ßo definido pelas colunas de $U$, e  $D^{-1}$ escala essas coordenadas para obter as estimativas dos coeficientes $\hat{\beta}$ [^3.4].
```mermaid
graph LR
  subgraph "Least Squares Solution"
    direction LR
      A["X^T X"] --> B["Inverse: (X^T X)^-1"]
      B --> C["X^T y"]
      C --> D["Beta_hat = (X^T X)^-1 X^T y"]
    end
    subgraph "Least Squares Solution using SVD"
        direction LR
        E["y"] --> F["U^T (p x N)"]
        F --> G["D^-1 (p x p)"]
        G --> H["V (p x p)"]
        H --> I["Beta_hat = V D^-1 U^T y"]
    end
```
> üí° **Exemplo Num√©rico:**
> Usando a matriz X e o vetor y dos exemplos anteriores, podemos calcular $\hat{\beta}$ diretamente e tamb√©m usando SVD.
>
> ```python
> # Calculando beta diretamente
> beta_hat_direct = np.linalg.solve(X.T @ X, X.T @ y)
> print("Beta estimado (diretamente):\n", beta_hat_direct)
> # Calculando beta usando SVD
> beta_hat_svd = V @ np.diag(1/D) @ U.T @ y
> print("Beta estimado (usando SVD):\n", beta_hat_svd)
> ```
> As duas formas de c√°lculo produzem resultados iguais (a menos de erros de arredondamento).
> A SVD decomp√µe o problema em um conjunto de opera√ß√µes mais est√°veis e informativas.

**Lemma 2:** *A solu√ß√£o de least squares $\hat{\beta}$ pode ser expressa como uma combina√ß√£o linear das coordenadas $U^Ty$ escaladas pelos inversos dos valores singulares correspondentes de $D$* [^3.4].

**Corol√°rio 2:** *Quando $X^TX$ n√£o √© invert√≠vel, ou quando os valores singulares $d_j$ s√£o pequenos, a solu√ß√£o de least squares $\hat{\beta}$ torna-se inst√°vel e de alta vari√¢ncia, o que motiva o uso de t√©cnicas de regulariza√ß√£o como ridge regression* [^3.4].
> üí° **Exemplo Num√©rico:**
> Para demonstrar a instabilidade quando os valores singulares s√£o pequenos, vamos adicionar uma nova coluna a $X$ que seja quase uma combina√ß√£o linear da primeira coluna, resultando em um valor singular pequeno.
> ```python
> X_colinear = np.array([[1, 2, 1.01], [3, 4, 3.03], [5, 6, 5.05]])
> U_c, D_c, VT_c = svd(X_colinear)
> print("Valores singulares da matriz colinear:\n", D_c)
> beta_hat_colinear = np.linalg.solve(X_colinear.T @ X_colinear, X_colinear.T @ y)
> print("Beta estimado com colinearidade:\n", beta_hat_colinear)
> ```
> Note que o valor singular $d_3$ √© muito pequeno. Isso causa alta vari√¢ncia nos coeficientes estimados pelo OLS, tornando-os inst√°veis a pequenas varia√ß√µes nos dados. A Ridge resolve esse problema.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o

A **ridge regression** busca estimar os coeficientes atrav√©s da minimiza√ß√£o da seguinte express√£o:

$$ \hat{\beta}_{ridge} = argmin_{\beta} (||y - X\beta||^2 + \lambda ||\beta||^2) $$
que pode ser expressa como:

$$\hat{\beta}_{ridge} = (X^TX + \lambda I)^{-1}X^Ty$$

onde $\lambda \geq 0$ √© o par√¢metro de regulariza√ß√£o.
Em termos da SVD de $X$, esta solu√ß√£o pode ser reescrita como:

$$\hat{\beta}_{ridge} = V(D^2 + \lambda I)^{-1}DU^Ty$$
Essa express√£o destaca como o **shrinkage** ocorre na **ridge regression**.  A matriz $D(D^2 + \lambda I)^{-1}D$ cont√©m elementos $d_j^2/(d_j^2 + \lambda)$. Os valores singulares maiores ser√£o menos afetados por este fator, enquanto os valores singulares menores (associados com dire√ß√µes de menor vari√¢ncia em $X$) ser√£o mais reduzidos [^3.4.1]. Em resumo, as dire√ß√µes de menor vari√¢ncia (menos significativas) s√£o mais afetadas pela regulariza√ß√£o do que as dire√ß√µes mais significativas, que possuem maior vari√¢ncia.
```mermaid
graph LR
  subgraph "Ridge Regression Minimization"
    direction LR
    A["Minimize (||y - XŒ≤||^2 + Œª||Œ≤||^2)"] --> B["Œ≤_hat_ridge = argmin (||y - XŒ≤||^2 + Œª||Œ≤||^2)"]
    B --> C["Œ≤_hat_ridge = (X^T X + ŒªI)^-1 X^T y"]
  end
  subgraph "Ridge Regression with SVD"
    direction LR
    D["y"] --> E["U^T"]
    E --> F["D"]
    F --> G["(D^2 + ŒªI)^-1"]
    G --> H["D"]
    H --> I["V"]
    I --> J["Œ≤_hat_ridge = V(D^2 + ŒªI)^-1 DU^T y"]
  end
  subgraph "Shrinkage Factor"
      direction TB
        J --> K["Shrinkage Factor: dj^2 / (dj^2 + Œª)"]
        K --> L["Larger Œª -> more shrinkage"]
  end
```
> üí° **Exemplo Num√©rico:**
> Vamos calcular os coeficientes da Ridge Regression usando a SVD, e comparar com a solu√ß√£o direta usando a fun√ß√£o da sklearn, variando o valor de $\lambda$.
> ```python
> from sklearn.linear_model import Ridge
> lambdas = [0.1, 1, 10]
> for lambda_ridge in lambdas:
>    # Ridge Regression (usando sklearn)
>    model_ridge = Ridge(alpha=lambda_ridge, fit_intercept = False)
>    model_ridge.fit(X, y)
>    beta_hat_ridge_sklearn = model_ridge.coef_
>    # Ridge Regression (usando SVD)
>    beta_hat_ridge_svd = V @ np.diag(D / (D**2 + lambda_ridge)) @ U.T @ y
>    print(f"Lambda: {lambda_ridge}")
>    print("Beta Ridge (sklearn):\n", beta_hat_ridge_sklearn)
>    print("Beta Ridge (SVD):\n", beta_hat_ridge_svd)
> ```
> Observa-se que ambas as solu√ß√µes para $\beta_{ridge}$ s√£o muito pr√≥ximas (a menos de erros de arredondamento).  Quando $\lambda$ aumenta, os coeficientes sofrem mais shrinkage, como previsto pela teoria.
>

> ‚úîÔ∏è **Destaque**: A **ridge regression** n√£o exclui vari√°veis completamente, mas reduz seu impacto, especialmente em dire√ß√µes de alta colinearidade.

**Lemma 3:** *Na ridge regression, o par√¢metro $\lambda$ controla a intensidade do shrinkage, e o fator $d_j^2/(d_j^2+\lambda)$ √© o fator de redu√ß√£o (shrinkage factor) aplicado √†s coordenadas de y no espa√ßo das componentes principais. Quanto maior $\lambda$, maior o shrinkage* [^3.4.1].

**Prova do Lemma 3:** Seja $ \hat{\beta}_{ridge} = V(D^2 + \lambda I)^{-1}DU^Ty$. Podemos expressar isto como
$\hat{\beta}_{ridge} = \sum_{j=1}^{p} v_j \frac{d_j^2}{d_j^2+\lambda} u_j^T y$.  O termo $d_j^2/(d_j^2+\lambda)$ √© o shrinkage factor. Se $\lambda = 0$, n√£o h√° shrinkage. Quando $\lambda$ aumenta, este termo reduz-se, aplicando mais shrinkage √† dire√ß√£o correspondente $\blacksquare$.
> üí° **Exemplo Num√©rico:**
> Para visualizar o efeito do fator de shrinkage, vamos calcular e plotar o fator $d_j^2 / (d_j^2 + \lambda)$ para os dois valores singulares $d_1$ e $d_2$, variando o valor de $\lambda$.
>
> ```python
> import matplotlib.pyplot as plt
> lambdas = np.linspace(0, 10, 200)
> shrinkage_factor_d1 = D[0]**2 / (D[0]**2 + lambdas)
> shrinkage_factor_d2 = D[1]**2 / (D[1]**2 + lambdas)
> plt.figure(figsize=(10, 6))
> plt.plot(lambdas, shrinkage_factor_d1, label='Shrinkage Factor d1')
> plt.plot(lambdas, shrinkage_factor_d2, label='Shrinkage Factor d2')
> plt.xlabel('Lambda (Œª)')
> plt.ylabel('Shrinkage Factor')
> plt.title('Shrinkage Factors vs. Lambda')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> O gr√°fico mostra como o fator de shrinkage varia com $\lambda$. Para $d_1$ (maior valor singular), o fator √© sempre pr√≥ximo de 1, indicando que pouco shrinkage √© aplicado nesta dire√ß√£o. Para $d_2$ (menor valor singular), o fator diminui rapidamente com o aumento de $\lambda$, o que indica que essa dire√ß√£o √© mais encolhida.
>

**Corol√°rio 3:** *O shrinkage, como visto na ridge regression, tamb√©m tem rela√ß√£o com o efeito dos dados na solu√ß√£o.  As dire√ß√µes em $X$ com alta vari√¢ncia s√£o aquelas onde a varia√ß√£o dos dados √© maior, e essas s√£o menos afetadas pela regulariza√ß√£o. As dire√ß√µes com baixa vari√¢ncia s√£o aquelas onde a varia√ß√£o dos dados √© pequena e, por isso, s√£o mais afetadas pela regulariza√ß√£o, ou seja, mais encolhidas* [^3.4.1].

###  Separating Hyperplanes e Perceptrons

No contexto de modelos lineares para classifica√ß√£o, a SVD tamb√©m pode ser usada para entender a geometria de **separating hyperplanes**. No entanto, a SVD n√£o √© uma ferramenta prim√°ria para esse tipo de an√°lise, visto que m√©todos como o Perceptron e outras abordagens focam mais em otimiza√ß√£o direta da fun√ß√£o de classifica√ß√£o [^4.5.1]. Os **separating hyperplanes** podem ser vistos como um caso particular de modelos lineares em um espa√ßo expandido de caracter√≠sticas, e a SVD pode, em teoria, auxiliar a decompor e entender a estrutura desses espa√ßos, mas n√£o √© uma aplica√ß√£o comum [^4.5.2].

### Pergunta Te√≥rica Avan√ßada: Como a SVD est√° relacionada com a interpreta√ß√£o de m√©todos de redu√ß√£o de dimensionalidade como o principal components regression (PCR) e partial least squares (PLS)?

**Resposta:**
Tanto o PCR quanto o PLS utilizam a ideia de proje√ß√µes lineares para reduzir a dimensionalidade dos dados. A SVD nos d√° uma vis√£o clara do que est√° sendo otimizado em cada um desses m√©todos.

**Lemma 4:** *O PCR utiliza a mesma base de componentes principais derivada pela SVD para construir a matriz de entrada $Z$.  O PCR  seleciona as primeiras M componentes ($z_1, \dots, z_M$) como entradas para a regress√£o, descartando as outras componentes com valores singulares menores* [^3.5.1].
A proje√ß√£o pode ser expressa como:
$$\hat{y}_{PCR} = \sum_{j=1}^{M} \theta_j z_j$$
onde $\theta_j = (z_j, y)/(z_j, z_j)$, com $M < p$.  A SVD nos permite entender quais dire√ß√µes em $X$ s√£o mais informativas e devem ser mantidas, e quais devem ser descartadas [^3.5.1].
```mermaid
graph LR
  subgraph "Principal Components Regression (PCR)"
      direction LR
    A["SVD of X"] --> B["Principal Components z1, z2, ..., zp"]
    B --> C["Select M Principal Components (z1 ... zM)"]
    C --> D["Regression on Selected Components: y_hat_PCR = sum (theta_j * zj)"]
   end
```
> üí° **Exemplo Num√©rico:**
> Vamos aplicar o PCR usando as duas primeiras componentes principais (M=2) e comparar com o OLS.
> ```python
> from sklearn.decomposition import PCA
> # PCA
> pca = PCA(n_components=2)
> Z = pca.fit_transform(X)
> # PCR
> model_pcr = LinearRegression()
> model_pcr.fit(Z, y)
> y_hat_pcr = model_pcr.predict(Z)
> print("Predi√ß√µes PCR:\n", y_hat_pcr)
> ```
> O PCR reduz a dimensionalidade dos dados para 2, usando as componentes principais. O resultado √© uma predi√ß√£o diferente da obtida por OLS, com menos vari√¢ncia.

**Corol√°rio 4:** *O PLS, por outro lado, deriva componentes principais usando tanto X quanto Y.  Ele constr√≥i componentes $z_j$ de forma iterativa, que maximizam a vari√¢ncia e a correla√ß√£o com o vetor de respostas $y$. A SVD revela que cada dire√ß√£o gerada pelo PLS tem uma vari√¢ncia e uma correla√ß√£o diferentes com $y$, e ao fazer o PLS usa componentes que s√£o importantes para explicar tanto a vari√¢ncia em $X$, quanto a varia√ß√£o em $Y$* [^3.5.2].
```mermaid
graph LR
   subgraph "Partial Least Squares (PLS)"
      direction LR
      A["X and y"] --> B["Iterative construction of components zj"]
      B --> C["Maximize variance and correlation with y"]
      C --> D["Use zj that explain both X and y variance"]
   end
```

> ‚ö†Ô∏è **Ponto Crucial**: Ambos os m√©todos utilizam proje√ß√µes lineares, mas o PCR depende unicamente da estrutura de $X$, enquanto PLS leva em considera√ß√£o $X$ e $y$. A SVD √© uma ferramenta √∫til para entender e comparar esses m√©todos.

### Conclus√£o

Este cap√≠tulo detalhou a SVD e sua rela√ß√£o com conceitos de modelos lineares, com um foco espec√≠fico em m√©todos de **shrinkage** para regress√£o linear. Vimos que a SVD nos permite n√£o apenas decompor uma matriz de dados, mas tamb√©m interpretar seus componentes principais e o papel dos valores singulares na modelagem linear e nas t√©cnicas de regulariza√ß√£o. As t√©cnicas como **ridge regression**, **principal component regression** e **partial least squares** s√£o todas constru√≠das sobre a ideia de projetar os dados em bases que simplificam o modelo e melhoram a capacidade de generaliza√ß√£o, a partir do entendimento do espa√ßo de dados baseado na decomposi√ß√£o da SVD.

### Footnotes

[^3.4.1]: "Here U and V are Nxpand p√ó p orthogonal matrices, with the columns of U spanning the column space of X, and the columns of V spanning the row space. D is a p√óp diagonal matrix, with diagonal entries d‚ÇÅ ‚â• d‚ÇÇ ‚â• ... ‚â• dp ‚â• 0 called the singular values of X. If one or more values dj = 0, X is singular... Using the singular value decomposition we can write the least squares fitted vector as Xls = X(X^TX)‚àí¬πXTy = UU^Ty" *(Trecho de <Linear Methods for Regression>)*
[^3.4]: "Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized residual sum of squares" *(Trecho de <Linear Methods for Regression>)*
[^3.5.1]: "Principal component regression forms the derived input columns zm = Xum, and then regresses y on Z‚ÇÅ, Z‚ÇÇ,..., ZM for some M < p. Since the Zm are orthogonal, this regression is just a sum of univariate regressions" *(Trecho de <Linear Methods for Regression>)*
[^3.5.2]: "This technique also constructs a set of linear combinations of the inputs for regression, but unlike principal components regression it uses y (in addition to X) for this construction... In this manner, partial least squares produces a sequence of derived, orthogonal inputs or directions z‚ÇÅ, z‚ÇÇ,..., zM" *(Trecho de <Linear Methods for Regression>)*
[^4.5.1]: "Descreva em texto corrido como a ideia de maximizar a margem de separa√ß√£o leva ao conceito de hiperplanos √≥timos, referenciando [8](4.5.2) para a formula√ß√£o do problema de otimiza√ß√£o e o uso do dual de Wolfe. Explique como as solu√ß√µes surgem a partir de combina√ß√µes lineares dos pontos de suporte. Se desejar, inclua detalhes do Perceptron de Rosenblatt e sua converg√™ncia sob condi√ß√µes espec√≠ficas, conforme em [7](4.5.1)." *(Trecho de <Instru√ß√µes do Usu√°rio>)*
[^4.5.2]: "Apresente teoremas, lemmas ou corol√°rios se necess√°rio para aprofundar a an√°lise te√≥rica, especialmente sobre a condi√ß√£o de separabilidade de dados e a garantia de converg√™ncia sob hip√≥teses de linear separability, utilizando [7](4.5.1) e [8](4.5.2) para fundamentar cada afirma√ß√£o." *(Trecho de <Instru√ß√µes do Usu√°rio>)*
