## MÃ©todos Lineares para RegressÃ£o e ClassificaÃ§Ã£o

```mermaid
graph LR
    subgraph "Linear Methods Overview"
        A["Linear Regression"] --> B("Regression Tasks")
        A --> C("Classification Tasks")
        C --> D["Logistic Regression"]
        C --> E["LDA"]
    end
```

### IntroduÃ§Ã£o

Este capÃ­tulo explora mÃ©todos lineares aplicados Ã  regressÃ£o e classificaÃ§Ã£o, fundamentais na modelagem estatÃ­stica e aprendizado de mÃ¡quina. MÃ©todos lineares, embora desenvolvidos na era prÃ©-computacional, continuam relevantes devido Ã  sua simplicidade, interpretabilidade e, em algumas situaÃ§Ãµes, capacidade de superar modelos nÃ£o lineares, especialmente quando lidamos com poucos dados, baixo sinal-ruÃ­do ou dados esparsos [^3.1]. AlÃ©m disso, mÃ©todos lineares servem como base para entender abordagens nÃ£o lineares mais avanÃ§adas, muitas vezes sendo extensÃµes diretas dos mÃ©todos lineares [^3.1]. A exploraÃ§Ã£o detalhada desses mÃ©todos Ã© crucial para a compreensÃ£o de modelos complexos e para o desenvolvimento de soluÃ§Ãµes robustas e eficazes em diversas aplicaÃ§Ãµes de anÃ¡lise de dados.

### Conceitos Fundamentais

**Conceito 1:** O **problema de classificaÃ§Ã£o** consiste em atribuir uma instÃ¢ncia de dados a uma de duas ou mais categorias predefinidas. Modelos lineares, como regressÃ£o linear e suas variaÃ§Ãµes, tentam encontrar uma funÃ§Ã£o linear que separe essas categorias [^4.1]. No entanto, o uso de mÃ©todos lineares pode introduzir um viÃ©s inerente Ã  suposiÃ§Ã£o de separabilidade linear, e pode nÃ£o ser apropriado quando os dados sÃ£o inerentemente nÃ£o lineares. A simplicidade desses modelos tambÃ©m pode levar a problemas de alta variÃ¢ncia quando o nÃºmero de parÃ¢metros Ã© comparÃ¡vel ou maior que o nÃºmero de amostras de treino, levando a um desempenho ruim fora da amostra de treino [^3.1]. O viÃ©s, por sua vez, descreve o erro sistemÃ¡tico, ou seja, a tendÃªncia do modelo em prever os resultados de forma consistentemente errada. A variÃ¢ncia, por outro lado, descreve a sensibilidade do modelo a pequenas variaÃ§Ãµes nos dados de treinamento, podendo levar a um desempenho inconsistente em diferentes amostras.

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine um cenÃ¡rio de classificaÃ§Ã£o binÃ¡ria onde temos 100 amostras com 2 features cada. Se usarmos um modelo linear com 3 parÃ¢metros (incluindo o bias) e os dados forem realmente nÃ£o lineares, o modelo terÃ¡ alto viÃ©s, pois nÃ£o conseguirÃ¡ capturar a complexidade dos dados. Por outro lado, se usarmos um modelo linear com muitas features geradas atravÃ©s de expansÃ£o polinomial (e.g., $x_1^2, x_2^2, x_1x_2$), ele pode ter baixa variÃ¢ncia no treinamento, mas alta variÃ¢ncia fora da amostra, pois se ajustarÃ¡ muito aos dados de treino e nÃ£o generalizarÃ¡ bem para novos dados.
> 
> Para ilustrar, vamos criar dados sintÃ©ticos:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
> from sklearn.preprocessing import PolynomialFeatures
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import mean_squared_error
>
> np.random.seed(42)
>
> # Dados nÃ£o lineares
> n_samples = 100
> X = np.sort(5 * np.random.rand(n_samples, 1), axis=0)
> y = np.sin(X).ravel() + np.random.randn(n_samples) * 0.2
>
> # DivisÃ£o em treino e teste
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # Modelo linear simples
> model_linear = LinearRegression()
> model_linear.fit(X_train, y_train)
> y_pred_linear_train = model_linear.predict(X_train)
> y_pred_linear_test = model_linear.predict(X_test)
> mse_linear_train = mean_squared_error(y_train, y_pred_linear_train)
> mse_linear_test = mean_squared_error(y_test, y_pred_linear_test)
>
> # Modelo com expansÃ£o polinomial (exemplo com grau 5)
> poly = PolynomialFeatures(degree=5)
> X_train_poly = poly.fit_transform(X_train)
> X_test_poly = poly.transform(X_test)
> model_poly = LinearRegression()
> model_poly.fit(X_train_poly, y_train)
> y_pred_poly_train = model_poly.predict(X_train_poly)
> y_pred_poly_test = model_poly.predict(X_test_poly)
> mse_poly_train = mean_squared_error(y_train, y_pred_poly_train)
> mse_poly_test = mean_squared_error(y_test, y_pred_poly_test)
>
> # VisualizaÃ§Ã£o dos resultados
> plt.figure(figsize=(10, 6))
> plt.scatter(X, y, label='Dados Reais', color='black')
> plt.plot(X_train, y_pred_linear_train, label=f'Linear (MSE Treino: {mse_linear_train:.2f})', color='red')
> plt.plot(X_test, y_pred_linear_test, label=f'Linear (MSE Teste: {mse_linear_test:.2f})', linestyle='--', color='red')
> plt.plot(X_train, y_pred_poly_train, label=f'Polinomial (MSE Treino: {mse_poly_train:.2f})', color='blue')
> plt.plot(X_test, y_pred_poly_test, label=f'Polinomial (MSE Teste: {mse_poly_test:.2f})', linestyle='--', color='blue')
> plt.legend()
> plt.title('ComparaÃ§Ã£o de Modelos Lineares com e sem ExpansÃ£o Polinomial')
> plt.xlabel('Feature X')
> plt.ylabel('VariÃ¡vel Y')
> plt.show()
> ```
> O cÃ³digo acima gera dados com um padrÃ£o senoidal. O modelo linear simples (em vermelho) mostra alto viÃ©s, pois nÃ£o consegue se ajustar bem aos dados. JÃ¡ o modelo polinomial de grau 5 (em azul) se ajusta muito bem aos dados de treino, mas pode nÃ£o generalizar tÃ£o bem para os dados de teste, o que indica um problema de alta variÃ¢ncia. O MSE (Mean Squared Error) dos modelos Ã© mostrado na legenda. Note que o MSE do modelo polinomial no conjunto de treino Ã© menor, mas no conjunto de teste Ã© maior, o que exemplifica o tradeoff viÃ©s-variÃ¢ncia.
> 
> ```mermaid
> flowchart LR
>   A["Nonlinear Data"] --> B("Simple Linear Model")
>   B --> C{"High Bias, Low Variance"}
>   A --> D("Linear Model with Polynomial Expansion")
>   D --> E{"Low Bias on Train, High Variance on Test"}
>   C --> F["Poor Results"]
>   E --> G["Poor Results on New Data"]
> ```

**Lemma 1:** Dada uma matriz de dados $X \in \mathbb{R}^{N \times p}$ e um vetor de resposta $y \in \mathbb{R}^{N}$, uma funÃ§Ã£o discriminante linear $f(x) = x^T\beta$ pode ser vista como o resultado da regressÃ£o de uma matriz indicadora de classes $Y$, onde cada linha de $Y$ codifica a classe a que a observaÃ§Ã£o correspondente em $X$ pertence. Mais formalmente, se temos $K$ classes, $Y \in \{0,1\}^{N \times K}$ onde $Y_{ik} = 1$ se a observaÃ§Ã£o $i$ pertence Ã  classe $k$ e $Y_{ik} = 0$ caso contrÃ¡rio, a funÃ§Ã£o discriminante obtida pela regressÃ£o linear de $Y$ sobre $X$ Ã© equivalente a $f(x) = x^T\beta$, onde $\beta$ Ã© o vetor de coeficientes estimado. Este resultado estabelece a relaÃ§Ã£o entre regressÃ£o de indicadores e decisÃµes de classe. A prova consiste em mostrar que a soluÃ§Ã£o de mÃ­nimos quadrados para $Y = X\beta$ leva a uma projeÃ§Ã£o da resposta sobre o espaÃ§o gerado por $X$, e esta projeÃ§Ã£o Ã© usada para realizar a decisÃ£o de classe [^4.2].

```mermaid
graph LR
    subgraph "Indicator Regression Lemma"
        A["Data Matrix:  $X \in \mathbb{R}^{N \times p}$"] --> B["Response Vector:  $y \in \mathbb{R}^N$"]
        B --> C["Indicator Matrix:  $Y \in \{0,1\}^{N \times K}$"]
        C --> D["Linear Discriminant Function:  $f(x) = x^T\beta$"]
        D --> E["Regression of  $Y$  on  $X$:  $Y = X\beta$"]
        E --> F["Decision Based on Projection of $y$ onto Span($X$)"]
    end
```

**Conceito 2:** A **Linear Discriminant Analysis (LDA)** Ã© um mÃ©todo de classificaÃ§Ã£o que assume que as classes tÃªm distribuiÃ§Ãµes Gaussianas com a mesma matriz de covariÃ¢ncia [^4.3]. A LDA constrÃ³i uma fronteira de decisÃ£o linear com base nas mÃ©dias e covariÃ¢ncias das classes, maximizando a separaÃ§Ã£o entre elas. Dada uma observaÃ§Ã£o $x$, a funÃ§Ã£o discriminante para a classe $k$ Ã©:
$$ \delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log \pi_k $$,
onde $\mu_k$ Ã© a mÃ©dia da classe $k$, $\Sigma$ Ã© a matriz de covariÃ¢ncia comum a todas as classes e $\pi_k$ Ã© a probabilidade *a priori* da classe $k$ [^4.3.1]. A suposiÃ§Ã£o de normalidade e covariÃ¢ncias iguais simplifica o problema, tornando a soluÃ§Ã£o analÃ­tica. A fronteira de decisÃ£o entre duas classes $k$ e $l$ Ã© dada por $\delta_k(x) = \delta_l(x)$, definindo um hiperplano linear no espaÃ§o de *features* [^4.3.2].

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um problema de classificaÃ§Ã£o com duas classes e duas features. As mÃ©dias das classes sÃ£o $\mu_1 = [1, 1]^T$ e $\mu_2 = [3, 3]^T$. A matriz de covariÃ¢ncia comum Ã© $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$. As probabilidades *a priori* sÃ£o $\pi_1 = 0.4$ e $\pi_2 = 0.6$.
>
> Para um novo ponto $x = [2, 2]^T$, calculamos as funÃ§Ãµes discriminantes:
>
> $\text{Step 1: } \Sigma^{-1} = \frac{1}{1 - 0.5^2} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \frac{4}{3} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix}$
>
> $\text{Step 2: } \delta_1(x) = \begin{bmatrix} 2 & 2 \end{bmatrix} \begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} + \log(0.4)$
>
> $\text{Step 3: } \delta_1(x) = \begin{bmatrix} 2 & 2 \end{bmatrix} \begin{bmatrix} 2/3 \\ 2/3 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 2/3 \\ 2/3 \end{bmatrix} + \log(0.4) = \frac{8}{3} - \frac{2}{3} + \log(0.4) \approx 2 + (-0.916) \approx 1.084$
>
> $\text{Step 4: } \delta_2(x) = \begin{bmatrix} 2 & 2 \end{bmatrix} \begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 3 & 3 \end{bmatrix} \begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix} + \log(0.6)$
>
> $\text{Step 5: } \delta_2(x) = \begin{bmatrix} 2 & 2 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 3 & 3 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \end{bmatrix} + \log(0.6) = 8 - 6 + \log(0.6) \approx 2 + (-0.511) \approx 1.489$
>
> Como $\delta_2(x) > \delta_1(x)$, o ponto $x$ Ã© classificado como pertencente Ã  classe 2.
>
> ```python
> import numpy as np
> from scipy.stats import multivariate_normal
>
> # ParÃ¢metros das classes
> mu1 = np.array([1, 1])
> mu2 = np.array([3, 3])
> sigma = np.array([[1, 0.5], [0.5, 1]])
> pi1 = 0.4
> pi2 = 0.6
>
> # Ponto para classificaÃ§Ã£o
> x = np.array([2, 2])
>
> # Calcula a inversa da matriz de covariÃ¢ncia
> sigma_inv = np.linalg.inv(sigma)
>
> # Calcula as funÃ§Ãµes discriminantes
> delta1 = x.T @ sigma_inv @ mu1 - 0.5 * mu1.T @ sigma_inv @ mu1 + np.log(pi1)
> delta2 = x.T @ sigma_inv @ mu2 - 0.5 * mu2.T @ sigma_inv @ mu2 + np.log(pi2)
>
> print(f'Delta1: {delta1:.3f}')
> print(f'Delta2: {delta2:.3f}')
>
> # Classifica com base nas funÃ§Ãµes discriminantes
> if delta1 > delta2:
>    print('Ponto classificado como classe 1')
> else:
>    print('Ponto classificado como classe 2')
> ```
> Este exemplo ilustra como a LDA usa as mÃ©dias, covariÃ¢ncias e probabilidades a priori para classificar um novo ponto.

```mermaid
graph LR
    subgraph "LDA Discriminant Function"
        A["Input: x"] --> B["Mean Vector of Class k: $\mu_k$"]
        A --> C["Common Covariance Matrix: $\Sigma$"]
        A --> D["Prior Probability of Class k: $\pi_k$"]
        B & C --> E["$\delta_k(x) = x^T \Sigma^{-1} \mu_k - 0.5 \mu_k^T \Sigma^{-1} \mu_k + \log(\pi_k)$"]
        E --> F["Classification based on max($\delta_k(x)$)"]
    end
```

**CorolÃ¡rio 1:** A funÃ§Ã£o discriminante linear da LDA, $\delta_k(x)$, pode ser vista como a projeÃ§Ã£o da observaÃ§Ã£o $x$ no espaÃ§o definido pelos parÃ¢metros do modelo, que sÃ£o as mÃ©dias das classes e a matriz de covariÃ¢ncia.  Esta projeÃ§Ã£o maximiza a separaÃ§Ã£o entre classes ao projetar os dados em um subespaÃ§o de dimensÃ£o menor, definido pela direÃ§Ã£o de mÃ¡xima separaÃ§Ã£o entre classes [^4.3.3]. Isto Ã©, podemos projetar $x$ em um espaÃ§o onde a variÃ¢ncia entre as classes Ã© maximizada em relaÃ§Ã£o Ã  variÃ¢ncia dentro da classe, simplificando o problema de classificaÃ§Ã£o [^4.3.1].

```mermaid
graph LR
    subgraph "LDA Projection"
        A["Observation: x"] --> B["Class Means: $\mu_k$"]
        A --> C["Covariance Matrix: $\Sigma$"]
        B & C --> D["Projection into subspace spanned by $\mu_k$ and $\Sigma$"]
        D --> E["Maximizes separation between classes"]
        E --> F["Simplifies Classification"]
    end
```

**Conceito 3:** A **Logistic Regression** Ã© um mÃ©todo estatÃ­stico de classificaÃ§Ã£o que modela a probabilidade de uma observaÃ§Ã£o pertencer a uma classe especÃ­fica usando a funÃ§Ã£o logÃ­stica [^4.4]. Ao contrÃ¡rio da LDA, que faz suposiÃ§Ãµes sobre a distribuiÃ§Ã£o dos dados, a regressÃ£o logÃ­stica modela diretamente a probabilidade *a posteriori* da classe, usando uma transformaÃ§Ã£o logit. O modelo Ã© dado por:
$$ p(Y=1|X) = \frac{1}{1 + e^{-(X\beta)}} $$,
onde $X\beta$ Ã© uma funÃ§Ã£o linear dos *features* de entrada, e a funÃ§Ã£o logÃ­stica garante que a probabilidade esteja entre 0 e 1 [^4.4.1]. Os parÃ¢metros $\beta$ sÃ£o estimados atravÃ©s da maximizaÃ§Ã£o da verossimilhanÃ§a, ou seja, encontrando os valores de $\beta$ que maximizam a probabilidade de observar os dados de treinamento [^4.4.2]. A regressÃ£o logÃ­stica tambÃ©m assume independÃªncia condicional entre as observaÃ§Ãµes dado os *features*, embora nÃ£o faÃ§a suposiÃ§Ãµes sobre a distribuiÃ§Ã£o das variÃ¡veis independentes [^4.4.3].  A regressÃ£o logÃ­stica pode ser usada para problemas de classificaÃ§Ã£o binÃ¡ria ou multiclasse, e o modelo Ã© particularmente Ãºtil quando nÃ£o se pode garantir a separabilidade linear perfeita entre as classes ou quando as suposiÃ§Ãµes de normalidade da LDA nÃ£o sÃ£o vÃ¡lidas [^4.4.4].

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos considerar um exemplo simplificado de regressÃ£o logÃ­stica com uma Ãºnica feature. Suponha que temos os seguintes dados de treinamento:
>
> | Feature (x) | Classe (y) |
> |-------------|------------|
> | 1           | 0          |
> | 2           | 0          |
> | 3           | 1          |
> | 4           | 1          |
> | 5           | 1          |
>
> Assumimos um modelo logÃ­stico com um Ãºnico coeficiente $\beta_1$ e um intercepto $\beta_0$, tal que $p(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}$. O objetivo Ã© encontrar os valores de $\beta_0$ e $\beta_1$ que melhor ajustam os dados.
>
> Usando um otimizador para maximizar a verossimilhanÃ§a, obtemos os valores $\beta_0 \approx -4.0$ e $\beta_1 \approx 1.5$.  
>
> Com esses valores, podemos calcular a probabilidade de um novo ponto pertencer Ã  classe 1. Por exemplo, para $x = 3.5$:
> $p(y=1|x=3.5) = \frac{1}{1 + e^{-(-4.0 + 1.5 \cdot 3.5)}} = \frac{1}{1 + e^{-1.25}} \approx 0.777$.
>
> O ponto Ã© classificado na classe 1, pois a probabilidade Ã© superior a 0.5.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LogisticRegression
>
> # Dados de treinamento
> X = np.array([[1], [2], [3], [4], [5]])
> y = np.array([0, 0, 1, 1, 1])
>
> # Treinamento do modelo
> model = LogisticRegression()
> model.fit(X, y)
>
> # ParÃ¢metros estimados
> beta0 = model.intercept_[0]
> beta1 = model.coef_[0][0]
>
> print(f'Beta0: {beta0:.3f}')
> print(f'Beta1: {beta1:.3f}')
>
> # Novo ponto
> x_new = np.array([[3.5]])
>
> # Probabilidade da classe 1
> prob_class1 = model.predict_proba(x_new)[0][1]
>
> print(f'Probabilidade da classe 1 para x=3.5: {prob_class1:.3f}')
>
> # ClassificaÃ§Ã£o
> class_pred = model.predict(x_new)[0]
> print(f'ClassificaÃ§Ã£o para x=3.5: {class_pred}')
>
> # VisualizaÃ§Ã£o da funÃ§Ã£o logÃ­stica
> x_plot = np.linspace(0, 6, 100).reshape(-1, 1)
> y_plot = model.predict_proba(x_plot)[:, 1]
>
> plt.scatter(X, y, color='black', label='Dados Reais')
>plt.plot(x_plot, y_plot, label='FunÃ§Ã£o LogÃ­stica', color='red')
>plt.xlabel('Feature X')
>plt.ylabel('Probabilidade da Classe 1')
>plt.legend()
>plt.title('RegressÃ£o LogÃ­stica')
>plt.show()
> ```
> Este exemplo mostra como a regressÃ£o logÃ­stica estima probabilidades e classifica os dados usando a funÃ§Ã£o logÃ­stica.

```mermaid
graph LR
    subgraph "Logistic Regression Model"
      A["Input Features: X"] --> B["Linear Combination: $X\beta$"]
      B --> C["Logistic Function: $1 / (1 + e^{-(X\beta)})$"]
      C --> D["Probability of Y=1 given X: $p(Y=1|X)$"]
      D --> E["Classification based on $p(Y=1|X) > 0.5$"]
    end
```
> âš ï¸ **Nota Importante**: A funÃ§Ã£o logit transforma a probabilidade $p$ em log-odds: $logit(p) = \log(\frac{p}{1-p})$, linearizando a relaÃ§Ã£o entre a probabilidade da classe e os *features* de entrada. Isto Ã©, regressÃ£o logÃ­stica usa uma transformaÃ§Ã£o nÃ£o linear na probabilidade, mas gera uma funÃ§Ã£o discriminante linear no espaÃ§o de parÃ¢metros, o que facilita a otimizaÃ§Ã£o dos parÃ¢metros [^4.4.1].

> â— **Ponto de AtenÃ§Ã£o**: Em casos de classes nÃ£o balanceadas, a regressÃ£o logÃ­stica pode ser sensÃ­vel Ã  classe majoritÃ¡ria, podendo prejudicar o desempenho na classe minoritÃ¡ria. Ã‰ fundamental balancear as classes ou usar mÃ©tricas de avaliaÃ§Ã£o apropriadas, como precision e recall, para avaliar o desempenho do modelo [^4.4.2].

> âœ”ï¸ **Destaque**: Tanto a LDA quanto a regressÃ£o logÃ­stica resultam em modelos de classificaÃ§Ã£o que produzem fronteiras de decisÃ£o lineares no espaÃ§o dos *features*.  No entanto, a LDA baseia-se na anÃ¡lise discriminante de classe com base na normalidade e covariÃ¢ncias iguais, enquanto a regressÃ£o logÃ­stica modela diretamente a probabilidade da classe, tornando-a uma escolha mais robusta em muitos cenÃ¡rios onde tais suposiÃ§Ãµes nÃ£o sÃ£o vÃ¡lidas [^4.5].

### RegressÃ£o Linear e MÃ­nimos Quadrados para ClassificaÃ§Ã£o

```mermaid
graph LR
    subgraph "Linear Regression for Classification"
        A["Encode Classes as Indicator Variables"] --> B["Fit Linear Model using Least Squares"]
        B --> C["Estimate Probabilities"]
        C --> D["Decision based on Threshold (e.g., 0.5)"]
        D --> E["Potential for Probability Extrapolation Issues"]
    end
```

A regressÃ£o linear pode ser aplicada a problemas de classificaÃ§Ã£o atravÃ©s da codificaÃ§Ã£o das classes como variÃ¡veis indicadoras. Por exemplo, em um problema de classificaÃ§Ã£o binÃ¡ria, uma classe pode ser codificada como 0 e a outra como 1. A regressÃ£o linear entÃ£o tenta ajustar uma reta ou hiperplano aos dados codificados [^4.2].  A saÃ­da da regressÃ£o linear Ã© interpretada como uma estimativa de probabilidade da classe, embora esses valores possam nÃ£o estar necessariamente entre 0 e 1 [^4.1]. Uma regra de decisÃ£o simples, como atribuir a uma classe se a previsÃ£o Ã© maior que 0.5, pode ser usada para classificar novas instÃ¢ncias. No entanto, essa abordagem Ã© sensÃ­vel a *outliers* e nÃ£o Ã© robusta para o problema de classificaÃ§Ã£o [^4.2].

A regressÃ£o linear aplicada diretamente a matrizes de indicadores nÃ£o leva em conta a covariÃ¢ncia entre as classes, como a LDA faz. Em situaÃ§Ãµes em que a covariÃ¢ncia entre as classes Ã© relevante para a decisÃ£o, a regressÃ£o linear pode ser menos eficaz do que a LDA, especialmente quando se deseja a projeÃ§Ã£o em hiperplanos que melhor separam as classes [^4.3].

> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar, considere o mesmo problema de classificaÃ§Ã£o binÃ¡ria, onde a classe 0 Ã© codificada como 0 e a classe 1 como 1. Usando os dados do exemplo anterior da regressÃ£o logÃ­stica:
>
> | Feature (x) | Classe (y) |
> |-------------|------------|
> | 1           | 0          |
> | 2           | 0          |
> | 3           | 1          |
> | 4           | 1          |
> | 5           | 1          |
>
> Usamos a regressÃ£o linear para ajustar um modelo do tipo $y = \beta_0 + \beta_1x$. Resolvendo o problema de mÃ­nimos quadrados:
>
> $\text{Step 1: } X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix}, y = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 1 \end{bmatrix}$
>
> $\text{Step 2: } X^TX = \begin{bmatrix} 5 & 15 \\ 15 & 55 \end{bmatrix}$
>
> $\text{Step 3: } (X^TX)^{-1} = \frac{1}{50} \begin{bmatrix} 55 & -15 \\ -15 & 5 \end{bmatrix} = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix}$
>
> $\text{Step 4: } X^Ty = \begin{bmatrix} 3 \\ 13 \end{bmatrix}$
>
> $\text{Step 5: } \beta = (X^TX)^{-1}X^Ty = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix} \begin{bmatrix} 3 \\ 13 \end{bmatrix} = \begin{bmatrix} -0.6 \\ 0.4 \end{bmatrix}$
>
> Assim, $\beta_0 \approx -0.6$ e $\beta_1 \approx 0.4$. O modelo linear Ã© $y = -0.6 + 0.4x$.
>
> Para classificar um novo ponto $x = 3.5$, temos $y = -0.6 + 0.4 \times 3.5 = 0.8$. Se usarmos um limiar de 0.5, o ponto Ã© classificado na classe 1. No entanto, para $x = 1$, teremos $y=-0.2$, e um limiar de 0.5 classifica incorretamente, pois o valor nÃ£o esta entre 0 e 1, o que Ã© problemÃ¡tico.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Dados de treinamento
> X = np.array([[1], [2], [3], [4], [5]])
> y = np.array([0, 0, 1, 1, 1])
>
> # Treinamento do modelo
> model = LinearRegression()
> model.fit(X, y)
>
> # ParÃ¢metros estimados
> beta0 = model.intercept_
> beta1 = model.coef_[0]
>
> print(f'Beta0: {beta0:.3f}')
> print(f'Beta1: {beta1:.3f}')
>
> # Novo ponto
> x_new = np.array([[3.5]])
>
> # PrevisÃ£o
> y_pred = model.predict(x_new)[0]
>
> print(f'PrevisÃ£o para x=3.5: {y_pred:.3f}')
>
> # ClassificaÃ§Ã£o com um limiar de 0.5
> class_pred = 1 if y_pred > 0.5 else 0
>
> print(f'ClassificaÃ§Ã£o para x=3.5: {class_pred}')
>
> x_new_low = np.array([[1]])
> y_pred_low = model.predict(x_new_low)[0]
> class_pred_low = 1 if y_pred_low > 0.5 else 0
> print(f'PrevisÃ£o para x=1: {y_pred_low:.3f}, classificaÃ§Ã£o: {class_pred_low}')
>
> # VisualizaÃ§Ã£o do modelo
> import matplotlib.pyplot as plt
> x_plot = np.linspace(0, 6, 100).reshape(-1, 1)
> y_plot = model.predict(x_plot)
> plt.scatter(X, y, color='black', label='Dados Reais')
> plt.plot(x_plot, y_plot, label='Modelo Linear', color='red')
> plt.axhline(0.5, color='gray', linestyle='--', label='Limiar 0.5')
> plt.xlabel('Feature X')
> plt.ylabel('PrevisÃ£o')
> plt.legend()
> plt.title('RegressÃ£o Linear para ClassificaÃ§Ã£o')
> plt.show()
> ```
> Este exemplo mostra como a regressÃ£o linear pode ser usada para classificaÃ§Ã£o, embora as estimativas de probabilidade nÃ£o sejam restritas ao intervalo [0, 1], o que pode ser um problema para a interpretaÃ§Ã£o e a tomada de decisÃ£o.

**Lemma 2:** Sob certas condiÃ§Ãµes, a projeÃ§Ã£o nos hiperplanos de decisÃ£o gerados pela regressÃ£o linear de indicadores Ã© equivalente aos discriminantes lineares da LDA, especialmente quando a variÃ¢ncia das classes Ã© aproximadamente igual.  A prova envolve mostrar que a minimizaÃ§Ã£o do erro quadrÃ¡tico em uma matriz indicadora leva a soluÃ§Ãµes que se aproximam das soluÃ§Ãµes da LDA quando as covariÃ¢ncias sÃ£o homogÃªneas, mas essa equivalÃªncia pode nÃ£o ser verdadeira em cenÃ¡rios com heterogeneidade das covariÃ¢ncias [^4.2], [^4.3].

```mermaid
graph LR
    subgraph "Equivalence Lemma"
        A["Linear Regression on Indicators"] --> B["Projection onto Decision Hyperplanes"]
        B --> C["LDA Linear Discriminants"]
        C --> D["Equivalence under Homogeneous Class Variances"]
        D --> E["Breakdown with Heterogeneous Covariances"]
    end
```

**CorolÃ¡rio 2:** A equivalÃªncia (sob certas condiÃ§Ãµes) entre as projeÃ§Ãµes da regressÃ£o linear e LDA permite simplificar a anÃ¡lise do modelo, fornecendo um entendimento da regressÃ£o linear em termos da anÃ¡lise discriminante, sendo possÃ­vel fazer anÃ¡lises comparativas dos resultados com maior facilidade em contextos especÃ­ficos [^4.3].

Em alguns casos, a regressÃ£o logÃ­stica fornece estimativas de probabilidade mais estÃ¡veis e bem-comportadas do que a regressÃ£o linear, especialmente quando hÃ¡ extrapolaÃ§Ãµes fora do intervalo [0, 1].  A regressÃ£o linear em matrizes de indicadores Ã© propensa a extrapolaÃ§Ãµes nÃ£o plausÃ­veis, levando a estimativas de probabilidade fora do intervalo de 0 a 1, o que nÃ£o ocorre na regressÃ£o logÃ­stica, que garante a saÃ­da neste intervalo [^4.4]. No entanto, quando o objetivo principal Ã© obter uma fronteira de decisÃ£o linear, a regressÃ£o linear com matrizes de indicadores pode ser suficiente e, em alguns casos, mais simples de implementar e computar [^4.2].

### MÃ©todos de SeleÃ§Ã£o de VariÃ¡veis e RegularizaÃ§Ã£o em ClassificaÃ§Ã£o

```mermaid
graph LR
    subgraph "Regularization in Classification"
        A["Logistic Regression"] --> B["L1 Regularization (Lasso)"]
        A --> C["L2 Regularization (Ridge)"]
        B --> D["Sparse Models, Feature Selection"]
        C --> E["Stable Models, Reduced Coefficient Variance"]
        D & E --> F["Improved Generalization and Interpretability"]
    end
```

A seleÃ§Ã£o de variÃ¡veis e a regularizaÃ§Ã£o sÃ£o cruciais para modelos de classificaÃ§Ã£o, especialmente quando o nÃºmero de preditores Ã© grande ou quando hÃ¡ multicolinearidade [^4.5]. MÃ©todos como a regressÃ£o logÃ­stica podem ser aprimorados com a adiÃ§Ã£o de termos de penalizaÃ§Ã£o para evitar *overfitting* e promover modelos mais parcimoniosos e interpretÃ¡veis [^4.4.4].

A penalizaÃ§Ã£o L1 (Lasso) em regressÃ£o logÃ­stica adiciona um termo proporcional Ã  soma dos valores absolutos dos coeficientes na funÃ§Ã£o de custo, levando a soluÃ§Ãµes esparsas, ou seja, com muitos coeficientes iguais a zero, e promovendo a seleÃ§Ã£o de variÃ¡veis [^4.4.4], [^4.5]. A penalizaÃ§Ã£o L2 (Ridge) adiciona um termo proporcional Ã  soma dos quadrados dos coeficientes, o que reduz a magnitude dos coeficientes, mas raramente os iguala a zero, promovendo modelos mais estÃ¡veis e robustos [^4.5.1]. A penalizaÃ§Ã£o L1 Ã© particularmente Ãºtil para a seleÃ§Ã£o de variÃ¡veis, enquanto a L2 reduz a variÃ¢ncia dos coeficientes, sendo ambas vantajosas para modelos de classificaÃ§Ã£o.

> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar a regularizaÃ§Ã£o L1 (Lasso) e L2 (Ridge), vamos usar dados simulados com 10 features, onde apenas algumas sÃ£o realmente importantes para a classificaÃ§Ã£o:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LogisticRegression
> from sklearn.model_selection import train_test_split
> from sklearn.preprocessing import StandardScaler
>
> np.random.seed(42)
>
> # Dados simulados com 10 features
> n_samples = 200
> n_features = 10
> X = np.random.randn(n_samples, n_features)
> # Features 2 e 7 sÃ£o mais importantes
> y = (0.5 * X[:, 1] + 0.8 * X[:, 6]