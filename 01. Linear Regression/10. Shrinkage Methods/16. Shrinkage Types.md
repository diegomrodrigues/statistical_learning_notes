## Shrinkage Methods: A Deep Dive into Proportional, Soft, and Hard Approaches

```mermaid
flowchart LR
    subgraph "Regularization Methods"
        A["Model Complexity Problem"] --> B("Shrinkage Techniques")
        B --> C{"Proportional Shrinkage"}
        B --> D{"Soft Shrinkage"}
        B --> E{"Hard Shrinkage"}
        C --> F["Ridge Regression"]
        D --> G["Lasso Regression"]
        E --> H["Subset Selection"]
    end
```

### Introdu√ß√£o
No campo da **modelagem estat√≠stica**, particularmente em problemas de **regress√£o e classifica√ß√£o**, a complexidade e o risco de *overfitting* s√£o desafios constantes [^4.1]. A capacidade de um modelo se ajustar excessivamente aos dados de treino, em detrimento de sua performance em dados n√£o vistos, leva a generaliza√ß√µes pobres e resultados n√£o confi√°veis. Para mitigar esse problema, foram desenvolvidas as chamadas t√©cnicas de **regulariza√ß√£o** ou *shrinkage*, que visam reduzir a magnitude dos coeficientes do modelo, favorecendo solu√ß√µes mais simples e robustas. Este cap√≠tulo explora em profundidade diferentes abordagens de *shrinkage*, com foco nas t√©cnicas proporcionais, *soft* e *hard* [^4.1].

### Conceitos Fundamentais
Para uma compreens√£o s√≥lida, √© crucial dominar alguns conceitos fundamentais [^4.1], [^4.2].

**Conceito 1: O Problema da Complexidade do Modelo**
Modelos de regress√£o linear, dados por $f(x) = \beta_0 + \sum_{j=1}^p X_j\beta_j$, s√£o suscet√≠veis ao *overfitting*, especialmente quando o n√∫mero de preditores ($p$) √© alto em rela√ß√£o ao n√∫mero de observa√ß√µes ($N$) [^4.1]. A regress√£o linear padr√£o (m√≠nimos quadrados) tende a gerar coeficientes que se ajustam aos ru√≠dos espec√≠ficos dos dados de treinamento, resultando em alta vari√¢ncia e baixa capacidade de generaliza√ß√£o.  O objetivo central das t√©cnicas de *shrinkage* √© controlar esse *trade-off* entre **vi√©s** e **vari√¢ncia** [^4.2].

> üí° **Exemplo Num√©rico:** Imagine um cen√°rio com 100 observa√ß√µes ($N=100$) e 50 preditores ($p=50$). Um modelo de regress√£o linear com m√≠nimos quadrados pode se ajustar excessivamente aos dados de treinamento, gerando coeficientes muito grandes que capturam o ru√≠do aleat√≥rio. Isso resultaria em um modelo com baixa capacidade de generaliza√ß√£o para novos dados. T√©cnicas de *shrinkage* ajudam a mitigar esse problema, reduzindo a magnitude dos coeficientes.

**Lemma 1:**  A complexidade de um modelo linear est√° diretamente relacionada √† magnitude de seus coeficientes.  Modelos com coeficientes maiores t√™m maior capacidade de se ajustar ao ru√≠do dos dados, levando a maior vari√¢ncia e menor capacidade de generaliza√ß√£o.
*Prova:*  Em regress√£o linear, os coeficientes s√£o estimados minimizando o RSS (Residual Sum of Squares), conforme definido em (3.2) [^4.2].  Coeficientes maiores tendem a produzir um RSS menor nos dados de treino, mas podem aumentar a probabilidade de se ajustarem ao ru√≠do, aumentando a vari√¢ncia. $\blacksquare$
```mermaid
graph LR
    subgraph "Model Complexity"
        A["Model Coefficients (Œ≤)"] --> B{"Magnitude of Œ≤"}
        B --> C{"High Magnitude"}
        B --> D{"Low Magnitude"}
        C --> E{"Overfitting Risk"}
        C --> F{"High Variance"}
        D --> G{"Simpler Model"}
        D --> H{"Better Generalization"}
    end
```

**Conceito 2: Linear Discriminant Analysis (LDA) e suas Premissas**
A **Linear Discriminant Analysis (LDA)** √© um m√©todo de classifica√ß√£o linear que assume que as classes s√£o normalmente distribu√≠das, com m√©dias diferentes, mas com matriz de covari√¢ncia comum [^4.3]. A LDA busca encontrar a combina√ß√£o linear de preditores que melhor separam as classes, maximizando a dist√¢ncia entre as m√©dias e minimizando a vari√¢ncia dentro das classes. Uma abordagem de classifica√ß√£o linear √© apresentada em [^4.2] onde $f(X) = \beta_0 + \sum_{j=1}^p X_j \beta_j$, e os coeficientes $\beta_j$ podem ser estimados via m√≠nimos quadrados. As suposi√ß√µes de normalidade e homogeneidade de vari√¢ncias em LDA, embora simplificadoras, podem influenciar a escolha do melhor m√©todo de *shrinkage*.

**Corol√°rio 1:**  Sob a suposi√ß√£o de classes Gaussianas com mesma covari√¢ncia, a fronteira de decis√£o ideal entre duas classes √© um hiperplano. A LDA busca uma proje√ß√£o linear dos dados que maximize a separabilidade das classes.
*Prova:* A fun√ß√£o discriminante linear na LDA,  $\delta_k(x) = x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + log(\pi_k)$ , mostra que a decis√£o de classe √© baseada em uma fun√ß√£o linear de x. Ao assumir covari√¢ncias iguais, $\Sigma$, essa fun√ß√£o simplifica a uma combina√ß√£o linear de $x$ [^4.3.1].  $\blacksquare$
```mermaid
graph LR
    subgraph "Linear Discriminant Analysis (LDA)"
        A["Gaussian Classes"] --> B["Different Means (Œºk)"]
        A --> C["Same Covariance (Œ£)"]
        B & C --> D["Linear Discriminant Function: Œ¥k(x)"]
         D --> E["Decision Boundary: Hyperplane"]
         E --> F["Linear Combination of X"]
    end
```

> üí° **Exemplo Num√©rico:** Considere duas classes com m√©dias $\mu_1 = [1, 1]^T$ e $\mu_2 = [3, 3]^T$, e uma matriz de covari√¢ncia comum $\Sigma = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$. A LDA encontrar√° um hiperplano (neste caso, uma linha) que separa as duas classes da melhor forma poss√≠vel.  A fun√ß√£o discriminante simplificada,  $\delta_k(x) = x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + log(\pi_k)$, mostrar√° que a decis√£o √© baseada em uma combina√ß√£o linear de *x*.

**Conceito 3: Regress√£o Log√≠stica e a Fun√ß√£o Logit**
A **Regress√£o Log√≠stica**, por sua vez, √© um m√©todo para problemas de classifica√ß√£o onde a vari√°vel resposta √© bin√°ria [^4.4]. Ela modela a probabilidade de um evento (classe) usando a fun√ß√£o *logit*, que √© uma transforma√ß√£o logar√≠tmica das *odds* de um evento: $logit(p(x)) = \log(\frac{p(x)}{1 - p(x)})$. A regress√£o log√≠stica estima os par√¢metros $\beta$ do modelo maximizando a verossimilhan√ßa dos dados [^4.4.1].  A regulariza√ß√£o √©, tamb√©m nesse contexto, essencial para estabilizar e melhorar a generaliza√ß√£o do modelo. A escolha entre LDA e regress√£o log√≠stica depende das suposi√ß√µes sobre a distribui√ß√£o dos dados, conforme detalhado nos t√≥picos [^4.3] e [^4.4].

> ‚ö†Ô∏è **Nota Importante**:  A Regress√£o Log√≠stica e LDA, apesar de serem diferentes em formula√ß√£o, podem levar a decis√µes de classe semelhantes quando os dados s√£o aproximadamente lineares [^4.5].
> ‚ùó **Ponto de Aten√ß√£o**:  Em situa√ß√µes com dados desbalanceados, a regress√£o log√≠stica pode ser mais adequada para modelar a probabilidade da classe minorit√°ria [^4.4.2].
> ‚úîÔ∏è **Destaque**:  A compara√ß√£o entre os coeficientes de LDA e regress√£o log√≠stica fornece insights valiosos sobre a import√¢ncia relativa dos preditores em cada contexto [^4.5].

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o
```mermaid
flowchart LR
    subgraph "Linear Regression for Classification"
      A["Classes encoded as indicator matrix Y"] --> B["Estimate coefficients Œ≤ using OLS"]
      B --> C["Prediction:  ≈∑ = XŒ≤"]
      C --> D["Assign to class with highest indicator value"]
    end
```
**Explica√ß√£o:**  Este diagrama ilustra o processo de regress√£o de indicadores e seu fluxo em um problema de classifica√ß√£o, em compara√ß√£o com outros m√©todos [^4.2].

A regress√£o linear pode ser utilizada em problemas de classifica√ß√£o, codificando as classes em uma matriz de indicadores e aplicando o m√©todo dos m√≠nimos quadrados para estimar os coeficientes [^4.2].  Neste contexto, cada coluna da matriz de indicadores representa uma classe, e a predi√ß√£o √© feita atribuindo uma nova observa√ß√£o √† classe cujo indicador apresenta o maior valor previsto. Formalmente, se $Y$ √© a matriz de indicadores onde $Y_{ik}$ = 1 se a i-√©sima observa√ß√£o pertence √† k-√©sima classe, e 0 caso contr√°rio,  o modelo de regress√£o √© dado por $Y=X\beta + \epsilon$ [^4.2]. O objetivo √© estimar $\beta$ de modo a minimizar o RSS.  A decis√£o de classe √© dada por $\hat{y} = argmax_k (X\hat{\beta})_k$.

> üí° **Exemplo Num√©rico:** Considere um problema de classifica√ß√£o com 3 classes e 4 observa√ß√µes. A matriz de indicadores *Y* e a matriz de preditores *X* podem ser representadas como:
>
> $$
> Y = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}, \quad
> X = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \\ 4 & 2 \end{bmatrix}
> $$
>
> Usando o m√©todo dos m√≠nimos quadrados, podemos estimar os coeficientes $\beta$ como $\hat{\beta} = (X^TX)^{-1}X^TY$.
>  ```python
> import numpy as np
> from numpy.linalg import inv
>
> X = np.array([[1, 2], [2, 1], [3, 3], [4, 2]])
> Y = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]])
>
> X_transpose = X.T
> beta_hat = np.dot(np.dot(inv(np.dot(X_transpose, X)), X_transpose), Y)
> print("Estimated beta: \n", beta_hat)
> ```
> A sa√≠da do c√≥digo mostra os coeficientes $\hat{\beta}$ que, quando multiplicados por um novo conjunto de preditores, permitir√£o a aloca√ß√£o √† classe com maior valor predito.
> Para uma nova observa√ß√£o $x_{new} = [2, 2]$, a predi√ß√£o √© $x_{new}\hat{\beta}$.

**Lemma 2:**  A regress√£o linear em matrizes de indicadores, sob certas condi√ß√µes, gera fronteiras de decis√£o lineares similares √† LDA.
*Prova:*  Quando as classes podem ser razoavelmente separadas por hiperplanos, o m√©todo de m√≠nimos quadrados aplicado √† matriz de indicadores encontra uma solu√ß√£o que se assemelha √†s proje√ß√µes geradas por um discriminante linear.  No entanto, sob condi√ß√µes de alta dimensionalidade e *collinearities* entre as classes, regress√£o linear pode levar a resultados inst√°veis ou mesmo a extrapola√ß√µes fora do intervalo [0,1]  [^4.2], [^4.3].  $\blacksquare$

**Corol√°rio 2:**  Embora a regress√£o linear em matriz de indicadores possa gerar resultados compar√°veis a outros m√©todos lineares como LDA, ela √© sens√≠vel a multicolinearidade e pode produzir estimativas menos est√°veis quando as classes n√£o s√£o bem separadas.
*Prova:* A estimativa de par√¢metros na regress√£o linear de indicadores pode ser inst√°vel e levar a extrapola√ß√µes problem√°ticas, principalmente quando a multicolinearidade entre as classes √© alta [^4.2]. A LDA, por sua vez, utiliza uma estrutura baseada na an√°lise de covari√¢ncia, que muitas vezes leva a estimativas mais robustas. $\blacksquare$

Apesar de sua simplicidade, a regress√£o linear com matrizes de indicadores apresenta limita√ß√µes, como a dificuldade em modelar probabilidades, e a possibilidade de gerar previs√µes fora do intervalo [0,1].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o
```mermaid
graph LR
    subgraph "Regularization in Classification"
        A["Logistic Regression"] --> B{"Negative Log-Likelihood"}
        B --> C["L1 Penalty: Œª‚àë|Œ≤j|"]
        B --> D["L2 Penalty: Œª‚àëŒ≤j¬≤"]
        C --> E["Promotes Sparsity"]
        D --> F["Shrinks Coefficients"]
        E & F --> G["Improved Generalization"]
    end
```
 A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o t√©cnicas cruciais para lidar com problemas de classifica√ß√£o em altas dimens√µes [^4.5], [^4.4.4]. As t√©cnicas de regulariza√ß√£o, como L1 e L2, imp√µem restri√ß√µes aos coeficientes do modelo,  penalizando os coeficientes maiores e promovendo modelos mais simples e generaliz√°veis. Na regress√£o log√≠stica, o logit do modelo √© geralmente ajustado utilizando um processo de m√°xima verossimilhan√ßa, que pode ser facilmente adaptado para incluir termos de regulariza√ß√£o.

A fun√ß√£o de custo em regress√£o log√≠stica, geralmente expressa como *negative log-likelihood*, pode ser regularizada pela adi√ß√£o de termos de penalidade. A **penalidade L1**, dada por $\lambda \sum_{j=1}^p |\beta_j|$, promove a esparsidade do modelo, fazendo com que alguns coeficientes sejam exatamente zero [^4.4.4]. Isso realiza uma sele√ß√£o de vari√°veis *embutida*, identificando os preditores mais relevantes.  Por outro lado, a **penalidade L2**, expressa como $\lambda \sum_{j=1}^p \beta_j^2$, reduz a magnitude de todos os coeficientes, promovendo estabilidade e evitando solu√ß√µes com par√¢metros muito grandes [^4.4.4], [^4.5]. Ambas as abordagens ajudam a lidar com o *overfitting*.

> üí° **Exemplo Num√©rico:** Para ilustrar a diferen√ßa entre L1 e L2, vamos gerar um conjunto de dados sint√©tico para regress√£o log√≠stica e aplicar ambas as regulariza√ß√µes:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LogisticRegression
> from sklearn.preprocessing import StandardScaler
> from sklearn.pipeline import make_pipeline
> from sklearn.model_selection import train_test_split
>
> # Gerar dados sint√©ticos
> np.random.seed(42)
> X = np.random.randn(100, 10)
> y = np.random.randint(0, 2, 100)
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # Criar modelos regularizados
> model_l1 = make_pipeline(StandardScaler(), LogisticRegression(penalty='l1', solver='liblinear', C=0.1))
> model_l2 = make_pipeline(StandardScaler(), LogisticRegression(penalty='l2', C=0.1))
>
> # Ajustar modelos
> model_l1.fit(X_train, y_train)
> model_l2.fit(X_train, y_train)
>
> # Obter coeficientes
> coef_l1 = model_l1.named_steps['logisticregression'].coef_[0]
> coef_l2 = model_l2.named_steps['logisticregression'].coef_[0]
>
> print("L1 Coefficients:", coef_l1)
> print("L2 Coefficients:", coef_l2)
>
> # Plotar coeficientes
> plt.figure(figsize=(10, 6))
> plt.plot(coef_l1, label='L1 (Lasso)')
> plt.plot(coef_l2, label='L2 (Ridge)')
> plt.xlabel('Coeficiente Index')
> plt.ylabel('Valor do Coeficiente')
> plt.title('Compara√ß√£o de coeficientes L1 e L2')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> Este exemplo demonstra que a regulariza√ß√£o L1 (Lasso) tende a zerar alguns coeficientes, resultando em um modelo mais esparso, enquanto a L2 (Ridge) apenas reduz a magnitude dos coeficientes.

**Lemma 3:** A penalidade L1 em regress√£o log√≠stica leva a coeficientes esparsos, ou seja, alguns coeficientes tendem a ser exatamente zero.
*Prova:* O termo da penalidade L1, dado por $\lambda \sum_{j=1}^p |\beta_j|$, √© n√£o-diferenci√°vel em zero, o que resulta na concentra√ß√£o de probabilidade nas bordas da regi√£o de solu√ß√£o.  Essa caracter√≠stica provoca a redu√ß√£o de alguns coeficientes para exatamente zero, efetivando uma sele√ß√£o de vari√°veis [^4.4.4], [^4.4.5]. $\blacksquare$

**Prova do Lemma 3:**  A prova formal envolve a an√°lise da subgradiente da fun√ß√£o de penalidade, mostrando como a n√£o-diferenciabilidade em zero faz com que algumas solu√ß√µes m√≠nimas ocorram onde um ou mais $\beta_j$ s√£o nulos [^4.4.3].  Este processo est√° relacionado √† busca por solu√ß√µes "esparsas" que simplificam a interpreta√ß√£o e melhoram a generaliza√ß√£o do modelo. $\blacksquare$

**Corol√°rio 3:**  A esparsidade promovida pela penalidade L1 facilita a interpreta√ß√£o dos modelos classificat√≥rios, identificando quais preditores s√£o mais relevantes para a decis√£o de classe.
*Prova:*  Ao reduzir a quantidade de par√¢metros ativos, a penaliza√ß√£o L1 torna o modelo mais parcimonioso e, consequentemente, mais f√°cil de ser interpretado. Os preditores com coeficientes diferentes de zero s√£o os que efetivamente contribuem para a decis√£o de classe [^4.4.5]. $\blacksquare$

> ‚ö†Ô∏è **Ponto Crucial**:  A combina√ß√£o das penalidades L1 e L2, conhecida como *Elastic Net*, permite aproveitar as vantagens de ambos os m√©todos, promovendo tanto a esparsidade quanto a estabilidade do modelo [^4.5].

### Separating Hyperplanes e Perceptrons
```mermaid
flowchart LR
    subgraph "Perceptron Algorithm"
        A["Initialize weights w and bias b"] --> B["For each training example x"]
        B --> C{"Classify x:  sign(w‚ãÖx + b)"}
        C --> D{"Correct Classification?"}
         D -- Yes --> B
        D -- No --> E["Update weights:  w = w + Œ∑x, b = b + Œ∑"]
         E --> B
        B --> F{"Converges if data is linearly separable"}
    end
```
A ideia de **hiperplanos separadores** √© fundamental para a classifica√ß√£o linear, representando a fronteira de decis√£o entre diferentes classes [^4.5.2]. Em um espa√ßo de alta dimensionalidade, um hiperplano √© uma superf√≠cie linear que divide o espa√ßo em regi√µes correspondentes a diferentes classes. A busca por um hiperplano √≥timo geralmente envolve a maximiza√ß√£o da margem de separa√ß√£o, ou seja, a dist√¢ncia entre o hiperplano e os pontos mais pr√≥ximos de cada classe. O modelo de regress√£o log√≠stica, embora estime probabilidades, tamb√©m gera hiperplanos de decis√£o lineares.

O **Perceptron**, um algoritmo cl√°ssico de aprendizagem, busca encontrar um hiperplano separador iterativamente [^4.5.1]. O algoritmo come√ßa com uma fronteira de decis√£o aleat√≥ria, e a cada itera√ß√£o, atualiza seus pesos com base em erros de classifica√ß√£o. Se a classifica√ß√£o de uma observa√ß√£o estiver incorreta, os pesos s√£o ajustados de modo a mover a fronteira de decis√£o na dire√ß√£o correta, garantindo eventualmente uma separa√ß√£o perfeita caso as classes sejam linearmente separ√°veis [^4.5.1].

> üí° **Exemplo Num√©rico:** Considere duas classes linearmente separ√°veis com os seguintes pontos: Classe 1:  $(-1, -1), (-2, -2)$; Classe 2: $(1, 1), (2, 2)$. Inicializamos pesos aleat√≥rios, por exemplo, $w = [0.1, 0.2]$ e um bias $b = 0.0$. O Perceptron itera sobre os pontos, atualizando os pesos quando uma classifica√ß√£o incorreta √© encontrada:
>
> 1. Ponto $(-1, -1)$:  $w \cdot x + b = 0.1*(-1) + 0.2*(-1) + 0 = -0.3$. Classifica√ß√£o incorreta (deveria ser > 0).  Atualiza $w = w + \eta x = [0.1, 0.2] + 1*[-1,-1] = [-0.9,-0.8]$, $b = b + \eta * 1 = 0 + 1 = 1$ (onde $\eta = 1$).
> 2. Ponto $(-2, -2)$: $w \cdot x + b = -0.9*(-2) + (-0.8)*(-2) + 1 = 1.8 + 1.6 + 1 = 4.4$. Classifica√ß√£o correta.
> 3. Ponto $(1, 1)$:  $w \cdot x + b = -0.9*(1) + (-0.8)*(1) + 1 = -0.7$. Classifica√ß√£o incorreta (deveria ser < 0). Atualiza $w = w - \eta x = [-0.9, -0.8] - 1*[1,1] = [-1.9, -1.8]$, $b = b - \eta * 1 = 1 - 1 = 0$
> 4. Ponto $(2, 2)$: $w \cdot x + b = -1.9*(2) + (-1.8)*(2) + 0 = -3.8 - 3.6 = -7.4$. Classifica√ß√£o correta.
>
> Este processo continua at√© que n√£o haja classifica√ß√µes incorretas, encontrando um hiperplano separador.

**Teorema:** Se os dados forem linearmente separ√°veis, o algoritmo do Perceptron converge em um n√∫mero finito de itera√ß√µes. A prova envolve a demonstra√ß√£o de que cada atualiza√ß√£o de pesos reduz a dist√¢ncia entre as classifica√ß√µes e a fronteira de decis√£o at√© que o erro seja nulo.
*Prova:* A prova do teorema do Perceptron usa o conceito de separabilidade dos dados para demonstrar que a cada atualiza√ß√£o da fun√ß√£o discriminante, o erro de classifica√ß√£o √© reduzido, garantindo a converg√™ncia em n√∫mero finito de passos [^4.5.1]. $\blacksquare$

### Pergunta Te√≥rica Avan√ßada (Exemplo): Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?
```mermaid
graph LR
    subgraph "LDA vs Bayesian Decision Rule"
        A["LDA"] --> B["Linear Discriminant Function: Œ¥k(x)"]
        A --> C["Maximizes class separation"]
        B --> D{"Derived from means and covariances"}
        E["Bayesian Decision Rule"] --> F["Posterior Probability: P(G=k|X=x)"]
        F --> G["Proportional to P(X=x|G=k)P(G=k)"]
        G --> H{"Derived from probability distributions"}
        D & H --> I["Equivalent under Gaussian assumption and equal covariance"]
    end
```
**Resposta:** A Linear Discriminant Analysis (LDA) e a Regra de Decis√£o Bayesiana compartilham um objetivo comum: encontrar a melhor forma de classificar dados, dadas as distribui√ß√µes das classes. Quando ambas assumem distribui√ß√µes Gaussianas com matrizes de covari√¢ncia iguais entre classes, elas se tornam profundamente relacionadas [^4.3].

Na **Regra de Decis√£o Bayesiana**, a decis√£o de classificar uma observa√ß√£o x para a classe k √© baseada na probabilidade posterior $P(G=k|X=x)$, que √© proporcional a $P(X=x|G=k)P(G=k)$ [^4.3]. Para distribui√ß√µes Gaussianas com mesma covari√¢ncia, $P(X=x|G=k) \sim \mathcal{N}(\mu_k, \Sigma)$ e a fun√ß√£o discriminante resultante √© uma fun√ß√£o linear de $x$ [^4.3.3].

Por outro lado, a **LDA** deriva uma fun√ß√£o discriminante linear diretamente, buscando maximizar a separa√ß√£o entre as classes. Dada a premissa de covari√¢ncias iguais para as classes, a LDA utiliza a f√≥rmula $\delta_k(x) = x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + log(\pi_k)$ [^4.3.1]. As similaridades entre LDA e a regra de decis√£o Bayesiana s√£o mais evidentes quando comparamos o discriminante da LDA com a probabilidade posterior logar√≠tmica. A diferen√ßa reside principalmente na abordagem utilizada: A regra Bayesiana parte das probabilidades posteriores, enquanto a LDA deriva a fun√ß√£o linear diretamente atrav√©s da an√°lise das m√©dias e covari√¢ncias.

**Lemma 4:**  Em distribui√ß√µes Gaussianas com covari√¢ncias iguais, as fronteiras de decis√£o definidas pela LDA s√£o id√™nticas √†s fronteiras obtidas pela regra de decis√£o Bayesiana.
*Prova:* Ao analisar as fun√ß√µes discriminantes de cada m√©todo e expandir seus termos, fica claro que ambos utilizam combina√ß√µes lineares de x, utilizando as m√©dias de cada classe ponderadas pela inversa da matriz de covari√¢ncia comum [^4.3], [^4.3.3].  Sob as premissas de normalidade e covari√¢ncias iguais, o discriminante da LDA se torna proporcional ao logaritmo da probabilidade posterior definida pela regra de Bayes.  $\blacksquare$

**Corol√°rio 4:**  Ao relaxar a suposi√ß√£o de covari√¢ncias iguais, a regra de decis√£o Bayesiana leva a fronteiras de decis√£o quadr√°ticas, conhecidas como **Quadratic Discriminant Analysis (QDA)**.  A LDA, que assume igualdade nas matrizes de covari√¢ncias, n√£o √© capaz de gerar tais fronteiras quadr√°ticas.
*Prova:* Sem a igualdade nas covari√¢ncias, o discriminante de Bayes torna-se uma fun√ß√£o quadr√°tica de $x$ dada por $\delta_k(x) = - \frac{1}{2}(x - \mu_k)^T \Sigma_k^{-1}(x - \mu_k) + \log(\pi_k)$ [^4.3], [^4.3.1]. $\blacksquare$

> ‚ö†Ô∏è **Ponto Crucial**: A escolha entre LDA e QDA depende da validade da suposi√ß√£o de homogeneidade das covari√¢ncias. Quando essa suposi√ß√£o √© violada, a QDA pode ser mais apropriada, embora com o custo de mais par√¢metros a serem estimados e um maior risco de *overfitting* [^4.3.1].

### Conclus√£o
Este cap√≠tulo abordou em profundidade as t√©cnicas de *shrinkage*, explorando abordagens proporcionais, *soft* e *hard* no contexto da modelagem estat√≠stica. O *trade-off* entre vi√©s e vari√¢ncia foi um tema central, e as ferramentas apresentadas visam a encontrar um equil√≠brio ideal para cada problema espec√≠fico [^4.1]. A explora√ß√£o de m√©todos como LDA e Regress√£o Log√≠stica, junto com a discuss√£o de regulariza√ß√£o L1, L2, e m√©todos como a regress√£o de indicadores,  forneceu uma vis√£o completa dos desafios e das solu√ß√µes na modelagem de dados complexos [^4.2], [^4.3], [^4.4], [^4.5].  O uso de modelos lineares, como hiperplanos separadores, Perceptrons, bem como a discuss√£o de sele√ß√µes de vari√°veis, e aprofundamento dos conceitos, fornecem uma base s√≥lida para a compreens√£o do que se segue nesse campo.
<!-- END DOCUMENT -->

### Footnotes
[^4.1]: "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,..., Xp. Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them." *(Trecho de Linear Methods for Regression)*
[^4.2]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation. Here the Bj's are unknown parameters or coefficients, and the variables X; can come from different sources" *(Trecho de Linear Methods for Regression)*
[^4.3]: "Typically we have a set of training data (X1, Y1) ... (xn, yn) from which to estimate the parameters Œ≤. Each xi = (Xi1, Xi2,...,xip)T is a vector of feature measurements for the ith case." *(Trecho de Linear Methods for Regression)*
[^4.3.1]: "Assuming (for the moment) that X has full column rank, and hence XTX is positive definite, we set the first derivative to zero  XT (y ‚Äì X·∫û) = 0 to obtain the unique solution  Œ≤ = (XTX)-1XTy." *(Trecho de Linear Methods for Regression)*
[^4.3.2]: "The predicted values at an input vector xo are given by f(xo) = (1 : xo)T·∫û; the fitted values at the training inputs are y = X·∫û = X(XX)-1X7y, where ≈∑i = f(xi)." *(Trecho de Linear Methods for Regression)*
[^4.3.3]: "We denote the column vectors of X by x0, x1,..., Xp, with x0 = 1. For much of what follows, this first column is treated like any other. These vectors span a subspace of IRN, also referred to as the column space of X." *(Trecho de Linear Methods for Regression)*
[^4.4]: "To draw inferences about the parameters and the model, additional assumptions are needed. We now assume that (3.1) is the correct model for the mean; that is, the conditional expectation of Y is linear in X1,..., Xp." *(Trecho de Linear Methods for Regression)*
[^4.4.1]: "We also assume that the deviations of Y around its expectation are additive and Gaussian. Hence  Y = E(Y|X1,..., Xp) + Œµ  =  Œ≤0 + \sum_{j=1}^p X_j\beta_j + Œµ, where the error Œµ is a Gaussian random variable with expectation zero and variance œÉ¬≤, written Œµ ~ N(0, œÉ¬≤)." *(Trecho de Linear Methods for Regression)*
[^4.4.2]: "Often we need to test for the significance of groups of coefficients simul- taneously. For example, to test if a categorical variable with k levels can be excluded from a model, we need to test whether the coefficients of the dummy variables used to represent the levels can all be set to zero." *(Trecho de Linear Methods for Regression)*
[^4.4.3]: "Here we use the F statistic,  F = (RSS0 - RSS1)/(P1 - —Ä–æ)  RSS1/(N-P1 ‚Äì 1) where RSS‚ÇÅ is the residual sum-of-squares for the least squares fit of the big- ger model with p‚ÇÅ +1 parameters, and RSSo the same for the nested smaller model with po + 1 parameters, having p1 - po parameters constrained to be zero." *(Trecho de Linear Methods for Regression)*
[^4.4.4]: "The F statistic measures the change in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an esti- mate of œÉ2." *(Trecho de Linear Methods for Regression)*
[^4.4.5]: "Under the Gaussian assumptions, and the null hypothesis that the smaller model is correct, the F statistic will have a Fp1-po,N-p1-1 dis- tribution." *(Trecho de Linear Methods for Regression)*
[^4.5]: "By retaining a subset of the predictors and discarding the rest, subset selection produces a model that is interpretable and has possibly lower predic- tion error than the full model." *(Trecho de Linear Methods for Regression)*
[^4.5.1]: "However, because it is a discrete process‚Äî variables are either retained or discarded‚Äîit often exhibits high variance, and so doesn't reduce the prediction error of the full model. Shrinkage methods are more continuous, and don't suffer as much from high variability." *(Trecho de Linear Methods for Regression)*
[^4.5.2]: "Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized residual sum of squares,"  *(Trecho de Linear Methods for Regression)*
