## Interpreta√ß√µes Geom√©tricas em Modelos de Regress√£o Linear

<imagem: Diagrama mostrando um espa√ßo vetorial tridimensional com um hiperplano representando o modelo linear, vetores de dados, proje√ß√µes dos dados no hiperplano, e res√≠duos, ilustrando a interpreta√ß√£o geom√©trica dos conceitos de regress√£o>
```mermaid
graph LR
    A[Espa√ßo dos Preditores] --> B("Hiperplano de Regress√£o");
    C[Vetor de Dados] --> B;
    C --> D("Proje√ß√£o no Hiperplano");
    D --> E("Res√≠duo");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#cfc,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

A compreens√£o geom√©trica dos modelos de regress√£o linear fornece *insights* valiosos e complementares √†s interpreta√ß√µes alg√©bricas e estat√≠sticas. Ao visualizarmos os dados, o espa√ßo dos preditores e o modelo de regress√£o como objetos geom√©tricos, podemos entender a fundo o significado das opera√ß√µes de m√≠nimos quadrados, o papel da ortogonalidade, a natureza das regulariza√ß√µes, e os algoritmos de sele√ß√£o de vari√°veis. Este cap√≠tulo explora as interpreta√ß√µes geom√©tricas mais importantes dos modelos de regress√£o linear, fornecendo uma base s√≥lida para a compreens√£o e utiliza√ß√£o das t√©cnicas abordadas.

### Interpreta√ß√£o Geom√©trica da Regress√£o Linear

Nesta se√ß√£o, exploraremos as interpreta√ß√µes geom√©tricas dos principais conceitos da regress√£o linear, que incluem:

**Espa√ßo dos Preditores e o Hiperplano de Regress√£o**
O espa√ßo dos preditores em um modelo de regress√£o linear pode ser visualizado como um espa√ßo vetorial [^46]. Cada preditor $x_j$ √© um eixo nesse espa√ßo, e cada observa√ß√£o pode ser representada como um ponto nesse espa√ßo, descrito por um vetor de preditores, $x_i = (x_{i1}, x_{i2}, \ldots, x_{ip})$. O modelo de regress√£o linear define uma rela√ß√£o linear entre esses preditores e a vari√°vel resposta $y$, que geometricamente corresponde a um hiperplano nesse espa√ßo [^46].
Este hiperplano, tamb√©m conhecido como o plano de regress√£o, representa as predi√ß√µes do modelo para todos os valores poss√≠veis dos preditores, dado pelos par√¢metros $\beta_0, \beta_1, \ldots, \beta_p$, definidos pela equa√ß√£o:
$$
\hat{y} = \beta_0 + \sum_{j=1}^p \beta_j x_j.
$$
> üí° **Exemplo Num√©rico:**
> Considere um modelo de regress√£o linear com dois preditores, $x_1$ e $x_2$, e um intercepto $\beta_0$. O modelo √© dado por $\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2$. Geometricamente, cada observa√ß√£o $(x_{i1}, x_{i2})$ pode ser vista como um ponto no plano $x_1$-$x_2$. O modelo de regress√£o define um plano nesse espa√ßo tridimensional (incluindo o eixo $\hat{y}$). Por exemplo, se $\beta_0 = 1$, $\beta_1 = 2$, e $\beta_2 = -1$, a predi√ß√£o para um ponto $(x_1 = 2, x_2 = 3)$ seria $\hat{y} = 1 + 2(2) - 1(3) = 2$. Este ponto $(2, 3, 2)$ reside no plano de regress√£o.
> ```mermaid
>  graph LR
>      A[Eixo x1] --> C(Plano x1-x2);
>      B[Eixo x2] --> C;
>      C --> D[Hiperplano de Regress√£o];
>      E[Ponto de Dado (x1,x2,y)] --> D;
> ```

**M√≠nimos Quadrados como Proje√ß√£o Ortogonal**
O m√©todo de m√≠nimos quadrados, utilizado para ajustar os modelos lineares, tem uma interpreta√ß√£o geom√©trica muito clara: a solu√ß√£o de m√≠nimos quadrados corresponde √† proje√ß√£o ortogonal do vetor de respostas $y$ no hiperplano definido pelos preditores [^12]. Seja $X$ a matriz de design e $\hat{y}$ o vetor de predi√ß√µes obtido pelo modelo de regress√£o linear, o vetor de res√≠duos,  $r = y - \hat{y}$,  √© ortogonal ao espa√ßo gerado pelas colunas da matriz X. Em termos geom√©tricos, isso significa que a solu√ß√£o de m√≠nimos quadrados $\hat{\beta}$ √© tal que o vetor de predi√ß√µes $\hat{y}$ √© o ponto mais pr√≥ximo do vetor de respostas $y$ no espa√ßo gerado pelas colunas de X, no sentido da dist√¢ncia euclidiana. O vetor de res√≠duos $r$, representa a diferen√ßa entre o vetor de resposta y e a sua proje√ß√£o $\hat{y}$, e portanto √© ortogonal ao hiperplano de regress√£o.
```mermaid
graph LR
    A[Vetor de Respostas y] --> B("Proje√ß√£o Ortogonal");
    B --> C[Hiperplano de Regress√£o];
     C-->D[Vetor de Predi√ß√µes yÃÇ];
    A --> E[Vetor de Res√≠duos r];
    E -- "Ortogonal" --> C;
   style A fill:#f9f,stroke:#333,stroke-width:2px
   style C fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#cfc,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
> Suponha que temos um vetor de respostas $y = \begin{bmatrix} 5 \\ 8 \\ 10 \end{bmatrix}$ e uma matriz de design $X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}$. O vetor $\hat{y}$ √© a proje√ß√£o ortogonal de $y$ no espa√ßo gerado pelas colunas de $X$. Calculando $\hat{\beta} = (X^T X)^{-1} X^T y$:
>
> $X^T = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 4 \end{bmatrix}$
>
> $X^T X = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix} = \begin{bmatrix} 3 & 9 \\ 9 & 29 \end{bmatrix}$
>
> $(X^T X)^{-1} = \frac{1}{3 \cdot 29 - 9 \cdot 9} \begin{bmatrix} 29 & -9 \\ -9 & 3 \end{bmatrix} = \frac{1}{6} \begin{bmatrix} 29 & -9 \\ -9 & 3 \end{bmatrix}$
>
> $X^T y = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} 5 \\ 8 \\ 10 \end{bmatrix} = \begin{bmatrix} 23 \\ 74 \end{bmatrix}$
>
> $\hat{\beta} = \frac{1}{6} \begin{bmatrix} 29 & -9 \\ -9 & 3 \end{bmatrix} \begin{bmatrix} 23 \\ 74 \end{bmatrix} = \frac{1}{6} \begin{bmatrix} 667 - 666 \\ -207 + 222 \end{bmatrix} = \frac{1}{6} \begin{bmatrix} 1 \\ 15 \end{bmatrix} = \begin{bmatrix} 1/6 \\ 15/6 \end{bmatrix} = \begin{bmatrix} 0.1667 \\ 2.5 \end{bmatrix}$
>
> $\hat{y} = X\hat{\beta} = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix} \begin{bmatrix} 0.1667 \\ 2.5 \end{bmatrix} = \begin{bmatrix} 5.1667 \\ 7.6667 \\ 10.1667 \end{bmatrix}$
>
> O vetor de res√≠duos √© $r = y - \hat{y} = \begin{bmatrix} 5 \\ 8 \\ 10 \end{bmatrix} - \begin{bmatrix} 5.1667 \\ 7.6667 \\ 10.1667 \end{bmatrix} = \begin{bmatrix} -0.1667 \\ 0.3333 \\ -0.1667 \end{bmatrix}$.
>
> O produto escalar entre os res√≠duos e as colunas de $X$ ser√° aproximadamente zero, confirmando a ortogonalidade:
> $X^T r = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} -0.1667 \\ 0.3333 \\ -0.1667 \end{bmatrix} = \begin{bmatrix} -0.1667 + 0.3333 - 0.1667 \\ -0.3334 + 0.9999 - 0.6668 \end{bmatrix} = \begin{bmatrix} -0.0001 \\ -0.0003 \end{bmatrix} \approx \begin{bmatrix} 0 \\ 0 \end{bmatrix}$.
>
> Isso demonstra que os res√≠duos s√£o ortogonais ao espa√ßo gerado pelas colunas de $X$, e que $\hat{y}$ √© a proje√ß√£o ortogonal de $y$ nesse espa√ßo.

**Res√≠duos como Vetores Ortogonais**
Na regress√£o linear, os res√≠duos $r_i = y_i - \hat{y}_i$ representam a diferen√ßa entre os valores observados e os valores preditos do modelo. Geometricamente, cada res√≠duo $r_i$ corresponde √† dist√¢ncia vertical entre um ponto de dado e o hiperplano de regress√£o [^13].
A condi√ß√£o de ortogonalidade dos res√≠duos com o espa√ßo dos preditores √© fundamental na solu√ß√£o de m√≠nimos quadrados:
$$ X^T(y - X\hat{\beta}) = 0 $$
Esta equa√ß√£o, em termos geom√©tricos, significa que o vetor de res√≠duos $(y-X\hat{\beta})$ √© ortogonal ao espa√ßo gerado pelas colunas de $X$, garantindo que o vetor das predi√ß√µes $\hat{y}=X\hat{\beta}$ √© o ponto mais pr√≥ximo do vetor $y$.

> üí° **Exemplo Num√©rico:**
> Usando o exemplo anterior, o vetor de res√≠duos foi calculado como $r = \begin{bmatrix} -0.1667 \\ 0.3333 \\ -0.1667 \end{bmatrix}$. Cada componente desse vetor representa a dist√¢ncia vertical entre um ponto de dado $y_i$ e o ponto correspondente no hiperplano de regress√£o $\hat{y}_i$. A ortogonalidade de $r$ com as colunas de $X$ (demonstrada no exemplo anterior) √© a garantia de que $\hat{y}$ √© a melhor aproxima√ß√£o de $y$ no espa√ßo linear gerado pelas colunas de $X$.

**Lemma 16:** O Subespa√ßo da Matriz de Design

As colunas da matriz de design X formam uma base para o subespa√ßo de preditores, ou seja, os diferentes vetores que cont√©m os valores das vari√°veis preditoras para as observa√ß√µes formam uma base para o espa√ßo de predi√ß√µes. O processo de m√≠nimos quadrados define uma proje√ß√£o ortogonal do vetor resposta nesse espa√ßo, resultando na predi√ß√£o $\hat{y} = X\beta$, onde $\beta$ √© o vetor de par√¢metros estimados. O vetor de res√≠duos $r = y-\hat{y}$ √© ortogonal ao espa√ßo gerado pelas colunas da matriz X.

**Corol√°rio 16:** Proje√ß√£o em um subespa√ßo

As predi√ß√µes da regress√£o linear resultam de uma proje√ß√£o ortogonal da vari√°vel resposta num subespa√ßo definido pelos preditores, onde o res√≠duo √© a menor dist√¢ncia poss√≠vel entre o vetor resposta e o subespa√ßo dos preditores. O modelo de regress√£o linear tenta construir um subespa√ßo onde a predi√ß√£o seja o mais pr√≥ximo poss√≠vel do vetor resposta.

### Interpreta√ß√£o Geom√©trica da Regulariza√ß√£o

<imagem: Diagrama comparando o efeito das penalidades L1 e L2 no espa√ßo dos par√¢metros, mostrando como elas levam a solu√ß√µes com diferentes propriedades de esparsidade e *shrinkage*>
```mermaid
graph LR
    A[Espa√ßo dos Par√¢metros] --> B("Regulariza√ß√£o L2 (Ridge)");
    A --> C("Regulariza√ß√£o L1 (Lasso)");
    B --> D("Restri√ß√£o Esf√©rica (Shrinkage)");
    C --> E("Restri√ß√£o Hiper-Octaedro (Sparsity)");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#cfc,stroke:#333,stroke-width:2px
```

As t√©cnicas de regulariza√ß√£o em modelos lineares podem ser interpretadas geometricamente, elucidando como elas promovem solu√ß√µes mais simples e generaliz√°veis.

**Regulariza√ß√£o L2 (Ridge) como Restri√ß√£o Esf√©rica**

Na regulariza√ß√£o L2 (Ridge), a fun√ß√£o objetivo adiciona uma penalidade proporcional √† soma dos quadrados dos coeficientes, $||\beta||_2^2$, √† fun√ß√£o de custo. Geometricamente, essa penalidade corresponde a uma restri√ß√£o esf√©rica no espa√ßo dos par√¢metros, onde a solu√ß√£o deve pertencer ao interior ou ao contorno de uma hiperesfera centrada na origem. Esta restri√ß√£o tende a encolher os coeficientes, aproximando-os de zero, o que reduz a *variance* do modelo.
A solu√ß√£o do problema de Ridge, $\hat{\beta}_{ridge} = (X^T X + \lambda I)^{-1} X^T y$, encontra o ponto onde a elipse da fun√ß√£o RSS intersecta a hiperesfera definida pela restri√ß√£o L2. Os contornos das solu√ß√µes do m√©todo de m√≠nimos quadrados s√£o elipses, e estas elipses intersectam os contornos da norma L2 na solu√ß√£o. As solu√ß√µes de Ridge, portanto, correspondem a proje√ß√µes da solu√ß√£o de m√≠nimos quadrados na regi√£o de restri√ß√£o definida por $||\beta||_2^2 \le t$.
Para valores maiores de $\lambda$, a restri√ß√£o se torna mais forte, e o raio da hiperesfera √© reduzido, levando a uma maior redu√ß√£o dos coeficientes e mais estabilidade.

> üí° **Exemplo Num√©rico:**
> Considere o mesmo conjunto de dados anterior, $X$ e $y$. Vamos aplicar a regulariza√ß√£o Ridge com $\lambda = 0.5$. A solu√ß√£o de Ridge √© dada por $\hat{\beta}_{ridge} = (X^T X + \lambda I)^{-1} X^T y$.
>
> $X^T X = \begin{bmatrix} 3 & 9 \\ 9 & 29 \end{bmatrix}$ (calculado anteriormente).
>
> $\lambda I = 0.5 \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.5 & 0 \\ 0 & 0.5 \end{bmatrix}$
>
> $X^T X + \lambda I = \begin{bmatrix} 3.5 & 9 \\ 9 & 29.5 \end{bmatrix}$
>
> $(X^T X + \lambda I)^{-1} = \frac{1}{3.5 \cdot 29.5 - 9 \cdot 9} \begin{bmatrix} 29.5 & -9 \\ -9 & 3.5 \end{bmatrix} = \frac{1}{24.25} \begin{bmatrix} 29.5 & -9 \\ -9 & 3.5 \end{bmatrix}$
>
> $\hat{\beta}_{ridge} = \frac{1}{24.25} \begin{bmatrix} 29.5 & -9 \\ -9 & 3.5 \end{bmatrix} \begin{bmatrix} 23 \\ 74 \end{bmatrix} = \frac{1}{24.25} \begin{bmatrix} 678.5 - 666 \\ -207 + 259 \end{bmatrix} = \frac{1}{24.25} \begin{bmatrix} 12.5 \\ 52 \end{bmatrix} = \begin{bmatrix} 0.515 \\ 2.144 \end{bmatrix}$
>
> Comparando com $\hat{\beta} = \begin{bmatrix} 0.1667 \\ 2.5 \end{bmatrix}$ (OLS), vemos que os coeficientes foram reduzidos em magnitude.  A restri√ß√£o esf√©rica da regulariza√ß√£o L2 "encolhe" os coeficientes em dire√ß√£o √† origem.

**Regulariza√ß√£o L1 (Lasso) como Restri√ß√£o em um Hiper-Octaedro**

Na regulariza√ß√£o L1 (Lasso), a fun√ß√£o objetivo adiciona uma penalidade proporcional √† soma dos valores absolutos dos coeficientes, $||\beta||_1$, √† fun√ß√£o de custo. Geometricamente, essa penalidade corresponde a uma restri√ß√£o em forma de hiper-octaedro no espa√ßo dos par√¢metros, onde a solu√ß√£o deve pertencer ao interior ou ao contorno deste objeto. Esta restri√ß√£o tende a gerar solu√ß√µes esparsas, j√° que o contorno do hiper-octaedro tem quinas, e muitas vezes intersecta a regi√£o definida pela RSS nos eixos, for√ßando alguns coeficientes a serem exatamente zero [^71].
A solu√ß√£o do Lasso corresponde a um ponto onde a elipse da fun√ß√£o RSS intersecta o hiper-octaedro da penalidade L1, $\lambda ||\beta||_1 \le t$. A solu√ß√£o do Lasso pode ser vista como uma proje√ß√£o da solu√ß√£o de m√≠nimos quadrados na regi√£o de restri√ß√£o definida por $||\beta||_1 \le t$, mas a geometria n√£o-suave do hiper-octaedro favorece solu√ß√µes com alguns coeficientes iguais a zero, o que significa que o modelo final depende de um subconjunto pequeno de par√¢metros.

> üí° **Exemplo Num√©rico:**
> Para o mesmo conjunto de dados, vamos demonstrar o efeito do Lasso. O c√°lculo exato da solu√ß√£o do Lasso √© mais complexo e requer m√©todos iterativos, mas para fins ilustrativos, vamos supor que a solu√ß√£o do Lasso com um $\lambda$ espec√≠fico resultou em $\hat{\beta}_{lasso} = \begin{bmatrix} 0 \\ 2.3 \end{bmatrix}$. Note que o primeiro coeficiente foi definido como zero. Isso ilustra como a penalidade L1 leva √† esparsidade do modelo, selecionando apenas os preditores mais relevantes. A restri√ß√£o em forma de hiper-octaedro favorece que a solu√ß√£o seja encontrada em um dos eixos, for√ßando alguns coeficientes a zero.

**Elastic Net como uma Combina√ß√£o de Restri√ß√µes**

Na Elastic Net, a fun√ß√£o objetivo combina penalidades L1 e L2. Geometricamente, a restri√ß√£o correspondente √© uma combina√ß√£o das regi√µes definidas por ambas as penalidades, com cantos mais suaves do que o diamante definido pelo Lasso, e com encolhimento dos par√¢metros, como no Ridge. A solu√ß√£o do Elastic Net corresponde a uma combina√ß√£o das solu√ß√µes do Lasso e Ridge, gerando modelos que promovem tanto *sparsity* como *shrinkage* nos coeficientes.
```mermaid
graph LR
    A[Regulariza√ß√£o Elastic Net] --> B("Combina√ß√£o de Restri√ß√µes L1 e L2");
    B --> C("Shrinkage e Sparsity");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
> Suponha que, para um dado $\lambda$ e uma propor√ß√£o entre as penalidades L1 e L2, a solu√ß√£o do Elastic Net seja $\hat{\beta}_{elastic} = \begin{bmatrix} 0.2 \\ 2.2 \end{bmatrix}$. Este exemplo mostra que a Elastic Net combina a propriedade de *shrinkage* do Ridge (coeficientes n√£o exatamente zero) e a tend√™ncia √† esparsidade do Lasso (coeficientes menores do que os estimados por OLS).

**Lemma 17:** Interpreta√ß√£o da Restri√ß√£o nos Coeficientes

As penaliza√ß√µes L1 e L2 podem ser entendidas como restri√ß√µes no espa√ßo dos par√¢metros, sendo o objetivo minimizar a fun√ß√£o de custo sujeita a essa restri√ß√£o.
- A restri√ß√£o L1 √© dada pela regi√£o $||\beta||_1 \leq t$, onde $t$ √© um par√¢metro relacionado com $\lambda$. Esta regi√£o corresponde a um hiper-octaedro, que tem cantos nos eixos. A interse√ß√£o entre este hiper-octaedro e as curvas de n√≠vel da fun√ß√£o de custo resulta em solu√ß√µes onde os coeficientes tendem a ser zero, induzindo *sparsity*.
- A restri√ß√£o L2 √© dada pela regi√£o $||\beta||_2^2 \leq t$, onde $t$ tamb√©m √© um par√¢metro relacionado com $\lambda$. Esta regi√£o corresponde a uma hiperesfera, que n√£o tem quinas, e induz os coeficientes a se tornarem menores em m√≥dulo, mas nunca totalmente iguais a zero, levando a uma solu√ß√£o mais est√°vel que o Lasso.
- A restri√ß√£o da Elastic Net corresponde a uma combina√ß√£o linear das normas L1 e L2, e a regi√£o correspondente tem cantos menos pronunciados que a norma L1 e a forma mais arredondada que a norma L2.

**Corol√°rio 17:**  Efeito Geom√©trico de Lambda

O par√¢metro de regulariza√ß√£o, $\lambda$, controla o tamanho da regi√£o de restri√ß√£o imposta pelo modelo de regulariza√ß√£o. Um $\lambda$ maior imp√µe restri√ß√µes mais fortes, com uma regi√£o de restri√ß√£o menor, enquanto um $\lambda$ menor imp√µe restri√ß√µes mais fracas, com uma regi√£o maior. A escolha de $\lambda$ define o equil√≠brio entre o ajuste dos dados e a estabilidade do modelo, e a sua interpreta√ß√£o geom√©trica ajuda a entender o impacto da regulariza√ß√£o nas propriedades da solu√ß√£o final.

### Algoritmos de Caminho e a Geometria do Lasso

<imagem: Descri√ß√£o detalhada da imagem: Gr√°fico da trajet√≥ria de coeficientes ao longo de um caminho de regulariza√ß√£o, mostrando como as dire√ß√µes dos coeficientes mudam √† medida que o par√¢metro de regulariza√ß√£o varia. Os coeficientes se movem em linha reta entre mudan√ßas no conjunto ativo.>

A interpreta√ß√£o geom√©trica do algoritmo LARS nos ajuda a entender o significado de como o caminho de solu√ß√£o do Lasso √© constru√≠do.

**LARS: um Caminho de Solu√ß√µes Piecewise Linear**

O algoritmo LARS computa todo o caminho de solu√ß√µes do Lasso, variando o par√¢metro de regulariza√ß√£o $\lambda$ desde zero at√© um ponto onde todos os coeficientes s√£o nulos. A interpreta√ß√£o geom√©trica desse caminho √© que ele √© *piecewise linear*, ou seja, consiste em uma s√©rie de segmentos de linha reta [^76]. Cada mudan√ßa na dire√ß√£o da solu√ß√£o corresponde √† entrada ou sa√≠da de uma vari√°vel no conjunto ativo.
Ao longo do caminho do LARS, todos os coeficientes no conjunto ativo, que corresponde √†s vari√°veis com maiores correla√ß√µes com o res√≠duo atual, se movem juntos at√© que uma nova vari√°vel tenha uma correla√ß√£o igual, momento em que ela tamb√©m √© incluida no conjunto ativo e o processo de movimentos dos par√¢metros continua, alterando a dire√ß√£o no caminho.
A piecewise linearidade garante que o n√∫mero de passos no LARS seja finito, no m√°ximo igual ao n√∫mero de preditores, e tamb√©m simplifica a computa√ß√£o do caminho inteiro, evitando a necessidade de iterar por todo o espa√ßo de par√¢metros.
```mermaid
sequenceDiagram
    participant LARS
    participant Coeficientes
    LARS->>Coeficientes: Inicializa√ß√£o (Œª=‚àû, Œ≤=0)
    loop Variando Œª
    LARS->>Coeficientes:  Ajuste coeficientes no conjunto ativo
        alt Nova vari√°vel entra no conjunto ativo
            LARS->>Coeficientes: Mudan√ßa de dire√ß√£o
        end
    end
     LARS->>Coeficientes: Fim (Œª=0, todos Œ≤)
```

> üí° **Exemplo Num√©rico:**
> Vamos simular um exemplo simplificado do caminho do LARS. Imagine que temos tr√™s preditores $x_1, x_2, x_3$. Inicialmente, $x_1$ √© o mais correlacionado com $y$, e seu coeficiente $\beta_1$ come√ßa a crescer linearmente enquanto os outros s√£o 0. Em um certo ponto, $x_2$ se torna igualmente correlacionado com o res√≠duo, e ent√£o $\beta_2$ tamb√©m come√ßa a crescer, enquanto $\beta_1$ continua, mas com uma taxa de crescimento menor. O caminho da solu√ß√£o do Lasso no espa√ßo dos coeficientes ($\beta_1, \beta_2, \beta_3$) seria uma s√©rie de segmentos de linha reta, onde as dire√ß√µes mudam quando novas vari√°veis entram no conjunto ativo.
> ```mermaid
> graph LR
>     A[In√≠cio (Œª=‚àû)] --> B(Œ≤1 cresce);
>     B --> C(Œ≤1 e Œ≤2 crescem);
>     C --> D(Œ≤1, Œ≤2 e Œ≤3 crescem);
>     D --> E[Fim (Œª=0)];
>     style A fill:#f9f,stroke:#333,stroke-width:2px
>     style E fill:#ccf,stroke:#333,stroke-width:2px
> ```

**Interpreta√ß√£o da Condi√ß√£o de Otimalidade**

A condi√ß√£o de otimalidade do Lasso requer que o gradiente da RSS (que define a elipse dos m√≠nimos quadrados) seja ortogonal ao hiperplano gerado pelas vari√°veis selecionadas, e que a norma L‚àû da correla√ß√£o com os preditores que est√£o fora do conjunto ativo seja menor que um certo valor ($\lambda$). O algoritmo LARS garante essa condi√ß√£o ao manter a igualdade das correla√ß√µes com o res√≠duo para todas as vari√°veis no conjunto ativo, definindo o caminho da solu√ß√£o.

**Lemma 18:** Rela√ß√£o entre a Geometria e o LARS

A constru√ß√£o geom√©trica do algoritmo LARS e a piecewise linearidade do caminho de solu√ß√µes do Lasso s√£o uma consequ√™ncia direta da forma da penalidade L1 e sua condi√ß√£o de otimalidade. Ao encontrar o preditor mais correlacionado com o res√≠duo e ao mover seus coeficientes at√© que outro preditor se torne igualmente correlacionado, o algoritmo segue um caminho que √© garantido de ser piecewise linear [^76].

**Corol√°rio 18:** A Geometria e a Efici√™ncia do LARS

A interpreta√ß√£o geom√©trica do caminho de solu√ß√µes do LARS, que se baseia na condi√ß√£o de ortogonalidade, permite computar todo o caminho das solu√ß√µes do Lasso de maneira eficiente, sem a necessidade de computar as solu√ß√µes para cada valor de $\lambda$ em separado.

### Conclus√£o

A interpreta√ß√£o geom√©trica dos modelos de regress√£o linear e suas extens√µes fornecem uma intui√ß√£o importante para a compreens√£o do funcionamento dos modelos e da sua rela√ß√£o com os dados. Ao visualizarmos o espa√ßo dos preditores, o hiperplano da regress√£o, a proje√ß√£o ortogonal dos res√≠duos, e as regi√µes de restri√ß√£o da regulariza√ß√£o, somos capazes de entender de forma mais profunda os fundamentos das t√©cnicas de modelagem, incluindo os m√©todos de sele√ß√£o de vari√°veis e de regulariza√ß√£o. A interpreta√ß√£o geom√©trica tamb√©m √© essencial para entender as propriedades e caracter√≠sticas dos algoritmos de caminho.

### Refer√™ncias

[^46]: "The predicted values at an input vector xo are given by f(xo) = (1 : xo)T·∫û"
[^12]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit."
[^13]: "We minimize RSS(3) = ||y ‚Äì XŒ≤||2 by choosing ·∫û so that the residual vector y - ≈∑ is orthogonal to this subspace"
[^71]: "In this view, the lasso, ridge regression and best subset selection are Bayes estimates with different priors. Note, however, that they are derived as posterior modes, that is, maximizers of the posterior."
[^25]: "When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and exhibit high variance."
[^76]: "By construction the coefficients in LAR change in a piecewise linear fashion."
[^30]: "The algorithm Least Angle Regression (LARS), generates the path of solutions of LASSO efficiently."
[^78]: "In Figure 3.7 we have plotted the estimated prediction error versus the quantity..."
