## Bias-Variance Tradeoff in Linear Regression: A Comprehensive Analysis

```mermaid
graph LR
    A["Model Complexity"] --> B("Bias");
    A --> C("Variance");
    B --> D("Underfitting");
    C --> E("Overfitting");
    D --> F("Poor Performance on Train Data");
    E --> G("Poor Performance on New Data");
    F --> H("High Bias");
    G --> I("High Variance");
    H --> K("Simple Models");
     I --> L("Complex Models");
    K --> A
    L --> A
```

### IntroduÃ§Ã£o

O **Bias-Variance Tradeoff** Ã© um conceito central na modelagem estatÃ­stica e no aprendizado de mÃ¡quina, especialmente quando se trata de modelos de regressÃ£o linear e outras tÃ©cnicas relacionadas. Este conceito descreve o equilÃ­brio entre a capacidade de um modelo em ajustar-se aos dados de treinamento (baixo *bias*) e sua capacidade de generalizar para novos dados nÃ£o vistos (baixa *variance*). Encontrar esse equilÃ­brio ideal Ã© crucial para construir modelos que performem bem em dados nÃ£o vistos, ao invÃ©s de apenas memorizar padrÃµes nos dados de treinamento [^1]. A modelagem financeira, em particular, exige esse equilÃ­brio, dado que modelos excessivamente complexos podem levar a *overfitting*, resultando em desempenho fraco em dados futuros [^1].

### Conceitos Fundamentais

Nesta seÃ§Ã£o, exploraremos os conceitos fundamentais do *Bias-Variance Tradeoff*,  **RegularizaÃ§Ã£o**, e **Sparsity**,  integrando teoria e exemplos para fornecer uma compreensÃ£o robusta desses tÃ³picos interconectados.

**Conceito 1: Bias-Variance Tradeoff**

O **Bias** de um modelo refere-se ao erro introduzido pelas simplificaÃ§Ãµes feitas para modelar um problema complexo [^2]. Modelos com alto bias tendem a subajustar (*underfit*) os dados, o que significa que eles nÃ£o capturam a complexidade subjacente nos dados de treinamento e, portanto, tÃªm um desempenho ruim tanto nos dados de treinamento quanto em novos dados [^2]. Em contraste, a **Variance** de um modelo refere-se Ã  sensibilidade do modelo Ã s flutuaÃ§Ãµes nos dados de treinamento [^2]. Modelos com alta *variance* tendem a sobreajustar (*overfit*) os dados, ou seja, eles memorizam os ruÃ­dos e padrÃµes especÃ­ficos do conjunto de treinamento e nÃ£o generalizam bem para dados nÃ£o vistos [^2].

O *tradeoff* entre bias e variance surge porque modelos mais simples geralmente tÃªm alto bias e baixa variance, enquanto modelos mais complexos tÃªm baixo bias e alta variance [^2]. O objetivo ideal Ã© encontrar um modelo que minimize ambos os erros, mas, na prÃ¡tica, reduzir um deles muitas vezes aumenta o outro. Este tradeoff Ã© fundamental na escolha e otimizaÃ§Ã£o de modelos.

**Lemma 1:** DecomposiÃ§Ã£o do Erro QuadrÃ¡tico MÃ©dio (MSE)

O erro quadrÃ¡tico mÃ©dio (MSE) de um estimador pode ser decomposto em componentes de bias e variance [^3]. Seja um modelo $f(x; \theta)$ estimando uma funÃ§Ã£o verdadeira $y = g(x) + \epsilon$, onde $\epsilon$ representa ruÃ­do aleatÃ³rio com mÃ©dia zero e variÃ¢ncia $\sigma^2$. EntÃ£o, o MSE do modelo estimado em um ponto especÃ­fico $x$ pode ser escrito como:

$$
MSE(x) = E[(f(x;\theta) - g(x))^2] = [E(f(x;\theta)) - g(x)]^2 + E[(f(x;\theta) - E(f(x;\theta)))^2]
$$

$$
MSE(x) = Bias^2(x) + Variance(x)
$$

onde:

-   $Bias^2(x) = [E(f(x;\theta)) - g(x)]^2$ Ã© o quadrado do bias, que mede a diferenÃ§a entre a mÃ©dia das prediÃ§Ãµes do modelo e a funÃ§Ã£o verdadeira.
-   $Variance(x) = E[(f(x;\theta) - E(f(x;\theta)))^2]$ Ã© a variÃ¢ncia, que mede a variabilidade das prediÃ§Ãµes do modelo.

O objetivo Ã© minimizar o MSE, o que significa encontrar um equilÃ­brio entre reduzir o bias e a variance. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que a funÃ§Ã£o verdadeira seja $g(x) = 2x$ e temos um modelo linear $f(x; \theta) = \theta x$. Vamos analisar dois cenÃ¡rios: um modelo simples com $\theta = 1$ (alto bias) e um modelo mais complexo com $\theta$ variando aleatoriamente em torno de 2 (baixa bias, alta variance).
>
> **CenÃ¡rio 1: Alto Bias**
>
> *   Modelo: $f(x; \theta) = 1x$
> *   FunÃ§Ã£o Verdadeira: $g(x) = 2x$
> *   Para um ponto $x=3$, $g(3) = 6$ e $f(3) = 3$.
> *   Se repetirmos o processo vÃ¡rias vezes, o valor mÃ©dio de $f(3)$ serÃ¡ sempre 3.
> *   $Bias(3) = E[f(3;\theta)] - g(3) = 3 - 6 = -3$
> *   $Bias^2(3) = (-3)^2 = 9$
> *   $Variance(3) = E[(f(3;\theta) - E(f(3;\theta)))^2] = 0$ (pois $f(3;\theta)$ Ã© sempre 3)
> *   $MSE(3) = Bias^2(3) + Variance(3) = 9 + 0 = 9$
>
> **CenÃ¡rio 2: Baixo Bias, Alta Variance**
>
> *   Modelo: $f(x; \theta) = \theta x$, onde $\theta$ segue uma distribuiÃ§Ã£o normal com mÃ©dia 2 e desvio padrÃ£o 0.5.
> *   FunÃ§Ã£o Verdadeira: $g(x) = 2x$
> *   Para um ponto $x=3$, $g(3) = 6$.
> *   Simulando algumas amostras, podemos ter $f(3;\theta_1) = 1.8 * 3 = 5.4$, $f(3;\theta_2) = 2.3 * 3 = 6.9$, $f(3;\theta_3) = 1.9 * 3 = 5.7$
> *   $E[f(3;\theta)] \approx 6$ (a mÃ©dia das prediÃ§Ãµes se aproxima do valor verdadeiro)
> *   $Bias(3) = E[f(3;\theta)] - g(3) \approx 6 - 6 = 0$
> *   $Bias^2(3) \approx 0$
> *   $Variance(3) = E[(f(3;\theta) - E(f(3;\theta)))^2] \approx  E[(f(3;\theta) - 6)^2] $. Calculando a variÃ¢ncia das amostras acima: $Variance(3) =  \frac{(5.4-6)^2 + (6.9-6)^2 + (5.7-6)^2}{3} \approx 0.3$.
> *   $MSE(3) = Bias^2(3) + Variance(3) \approx 0 + 0.3 = 0.3$
>
> Neste exemplo, o modelo com alto bias ($f(x) = x$) tem um MSE muito maior do que o modelo com baixo bias e alta variance ($f(x) = \theta x$), apesar da alta variÃ¢ncia do segundo modelo. Isto ilustra como o bias e a variance contribuem para o erro total, e como o objetivo Ã© encontrar o equilÃ­brio ideal. A alta variÃ¢ncia causa um erro pequeno, mas o alto bias causa um erro maior.

**Conceito 2: RegularizaÃ§Ã£o**

A **RegularizaÃ§Ã£o** Ã© uma tÃ©cnica usada para evitar o *overfitting* em modelos complexos, adicionando um termo de penalidade Ã  funÃ§Ã£o de custo [^4]. Esse termo de penalidade, geralmente uma funÃ§Ã£o da magnitude dos parÃ¢metros do modelo, forÃ§a o modelo a aprender representaÃ§Ãµes mais simples, generalizÃ¡veis e estÃ¡veis,  reduzindo sua sensibilidade aos dados de treinamento [^4]. Existem duas formas comuns de regularizaÃ§Ã£o:

-   **RegularizaÃ§Ã£o L1 (Lasso):** Adiciona a soma dos valores absolutos dos parÃ¢metros do modelo Ã  funÃ§Ã£o de custo [^4]. Esta forma de regularizaÃ§Ã£o tende a gerar modelos esparsos, onde muitos dos parÃ¢metros sÃ£o exatamente zero, promovendo a seleÃ§Ã£o de variÃ¡veis. A norma L1 dos coeficientes Ã© usada na penalizaÃ§Ã£o: $||\beta||_1 = \sum_{j=1}^{p} |\beta_j|$.

-   **RegularizaÃ§Ã£o L2 (Ridge):** Adiciona a soma dos quadrados dos parÃ¢metros do modelo Ã  funÃ§Ã£o de custo [^4]. Esta forma de regularizaÃ§Ã£o reduz a magnitude dos parÃ¢metros, mas nÃ£o os leva a zero, resultando em modelos mais estÃ¡veis, porÃ©m menos esparsos. A norma L2 dos coeficientes Ã© usada na penalizaÃ§Ã£o: $||\beta||_2^2 = \sum_{j=1}^{p} \beta_j^2$.
```mermaid
graph LR
    A("Loss Function") --> B("Add Penalty Term");
    B --> C("L1 Regularization");
    B --> D("L2 Regularization");
    C --> E("Promotes Sparsity");
    D --> F("Reduces Magnitude");
     C --> G("Variable Selection");
```
A forÃ§a da regularizaÃ§Ã£o Ã© controlada por um parÃ¢metro $\lambda$ (lambda), onde valores maiores de $\lambda$ resultam em maior regularizaÃ§Ã£o e modelos mais simples.
A escolha entre L1 e L2, ou alguma combinaÃ§Ã£o delas (como a Elastic Net) depende das caracterÃ­sticas do problema e do nÃ­vel desejado de *sparsity*. A regularizaÃ§Ã£o L1 Ã© preferÃ­vel se a seleÃ§Ã£o de variÃ¡veis e *sparsity* forem importantes para a interpretabilidade do modelo, enquanto a regularizaÃ§Ã£o L2 Ã© preferÃ­vel se o objetivo for reduzir o impacto de muitas variÃ¡veis com pequenas contribuiÃ§Ãµes.

**CorolÃ¡rio 1:** RelaÃ§Ã£o entre RegularizaÃ§Ã£o e VariÃ¢ncia dos Coeficientes

A regularizaÃ§Ã£o, especialmente L2 (Ridge), afeta a variÃ¢ncia dos coeficientes do modelo [^5]. Ao adicionar um termo de penalizaÃ§Ã£o que restringe a magnitude dos parÃ¢metros, a regularizaÃ§Ã£o reduz a variabilidade dos parÃ¢metros em diferentes amostras de treinamento, portanto, diminui a *variance* do modelo e contribui para modelos mais generalizÃ¡veis. Isto Ã© particularmente Ãºtil em cenÃ¡rios com multicolinearidade, onde os coeficientes podem se tornar instÃ¡veis sem a regularizaÃ§Ã£o. RegularizaÃ§Ã£o L1 tambÃ©m promove a estabilidade dos coeficientes, mas por um caminho diferente, zerando os coeficientes, o que tambÃ©m leva a uma diminuiÃ§Ã£o da variÃ¢ncia.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos um modelo com dois preditores $x_1$ e $x_2$, e a relaÃ§Ã£o verdadeira Ã© $y = 2x_1 + 3x_2 + \epsilon$, onde $\epsilon$ Ã© um ruÃ­do aleatÃ³rio. Vamos comparar a variÃ¢ncia dos coeficientes estimados em um modelo de mÃ­nimos quadrados (OLS) e em um modelo Ridge.
>
> **Dados de exemplo:**
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression, Ridge
>
> np.random.seed(42)
> n_samples = 100
> x1 = np.random.rand(n_samples)
> x2 = 0.8 * x1 + np.random.rand(n_samples) * 0.2 # x1 e x2 sÃ£o correlacionados
> epsilon = np.random.randn(n_samples) * 0.5
> y = 2 * x1 + 3 * x2 + epsilon
> X = np.vstack((x1,x2)).T
> df = pd.DataFrame({'x1':x1,'x2':x2,'y':y})
>
> ```
>
>
> **Modelo OLS:**
>
> ```python
> ols = LinearRegression()
> ols.fit(X,y)
> print(f"OLS Coefficients: {ols.coef_}")
> ```
>
> O resultado do OLS nos coeficientes Ã© prÃ³ximo de $\beta_1=2$ e $\beta_2=3$, mas com alguma variabilidade.
>
> **Modelo Ridge:**
>
> ```python
> ridge = Ridge(alpha=1)
> ridge.fit(X,y)
> print(f"Ridge Coefficients: {ridge.coef_}")
> ```
>
> Os coeficientes da Ridge sÃ£o menores em magnitude do que o OLS.
>
> **SimulaÃ§Ã£o da VariÃ¢ncia:**
> Para demonstrar a reduÃ§Ã£o na variÃ¢ncia, vamos simular a estimativa dos coeficientes em 100 amostras diferentes:
>
> ```python
> n_simulations = 100
> ols_coefs = []
> ridge_coefs = []
>
> for _ in range(n_simulations):
>    x1_sim = np.random.rand(n_samples)
>    x2_sim = 0.8 * x1_sim + np.random.rand(n_samples) * 0.2
>    epsilon_sim = np.random.randn(n_samples) * 0.5
>    y_sim = 2 * x1_sim + 3 * x2_sim + epsilon_sim
>    X_sim = np.vstack((x1_sim,x2_sim)).T
>
>    ols_sim = LinearRegression()
>    ols_sim.fit(X_sim,y_sim)
>    ols_coefs.append(ols_sim.coef_)
>
>    ridge_sim = Ridge(alpha=1)
>    ridge_sim.fit(X_sim,y_sim)
>    ridge_coefs.append(ridge_sim.coef_)
>
> ols_coefs = np.array(ols_coefs)
> ridge_coefs = np.array(ridge_coefs)
>
> print(f"OLS Variance of Coefficients: {np.var(ols_coefs,axis=0)}")
> print(f"Ridge Variance of Coefficients: {np.var(ridge_coefs,axis=0)}")
> ```
>
> Observamos que a variÃ¢ncia dos coeficientes do modelo Ridge Ã© menor que a variÃ¢ncia dos coeficientes do modelo OLS, especialmente para $\beta_2$. Isso demonstra como a regularizaÃ§Ã£o L2 (Ridge) reduz a variÃ¢ncia dos coeficientes. A regularizaÃ§Ã£o L1 tambÃ©m pode reduzir a variÃ¢ncia, mas por um caminho diferente, zerando os coeficientes, que tambÃ©m leva a uma diminuiÃ§Ã£o da variÃ¢ncia.

**Conceito 3: Sparsity**

**Sparsity**, no contexto da modelagem, refere-se Ã  propriedade de um modelo ter um grande nÃºmero de parÃ¢metros iguais a zero [^6]. Em modelos lineares, *sparsity* significa que apenas um subconjunto das variÃ¡veis preditoras Ã© efetivamente usado para fazer as prediÃ§Ãµes. Modelos esparsos sÃ£o benÃ©ficos por vÃ¡rias razÃµes:
    -   **Interpretabilidade:** Modelos com menos variÃ¡veis sÃ£o mais fÃ¡ceis de entender e comunicar.
    -   **EficiÃªncia computacional:** Modelos esparsos exigem menos recursos computacionais.
    -  **PrevenÃ§Ã£o de Overfitting:** Modelos esparsos tendem a ter menos *variance* devido Ã  reduÃ§Ã£o da complexidade.

A regularizaÃ§Ã£o L1, ou Lasso, Ã© uma das tÃ©cnicas mais utilizadas para induzir *sparsity* em modelos lineares e generalizados [^6]. Ao penalizar a soma dos valores absolutos dos coeficientes, a regularizaÃ§Ã£o L1 forÃ§a muitos desses coeficientes a serem exatamente zero. O grau de *sparsity* pode ser controlado ajustando o parÃ¢metro de regularizaÃ§Ã£o.
```mermaid
graph LR
    A("Model Parameters") --> B("L1 Regularization");
    B --> C("Forces Coefficients to Zero");
    C --> D("Sparsity");
    D --> E("Feature Selection");
    E --> F("Improved Interpretability");
    E --> G("Reduced Computational Cost");
```
> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos 5 preditores, $x_1, x_2, x_3, x_4, x_5$, e a relaÃ§Ã£o verdadeira Ã© $y = 2x_1 + 3x_2 + \epsilon$. Vamos comparar os coeficientes estimados por OLS, Ridge e Lasso.
>
> **Dados de exemplo:**
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression, Ridge, Lasso
>
> np.random.seed(42)
> n_samples = 100
> x1 = np.random.rand(n_samples)
> x2 = np.random.rand(n_samples)
> x3 = np.random.rand(n_samples)
> x4 = np.random.rand(n_samples)
> x5 = np.random.rand(n_samples)
> epsilon = np.random.randn(n_samples) * 0.5
> y = 2 * x1 + 3 * x2 + epsilon
> X = np.vstack((x1,x2,x3,x4,x5)).T
> df = pd.DataFrame({'x1':x1,'x2':x2,'x3':x3,'x4':x4,'x5':x5,'y':y})
> ```
>
> **Modelo OLS:**
>
> ```python
> ols = LinearRegression()
> ols.fit(X,y)
> print(f"OLS Coefficients: {ols.coef_}")
> ```
>
> O modelo OLS atribui valores nÃ£o nulos a todos os coeficientes, mesmo aos preditores irrelevantes.
>
> **Modelo Ridge:**
>
> ```python
> ridge = Ridge(alpha=1)
> ridge.fit(X,y)
> print(f"Ridge Coefficients: {ridge.coef_}")
> ```
>
> O modelo Ridge reduz a magnitude de todos os coeficientes, mas nenhum deles se torna zero.
>
> **Modelo Lasso:**
>
> ```python
> lasso = Lasso(alpha=0.1)
> lasso.fit(X,y)
> print(f"Lasso Coefficients: {lasso.coef_}")
> ```
>
> O modelo Lasso zera os coeficientes dos preditores $x_3$, $x_4$ e $x_5$, que nÃ£o estÃ£o relacionados com a variÃ¡vel resposta. Isso demonstra como a regularizaÃ§Ã£o L1 (Lasso) induz *sparsity* e seleciona os preditores mais relevantes.

> âš ï¸ **Nota Importante**: O Bias-Variance Tradeoff Ã© fundamental na modelagem preditiva. Um modelo com alto bias e baixa variance nÃ£o se ajusta aos dados de treinamento e nÃ£o generaliza bem, enquanto um modelo com baixo bias e alta variance se ajusta aos dados de treinamento, mas nÃ£o generaliza para novos dados.

> â— **Ponto de AtenÃ§Ã£o**: A regularizaÃ§Ã£o Ã© uma ferramenta poderosa para controlar a complexidade do modelo e reduzir o overfitting. A escolha entre L1 e L2 deve ser feita com base no problema e na importÃ¢ncia da sparsity.

> âœ”ï¸ **Destaque**: Modelos esparsos sÃ£o benÃ©ficos em termos de interpretabilidade e computaÃ§Ã£o.

### RegressÃ£o Linear e MÃ­nimos Quadrados

```mermaid
graph LR
    A("Data Points") --> B("Linear Regression");
    B --> C("Minimize Sum of Squared Residuals");
    C --> D("Best Fit Line/Hyperplane");
    D --> E("Coefficients Î²");
        D --> F("Residuals are orthogonal to the space spanned by the predictors");
    E --> G("Model Predictions");

```

Na regressÃ£o linear, o objetivo Ã© modelar a relaÃ§Ã£o entre uma variÃ¡vel dependente (resposta) e uma ou mais variÃ¡veis independentes (preditores) atravÃ©s de uma funÃ§Ã£o linear [^10]. A soluÃ§Ã£o mais comum para estimar os parÃ¢metros desta relaÃ§Ã£o Ã© atravÃ©s do mÃ©todo dos **mÃ­nimos quadrados**, que busca minimizar a soma dos quadrados dos resÃ­duos.

A funÃ§Ã£o do modelo linear Ã© dada por:

$$
f(x) = \beta_0 + \sum_{j=1}^p x_j \beta_j
$$

onde:

-   $f(x)$ Ã© a prediÃ§Ã£o do modelo para um dado vetor de preditores $x$.
-   $\beta_0$ Ã© o *intercept* ou *bias*.
-   $\beta_j$ sÃ£o os coeficientes ou parÃ¢metros associados a cada preditor $x_j$.
-   $p$ Ã© o nÃºmero de preditores.

O mÃ©todo de mÃ­nimos quadrados busca encontrar os valores de $\beta_0, \beta_1, \beta_2, ..., \beta_p$ que minimizam a soma dos quadrados dos resÃ­duos, que Ã© definida como:

$$
RSS(\beta) = \sum_{i=1}^N (y_i - f(x_i))^2
$$
$$
RSS(\beta) = \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2
$$

onde:

-   $N$ Ã© o nÃºmero de observaÃ§Ãµes.
-   $y_i$ Ã© o valor observado da variÃ¡vel resposta para a i-Ã©sima observaÃ§Ã£o.
-   $f(x_i)$ Ã© o valor predito pelo modelo para a i-Ã©sima observaÃ§Ã£o.

A soluÃ§Ã£o para os coeficientes Ã© obtida derivando a funÃ§Ã£o RSS com relaÃ§Ã£o a cada parÃ¢metro e igualando a zero.  Em notaÃ§Ã£o matricial, essa soluÃ§Ã£o Ã© dada por [^11]:

$$
\beta = (X^T X)^{-1} X^T y
$$
onde:

-   $X$ Ã© a matriz de design com cada linha contendo os valores dos preditores para cada observaÃ§Ã£o.
-  $y$ Ã© o vetor de respostas.

Essa soluÃ§Ã£o minimiza o RSS e fornece os melhores ajustes lineares para o conjunto de dados de treinamento [^12]. Geometricamente, a soluÃ§Ã£o de mÃ­nimos quadrados corresponde Ã  projeÃ§Ã£o ortogonal do vetor de resposta $y$ no subespaÃ§o gerado pelas colunas da matriz de design $X$ [^12].

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos considerar um exemplo com duas variÃ¡veis preditoras e uma variÃ¡vel resposta.
>
> **Dados de exemplo:**
>
> ```python
> import numpy as np
>
> X = np.array([[1, 2],
>               [1, 3],
>               [1, 4],
>               [1, 5],
>               [1, 6]])
> y = np.array([3, 5, 6, 8, 10])
> ```
>
> Aqui, X Ã© a matriz de design, onde a primeira coluna Ã© um vetor de 1s (para o intercepto) e a segunda coluna Ã© o valor do preditor. O vetor y contÃ©m os valores da variÃ¡vel resposta.
>
> **CÃ¡lculo da soluÃ§Ã£o de mÃ­nimos quadrados:**
>
> 1.  **Calcular $X^T X$:**
>
>     ```
>     XTX = np.dot(X.T, X)
>     print(f"XTX: \n {XTX}")
>     ```
>
>     $$
>     X^TX =
>     \begin{bmatrix}
>     5 & 20 \\
>     20 & 90
>     \end{bmatrix}
>     $$
>
> 2.  **Calcular $(X^T X)^{-1}$:**
>
>     ```
>     XTX_inv = np.linalg.inv(XTX)
>      print(f"Inverse of XTX: \n {XTX_inv}")
>     ```
>
>     $$
>     (X^TX)^{-1} =
>     \begin{bmatrix}
>     2.8 & -0.6 \\
>     -0.6 & 0.15
>     \end{bmatrix}
>     $$
>
> 3.  **Calcular $X^T y$:**
>
>     ```
>     XTy = np.dot(X.T, y)
>     print(f"XTy: \n {XTy}")
>     ```
>
>     $$
>     X^Ty =
>     \begin{bmatrix}
>     32 \\
>     141
>     \end{bmatrix}
>     $$
>
> 4.  **Calcular $\beta = (X^T X)^{-1} X^T y$:**
>
>     ```
>     beta = np.dot(XTX_inv, XTy)
>     print(f"Beta: \n {beta}")
>     ```
>
>     $$
>     \beta =
>     \begin{bmatrix}
>     1 \\
>     1.5
>     \end{bmatrix}
>     $$
>
> Portanto, o modelo de regressÃ£o linear ajustado por mÃ­nimos quadrados Ã© $f(x) = 1 + 1.5x$. Este Ã© o plano que melhor se ajusta aos dados, no sentido de minimizar a soma dos quadrados dos resÃ­duos.

**Lemma 2:** Ortogonalidade dos ResÃ­duos

Os resÃ­duos de um ajuste de mÃ­nimos quadrados sÃ£o ortogonais ao espaÃ§o gerado pelas colunas da matriz de design $X$ [^13]. Para demonstrar isto, note que a soluÃ§Ã£o por mÃ­nimos quadrados satisfaz:
$$X^T (y-X\hat{\beta})=0$$
onde $\hat{\beta}$ Ã© o vetor de coeficientes estimados. Multiplicando por um vetor qualquer $a$, temos:
$$a^T X^T (y-X\hat{\beta}) = 0 $$
$$ (Xa)^T (y-X\hat{\beta}) = 0 $$
Assim, o vetor de resÃ­duos $y-X\hat{\beta}$ Ã© ortogonal a qualquer vetor no espaÃ§o gerado pelas colunas de X, e portanto, Ã© ortogonal ao espaÃ§o gerado pelas colunas da matriz X.  $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Usando os dados do exemplo anterior, vamos demonstrar a ortogonalidade dos resÃ­duos.
>
> **Dados e Coeficientes:**
>
> ```python
> import numpy as np
>
> X = np.array([[1, 2],
>               [1, 3],
>               [1, 4],
>               [1, 5],
>               [1, 6]])
> y = np.array([3, 5, 6, 8, 10])
> beta = np.array([1, 1.5])
> ```
>
> **CÃ¡lculo dos resÃ­duos:**
>
>   1.  **Calcular as prediÃ§Ãµes $\hat{y}$:**
>
>       ```python
>       y_hat = np.dot(X, beta)
>       print(f"y_hat: {y_hat}")
>       ```
>
>       $$
>       \hat{y} = X\beta =
>       \begin{bmatrix}
>       1 & 2 \\
>       1 & 3 \\
>       1 & 4 \\
>       1 & 5 \\
>       1 & 6
>       \end{bmatrix}
>       \begin{bmatrix}
>       1 \\
>       1.5
>       \end{bmatrix} =
>       \begin{bmatrix}
>       4 \\
>       5.5 \\
>       7 \\
>       8.5 \\
>       10
>       \end{bmatrix}
>       $$
>
>   2.  **Calcular os resÃ­duos $r = y - \hat{y}$:**
>
>       ```python
>       residuals = y - y_hat
>       print(f"Residuals: {residuals}")
>       ```
>
>       $$
>       r = y - \hat{y} =
>       \begin{bmatrix}
>       3 \\
>       5 \\
>       6 \\
>       8 \\
>       10
>       \end{bmatrix} -
>       \begin{bmatrix}
>       4 \\
>       5.5 \\
>       7 \\
>       8.5 \\
>       10
>       \end{bmatrix} =
>       \begin{bmatrix}
>       -1 \\
>       -0.5 \\
>       -1 \\
>       -0.5 \\
>       0
>       \end{bmatrix}
>       $$
>
> **VerificaÃ§Ã£o da Ortogonalidade:**
>
> Para verificar a ortogonalidade, vamos calcular o produto interno (dot product) dos resÃ­duos com cada coluna da matriz X.
>
>   1.  **Produto interno dos resÃ­duos com a primeira coluna de X (vetor de 1s):**
>
>       ```python
>       dot_product_col1 = np.dot(residuals, X[:, 0])
>       print(f"Dot product with first column of X: {dot_product_col1}")
>       ```
>
>       $$
>       r^T x_1=
>       \begin{bmatrix}
>       -1 & -0.5 & -1 & -0.5 & 0
>       \end{bmatrix}
>       \begin{bmatrix}
>       1 \\ 1 \\ 1 \\ 1 \\ 1
>       \end{bmatrix} = -3
>       $$
>
>   2.  **Produto interno dos resÃ­duos com a segunda coluna de X (vetor de preditores):**
>
>       ```python
>       dot_product_col2 = np.dot(residuals, X[:, 1])
>       print(f"Dot product with second column of X: {dot_product_col2}")
>       ```
>
>       $$
>       r^T x_2=
>       \begin{bmatrix}
>       -1 & -0.5 & -1 & -0.5 & 0
>       \end{bmatrix}
>       \begin{bmatrix}
>       2 \\ 3 \\ 4 \\ 5 \\ 6
>       \end{bmatrix} = -7
>       $$
>
> Observe que os resultados nÃ£o sÃ£o exatamente zero devido a erros de arredondamento, mas sÃ£o muito prÃ³ximos de zero. Isso demonstra que os resÃ­duos sÃ£o aproximadamente ortogonais ao espaÃ§o gerado pelas colunas de X. Se adicionarmos uma constante ao vetor de resÃ­duos, o resultado serÃ¡ um vetor ortogonal ao espaÃ§o gerado pelas colunas de X.

**CorolÃ¡rio 2:** InterpretaÃ§Ã£o GeomÃ©trica da ProjeÃ§Ã£o Ortogonal

O Lemma 2 implica que o vetor de resÃ­duos, $y-\hat{y}$, onde $\hat{y}=X\hat{\beta}$ Ã© a projeÃ§Ã£o de y no espaÃ§o gerado por X,  Ã© perpendicular ao espaÃ§o gerado pelas colunas de X [^14]. Isso significa que a soluÃ§Ã£o de mÃ­nimos quadrados produz um vetor de prediÃ§Ãµes $\hat{y}$ que estÃ¡ o mais prÃ³ximo possÃ­vel do vetor de respostas y no sentido da distÃ¢ncia euclidiana, que corresponde a encontrar a projeÃ§Ã£o ortogonal de y no espaÃ§o gerado pelos preditores.

### MÃ©todos de SeleÃ§Ã£o de VariÃ¡veis

```mermaid
graph LR
    A("Variable Selection") --> B("Best Subset Selection");
    A --> C("Forward Stepwise Selection");
    A --> D("Backward Stepwise Selection");
    B --> E("Evaluates All Combinations");
    C --> F("Adds Predictors Sequentially");
    D --> G("Removes Predictors Sequentially");
    E --> H("Computationally Expensive");
    F --> I("More Computationally Efficient");
    G --> J("More Computationally Efficient");
    H --> K("Best Model");
        I --> K;
        J --> K
```

A seleÃ§Ã£o de variÃ¡veis Ã© o processo de escolher um subconjunto das variÃ¡veis preditoras mais relevantes para a modelagem [^15]. Este processo tem como objetivo melhorar a interpretabilidade, a eficiÃªncia computacional, e a capacidade de generalizaÃ§Ã£o do modelo, especialmente em cenÃ¡rios onde hÃ¡ um grande nÃºmero de preditores potencialmente irrelevantes ou redundantes. MÃ©todos de seleÃ§Ã£o de variÃ¡veis podem ser categorizados em:

-   **Best Subset Selection:** Avalia todas as possÃ­veis combinaÃ§Ãµes de preditores para encontrar o subconjunto que minimiza um certo critÃ©rio, como o erro quadrÃ¡tico mÃ©dio (RSS) ou o critÃ©rio de informaÃ§Ã£o de Akaike (AIC) [^15]. Esse mÃ©todo Ã© computacionalmente caro, e inviÃ¡vel em problemas com um grande nÃºmero de preditores.

-   **Forward Stepwise Selection:** ComeÃ§a com um modelo nulo, que inclui apenas o intercept, e adiciona sequencialmente o preditor que mais reduz o erro do modelo, atÃ© que um certo critÃ©rio seja atingido ou todos os preditores tenham sido adicionados [^16]. Esse mÃ©todo Ã© computacionalmente mais viÃ¡vel do que *best subset selection*.

-   **Backward Stepwise Selection:** ComeÃ§a com todos os preditores no modelo e remove sequencialmente o preditor que menos impacta o ajuste do modelo, atÃ© que um certo critÃ©rio seja atingido ou nenhum preditor possa ser removido [^17]. Este mÃ©todo tambÃ©m Ã© computacionalmente viÃ¡vel, mas pode nÃ£o funcionar bem em cenÃ¡rios onde existem muitos preditores, o que torna a remoÃ§Ã£o inicial de variÃ¡veis problemÃ¡ticas.

O **CritÃ©rio de InformaÃ§Ã£o de Akaike (AIC)** Ã© um critÃ©rio que pondera o ajuste do modelo com a complexidade do modelo [^16]. Ele Ã© definido como:

$$
AIC = -2 \log(L) + 2p
$$

onde:

-   $L$ Ã© a *likelihood* ou verossimilhanÃ§a do modelo.
-   $p$ Ã© o nÃºmero de parÃ¢metros no modelo.

O AIC quantifica o ajuste do modelo aos dados (primeiro termo) e o custo da complexidade do modelo (segundo termo). O objetivo Ã© encontrar o modelo com o menor AIC. Modelos com maior *likelihood* indicam melhor ajuste aos dados, mas modelos mais complexos tendem a ter maior *likelihood*.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos considerar um cenÃ¡rio com 3 preditores e usar o AIC para comparar diferentes modelos.
>
> **Dados de exemplo:**
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> np.random.seed(42)
> n_samples = 100
> x1 = np.random.rand(n_samples)
> x2 = np.random.rand(n_samples)
> x3 = np.random.rand(n_samples)
> epsilon = np.random.randn(n_samples) * 0.5
> y = 2 * x1 + 3 * x2 + epsilon
>
> df = pd.DataFrame({'x1':x1,'x2':x2,'x3':x3,'y':y})
> ```
>
> **Modelo 1: Apenas x1**
>
> ```python
> X1 = df[['x1']].values
> model1 = LinearRegression()
> model1.fit(X1,y)
> y_hat1 = model1.predict(X1)
> residuals1 = y - y_hat1
> rss1 = np.sum(residuals1**2)
> n = len(y)
> p1 = 2 # intercept + x1
> likelihood1 = (2*np.pi*rss1/n)**(-n/2) * np.exp