## Bias-Variance Tradeoff in Linear Regression Models

```mermaid
graph LR
    A("Model Complexity") -->|High| B("High Variance")
    A -->|Low| C("High Bias")
    B --> D("Overfitting")
    C --> E("Underfitting")
    D --> F("Poor Generalization")
    E --> F
    F --> G("High Error")
    A --> H("Optimal Complexity")
    H --> I("Balanced Bias & Variance")
    I --> J("Good Generalization")
    J --> K("Low Error")
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

A modelagem estat√≠stica, especialmente no contexto da regress√£o linear e aprendizado de m√°quina, busca construir modelos que generalizem bem para dados n√£o vistos, ou seja, que consigam fazer previs√µes precisas com dados que n√£o foram usados no seu treinamento [^1]. No entanto, essa busca enfrenta um desafio fundamental conhecido como **Bias-Variance Tradeoff**, que dita a necessidade de balancear a complexidade do modelo para evitar problemas de subajuste (*underfitting*) e sobreajuste (*overfitting*) [^1].

Este cap√≠tulo explora detalhadamente esse tradeoff, crucial para a constru√ß√£o de modelos robustos e confi√°veis em diversas aplica√ß√µes, incluindo as finan√ßas quantitativas. Compreender esse conceito e como ele se manifesta √© essencial para escolher modelos de regress√£o que otimizem tanto a capacidade de ajuste aos dados de treinamento quanto a habilidade de generaliza√ß√£o para novos dados.

### Conceitos Fundamentais

√â importante compreender alguns conceitos fundamentais antes de nos aprofundarmos no Bias-Variance Tradeoff.

**Conceito 1: Bias-Variance Tradeoff**

O **Bias** de um modelo refere-se √† diferen√ßa entre a previs√£o m√©dia do modelo e o valor verdadeiro que estamos tentando prever [^2]. Um modelo com *alto bias* faz suposi√ß√µes fortes sobre os dados, levando a um subajuste (*underfitting*). Este tipo de modelo ignora padr√µes relevantes nos dados de treinamento, resultando em previs√µes imprecisas tanto para dados de treinamento quanto para dados n√£o vistos. Modelos com alto bias s√£o normalmente mais simples e menos flex√≠veis [^2].

A **Vari√¢ncia** de um modelo, por outro lado, refere-se √† sensibilidade do modelo a pequenas mudan√ßas nos dados de treinamento [^2]. Um modelo com *alta vari√¢ncia* √© altamente flex√≠vel, se ajustando muito bem aos dados de treinamento. Entretanto, este tipo de modelo tamb√©m se ajusta ao ru√≠do aleat√≥rio ou peculiaridades nos dados de treinamento, levando a sobreajuste (*overfitting*). Um modelo sobreajustado tende a generalizar mal para dados n√£o vistos, pois a sua alta sensibilidade aos dados de treinamento causa previs√µes imprecisas [^2].

> ‚ö†Ô∏è **Nota Importante**: O *Bias-Variance Tradeoff* √© a busca por um equil√≠brio ideal entre a simplicidade (baixo bias) e a flexibilidade (baixa vari√¢ncia) de um modelo, para obter o melhor desempenho em dados n√£o vistos.

**Lemma 1: Decomposi√ß√£o do Erro Quadr√°tico M√©dio (MSE)**

Para entender melhor o tradeoff, podemos decompor o Erro Quadr√°tico M√©dio (MSE) de um modelo em seus componentes de bias e vari√¢ncia [^3]. O MSE √© uma medida comum para avaliar a performance de modelos de regress√£o.

Formalmente, o MSE pode ser decomposto como:
$$
MSE(x) = E[(f(x) - Y)^2] = (E[f(x)] - Y)^2 + E[(f(x)-E[f(x)])^2]
$$
Onde:
*   $f(x)$ √© a previs√£o do modelo para uma dada entrada $x$.
*   $Y$ √© o valor verdadeiro correspondente.
*  $E[f(x)]$ representa a expectativa da previs√£o, ou seja, a m√©dia das previs√µes do modelo ao longo de v√°rios treinamentos com diferentes conjuntos de dados.
*   $(E[f(x)]-Y)^2$ √© o bias quadr√°tico.
*    $E[(f(x)-E[f(x)])^2]$ √© a vari√¢ncia.
*    $MSE(x)$ √© o erro quadrado m√©dio da predi√ß√£o, calculado como a m√©dia dos erros quadrados das predi√ß√µes do modelo.

Essa equa√ß√£o revela que o erro total ($MSE$) √© composto por duas partes: o bias quadr√°tico, que mede o qu√£o longe a previs√£o m√©dia do modelo est√° do valor verdadeiro, e a vari√¢ncia, que mede o qu√£o sens√≠vel o modelo √© √†s varia√ß√µes nos dados de treinamento [^3].

**Demonstra√ß√£o:**
$$
MSE(x) = E[(f(x) - Y)^2] = E[(f(x) - E[f(x)] + E[f(x)] - Y)^2]
$$
$$
= E[(f(x) - E[f(x)])^2 + 2(f(x) - E[f(x)])(E[f(x)] - Y) + (E[f(x)] - Y)^2]
$$
$$
= E[(f(x) - E[f(x)])^2] + 2(E[f(x)] - E[f(x)])(E[f(x)]-Y)+(E[f(x)]-Y)^2
$$
$$
= E[(f(x)-E[f(x)])^2] + (E[f(x)]-Y)^2.
$$
$\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que temos um modelo de regress√£o linear que tenta prever o pre√ßo de casas ($Y$) com base no tamanho em metros quadrados ($x$). O valor verdadeiro de uma casa √© $Y = 500$ (em milhares de reais).
>
> Modelo 1 (Simples): $f_1(x) = 200 + 0.5x$
> Modelo 2 (Complexo): $f_2(x) = 100 + 0.8x + 0.001x^2$
>
> Vamos supor que temos 10 conjuntos de dados de treinamento ligeiramente diferentes. Ap√≥s treinar os dois modelos com esses conjuntos, obtemos as seguintes previs√µes (para o mesmo valor de $x$):
>
> | Treino | $f_1(x)$ | $f_2(x)$ |
> |--------|---------|---------|
> | 1      | 480     | 450     |
> | 2      | 490     | 520     |
> | 3      | 470     | 550     |
> | 4      | 500     | 480     |
> | 5      | 485     | 510     |
> | 6      | 495     | 460     |
> | 7      | 475     | 530     |
> | 8      | 505     | 490     |
> | 9      | 488     | 540     |
> | 10     | 492     | 470     |
>
> Calculando as m√©dias e as vari√¢ncias:
>
> $E[f_1(x)] = \frac{480+490+470+500+485+495+475+505+488+492}{10} = 487$
> $E[f_2(x)] = \frac{450+520+550+480+510+460+530+490+540+470}{10} = 500$
>
> $Bias^2(f_1(x)) = (487 - 500)^2 = 169$
> $Bias^2(f_2(x)) = (500 - 500)^2 = 0$
>
> $Var(f_1(x)) = \frac{(480-487)^2 + (490-487)^2 + \ldots + (492-487)^2}{10}  \approx 107$
> $Var(f_2(x)) = \frac{(450-500)^2 + (520-500)^2 + \ldots + (470-500)^2}{10} \approx 920$
>
> $MSE(f_1(x)) = 169 + 107 = 276$
> $MSE(f_2(x)) = 0 + 920 = 920$
>
> O Modelo 1 tem um bias maior, mas uma vari√¢ncia menor, enquanto o Modelo 2 tem um bias menor, mas uma vari√¢ncia muito maior. O MSE do Modelo 1 √© menor, indicando melhor desempenho neste cen√°rio, apesar do bias. Este exemplo ilustra como modelos mais complexos podem ter baixa bias, mas alta vari√¢ncia, e como o MSE captura esse tradeoff.

**Conceito 2: Regulariza√ß√£o**

A **Regulariza√ß√£o** √© um conjunto de t√©cnicas que adicionam restri√ß√µes √† complexidade de um modelo, com o objetivo de controlar a sua vari√¢ncia, e assim, evitar sobreajuste [^4]. As t√©cnicas de regulariza√ß√£o introduzem uma penalidade na fun√ß√£o objetivo que aumenta com a complexidade do modelo (por exemplo, a magnitude dos seus coeficientes), encorajando modelos mais simples, por√©m mais generaliz√°veis [^4].
Dois tipos comuns de regulariza√ß√£o s√£o:
*  **L1 (Lasso):** adiciona uma penalidade proporcional √† soma dos valores absolutos dos coeficientes do modelo. Isso promove a *sparsity*, ou seja, alguns coeficientes s√£o levados a exatamente zero, efetivamente selecionando vari√°veis.
*   **L2 (Ridge):** adiciona uma penalidade proporcional √† soma dos quadrados dos coeficientes. Isso reduz a magnitude de todos os coeficientes, o que leva a modelos com menor vari√¢ncia, por√©m n√£o leva coeficientes a exatamente zero.

> ‚ùó **Ponto de Aten√ß√£o**: A regulariza√ß√£o √© uma ferramenta poderosa para controlar a complexidade de um modelo e mitigar a vari√¢ncia, mas a escolha adequada do tipo e da intensidade de regulariza√ß√£o (o par√¢metro lambda na equa√ß√£o) √© crucial para um bom desempenho [^4].

**Corol√°rio 1:**  A regulariza√ß√£o, por meio das penalidades L1 e L2, tem o efeito de reduzir a vari√¢ncia dos coeficientes do modelo. Ao penalizar coeficientes com maiores magnitudes, a regulariza√ß√£o for√ßa os coeficientes a encolher em dire√ß√£o a zero, tornando o modelo menos sens√≠vel √†s varia√ß√µes aleat√≥rias nos dados de treinamento. A penalidade L1 tende a criar um modelo mais esparso, enquanto a penalidade L2 cria um modelo com coeficientes menores.

**Conceito 3: Sparsity**

A **Sparsity**, no contexto de modelos de regress√£o, refere-se √† condi√ß√£o em que muitos coeficientes do modelo s√£o iguais a zero [^6]. Modelos esparsos s√£o mais f√°ceis de interpretar, pois apenas as vari√°veis mais relevantes s√£o selecionadas. A regulariza√ß√£o L1 (Lasso) √© particularmente eficaz para promover a sparsity, levando a modelos mais simples e mais interpret√°veis [^6].
*   Modelos esparsos facilitam a identifica√ß√£o de vari√°veis importantes, o que √© essencial para gerar *insights* e interpretar rela√ß√µes causais.
*   Al√©m disso, modelos esparsos reduzem o risco de *overfitting*, pois a sele√ß√£o de vari√°veis leva a uma redu√ß√£o da vari√¢ncia do modelo.
*    A sparsity tamb√©m pode melhorar a efici√™ncia computacional, uma vez que a avalia√ß√£o do modelo pode ser executada com menos vari√°veis.

> ‚úîÔ∏è **Destaque**: A sparsity, obtida atrav√©s de m√©todos como o Lasso, melhora tanto a interpretabilidade quanto a capacidade de generaliza√ß√£o de um modelo, estabelecendo um balan√ßo mais adequado entre bias e vari√¢ncia [^6].

### Regress√£o Linear e M√≠nimos Quadrados

```mermaid
graph LR
    A["Data Points (x, y)"] --> B["Linear Regression Model"]
    B --> C["Predicted Values (f(x))"]
    A --> D["Observed Values (y)"]
    C --> E["Residuals (y - f(x))"]
    D --> E
    E --> F["Minimize Sum of Squared Residuals (RSS)"]
    F --> G["Optimal Coefficients (Œ≤)"]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

A **Regress√£o Linear** √© um modelo estat√≠stico que assume que a rela√ß√£o entre a vari√°vel dependente (ou resposta) e as vari√°veis independentes (ou preditoras) pode ser aproximada por uma fun√ß√£o linear [^10]. O objetivo do modelo √© encontrar os coeficientes que melhor ajustam a combina√ß√£o linear das vari√°veis preditoras aos dados de resposta.

Em nota√ß√£o matem√°tica, um modelo linear tem a forma:

$$
f(x) = \beta_0 + \sum_{j=1}^{p} x_j \beta_j
$$

Onde:
*   $f(x)$ √© a previs√£o do modelo para uma dada entrada $x$
*  $\beta_0$ √© o intercepto do modelo
*  $x_j$ √© a j-√©sima vari√°vel preditora
*  $\beta_j$ √© o coeficiente associado √† j-√©sima vari√°vel preditora
*  $p$ √© o n√∫mero de vari√°veis preditoras.

A estima√ß√£o dos coeficientes ($Œ≤$) √© comumente realizada atrav√©s do m√©todo dos **M√≠nimos Quadrados** (*Least Squares*). Este m√©todo procura minimizar a soma dos quadrados dos res√≠duos (RSS), que representa a diferen√ßa entre os valores observados e os valores preditos pelo modelo [^10]:
$$
RSS(\beta) = \sum_{i=1}^{N} (y_i - f(x_i))^2
$$

Onde:
*   $y_i$ √© o valor observado da vari√°vel de resposta para a i-√©sima observa√ß√£o
*  $f(x_i)$ √© o valor predito da vari√°vel de resposta para a i-√©sima observa√ß√£o
*  $N$ √© o n√∫mero de observa√ß√µes.
A solu√ß√£o para a estima√ß√£o dos coeficientes $\beta$ que minimiza o RSS √© dada por:

$$
\hat{\beta} = (X^TX)^{-1}X^Ty
$$

Onde:
* $X$ √© a matriz de design, que cont√©m as vari√°veis preditoras e um vetor de 1s para o intercepto.
*  $y$ √© o vetor dos valores observados da vari√°vel de resposta.
   

Este m√©todo √© popular por sua simplicidade, efici√™ncia computacional e propriedades estat√≠sticas bem compreendidas [^10]. A solu√ß√£o de m√≠nimos quadrados encontra o melhor ajuste linear aos dados, no sentido de minimizar o erro quadr√°tico m√©dio.

No entanto, uma limita√ß√£o do m√©todo dos m√≠nimos quadrados √© sua sensibilidade a *outliers* (observa√ß√µes at√≠picas) nos dados [^14]. Em cen√°rios com dados com ru√≠do ou outliers, o m√©todo de m√≠nimos quadrados pode levar a modelos com vari√¢ncia excessiva, j√° que o objetivo √© ajustar o modelo a todos os pontos, inclusive os *outliers*. Este comportamento inadequado pode ser remediado ao aplicar t√©cnicas de regulariza√ß√£o.

**Lemma 2:** A solu√ß√£o de m√≠nimos quadrados ($\beta$) √© obtida quando o vetor res√≠duos $y-X\beta$ √© ortogonal ao espa√ßo coluna da matriz de design $X$ [^11]. Isto √©: $X^T(y-X\beta) = 0$, onde $y$ √© o vetor de respostas e $X$ √© a matriz de design.

**Demonstra√ß√£o:**
A fun√ß√£o objetivo a ser minimizada √© $RSS = ||y - X\beta||^2$. Tomando a derivada em rela√ß√£o a $\beta$ e igualando a zero temos:
$$
\frac{\partial RSS}{\partial \beta} = -2X^T(y-X\beta)=0
$$
Rearranjando, obtemos:
$$
X^T(y-X\beta)=0
$$
Este resultado demonstra que o vetor res√≠duo ($y - X\beta$) √© ortogonal ao espa√ßo coluna de $X$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simples com duas vari√°veis preditoras e uma vari√°vel resposta. Temos os seguintes dados:
>
> | Observa√ß√£o | $x_1$ | $x_2$ | $y$  |
> |------------|-------|-------|------|
> | 1          | 1     | 2     | 5    |
> | 2          | 2     | 3     | 8    |
> | 3          | 3     | 4     | 11   |
> | 4          | 4     | 5     | 14   |
>
> A matriz de design $X$ e o vetor de resposta $y$ s√£o:
>
> $$
> X = \begin{bmatrix}
> 1 & 1 & 2 \\
> 1 & 2 & 3 \\
> 1 & 3 & 4 \\
> 1 & 4 & 5
> \end{bmatrix}, \quad
> y = \begin{bmatrix}
> 5 \\ 8 \\ 11 \\ 14
> \end{bmatrix}
> $$
>
> Primeiro, calculamos $X^TX$:
>
> $$
> X^TX = \begin{bmatrix}
> 1 & 1 & 1 & 1 \\
> 1 & 2 & 3 & 4 \\
> 2 & 3 & 4 & 5
> \end{bmatrix}
> \begin{bmatrix}
> 1 & 1 & 2 \\
> 1 & 2 & 3 \\
> 1 & 3 & 4 \\
> 1 & 4 & 5
> \end{bmatrix}
> = \begin{bmatrix}
> 4 & 10 & 14 \\
> 10 & 30 & 40 \\
> 14 & 40 & 54
> \end{bmatrix}
> $$
>
> Em seguida, calculamos $(X^TX)^{-1}$:
>
> $$
> (X^TX)^{-1} \approx \begin{bmatrix}
> 6.5 & -3 & 0.5 \\
> -3 & 1.75 & -0.25 \\
> 0.5 & -0.25 & 0.083
> \end{bmatrix}
> $$
>
> Agora, calculamos $X^Ty$:
>
> $$
> X^Ty = \begin{bmatrix}
> 1 & 1 & 1 & 1 \\
> 1 & 2 & 3 & 4 \\
> 2 & 3 & 4 & 5
> \end{bmatrix}
> \begin{bmatrix}
> 5 \\ 8 \\ 11 \\ 14
> \end{bmatrix} = \begin{bmatrix}
> 38 \\ 110 \\ 152
> \end{bmatrix}
> $$
>
> Finalmente, calculamos os coeficientes $\hat{\beta}$:
>
> $$
> \hat{\beta} = (X^TX)^{-1}X^Ty \approx \begin{bmatrix}
> 6.5 & -3 & 0.5 \\
> -3 & 1.75 & -0.25 \\
> 0.5 & -0.25 & 0.083
> \end{bmatrix}
> \begin{bmatrix}
> 38 \\ 110 \\ 152
> \end{bmatrix} = \begin{bmatrix}
> 2 \\ 1 \\ 1
> \end{bmatrix}
> $$
>
> Assim, o modelo de regress√£o linear estimado √©: $f(x) = 2 + 1x_1 + 1x_2$. Os coeficientes estimados s√£o $\hat{\beta_0} = 2$, $\hat{\beta_1} = 1$ e $\hat{\beta_2} = 1$.

**Corol√°rio 2:** A ortogonalidade do vetor res√≠duo em rela√ß√£o ao espa√ßo coluna da matriz de design simplifica a an√°lise da solu√ß√£o de m√≠nimos quadrados. O Lemma 2 nos permite facilmente obter os valores preditos a partir da proje√ß√£o ortogonal de $y$ sobre o espa√ßo coluna de $X$ [^12].

### M√©todos de Sele√ß√£o de Vari√°veis

```mermaid
<<<<<<< HEAD
graph TD
    A[Variable Selection Methods] --> B[Best Subset Selection];
    A --> C[Forward Selection];
    A --> D[Backward Selection];
    A --> E[Stepwise Selection];
    A --> F["Regularization (L1/L2)"];
     B --> G("Evaluates all Subsets");
      C --> H("Starts with null model, adds variables");
      D --> I("Starts with full model, removes variables");
      E --> J("Combines forward and backward steps");
      F --> K("Adds Penalty to minimize coefficients");

=======
graph LR
    A("Variable Selection") --> B("Information Criteria")
    B --> C("AIC")
    B --> D("BIC")
    A --> E("Subset Selection")
    E --> F("Best Subset Selection")
    A --> G("Stepwise Methods")
    G --> H("Forward Stepwise")
    G --> I("Backward Stepwise")
    style A fill:#f9f,stroke:#333,stroke-width:2px
>>>>>>> 25269480a9ab6a6f7189729296b07543a7078b7d
```

**Sele√ß√£o de Vari√°veis** √© o processo de identificar um subconjunto de vari√°veis preditoras que s√£o mais relevantes para o modelo, eliminando aquelas que s√£o redundantes ou que contribuem pouco para a performance do modelo. Este processo √© crucial para simplificar modelos, melhorar a interpretabilidade, e mitigar problemas como sobreajuste e instabilidade em modelos de regress√£o linear [^15].
Uma forma comum de realizar a sele√ß√£o de vari√°veis √© atrav√©s de crit√©rios de informa√ß√£o. Dois crit√©rios populares para sele√ß√£o de modelos s√£o:

*   **Akaike Information Criterion (AIC):** O AIC √© um crit√©rio para avaliar a qualidade de diferentes modelos estat√≠sticos, estimando o qu√£o bem um modelo se ajusta aos dados, penalizando a complexidade do modelo [^16]. O AIC √© definido como:

$$
AIC = -2\log(L) + 2p
$$

Onde $L$ √© a verossimilhan√ßa do modelo e $p$ √© o n√∫mero de par√¢metros. O termo $2p$ penaliza a complexidade do modelo, favorecendo modelos mais simples [^17]. Modelos com valores menores de AIC s√£o preferidos.
*   **Bayesian Information Criterion (BIC):** O BIC √© similar ao AIC, mas com uma penalidade mais forte para a complexidade do modelo [^17]. O BIC √© definido como:

$$
BIC = -2\log(L) + p\log(N)
$$

Onde $N$ √© o n√∫mero de observa√ß√µes. O BIC tende a selecionar modelos mais simples do que o AIC, especialmente com amostras de dados maiores, favorecendo modelos com menor vari√¢ncia.

Ambos os crit√©rios visam um equil√≠brio entre ajuste aos dados e complexidade do modelo [^17].

**Lemma 3:** O crit√©rio AIC √© uma estimativa do risco de predi√ß√£o de um modelo.

**Prova do Lemma 3:**

O risco de predi√ß√£o de um modelo $f_\theta$ √© definido como:
$$R(\theta)=E_X [E_{Y|X}  (Y - f_\theta(X))^2 ]$$

A partir dos resultados de Akaike (1973), sob certas condi√ß√µes (ver https://projecteuclid.org/journals/information-and-control/volume-16/issue-1/An-Information-Theoretic-Extension-of-the-Maximum-Likelihood-Principle/10.1016/S0019-9958(70)80005-4.full), e utilizando aproxima√ß√µes e argumentos assint√≥ticos, pode-se mostrar que:

$$-2\log(L) \approx R(\theta) + C$$

Onde C √© uma constante que n√£o depende do modelo. Assim, o AIC √© um estimador (enviesado) do risco de predi√ß√£o que √© corrigido pela penalidade de complexidade 2p. O termo de bias √© $2p$.

$\blacksquare$

**Corol√°rio 3:** O crit√©rio BIC, ao adicionar um termo de penalidade que cresce com o tamanho da amostra, tende a selecionar modelos mais simples do que o AIC.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados com $N=100$ observa√ß√µes e estamos comparando dois modelos de regress√£o linear:
>
> *   Modelo 1: 3 vari√°veis preditoras ($p=4$, incluindo o intercepto). O log-verossimilhan√ßa √© -200.
> *   Modelo 2: 6 vari√°veis preditoras ($p=7$, incluindo o intercepto). O log-verossimilhan√ßa √© -180.
>
> Calculando o AIC:
>
> $AIC_1 = -2(-200) + 2(4) = 400 + 8 = 408$
> $AIC_2 = -2(-180) + 2(7) = 360 + 14 = 374$
>
> Calculando o BIC:
>
> $BIC_1 = -2(-200) + 4\log(100) = 400 + 4(4.605) \approx 418.42$
> $BIC_2 = -2(-180) + 7\log(100) = 360 + 7(4.605) \approx 392.24$
>
> O Modelo 2 tem um AIC e BIC menores do que o Modelo 1. No entanto, o BIC penaliza mais o modelo complexo. Portanto, o modelo 2 √© preferido de acordo com os dois criterios.

Outras abordagens comuns para sele√ß√£o de vari√°veis incluem [^18]:

*  **Best Subset Selection:** examina todos os poss√≠veis subconjuntos de vari√°veis preditoras e escolhe aquele com o melhor ajuste (menor RSS ou melhor crit√©rio de informa√ß√£o). Este m√©todo √© computacionalmente invi√°vel quando o n√∫mero de vari√°veis √© grande.

*  **Forward Stepwise Selection:** come√ßa com um modelo sem nenhuma vari√°vel e adiciona, iterativamente, a vari√°vel que mais melhora o ajuste do modelo, at√© que nenhum ganho adicional seja alcan√ßado. Este m√©todo √© menos custoso computacionalmente do que o Best Subset Selection, mas nem sempre garante o melhor subconjunto.

*   **Backward Stepwise Selection:** come√ßa com o modelo completo (com todas as vari√°veis) e remove, iterativamente, a vari√°vel que menos contribui para o ajuste do modelo, at√© que todas as vari√°veis restantes sejam consideradas importantes. Este m√©todo s√≥ pode ser usado quando o n√∫mero de observa√ß√µes √© maior do que o n√∫mero de vari√°veis.

### M√©todos de Regulariza√ß√£o: Ridge e Lasso

```mermaid
graph LR
    A("Regularization") --> B("L2 (Ridge)")
    A --> C("L1 (Lasso)")
    B --> D("Shrinks coefficients")
    B --> E("Reduces variance")
    C --> F("Promotes sparsity")
    C --> G("Variable selection")
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

**Ridge Regression** adiciona uma penalidade L2 √† fun√ß√£o objetivo, que √© proporcional √† soma dos quadrados dos coeficientes [^23]. A fun√ß√£o objetivo da Ridge √© dada por:

$$
\underset{\beta}{\text{min}} ||y-X\beta||^2 + \lambda||\beta||^2
$$
Onde:
*   $\lambda$ √© o par√¢metro de regulariza√ß√£o, controlando a intensidade da penaliza√ß√£o. Quanto maior o $\lambda$, maior a penalidade.
*   $||\beta||^2 = \sum^{p}_{j=1}\beta_j^2$  √© a soma dos quadrados dos coeficientes.
*   $||y-X\beta||^2$ √© a soma dos quadrados dos res√≠duos (RSS)

A solu√ß√£o para a Ridge √© dada por [^22]:
$$
\hat{\beta}^{ridge} = (X^TX + \lambda I)^{-1}X^Ty
$$

Onde $I$ √© a matriz identidade e $(X^TX+\lambda I)^{-1}$ sempre existe se  $\lambda$ > 0.
A regulariza√ß√£o L2 encolhe os coeficientes em dire√ß√£o a zero, reduzindo a vari√¢ncia do modelo e mitigando o *overfitting*. A solu√ß√£o da Ridge √© √∫nica, mesmo que $(X^TX)$ seja singular (o que acontece quando existem multicolinearidade) [^22]. No entanto, a regulariza√ß√£o L2 n√£o leva a coeficientes exatamente iguais a zero, ou seja, a *sparsity* n√£o √© um objetivo desse m√©todo.
**Lasso Regression** adiciona uma penalidade L1 √† fun√ß√£o objetivo, que √© proporcional √† soma dos valores absolutos dos coeficientes [^23]. A fun√ß√£o objetivo do Lasso √© dada por:

$$
\underset{\beta}{\text{min}} ||y-X\beta||^2 + \lambda||\beta||_1
$$
Onde:
*   $\lambda$ √© o par√¢metro de regulariza√ß√£o.
*   $||\beta||_1 = \sum_{j=1}^p|\beta_j|$  √© a soma dos valores absolutos dos coeficientes.
*   $||y-X\beta||^2$ √© a soma dos quadrados dos res√≠duos.

A solu√ß√£o para a Lasso n√£o possui uma forma anal√≠tica simples, e geralmente, precisa ser obtida atrav√©s de m√©todos num√©ricos. A penalidade L1 promove *sparsity* no modelo, pois alguns coeficientes s√£o levados a zero, automaticamente, selecionando as vari√°veis mais relevantes [^23].
*   O par√¢metro de regulariza√ß√£o $\lambda$ controla a intensidade da penaliza√ß√£o na fun√ß√£o objetivo. Um grande valor de $\lambda$ leva a uma maior penalidade e consequentemente mais *shrinkage* dos coeficientes (levando a um modelo mais simples). Um pequeno valor de $\lambda$ leva a uma menor penalidade e um modelo com menor *bias* e maior vari√¢ncia [^24].
*   A regi√£o de restri√ß√£o para o Lasso (a norma L1) √© um diamante, o que resulta em solu√ß√µes mais esparsas. A regi√£o de restri√ß√£o para a Ridge (a norma L2) √© um elipsoide, e os coeficientes s√£o apenas encolhidos em dire√ß√£o a zero, sem necessariamente se tornarem exatamente zero [^25].

> üí° **Exemplo Num√©rico:**
>
> Vamos usar os mesmos dados do exemplo anterior para ilustrar Ridge e Lasso.
>
> $$
> X = \begin{bmatrix}
> 1 & 1 & 2 \\
> 1 & 2 & 3 \\
> 1 & 3 & 4 \\
> 1 & 4 & 5
> \end{bmatrix}, \quad
> y = \begin{bmatrix}
> 5 \\ 8 \\ 11 \\ 14
> \end{bmatrix}
> $$
>
> **Ridge Regression:**
> Vamos usar $\lambda = 0.5$. Primeiro, calculamos $X^TX$:
>
> $$
> X^TX = \begin{bmatrix}
> 4 & 10 & 14 \\
> 10 & 30 & 40 \\
> 14 & 40 & 54
> \end{bmatrix}
> $$
>
> Em seguida, adicionamos $\lambda I$:
>
> $$
> X^TX + \lambda I = \begin{bmatrix}
> 4.5 & 10 & 14 \\
> 10 & 30.5 & 40 \\
> 14 & 40 & 54.5
> \end{bmatrix}
> $$
>
> Calculamos a inversa:
>
> $$
> (X^TX + \lambda I)^{-1} \approx \begin{bmatrix}
> 5.21 & -2.42 & 0.42 \\
> -2.42 & 1.35 & -0.22 \\
> 0.42 & -0.22 & 0.07
> \end{bmatrix}
> $$
>
> Usamos $X^Ty$ do exemplo anterior:
>
> $$
> X^Ty = \begin{bmatrix}
> 38 \\ 110 \\ 152
> \end{bmatrix}
> $$
>
> Calculamos os coeficientes $\hat{\beta}^{ridge}$:
>
> $$
> \hat{\beta}^{ridge} = (X^TX + \lambda I)^{-1}X^Ty \approx \begin{bmatrix}
> 5.21 & -2.42 & 0.42 \\
> -2.42 & 1.35 & -0.22 \\
> 0.42 & -0.22 & 0.07
> \end{bmatrix}
> \begin{bmatrix}
> 38 \\ 110 \\ 152
> \end{bmatrix} = \begin{bmatrix}
> 2.02 \\ 0.99 \\ 0.99
> \end{bmatrix}
> $$
>
> Os coeficientes da Ridge s√£o menores em magnitude quando comparados aos coeficientes da regress√£o linear sem regulariza√ß√£o.
>
> **Lasso Regression:**
> A solu√ß√£o para o Lasso n√£o tem forma anal√≠tica e requer m√©todos num√©ricos. Usando um solver, com $\lambda = 0.5$, podemos obter uma solu√ß√£o aproximada:
>
> $$
> \hat{\beta}^{lasso} \approx \begin{bmatrix}
> 2.5 \\ 0.8 \\ 0.7
> \end{bmatrix}
> $$
>
> Neste caso, a solu√ß√£o do Lasso tamb√©m tem coeficientes menores, mas a magnitude dos coeficientes √© diferente da Ridge. Para um valor de $\lambda$ maior, um dos coeficientes poderia ser levado a zero.
>
> | Method | $\beta_0$ | $\beta_1$ | $\beta_2$ |
> |--------|----------|----------|----------|
> | OLS    | 2        | 1        | 1        |
> | Ridge  | 2.02     | 0.99     | 0.99     |
> | Lasso  | 2.5      | 0.8      | 0.7      |
>
> Este exemplo ilustra como Ridge e Lasso encolhem os coeficientes, mas a penalidade L1 (Lasso) pode levar a coeficientes exatamente iguais a zero com um $\lambda$ maior.

**Lemma 4:** A penalidade L2 (Ridge) leva a coeficientes menores em norma, por isso controla a vari√¢ncia do modelo.
**Prova do Lemma 4:** Ao adicionar a penalidade $||\beta||_2$ a fun√ß√£o objetivo, a solu√ß√£o $\hat{\beta}^{ridge}$ √© dada por:
$$
\hat{\beta}^{ridge} = (X^TX + \lambda I)^{-1}X^Ty.
$$
O termo $\lambda I$ na matriz a ser invertida tem o efeito de adicionar uma constante positiva aos seus autovalores, o que leva a uma redu√ß√£o em norma dos coeficientes obtidos em compara√ß√£o com a solu√ß√£o de m√≠nimos quadrados. Para valores grandes de $\lambda$, os coeficientes tendem a ser menores, j√° que a penalidade imp√µe um custo por coeficientes com grandes valores. $\blacksquare$
