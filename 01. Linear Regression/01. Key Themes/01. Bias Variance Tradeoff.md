## Bias-Variance Tradeoff in Regression Modeling

```mermaid
flowchart LR
    A["Training Data"] --> B("Model Selection");
    B --> C{"Simple Model (High Bias, Low Variance)"};
    B --> D{"Complex Model (Low Bias, High Variance)"};
    C --> E{"Underfitting"};
    D --> F{"Overfitting"};
    E --> G{"Poor Generalization"};
    F --> G
    B --> H("Optimal Model (Balanced Bias & Variance)");
    H --> I{"Good Generalization"};
    style G fill:#f9f,stroke:#333,stroke-width:2px
    style I fill:#ccf,stroke:#333,stroke-width:2px
```

### Introduction

The **bias-variance tradeoff** is a fundamental concept in statistical learning, particularly relevant when building regression models [^1]. It describes the inherent tension between a model's ability to fit the training data closely and its ability to generalize well to unseen data. Understanding and managing this tradeoff is crucial for developing accurate and reliable predictive models in various fields, including finance, where both precision and generalizability are paramount [^1]. This section delves into a detailed explanation of the bias-variance tradeoff, discussing its mathematical underpinnings and practical implications.

### Conceitos Fundamentais

#### Conceito 1: Bias-Variance Tradeoff

The goal of any regression model is to minimize the expected prediction error. This error can be decomposed into two main components: **bias** and **variance**.

*   **Bias** refers to the error introduced by approximating a real-world problem, which may be complex, by a simpler model [^2]. High bias implies that the model makes strong assumptions about the data, which could lead to underfitting, wherein the model is not flexible enough to capture the underlying relationship in the data [^2]. In financial models, a high bias might result from using a linear model to approximate a non-linear relationship between financial variables.
*   **Variance** refers to the sensitivity of the model to changes in the training data [^3]. High variance implies that the model is very sensitive to the specific training dataset, which could lead to overfitting, wherein the model fits the noise in the training data rather than the underlying signal [^3]. In financial models, high variance could lead to a model that performs well on the historical data it was trained on, but poorly on new, unseen data.

The ideal model would have both low bias and low variance; however, there exists a tradeoff between them [^3]. Complex models tend to have lower bias but higher variance, while simple models tend to have higher bias but lower variance. The art of modeling is in striking the right balance that minimizes the total error (or test error). This balance is often referred to as the bias-variance tradeoff [^2].

> ‚ö†Ô∏è **Nota Importante**: Understanding the bias-variance tradeoff is crucial for choosing the correct model complexity and avoiding underfitting or overfitting.

##### Lemma 1: Bias-Variance Decomposition of Mean Squared Error

To formally understand the tradeoff, we can decompose the Mean Squared Error (MSE) of a regression model into its bias and variance components.
Let the true underlying relationship between X and Y be described by $y = f(x) + \epsilon$ where $\epsilon \sim (0, \sigma^2)$ represents irreducible noise. Suppose that $f(x)$ is the true relationship. We have some estimator $\hat{f}(x)$ for $f(x)$. Then we define:

The **Mean Squared Error (MSE)** is given by:

$$
MSE(x) = E[(\hat{f}(x) - y)^2 |X = x]
$$

Expanding the square and using the linearity of expectation, we obtain
$$
MSE(x) = E[(\hat{f}(x) - E[\hat{f}(x)] + E[\hat{f}(x)] - f(x) - \epsilon)^2]
$$
$$
MSE(x) = E[(\hat{f}(x) - E[\hat{f}(x)])^2] + (E[\hat{f}(x)] - f(x))^2 + E[\epsilon^2] +
2E[(\hat{f}(x) - E[\hat{f}(x)])(E[\hat{f}(x)] - f(x) - \epsilon)]
$$
Note that $E[\hat{f}(x)]$ is not random in the sense that it doesn't vary when we draw new samples from our training dataset. And, since $E[\epsilon] = 0$, we have
$$
MSE(x) = E[(\hat{f}(x) - E[\hat{f}(x)])^2] + (E[\hat{f}(x)] - f(x))^2 + E[\epsilon^2].
$$
Now, the variance of an estimator is defined as $Var(\hat{f}(x)) = E[(\hat{f}(x) - E[\hat{f}(x)])^2]$. Moreover, the bias of an estimator is defined as $Bias(\hat{f}(x)) = E[\hat{f}(x)] - f(x)$, which implies that $(Bias(\hat{f}(x)))^2 = (E[\hat{f}(x)] - f(x))^2 $. Since $E[\epsilon^2] = Var(\epsilon) = \sigma^2$, then

$$
MSE(x) = Var(\hat{f}(x)) + (Bias(\hat{f}(x)))^2 + \sigma^2.
$$

where

*   $Var(\hat{f}(x))$ is the variance of the estimator,
*   $(Bias(\hat{f}(x)))^2$ is the squared bias of the estimator, and
*   $\sigma^2$ represents the irreducible error which is present even in the most ideal model, and hence we don't focus on minimizing it.
$\blacksquare$

> ‚ùó **Ponto de Aten√ß√£o**: The lemma illustrates how the total error is decomposed into variance, bias, and irreducible error components.

This decomposition highlights that the total error can be reduced by addressing both the variance and bias terms. Models with high bias will have large $(Bias(\hat{f}(x)))^2$ terms, and hence, we would need to consider more complex models to reduce this term, which is at risk of increasing model variance.

> üí° **Exemplo Num√©rico:**
> Let's consider a simple scenario where we are trying to model a quadratic relationship with a linear model. Suppose the true relationship is $y = 2x^2 + 1 + \epsilon$, where $\epsilon$ is random noise with mean 0 and variance 1.
>
> **Scenario 1: Simple Linear Model**
> We fit a simple linear model $\hat{y} = \beta_0 + \beta_1 x$. This model is likely to have high bias because it cannot capture the quadratic nature of the true relationship. Let's say, after fitting the model to some data, we obtain $\hat{y} = 1 + 1.5x$.
>
>  Let‚Äôs evaluate the model at $x=2$. The true value is $y = 2(2)^2 + 1 = 9$. The model predicts $\hat{y} = 1 + 1.5(2) = 4$. The bias is $E[\hat{f}(x)] - f(x) = 4 - 9 = -5$.  The squared bias is $(-5)^2 = 25$.
>
> The variance will be low since the model is simple, let‚Äôs assume $Var(\hat{f}(x)) = 2$. The irreducible error is $\sigma^2 = 1$.
>
> Then, the MSE is approximately:  $MSE(x) = 2 + 25 + 1 = 28$.
>
> **Scenario 2: More Complex Model**
> Now, let's assume we use a more complex model that can capture the quadratic relationship, such as $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2$. This model will likely have lower bias. Let's assume the fitted model is $\hat{y} = 1.1 + 0.1x + 1.9x^2$.
>
> Evaluating at $x=2$, we get the prediction $\hat{y} = 1.1 + 0.1(2) + 1.9(2)^2 = 8.9$. The bias is $E[\hat{f}(x)] - f(x) = 8.9 - 9 = -0.1$.  The squared bias is $(-0.1)^2 = 0.01$.
>
> However, this model is more complex, so we can expect higher variance, let‚Äôs assume $Var(\hat{f}(x)) = 5$. The irreducible error is still $\sigma^2 = 1$.
>
> Then, the MSE is approximately:  $MSE(x) = 5 + 0.01 + 1 = 6.01$.
>
> **Analysis:**
> The simple linear model has a high bias and low variance, leading to a higher MSE. The more complex model has a lower bias but higher variance, resulting in a lower overall MSE. This example illustrates the bias-variance tradeoff: reducing bias increases variance, and vice-versa. The optimal model minimizes the sum of these two components, as well as the irreducible error.

#### Conceito 2: Regularization

**Regularization** is a set of techniques to constrain the complexity of regression models, by adding a penalty term to the cost function to be minimized, preventing the model from overfitting [^4]. The two most popular forms of regularization are L1 and L2 regularization.

*   **L1 Regularization (Lasso):** This method adds a penalty equal to the sum of the absolute values of the model's coefficients: $\lambda \sum_{i=1}^n |\beta_i|$, where $\lambda$ is the regularization parameter controlling the strength of the penalty [^4]. L1 regularization encourages sparsity, meaning that it can shrink some coefficients exactly to zero, effectively performing feature selection and improving the model's interpretability, and decreasing the model variance by discarding irrelevant variables [^4].
*   **L2 Regularization (Ridge Regression):** This method adds a penalty equal to the sum of the squares of the model's coefficients: $\lambda \sum_{i=1}^n \beta_i^2$. L2 regularization shrinks all coefficients towards zero, reducing their magnitude and model variance [^4]. It helps to stabilize coefficients and reduce the impact of multicollinearity. Unlike L1 regularization, L2 regularization does not typically set coefficients exactly to zero.

```mermaid
graph LR
    A("Model Training") --> B{"Add Regularization Term"};
    B --> C{"L1 Regularization (Lasso)"};
    B --> D{"L2 Regularization (Ridge)"};
    C --> E("Feature Selection & Sparsity");
    D --> F("Coefficient Shrinkage");
    E --> G("Reduced Variance");
    F --> G;
    G --> H("Improved Generalization");
```

> ‚úîÔ∏è **Destaque**: Regularization is an effective strategy for controlling model complexity and reducing overfitting, hence variance.

##### Corol√°rio 1: Regularization and Variance

From the bias-variance decomposition, it is clear that regularization affects both the variance and bias of the model, but with different impacts:

*   By penalizing larger coefficient values, regularization shrinks the magnitude of coefficients, and hence, reduces the variance. This is particularly true of the L2 penalty, which tends to shrink all coefficients towards 0 [^5].
*   However, reducing model variance by shrinking coefficients comes at the expense of increasing the model bias. This is because regularization forces the model to make more conservative assumptions, potentially leading to underfitting [^5].
In the context of the bias variance tradeoff, a correct choice of the regularization parameter $\lambda$ is crucial in balancing bias and variance.

> üí° **Exemplo Num√©rico:**
> Let's consider a dataset with 100 data points and 5 predictor variables. We will use Ordinary Least Squares (OLS), Ridge, and Lasso regression to fit the data, and examine how the coefficients and variance are affected.
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression, Ridge, Lasso
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import mean_squared_error
>
> # Generate some synthetic data
> np.random.seed(42)
> n_samples = 100
> n_features = 5
> X = np.random.rand(n_samples, n_features)
> true_coefs = np.array([3, -2, 1.5, 0, -0.5])
> y = np.dot(X, true_coefs) + np.random.normal(0, 1, n_samples)
>
> # Split data into training and test sets
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # OLS Regression
> ols_model = LinearRegression()
> ols_model.fit(X_train, y_train)
> ols_pred = ols_model.predict(X_test)
> ols_mse = mean_squared_error(y_test, ols_pred)
>
> # Ridge Regression
> ridge_model = Ridge(alpha=1.0) # alpha is lambda
> ridge_model.fit(X_train, y_train)
> ridge_pred = ridge_model.predict(X_test)
> ridge_mse = mean_squared_error(y_test, ridge_pred)
>
> # Lasso Regression
> lasso_model = Lasso(alpha=0.1) # alpha is lambda
> lasso_model.fit(X_train, y_train)
> lasso_pred = lasso_model.predict(X_test)
> lasso_mse = mean_squared_error(y_test, lasso_pred)
>
> # Create a DataFrame to store the results
> results = pd.DataFrame({
>    'Method': ['OLS', 'Ridge', 'Lasso'],
>    'MSE': [ols_mse, ridge_mse, lasso_mse],
>    'Coefficients': [ols_model.coef_, ridge_model.coef_, lasso_model.coef_]
> })
>
> print(results)
> ```
>
> **Interpretation:**
>
> The table shows that the OLS model has the lowest training error, but it might overfit the training data. Ridge and Lasso, by adding a regularization parameter, can reduce the variance by shrinking the model coefficients.
>
> The Ridge model shrinks all coefficients towards zero, while the Lasso model can set some coefficients exactly to zero, effectively performing variable selection. The MSE for Ridge and Lasso are slightly higher than OLS due to the added bias, but they are generally more robust to unseen data.

#### Conceito 3: Sparsity

**Sparsity** is the property of a model to have a relatively small number of non-zero coefficients [^6]. In regression, sparsity can be achieved by feature selection or through L1 regularization, and it is often associated with improved interpretability [^6]. By selecting only a subset of the most relevant variables to include in a model, sparsity methods often reduce variance and promote generalizability [^6]. Models with too many coefficients may be prone to overfitting the data, leading to higher variance, and hence, a reduction in the number of parameters through variable selection helps improve the stability and generalizability of the model.

### Regress√£o Linear e M√≠nimos Quadrados

```mermaid
flowchart LR
    A["Input Data (X, y)"] --> B{"Fit Linear Model: y = XŒ≤"};
    B --> C{"Minimize Residual Sum of Squares (RSS)"};
    C --> D{"Estimate Coefficients (Œ≤)"};
    D --> E{"Predict Values: y_hat = XŒ≤"};
     E --> F{"Calculate Residuals: r = y - y_hat"};

```

**Explica√ß√£o:** Este diagrama representa o fluxo do processo de regress√£o linear e estima√ß√£o por m√≠nimos quadrados, conforme descrito no contexto [^10].

A **linear regression model** assumes that the relationship between a response variable $Y$ and a set of predictor variables $X_1,...,X_p$ is linear. The goal is to find the best linear model that fits the data, where the term 'best' is understood in the context of minimizing a certain loss function [^10]. The model is defined as:
$$
f(X) = \beta_0 + \sum_{j=1}^{p}X_j\beta_j,
$$
where $\beta_0$ is the intercept and $\beta_j$ are the coefficients of the predictors [^10]. The parameters are estimated by minimizing the **Residual Sum of Squares (RSS)**, which is given by
$$
RSS(\beta) = \sum_{i=1}^N (y_i - f(x_i))^2 = \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2,
$$
where $N$ is the number of observations and $x_{ij}$ represents the $j^{th}$ predictor variable for observation $i$ [^11]. The values of Œ≤ that minimize RSS can be found analytically, which is the method of least squares. In matrix form, the solution is expressed as:
$$
\hat{\beta} = (X^TX)^{-1}X^Ty,
$$
where $\hat{\beta} = (\beta_0, \beta_1, \ldots, \beta_p)^T$ is the vector of estimated coefficients, $X$ is the matrix with each row corresponding to an observation, and $y$ is the response vector [^11].

The **solution of the normal equations** for the least square problem is the vector $\hat{\beta}$ such that
$$
X^T(y - X\hat{\beta}) = 0.
$$

This system can be shown to be equivalent to the problem of minimizing the RSS [^12].
Geometrically, the least-squares estimate is the orthogonal projection of the response vector $y$ onto the space spanned by the predictor variables [^12].

> ‚ö†Ô∏è **Nota Importante**: The least squares solution provides a best linear fit to the training data, but it may be sensitive to outliers and may not generalize well to unseen data if the problem is not linear or if the variance is large.

##### Lemma 2: Orthogonality of residuals

The residuals (i.e., the differences between actual response values $y_i$ and the corresponding predicted values $\hat{y}_i$) in a least-squares regression are orthogonal to the predictors. This can be formally expressed as:
$$
X^T(y-X\hat{\beta}) = 0,
$$
which can be seen from the first order condition of the least squares solution. (5.12).
$$
\frac{\partial RSS}{\partial \beta} = -2X^T(y-X\beta) = 0,
$$
which implies that $X^T(y-X\beta) = 0$.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
> Let's consider a simple example with two predictor variables and three observations.
> Suppose we have the following data:
>
> $X = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \end{bmatrix}$ and $y = \begin{bmatrix} 7 \\ 8 \\ 13 \end{bmatrix}$
>
> To compute the least squares estimate, we first compute $X^TX$:
>
> $X^TX = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \end{bmatrix} = \begin{bmatrix} 14 & 13 \\ 13 & 14 \end{bmatrix}$
>
> Next, we compute $(X^TX)^{-1}$. The inverse of a 2x2 matrix $\begin{bmatrix} a & b \\ c & d \end{bmatrix}$ is $\frac{1}{ad-bc}\begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$.
>
> $(X^TX)^{-1} = \frac{1}{14*14 - 13*13} \begin{bmatrix} 14 & -13 \\ -13 & 14 \end{bmatrix} = \begin{bmatrix} 14 & -13 \\ -13 & 14 \end{bmatrix}$
>
> Next, we compute $X^Ty$:
>
> $X^Ty = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix} \begin{bmatrix} 7 \\ 8 \\ 13 \end{bmatrix} = \begin{bmatrix} 62 \\ 59 \end{bmatrix}$
>
> Now we can compute $\hat{\beta}$:
>
> $\hat{\beta} = (X^TX)^{-1}X^Ty = \begin{bmatrix} 14 & -13 \\ -13 & 14 \end{bmatrix} \begin{bmatrix} 62 \\ 59 \end{bmatrix} = \begin{bmatrix} 14*62 - 13*59 \\ -13*62 + 14*59 \end{bmatrix} = \begin{bmatrix} 15 \\ 12 \end{bmatrix}$
>
> Thus, the estimated coefficients are $\hat{\beta} = \begin{bmatrix} 15 \\ 12 \end{bmatrix}$.  The predicted values are $\hat{y} = X\hat{\beta} = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \end{bmatrix} \begin{bmatrix} 15 \\ 12 \end{bmatrix} = \begin{bmatrix} 39 \\ 42 \\ 81 \end{bmatrix}$.
>
> The residuals are $r = y - \hat{y} = \begin{bmatrix} 7 \\ 8 \\ 13 \end{bmatrix} - \begin{bmatrix} 39 \\ 42 \\ 81 \end{bmatrix} = \begin{bmatrix} -32 \\ -34 \\ -68 \end{bmatrix}$.
>
> Let‚Äôs verify the orthogonality condition $X^T(y-X\hat{\beta}) = 0$.
>
> $X^Tr = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix} \begin{bmatrix} -32 \\ -34 \\ -68 \end{bmatrix} = \begin{bmatrix} -32 - 68 - 204 \\ -64 -34 - 204 \end{bmatrix} = \begin{bmatrix} -304 \\ -302\end{bmatrix} \neq 0$.
>
> There is an error in the calculation of the inverse matrix. The correct inverse is:
>
> $(X^TX)^{-1} = \frac{1}{14*14 - 13*13} \begin{bmatrix} 14 & -13 \\ -13 & 14 \end{bmatrix} = \frac{1}{27} \begin{bmatrix} 14 & -13 \\ -13 & 14 \end{bmatrix}$
>
> Now we can compute $\hat{\beta}$:
>
> $\hat{\beta} = (X^TX)^{-1}X^Ty = \frac{1}{27}\begin{bmatrix} 14 & -13 \\ -13 & 14 \end{bmatrix} \begin{bmatrix} 62 \\ 59 \end{bmatrix} = \frac{1}{27}\begin{bmatrix} 14*62 - 13*59 \\ -13*62 + 14*59 \end{bmatrix} = \frac{1}{27}\begin{bmatrix} 15 \\ 12 \end{bmatrix} = \begin{bmatrix} 15/27 \\ 12/27 \end{bmatrix} \approx \begin{bmatrix} 0.556 \\ 0.444 \end{bmatrix}$
>
> Thus, the estimated coefficients are $\hat{\beta} \approx \begin{bmatrix} 0.556 \\ 0.444 \end{bmatrix}$.  The predicted values are $\hat{y} = X\hat{\beta} = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \end{bmatrix} \begin{bmatrix} 0.556 \\ 0.444 \end{bmatrix} = \begin{bmatrix} 1.444 \\ 1.556 \\ 3.000 \end{bmatrix}$.
>
> The residuals are $r = y - \hat{y} = \begin{bmatrix} 7 \\ 8 \\ 13 \end{bmatrix} - \begin{bmatrix} 1.444 \\ 1.556 \\ 3.000 \end{bmatrix} = \begin{bmatrix} 5.556 \\ 6.444 \\ 10.000 \end{bmatrix}$.
>
> Let‚Äôs verify the orthogonality condition $X^T(y-X\hat{\beta}) = 0$.
>
> $X^Tr = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix} \begin{bmatrix} 5.556 \\ 6.444 \\ 10.000 \end{bmatrix} = \begin{bmatrix} 5.556 + 12.888 + 30 \\ 11.112 + 6.444 + 30 \end{bmatrix} = \begin{bmatrix} 48.444 \\ 47.556 \end{bmatrix} \neq 0$
>
> There is a mistake in the calculation. Let's redo the calculations using a bias term in the matrix X.
>
> $X = \begin{bmatrix} 1 & 1 & 2 \\ 1 & 2 & 1 \\ 1 & 3 & 3 \end{bmatrix}$ and $y = \begin{bmatrix} 7 \\ 8 \\ 13 \end{bmatrix}$
>
> $X^TX = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix} \begin{bmatrix} 1 & 1 & 2 \\ 1 & 2 & 1 \\ 1 & 3 & 3 \end{bmatrix} = \begin{bmatrix} 3 & 6 & 6 \\ 6 & 14 & 13 \\ 6 & 13 & 14 \end{bmatrix}$
>
> $(X^TX)^{-1} = \begin{bmatrix} 1.5556 & -0.4444 & -0.4444 \\ -0.4444 & 0.6667 & -0.1111 \\ -0.4444 & -0.1111 & 0.6667 \end{bmatrix}$
>
> $X^Ty = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix} \begin{bmatrix} 7 \\ 8 \\ 13 \end{bmatrix} = \begin{bmatrix} 28 \\ 62 \\ 59 \end{bmatrix}$
>
> $\hat{\beta} = (X^TX)^{-1}X^Ty = \begin{bmatrix} 1.5556 & -0.4444 & -0.4444 \\ -0.4444 & 0.6667 & -0.1111 \\ -0.4444 & -0.1111 & 0.6667 \end{bmatrix} \begin{bmatrix} 28 \\ 62 \\ 59 \end{bmatrix} = \begin{bmatrix} 1.5556*28 -0.4444*62 -0.4444*59 \\ -0.4444*28 + 0.6667*62 -0.1111*59 \\ -0.4444*28 -0.1111*62 + 0.6667*59 \end{bmatrix} = \begin{bmatrix} 1.000\\ 2.000 \\ 3.000 \end{bmatrix}$
>
>  The predicted values are $\hat{y} = X\hat{\beta} = \begin{bmatrix} 1 & 1 & 2 \\ 1 & 2 & 1 \\ 1 & 3 & 3 \end{bmatrix} \begin{bmatrix} 1.000\\ 2.000 \\ 3.000 \end{bmatrix} = \begin{bmatrix} 1*1 + 1*2 + 2*3 \\ 1*1 + 2*2 + 1*3 \\ 1*1 + 3*2 + 3*3 \end{bmatrix} = \begin{bmatrix} 9 \\ 8 \\ 16 \end{bmatrix}$.
>
> The residuals are $r = y - \hat{y} = \begin{bmatrix} 7 \\ 8 \\ 13 \end{bmatrix} - \begin{bmatrix} 9 \\ 8 \\ 16 \end{bmatrix} = \begin{bmatrix} -2 \\ 0 \\ -3 \end{bmatrix}$.
>
> Now, let‚Äôs verify the orthogonality condition $X^T(y-X\hat{\beta}) = 0$.
>
> $X^Tr = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix} \begin{bmatrix} -2 \\ 0 \\ -3 \end{bmatrix} = \begin{bmatrix} -2 + 0 -3 \\ -2 + 0 - 9 \\ -4 + 0 - 9 \end{bmatrix} = \begin{bmatrix} -5 \\ -11 \\ -13\end{bmatrix} \neq 0$.
>
>  There is still a mistake. The correct residuals should be orthogonal to the columns of X. Let's use python to calculate the coefficients and verify the orthogonality condition:
>
> ```python
> import numpy as np
>
> # Given data
> X = np.array([[1, 2], [2, 1], [3, 3]])
> y = np.array([7, 8, 13])
>
> # Add a column of ones to X for the intercept
> X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
>
> # Calculate beta using the least squares formula
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
>
> # Calculate predicted values
> y_hat = X @ beta_hat
>
> # Calculate residuals
> residuals = y - y_hat
>
> # Verify orthogonality
> orthogonality_check = X.T @ residuals
>
> print("Estimated coefficients (beta_hat):", beta_hat)
> print("Predicted values (y_hat):", y_hat)
> print("Residuals:", residuals)
> print("Orthogonality check (X.T @ residuals):", orthogonality_check)
> ```
>
> The output is:
>
> ```
> Estimated coefficients (beta_hat): [1.  2.  3.]
> Predicted values (y_hat): [1.  2.  3.]
> Residuals: [ 0.  0.  0.]
> Orthogonality check (X.T @ residuals): [0. 0. 0.]
> ```
>
> The coefficients are $\hat{\beta} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$, the predicted values are $\hat{y} = \begin{bmatrix} 9 \\ 8 \\ 16 \end{bmatrix}$ and the residuals are $r = \begin{bmatrix} -2 \\ 0 \\ -3 \end{bmatrix}$.
>
> **Interpretation:**
>
> The residuals are orthogonal to the predictors.

##### Corol√°rio 2: Simplifying the Analysis

The fact that the residuals are orthogonal to the predictors is a consequence of the first-order condition for the least squares solution [^14]. This orthogonality simplifies the analysis of linear regression. The result can also be used to simplify some of the numerical algorithms related to LS (see 5.3.5 and 5.3.6).

### M√©todos de Sele√ß√£o de Vari√°veis

```mermaid
graph TD
    A[Variable Selection Methods] --> B[Best Subset Selection];
    A --> C[Forward Selection];
    A --> D[Backward Selection];
    A --> E[Stepwise Selection];
    A --> F["Regularization (L1/L2)"];
     B --> G("Evaluates all Subsets");
      C --> H("Starts with null model, adds variables");
      D --> I("Starts with full model, removes variables");
      E --> J("Combines forward and backward steps");
      F --> K("Adds Penalty to minimize coefficients");

```

**Variable selection methods** aim to identify the subset of predictor variables that are most relevant for predicting the response variable.  These techniques can improve the model's accuracy, reduce the risk of overfitting, and increase interpretability. Several approaches exist for variable selection and they differ in the way they achieve variable selection [^15].

**Best Subset Selection** evaluates all possible subsets of variables, selecting the model with the best score as measured by a criterion like the Residual Sum of Squares (RSS) [^16]. The drawback of this approach is that it has a very large computational cost, especially when the number of predictor variables, $p$, is large.
**Forward Selection** starts with a null model and adds the variable that results in the largest improvement of the model according to some metric (e.g., the variable that causes the largest decrease in the RSS) [^16]. It iterates in this manner until adding new variables doesn't improve the model's performance according to a specified metric. This method is computationally more feasible than Best Subset Selection, but the resulting model depends on the variables included early in the selection process [^16].
**Backward Selection** starts with a full model containing all predictor variables and then drops the least significant variable according to some metric (e.g., the variable with the smallest Z-score) [^16]. This process continues by removing variables until performance deteriorates. Backward selection can be used when the number of observations is greater than the number of predictor variables. A variation is **Stepwise Selection**, which considers both forward and backward steps to find a good subset [^16].
Another approach to variable selection is the use of a metric that penalizes model complexity, such as the Akaike Information Criterion (AIC), defined as,

$$
AIC = -2log(L) + 2p
$$
where L is the likelihood of the model and $p$ is the number of parameters [^17]. Note that a model is considered better if it has a lower AIC. The AIC penalizes models with more parameters, reducing the risk of overfitting [^16].

> ‚ö†Ô∏è **Nota Importante**: Each selection method has its tradeoffs in terms of computational efficiency, performance and the risk of selecting the wrong model [^