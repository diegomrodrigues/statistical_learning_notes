## OrthogonalizaÃ§Ã£o em Modelos de RegressÃ£o Linear: Uma AnÃ¡lise AvanÃ§ada

```mermaid
flowchart LR
    A["EspaÃ§o Vetorial"] --> B("Base Ortogonal");
    B --> C("ProjeÃ§Ã£o Ortogonal");
    C --> D("RegressÃ£o Linear");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

### IntroduÃ§Ã£o

A **OrtogonalizaÃ§Ã£o** Ã© um conceito fundamental em Ã¡lgebra linear e em anÃ¡lise numÃ©rica, com um papel crucial em muitos algoritmos de aprendizado de mÃ¡quina e anÃ¡lise de dados, especialmente em modelos de regressÃ£o linear [^206]. No contexto da regressÃ£o linear, a ortogonalizaÃ§Ã£o Ã© usada principalmente para construir bases ortonormais para espaÃ§os vetoriais, simplificando cÃ¡lculos, aumentando a estabilidade numÃ©rica e proporcionando *insights* importantes sobre a estrutura dos dados [^206]. Este capÃ­tulo explorarÃ¡ os fundamentos teÃ³ricos da ortogonalizaÃ§Ã£o, os mÃ©todos mais relevantes, como as matrizes de Householder e Givens, o processo de fatoraÃ§Ã£o QR, e suas aplicaÃ§Ãµes na resoluÃ§Ã£o de problemas de regressÃ£o linear, incluindo as soluÃ§Ãµes de mÃ­nimos quadrados.

### Matrizes de Householder e Givens: Fundamentos da OrtogonalizaÃ§Ã£o

Nesta seÃ§Ã£o, vamos apresentar os conceitos de matrizes de Householder e de Givens, que sÃ£o a base de diversos algoritmos de ortogonalizaÃ§Ã£o.

**Matrizes de Householder**

Uma **matriz de Householder**, tambÃ©m conhecida como reflexÃ£o de Householder, Ã© uma matriz ortogonal que representa uma reflexÃ£o em relaÃ§Ã£o a um hiperplano que passa pela origem [^209]. Uma matriz de Householder Ã© definida por:

$$
P = I - 2\frac{vv^T}{v^T v}
$$

onde:

-   $I$ Ã© a matriz identidade de tamanho $n \times n$.
-   $v$ Ã© um vetor nÃ£o nulo em $\mathbb{R}^n$.
-   $vv^T$ Ã© o produto externo de $v$ por si mesmo.
-   $v^T v$ Ã© o produto interno de $v$ por si mesmo, que representa a norma ao quadrado do vetor $v$.

As matrizes de Householder possuem as seguintes propriedades:

1.  **Simetria:** $P = P^T$.

2.  **Ortogonalidade:** $P^T P = P P^T = I$, o que implica que $P = P^{-1}$.

3.  **ReflexÃ£o:** Uma multiplicaÃ§Ã£o por uma matriz de Householder espelha o vetor em relaÃ§Ã£o ao hiperplano ortogonal ao vetor $v$.

O vetor $v$ Ã© chamado de **vetor de Householder** e define a direÃ§Ã£o da reflexÃ£o. Uma propriedade chave das matrizes de Householder Ã© que elas podem ser usadas para introduzir zeros seletivamente em um vetor ou matriz [^209].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos criar uma matriz de Householder para refletir um vetor. Considere o vetor $x = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$. Queremos refletir $x$ de forma que a primeira coordenada fique inalterada, enquanto as outras coordenadas sejam transformadas. Para isso, vamos construir um vetor $v$ tal que $v = x - \|x\|e_1$, onde $e_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ e $\|x\| = \sqrt{1^2 + 2^2 + 3^2} = \sqrt{14}$.
>
> 1.  Calculamos $\|x\|$:
>     $\|x\| = \sqrt{1^2 + 2^2 + 3^2} = \sqrt{14} \approx 3.74$
>
> 2.  Calculamos $v$:
>     $v = x - \|x\|e_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} - 3.74 \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} -2.74 \\ 2 \\ 3 \end{bmatrix}$
>
> 3.  Calculamos $vv^T$:
>     $vv^T = \begin{bmatrix} -2.74 \\ 2 \\ 3 \end{bmatrix} \begin{bmatrix} -2.74 & 2 & 3 \end{bmatrix} = \begin{bmatrix} 7.5076 & -5.48 & -8.22 \\ -5.48 & 4 & 6 \\ -8.22 & 6 & 9 \end{bmatrix}$
>
> 4.  Calculamos $v^Tv$:
>     $v^Tv = (-2.74)^2 + 2^2 + 3^2 = 7.5076 + 4 + 9 = 20.5076$
>
> 5.  Calculamos $P$:
>     $P = I - 2\frac{vv^T}{v^T v} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} - 2 \frac{1}{20.5076} \begin{bmatrix} 7.5076 & -5.48 & -8.22 \\ -5.48 & 4 & 6 \\ -8.22 & 6 & 9 \end{bmatrix} = \begin{bmatrix} 0.268 & 0.535 & 0.803 \\ 0.535 & 0.609 & -0.585 \\ 0.803 & -0.585 & 0.122 \end{bmatrix}$
>
> 6.  Aplicamos $P$ em $x$:
>     $Px = \begin{bmatrix} 0.268 & 0.535 & 0.803 \\ 0.535 & 0.609 & -0.585 \\ 0.803 & -0.585 & 0.122 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 3.74 \\ 0 \\ 0 \end{bmatrix}$
>
>   O resultado mostra que o vetor $x$ foi refletido, tendo as suas duas Ãºltimas componentes zeradas.

**Matrizes de Givens**

Uma **matriz de Givens**, tambÃ©m conhecida como rotaÃ§Ã£o de Givens, Ã© uma matriz ortogonal que representa uma rotaÃ§Ã£o em um plano bidimensional. Uma matriz de Givens Ã© definida por:

```mermaid
    flowchart LR
        subgraph "G(i, k, Î¸)"
        A[1] --> B[0]
        B --> C[c]
        C --> D[s]
        D --> E[0]
        E --> F[1]
        end
       style A fill:#fff,stroke:#333,stroke-width:1px
       style F fill:#fff,stroke:#333,stroke-width:1px
       style C fill:#ccf,stroke:#333,stroke-width:1px
       style D fill:#ccf,stroke:#333,stroke-width:1px
        B -.-> C
        C -.-> D
```

$$
G(i, k, \theta) =
\begin{bmatrix}
    1 & \cdots & 0 & \cdots & 0 & \cdots & 0 \\
    \vdots & \ddots & \vdots & & \vdots & & \vdots \\
    0 & \cdots & c & \cdots & s & \cdots & 0 \\
    \vdots & & \vdots & \ddots & \vdots & & \vdots \\
    0 & \cdots & -s & \cdots & c & \cdots & 0 \\
    \vdots & & \vdots & & \vdots & \ddots & \vdots \\
    0 & \cdots & 0 & \cdots & 0 & \cdots & 1
\end{bmatrix}
$$

onde:

-   $c = \cos(\theta)$ e $s = \sin(\theta)$
-   $i$ e $k$ sÃ£o os Ã­ndices das linhas e colunas que sÃ£o afetadas pela rotaÃ§Ã£o.

As matrizes de Givens possuem as seguintes propriedades:
    1. **Ortogonalidade:** $G(i, k, \theta)^T G(i, k, \theta) = G(i, k, \theta)G(i, k, \theta)^T = I$.
    2.  **RotaÃ§Ã£o:** A multiplicaÃ§Ã£o por uma matriz de Givens representa uma rotaÃ§Ã£o no plano definido pelas dimensÃµes $i$ e $k$.
    3. **Seletividade:** As matrizes de Givens sÃ£o utilizadas para introduzir zeros de forma seletiva numa matriz ou vetor, afetando apenas duas linhas ou colunas de cada vez.

A escolha do Ã¢ngulo $\theta$ permite zerar um elemento especÃ­fico num vetor ou matriz [^215].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos criar uma matriz de Givens para zerar um elemento de um vetor. Considere o vetor $x = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$. Queremos zerar o segundo elemento utilizando uma rotaÃ§Ã£o.
>
> 1.  Calculamos o raio $r = \sqrt{3^2+4^2} = 5$
> 2.  Calculamos $c = \frac{3}{5} = 0.6$ e $s = \frac{4}{5} = 0.8$.
> 3.  A matriz de Givens Ã© entÃ£o:
>     $G(1,2,\theta) = \begin{bmatrix} 0.6 & 0.8 \\ -0.8 & 0.6 \end{bmatrix}$
> 4.  Aplicamos $G$ em $x$:
>    $Gx = \begin{bmatrix} 0.6 & 0.8 \\ -0.8 & 0.6 \end{bmatrix} \begin{bmatrix} 3 \\ 4 \end{bmatrix} = \begin{bmatrix} 0.6*3 + 0.8*4 \\ -0.8*3 + 0.6*4 \end{bmatrix} = \begin{bmatrix} 5 \\ 0 \end{bmatrix}$
>
>    O resultado mostra que o segundo elemento do vetor foi zerado.

**Lemma 14:** CondiÃ§Ãµes para a Ortogonalidade de Householder e Givens

Tanto as matrizes de Householder quanto as de Givens preservam a norma de qualquer vetor ao qual sÃ£o aplicadas, o que garante a sua estabilidade numÃ©rica. Para demonstrar isso, vamos assumir que $P$ Ã© uma matriz de Householder e $G$ Ã© uma matriz de Givens.
A matriz de Householder Ã© ortogonal por definiÃ§Ã£o:
$$ P^T P = (I - 2\frac{vv^T}{v^T v}) (I - 2\frac{vv^T}{v^T v}) = I - 4 \frac{vv^T}{v^T v} + 4 \frac{vv^Tvv^T}{(v^T v)^2} $$
Como $v^T v$ Ã© um escalar, entÃ£o:
$$ P^T P = I - 4 \frac{vv^T}{v^T v} + 4 \frac{vv^T}{v^T v} = I $$
e o mesmo pode ser demonstrado que $P P^T = I$, e assim $P$ Ã© ortogonal.

A matriz de Givens tambÃ©m Ã© ortogonal por definiÃ§Ã£o:

$$
G(i, k, \theta)^T G(i, k, \theta) =
\begin{bmatrix}
    1 & \cdots & 0 & \cdots & 0 & \cdots & 0 \\
    \vdots & \ddots & \vdots & & \vdots & & \vdots \\
    0 & \cdots & c & \cdots & -s & \cdots & 0 \\
    \vdots & & \vdots & \ddots & \vdots & & \vdots \\
    0 & \cdots & s & \cdots & c & \cdots & 0 \\
    \vdots & & \vdots & & \vdots & \ddots & \vdots \\
    0 & \cdots & 0 & \cdots & 0 & \cdots & 1
\end{bmatrix}
\begin{bmatrix}
    1 & \cdots & 0 & \cdots & 0 & \cdots & 0 \\
    \vdots & \ddots & \vdots & & \vdots & & \vdots \\
    0 & \cdots & c & \cdots & s & \cdots & 0 \\
    \vdots & & \vdots & \ddots & \vdots & & \vdots \\
    0 & \cdots & -s & \cdots & c & \cdots & 0 \\
    \vdots & & \vdots & & \vdots & \ddots & \vdots \\
    0 & \cdots & 0 & \cdots & 0 & \cdots & 1
\end{bmatrix}
$$

Realizando a multiplicaÃ§Ã£o, vemos que ela resulta na matriz identidade. $\blacksquare$

**CorolÃ¡rio 14:** PreservaÃ§Ã£o da Norma

A ortogonalidade das matrizes de Householder e Givens implica que elas preservam a norma de qualquer vetor ao qual sÃ£o aplicadas. Isto significa que para uma matriz de Householder $P$, e um vetor $x$ qualquer, temos $\|Px\|_2 = \|x\|_2$, e o mesmo ocorre para uma matriz de Givens $G$, ou seja $\|Gx\|_2 = \|x\|_2$.

### A FatorizaÃ§Ã£o QR: Uma Ferramenta Fundamental

```mermaid
flowchart LR
    A["Matriz A"] --> B("Matriz Ortogonal Q")
    A --> C("Matriz Triangular Superior R")
    B -.->|MultiplicaÃ§Ã£o| C
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
```

A **FatorizaÃ§Ã£o QR** Ã© uma tÃ©cnica fundamental na Ã¡lgebra linear e em diversas aplicaÃ§Ãµes, como a soluÃ§Ã£o de sistemas lineares e problemas de mÃ­nimos quadrados [^223]. Dado uma matriz $A$ com $m \ge n$, a fatorizaÃ§Ã£o QR decompÃµe $A$ em um produto de uma matriz ortogonal $Q \in \mathbb{R}^{m \times m}$ e uma matriz triangular superior $R \in \mathbb{R}^{m \times n}$, tal que:

$$
A = QR
$$

Onde $Q^T Q = I$.

Em muitas aplicaÃ§Ãµes, estamos interessados na forma reduzida do fatorizaÃ§Ã£o QR. Se a matriz $A$ tem posto completo, ou seja $rank(A) = n$, podemos escrever:

$$
A = Q_1R_1
$$

onde $Q_1 \in \mathbb{R}^{m \times n}$ Ã© uma matriz com colunas ortonormais (as primeiras $n$ colunas de $Q$) e $R_1 \in \mathbb{R}^{n \times n}$ Ã© uma matriz triangular superior.
O processo de fatorizaÃ§Ã£o QR pode ser implementado utilizando as matrizes de Householder e Givens.

**FatoraÃ§Ã£o QR utilizando Householder**

A fatorizaÃ§Ã£o QR utilizando Householder envolve a aplicaÃ§Ã£o de uma sequÃªncia de matrizes de Householder Ã  matriz $A$, com o objetivo de transformÃ¡-la numa matriz triangular superior. Os passos do algoritmo sÃ£o:

1.  **InicializaÃ§Ã£o:** Define-se $A_0 = A$ e $Q_0 = I$, onde $I$ Ã© a matriz identidade.
2.  **IteraÃ§Ã£o:** Em cada iteraÃ§Ã£o $j$, determinamos um vetor de Householder $v_j$ para zerar todos os elementos abaixo da diagonal na j-Ã©sima coluna da matriz $A_{j-1}$. Construimos a matriz de Householder
$$H_j = I - 2\frac{v_j v_j^T}{v_j^T v_j}$$
e atualizamos $A_j$ e $Q_j$:
$$ A_j=H_jA_{j-1} $$
$$ Q_j = Q_{j-1}H_j$$
3.  **RepetiÃ§Ã£o:** Repetimos o processo iterativo atÃ© que a matriz $A_n$ seja triangular superior.
4.  **Resultado:** No final, a matriz original $A$ Ã© decomposta em um produto de uma matriz ortogonal $Q = Q_n$ e uma matriz triangular superior $R = A_n$, de tal forma que $A=QR$.

O procedimento da fatorizaÃ§Ã£o QR por Householder Ã© mais eficiente computacionalmente quando se usa uma representaÃ§Ã£o compacta do produto de matrizes de Householder, utilizando o facto que $Q = H_1 H_2 \ldots H_n$ onde $H_i$ Ã© uma matriz de Householder.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos fatorar uma matriz $A$ usando Householder. Considere a matriz $A = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 1 \end{bmatrix}$.
>
> 1.  **InicializaÃ§Ã£o:** $A_0 = A$ e $Q_0 = I_3$.
>
> 2.  **IteraÃ§Ã£o 1:**
>     -   Para a primeira coluna de $A_0$, $a_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$, calculamos o vetor de Householder $v_1 = a_1 - \|a_1\| e_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} - \sqrt{3} \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 - \sqrt{3} \\ 1 \\ 1 \end{bmatrix} \approx \begin{bmatrix} -0.732 \\ 1 \\ 1 \end{bmatrix}$.
>     -   Calculamos a matriz de Householder $H_1 = I - 2\frac{v_1v_1^T}{v_1^Tv_1} \approx \begin{bmatrix} -0.577 & 0.577 & 0.577 \\ 0.577 & 0.816 & -0.408 \\ 0.577 & -0.408 & 0.816 \end{bmatrix}$
>     -   Atualizamos $A_1 = H_1A_0 \approx \begin{bmatrix} -1.732 & -2.887 \\ 0 & 1.155 \\ 0 & -0.577 \end{bmatrix}$ e $Q_1 = Q_0H_1 = H_1$.
>
> 3.  **IteraÃ§Ã£o 2:**
>     -   Consideramos a segunda coluna de $A_1$ a partir da segunda linha $a_2 = \begin{bmatrix} 1.155 \\ -0.577 \end{bmatrix}$.
>     -   Calculamos o vetor de Householder $v_2 = a_2 - \|a_2\| e_1 = \begin{bmatrix} 1.155 \\ -0.577 \end{bmatrix} - \sqrt{1.666} \begin{bmatrix} 1 \\ 0 \end{bmatrix} \approx \begin{bmatrix} -0.134 \\ -0.577 \end{bmatrix}$.
>     -   Calculamos a matriz de Householder $H_2 = I - 2\frac{v_2v_2^T}{v_2^Tv_2} \approx \begin{bmatrix} -0.833 & -0.555 \\ -0.555 & 0.833 \end{bmatrix}$.
>     -   Expandimos $H_2$ para $3x3$: $H_2' = \begin{bmatrix} 1 & 0 & 0 \\ 0 & -0.833 & -0.555 \\ 0 & -0.555 & 0.833 \end{bmatrix}$
>     -   Atualizamos $A_2 = H_2'A_1 \approx \begin{bmatrix} -1.732 & -2.887 \\ 0 & -1.333 \\ 0 & 0 \end{bmatrix}$ e $Q_2 = Q_1H_2' \approx \begin{bmatrix} -0.577 & -0.667 & -0.408 \\ 0.577 & -0.333 & -0.784 \\ 0.577 & -0.667 & 0.408 \end{bmatrix}$.
>
> 4.  **Resultado:** $Q = Q_2$ e $R = A_2$. Assim $A \approx QR$.
>
> ```python
> import numpy as np
> from scipy.linalg import qr
>
> A = np.array([[1, 2], [1, 3], [1, 1]], dtype=float)
> Q, R = qr(A)
>
> print("Q:\n", Q)
> print("R:\n", R)
> print("Q @ R:\n", Q @ R)
> ```
>
> Este exemplo demonstra como a fatorizaÃ§Ã£o QR usando Householder transforma a matriz original $A$ numa matriz ortogonal $Q$ e uma matriz triangular superior $R$.

**FatoraÃ§Ã£o QR utilizando Givens**

A fatorizaÃ§Ã£o QR utilizando Givens envolve a aplicaÃ§Ã£o de uma sequÃªncia de matrizes de rotaÃ§Ã£o de Givens Ã  matriz $A$, com o objetivo de transformÃ¡-la numa matriz triangular superior [^227]. Os passos do algoritmo sÃ£o:

1. **InicializaÃ§Ã£o**: ComeÃ§amos com a matriz original $A$.
2.  **IteraÃ§Ã£o**: Percorremos as entradas da matriz $A$ da diagonal para baixo, zerando cada entrada abaixo da diagonal com uma matriz de Givens apropriada. Para zerar a entrada $a_{i,j}$, construimos uma matriz de Givens, $G(i,j, \theta)$ que ao ser aplicada na esquerda, zera a entrada desejada. Ao aplicar uma matriz de Givens, somente duas linhas sÃ£o modificadas.
3.  **RepetiÃ§Ã£o**: Repetimos a aplicaÃ§Ã£o das matrizes de Givens atÃ© que a matriz $A$ seja triangular superior.
4.  **Resultado:** No final, a matriz original $A$ Ã© transformada em um produto de matrizes ortogonais $Q = G_t \ldots G_1$ e uma matriz triangular superior $R$, de forma que $A=QR$.

O processo de fatorizaÃ§Ã£o QR por Givens pode ser implementado usando diferentes padrÃµes de zero, como a introduÃ§Ã£o de zeros por linha, coluna ou diagonal, e a escolha depende do problema particular em questÃ£o.
A principal vantagem da fatorizaÃ§Ã£o por Givens Ã© que ela Ã© mais flexÃ­vel para lidar com problemas de *sparsity*.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos fatorar a mesma matriz $A = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 1 \end{bmatrix}$ usando Givens rotations.
>
> 1. **InicializaÃ§Ã£o:** ComeÃ§amos com $A_0 = A$.
> 2. **IteraÃ§Ã£o 1:**
>    - Queremos zerar $a_{21} = 1$. Usamos uma rotaÃ§Ã£o de Givens $G_1$ entre as linhas 1 e 2.
>    - $r = \sqrt{1^2 + 1^2} = \sqrt{2}$, $c = 1/\sqrt{2}$, $s = -1/\sqrt{2}$.
>    - $G_1 = \begin{bmatrix} 1/\sqrt{2} & -1/\sqrt{2} & 0 \\ 1/\sqrt{2} & 1/\sqrt{2} & 0 \\ 0 & 0 & 1 \end{bmatrix} \approx \begin{bmatrix} 0.707 & -0.707 & 0 \\ 0.707 & 0.707 & 0 \\ 0 & 0 & 1 \end{bmatrix}$.
>    - $A_1 = G_1 A_0 \approx \begin{bmatrix} 1.414 & 3.535 \\ 0 & 0.707 \\ 1 & 1 \end{bmatrix}$.
> 3. **IteraÃ§Ã£o 2:**
>    - Queremos zerar $a_{31} = 1$. Usamos uma rotaÃ§Ã£o de Givens $G_2$ entre as linhas 1 e 3.
>    - $r = \sqrt{1.414^2 + 1^2} = \sqrt{3}$, $c = 1.414/\sqrt{3}$, $s = -1/\sqrt{3}$.
>   - $G_2 = \begin{bmatrix} 1.154 & 0 & -0.577 \\ 0 & 1 & 0 \\ 0.577 & 0 & 1.154 \end{bmatrix} \approx \begin{bmatrix} 0.816 & 0 & -0.577 \\ 0 & 1 & 0 \\ 0.577 & 0 & 0.816 \end{bmatrix}$.
>    - $A_2 = G_2 A_1 \approx \begin{bmatrix} 1.732 & 2.887 \\ 0 & 0.707 \\ 0 & -0.577 \end{bmatrix}$.
> 4. **IteraÃ§Ã£o 3:**
>    - Queremos zerar $a_{32} = -0.577$. Usamos uma rotaÃ§Ã£o de Givens $G_3$ entre as linhas 2 e 3.
>    - $r = \sqrt{0.707^2 + (-0.577)^2} = 1$, $c = 0.707$, $s = 0.577$.
>    - $G_3 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0.707 & 0.577 \\ 0 & -0.577 & 0.707 \end{bmatrix}$.
>    - $A_3 = G_3 A_2 \approx \begin{bmatrix} 1.732 & 2.887 \\ 0 & 0.999 \\ 0 & 0 \end{bmatrix}$.
> 5. **Resultado:** $R = A_3$ e $Q = G_1^T G_2^T G_3^T$. Assim $A \approx QR$.
>
> ```python
> import numpy as np
> from scipy.linalg import qr
>
> A = np.array([[1, 2], [1, 3], [1, 1]], dtype=float)
> Q, R = qr(A)
>
> print("Q:\n", Q)
> print("R:\n", R)
> print("Q @ R:\n", Q @ R)
> ```
>
> Este exemplo mostra como a fatorizaÃ§Ã£o QR usando Givens transforma a matriz $A$ em uma matriz ortogonal $Q$ e uma matriz triangular superior $R$.

**Lemma 15:**  O SubespaÃ§o Gerado pela FatorizaÃ§Ã£o QR

O subespaÃ§o gerado pelas colunas da matriz original $A$ Ã© o mesmo subespaÃ§o gerado pelas colunas da matriz $Q_1$ da fatorizaÃ§Ã£o QR [^225]. Isto Ã©, se $A = Q_1R_1$ entÃ£o $ran(A) = ran(Q_1)$.

**Prova do Lemma 15:**
Sabemos que $A=Q_1 R_1$ o que implica que todas as colunas de $A$ sÃ£o combinaÃ§Ãµes lineares das colunas de $Q_1$, o que implica que $ran(A) \subseteq ran(Q_1)$. Dado que $R_1$ Ã© uma matriz invertÃ­vel, temos que $Q_1 = A R_1^{-1}$, e assim todas as colunas de $Q_1$ sÃ£o combinaÃ§Ãµes lineares das colunas de $A$, o que implica que $ran(Q_1) \subseteq ran(A)$. Juntando os dois resultados, temos que $ran(A) = ran(Q_1)$. $\blacksquare$

**CorolÃ¡rio 15:** Bases Ortonormais e ProjeÃ§Ã£o Ortogonal

O Lemma 15 implica que a fatorizaÃ§Ã£o QR Ã© Ãºtil para encontrar uma base ortonormal para um subespaÃ§o gerado por um conjunto de vetores (as colunas de A), que, por sua vez, permite realizar a projeÃ§Ã£o ortogonal de qualquer vetor neste subespaÃ§o. A projeÃ§Ã£o ortogonal de um vetor $b$ sobre o subespaÃ§o gerado pelas colunas de $A$ Ã© dada por $P_A b = Q_1Q_1^Tb$, onde $Q_1$ Ã© o fator ortogonal da fatorizaÃ§Ã£o QR de A.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos projetar um vetor $b$ no subespaÃ§o gerado pelas colunas de $A$. Considere $A = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 1 \end{bmatrix}$ e $b = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}$.
>
> 1.  Calculamos a fatorizaÃ§Ã£o QR de $A$: $A = Q_1R_1$, onde $Q_1$ sÃ£o as primeiras duas colunas de $Q$ obtidas no exemplo anterior com Householder, $Q_1 \approx \begin{bmatrix} -0.577 & -0.816 \\ 0.577 & -0.408 \\ 0.577 & 0.408 \end{bmatrix}$.
> 2.  Calculamos a projeÃ§Ã£o de $b$ no espaÃ§o gerado por $A$:
>     $P_A b = Q_1Q_1^T b = \begin{bmatrix} -0.577 & -0.816 \\ 0.577 & -0.408 \\ 0.577 & 0.408 \end{bmatrix} \begin{bmatrix} -0.577 & 0.577 & 0.577 \\ -0.816 & -0.408 & 0.408 \end{bmatrix} \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix} \approx \begin{bmatrix} 4.666 \\ 4.333 \\ 6.000 \end{bmatrix}$.
>
> A projeÃ§Ã£o de $b$ no subespaÃ§o gerado pelas colunas de $A$ Ã© aproximadamente $\begin{bmatrix} 4.666 \\ 4.333 \\ 6.000 \end{bmatrix}$.

###  AplicaÃ§Ãµes da OrtogonalizaÃ§Ã£o em RegressÃ£o Linear

Nesta seÃ§Ã£o vamos descrever o papel da ortogonalizaÃ§Ã£o no contexto da resoluÃ§Ã£o de problemas de regressÃ£o linear por mÃ­nimos quadrados.

**MÃ­nimos Quadrados com FatorizaÃ§Ã£o QR**

A fatorizaÃ§Ã£o QR pode ser usada para resolver o problema de mÃ­nimos quadrados de forma mais estÃ¡vel e eficiente em relaÃ§Ã£o Ã  soluÃ§Ã£o obtida atravÃ©s das equaÃ§Ãµes normais [^237]. Dado um problema de mÃ­nimos quadrados $\min_{\beta}||y-X\beta||_2^2$, calculamos a fatorizaÃ§Ã£o QR da matriz de design $X$, o que resulta em $X = QR$. O problema de otimizaÃ§Ã£o se torna:

```mermaid
flowchart LR
    A["MinimizaÃ§Ã£o"] --> B("FatorizaÃ§Ã£o QR de X")
    B --> C("ResoluÃ§Ã£o do Sistema RÎ²=Q^Ty")
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
```

$$
\min_{\beta} ||y - QR\beta||_2^2
$$

Como $Q$ Ã© ortogonal, o que implica que $\| Qz \| = \|z\|$, temos que:
$$
\min_{\beta} || Q^T y - Q^TQR\beta||_2^2 = \min_{\beta} ||Q^T y - R\beta||_2^2
$$

A soluÃ§Ã£o pode ser encontrada resolvendo o sistema triangular superior $R\beta = Q^Ty$, o que tem uma soluÃ§Ã£o mais estÃ¡vel que a soluÃ§Ã£o das equaÃ§Ãµes normais [^237].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos resolver um problema de mÃ­nimos quadrados usando a fatorizaÃ§Ã£o QR. Considere o modelo de regressÃ£o linear $y = X\beta + \epsilon$, onde $X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 1 \end{bmatrix}$ e $y = \begin{bmatrix} 7 \\ 8 \\ 5 \end{bmatrix}$.
>
> 1.  Calculamos a fatorizaÃ§Ã£o QR de $X$: $X = QR$, onde $Q \approx \begin{bmatrix} -0.577 & -0.816 & 0 \\ 0.577 & -0.408 & -0.707 \\ 0.577 & 0.408 & 0.707 \end{bmatrix}$ e $R \approx \begin{bmatrix} -1.732 & -2.886 \\ 0 & -1.224 \\ 0 & 0 \end{bmatrix}$
> 2.  Calculamos $Q^Ty$:
>     $Q^Ty \approx \begin{bmatrix} -0.577 & 0.577 & 0.577 \\ -0.816 & -0.408 & 0.408 \\ 0 & -0.707 & 0.707 \end{bmatrix} \begin{bmatrix} 7 \\ 8 \\ 5 \end{bmatrix} = \begin{bmatrix} 3.464 \\ -1.224 \\ -2.121 \end{bmatrix}$
> 3. Resolvemos $R\beta = Q^Ty$:
>     $\begin{bmatrix} -1.732 & -2.886 \\ 0 & -1.224 \end{bmatrix} \begin{bmatrix} \beta_1 \\ \beta_2 \end{bmatrix} = \begin{bmatrix} 3.464 \\ -1.224 \end{bmatrix}$
>     Resolvendo o sistema, obtemos $\beta_2 = 1$ e $\beta_1 = -3.464/1.732 - 2.886/1.732 = -2 - 1.666 = -3.666$
>     $\beta \approx \begin{bmatrix} -3.666 \\ 1 \end{bmatrix}$
>
> ```python
> import numpy as np
> from scipy.linalg import qr
>
> X = np.array([[1, 2], [1, 3], [1, 1