## Path Algorithms em Regress√£o Linear: LARS e suas Varia√ß√µes

```mermaid
flowchart TD
    A["Start"] --> B{"Find variable with max correlation to residual"};
    B --> C{Is another variable equally correlated?};
    C -- Yes --> D["Move coefficients jointly"];
    D --> E{"Are all variables in model?"};
    C -- No --> F["Move only the new variable coefficient"];
    F --> G{"Are all variables in model?"};
    E -- No --> B;
    G -- No --> B;
    E -- Yes --> H["End"];
    G -- Yes --> H;
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#ffc,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3,4,5,6,7,8 stroke-width:2px
    subgraph LARS
      D
      E
    end
    subgraph Forward Stagewise
      F
      G
    end
```

### Introdu√ß√£o
Em modelos de regress√£o linear, especialmente quando se busca solu√ß√µes esparsas atrav√©s de m√©todos como o Lasso, os **path algorithms** (algoritmos de caminho) s√£o ferramentas valiosas para explorar um espectro de solu√ß√µes ao longo de diferentes n√≠veis de regulariza√ß√£o. Em vez de encontrar apenas uma solu√ß√£o √≥tima para um valor fixo do par√¢metro de regulariza√ß√£o, esses algoritmos tra√ßam todo o caminho de solu√ß√µes poss√≠veis para diferentes valores do par√¢metro, oferecendo uma vis√£o detalhada de como o modelo se comporta em diferentes n√≠veis de complexidade [^30].

Este cap√≠tulo explorar√° o algoritmo **Least Angle Regression (LARS)** e suas varia√ß√µes, que computam eficientemente as trajet√≥rias de solu√ß√µes para modelos regularizados, como o Lasso. Compreender esses algoritmos √© fundamental para explorar e analisar modelos que envolvem sele√ß√£o de vari√°veis e para realizar o ajuste do n√≠vel de *sparsity* desejado para cada aplica√ß√£o [^31].

### Conceitos Fundamentais

Antes de detalharmos o funcionamento do LARS e suas varia√ß√µes, vamos definir alguns conceitos chave.

**Conceito 1: Caminho de Solu√ß√µes**
O caminho de solu√ß√µes em um modelo regularizado se refere √† sequ√™ncia de modelos obtidos ao variar o par√¢metro de regulariza√ß√£o. Em cada valor deste par√¢metro, um modelo diferente √© gerado, com uma diferente configura√ß√£o de coeficientes [^31].
*   O caminho de solu√ß√µes √© √∫til para entender como a regulariza√ß√£o afeta os coeficientes do modelo e a sua complexidade.
*   Explorar o caminho de solu√ß√µes permite identificar as vari√°veis preditoras que t√™m mais impacto no modelo e aquelas que s√£o redundantes.
*   Algoritmos de caminho computam eficientemente a trajet√≥ria de solu√ß√µes, sem a necessidade de recalcular todo o modelo para cada valor do par√¢metro de regulariza√ß√£o.

> üí° **Exemplo Num√©rico:**
> Imagine que estamos modelando o pre√ßo de casas usando duas vari√°veis preditoras: √°rea (em metros quadrados) e n√∫mero de quartos. Ao aplicar a regulariza√ß√£o Lasso e variar o par√¢metro $\lambda$, o caminho de solu√ß√µes poderia mostrar que, para valores altos de $\lambda$, ambos os coeficientes s√£o levados a zero. √Ä medida que $\lambda$ diminui, o coeficiente da √°rea come√ßa a crescer primeiro, indicando que √© um preditor mais forte, enquanto o coeficiente do n√∫mero de quartos s√≥ come√ßa a crescer para valores menores de $\lambda$. Isso nos ajuda a entender a import√¢ncia relativa de cada vari√°vel na predi√ß√£o do pre√ßo das casas.

> ‚ö†Ô∏è **Nota Importante**: O conceito de caminho de solu√ß√µes permite uma an√°lise mais abrangente da regulariza√ß√£o, revelando o trade-off entre a complexidade do modelo e a sua capacidade de ajuste [^30].

**Conceito 2: Forward Stepwise Regression e suas limita√ß√µes**
O Forward Stepwise Regression (FSR) √© um algoritmo iterativo para sele√ß√£o de vari√°veis que come√ßa com um modelo sem nenhuma vari√°vel preditora e, em cada passo, adiciona a vari√°vel que mais reduz o erro quadr√°tico m√©dio (RSS) do modelo [^15]. O FSR tem uma natureza "gulosa", uma vez que, a cada passo, seleciona a vari√°vel que parece mais promissora no momento, sem levar em considera√ß√£o as vari√°veis futuras.
*   O Forward Stepwise Regression tem baixo custo computacional, mas pode perder solu√ß√µes √≥timas, j√° que ele n√£o revisita as escolhas feitas em etapas anteriores [^15].
*   Ele tamb√©m pode gerar modelos inst√°veis na presen√ßa de vari√°veis altamente correlacionadas.
*   O Forward Stepwise Regression gera um caminho de modelos, indexado pelo n√∫mero de vari√°veis selecionadas, similar ao que √© gerado pelos m√©todos de regulariza√ß√£o [^16].

> üí° **Exemplo Num√©rico:**
> Suponha que temos tr√™s vari√°veis preditoras: $x_1$, $x_2$ e $x_3$. O FSR primeiro avalia qual delas tem a maior correla√ß√£o com a vari√°vel resposta $y$. Se $x_1$ for a mais correlacionada, ele a adiciona ao modelo. No pr√≥ximo passo, ele avalia qual das vari√°veis restantes ($x_2$ ou $x_3$) tem a maior correla√ß√£o com o res√≠duo atual (a diferen√ßa entre $y$ e as predi√ß√µes feitas usando $x_1$). Se $x_2$ for a mais correlacionada, ele a adiciona ao modelo, e assim por diante. Se $x_1$ e $x_2$ forem altamente correlacionadas entre si, o FSR pode adicionar $x_1$ primeiro e depois $x_2$, mesmo que a combina√ß√£o de $x_1$ e $x_3$ fosse uma op√ß√£o melhor, ilustrando sua natureza "gulosa".

> ‚ùó **Ponto de Aten√ß√£o**:  O Forward Stepwise Regression √© um m√©todo simples para sele√ß√£o de vari√°veis, mas sua natureza "gulosa" pode impedir a obten√ß√£o de resultados √≥timos [^16].
**Conceito 3: Algoritmos de Caminho**
Algoritmos de caminho, ou *path algorithms*, s√£o procedimentos que computam, eficientemente, as solu√ß√µes de um modelo regularizado em uma sequ√™ncia de valores para o par√¢metro de regulariza√ß√£o. Esses algoritmos s√£o otimizados para explorar eficientemente o caminho de solu√ß√µes, evitando o rec√°lculo do modelo para cada valor individual do par√¢metro [^30].
*   Algoritmos de caminho s√£o importantes em problemas de *sparsity* pois eles permitem rastrear o comportamento dos coeficientes e identificar quando as vari√°veis se tornam irrelevantes (ou seja, seus coeficientes s√£o levados a zero) [^31].
*   Eles permitem obter uma vis√£o completa do comportamento do modelo em fun√ß√£o da intensidade da regulariza√ß√£o.
*   Um exemplo importante de algoritmo de caminho √© o LARS (Least Angle Regression) [^30].

> üí° **Exemplo Num√©rico:**
> Em vez de treinar modelos Lasso separadamente para $\lambda$ = 0.1, 0.2, 0.3, etc., um algoritmo de caminho como o LARS calcula o caminho de solu√ß√µes, que nos mostra como os coeficientes mudam continuamente √† medida que $\lambda$ diminui. Isso √© muito mais eficiente do que treinar v√°rios modelos independentemente.

> ‚úîÔ∏è **Destaque**: Algoritmos de caminho computam o caminho de solu√ß√µes eficientemente, revelando como os coeficientes do modelo se comportam √† medida que variamos a intensidade da regulariza√ß√£o.

### Least Angle Regression (LARS)

```mermaid
sequenceDiagram
    participant LARS
    participant Data
    LARS->>Data: Initialize: all coefficients = 0, residual = y
    loop Iteration
        LARS->>Data: Find variable x_j with max correlation to residual
        LARS-->>LARS: Move coefficient beta_j along correlation direction
        LARS->>Data: Are there other variables equally correlated?
        alt Yes
            LARS-->>LARS: Move current coefficients together to reduce residual (RSS)
        else No
           LARS->>LARS:  Keep moving coefficient of x_j
        end
       LARS->>Data: Until all variables are added or stop criteria is reached
    end
    LARS-->>Data: Return Path of solutions
```

O algoritmo **Least Angle Regression (LARS)** √© um m√©todo iterativo para computar o caminho completo de solu√ß√µes do Lasso, ou seja, uma sequ√™ncia de modelos com diferentes n√≠veis de *sparsity* ao longo de diferentes valores de $\lambda$ [^31]. O LARS, ao contr√°rio do Forward Stepwise Regression, realiza as atualiza√ß√µes dos coeficientes de maneira incremental, e n√£o de forma abrupta [^60].

O algoritmo LARS come√ßa com todos os coeficientes do modelo iguais a zero e, iterativamente, adiciona vari√°veis ao modelo, seguindo os seguintes passos [^32]:
1.  Identifica a vari√°vel preditora $x_j$ que est√° mais correlacionada com o res√≠duo atual. Ou seja, encontra o √≠ndice $j$ que maximiza a correla√ß√£o:
    $$
    j = \arg \max_{i} |x_i^T r|.
    $$
2.   Move o coeficiente $\beta_j$ (da vari√°vel $x_j$) na dire√ß√£o do sinal dessa correla√ß√£o, at√© que outra vari√°vel preditora ($x_k$) se torne igualmente correlacionada com o res√≠duo. Formalmente, o coeficiente √© atualizado de forma incremental $\beta_j := \beta_j + \alpha \, sign(x_j^Tr)$.
3.  Move os coeficientes das vari√°veis selecionadas ($x_j$ e $x_k$) de forma conjunta, na dire√ß√£o que minimiza o res√≠duo (RSS), at√© que uma nova vari√°vel preditora se torne igualmente correlacionada com o res√≠duo.
4.  O algoritmo prossegue dessa forma, adicionando vari√°veis ao modelo e movendo seus coeficientes em conjunto, at√© que todas as vari√°veis tenham sido adicionadas ou a condi√ß√£o de parada (correla√ß√£o nula) seja satisfeita.

Uma caracter√≠stica fundamental do LARS √© que as vari√°veis s√£o inclu√≠das ao modelo de forma gradual, e os seus coeficientes s√£o movidos em conjunto, o que √© distinto de um m√©todo guloso como o Forward Stepwise. O algoritmo LARS tamb√©m permite uma interpreta√ß√£o geom√©trica que demonstra que ele sempre move os coeficientes na dire√ß√£o que mant√©m as vari√°veis selecionadas igualmente correlacionadas com o res√≠duo [^32].

> üí° **Exemplo Num√©rico:**
> Vamos considerar um exemplo com tr√™s vari√°veis preditoras ($x_1$, $x_2$, e $x_3$) e uma vari√°vel resposta $y$. Inicialmente, todos os coeficientes s√£o zero, e o res√≠duo √© $r = y$.
>
> **Passo 1:** Suponha que $x_1$ tenha a maior correla√ß√£o com $r$. Ent√£o, $\beta_1$ come√ßa a crescer na dire√ß√£o do sinal da correla√ß√£o.
>
> **Passo 2:** √Ä medida que $\beta_1$ cresce, a correla√ß√£o de $x_1$ com o res√≠duo diminui. Suponha que, em algum ponto, a correla√ß√£o de $x_2$ com o res√≠duo se iguale √† de $x_1$.
>
> **Passo 3:** Agora, LARS move $\beta_1$ e $\beta_2$ juntos, de forma que ambas as vari√°veis permane√ßam igualmente correlacionadas com o res√≠duo.
>
> **Passo 4:** Se em algum momento, a correla√ß√£o de $x_3$ com o res√≠duo se iguala a de $x_1$ e $x_2$, os tr√™s coeficientes s√£o movidos juntos, e assim por diante.
>
> Este processo continua at√© que todas as vari√°veis tenham sido adicionadas ao modelo ou a condi√ß√£o de parada seja alcan√ßada.

**Lemma 1:** O algoritmo LARS gera o caminho de solu√ß√µes do LASSO movendo os coeficientes de forma incremental, de modo que a correla√ß√£o das vari√°veis selecionadas com o res√≠duo seja igual em cada etapa do algoritmo [^36].
**Prova do Lemma 1:** Durante o algoritmo LARS, a dire√ß√£o dos coeficientes √© escolhida para manter as vari√°veis preditoras no conjunto ativo igualmente correlacionadas com o res√≠duo atual. Formalmente, o movimento dos coeficientes segue uma dire√ß√£o $\delta = (X_A^T X_A)^{-1} X_A^T r$, que √© a dire√ß√£o que faz com que o res√≠duo se torne ortogonal as vari√°veis selecionadas. $\blacksquare$

**Corol√°rio 1:** O algoritmo LARS fornece um caminho completo de solu√ß√µes para a regress√£o Lasso, onde cada ponto do caminho corresponde a uma solu√ß√£o diferente do Lasso, conforme o par√¢metro de regulariza√ß√£o $\lambda$ √© variado. Ao longo do caminho, os coeficientes s√£o movidos de forma gradual e novas vari√°veis s√£o inclu√≠das no modelo [^38].

O algoritmo LARS possui uma conex√£o direta com o problema do LASSO [^31]. As solu√ß√µes computadas atrav√©s do algoritmo LARS correspondem √†s solu√ß√µes √≥timas do problema LASSO para valores espec√≠ficos do par√¢metro de regulariza√ß√£o $\lambda$. O algoritmo fornece o caminho completo das solu√ß√µes do LASSO sem a necessidade de resolver o problema de otimiza√ß√£o para cada valor de $\lambda$. O m√©todo LARS tamb√©m √© um m√©todo eficiente para identificar o ponto em que os coeficientes do Lasso atingem o valor zero, o que n√£o √© direto na defini√ß√£o da penalidade L1 [^32].

> üí° **Exemplo Num√©rico:**
> Suponha que, ao aplicar o LARS, a primeira vari√°vel a entrar no modelo seja $x_1$, com um coeficiente $\beta_1$ que cresce at√© um certo ponto. Depois, $x_2$ entra e $\beta_1$ e $\beta_2$ crescem juntos. Em algum ponto, $\beta_2$ atinge zero, o que significa que, para um certo valor de $\lambda$, $x_2$ deixa de ser relevante. O LARS nos mostra exatamente em qual valor de $\lambda$ isso acontece, permitindo que selecionemos o modelo LASSO com a complexidade desejada.

### Varia√ß√µes do LARS: Forward Stagewise Regression

```mermaid
    flowchart TD
    A[Start] --> B{"Find most correlated variable with residual"};
    B --> C{"Compute coefficient for this variable"};
    C --> D{"Update Residual"};
    D --> E{"Is stopping criterion met?"};
    E -- No --> B;
    E -- Yes --> F[End];
    style C fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3,4 stroke-width:2px
```

O **Forward Stagewise Regression (FS)** √© uma vers√£o ainda mais restrita do Forward Stepwise [^59]. Ele tamb√©m come√ßa com um modelo sem vari√°veis e adiciona uma vari√°vel preditora por vez, baseando-se na maior correla√ß√£o com o res√≠duo, como no LARS [^60].
A diferen√ßa crucial entre o LARS e o Forward Stagewise √© que no LARS os coeficientes das vari√°veis s√£o movidos em conjunto, enquanto no Forward Stagewise apenas o coeficiente da vari√°vel rec√©m-selecionada √© movido, enquanto os outros coeficientes s√£o mantidos fixos em sua itera√ß√£o anterior [^60]. Este comportamento mais cauteloso e incremental no Forward Stagewise o torna menos eficiente do que o LARS, necessitando de um maior n√∫mero de itera√ß√µes para atingir o mesmo n√≠vel de ajuste aos dados [^60].

De maneira mais formal, o Forward Stagewise Regression pode ser descrito como [^60]:
1.  Come√ßa com um modelo com todos os coeficientes iguais a zero e um res√≠duo igual a y - y.
2.  Encontra a vari√°vel preditora mais correlacionada com o res√≠duo atual.
3.  Calcula o coeficiente da vari√°vel escolhida atrav√©s de uma regress√£o linear simples do res√≠duo sobre a vari√°vel escolhida.
4. Atualiza o res√≠duo de acordo com o novo coeficiente.
5. Repete os passos 2-4 at√© atingir a condi√ß√£o de parada.

Em compara√ß√£o com o LARS, o Forward Stagewise √© um procedimento mais lento, pois ele ajusta apenas um coeficiente por vez, e o processo de busca pelas vari√°veis mais correlacionadas com o res√≠duo atual precisa ser repetido em cada itera√ß√£o, sem levar em considera√ß√£o a informa√ß√£o obtida nas itera√ß√µes anteriores [^32]. No entanto, o Forward Stagewise tamb√©m possui uma propriedade importante: ele gera um caminho de solu√ß√µes que se aproxima da solu√ß√£o de m√≠nimos quadrados de maneira gradual, o que pode ser interessante em situa√ß√µes com muitas vari√°veis preditoras [^60].
*  A principal diferen√ßa entre LARS e Forward Stagewise √© que o LARS move o coeficiente e a dire√ß√£o de busca de forma conjunta, enquanto o Forward Stagewise s√≥ move o coeficiente da vari√°vel rec√©m adicionada [^59].
* A inclus√£o de uma vari√°vel no Forward Stagewise n√£o considera, a influ√™ncia das outras vari√°veis selecionadas nas itera√ß√µes anteriores. Isto o torna computacionalmente mais simples, mas menos eficiente na escolha da melhor dire√ß√£o de busca [^32].

> üí° **Exemplo Num√©rico:**
> Consideremos novamente as vari√°veis $x_1$, $x_2$ e $x_3$.
>
> **Passo 1:** O FS encontra a vari√°vel mais correlacionada com o res√≠duo inicial (digamos, $x_1$) e calcula seu coeficiente $\beta_1$ por regress√£o linear simples.
>
> **Passo 2:** O res√≠duo √© atualizado, mas o FS mant√©m $\beta_1$ fixo. Ele ent√£o encontra a pr√≥xima vari√°vel mais correlacionada com o novo res√≠duo (digamos, $x_2$) e calcula seu coeficiente $\beta_2$.
>
> **Passo 3:** O processo continua, adicionando uma vari√°vel e ajustando apenas seu coeficiente a cada passo, sem ajustar os coeficientes de vari√°veis previamente adicionadas.
>
> Este processo √© mais lento porque n√£o ajusta todos os coeficientes em conjunto, mas pode ser mais est√°vel em algumas situa√ß√µes.

> ‚ö†Ô∏è **Ponto Crucial**: O Forward Stagewise Regression √© menos eficiente do que o LARS, j√° que cada etapa ajusta apenas uma vari√°vel, levando a um caminho de solu√ß√£o mais lento, mas tamb√©m mais cauteloso e menos propenso a sobreajuste [^60].

**Lemma 2**: O Forward Stagewise Regression converge para a solu√ß√£o de m√≠nimos quadrados, mas o faz por meio de etapas incrementais e em n√∫mero de itera√ß√µes que podem ser muito maiores do que o n√∫mero de vari√°veis preditoras [^60].
**Prova do Lemma 2:** Na abordagem Forward Stagewise, os coeficientes s√£o atualizados de forma incremental, o que garante a converg√™ncia ao √≥timo. No entanto, as vari√°veis s√£o movidas em etapas isoladas. A cada etapa, somente o coeficiente da vari√°vel mais correlacionada com o res√≠duo √© movido, o que pode levar a uma converg√™ncia mais lenta em dire√ß√£o √† solu√ß√£o de m√≠nimos quadrados [^60]. $\blacksquare$

**Corol√°rio 2:** Embora menos eficiente computacionalmente, o Forward Stagewise Regression pode ser √∫til em situa√ß√µes em que a suavidade e a controlabilidade da trajet√≥ria de solu√ß√µes s√£o mais importantes que a velocidade da converg√™ncia, ou em problemas de alta dimensionalidade [^60].

### Pergunta Te√≥rica Avan√ßada (Exemplo): Como a conex√£o entre LARS e LASSO √© explorada na pr√°tica para otimiza√ß√£o e sele√ß√£o de modelos?
**Resposta:**

A conex√£o entre o algoritmo LARS e o problema de otimiza√ß√£o do LASSO √© fundamental para a compreens√£o da sua import√¢ncia pr√°tica. O LARS [^31], n√£o √© um algoritmo de otimiza√ß√£o direta para o LASSO; ele n√£o encontra a solu√ß√£o para um valor espec√≠fico de $\lambda$. Em vez disso, o LARS calcula o caminho completo de solu√ß√µes para o LASSO, ou seja, encontra um conjunto de modelos que correspondem a diferentes valores de $\lambda$.

Na pr√°tica, o LARS permite uma explora√ß√£o eficiente e completa do espa√ßo de par√¢metros do LASSO. Este caminho de solu√ß√µes, calculado pelo algoritmo LARS, permite selecionar o valor de $\lambda$ que fornece o modelo com o melhor desempenho preditivo atrav√©s de *cross-validation* ou outros crit√©rios [^60]. Ou seja, usando o LARS computamos todo o conjunto de modelos correspondente a diferentes valores de lambda e depois escolhemos aquele com melhor performance. O LARS n√£o computa o valor correto de $\lambda$ que otimiza o modelo para cada situa√ß√£o.

Al√©m disso, a efici√™ncia computacional do algoritmo LARS permite que ele seja usado na pr√°tica para problemas de alta dimensionalidade [^60]. A combina√ß√£o de LARS e m√©todos de avalia√ß√£o como o *cross-validation* oferece uma forma robusta para sele√ß√£o de vari√°veis, ao mesmo tempo em que explora o tradeoff entre bias e vari√¢ncia.
*   O LARS n√£o apenas calcula a solu√ß√£o para um determinado $\lambda$, mas tamb√©m oferece um caminho completo de solu√ß√µes para todos os valores poss√≠veis de $\lambda$.

* A conex√£o com o LASSO torna o LARS √∫til para problemas que exigem solu√ß√µes esparsas e a identifica√ß√£o das vari√°veis mais relevantes.

* O caminho de solu√ß√µes permite ao praticante avaliar a sensibilidade do modelo a diferentes intensidades de regulariza√ß√£o.

> üí° **Exemplo Num√©rico:**
> Suponha que aplicamos o LARS em um conjunto de dados e obtemos o caminho de solu√ß√µes. Para cada ponto nesse caminho (que corresponde a um valor diferente de $\lambda$), podemos calcular o erro de valida√ß√£o cruzada. Podemos ent√£o selecionar o valor de $\lambda$ que minimiza o erro de valida√ß√£o cruzada, que nos dar√° um modelo LASSO com bom desempenho de generaliza√ß√£o. O LARS torna essa busca muito mais eficiente, pois calcula todos os modelos ao longo do caminho em vez de treinar modelos Lasso separados para cada valor de $\lambda$.

**Lemma 3:** O algoritmo LARS, por produzir o caminho de solu√ß√µes do LASSO, permite uma sele√ß√£o do par√¢metro $\lambda$  e a explora√ß√£o do tradeoff bias-vari√¢ncia atrav√©s de m√©todos como *cross-validation*
**Prova do Lemma 3:** O algoritmo LARS produz uma sequ√™ncia de modelos Lasso para diferentes valores do par√¢metro de regulariza√ß√£o $\lambda$, a partir do conhecimento das correla√ß√µes entre as vari√°veis preditoras e o res√≠duo. Estes modelos variam em sua complexidade (n√∫mero de coeficientes n√£o zero), o que permite avaliar e comparar seu desempenho atrav√©s de m√©tricas como o MSE (erro quadr√°tico m√©dio). Atrav√©s da *cross-validation*, o desempenho desses modelos √© estimado em conjuntos de dados n√£o usados para treinamento, o que permite determinar o valor de $\lambda$ (e, portanto, o modelo) que fornece melhor generaliza√ß√£o, e tamb√©m balanceamento entre *bias* e vari√¢ncia. $\blacksquare$

**Corol√°rio 3:**  A efici√™ncia do LARS permite selecionar o modelo adequado em um amplo conjunto de modelos obtidos pela varia√ß√£o do par√¢metro de regulariza√ß√£o, explorando o tradeoff entre ajuste aos dados e complexidade do modelo [^60].
