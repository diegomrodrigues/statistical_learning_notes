## Formula√ß√£o Matem√°tica: RSS(Œ≤) = ||y - XŒ≤||¬≤ em Modelos de Regress√£o Linear

```mermaid
graph LR
    A["Vetor de Respostas 'y'"] --> B["Matriz de Design 'X'"];
    B --> C["Vetor de Par√¢metros 'Œ≤'"];
    C --> D["Vetor de Predi√ß√µes 'XŒ≤'"];
    A --> E["Vetor de Res√≠duos 'y - XŒ≤'"];
    D --> E;
    E --> F["RSS(Œ≤) = ||y - XŒ≤||¬≤"];
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ddf,stroke:#333,stroke-width:2px
    style C fill:#eef,stroke:#333,stroke-width:2px
    style D fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#afa,stroke:#333,stroke-width:2px
    style F fill:#faa,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

A formula√ß√£o matem√°tica da **Soma dos Quadrados dos Res√≠duos (RSS)** como $RSS(\beta) = ||y - X\beta||^2$ √© fundamental na teoria e pr√°tica da regress√£o linear. Esta formula√ß√£o expressa o objetivo central da regress√£o linear por m√≠nimos quadrados, que consiste em encontrar os par√¢metros $\beta$ que minimizam a norma euclidiana do vetor de res√≠duos. Esta express√£o resume os conceitos matem√°ticos da regress√£o linear, usando os conceitos de √°lgebra linear. Nesta se√ß√£o, vamos explorar cada um dos componentes dessa formula√ß√£o, sua interpreta√ß√£o geom√©trica, e o porqu√™ dela ser a base do m√©todo dos m√≠nimos quadrados.

### Componentes da Formula√ß√£o RSS(Œ≤) = ||y - XŒ≤||¬≤

A formula√ß√£o $RSS(\beta) = ||y - X\beta||^2$ √© uma representa√ß√£o concisa da fun√ß√£o de custo utilizada na regress√£o linear. Para entend√™-la por completo, precisamos detalhar cada um dos seus componentes:

1.  **y:** O vetor $y \in \mathbb{R}^N$ representa o vetor de respostas ou valores observados da vari√°vel dependente. Cada elemento $y_i$ do vetor corresponde ao valor observado da vari√°vel resposta para a $i$-√©sima observa√ß√£o [^10]. A dimens√£o $N$ do vetor √© o n√∫mero de observa√ß√µes.

> üí° **Exemplo Num√©rico:**
> Suponha que temos um conjunto de dados com 4 observa√ß√µes da vari√°vel dependente, onde os valores observados s√£o $y_1 = 5$, $y_2 = 8$, $y_3 = 12$, e $y_4 = 15$. O vetor $y$ seria representado como:
>
> $y = \begin{bmatrix} 5 \\ 8 \\ 12 \\ 15 \end{bmatrix}$

2.  **X:** A matriz $X \in \mathbb{R}^{N \times (p+1)}$ √© a matriz de design, que cont√©m as informa√ß√µes sobre os preditores para cada observa√ß√£o [^11]. A matriz X √© organizada da seguinte forma:
$$
X=
\begin{bmatrix}
    1 & x_{11} & x_{12} & \ldots & x_{1p} \\
    1 & x_{21} & x_{22} & \ldots & x_{2p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_{N1} & x_{N2} & \ldots & x_{Np}
\end{bmatrix}
$$

- A primeira coluna √© composta por 1, que representam o *intercepto* do modelo, e servem para modelar uma m√©dia na resposta.
- As colunas seguintes cont√©m os valores dos preditores $x_j$, onde $x_{ij}$ √© o valor do j-√©simo preditor para a i-√©sima observa√ß√£o.
- A dimens√£o da matriz X √© $N \times (p+1)$, onde $N$ √© o n√∫mero de observa√ß√µes e $p$ √© o n√∫mero de preditores.

> üí° **Exemplo Num√©rico:**
> Considere que temos duas vari√°veis preditoras ($p=2$) e as mesmas 4 observa√ß√µes do exemplo anterior. Suponha que os valores dos preditores sejam:
>  - Para a primeira observa√ß√£o: $x_{11} = 2$ e $x_{12} = 3$
>  - Para a segunda observa√ß√£o: $x_{21} = 4$ e $x_{22} = 5$
>  - Para a terceira observa√ß√£o: $x_{31} = 6$ e $x_{32} = 7$
>  - Para a quarta observa√ß√£o: $x_{41} = 8$ e $x_{42} = 9$
> A matriz $X$ seria:
>
> $X = \begin{bmatrix} 1 & 2 & 3 \\ 1 & 4 & 5 \\ 1 & 6 & 7 \\ 1 & 8 & 9 \end{bmatrix}$

3.  **Œ≤:** O vetor $\beta \in \mathbb{R}^{p+1}$ representa o vetor de par√¢metros ou coeficientes do modelo linear [^11]. O vetor de par√¢metros, que deve ser determinado, √© da forma:

$$
\beta=
\begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    \vdots \\
    \beta_p
\end{bmatrix}
$$

   - $\beta_0$ √© o coeficiente do *intercept* do modelo, e $\beta_j$ s√£o os coeficientes dos preditores $x_j$.

> üí° **Exemplo Num√©rico:**
> No nosso exemplo com dois preditores, o vetor $\beta$ teria tr√™s elementos: $\beta_0$ (intercepto), $\beta_1$ (coeficiente para o primeiro preditor), e $\beta_2$ (coeficiente para o segundo preditor). Suponha que, ap√≥s a regress√£o, encontramos:
>
> $\beta = \begin{bmatrix} 1 \\ 0.5 \\ 1.2 \end{bmatrix}$
> Isso significa que o modelo linear ajustado √© $\hat{y} = 1 + 0.5x_1 + 1.2x_2$

4.  **XŒ≤:** O produto $X\beta$ representa o vetor de predi√ß√µes do modelo, ou seja, os valores preditos da vari√°vel dependente. O resultado do produto √© um vetor em $\mathbb{R}^N$, obtido por:

$$
X\beta=
\begin{bmatrix}
    \beta_0 + \sum_{j=1}^p x_{1j} \beta_j\\
    \beta_0 + \sum_{j=1}^p x_{2j} \beta_j\\
    \vdots\\
     \beta_0 + \sum_{j=1}^p x_{Nj} \beta_j
\end{bmatrix}
$$

onde cada elemento corresponde ao valor predito $\hat{y_i}$ para cada observa√ß√£o $i$.

> üí° **Exemplo Num√©rico:**
> Usando os valores de $X$ e $\beta$ dos exemplos anteriores, podemos calcular o vetor de predi√ß√µes $X\beta$:
>
> $X\beta = \begin{bmatrix} 1 & 2 & 3 \\ 1 & 4 & 5 \\ 1 & 6 & 7 \\ 1 & 8 & 9 \end{bmatrix} \begin{bmatrix} 1 \\ 0.5 \\ 1.2 \end{bmatrix} = \begin{bmatrix} 1 + 2(0.5) + 3(1.2) \\ 1 + 4(0.5) + 5(1.2) \\ 1 + 6(0.5) + 7(1.2) \\ 1 + 8(0.5) + 9(1.2) \end{bmatrix} = \begin{bmatrix} 5.6 \\ 9 \\ 12.4 \\ 15.8 \end{bmatrix}$
>
> Portanto, os valores preditos s√£o $\hat{y}_1 = 5.6$, $\hat{y}_2 = 9$, $\hat{y}_3 = 12.4$, e $\hat{y}_4 = 15.8$.

5. **y - XŒ≤:** O termo $(y - X\beta)$ representa o vetor de res√≠duos $r$, onde cada elemento $r_i$ corresponde ao res√≠duo (diferen√ßa entre valor observado e valor predito) para a i-√©sima observa√ß√£o.
$$
r = y - X\beta =
\begin{bmatrix}
    y_1 - (\beta_0 + \sum_{j=1}^p x_{1j} \beta_j) \\
    y_2 - (\beta_0 + \sum_{j=1}^p x_{2j} \beta_j) \\
    \vdots \\
    y_N - (\beta_0 + \sum_{j=1}^p x_{Nj} \beta_j)
\end{bmatrix}
$$

> üí° **Exemplo Num√©rico:**
> Calculando os res√≠duos usando o vetor $y$ original e o vetor de predi√ß√µes $X\beta$:
>
> $r = y - X\beta = \begin{bmatrix} 5 \\ 8 \\ 12 \\ 15 \end{bmatrix} - \begin{bmatrix} 5.6 \\ 9 \\ 12.4 \\ 15.8 \end{bmatrix} = \begin{bmatrix} -0.6 \\ -1 \\ -0.4 \\ -0.8 \end{bmatrix}$
>
> Os res√≠duos s√£o $r_1 = -0.6$, $r_2 = -1$, $r_3 = -0.4$, e $r_4 = -0.8$.

6.  **||y - XŒ≤||¬≤:** O termo $||y - X\beta||^2$ representa o quadrado da norma euclidiana (norma L2) do vetor de res√≠duos, que corresponde √† soma dos quadrados dos res√≠duos. Esta norma √© dada por:
$$
||y - X\beta||^2 =  \sum_{i=1}^N (y_i - x_i^T\beta)^2
$$

> üí° **Exemplo Num√©rico:**
> Calculando a soma dos quadrados dos res√≠duos (RSS) usando os res√≠duos calculados anteriormente:
>
> $RSS(\beta) = ||y - X\beta||^2 = (-0.6)^2 + (-1)^2 + (-0.4)^2 + (-0.8)^2 = 0.36 + 1 + 0.16 + 0.64 = 2.16$
>
> O valor de RSS √© 2.16 para este conjunto de dados e par√¢metros $\beta$. O objetivo da regress√£o linear √© encontrar os valores de $\beta$ que minimizam este valor.

A fun√ß√£o de custo $RSS(\beta)$ √© a soma dos quadrados dos res√≠duos, e o objetivo do m√©todo de m√≠nimos quadrados √© encontrar o vetor $\beta$ que minimize esta fun√ß√£o de custo.

### Interpreta√ß√£o Geom√©trica da Formula√ß√£o

A formula√ß√£o $RSS(\beta) = ||y - X\beta||^2$ tem uma interpreta√ß√£o geom√©trica importante [^12]:

1.  **Espa√ßo dos Dados:** O vetor de respostas $y$ e o vetor de predi√ß√µes $X\beta$ s√£o pontos no espa√ßo dos dados, onde cada dimens√£o corresponde a uma observa√ß√£o.
2.  **Proje√ß√£o Ortogonal:** A solu√ß√£o de m√≠nimos quadrados $\hat{\beta}$ minimiza a dist√¢ncia euclidiana (a norma L2) entre o vetor $y$ e o espa√ßo gerado pelas colunas da matriz $X$. Geometricamente, isso corresponde a encontrar a proje√ß√£o ortogonal do vetor $y$ no subespa√ßo gerado pelas colunas de $X$.
3.  **Res√≠duos:** O vetor $y-X\hat{\beta}$ √© o vetor de res√≠duos que representa a diferen√ßa entre o valor observado e a sua proje√ß√£o no espa√ßo dos preditores. Este vetor √© ortogonal a todos os vetores neste espa√ßo.
4.  **Dist√¢ncia Euclidiana:** Minimizar $||y - X\beta||^2$ equivale a minimizar o quadrado da dist√¢ncia euclidiana entre o vetor de respostas e o vetor de predi√ß√µes, que √© a proje√ß√£o de y no espa√ßo de preditores.

> üí° **Exemplo Num√©rico:**
> Imagine um espa√ßo 2D onde o vetor $y$ √© um ponto. As colunas da matriz $X$ definem um subespa√ßo (uma linha, neste caso). A regress√£o linear busca o ponto nesse subespa√ßo que est√° mais pr√≥ximo de $y$. O vetor de res√≠duos √© a linha que conecta o ponto $y$ √† sua proje√ß√£o no subespa√ßo, e esse vetor √© perpendicular (ortogonal) ao subespa√ßo.
>
> ```mermaid
> graph LR
>     A(Espa√ßo dos Preditores) --> B(Proje√ß√£o de y);
>     C(y) --> B;
>     C --> D(Res√≠duos);
>     style D fill:#f9f,stroke:#333,stroke-width:2px
>     style B fill:#ccf,stroke:#333,stroke-width:2px
>     style A fill:#ddf,stroke:#333,stroke-width:2px
> ```

A interpreta√ß√£o geom√©trica nos ajuda a entender que o objetivo do m√©todo dos m√≠nimos quadrados √© encontrar o ponto mais pr√≥ximo do vetor de respostas no espa√ßo gerado pelos preditores, no sentido da dist√¢ncia euclidiana.

**Lemma 16:** Convexidade da Fun√ß√£o RSS

A fun√ß√£o de custo RSS √© uma fun√ß√£o convexa em rela√ß√£o aos par√¢metros $\beta$. Esta propriedade implica que a solu√ß√£o do problema de otimiza√ß√£o de m√≠nimos quadrados (se ela existir) √© um m√≠nimo global, e n√£o apenas um m√≠nimo local.

**Prova do Lemma 16:**

A fun√ß√£o RSS pode ser escrita como:

$$
RSS(\beta) = (y - X\beta)^T (y - X\beta)
$$
$$
RSS(\beta) = y^T y - 2 y^T X\beta + \beta^T X^T X \beta
$$

Para mostrar que a fun√ß√£o √© convexa, √© necess√°rio mostrar que a sua matriz Hessiana (matriz das segundas derivadas) √© semi-definida positiva. A matriz Hessiana √© dada por:
$$ H = \frac{\partial^2 RSS}{\partial \beta \partial \beta^T} = 2 X^T X $$
Dado que $X^T X$ √© sempre semi-definida positiva, j√° que  $z^T X^T X z= ||Xz||^2 \ge 0$, para qualquer vetor $z$, a fun√ß√£o RSS √© convexa. $\blacksquare$

**Corol√°rio 16:** Unicidade e Estabilidade da Solu√ß√£o

A convexidade da fun√ß√£o de custo implica que a solu√ß√£o do problema de m√≠nimos quadrados, se existir, √© √∫nica, e tamb√©m garante que a solu√ß√£o seja global e n√£o um m√≠nimo local. Al√©m disso, em muitos casos, esta solu√ß√£o corresponde a uma matriz sim√©trica e positiva definida.
A garantia de que a solu√ß√£o de m√≠nimos quadrados √© um m√≠nimo global, garante que a busca pela solu√ß√£o possa ser feita atrav√©s de m√©todos iterativos ou diretos que levam √† solu√ß√£o √≥tima.

### Implica√ß√µes da Formula√ß√£o RSS(Œ≤) = ||y - XŒ≤||¬≤

A formula√ß√£o matem√°tica da RSS como o quadrado da norma euclidiana do vetor de res√≠duos tem implica√ß√µes cruciais:

1.  **Otimiza√ß√£o:** A formula√ß√£o nos mostra que a regress√£o linear por m√≠nimos quadrados √© um problema de otimiza√ß√£o onde o objetivo √© encontrar os par√¢metros que minimizam o erro quadr√°tico.
2.  **Geometria:** A formula√ß√£o nos permite visualizar a regress√£o linear como um problema geom√©trico de proje√ß√£o ortogonal, onde o vetor de predi√ß√µes √© a proje√ß√£o do vetor de resposta no subespa√ßo gerado pelos preditores.
3.  **C√°lculo e An√°lise:**  A formula√ß√£o matem√°tica permite derivar uma solu√ß√£o anal√≠tica para o problema de m√≠nimos quadrados. As propriedades dos m√©todos de estima√ß√£o podem ser analisadas a partir das propriedades matem√°ticas dos par√¢metros.
4.  **Penaliza√ß√£o e Regulariza√ß√£o:** A formula√ß√£o da RSS √© uma base para m√©todos de regulariza√ß√£o, onde a fun√ß√£o de custo √© modificada ao incluir termos adicionais que penalizam a magnitude dos par√¢metros ou a sua complexidade.

> üí° **Exemplo Num√©rico:**
> Na regulariza√ß√£o Ridge, por exemplo, adicionamos um termo √† fun√ß√£o RSS, que penaliza valores grandes para $\beta$. A nova fun√ß√£o de custo seria $RSS_{ridge}(\beta) = ||y - X\beta||^2 + \lambda ||\beta||^2$, onde $\lambda$ √© um par√¢metro de ajuste. Isso ilustra como a formula√ß√£o da RSS serve como base para outros m√©todos.
>
> O termo $\lambda ||\beta||^2$ adicionado penaliza a magnitude dos par√¢metros, e induz um modelo com par√¢metros menores, e que seja menos sens√≠vel a varia√ß√µes nos dados de treino.
>
```mermaid
graph LR
    A["RSS(Œ≤) = ||y - XŒ≤||¬≤"] --> B("Otimiza√ß√£o");
    A --> C("Geometria (Proje√ß√£o Ortogonal)");
    A --> D("C√°lculo e An√°lise");
    A --> E("Base para Regulariza√ß√£o");
    style A fill:#afa,stroke:#333,stroke-width:2px
```
A minimiza√ß√£o da soma dos quadrados dos res√≠duos √© uma pr√°tica que est√° enraizada na teoria da estat√≠stica, e tamb√©m nos m√©todos da √°lgebra linear.

> ‚ö†Ô∏è **Nota Importante**: A formula√ß√£o $RSS(\beta) = ||y - X\beta||^2$ expressa o objetivo de minimizar a norma euclidiana do vetor de res√≠duos.

> ‚ùó **Ponto de Aten√ß√£o**: A solu√ß√£o de m√≠nimos quadrados √© obtida minimizando esta fun√ß√£o de custo, e corresponde √† proje√ß√£o ortogonal do vetor de resposta no subespa√ßo dos preditores.

> ‚úîÔ∏è **Destaque**: A fun√ß√£o RSS √© convexa, o que garante que a solu√ß√£o, se existir, seja um m√≠nimo global.

### Conclus√£o

A formula√ß√£o matem√°tica da soma dos quadrados dos res√≠duos, $RSS(\beta) = ||y - X\beta||^2$, fornece uma base s√≥lida e concisa para o estudo e compreens√£o da regress√£o linear. Esta formula√ß√£o combina conceitos de √°lgebra linear e estat√≠stica, e tem diversas implica√ß√µes pr√°ticas. O entendimento de cada um dos componentes da equa√ß√£o, a sua interpreta√ß√£o geom√©trica, e a convexidade da fun√ß√£o s√£o aspectos que ajudam a usar a regress√£o linear de forma eficaz e a aplicar t√©cnicas de otimiza√ß√£o e regulariza√ß√£o.

### Refer√™ncias

[^10]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Methods for Regression)*
[^11]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^12]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Methods for Regression)*
