## Distribui√ß√£o dos Estimadores de Par√¢metros: $\beta \sim N(\beta, (X^TX)^{-1}\sigma^2)$ em Modelos de Regress√£o Linear

```mermaid
flowchart LR
    A["Data (X, y)"] --> B{Least Squares Estimation};
    B --> C["$\hat{\beta} = (X^TX)^{-1}X^Ty$"];
    C --> D["$\hat{\beta} \sim N(\beta, (X^TX)^{-1}\sigma^2)$"];
    D --> E["Statistical Inference (Hypothesis tests, Confidence Intervals)"];
    style B fill:#f9f,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Em modelos de regress√£o linear, a distribui√ß√£o dos estimadores de par√¢metros, dada por $\beta \sim N(\beta, (X^TX)^{-1}\sigma^2)$, √© uma propriedade fundamental que permite realizar infer√™ncias estat√≠sticas sobre os coeficientes do modelo. Esta express√£o descreve como os estimadores de par√¢metros $\hat{\beta}$, obtidos por m√≠nimos quadrados, se distribuem em torno dos seus valores verdadeiros $\beta$, e √© a base para testes de hip√≥teses, constru√ß√£o de intervalos de confian√ßa e avalia√ß√£o da incerteza associada aos par√¢metros do modelo. Nesta se√ß√£o, exploraremos em detalhes cada componente dessa distribui√ß√£o, sua deriva√ß√£o e sua import√¢ncia na an√°lise da regress√£o linear.

### A Distribui√ß√£o dos Estimadores de Par√¢metros

Em um modelo de regress√£o linear, a suposi√ß√£o de que os erros aleat√≥rios $\epsilon_i$ seguem uma distribui√ß√£o Normal com m√©dia zero e vari√¢ncia $\sigma^2$, ou seja, $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$, implica que os estimadores de par√¢metros, obtidos atrav√©s do m√©todo de m√≠nimos quadrados, seguem uma distribui√ß√£o Normal multidimensional:

$$
\hat{\beta} \sim \mathcal{N}(\beta, (X^T X)^{-1}\sigma^2)
$$

onde:

-   $\hat{\beta} \in \mathbb{R}^{p+1}$ √© o vetor dos estimadores de m√≠nimos quadrados dos par√¢metros.
-   $\beta \in \mathbb{R}^{p+1}$ √© o vetor dos valores verdadeiros dos par√¢metros do modelo (que usualmente s√£o desconhecidos).
-   $X \in \mathbb{R}^{N \times (p+1)}$ √© a matriz de design (com o intercepto e os preditores).
-   $(X^T X)^{-1}$ √© a inversa da matriz $X^T X$.
-   $\sigma^2$ √© a vari√¢ncia do erro aleat√≥rio.
-  $\mathcal{N}$ denota a distribui√ß√£o normal multidimensional.

Esta express√£o significa que o vetor de par√¢metros $\hat{\beta}$, quando estimado por m√≠nimos quadrados, tem uma distribui√ß√£o normal com m√©dia no valor verdadeiro $\beta$ e uma matriz de vari√¢ncia-covari√¢ncia dada por $(X^T X)^{-1}\sigma^2$. A distribui√ß√£o dos par√¢metros permite entender as propriedades dos par√¢metros estimados.

> üí° **Exemplo Num√©rico:**
> Suponha que temos um modelo de regress√£o linear com um preditor e um intercepto, onde o verdadeiro modelo √© $y = 2 + 3x + \epsilon$, com $\epsilon \sim \mathcal{N}(0, 0.5^2)$. Vamos gerar um conjunto de dados para ilustrar:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> np.random.seed(42)
> n_samples = 100
> X = np.column_stack((np.ones(n_samples), np.random.rand(n_samples) * 5))  # Add intercept
> true_beta = np.array([2, 3])
> sigma = 0.5
> epsilon = np.random.normal(0, sigma, n_samples)
> y = X @ true_beta + epsilon
>
> # Calculate beta_hat
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
>
> print("Estimated beta:", beta_hat)
> ```
>
> Neste exemplo, geramos dados a partir de um modelo linear com par√¢metros verdadeiros $\beta = [2, 3]$. O c√≥digo calcula $\hat{\beta}$ utilizando a f√≥rmula de m√≠nimos quadrados. Executando o c√≥digo, podemos observar que os valores de $\hat{\beta}$ s√£o pr√≥ximos dos valores verdadeiros, e que eles variam de uma amostra para outra, seguindo uma distribui√ß√£o normal multidimensional. A vari√¢ncia desta distribui√ß√£o √© dada por $(X^TX)^{-1}\sigma^2$, que pode ser estimada usando os dados.

### Deriva√ß√£o da Distribui√ß√£o dos Estimadores de Par√¢metros

A deriva√ß√£o da distribui√ß√£o dos estimadores de par√¢metros se baseia em dois resultados:

1. **Linearidade dos Estimadores de M√≠nimos Quadrados**:  Os estimadores de m√≠nimos quadrados s√£o uma combina√ß√£o linear das vari√°veis respostas $y$, o que resulta em:

$$
\hat{\beta} = (X^T X)^{-1} X^T y
$$
Substituindo a forma do modelo $y = X\beta + \epsilon$ na express√£o de $\hat{\beta}$ temos:
$$
\hat{\beta} = (X^T X)^{-1} X^T(X\beta+\epsilon)
$$
$$
\hat{\beta} = (X^T X)^{-1} X^T X\beta+(X^T X)^{-1}X^T\epsilon
$$
$$
\hat{\beta} = \beta+(X^T X)^{-1}X^T\epsilon
$$
2.  **Distribui√ß√£o dos Erros:** A suposi√ß√£o de que os erros t√™m distribui√ß√£o normal $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$ e que os erros s√£o independentes e identicamente distribu√≠dos garante que as combina√ß√µes lineares dos erros tamb√©m ter√£o uma distribui√ß√£o normal.

Combinando esses dois resultados, temos que o estimador $\hat{\beta}$ segue uma distribui√ß√£o normal multidimensional, com:

-   **M√©dia (Esperan√ßa):**
    $$
    E[\hat{\beta}] = E[ \beta+(X^T X)^{-1}X^T\epsilon] = \beta + (X^T X)^{-1}X^TE[\epsilon] = \beta
    $$
  j√° que a m√©dia dos erros √© zero, $E(\epsilon) = 0$.

-   **Matriz de Vari√¢ncia-Covari√¢ncia:**

```mermaid
flowchart LR
    A[Var($\hat{\beta}$)] --> B[Var($\beta + (X^TX)^{-1}X^T\epsilon$)];
    B --> C[Var($(X^TX)^{-1}X^T\epsilon$)];
    C --> D[$(X^TX)^{-1}X^T Var(\epsilon) ((X^TX)^{-1}X^T)^T$];
    D --> E[$(X^TX)^{-1}X^T (\sigma^2I) X(X^TX)^{-1}$];
    E --> F[$\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}$];
    F --> G[$\sigma^2(X^TX)^{-1}$];
    style A fill:#ccf,stroke:#333,stroke-width:2px
```

$$
\begin{aligned}
Var(\hat{\beta}) &= Var(\beta + (X^T X)^{-1} X^T \epsilon ) \\
&= Var((X^T X)^{-1} X^T \epsilon) \\
&= (X^T X)^{-1} X^T Var(\epsilon) ( (X^T X)^{-1} X^T )^T \\
&= (X^T X)^{-1} X^T (\sigma^2 I) X (X^T X)^{-1} \\
&=  \sigma^2 (X^T X)^{-1} X^T X (X^T X)^{-1} \\
&= \sigma^2 (X^T X)^{-1}
\end{aligned}
$$
o que confirma que $Var(\hat{\beta})=(X^TX)^{-1}\sigma^2$

Portanto, $\hat{\beta} \sim \mathcal{N}(\beta, (X^T X)^{-1}\sigma^2)$.

> üí° **Exemplo Num√©rico (Continua√ß√£o):**
>
> Podemos calcular a matriz de vari√¢ncia-covari√¢ncia estimada para o exemplo anterior:
>
> ```python
> # Calculate (X^T X)^-1
> xtx_inv = np.linalg.inv(X.T @ X)
>
> # Estimate sigma^2 using residuals
> y_hat = X @ beta_hat
> residuals = y - y_hat
> sigma_sq_hat = np.sum(residuals**2) / (n_samples - X.shape[1])
>
> # Calculate estimated variance-covariance matrix
> var_cov_beta_hat = sigma_sq_hat * xtx_inv
>
> print("Estimated Variance-Covariance Matrix of beta_hat:\n", var_cov_beta_hat)
> ```
>
> Este c√≥digo calcula $(X^TX)^{-1}$ e estima $\sigma^2$ a partir dos res√≠duos do modelo. A matriz de vari√¢ncia-covari√¢ncia estimada √© ent√£o calculada. Esta matriz mostra a vari√¢ncia de cada par√¢metro ($\hat{\beta}_0$ e $\hat{\beta}_1$) na diagonal e a covari√¢ncia entre eles fora da diagonal.
>
> A matriz de vari√¢ncia-covari√¢ncia estimada pode ser usada para calcular os erros padr√£o dos coeficientes:
>
> ```python
> std_errors = np.sqrt(np.diag(var_cov_beta_hat))
> print("Standard errors:", std_errors)
> ```
>
> Os erros padr√£o s√£o as ra√≠zes quadradas dos elementos diagonais da matriz de vari√¢ncia-covari√¢ncia e podem ser usados para construir intervalos de confian√ßa para os par√¢metros.

**Lemma 27:** Propriedades da Normal Multidimensional

A distribui√ß√£o normal multidimensional √© uma generaliza√ß√£o da distribui√ß√£o normal univariada para m√∫ltiplos par√¢metros [^10]. Se um vetor aleat√≥rio X segue uma distribui√ß√£o normal multidimensional, ent√£o qualquer combina√ß√£o linear de seus componentes tamb√©m segue uma distribui√ß√£o normal. Al√©m disso, a distribui√ß√£o √© completamente definida pelo seu vetor m√©dio e pela sua matriz de vari√¢ncia-covari√¢ncia.

**Prova do Lemma 27:**
A distribui√ß√£o normal multidimensional √© definida pela fun√ß√£o densidade:
$$ f(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)) $$
onde $\mu$ √© o vetor da m√©dia, e $\Sigma$ √© a matriz de covari√¢ncia.
As propriedades b√°sicas da distribui√ß√£o normal s√£o, em geral, conservadas na distribui√ß√£o multidimensional.  Um resultado √∫til √© o de que transforma√ß√µes lineares de vetores com distribui√ß√£o normal multidimensional tamb√©m seguem uma distribui√ß√£o normal multidimensional. $\blacksquare$

**Corol√°rio 27:** Utiliza√ß√£o da distribui√ß√£o Normal

A distribui√ß√£o normal multidimensional √© fundamental para infer√™ncia estat√≠stica. As propriedades da normal podem ser usadas para definir testes de hip√≥teses, e intervalos de confian√ßa, al√©m de outros procedimentos. Em particular, as margens de erro podem ser computadas utilizando a vari√¢ncia dos estimadores e propriedades da normal.

###  Interpreta√ß√£o Geom√©trica da Distribui√ß√£o dos Par√¢metros

```mermaid
flowchart LR
    A[Variance-Covariance Matrix $(X^TX)^{-1}\sigma^2$] --> B[Defines Ellipsoids];
    B --> C[Ellipsoid Shape (Eigenvalues/Vectors)];
    C --> D[Parameter Correlation (Off-diagonal elements)];
    D --> E[Collinearity Influence (Condition Number of $X^TX$)];
    E --> F[Parameter Uncertainty/Stability];
    style A fill:#aaf,stroke:#333,stroke-width:2px
```

A distribui√ß√£o dos estimadores de par√¢metros tem uma interpreta√ß√£o geom√©trica no espa√ßo dos par√¢metros:
    1.  **Elips√≥ides de Confian√ßa:** A matriz de vari√¢ncia-covari√¢ncia, $(X^T X)^{-1}\sigma^2$, define elips√≥ides de confian√ßa no espa√ßo dos par√¢metros. Estes elips√≥ides representam regi√µes onde os verdadeiros valores dos par√¢metros s√£o prov√°veis de estarem, a um certo n√≠vel de confian√ßa.
    2.  **Forma do Elips√≥ide:** A forma e o tamanho dos elips√≥ides s√£o determinadas pelos autovalores e autovetores da matriz de vari√¢ncia-covari√¢ncia, $(X^TX)^{-1}$, e pela vari√¢ncia do erro, $\sigma^2$. Autovalores maiores indicam maior incerteza na dire√ß√£o dos autovetores correspondentes.
    3.  **Correla√ß√£o entre Par√¢metros:** Os elementos fora da diagonal da matriz $(X^T X)^{-1}\sigma^2$ quantificam a correla√ß√£o entre os par√¢metros.
    4. **Influ√™ncia da Colinearidade**: Se os preditores s√£o colineares, a matriz $(X^T X)$ tem um n√∫mero de condi√ß√£o alto, resultando em autovalores pequenos e grandes. Os autovalores da matriz inversa s√£o o inverso dos autovalores de $(X^TX)$, e autovalores pequenos se tornam autovalores grandes na matriz inversa, resultando em alta vari√¢ncia dos coeficientes, em algumas dire√ß√µes do espa√ßo dos par√¢metros.
    5. **Incerteza e Estabilidade**: A geometria da matriz de vari√¢ncia-covari√¢ncia fornece uma vis√£o da incerteza dos par√¢metros, e modelos com grande varia√ß√£o indicam a necessidade de aplicar algum tipo de regulariza√ß√£o ou pr√©-processamento para lidar com a instabilidade dos par√¢metros.

> üí° **Exemplo Num√©rico (Visualiza√ß√£o):**
>
> Para visualizar a interpreta√ß√£o geom√©trica, vamos gerar dados com colinearidade e observar como isso afeta a matriz de vari√¢ncia-covari√¢ncia e os elips√≥ides de confian√ßa:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> import matplotlib.patches as patches
> from scipy.stats import chi2
>
> np.random.seed(42)
>
> # Generate colinear data
> n_samples = 100
> X1 = np.random.rand(n_samples) * 5
> X2 = X1 + np.random.normal(0, 0.2, n_samples) # X2 is highly correlated with X1
> X = np.column_stack((np.ones(n_samples), X1, X2))
> true_beta = np.array([1, 2, -1.5])
> sigma = 0.5
> epsilon = np.random.normal(0, sigma, n_samples)
> y = X @ true_beta + epsilon
>
> # Calculate beta_hat and variance-covariance matrix
> beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
> y_hat = X @ beta_hat
> residuals = y - y_hat
> sigma_sq_hat = np.sum(residuals**2) / (n_samples - X.shape[1])
> var_cov_beta_hat = sigma_sq_hat * np.linalg.inv(X.T @ X)
>
> # Plotting
> fig, ax = plt.subplots(figsize=(8, 8))
>
> # Plot the center of the ellipse
> ax.scatter(beta_hat[1], beta_hat[2], color='red', marker='x', s=100, label='Estimated Beta')
>
> # Get eigenvalues and eigenvectors
> eigenvals, eigenvecs = np.linalg.eig(var_cov_beta_hat[1:3, 1:3])
>
> # Calculate the angle of the ellipse
> angle = np.degrees(np.arctan2(eigenvecs[1, 0], eigenvecs[0, 0]))
>
> # Calculate the width and height of the ellipse based on the eigenvalues
> width = 2 * np.sqrt(chi2.ppf(0.95, 2) * eigenvals[0])
> height = 2 * np.sqrt(chi2.ppf(0.95, 2) * eigenvals[1])
>
> # Create an ellipse patch
> ellipse = patches.Ellipse((beta_hat[1], beta_hat[2]), width, height, angle=angle, fill=False, edgecolor='blue', label='95% Confidence Ellipse')
>
> ax.add_patch(ellipse)
> ax.set_xlabel(r'$\beta_1$')
> ax.set_ylabel(r'$\beta_2$')
> ax.set_title('Confidence Ellipse for Parameters')
> ax.legend()
> ax.grid(True)
> plt.show()
>
> print("Estimated beta:", beta_hat)
> print("Variance-Covariance Matrix of beta_hat:\n", var_cov_beta_hat)
> ```
>
> Este c√≥digo gera dados com alta colinearidade entre os preditores X1 e X2. Ele calcula $\hat{\beta}$ e a matriz de vari√¢ncia-covari√¢ncia. Em seguida, ele usa os autovalores e autovetores da matriz de vari√¢ncia-covari√¢ncia para plotar um elips√≥ide de confian√ßa, mostrando como a colinearidade afeta a forma e orienta√ß√£o do elips√≥ide.  Observe como o elips√≥ide √© alongado na dire√ß√£o da colinearidade (uma linha diagonal), indicando alta incerteza na estimativa dos par√¢metros naquela dire√ß√£o.

### Implica√ß√µes Pr√°ticas

```mermaid
flowchart LR
    A[$\hat{\beta} \sim N(\beta, (X^TX)^{-1}\sigma^2)$] --> B[Hypothesis Tests (t-tests, F-tests)];
    A --> C[Confidence Intervals];
    A --> D[Analysis of Variance (ANOVA)];
    A --> E[Basis for Regularization Techniques];
     style A fill:#afa,stroke:#333,stroke-width:2px
```

A distribui√ß√£o dos estimadores de par√¢metros, $\hat{\beta} \sim \mathcal{N}(\beta, (X^T X)^{-1}\sigma^2)$, tem implica√ß√µes pr√°ticas:
    1.  **Testes de Hip√≥teses:** Permite construir testes de hip√≥teses sobre a signific√¢ncia dos par√¢metros, como os testes t e testes F.
    2.  **Intervalos de Confian√ßa:** Permite calcular intervalos de confian√ßa para os par√¢metros, que fornecem uma estimativa da regi√£o em torno do valor m√©dio que cont√©m os par√¢metros verdadeiros com uma dada probabilidade.
    3.  **An√°lise de Vari√¢ncia:** Permite calcular e comparar diferentes modelos atrav√©s de an√°lise da vari√¢ncia dos par√¢metros e do modelo.
    4.  **Base para Regulariza√ß√£o:** As propriedades da distribui√ß√£o dos par√¢metros tamb√©m influenciam o desenvolvimento de t√©cnicas de regulariza√ß√£o, que reduzem a vari√¢ncia dos par√¢metros e melhoram a capacidade de generaliza√ß√£o dos modelos.

> üí° **Exemplo Num√©rico (Testes de Hip√≥teses e Intervalos de Confian√ßa):**
>
> Vamos usar os resultados do primeiro exemplo num√©rico para realizar um teste de hip√≥teses e construir intervalos de confian√ßa:
>
> ```python
> from scipy.stats import t
>
> # Degrees of freedom
> df = n_samples - X.shape[1]
>
> # Calculate t-statistic and p-value for the first parameter
> t_stat_0 = beta_hat[0] / std_errors[0]
> p_value_0 = 2 * (1 - t.cdf(abs(t_stat_0), df))
>
> # Calculate t-statistic and p-value for the second parameter
> t_stat_1 = beta_hat[1] / std_errors[1]
> p_value_1 = 2 * (1 - t.cdf(abs(t_stat_1), df))
>
> # Confidence interval for the first parameter (95%)
> confidence_level = 0.95
> alpha = 1 - confidence_level
> t_critical = t.ppf(1 - alpha/2, df)
> lower_bound_0 = beta_hat[0] - t_critical * std_errors[0]
> upper_bound_0 = beta_hat[0] + t_critical * std_errors[0]
>
> # Confidence interval for the second parameter (95%)
> lower_bound_1 = beta_hat[1] - t_critical * std_errors[1]
> upper_bound_1 = beta_hat[1] + t_critical * std_errors[1]
>
> print(f"T-statistic for beta_0: {t_stat_0:.2f}, p-value: {p_value_0:.3f}")
> print(f"T-statistic for beta_1: {t_stat_1:.2f}, p-value: {p_value_1:.3f}")
> print(f"95% Confidence interval for beta_0: [{lower_bound_0:.2f}, {upper_bound_0:.2f}]")
> print(f"95% Confidence interval for beta_1: [{lower_bound_1:.2f}, {upper_bound_1:.2f}]")
> ```
> Este c√≥digo calcula as estat√≠sticas t e os valores p para testar a hip√≥tese de que cada par√¢metro √© igual a zero. Ele tamb√©m calcula os intervalos de confian√ßa de 95% para cada par√¢metro, mostrando a faixa em que os verdadeiros valores dos par√¢metros provavelmente se encontram. Se o valor p for menor que um n√≠vel de signific√¢ncia (como 0.05), rejeitamos a hip√≥tese nula de que o par√¢metro √© zero, indicando que ele √© estatisticamente significativo.

A distribui√ß√£o dos estimadores $\hat{\beta}$ √© um resultado fundamental na estat√≠stica dos modelos lineares e serve de base para diferentes abordagens de an√°lise.

> ‚ö†Ô∏è **Nota Importante**: A distribui√ß√£o dos estimadores de par√¢metros em regress√£o linear √© dada por  $\beta \sim N(\beta, (X^TX)^{-1}\sigma^2)$, que assume que os erros s√£o normalmente distribu√≠dos com m√©dia zero e vari√¢ncia constante.
> ‚ùó **Ponto de Aten√ß√£o**: A matriz de vari√¢ncia-covari√¢ncia, $(X^TX)^{-1}\sigma^2$,  define a forma da distribui√ß√£o normal multidimensional dos par√¢metros,  controlando a sua variabilidade e correla√ß√£o.

> ‚úîÔ∏è **Destaque**: Os erros padr√£o e intervalos de confian√ßa, utilizados para realizar infer√™ncia, s√£o calculados a partir da matriz de vari√¢ncia-covari√¢ncia dos par√¢metros.

### Conclus√£o
A distribui√ß√£o dos estimadores de par√¢metros, $\hat{\beta} \sim \mathcal{N}(\beta, (X^T X)^{-1}\sigma^2)$, √© uma propriedade fundamental dos modelos de regress√£o linear, e fornece uma forma de quantificar a incerteza dos par√¢metros e entender as suas rela√ß√µes. A interpreta√ß√£o geom√©trica dessa distribui√ß√£o e a sua deriva√ß√£o matem√°tica facilitam a constru√ß√£o de intervalos de confian√ßa, a realiza√ß√£o de testes de hip√≥teses, e a compreens√£o do comportamento dos par√¢metros estimados.

### Refer√™ncias

[^47]: "The N-p-1 rather than N in the denominator makes ÀÜœÉ2 an unbiased estimate of œÉ2: E(ÀÜœÉ2) = œÉ2." *(Trecho de Linear Methods for Regression)*
[^11]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^10]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Methods for Regression)*
