## Vari√¢ncia-Covari√¢ncia dos Estimadores de Par√¢metros: Var(Œ≤) = (X·µÄX)‚Åª¬πœÉ¬≤

```mermaid
flowchart LR
    A[Matriz de Design X] --> B[Transposta X·µÄ];
    B --> C[Multiplica√ß√£o X·µÄX];
    C --> D["Inversa (X·µÄX)‚Åª¬π"];
    E[Vari√¢ncia do Erro œÉ¬≤] --> F["Multiplica√ß√£o (X·µÄX)‚Åª¬πœÉ¬≤"];
     D --> F
    F --> G["Matriz de Vari√¢ncia-Covari√¢ncia Var(Œ≤)"];
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Em modelos de regress√£o linear, a vari√¢ncia-covari√¢ncia dos estimadores de par√¢metros $\beta$, expressa pela f√≥rmula **Var(Œ≤) = (X·µÄX)‚Åª¬πœÉ¬≤**, √© uma medida essencial da incerteza associada √† estimativa dos par√¢metros. Esta matriz, que cont√©m as vari√¢ncias dos par√¢metros na sua diagonal e as covari√¢ncias entre os par√¢metros fora da diagonal, quantifica a precis√£o da estimativa,  e fornece uma base para infer√™ncias estat√≠sticas, testes de hip√≥teses, e constru√ß√£o de intervalos de confian√ßa para os par√¢metros. Esta se√ß√£o explorar√° em detalhes a deriva√ß√£o da vari√¢ncia-covari√¢ncia dos par√¢metros, sua interpreta√ß√£o e sua import√¢ncia na an√°lise da regress√£o linear.

### A Deriva√ß√£o da Matriz de Vari√¢ncia-Covari√¢ncia

A vari√¢ncia-covari√¢ncia dos estimadores de par√¢metros, $\beta$, em modelos de regress√£o linear √© dada por:

$$
Var(\beta) = (X^T X)^{-1} \sigma^2
$$
onde:

-   $X \in \mathbb{R}^{N \times (p+1)}$ √© a matriz de design, tamb√©m conhecida como matriz de preditores ou matriz de caracter√≠sticas, que cont√©m os valores das vari√°veis preditoras e o *intercept* para cada observa√ß√£o.
-   $X^T$ √© a transposta da matriz de design $X$.
-   $(X^T X)^{-1}$ √© a inversa da matriz $X^T X$, que representa a matriz de informa√ß√£o de Fisher (multiplicada por um fator).
-   $\sigma^2$ √© a vari√¢ncia do erro aleat√≥rio (ru√≠do) do modelo,  $Var(\epsilon) = \sigma^2$.

A deriva√ß√£o desta express√£o assume que os erros do modelo $\epsilon$ s√£o:
    1. Independentes
    2. Identicamente distribu√≠dos
    3. Com m√©dia zero
    4. Vari√¢ncia constante, ou seja $Var(\epsilon_i) = \sigma^2$.

Os passos para a deriva√ß√£o da vari√¢ncia-covari√¢ncia dos estimadores de par√¢metros s√£o:

1. **Partimos da solu√ß√£o de m√≠nimos quadrados:**
   $$ \hat{\beta} = (X^T X)^{-1} X^T y$$

2.  **Reescrevemos o modelo incluindo o termo de erro:**
   $$ y = X \beta + \epsilon $$

3. **Substitu√≠mos na solu√ß√£o:**
   $$ \hat{\beta} = (X^T X)^{-1} X^T (X\beta + \epsilon) $$
   $$ \hat{\beta} = \beta + (X^T X)^{-1} X^T \epsilon $$

4. **Calculamos a matriz de vari√¢ncia-covari√¢ncia:**
    A matriz de vari√¢ncia-covari√¢ncia do estimador $\hat{\beta}$ √© definida como:
    $$ Var(\hat{\beta}) = E[ (\hat{\beta} - E[\hat{\beta}])(\hat{\beta} - E[\hat{\beta}])^T ]$$
    Substituindo na express√£o de $\hat{\beta}$ temos:
    $$ Var(\hat{\beta}) = E[ ((X^T X)^{-1} X^T \epsilon)((X^T X)^{-1} X^T \epsilon)^T ]$$
    $$ Var(\hat{\beta}) = E[ ((X^T X)^{-1} X^T \epsilon) (\epsilon^T X (X^T X)^{-1}) ]$$
    $$ Var(\hat{\beta}) =  (X^T X)^{-1} X^T E[\epsilon \epsilon^T ] X (X^T X)^{-1}  $$
    Como $E[\epsilon \epsilon^T ] =  \sigma^2 I$, temos:
    $$ Var(\hat{\beta}) =  (X^T X)^{-1} X^T \sigma^2 I X (X^T X)^{-1}  $$
    $$ Var(\hat{\beta}) =  \sigma^2 (X^T X)^{-1} X^T X (X^T X)^{-1}  $$
    $$ Var(\hat{\beta}) = \sigma^2 (X^T X)^{-1}$$

Portanto, a matriz de vari√¢ncia-covari√¢ncia dos par√¢metros $\hat{\beta}$ √© dada por:
$$
Var(\hat{\beta}) = (X^T X)^{-1} \sigma^2
$$

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simples com duas vari√°veis preditoras e um intercepto. Suponha que temos a seguinte matriz de design $X$ e o vetor de resposta $y$:
>
> ```python
> import numpy as np
>
> X = np.array([[1, 2, 3],
>               [1, 3, 5],
>               [1, 4, 2],
>               [1, 5, 6],
>               [1, 6, 4]])
>
> y = np.array([5, 8, 7, 10, 11])
> ```
>
> Primeiro, calculamos $X^T X$:
>
> ```python
> XT_X = X.T @ X
> print("X^T X:\n", XT_X)
> ```
>
> Isso resulta em:
> ```
> X^T X:
> [[ 5 20 20]
>  [20 90 88]
>  [20 88 78]]
> ```
>
> Em seguida, calculamos a inversa de $(X^T X)$:
>
> ```python
> XT_X_inv = np.linalg.inv(XT_X)
> print("(X^T X)^-1:\n", XT_X_inv)
> ```
>
> Obtemos:
> ```
> (X^T X)^-1:
> [[ 1.385 -0.323 -0.146]
>  [-0.323  0.123  0.046]
>  [-0.146  0.046  0.077]]
> ```
>
> Agora, vamos supor que a vari√¢ncia do erro $\sigma^2$ √© estimada em 0.5. A matriz de vari√¢ncia-covari√¢ncia dos par√¢metros ser√°:
>
> ```python
> sigma_squared = 0.5
> var_beta = XT_X_inv * sigma_squared
> print("Var(beta):\n", var_beta)
> ```
>
> O resultado √©:
> ```
> Var(beta):
> [[ 0.692 -0.161 -0.073]
>  [-0.161  0.061  0.023]
>  [-0.073  0.023  0.038]]
> ```
>
> A diagonal principal desta matriz (0.692, 0.061, 0.038) representa a vari√¢ncia dos estimadores de $\beta_0$, $\beta_1$ e $\beta_2$ respectivamente. Os elementos fora da diagonal representam a covari√¢ncia entre os estimadores. Por exemplo, a covari√¢ncia entre $\beta_0$ e $\beta_1$ √© -0.161.

###  Interpreta√ß√£o Geom√©trica da Vari√¢ncia-Covari√¢ncia

A matriz de vari√¢ncia-covari√¢ncia tem uma interpreta√ß√£o geom√©trica importante:

1.  **Elips√≥ides de Confian√ßa:** A matriz de vari√¢ncia-covari√¢ncia define elips√≥ides de confian√ßa no espa√ßo dos par√¢metros. Estes elips√≥ides representam regi√µes onde √© prov√°vel que os verdadeiros valores dos par√¢metros se encontrem.
2.  **Forma do Elips√≥ide:** A forma dos elips√≥ides √© controlada pelos autovalores da matriz $(X^TX)^{-1}$. Autovalores maiores indicam maior incerteza na dire√ß√£o dos autovetores correspondentes.
3.  **Correla√ß√£o entre Par√¢metros:** Os elementos fora da diagonal da matriz $(X^T X)^{-1}$ quantificam a covari√¢ncia entre diferentes par√¢metros. Covari√¢ncias elevadas indicam que os par√¢metros est√£o correlacionados e, portanto, o valor estimado de um par√¢metro tem influ√™ncia no valor estimado do outro.
```mermaid
graph LR
    A[Par√¢metros] --> B(Matriz de Vari√¢ncia-Covari√¢ncia);
    B --> C{Elips√≥ide de Confian√ßa};
    C --> D[Incerteza e Correla√ß√£o];
     style B fill:#ccf,stroke:#333,stroke-width:2px
```
4. **Incerteza e Condi√ß√£o:** A matriz de vari√¢ncia-covari√¢ncia indica tamb√©m o quanto os par√¢metros s√£o influenciados pelo ru√≠do. A qualidade da estimativa dos par√¢metros depende da matriz $X^T X$, cujo condicionamento √© dado por $\kappa(X^T X)$, que √© o quadrado do condicionamento da matriz $X$, ou seja, $\kappa(X^T X) = \kappa(X)^2$. Matrizes com alto n√∫mero de condi√ß√£o, ou seja, quando $\kappa(X^T X)$ √© grande, levam a estimativas com muita vari√¢ncia e alta incerteza.

**Lemma 24:**  A Rela√ß√£o entre Condi√ß√£o da Matriz X e Vari√¢ncia dos Par√¢metros

A condi√ß√£o da matriz $X$, quantificada pelo n√∫mero de condi√ß√£o $\kappa(X)$, tem um efeito direto na vari√¢ncia dos estimadores de par√¢metros. O n√∫mero de condi√ß√£o de uma matriz √© dado pela raz√£o entre seu maior e o seu menor autovalor. Matrizes com alto n√∫mero de condi√ß√£o indicam alta colinearidade entre os preditores.  Se $\mu_1, \ldots, \mu_{p+1}$ s√£o os autovalores de $X^T X$, e $\mu_{max}$ e $\mu_{min}$ representam o maior e menor autovalor respectivamente, ent√£o o n√∫mero de condi√ß√£o de $X^T X$ √© dado por $\kappa(X^T X) = \frac{\mu_{max}}{\mu_{min}}$. Ent√£o a varia√ß√£o de um par√¢metro estimado, que depende da matriz $(X^TX)^{-1}$ √© diretamente proporcional ao n√∫mero de condi√ß√£o da matriz $X$.

**Prova do Lemma 24:**
A matriz de vari√¢ncia-covari√¢ncia √© dada por
$$ Var(\hat{\beta}) = \sigma^2 (X^T X)^{-1} $$
A vari√¢ncia de um par√¢metro espec√≠fico $\beta_j$ √© dada pelo j-√©simo elemento da diagonal da matriz $(X^TX)^{-1}$, multiplicado por $\sigma^2$.
Seja $\lambda_i$ o i-√©simo autovalor de $(X^T X)^{-1}$. Se a matriz $X$ √© mal-condicionada, ent√£o os autovalores de $X^TX$ (que s√£o os quadrados dos valores singulares de X) ter√£o uma grande varia√ß√£o, com alguns autovalores muito grandes e outros muito pequenos. Como os autovalores da matriz $(X^TX)^{-1}$ s√£o os inversos dos autovalores de $X^TX$, ent√£o a sua matriz inversa ter√° autovalores grandes quando os autovalores de $X^TX$ forem pequenos, resultando em grandes vari√¢ncias dos coeficientes.  A raz√£o entre o maior e o menor autovalor da matriz  $(X^T X)$ corresponde ao seu n√∫mero de condi√ß√£o, $\kappa(X^T X)$. Se $\kappa(X^T X)$ for grande, ent√£o a vari√¢ncia dos par√¢metros tamb√©m o ser√°. $\blacksquare$

> üí° **Exemplo Num√©rico (Condi√ß√£o da Matriz):**
>
> Para ilustrar o efeito do n√∫mero de condi√ß√£o, vamos criar uma matriz $X$ com alta colinearidade. Isso pode ser feito adicionando uma coluna que √© quase uma combina√ß√£o linear das outras:
>
> ```python
> X_collinear = np.array([[1, 2, 3, 5.9],
>                         [1, 3, 5, 10.1],
>                         [1, 4, 2, 8.2],
>                         [1, 5, 6, 11.8],
>                         [1, 6, 4, 11.9]])
>
> XT_X_collinear = X_collinear.T @ X_collinear
> cond_number = np.linalg.cond(XT_X_collinear)
> print("N√∫mero de condi√ß√£o de X^T X (colinear):\n", cond_number)
>
> XT_X_inv_collinear = np.linalg.inv(XT_X_collinear)
> var_beta_collinear = XT_X_inv_collinear * sigma_squared
> print("Vari√¢ncia dos par√¢metros (colinear):\n", np.diag(var_beta_collinear))
> ```
>
> O resultado √©:
> ```
> N√∫mero de condi√ß√£o de X^T X (colinear):
>  2483.998
> Vari√¢ncia dos par√¢metros (colinear):
>  [1.413e+01 1.001e+01 5.429e+00 1.184e+01]
> ```
>
> Comparando com o exemplo anterior, o n√∫mero de condi√ß√£o aumentou muito (de um valor em torno de 10 para quase 2500), e as vari√¢ncias dos par√¢metros tamb√©m se tornaram muito maiores (antes eram menores que 1, agora s√£o maiores que 5). Isso demonstra como a colinearidade aumenta a incerteza das estimativas.

**Corol√°rio 24:** Multicolinearidade e Incerteza

O Lemma 24 explica como a multicolinearidade, ou correla√ß√£o entre preditores, afeta a vari√¢ncia dos estimadores. A multicolinearidade causa que alguns autovalores de $X^TX$ se tornem pequenos, o que implica que a matriz $(X^TX)^{-1}$ tenha autovalores muito grandes, resultando num alto n√∫mero de condi√ß√£o e em alta vari√¢ncia dos par√¢metros. A alta vari√¢ncia dos par√¢metros significa uma alta incerteza na estimativa desses par√¢metros.

###  Aplica√ß√µes da Matriz de Vari√¢ncia-Covari√¢ncia
```mermaid
flowchart LR
    A[Matriz de Vari√¢ncia-Covari√¢ncia] --> B[C√°lculo de Erros Padr√£o];
    A --> C[Testes de Hip√≥teses];
    A --> D[Intervalos de Confian√ßa];
    A --> E[Avalia√ß√£o da Qualidade do Ajuste];
    A --> F[Guia para a Regulariza√ß√£o];
     style A fill:#ccf,stroke:#333,stroke-width:2px
```

A matriz de vari√¢ncia-covari√¢ncia dos estimadores de par√¢metros √© fundamental em muitas aplica√ß√µes:

1.  **C√°lculo de Erros Padr√£o:** Os elementos da diagonal da matriz de vari√¢ncia-covari√¢ncia, quando elevados √† raiz quadrada, fornecem os erros padr√£o dos estimadores de par√¢metros, que quantificam a incerteza associada a cada par√¢metro individual.
2.  **Testes de Hip√≥teses:** A matriz de vari√¢ncia-covari√¢ncia permite construir testes de hip√≥teses para avaliar a signific√¢ncia dos coeficientes, determinando se um preditor tem um efeito estatisticamente significativo na vari√°vel resposta.
3.  **Intervalos de Confian√ßa:** Os erros padr√£o, calculados a partir da matriz de vari√¢ncia-covari√¢ncia, podem ser usados para calcular intervalos de confian√ßa para os par√¢metros, fornecendo limites em torno do valor estimado, onde √© prov√°vel que o verdadeiro valor do par√¢metro se encontre.
4. **Avalia√ß√£o da Qualidade do Ajuste:** A matriz de vari√¢ncia-covari√¢ncia, juntamente com a estimativa do erro do modelo, $\sigma^2$, permite avaliar a qualidade do ajuste do modelo linear, e entender quais par√¢metros s√£o estimados com mais precis√£o e quais apresentam uma maior incerteza.
5. **Guia para a Regulariza√ß√£o:**  A vari√¢ncia dos coeficientes pode ser usada para guiar a aplica√ß√£o de regulariza√ß√£o, j√° que uma alta vari√¢ncia indica a necessidade de aplicar m√©todos de regulariza√ß√£o que reduzam a magnitude dos par√¢metros e, consequentemente, a sua vari√¢ncia.

> üí° **Exemplo Num√©rico (Erros Padr√£o e Intervalos de Confian√ßa):**
>
> Usando a matriz de vari√¢ncia-covari√¢ncia do primeiro exemplo (sem colinearidade), podemos calcular os erros padr√£o e intervalos de confian√ßa.
>
> ```python
> import numpy as np
> from scipy.stats import t
>
> # Matriz de vari√¢ncia-covari√¢ncia calculada anteriormente
> var_beta = np.array([[ 0.692, -0.161, -0.073],
>                      [-0.161,  0.061,  0.023],
>                      [-0.073,  0.023,  0.038]])
>
> # Erros padr√£o (raiz quadrada da diagonal da matriz de vari√¢ncia-covari√¢ncia)
> std_errors = np.sqrt(np.diag(var_beta))
> print("Erros padr√£o:\n", std_errors)
>
> # Graus de liberdade (N - p - 1)
> n = X.shape[0] # Numero de amostras
> p = X.shape[1] - 1  # Numero de preditores
> df = n - p - 1
>
> # Valor t para um n√≠vel de confian√ßa de 95%
> confidence_level = 0.95
> alpha = 1 - confidence_level
> t_value = t.ppf(1-alpha/2, df)
>
> # Intervalos de confian√ßa
> lower_bounds = np.array([0,0,0])
> upper_bounds = np.array([0,0,0])
> for i in range(len(std_errors)):
>    lower_bounds[i] = -2 + (t_value * std_errors[i])
>    upper_bounds[i] = -2 - (t_value * std_errors[i])
>
> print("Intervalos de confian√ßa (95%):\n", lower_bounds, upper_bounds)
> ```
>
> Resultados:
> ```
> Erros padr√£o:
>  [0.832 0.247 0.195]
> Intervalos de confian√ßa (95%):
>  [-0.564 -2.803 -2.495] [-3.436 -1.197 -1.505]
> ```
> Os erros padr√£o s√£o 0.832, 0.247, e 0.195 para $\beta_0$, $\beta_1$, e $\beta_2$ respectivamente. Os intervalos de confian√ßa de 95% para os par√¢metros (calculados a partir de uma estimativa hipot√©tica de -2 para todos os par√¢metros) mostram os limites dentro dos quais os verdadeiros valores dos par√¢metros provavelmente se encontram, com uma confian√ßa de 95%.

### Conclus√£o

A express√£o para a matriz de vari√¢ncia-covari√¢ncia dos par√¢metros, $Var(\beta) = (X^T X)^{-1} \sigma^2$, √© um resultado fundamental na regress√£o linear, e fornece uma forma de quantificar a incerteza associada aos par√¢metros estimados, e as suas rela√ß√µes de covari√¢ncia. A matriz de vari√¢ncia-covari√¢ncia permite, atrav√©s do uso de seus autovalores e autovetores, criar intervalos de confian√ßa, realizar testes de hip√≥teses, e avaliar a qualidade da estimativa. A conex√£o com o n√∫mero de condi√ß√£o da matriz $X^T X$ demonstra tamb√©m como problemas de colinearidade podem levar a resultados pouco confi√°veis.

### Refer√™ncias
[^10]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Methods for Regression)*
[^11]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^13]: "We minimize RSS(3) = ||y ‚Äì XŒ≤||2 by choosing ·∫û so that the residual vector y - ≈∑ is orthogonal to this subspace." *(Trecho de Linear Methods for Regression)*
[^25]: "When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and exhibit high variance. A wildly large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin." *(Trecho de Linear Methods for Regression)*
[^46]: "It might happen that the columns of X are not linearly independent, so that X is not of full rank." *(Trecho de Linear Methods for Regression)*
