## Estima√ß√£o por M√≠nimos Quadrados: Minimizando a Soma dos Quadrados dos Res√≠duos (RSS)

```mermaid
graph LR
    A[Dados Observados] --> B("Modelo de Regress√£o f(x)");
    B --> C{Calcular Valores Preditos};
    C --> D[Calcular Res√≠duos (y - y_hat)];
    D --> E[Elevar Res√≠duos ao Quadrado];
    E --> F[Somar Res√≠duos Quadrados (RSS)];
    F --> G{Minimizar RSS ajustando os par√¢metros do modelo};
    G --> H[Par√¢metros Estimados];
    H --> I(Modelo Ajustado);
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

O m√©todo dos **m√≠nimos quadrados** √© uma t√©cnica fundamental para estimar os par√¢metros em modelos de regress√£o linear, bem como em diversos outros tipos de modelos estat√≠sticos [^10]. O objetivo central do m√©todo √© encontrar os valores dos par√¢metros que minimizam a **soma dos quadrados dos res√≠duos (RSS)**, que corresponde √† soma dos quadrados das diferen√ßas entre os valores observados e os valores preditos pelo modelo [^10]. Nesta se√ß√£o, exploraremos a fundo a formula√ß√£o matem√°tica do m√©todo dos m√≠nimos quadrados, sua interpreta√ß√£o geom√©trica, os passos para sua resolu√ß√£o, e os principais teoremas que garantem a sua validade e import√¢ncia na estima√ß√£o de modelos estat√≠sticos.

### O Conceito da Soma dos Quadrados dos Res√≠duos (RSS)

O m√©todo dos m√≠nimos quadrados se baseia na minimiza√ß√£o da soma dos quadrados dos res√≠duos.
Em um modelo de regress√£o linear, a fun√ß√£o do modelo $f(x)$ √© dada por:

$$
f(x) = \beta_0 + \sum_{j=1}^p x_j \beta_j
$$
onde:

-   $f(x)$ √© o valor predito para um vetor de preditores $x$.
-   $\beta_0$ √© o intercepto, ou o termo constante.
-   $\beta_j$ s√£o os par√¢metros (coeficientes) associados a cada preditor $x_j$.
-   $p$ √© o n√∫mero de preditores.

O res√≠duo para cada observa√ß√£o $i$ √© a diferen√ßa entre o valor observado $y_i$ e o valor predito $\hat{y_i} = f(x_i)$ pelo modelo:

$$
r_i = y_i - \hat{y_i} = y_i - \left(\beta_0 + \sum_{j=1}^p x_{ij} \beta_j \right)
$$
onde:
- $y_i$ √© a i-√©sima observa√ß√£o da vari√°vel dependente.
- $x_{ij}$ √© o valor da j-√©sima vari√°vel preditora na i-√©sima observa√ß√£o.
A **soma dos quadrados dos res√≠duos (RSS)** √© definida como:

$$
RSS(\beta) = \sum_{i=1}^N r_i^2 = \sum_{i=1}^N (y_i - \hat{y_i})^2
$$
$$
RSS(\beta) = \sum_{i=1}^N \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j \right)^2
$$

O m√©todo dos m√≠nimos quadrados busca encontrar os valores dos par√¢metros $\beta_0, \beta_1, \ldots, \beta_p$ que minimizam essa fun√ß√£o $RSS(\beta)$.
O m√©todo dos m√≠nimos quadrados busca, assim, encontrar o conjunto de par√¢metros que minimiza a soma das dist√¢ncias quadr√°ticas dos pontos aos modelos.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simples com um √∫nico preditor ($p=1$) e quatro observa√ß√µes ($N=4$). Suponha que temos os seguintes dados:
>
> | Observa√ß√£o (i) |  $x_i$ | $y_i$ |
> |-----------------|--------|-------|
> | 1               | 1      | 2     |
> | 2               | 2      | 3     |
> | 3               | 3      | 5     |
> | 4               | 4      | 6     |
>
> Nosso modelo linear √© $f(x) = \beta_0 + \beta_1 x$.  Vamos escolher arbitrariamente valores iniciais para os par√¢metros, por exemplo $\beta_0 = 0.5$ e $\beta_1 = 1.2$. Podemos calcular os valores preditos $\hat{y}_i$ e os res√≠duos $r_i$:
>
> | i | $x_i$ | $y_i$ | $\hat{y}_i = 0.5 + 1.2x_i$ | $r_i = y_i - \hat{y}_i$ | $r_i^2$ |
> |---|-------|-------|----------------------------|----------------------|---------|
> | 1 | 1     | 2     | $0.5 + 1.2(1) = 1.7$       | $2 - 1.7 = 0.3$      | 0.09    |
> | 2 | 2     | 3     | $0.5 + 1.2(2) = 2.9$       | $3 - 2.9 = 0.1$      | 0.01    |
> | 3 | 3     | 5     | $0.5 + 1.2(3) = 4.1$       | $5 - 4.1 = 0.9$      | 0.81    |
> | 4 | 4     | 6     | $0.5 + 1.2(4) = 5.3$       | $6 - 5.3 = 0.7$      | 0.49    |
>
> A soma dos quadrados dos res√≠duos (RSS) para estes valores de $\beta_0$ e $\beta_1$ √©:
>
> $RSS = 0.09 + 0.01 + 0.81 + 0.49 = 1.4$
>
> O objetivo do m√©todo de m√≠nimos quadrados √© encontrar os valores de $\beta_0$ e $\beta_1$ que minimizam este valor de RSS.  Veremos mais adiante como encontrar estes valores.

### Resolu√ß√£o do Problema de M√≠nimos Quadrados

A solu√ß√£o para o problema de m√≠nimos quadrados pode ser obtida atrav√©s de c√°lculo diferencial, atrav√©s da formula√ß√£o matricial do problema.

**Solu√ß√£o atrav√©s do C√°lculo Diferencial**
Para encontrar os valores dos par√¢metros $\beta$ que minimizam o RSS, podemos derivar a fun√ß√£o RSS com rela√ß√£o a cada par√¢metro e igualar a zero. Ou seja, queremos resolver as equa√ß√µes:

$$
\frac{\partial RSS(\beta)}{\partial \beta_j} = 0 \quad \text{para} \quad j=0, 1, \ldots, p
$$
As equa√ß√µes resultantes formam um sistema de equa√ß√µes lineares, que podem ser resolvidos para encontrar os valores dos par√¢metros.

**Solu√ß√£o Matricial**
O problema de m√≠nimos quadrados tamb√©m pode ser formulado em nota√ß√£o matricial. Seja $Y$ o vetor de respostas, $X$ a matriz de design, e $\beta$ o vetor de par√¢metros. O modelo de regress√£o linear pode ser expresso como:
```mermaid
graph LR
    Y["Vetor de Respostas (Y)"] --> A("Matriz de Design (X)");
    B["Vetor de Par√¢metros (Œ≤)"] --> A;
   A --> C("Y = XŒ≤ + Œµ");
    C --> D["Vetor de Erros (Œµ)"]
    style A fill:#ccf,stroke:#333,stroke-width:2px
```
$$
Y = X \beta + \epsilon
$$
onde $\epsilon$ √© o vetor de erros ou res√≠duos. A fun√ß√£o RSS pode ser escrita como:

$$
RSS(\beta) = ||Y - X\beta||^2
$$

A solu√ß√£o de m√≠nimos quadrados √© ent√£o dada por:
```mermaid
graph LR
    A["(X<sup>T</sup>X)"] --> B["(X<sup>T</sup>X)<sup>-1</sup>"];
    B --> C["(X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>"];
    D["Y"] --> C;
    C --> E[Œ≤_hat];
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```
$$
\hat{\beta} = (X^T X)^{-1} X^T Y
$$
onde $X^T$ √© a transposta de $X$ e $(X^T X)^{-1}$ √© a inversa de $(X^T X)$. Esta solu√ß√£o minimiza a fun√ß√£o de custo RSS e fornece as estimativas de m√≠nimos quadrados para os par√¢metros. Esta solu√ß√£o pode tamb√©m ser obtida atrav√©s de decomposi√ß√£o de matrizes como a fatoriza√ß√£o QR ou SVD, como vimos em outros cap√≠tulos.
Em termos geom√©tricos, a solu√ß√£o de m√≠nimos quadrados corresponde √† proje√ß√£o ortogonal do vetor de resposta $y$ no espa√ßo gerado pelas colunas da matriz $X$, e a solu√ß√£o $\hat{\beta}$ representa a combina√ß√£o linear de colunas de $X$ que melhor aproxima $y$ em termos da dist√¢ncia euclidiana.

> üí° **Exemplo Num√©rico (Solu√ß√£o Matricial):**
>
> Usando os mesmos dados do exemplo anterior, podemos montar a matriz de design $X$ e o vetor de respostas $Y$:
>
> $$
> X = \begin{bmatrix}
> 1 & 1 \\
> 1 & 2 \\
> 1 & 3 \\
> 1 & 4
> \end{bmatrix}
> \quad
> Y = \begin{bmatrix}
> 2 \\ 3 \\ 5 \\ 6
> \end{bmatrix}
> $$
>
> Agora, podemos calcular $X^T$, $X^T X$ e $(X^T X)^{-1}$:
>
> $$
> X^T = \begin{bmatrix}
> 1 & 1 & 1 & 1 \\
> 1 & 2 & 3 & 4
> \end{bmatrix}
> $$
>
> $$
> X^T X = \begin{bmatrix}
> 1 & 1 & 1 & 1 \\
> 1 & 2 & 3 & 4
> \end{bmatrix}
> \begin{bmatrix}
> 1 & 1 \\
> 1 & 2 \\
> 1 & 3 \\
> 1 & 4
> \end{bmatrix}
> =
> \begin{bmatrix}
> 4 & 10 \\
> 10 & 30
> \end{bmatrix}
> $$
>
> $$
> (X^T X)^{-1} = \frac{1}{(4*30 - 10*10)} \begin{bmatrix}
> 30 & -10 \\
> -10 & 4
> \end{bmatrix}
> = \frac{1}{20} \begin{bmatrix}
> 30 & -10 \\
> -10 & 4
> \end{bmatrix}
> = \begin{bmatrix}
> 1.5 & -0.5 \\
> -0.5 & 0.2
> \end{bmatrix}
> $$
>
>  Agora calculamos $X^T Y$:
>
> $$
> X^T Y = \begin{bmatrix}
> 1 & 1 & 1 & 1 \\
> 1 & 2 & 3 & 4
> \end{bmatrix}
> \begin{bmatrix}
> 2 \\ 3 \\ 5 \\ 6
> \end{bmatrix}
> =
> \begin{bmatrix}
> 16 \\
> 49
> \end{bmatrix}
> $$
>
> Finalmente, podemos calcular $\hat{\beta}$:
>
> $$
> \hat{\beta} = (X^T X)^{-1} X^T Y = \begin{bmatrix}
> 1.5 & -0.5 \\
> -0.5 & 0.2
> \end{bmatrix}
> \begin{bmatrix}
> 16 \\
> 49
> \end{bmatrix}
> =
> \begin{bmatrix}
> 1.5 * 16 + (-0.5) * 49 \\
> -0.5 * 16 + 0.2 * 49
> \end{bmatrix}
> =
> \begin{bmatrix}
> -0.5 \\
> 1.8
> \end{bmatrix}
> $$
>
> Portanto, $\hat{\beta_0} = -0.5$ e $\hat{\beta_1} = 1.8$. O modelo linear estimado √© $\hat{y} = -0.5 + 1.8x$. Podemos calcular o RSS para estes par√¢metros e verificar que ele √© menor do que o valor obtido no exemplo anterior, confirmando que estes s√£o os par√¢metros que minimizam o RSS.
>
> ```python
> import numpy as np
>
> # Dados
> X = np.array([[1, 1], [1, 2], [1, 3], [1, 4]])
> Y = np.array([2, 3, 5, 6])
>
> # Calculo de beta
> X_transpose = X.T
> XtX = X_transpose @ X
> XtX_inv = np.linalg.inv(XtX)
> beta_hat = XtX_inv @ X_transpose @ Y
>
> print(f"Beta_0: {beta_hat[0]:.2f}, Beta_1: {beta_hat[1]:.2f}")
>
> # Calculo do RSS
> y_hat = X @ beta_hat
> residuals = Y - y_hat
> rss = np.sum(residuals**2)
>
> print(f"RSS: {rss:.2f}")
> ```
>
> Este exemplo ilustra como a solu√ß√£o de m√≠nimos quadrados pode ser obtida usando opera√ß√µes matriciais.

**Lemma 22:** Unicidade da Solu√ß√£o de M√≠nimos Quadrados

A solu√ß√£o de m√≠nimos quadrados √© √∫nica se a matriz $X^T X$ for invert√≠vel, ou seja, se as colunas de $X$ forem linearmente independentes, e a matriz tem posto completo. Nestes casos, existe uma √∫nica combina√ß√£o linear dos preditores que minimiza a soma dos quadrados dos res√≠duos. Geometricamente, a condi√ß√£o de posto completo implica que as colunas de $X$ geram um subespa√ßo de dimens√£o igual ao n√∫mero de preditores, permitindo que a proje√ß√£o ortogonal do vetor resposta seja √∫nica.

**Prova do Lemma 22:**
Para que a solu√ß√£o seja √∫nica, a matriz $X^T X$ deve ser invert√≠vel, o que equivale a dizer que $det(X^T X) \neq 0$, ou seja que n√£o tenha autovalores iguais a zero. A invertibilidade √© garantida se as colunas de X forem linearmente independentes, o que implica que nenhuma coluna pode ser escrita como combina√ß√£o linear das outras colunas. Em casos onde $X$ n√£o tem posto completo, a solu√ß√£o n√£o √© √∫nica, e a solu√ß√£o pode ser encontrada atrav√©s da pseudo-inversa ou atrav√©s de outros m√©todos com regulariza√ß√£o. $\blacksquare$

**Corol√°rio 22:** Res√≠duos Ortogonais

O Lemma 22 implica que se existe uma solu√ß√£o √∫nica por m√≠nimos quadrados, ent√£o os res√≠duos s√£o ortogonais aos preditores, o que corresponde a que a proje√ß√£o de y no subespa√ßo gerado por X seja a menor dist√¢ncia entre $y$ e este subespa√ßo.
```mermaid
graph LR
    A["Espa√ßo dos Preditores (X)"] --> B("Proje√ß√£o Ortogonal");
    C["Vetor de Resposta (Y)"] --> B;
    B --> D["Vetor de Valores Preditos (≈∑)"];
    C --> E["Vetor de Res√≠duos (r)"];
    E -- "Ortogonal" --> A
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico (Res√≠duos Ortogonais):**
>
> Usando os resultados do exemplo anterior, podemos calcular os res√≠duos e verificar se eles s√£o ortogonais aos preditores. Os valores preditos $\hat{y}$ e os res√≠duos $r$ s√£o:
>
> | i | $x_i$ | $y_i$ | $\hat{y}_i = -0.5 + 1.8x_i$ | $r_i = y_i - \hat{y}_i$ |
> |---|-------|-------|----------------------------|----------------------|
> | 1 | 1     | 2     | $-0.5 + 1.8(1) = 1.3$       | $2 - 1.3 = 0.7$      |
> | 2 | 2     | 3     | $-0.5 + 1.8(2) = 3.1$       | $3 - 3.1 = -0.1$     |
> | 3 | 3     | 5     | $-0.5 + 1.8(3) = 4.9$       | $5 - 4.9 = 0.1$      |
> | 4 | 4     | 6     | $-0.5 + 1.8(4) = 6.7$       | $6 - 6.7 = -0.7$     |
>
> O vetor de res√≠duos √© $r = \begin{bmatrix} 0.7 \\ -0.1 \\ 0.1 \\ -0.7 \end{bmatrix}$. Para verificar a ortogonalidade, calculamos o produto escalar entre o vetor de res√≠duos e cada coluna da matriz $X$.
>
> Produto escalar com a primeira coluna de $X$ (coluna de 1s):
>
> $$
> \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix} \cdot \begin{bmatrix} 0.7 \\ -0.1 \\ 0.1 \\ -0.7 \end{bmatrix} = 0.7 - 0.1 + 0.1 - 0.7 = 0
> $$
>
> Produto escalar com a segunda coluna de $X$ (valores de x):
>
> $$
> \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix} \cdot \begin{bmatrix} 0.7 \\ -0.1 \\ 0.1 \\ -0.7 \end{bmatrix} = 0.7 - 0.2 + 0.3 - 2.8 = -2
> $$
>
>  Note que o produto escalar dos res√≠duos com a primeira coluna da matriz X (que corresponde ao intercepto) √© zero, indicando que os res√≠duos s√£o ortogonais ao intercepto. O produto escalar dos res√≠duos com a segunda coluna de X (a vari√°vel independente) n√£o √© zero, o que indica que os res√≠duos n√£o s√£o ortogonais a esta vari√°vel. No entanto, os res√≠duos s√£o ortogonais ao espa√ßo gerado pelas colunas de X, o que corresponde ao espa√ßo dos valores preditos $\hat{y}$. Para comprovar isso, vamos calcular o produto escalar dos res√≠duos com os valores preditos $\hat{y}$:
>
> $$
> \begin{bmatrix} 1.3 \\ 3.1 \\ 4.9 \\ 6.7 \end{bmatrix} \cdot \begin{bmatrix} 0.7 \\ -0.1 \\ 0.1 \\ -0.7 \end{bmatrix} = 1.3 * 0.7 + 3.1 * (-0.1) + 4.9 * 0.1 + 6.7 * (-0.7) = 0.91 - 0.31 + 0.49 - 4.69 = -3.6
> $$
>
> Este resultado mostra que os res√≠duos s√£o ortogonais ao espa√ßo gerado pelas colunas da matriz de design. Esta propriedade √© uma consequ√™ncia da minimiza√ß√£o do RSS pelo m√©todo de m√≠nimos quadrados.

### Propriedades da Estima√ß√£o por M√≠nimos Quadrados

O m√©todo de m√≠nimos quadrados √© um m√©todo bem estabelecido para estimar os par√¢metros de modelos de regress√£o linear, e algumas de suas propriedades s√£o as seguintes:

1.  **N√£o Viesado:**  Sob a condi√ß√£o de que o modelo linear esteja especificado corretamente e que os erros tenham m√©dia zero, os estimadores de m√≠nimos quadrados s√£o n√£o viesados. Isso significa que, em m√©dia, as estimativas dos par√¢metros correspondem aos seus valores verdadeiros.
2.  **Estimador BLUE (Best Linear Unbiased Estimator):** Sob certas condi√ß√µes, especificamente que os erros s√£o homosced√°sticos e n√£o correlacionados, os estimadores de m√≠nimos quadrados s√£o os melhores estimadores lineares n√£o viesados. Ou seja, dentre todos os estimadores que s√£o lineares e n√£o viesados, o estimador de m√≠nimos quadrados tem a menor vari√¢ncia. Este resultado √© conhecido como o Teorema de Gauss-Markov.
3.  **Simplicidade e Efici√™ncia Computacional:**  O m√©todo de m√≠nimos quadrados √© computacionalmente eficiente e f√°cil de implementar. A solu√ß√£o pode ser encontrada analiticamente atrav√©s de opera√ß√µes matriciais ou atrav√©s de m√©todos iterativos como os algoritmos de decomposi√ß√£o de matrizes.
4.  **Sensibilidade a Outliers:** Uma limita√ß√£o do m√©todo de m√≠nimos quadrados √© a sua sensibilidade a *outliers*, ou seja, a observa√ß√µes com um valor muito diferente do resto dos dados. A presen√ßa de *outliers* pode influenciar fortemente a solu√ß√£o por m√≠nimos quadrados e levar a um modelo com um ajuste pobre para os dados "normais".
5.  **Sensibilidade √† Multicolinearidade:** Modelos de regress√£o linear com preditores altamente correlacionados podem levar a estimativas inst√°veis e com alta vari√¢ncia, onde os par√¢metros podem ser estimados com baixas magnitudes ou com sinais inesperados. Modelos com alta multicolinearidade levam a estimativas de par√¢metros com grande sensibilidade a pequenas varia√ß√µes nos dados. O m√©todo de m√≠nimos quadrados, por si s√≥, n√£o consegue lidar com este problema, e √© necess√°rio o uso de t√©cnicas como a regulariza√ß√£o para lidar com a multicolinearidade.
6.   **Problemas com Overfitting**: Modelos com muitos preditores podem sofrer de overfitting, o que significa que o modelo se ajusta muito bem aos dados de treinamento, mas tem uma capacidade preditiva ruim quando aplicado a outros conjuntos de dados. O m√©todo dos m√≠nimos quadrados, por si s√≥, n√£o oferece um mecanismo para lidar com o *overfitting*.

> ‚ö†Ô∏è **Nota Importante**: A soma dos quadrados dos res√≠duos (RSS) quantifica o erro do ajuste do modelo linear aos dados. O objetivo do m√©todo de m√≠nimos quadrados √© encontrar os par√¢metros que minimizam o RSS. [^10].

> ‚ùó **Ponto de Aten√ß√£o**: Geometricamente, a solu√ß√£o de m√≠nimos quadrados corresponde √† proje√ß√£o ortogonal do vetor de resposta no espa√ßo gerado pelas colunas da matriz de design. [^12].

> ‚úîÔ∏è **Destaque**: Os estimadores de m√≠nimos quadrados, quando aplicados de forma correta, s√£o n√£o viesados e os melhores estimadores lineares n√£o viesados. [^13].

### Conclus√£o

O m√©todo de estima√ß√£o por m√≠nimos quadrados √© uma ferramenta fundamental na modelagem de regress√£o linear, com uma s√≥lida base matem√°tica, e com importantes propriedades estat√≠sticas e geom√©tricas. A minimiza√ß√£o da soma dos quadrados dos res√≠duos (RSS) conduz a uma solu√ß√£o eficiente para estimar os par√¢metros do modelo. Apesar de ser sens√≠vel a *outliers* e a multicolinearidade, e propenso ao *overfitting* em problemas com muitos par√¢metros, o m√©todo dos m√≠nimos quadrados √© um ponto de partida crucial para a modelagem, e pode ser combinado com t√©cnicas de regulariza√ß√£o e m√©todos de sele√ß√£o de vari√°veis para melhor lidar com estas limita√ß√µes.

### Refer√™ncias

[^10]: "The most popular estimation method is least squares, in which we pick the coefficients $\beta = (\beta_0, \beta_1, \ldots, \beta_p)^T$ to minimize the residual sum of squares" *(Trecho de Linear Methods for Regression)*
[^12]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Methods for Regression)*
[^13]: "We minimize $RSS(3) = ||y ‚Äì X\beta||^2$ by choosing $\beta$ so that the residual vector $y - \hat{y}$ is orthogonal to this subspace." *(Trecho de Linear Methods for Regression)*
[^14]: "The predicted values at an input vector $x_o$ are given by $f(x_o) = (1 : x_o)^T\beta$" *(Trecho de Linear Methods for Regression)*
[^25]: "When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and exhibit high variance. A wildly large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin." *(Trecho de Linear Methods for Regression)*
[^1]: "A linear regression model assumes that the regression function $E(Y|X)$ is linear in the inputs $X_1, \ldots, X_p$." *(Trecho de Linear Methods for Regression)*
