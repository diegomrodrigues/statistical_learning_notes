## Interpreta√ß√£o Geom√©trica: Proje√ß√£o Ortogonal de y no Espa√ßo das Colunas de X e a Matriz Hat (H = X(X·µÄX)‚Åª¬πX·µÄ) como Matriz de Proje√ß√£o

```mermaid
  flowchart LR
      A["Vetor y (Respostas Observadas)"]
      B["Subespa√ßo das Colunas de X"]
      C["Vetor ≈∑ (Proje√ß√£o de y)"]
      D["Vetor de Res√≠duos (y-≈∑)"]
      E["Matriz de Proje√ß√£o H"]

      A -->|Projeta| B
      B --> C
      A --> D
      E --> C
      style A fill:#f9f,stroke:#333,stroke-width:2px
      style B fill:#ccf,stroke:#333,stroke-width:2px
      style C fill:#cfc,stroke:#333,stroke-width:2px
      style D fill:#fcc,stroke:#333,stroke-width:2px
      style E fill:#cff,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

A interpreta√ß√£o geom√©trica da regress√£o linear fornece *insights* valiosos sobre o significado da solu√ß√£o por m√≠nimos quadrados. A ideia central √© que a solu√ß√£o $\hat{\beta} = (X^T X)^{-1} X^T y$ resulta na proje√ß√£o ortogonal do vetor de respostas $y$ no subespa√ßo gerado pelas colunas da matriz de design $X$. Nesta se√ß√£o, exploraremos como a matriz **Hat**, ou matriz de proje√ß√£o, definida por $H = X(X^T X)^{-1}X^T$, realiza essa proje√ß√£o e detalharemos as implica√ß√µes geom√©tricas e matem√°ticas dessa opera√ß√£o.

### A Proje√ß√£o Ortogonal de y no Espa√ßo das Colunas de X

Na regress√£o linear, o objetivo √© aproximar um vetor de respostas $y$ por uma combina√ß√£o linear das colunas da matriz de design $X$. Essa aproxima√ß√£o pode ser vista como uma proje√ß√£o ortogonal do vetor $y$ no subespa√ßo gerado pelas colunas de $X$, onde a dist√¢ncia entre o vetor resposta original $y$ e o sua proje√ß√£o nesse subespa√ßo √© minimizada.
Matematicamente, o subespa√ßo das colunas de $X$ √© definido por:

$$
\text{ran}(X) = \{X\beta : \beta \in \mathbb{R}^{p+1} \}
$$

onde:
- $\text{ran}(X)$ indica o espa√ßo gerado pelas colunas de $X$.
- $\beta$ √© o vetor de par√¢metros que corresponde a uma combina√ß√£o linear das colunas da matriz $X$.

A proje√ß√£o ortogonal de $y$ em $\text{ran}(X)$ corresponde ao vetor $\hat{y}$ que minimiza a dist√¢ncia euclidiana $||y - \hat{y}||$ e √© tamb√©m tal que o res√≠duo $y - \hat{y}$ √© ortogonal ao espa√ßo $\text{ran}(X)$. O vetor $\hat{y}$ √© dado por:
$$
\hat{y} = X\hat{\beta}
$$
onde $\hat{\beta}$ √© a solu√ß√£o por m√≠nimos quadrados.
Geometricamente, o vetor $\hat{y}$ √© a proje√ß√£o ortogonal do vetor de respostas $y$ no subespa√ßo gerado pelas colunas de $X$. Essa proje√ß√£o √© a solu√ß√£o do problema de m√≠nimos quadrados, j√° que ela minimiza a dist√¢ncia entre o vetor $y$ e o subespa√ßo $\text{ran}(X)$.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simples com duas observa√ß√µes e um √∫nico preditor (al√©m do intercepto). Suponha que temos:
>
> $$
> X = \begin{bmatrix}
> 1 & 2 \\
> 1 & 3
> \end{bmatrix}, \quad y = \begin{bmatrix}
> 5 \\
> 8
> \end{bmatrix}
> $$
>
> Aqui, a primeira coluna de $X$ √© o intercepto (todos 1s) e a segunda coluna √© o preditor. O vetor $y$ cont√©m os valores de resposta. Vamos calcular $\hat{\beta}$ e $\hat{y}$.
>
> **Passo 1: Calcular $X^T X$**
>
> $$
> X^T X = \begin{bmatrix}
> 1 & 1 \\
> 2 & 3
> \end{bmatrix} \begin{bmatrix}
> 1 & 2 \\
> 1 & 3
> \end{bmatrix} = \begin{bmatrix}
> 2 & 5 \\
> 5 & 13
> \end{bmatrix}
> $$
>
> **Passo 2: Calcular $(X^T X)^{-1}$**
>
> $$
> (X^T X)^{-1} = \frac{1}{(2 \times 13) - (5 \times 5)} \begin{bmatrix}
> 13 & -5 \\
> -5 & 2
> \end{bmatrix} = \begin{bmatrix}
> 13 & -5 \\
> -5 & 2
> \end{bmatrix}
> $$
>
> **Passo 3: Calcular $X^T y$**
>
> $$
> X^T y = \begin{bmatrix}
> 1 & 1 \\
> 2 & 3
> \end{bmatrix} \begin{bmatrix}
> 5 \\
> 8
> \end{bmatrix} = \begin{bmatrix}
> 13 \\
> 34
> \end{bmatrix}
> $$
>
> **Passo 4: Calcular $\hat{\beta} = (X^T X)^{-1} X^T y$**
>
> $$
> \hat{\beta} = \begin{bmatrix}
> 13 & -5 \\
> -5 & 2
> \end{bmatrix} \begin{bmatrix}
> 13 \\
> 34
> \end{bmatrix} = \begin{bmatrix}
> 13 \times 13 + (-5) \times 34 \\
> (-5) \times 13 + 2 \times 34
> \end{bmatrix} = \begin{bmatrix}
> -37 \\
> 3
> \end{bmatrix}
> $$
>
> **Passo 5: Calcular $\hat{y} = X\hat{\beta}$**
>
> $$
> \hat{y} = \begin{bmatrix}
> 1 & 2 \\
> 1 & 3
> \end{bmatrix} \begin{bmatrix}
> -37 \\
> 17
> \end{bmatrix} = \begin{bmatrix}
> 1 \times -37 + 2 \times 17 \\
> 1 \times -37 + 3 \times 17
> \end{bmatrix} = \begin{bmatrix}
> -3 \\
> 14
> \end{bmatrix}
> $$
>
> Assim, $\hat{\beta} = \begin{bmatrix} -37 \\ 17 \end{bmatrix}$ e $\hat{y} = \begin{bmatrix} -3 \\ 14 \end{bmatrix}$. O vetor $\hat{y}$ √© a proje√ß√£o ortogonal de $y$ no espa√ßo coluna de $X$. O res√≠duo √© $y - \hat{y} = \begin{bmatrix} 5 \\ 8 \end{bmatrix} - \begin{bmatrix} -3 \\ 14 \end{bmatrix} = \begin{bmatrix} 8 \\ -6 \end{bmatrix}$. Pode ser verificado que o res√≠duo √© ortogonal √†s colunas de $X$.

### A Matriz Hat (H): Proje√ß√£o Ortogonal

A matriz **Hat** ou matriz de proje√ß√£o, denotada por $H$, desempenha um papel central na visualiza√ß√£o e compreens√£o do processo de m√≠nimos quadrados [^46]. Ela √© definida como:

$$
H = X(X^T X)^{-1} X^T
$$
onde:
- $X$ √© a matriz de design.
- $(X^TX)^{-1}$ √© a inversa da matriz $X^TX$.
- $X^T$ √© a transposta da matriz $X$.

A matriz Hat tem as seguintes propriedades importantes:
1. **Proje√ß√£o Ortogonal:** Ao multiplicar o vetor de resposta $y$ por $H$, obtemos o vetor de predi√ß√µes $\hat{y}$, que corresponde √† proje√ß√£o ortogonal de $y$ no subespa√ßo gerado pelas colunas de X:
 $$ \hat{y} = Hy = X (X^T X)^{-1} X^T y $$

```mermaid
sequenceDiagram
    participant y
    participant H
    participant ≈∑
    y->>H: Multiplica
    H-->>≈∑: Retorna a proje√ß√£o de y
```

2. **Idempot√™ncia:** A matriz Hat √© idempotente, o que significa que $H^2 = H$. Isso demonstra que ao aplicar a proje√ß√£o uma segunda vez o resultado permanece o mesmo, e tamb√©m corresponde ao fato que a proje√ß√£o de um vetor que est√° em um subespa√ßo, nesse mesmo subespa√ßo, √© igual a ele mesmo.
$$
H^2 = (X(X^T X)^{-1}X^T)(X(X^T X)^{-1}X^T) = X(X^T X)^{-1}(X^TX) (X^T X)^{-1}X^T
= X(X^T X)^{-1} X^T = H
$$
3. **Simetria:** A matriz Hat √© sim√©trica, o que significa que $H^T = H$.
$$
H^T = (X(X^T X)^{-1}X^T)^T= X( (X^T X)^{-1})^TX^T
$$
Como $X^TX$ √© sim√©trica, ent√£o $X^T X = (X^T X)^T$, e como a transposta da inversa √© a inversa da transposta, temos que $((X^TX)^{-1})^T = (X^TX)^{-1}$, o que significa que $H^T = H$.

4.  **Conex√£o com Res√≠duos:** A matriz ($I - H$), onde $I$ √© a matriz identidade, projeta o vetor de respostas no espa√ßo ortogonal ao espa√ßo gerado pelas colunas de X, resultando no vetor de res√≠duos:
 $$
 y - \hat{y} = (I - H)y
 $$
```mermaid
sequenceDiagram
    participant y
    participant I-H
    participant r
    y->>I-H: Multiplica
    I-H-->>r: Retorna o vetor de res√≠duos (y-≈∑)
```
5. **Depend√™ncia de X:** √â importante enfatizar que a matriz Hat depende apenas da matriz de design X, e n√£o do vetor de respostas $y$. Isso significa que uma vez que a matriz de design $X$ √© determinada, a matriz de proje√ß√£o fica definida.

> üí° **Exemplo Num√©rico (continua√ß√£o):**
>
> Usando os mesmos dados do exemplo anterior, vamos calcular a matriz Hat $H$ e verificar suas propriedades.
>
> **Passo 1: Calcular a Matriz Hat $H = X(X^T X)^{-1} X^T$**
>
> J√° calculamos $(X^T X)^{-1}$ no exemplo anterior:
>
> $$
> (X^T X)^{-1} = \begin{bmatrix}
> 13 & -5 \\
> -5 & 2
> \end{bmatrix}
> $$
>
> Agora, calculamos $H$:
>
> $$
> H = \begin{bmatrix}
> 1 & 2 \\
> 1 & 3
> \end{bmatrix} \begin{bmatrix}
> 13 & -5 \\
> -5 & 2
> \end{bmatrix} \begin{bmatrix}
> 1 & 1 \\
> 2 & 3
> \end{bmatrix} = \begin{bmatrix}
> 3 & -1 \\
> -2 & 1
> \end{bmatrix} \begin{bmatrix}
> 1 & 1 \\
> 2 & 3
> \end{bmatrix} = \begin{bmatrix}
> 1 & 0 \\
> 0 & 1
> \end{bmatrix}
> $$
>
> **Passo 2: Verificar a Proje√ß√£o Ortogonal $\hat{y} = Hy$**
>
> $$
> \hat{y} = Hy = \begin{bmatrix}
> 1 & 0 \\
> 0 & 1
> \end{bmatrix} \begin{bmatrix}
> 5 \\
> 8
> \end{bmatrix} = \begin{bmatrix}
> 5 \\
> 8
> \end{bmatrix}
> $$
>
> Neste caso particular, como $X$ tem posto completo, a proje√ß√£o de $y$ no espa√ßo das colunas de $X$ retorna o pr√≥prio $y$. No entanto, em casos gerais, $y$ ser√° projetado no subespa√ßo das colunas de $X$.
>
> **Passo 3: Verificar a Idempot√™ncia $H^2 = H$**
>
> $$
> H^2 = \begin{bmatrix}
> 1 & 0 \\
> 0 & 1
> \end{bmatrix} \begin{bmatrix}
> 1 & 0 \\
> 0 & 1
> \end{bmatrix} = \begin{bmatrix}
> 1 & 0 \\
> 0 & 1
> \end{bmatrix} = H
> $$
>
> **Passo 4: Verificar a Simetria $H^T = H$**
>
> $$
> H^T = \begin{bmatrix}
> 1 & 0 \\
> 0 & 1
> \end{bmatrix}^T = \begin{bmatrix}
> 1 & 0 \\
> 0 & 1
> \end{bmatrix} = H
> $$
>
> **Passo 5: Calcular os Res√≠duos $(I - H)y$**
>
> $$
> (I - H)y = \left( \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} - \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \right) \begin{bmatrix} 5 \\ 8 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 5 \\ 8 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
> $$
>
> Como esperado, os res√≠duos s√£o zero neste caso, pois $y$ j√° pertence ao espa√ßo gerado pelas colunas de $X$.

A matriz hat (H) tamb√©m √© fundamental na an√°lise das propriedades de modelos de regress√£o linear. Por exemplo, a diagonal da matriz $H$ corresponde aos valores de *leverage*, que indicam o quanto uma observa√ß√£o individual influencia o modelo de regress√£o.

**Lemma 19:** Proje√ß√£o Ortogonal e Dist√¢ncia M√≠nima
A proje√ß√£o ortogonal √© o ponto no espa√ßo gerado por X que est√° mais pr√≥ximo do vetor de respostas. Os vetores de res√≠duos $r = y - X\hat{\beta}$  s√£o ortogonais ao espa√ßo gerado pelas colunas de $X$.

**Prova do Lemma 19:**
A condi√ß√£o de m√≠nimos quadrados √© dada por $\frac{\partial RSS(\beta)}{\partial \beta} = -2X^T(y-X\beta) = 0$, o que implica que $X^T(y-X\hat{\beta}) = 0$. Ao multiplicar a condi√ß√£o por um vetor qualquer $a$, temos:
$$a^T X^T(y-X\hat{\beta}) = 0$$
$$ (Xa)^T(y-X\hat{\beta})=0$$
Como $Xa$ √© uma combina√ß√£o linear das colunas de $X$, o res√≠duo $y - X\hat{\beta}$ √© ortogonal a todos os vetores nesse subespa√ßo. $\blacksquare$

**Corol√°rio 19:**  Proje√ß√£o Ortogonal como a Solu√ß√£o do Problema de M√≠nimos Quadrados

O Lemma 19 garante que a solu√ß√£o de m√≠nimos quadrados √© o resultado da proje√ß√£o ortogonal do vetor de resposta no subespa√ßo gerado pelos preditores. A condi√ß√£o de ortogonalidade dos res√≠duos √© essencial para garantir a minimiza√ß√£o da dist√¢ncia euclidiana.
O Corol√°rio implica tamb√©m que a matriz de proje√ß√£o, $H = X(X^TX)^{-1}X^T$ computa o vetor $\hat{y}$ como a proje√ß√£o ortogonal do vetor $y$ no espa√ßo gerado pelas colunas da matriz $X$.

### Implica√ß√µes da Interpreta√ß√£o Geom√©trica

A interpreta√ß√£o geom√©trica da regress√£o linear e o conceito da matriz Hat t√™m diversas implica√ß√µes:
    1.  **Visualiza√ß√£o da Solu√ß√£o:**  Ela permite visualizar a solu√ß√£o do problema de m√≠nimos quadrados como a proje√ß√£o ortogonal de um vetor em um subespa√ßo.
    2. **Conex√£o entre √Ålgebra e Geometria:** Demonstra como o conceito da √°lgebra linear da proje√ß√£o ortogonal est√° relacionado ao problema da regress√£o linear.
    3.  **Interpreta√ß√£o do Res√≠duo:** Ao projetar o vetor de resposta, $y$, no subespa√ßo gerado por X, a diferen√ßa √© o vetor de res√≠duos, que √© perpendicular a esse subespa√ßo.
    4.  **Papel da Matriz Hat:** A matriz Hat, H,  √© uma ferramenta que realiza a proje√ß√£o e que permite que se entenda como a solu√ß√£o de m√≠nimos quadrados √© obtida.
    5. **Base para An√°lise da Influ√™ncia:** A diagonal da matriz Hat cont√©m a informa√ß√£o da *leverage* de cada ponto, ou seja, o quanto um ponto influencia a posi√ß√£o do hiperplano de regress√£o.
    6.  **Estabilidade e Condicionamento:** A interpreta√ß√£o geom√©trica nos ajuda a entender a import√¢ncia da ortogonalidade e as limita√ß√µes de modelos com preditores altamente colineares (onde o √¢ngulo entre vetores se aproxima de 0) .

### Conclus√£o

A interpreta√ß√£o geom√©trica do problema de regress√£o linear e a utiliza√ß√£o da matriz Hat como projetor s√£o ferramentas essenciais para uma compreens√£o mais profunda do funcionamento do modelo linear e dos algoritmos de m√≠nimos quadrados. A proje√ß√£o ortogonal √© a solu√ß√£o geom√©trica do problema, e a matriz Hat quantifica o efeito dessa proje√ß√£o. A matriz Hat tamb√©m √© √∫til na an√°lise de sensibilidade e nas propriedades dos par√¢metros estimados, mostrando que modelos com alta multicolinearidade podem ser sens√≠veis a varia√ß√µes nos dados.

### Refer√™ncias

[^46]: "The predicted values at an input vector xo are given by f(xo) = (1 : xo)T·∫û;" *(Trecho de Linear Methods for Regression)*
[^12]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Methods for Regression)*
[^13]: "We minimize RSS(3) = ||y ‚Äì XŒ≤||2 by choosing ·∫û so that the residual vector y - ≈∑ is orthogonal to this subspace." *(Trecho de Linear Methods for Regression)*
[^11]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
