## Suposi√ß√µes Estat√≠sticas em Modelos de Regress√£o Linear: Expectativa Condicional Linear e Erro Gaussiano Aditivo

```mermaid
graph LR
    A["Modelo de Regress√£o Linear"] --> B("Suposi√ß√£o 1: Expectativa Condicional Linear E(Y|X)");
    A --> C("Suposi√ß√£o 2: Erro Gaussiano Aditivo Œµ ~ N(0, œÉ¬≤)");
    B --> D("E(Y|X) = Œ≤‚ÇÄ + Œ£(X‚±ºŒ≤‚±º)");
    C --> E("Erros independentes, m√©dia 0, vari√¢ncia constante");
    D --> F("Efeito linear dos preditores");
    E --> G("Distribui√ß√£o normal dos erros");
    F & G --> H("Base para infer√™ncia e predi√ß√£o");
```

### Introdu√ß√£o
Os modelos de regress√£o linear s√£o constru√≠dos com base em um conjunto de suposi√ß√µes estat√≠sticas, que garantem a validade te√≥rica do modelo e as propriedades dos estimadores. As duas suposi√ß√µes mais importantes s√£o: a linearidade da expectativa condicional de $Y$ dado $X$ e a natureza gaussiana e aditiva do erro, $\epsilon \sim \mathcal{N}(0, \sigma^2)$. Nesta se√ß√£o, vamos explorar cada uma dessas suposi√ß√µes, discutir suas implica√ß√µes, e como a viola√ß√£o dessas premissas afeta a validade e as propriedades do modelo.

### A Suposi√ß√£o da Linearidade da Expectativa Condicional

A primeira suposi√ß√£o fundamental em modelos de regress√£o linear √© que a **expectativa condicional da vari√°vel resposta Y dado os preditores X √© linear** [^1]. Matematicamente, esta suposi√ß√£o √© expressa como:
$$
E(Y|X) = \beta_0 + \sum_{j=1}^p X_j \beta_j
$$
onde:
   -  $E(Y|X)$ representa a expectativa condicional da vari√°vel resposta $Y$ dado um conjunto de preditores $X$.
   -  $\beta_0$ √© o intercepto.
   - $X_j$ s√£o as vari√°veis preditoras.
   - $\beta_j$ s√£o os coeficientes associados √†s vari√°veis preditoras.
   - $p$ √© o n√∫mero de preditores.
Esta suposi√ß√£o implica que o valor m√©dio de $Y$ para um dado conjunto de preditores $X$ se encontra sobre um hiperplano definido pelos par√¢metros $\beta$, e que o efeito de cada preditor na resposta √© linear. Isso n√£o quer dizer que a rela√ß√£o entre $Y$ e $X$ seja linear, e transforma√ß√µes nas vari√°veis podem ser utilizadas para ajustar o modelo a rela√ß√µes n√£o lineares, mantendo a linearidade nos par√¢metros.

A linearidade da expectativa condicional √© crucial pois garante que a fun√ß√£o de regress√£o linear $f(x) = \beta_0 + \sum_{j=1}^p X_j \beta_j$, seja uma aproxima√ß√£o razo√°vel de $E(Y|X)$.  Quando esta suposi√ß√£o n√£o se mant√©m, o modelo de regress√£o linear pode levar a estimativas enviesadas e predi√ß√µes incorretas, ou seja, pode apresentar alto *bias*.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados onde a vari√°vel resposta $Y$ √© o pre√ßo de uma casa e a vari√°vel preditora $X$ √© a √°rea da casa (em metros quadrados). A suposi√ß√£o de linearidade da expectativa condicional implica que o pre√ßo m√©dio da casa aumenta linearmente com a √°rea.
>
> Vamos assumir um modelo simples com apenas um preditor:
> $$E(Y|X) = \beta_0 + \beta_1 X$$
>
> Se ap√≥s ajustar o modelo obtivermos $\beta_0 = 50000$ e $\beta_1 = 1500$, isso significa que:
>
>  -  O pre√ßo base de uma casa (quando a √°rea √© 0) √© de R$50.000 (este valor n√£o tem interpreta√ß√£o pr√°tica neste exemplo).
>  -  Para cada metro quadrado adicional, o pre√ßo m√©dio da casa aumenta em R$1.500.
>
>  Se uma casa tem 100 metros quadrados, o pre√ßo m√©dio estimado seria:
>  $$E(Y|X=100) = 50000 + 1500 * 100 = 200000$$
>
>  Agora, se a rela√ß√£o real fosse n√£o linear, por exemplo, o pre√ßo aumentasse menos para √°reas maiores (efeito de satura√ß√£o), a suposi√ß√£o de linearidade seria violada. Nesse caso, poder√≠amos usar uma transforma√ß√£o nos dados como $\log(X)$ ou $X^2$ para modelar a n√£o linearidade, mantendo a linearidade nos par√¢metros do modelo transformado.

### A Suposi√ß√£o do Erro Gaussiano Aditivo

A segunda suposi√ß√£o fundamental em modelos de regress√£o linear √© que os erros aleat√≥rios $\epsilon_i$ s√£o independentes e identicamente distribu√≠dos com uma distribui√ß√£o Gaussiana (normal) com m√©dia zero e vari√¢ncia constante $\sigma^2$. Essa suposi√ß√£o √© expressa como:
$$
\epsilon_i \sim \mathcal{N}(0, \sigma^2)
$$
ou equivalentemente como
$$
\epsilon \sim \mathcal{N}(0, \sigma^2 I)
$$
onde:
   -  $\epsilon_i$ √© o erro aleat√≥rio associado √† i-√©sima observa√ß√£o.
   -  $\mathcal{N}$ denota a distribui√ß√£o normal (Gaussiana).
   -  $0$ √© a m√©dia zero, ou seja, $E(\epsilon_i)=0$.
   -  $\sigma^2$ √© a vari√¢ncia constante, que significa que a vari√¢ncia do erro √© a mesma para todas as observa√ß√µes, ou seja $Var(\epsilon_i)=\sigma^2$.
        - $I$ √© a matriz identidade.

Essa suposi√ß√£o implica que os erros s√£o distribu√≠dos simetricamente em torno da sua m√©dia (zero), com uma probabilidade que decai rapidamente com o aumento da dist√¢ncia a partir da m√©dia. Esta suposi√ß√£o n√£o imp√µe que os dados, por si s√≥, tenham distribui√ß√£o normal.
O erro gaussiano tamb√©m √© denominado como **aditivo**, j√° que ele √© somado ao valor esperado da resposta, como em:

```mermaid
flowchart LR
    A["Y (Observa√ß√£o)"] -->|Modelo Linear| B{"E(Y|X) (Expectativa)"};
    B -->|Erro Aditivo Œµ| C["Y = E(Y|X) + Œµ"];
```

$$
Y = \beta_0 + \sum_{j=1}^p X_j \beta_j + \epsilon
$$
Onde o erro $\epsilon$ √© independente dos preditores.

As suposi√ß√µes sobre o erro garantem a validade de infer√™ncias estat√≠sticas no modelo de regress√£o linear, al√©m de justificar o uso da estima√ß√£o por m√≠nimos quadrados [^47].

> üí° **Exemplo Num√©rico:**
>
> Imagine que estamos modelando a altura de plantas ($Y$) em fun√ß√£o da quantidade de fertilizante ($X$). Ap√≥s ajustar um modelo linear, obtemos os seguintes resultados para algumas observa√ß√µes:
>
> | Observa√ß√£o | Fertilizante (X) | Altura Observada (Y) | Altura Estimada (≈∂) | Erro (Œµ = Y - ≈∂) |
> |------------|-------------------|---------------------|--------------------|-------------------|
> | 1          | 2                 | 10                  | 11                 | -1                |
> | 2          | 4                 | 15                  | 14                 | 1                 |
> | 3          | 6                 | 18                  | 19                 | -1                |
> | 4          | 8                 | 24                  | 23                 | 1                 |
> | 5          | 10                | 28                  | 29                 | -1                |
>
> A suposi√ß√£o de erro gaussiano aditivo implica que esses erros (-1, 1, -1, 1, -1) devem se comportar como amostras aleat√≥rias de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia constante.
>
> Se calcularmos a m√©dia desses erros, obteremos 0, e a vari√¢ncia amostral ser√° pr√≥xima a $\sigma^2$, que √© a vari√¢ncia estimada dos erros. Idealmente, se plotarmos um histograma ou um gr√°fico de probabilidade normal dos erros, deveremos ver uma distribui√ß√£o aproximadamente normal centrada em zero.
>
> A suposi√ß√£o de vari√¢ncia constante (homocedasticidade) significa que a dispers√£o dos erros deve ser similar para todos os n√≠veis de fertilizante. Se, por exemplo, a vari√¢ncia dos erros aumentasse com a quantidade de fertilizante, ter√≠amos uma viola√ß√£o da homocedasticidade (heterocedasticidade).

**Lemma 26:** O Teorema de Gauss-Markov

Sob as suposi√ß√µes de que $E(Y|X)$ √© linear e que os erros $\epsilon_i$ t√™m m√©dia zero, vari√¢ncia constante, e s√£o n√£o correlacionados, os estimadores de m√≠nimos quadrados s√£o os melhores estimadores lineares n√£o viesados (BLUE, *Best Linear Unbiased Estimators*). Este resultado, chamado de Teorema de Gauss-Markov, demonstra que a solu√ß√£o por m√≠nimos quadrados tem um √≥timo desempenho, na classe de estimadores lineares n√£o viesados. A suposi√ß√£o da distribui√ß√£o normal para os erros n√£o √© necess√°ria para que este resultado se mantenha.

**Prova do Lemma 26:**
Para demonstrar o teorema de Gauss-Markov, seja $\hat{\beta} = (X^T X)^{-1} X^T y$ o estimador de m√≠nimos quadrados e seja $\tilde{\beta} = Cy$ um outro estimador linear n√£o viesado de $\beta$. Para que o estimador $\tilde{\beta}$ seja n√£o viesado, √© necess√°rio que $E[\tilde{\beta}] = E[Cy] = \beta$, e usando $y = X\beta+\epsilon$ temos que $E[C(X\beta+\epsilon)] = CX\beta = \beta$, ou seja $CX = I$ onde $I$ √© a matriz identidade. A vari√¢ncia do estimador $\tilde{\beta}$ √© dada por:
$$
Var(\tilde{\beta}) = Var(Cy) = C Var(y) C^T = \sigma^2 C C^T
$$
Como $Var(\hat{\beta}) = \sigma^2 (X^T X)^{-1}$, e $CX=I$, podemos escrever:
$$
C C^T - (X^T X)^{-1} = (C - (X^TX)^{-1}X^T)(C - (X^TX)^{-1}X^T)^T + (X^TX)^{-1}
$$
Como $(C - (X^TX)^{-1}X^T)(C - (X^TX)^{-1}X^T)^T$ √© uma matriz positiva semi definida, ent√£o
$$
C C^T \geq (X^T X)^{-1}
$$
e portanto,
$$
Var(\tilde{\beta}) \geq Var(\hat{\beta})
$$
o que demonstra que o estimador de m√≠nimos quadrados √© um estimador com menor vari√¢ncia em rela√ß√£o a um estimador gen√©rico n√£o viesado. $\blacksquare$

**Corol√°rio 26:** Interpreta√ß√£o Geom√©trica da Proje√ß√£o Ortogonal

O Lemma 26, combinado com a interpreta√ß√£o geom√©trica da solu√ß√£o por m√≠nimos quadrados, mostra que a proje√ß√£o ortogonal do vetor $y$ no espa√ßo gerado por X fornece o estimador n√£o viesado de menor vari√¢ncia quando $E(Y|X)$ √© linear e os erros s√£o homoced√°sticos e n√£o correlacionados. O modelo linear √© a "melhor" aproxima√ß√£o linear da expectativa condicional, e tamb√©m do ponto de vista preditivo.

### Implica√ß√µes das Suposi√ß√µes Estat√≠sticas

As suposi√ß√µes de linearidade da expectativa condicional e do erro gaussiano aditivo t√™m diversas implica√ß√µes:

1. **Validade da Estima√ß√£o:** A linearidade da expectativa condicional garante que o m√©todo dos m√≠nimos quadrados produz estimativas n√£o viesadas para os par√¢metros do modelo, quando o modelo √© corretamente especificado, o que √© fundamental para infer√™ncias estat√≠sticas e predi√ß√µes confi√°veis.
2.  **Teorema de Gauss-Markov:** A suposi√ß√£o do erro gaussiano garante que a estima√ß√£o por m√≠nimos quadrados seja BLUE (melhor estimador linear n√£o viesado) e tamb√©m garante a validade das infer√™ncias estat√≠sticas, que se baseiam na distribui√ß√£o normal dos erros.
3.  **Justifica√ß√£o para a Soma dos Quadrados dos Res√≠duos:** A hip√≥tese de erro gaussiano aditivo √© uma justificativa para o uso da fun√ß√£o RSS como uma fun√ß√£o de custo, j√° que a distribui√ß√£o normal do erro √© uma consequ√™ncia de assumir que o modelo linear captura a estrutura principal dos dados, e as varia√ß√µes remanescentes s√£o aleat√≥rias.
4.  **Incerteza e Infer√™ncia:** As propriedades da distribui√ß√£o normal do erro s√£o utilizadas para realizar testes de hip√≥teses e construir intervalos de confian√ßa para os par√¢metros do modelo.
5. **Aproxima√ß√µes da Realidade:** √â importante entender que ambas as suposi√ß√µes s√£o aproxima√ß√µes da realidade, e que na maioria das situa√ß√µes pr√°ticas, as propriedades do modelo linear podem ser afetadas pela viola√ß√£o destas suposi√ß√µes. T√©cnicas como a regulariza√ß√£o podem auxiliar a construir modelos mais robustos, independentemente da distribui√ß√£o dos erros.

### Viola√ß√£o das Suposi√ß√µes e suas Consequ√™ncias

√â importante notar que nem sempre as suposi√ß√µes estat√≠sticas s√£o v√°lidas. As viola√ß√µes dessas suposi√ß√µes podem levar a resultados enviesados e a um desempenho preditivo fraco.
    1. **N√£o Linearidade:** Se a verdadeira rela√ß√£o entre os preditores e a vari√°vel resposta √© n√£o linear, a suposi√ß√£o de que $E(Y|X)$ √© uma fun√ß√£o linear dos preditores ser√° violada, o que pode resultar em um modelo com alto *bias* e com pobre performance preditiva. Nesses casos, transforma√ß√µes nos preditores, ou m√©todos n√£o lineares, devem ser utilizados.
   2. **Heteroscedasticidade:** Se a vari√¢ncia do erro n√£o √© constante e varia com os n√≠veis dos preditores, ent√£o as estimativas de m√≠nimos quadrados ser√£o menos eficientes do que outros modelos (que levam em considera√ß√£o a heteroscedasticidade). Nestes casos, m√©todos de m√≠nimos quadrados ponderados devem ser utilizados para obter estimativas mais precisas.
   3. **Erros N√£o Normais:** A hip√≥tese de que os erros tem distribui√ß√£o normal permite testes estat√≠sticos e constru√ß√£o de intervalos de confian√ßa. Quando esta hip√≥tese √© violada, outros testes devem ser utilizados, e as infer√™ncias estat√≠sticas podem n√£o ser v√°lidas.

> üí° **Exemplo Num√©rico (Viola√ß√£o da Normalidade):**
>
> Suponha que, ao analisar os res√≠duos de um modelo de regress√£o, observamos que a distribui√ß√£o dos erros √© fortemente assim√©trica √† direita, ou seja, h√° muitos erros pequenos e poucos erros grandes positivos. Isso indicaria uma viola√ß√£o da suposi√ß√£o de normalidade.
>
> Uma forma de visualizar isso seria atrav√©s de um histograma dos res√≠duos:
>
> ```mermaid
>  graph LR
>      A[Res√≠duos] --> B(Histograma);
>      B --> C{Assimetria √† Direita};
> ```
>
> Neste caso, testes de hip√≥tese e intervalos de confian√ßa baseados na distribui√ß√£o normal podem ser inv√°lidos, e devemos considerar outros m√©todos de infer√™ncia estat√≠stica ou transformar a vari√°vel resposta.
>
> üí° **Exemplo Num√©rico (Heterocedasticidade):**
>
> Imagine que estamos modelando o tempo de conclus√£o de uma tarefa em fun√ß√£o do n√∫mero de participantes. Se a variabilidade dos tempos de conclus√£o aumentar conforme o n√∫mero de participantes cresce, teremos heterocedasticidade. Isso pode ser visualizado em um gr√°fico dos res√≠duos contra os valores preditos:
>
> ```mermaid
>  graph LR
>      A[Valores Preditos] --> B(Gr√°fico de Res√≠duos);
>      B --> C{Dispers√£o Crescente};
> ```
>
> Neste caso, a vari√¢ncia dos erros n√£o √© constante, e a suposi√ß√£o de homocedasticidade √© violada. Isso pode levar a estimativas de par√¢metros menos precisas e intervalos de confian√ßa inv√°lidos. Podemos usar m√≠nimos quadrados ponderados para levar em conta a heterocedasticidade.

√â fundamental verificar as suposi√ß√µes estat√≠sticas antes da aplica√ß√£o dos modelos de regress√£o, e considerar m√©todos de corre√ß√£o caso as suposi√ß√µes n√£o sejam v√°lidas. A validade de um modelo est√° diretamente ligada a qu√£o bem as suposi√ß√µes s√£o respeitadas.
A regulariza√ß√£o pode ser utilizada como uma ferramenta para lidar com a viola√ß√£o destas suposi√ß√µes, construindo modelos que s√£o mais robustos, menos suscet√≠veis ao ru√≠do dos dados e mais generaliz√°veis para dados n√£o vistos.

> ‚ö†Ô∏è **Nota Importante**: A suposi√ß√£o da linearidade de E(Y|X) implica que a rela√ß√£o entre a resposta e os preditores √© linear no espa√ßo dos preditores, e que transforma√ß√µes dos preditores podem ser utilizadas para modelar rela√ß√µes n√£o lineares.

> ‚ùó **Ponto de Aten√ß√£o**: A suposi√ß√£o do erro gaussiano aditivo, com m√©dia zero e vari√¢ncia constante, garante que as estimativas de m√≠nimos quadrados sejam os melhores estimadores lineares n√£o viesados.

> ‚úîÔ∏è **Destaque**: A viola√ß√£o das suposi√ß√µes de linearidade da expectativa condicional e do erro gaussiano aditivo podem levar a resultados enviesados e modelos pouco preditivos, o que justifica a import√¢ncia de verificar estas hip√≥teses.

### Conclus√£o

As suposi√ß√µes estat√≠sticas de que a expectativa condicional de $Y$ dado $X$ √© linear, e que os erros s√£o Gaussianos, aditivos, independentes e identicamente distribu√≠dos s√£o fundamentais para o desenvolvimento e aplica√ß√£o de modelos de regress√£o linear. Ao entender essas suposi√ß√µes e suas implica√ß√µes, e ao utilizar abordagens como as transforma√ß√µes, a regulariza√ß√£o, e o tratamento dos *outliers* e da multicolinearidade, podemos construir modelos mais robustos, generaliz√°veis e √∫teis para problemas do mundo real.

### Refer√™ncias

[^1]: "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,..., Xp." *(Trecho de Linear Methods for Regression)*
[^47]: "The N-p-1 rather than N in the denominator makes 6 an unbiased estimate of œÉ2: E(62) = œÉ2." *(Trecho de Linear Methods for Regression)*
[^11]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
