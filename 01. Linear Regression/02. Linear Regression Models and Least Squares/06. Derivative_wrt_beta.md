## Deriva√ß√£o da Soma dos Quadrados dos Res√≠duos em Rela√ß√£o a Œ≤: -2X·µÄ(y - XŒ≤)

```mermaid
flowchart LR
    A[Start] --> B{Define RSS(Œ≤) = $||y - X\beta||^2$};
    B --> C{Rewrite RSS(Œ≤) = $(y - X\beta)^T(y - X\beta)$};
    C --> D{Expand RSS(Œ≤) to $y^Ty - y^TX\beta - \beta^TX^Ty + \beta^TX^TX\beta$};
    D --> E{Simplify RSS(Œ≤) to $y^Ty - 2y^TX\beta + \beta^TX^TX\beta$};
    E --> F{Derive RSS(Œ≤) w.r.t. Œ≤};
    F --> G{Apply derivation rules};
    G --> H{Obtain $\frac{\partial RSS(\beta)}{\partial \beta} = -2X^Ty + 2X^TX\beta$};
    H --> I{Simplify to $-2X^T(y - X\beta)$};
    I --> J{Set derivative to zero: $-2X^T(y - X\beta) = 0$};
    J --> K{Solve for Œ≤: $\beta = (X^TX)^{-1}X^Ty$};
    K --> L[End];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style L fill:#f9f,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

A deriva√ß√£o da **Soma dos Quadrados dos Res√≠duos (RSS)** em rela√ß√£o aos par√¢metros $\beta$, que resulta na express√£o **-2X·µÄ(y - XŒ≤)**, √© um passo fundamental para a compreens√£o e resolu√ß√£o do problema de regress√£o linear por m√≠nimos quadrados. Esta derivada, tamb√©m conhecida como o gradiente da fun√ß√£o RSS, indica a dire√ß√£o do maior aumento do erro e serve como base para algoritmos de otimiza√ß√£o, como o m√©todo de m√≠nimos quadrados. Nesta se√ß√£o, vamos detalhar o processo de deriva√ß√£o, usando nota√ß√£o matricial, e explorar o significado do resultado.

### O Processo de Deriva√ß√£o

A fun√ß√£o da soma dos quadrados dos res√≠duos (RSS) √© definida como:
$$
RSS(\beta) = ||y - X\beta||^2
$$
onde:
-   $y$ √© o vetor de respostas.
-   $X$ √© a matriz de design.
-   $\beta$ √© o vetor de par√¢metros.

Para encontrar o vetor $\beta$ que minimiza o RSS, √© necess√°rio encontrar o seu gradiente com rela√ß√£o a $\beta$ e igualar a zero. O gradiente, neste caso, corresponde a derivada de $RSS(\beta)$ em rela√ß√£o ao vetor $\beta$. Para realizar a derivada em nota√ß√£o matricial, reescrevemos a equa√ß√£o do RSS utilizando o produto interno de vetores:

$$
RSS(\beta) = (y - X\beta)^T (y - X\beta)
$$
Expandindo esta express√£o, obtemos:
$$
RSS(\beta) = (y^T - (X\beta)^T) (y - X\beta) = y^Ty - y^TX\beta - \beta^T X^T y + \beta^T X^T X\beta
$$
Usando a propriedade de que $(AB)^T = B^TA^T$, temos que $y^T X\beta$ √© um escalar, e a sua transposta tamb√©m √© um escalar igual, e portanto $y^T X\beta = \beta^T X^T y$. Ent√£o podemos escrever o RSS como:
$$
RSS(\beta) = y^T y - 2 y^TX\beta + \beta^T X^TX \beta
$$
Agora, precisamos derivar esta express√£o com respeito a $\beta$. Usamos a seguinte propriedade da deriva√ß√£o matricial:
-   $\frac{\partial}{\partial x} a^Tx = a$
-    $\frac{\partial}{\partial x} x^T A x = (A+A^T)x$
onde $a$ √© um vetor, $x$ √© um vetor e $A$ √© uma matriz.
Aplicando estas propriedades, temos:
$$
\frac{\partial RSS(\beta)}{\partial \beta} =  \frac{\partial}{\partial \beta} (y^T y - 2 y^TX\beta + \beta^T X^TX \beta)
$$
$$
\frac{\partial RSS(\beta)}{\partial \beta} =  -2 X^T y + 2 X^T X\beta
$$
$$
\frac{\partial RSS(\beta)}{\partial \beta} = 2 X^T (X\beta - y)
$$
A solu√ß√£o por m√≠nimos quadrados se encontra onde esta derivada √© zero:
$$
2 X^T (X\beta - y) = 0
$$
$$
X^T X\beta = X^T y
$$
```mermaid
flowchart LR
    A[RSS Derivative = 0] --> B{$X^T(X\beta - y) = 0$};
    B --> C{$X^TX\beta = X^Ty$};
    C --> D{Solve for Œ≤: $(X^TX)\beta = X^Ty$};
    D --> E{$\hat{\beta} = (X^TX)^{-1}X^Ty$};
    style A fill:#f9f,stroke:#333,stroke-width:2px
```
$$
\hat{\beta} = (X^T X)^{-1} X^T y
$$
Portanto, a derivada da fun√ß√£o RSS com rela√ß√£o a $\beta$ √©:

$$
\frac{\partial RSS(\beta)}{\partial \beta} = -2 X^T (y - X\beta)
$$

O resultado **-2X·µÄ(y - XŒ≤)** √© o gradiente da fun√ß√£o RSS em rela√ß√£o ao vetor de par√¢metros $\beta$ [^3]. O sinal negativo indica que esta derivada aponta na dire√ß√£o oposta ao crescimento da fun√ß√£o RSS, e o valor √© duas vezes o produto da transposta da matriz de design com o vetor de res√≠duos, o que indica a dire√ß√£o de inclina√ß√£o da fun√ß√£o RSS.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simples com 3 pontos de dados para ilustrar o c√°lculo do gradiente.
>
> Seja $X$ a matriz de design:
> $$
> X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}
> $$
> e $y$ o vetor de respostas:
> $$
> y = \begin{bmatrix} 5 \\ 6 \\ 7 \end{bmatrix}
> $$
>
> Inicialmente, vamos escolher um vetor $\beta$ inicial, por exemplo:
> $$
> \beta = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
> $$
>
> **Passo 1: Calcular $X\beta$**
> $$
> X\beta = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \\ 5 \end{bmatrix}
> $$
>
> **Passo 2: Calcular o vetor de res√≠duos $(y - X\beta)$**
> $$
> y - X\beta = \begin{bmatrix} 5 \\ 6 \\ 7 \end{bmatrix} - \begin{bmatrix} 3 \\ 4 \\ 5 \end{bmatrix} = \begin{bmatrix} 2 \\ 2 \\ 2 \end{bmatrix}
> $$
>
> **Passo 3: Calcular $X^T$**
> $$
> X^T = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 4 \end{bmatrix}
> $$
>
> **Passo 4: Calcular o gradiente $-2X^T(y - X\beta)$**
> $$
> -2X^T(y - X\beta) = -2 \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \\ 2 \end{bmatrix} = -2 \begin{bmatrix} 6 \\ 18 \end{bmatrix} = \begin{bmatrix} -12 \\ -36 \end{bmatrix}
> $$
>
> O gradiente resultante √© $\begin{bmatrix} -12 \\ -36 \end{bmatrix}$. Este vetor indica a dire√ß√£o de maior aumento da fun√ß√£o RSS para o valor atual de $\beta$. Um algoritmo de otimiza√ß√£o usaria este gradiente para ajustar $\beta$ em dire√ß√£o oposta, minimizando o RSS.

### Interpreta√ß√£o Geom√©trica do Gradiente
```mermaid
flowchart LR
    A["Gradient -2X·µÄ(y - XŒ≤)"] --> B{"Direction of steepest increase of RSS"};
    B --> C{"Points opposite to the direction to minimize RSS"};
    C --> D{"Zero gradient means residuals (y-XŒ≤) orthogonal to column space of X"};
    D --> E{"XŒ≤ÃÇ is the orthogonal projection of y onto the subspace spanned by the columns of X"};
     style A fill:#f9f,stroke:#333,stroke-width:2px
```

O gradiente $\frac{\partial RSS(\beta)}{\partial \beta} = -2 X^T (y - X\beta)$ tem uma interpreta√ß√£o geom√©trica importante no espa√ßo dos par√¢metros:
1. **Dire√ß√£o de M√°ximo Crescimento:** O gradiente -2X·µÄ(y - XŒ≤) aponta para a dire√ß√£o no espa√ßo dos par√¢metros onde o RSS cresce mais rapidamente.
2. **Perpendicularidade:** A condi√ß√£o para que a solu√ß√£o de m√≠nimos quadrados seja encontrada √© que o gradiente seja igual a zero, ou seja $X^T(y-X\beta)=0$. Essa condi√ß√£o corresponde a que o vetor de res√≠duos $(y-X\beta)$ seja ortogonal ao espa√ßo gerado pelas colunas da matriz $X$, como j√° visto em outros cap√≠tulos.
3. **Proje√ß√£o Ortogonal:** Geometricamente, o vetor $X\hat{\beta}$ √© a proje√ß√£o ortogonal do vetor de resposta $y$ no subespa√ßo gerado pelas colunas da matriz de design $X$. A derivada da fun√ß√£o RSS corresponde √† dire√ß√£o de m√°xima inclina√ß√£o no espa√ßo dos par√¢metros, e o ponto onde essa derivada √© zero corresponde √† proje√ß√£o ortogonal do vetor de respostas no espa√ßo gerado pelos preditores.
4. **Busca da solu√ß√£o:** Algoritmos de otimiza√ß√£o usam a dire√ß√£o do gradiente para buscar o m√≠nimo da fun√ß√£o RSS, partindo de uma estimativa inicial e fazendo ajustes para chegar √† solu√ß√£o √≥tima.
5. **Superf√≠cie RSS:**  A derivada -2X·µÄ(y - XŒ≤) pode ser vista como a inclina√ß√£o da superf√≠cie do RSS em rela√ß√£o ao vetor de par√¢metros Œ≤. Ao fazer a derivada igual a zero, encontramos o ponto no espa√ßo dos par√¢metros onde a superf√≠cie da RSS tem uma inclina√ß√£o zero, o que corresponde a um ponto de m√≠nimo (ou um ponto de sela).

**Lemma 18:**  Convexidade da Fun√ß√£o RSS

A fun√ß√£o RSS, dada por $RSS(\beta) = ||y - X\beta||^2$, √© convexa em rela√ß√£o a $\beta$.  A convexidade da fun√ß√£o de custo garante que a solu√ß√£o por m√≠nimos quadrados, quando existir, seja um m√≠nimo global.

**Prova do Lemma 18:**

Para mostrar que $RSS(\beta)$ √© convexa, mostramos que a matriz Hessiana, que cont√©m as derivadas de segunda ordem, √© semi-definida positiva. A Hessiana √© definida como:

$$
H = \frac{\partial^2 RSS(\beta)}{\partial \beta \partial \beta^T} = \frac{\partial}{\partial \beta} (-2 X^T(y - X\beta))^T
= 2X^TX
$$

A matriz $X^T X$ √© sempre semi-definida positiva, pois, para qualquer vetor $z$, temos que $z^T (X^T X)z = (Xz)^T Xz = ||Xz||^2 \ge 0$. Como a matriz Hessiana √© semi-definida positiva, a fun√ß√£o $RSS(\beta)$ √© convexa e o m√≠nimo encontrado corresponde a um m√≠nimo global. $\blacksquare$

```mermaid
flowchart LR
    A[RSS(Œ≤) = $||y - X\beta||^2$] --> B{Hessian of RSS = $2X^TX$};
    B --> C{$X^TX$ is always positive semi-definite};
    C --> D{Hessian is positive semi-definite};
    D --> E[RSS(Œ≤) is convex];
    E --> F[Global minimum is guaranteed];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#f9f,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
>
> Continuando com o exemplo anterior, vamos calcular a matriz Hessiana $H = 2X^TX$:
>
> **Passo 1: Calcular $X^T X$**
> $$
> X^T X = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix} = \begin{bmatrix} 3 & 9 \\ 9 & 29 \end{bmatrix}
> $$
>
> **Passo 2: Calcular $2X^TX$**
> $$
> H = 2X^TX = 2 \begin{bmatrix} 3 & 9 \\ 9 & 29 \end{bmatrix} = \begin{bmatrix} 6 & 18 \\ 18 & 58 \end{bmatrix}
> $$
>
> A matriz Hessiana √© $\begin{bmatrix} 6 & 18 \\ 18 & 58 \end{bmatrix}$. Para verificar se √© semi-definida positiva, podemos calcular seus autovalores ou verificar se todos os menores principais s√£o n√£o-negativos. Neste caso, ambos os autovalores s√£o positivos, confirmando que a matriz √© definida positiva e, portanto, a fun√ß√£o RSS √© convexa.

**Corol√°rio 18:** Implica√ß√µes da Convexidade

A convexidade da fun√ß√£o RSS tem implica√ß√µes importantes. Uma fun√ß√£o convexa tem apenas um m√≠nimo global, o que facilita a sua otimiza√ß√£o, e garante que o m√©todo de m√≠nimos quadrados chegue a uma solu√ß√£o √∫nica e est√°vel, se ela existir.

###  Relev√¢ncia da Derivada na Estima√ß√£o de M√≠nimos Quadrados
```mermaid
flowchart LR
    A[Derivative -2X·µÄ(y - XŒ≤)] --> B{Defines the solution: $\hat{\beta} = (X^TX)^{-1}X^Ty$};
    B --> C{Used in optimization algorithms};
    C --> D{Gradient descent uses the gradient to approach optimal solution};
     D --> E{Links likelihood formulation and optimization};
     E --> F{Vector (y-XŒ≤) represents the model error};
      style A fill:#f9f,stroke:#333,stroke-width:2px
```

A derivada da fun√ß√£o RSS, dada por -2X·µÄ(y - XŒ≤), √© fundamental no m√©todo de m√≠nimos quadrados porque:

1.  **Defini√ß√£o da Solu√ß√£o:** Igualar a derivada a zero leva √† solu√ß√£o de m√≠nimos quadrados, $\hat{\beta} = (X^T X)^{-1} X^T y$.
2.  **Algoritmos de Otimiza√ß√£o:** A derivada √© utilizada por algoritmos iterativos de otimiza√ß√£o para buscar o m√≠nimo da fun√ß√£o RSS. M√©todos como *Gradient Descent*, *Stochastic Gradient Descent*, e seus relacionados, se baseiam no gradiente para atualizar os par√¢metros do modelo e aproximar a solu√ß√£o √≥tima.
3.  **Fundamentos da Estat√≠stica:** A derivada da fun√ß√£o RSS √© baseada na formula√ß√£o da fun√ß√£o de *likelihood* no modelo de regress√£o linear, e representa a conex√£o entre a estat√≠stica e a otimiza√ß√£o.
4. **Compreens√£o do Erro:** O vetor $y - X\beta$, que aparece dentro da derivada, representa o erro do modelo, e a sua ortogonalidade com a matriz de design (quando a derivada √© zero) √© um requisito da solu√ß√£o de m√≠nimos quadrados.

> ‚ö†Ô∏è **Nota Importante**: A derivada da RSS em rela√ß√£o a Œ≤, -2X·µÄ(y - XŒ≤), corresponde ao gradiente da fun√ß√£o, indicando a dire√ß√£o do maior aumento do erro.
 
> ‚ùó **Ponto de Aten√ß√£o**:  Igualar essa derivada a zero nos permite obter a solu√ß√£o de m√≠nimos quadrados.

> ‚úîÔ∏è **Destaque**: A derivada -2X·µÄ(y - XŒ≤) √© fundamental tanto na defini√ß√£o da solu√ß√£o de m√≠nimos quadrados, como na constru√ß√£o de algoritmos de otimiza√ß√£o.

### Conclus√£o

A deriva√ß√£o da Soma dos Quadrados dos Res√≠duos, RSS, em rela√ß√£o aos par√¢metros $\beta$ √© um passo essencial para encontrar a solu√ß√£o de m√≠nimos quadrados. A express√£o resultante, -2X·µÄ(y - XŒ≤), √© o gradiente da fun√ß√£o RSS e representa a dire√ß√£o do maior aumento do erro. A derivada permite derivar a solu√ß√£o anal√≠tica, bem como desenvolver algoritmos de otimiza√ß√£o e permite entender a solu√ß√£o em termos de ortogonalidade. O resultado da deriva√ß√£o enfatiza a combina√ß√£o de conceitos estat√≠sticos, alg√©bricos e geom√©tricos utilizados na regress√£o linear.

### Refer√™ncias

[^10]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Methods for Regression)*
[^11]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^12]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Methods for Regression)*
[^13]: "We minimize RSS(3) = ||y ‚Äì XŒ≤||2 by choosing ·∫û so that the residual vector y - ≈∑ is orthogonal to this subspace" *(Trecho de Linear Methods for Regression)*
