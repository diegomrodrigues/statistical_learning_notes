## Formula√ß√£o Matricial da Soma dos Quadrados dos Res√≠duos: $RSS(\beta) = (y - X\beta)^T(y - X\beta)$

```mermaid
graph LR
    y("y (Vetor de Respostas)")
    X("X (Matriz de Design)")
    beta("Œ≤ (Vetor de Par√¢metros)")
    Xbeta("XŒ≤ (Predi√ß√µes)")
    y_minus_Xbeta("y - XŒ≤ (Res√≠duos)")
    y_minus_Xbeta_T("(y - XŒ≤)·µÄ")
    RSS("(y - XŒ≤)·µÄ(y - XŒ≤) (RSS)")
    y --> y_minus_Xbeta
    X --> Xbeta
    beta --> Xbeta
    Xbeta --> y_minus_Xbeta
    y_minus_Xbeta --> y_minus_Xbeta_T
    y_minus_Xbeta_T --> RSS
    y_minus_Xbeta --> RSS
    style y fill:#f9f,stroke:#333,stroke-width:2px
    style X fill:#ccf,stroke:#333,stroke-width:2px
    style beta fill:#ccf,stroke:#333,stroke-width:2px
    style Xbeta fill:#ccf,stroke:#333,stroke-width:2px
    style y_minus_Xbeta fill:#fcc,stroke:#333,stroke-width:2px
    style y_minus_Xbeta_T fill:#fcc,stroke:#333,stroke-width:2px
    style RSS fill:#cfc,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

A formula√ß√£o matem√°tica da **Soma dos Quadrados dos Res√≠duos (RSS)** em nota√ß√£o matricial, dada por $RSS(\beta) = (y - X\beta)^T (y - X\beta)$, √© uma representa√ß√£o concisa e poderosa que encapsula a ess√™ncia do m√©todo dos m√≠nimos quadrados na regress√£o linear. Essa formula√ß√£o, al√©m de ser elegante e compacta, facilita a an√°lise matem√°tica e a implementa√ß√£o computacional de algoritmos relacionados √† regress√£o linear. Neste cap√≠tulo, vamos detalhar cada componente dessa express√£o, analisar sua relev√¢ncia e explorar sua rela√ß√£o com a solu√ß√£o de m√≠nimos quadrados.

### Componentes da Formula√ß√£o Matricial 

### $RSS(\beta) = (y - X\beta)^T(y - X\beta)$

A formula√ß√£o matricial $RSS(\beta) = (y - X\beta)^T (y - X\beta)$ apresenta a fun√ß√£o de custo da regress√£o linear de forma concisa e eficiente. Vamos analisar cada componente em detalhes:

1.  **y:** O vetor $y \in \mathbb{R}^N$ representa o vetor de respostas ou valores observados da vari√°vel dependente, onde cada componente $y_i$ representa a i-√©sima observa√ß√£o [^10].
2.  **X:** A matriz $X \in \mathbb{R}^{N \times (p+1)}$ √© a matriz de design, tamb√©m conhecida como matriz de preditores ou matriz de caracter√≠sticas. Esta matriz tem $N$ linhas, representando as $N$ observa√ß√µes, e $p+1$ colunas, representando o intercepto (primeira coluna de 1) e os $p$ preditores [^11].
3. **Œ≤:** O vetor $\beta \in \mathbb{R}^{p+1}$ representa o vetor de par√¢metros ou coeficientes do modelo linear. Ele inclui o intercepto e os coeficientes associados aos preditores [^11].
4.  **XŒ≤:** O produto matricial $X\beta$ representa o vetor de predi√ß√µes do modelo, onde cada elemento $(X\beta)_i$ corresponde √† predi√ß√£o do modelo para a $i$-√©sima observa√ß√£o. Em ess√™ncia, o produto $X\beta$ transforma o vetor de par√¢metros $\beta$ numa representa√ß√£o do espa√ßo das observa√ß√µes.
5.  **(y - XŒ≤):** A diferen√ßa $(y - X\beta)$ resulta no vetor de res√≠duos, denotado por $r$, onde cada componente $r_i$ corresponde ao res√≠duo da i-√©sima observa√ß√£o, que √© a diferen√ßa entre o valor observado ($y_i$) e o valor predito pelo modelo ($(X\beta)_i$).
6. **(y - XŒ≤)·µÄ:** O termo $(y - X\beta)^T$ representa a transposta do vetor de res√≠duos. A transposi√ß√£o troca linhas por colunas e colunas por linhas, e tem a propriedade de transformar vetores coluna em vetores linha e vice-versa.
7.  **(y - XŒ≤)·µÄ(y - XŒ≤):** Finalmente, o produto $(y - X\beta)^T (y - X\beta)$ √© o produto interno do vetor de res√≠duos consigo mesmo, que resulta na soma dos quadrados de cada componente do vetor de res√≠duos, que √© a soma dos quadrados dos res√≠duos (RSS):
   $$
   (y - X\beta)^T (y - X\beta) = \sum_{i=1}^{N} (y_i - (X\beta)_i)^2 = \sum_{i=1}^N r_i^2
   $$
   onde:
       -  $N$ √© o n√∫mero de observa√ß√µes,
       -  $y_i$ √© a i-√©sima observa√ß√£o da vari√°vel dependente,
       - $(X\beta)_i$ √© o valor predito para a i-√©sima observa√ß√£o, e
        - $r_i$ √© o res√≠duo da i-√©sima observa√ß√£o.

O objetivo do m√©todo dos m√≠nimos quadrados √© encontrar o vetor de par√¢metros $\beta$ que minimize o valor da soma dos quadrados dos res√≠duos, $RSS(\beta)$.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simples com $N=3$ observa√ß√µes e um √∫nico preditor (al√©m do intercepto), ou seja, $p=1$.
>
> Suponha que temos os seguintes dados:
>
> $$y = \begin{bmatrix} 5 \\ 8 \\ 10 \end{bmatrix}, \quad X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}$$
>
> Aqui, a primeira coluna de $X$ √© o intercepto (todos 1s), e a segunda coluna s√£o os valores do preditor.
>
> Vamos assumir um vetor de par√¢metros $\beta$ como:
>
> $$\beta = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$$
>
> Agora, vamos calcular $X\beta$:
>
> $$X\beta = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 1*1 + 2*2 \\ 1*1 + 3*2 \\ 1*1 + 4*2 \end{bmatrix} = \begin{bmatrix} 5 \\ 7 \\ 9 \end{bmatrix}$$
>
> Em seguida, calculamos o vetor de res√≠duos $r = y - X\beta$:
>
> $$r = y - X\beta = \begin{bmatrix} 5 \\ 8 \\ 10 \end{bmatrix} - \begin{bmatrix} 5 \\ 7 \\ 9 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix}$$
>
> Finalmente, calculamos a soma dos quadrados dos res√≠duos $RSS(\beta) = r^T r$:
>
> $$RSS(\beta) = r^T r = \begin{bmatrix} 0 & 1 & 1 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix} = 0^2 + 1^2 + 1^2 = 2$$
>
> Este valor de 2 representa a soma dos quadrados dos res√≠duos para o vetor de par√¢metros $\beta = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$. O objetivo da regress√£o linear √© encontrar o vetor $\beta$ que minimize este valor.

### Interpreta√ß√£o Geom√©trica da Formula√ß√£o Matricial

A formula√ß√£o matricial da RSS tem uma clara interpreta√ß√£o geom√©trica [^12]:

1.  **Vetores no Espa√ßo dos Dados:** O vetor $y$ e o vetor de predi√ß√µes $X\beta$ podem ser vistos como vetores no espa√ßo das observa√ß√µes, $\mathbb{R}^N$. Cada componente desses vetores corresponde a uma observa√ß√£o.
2.  **Dist√¢ncia Euclidiana:** O termo $||y - X\beta||$ representa a dist√¢ncia euclidiana entre o vetor de respostas $y$ e o vetor de predi√ß√µes $X\beta$. Minimizar $||y - X\beta||^2$ √© equivalente a minimizar o quadrado da dist√¢ncia entre esses dois vetores.
3.  **Proje√ß√£o Ortogonal:** A solu√ß√£o de m√≠nimos quadrados $\hat{\beta}$ faz com que o vetor de predi√ß√µes $X\hat{\beta}$ seja a proje√ß√£o ortogonal do vetor de resposta $y$ no subespa√ßo gerado pelas colunas da matriz $X$. A proje√ß√£o ortogonal garante que a dist√¢ncia entre os vetores seja m√≠nima.
4.  **Ortogonalidade dos Res√≠duos:** O vetor de res√≠duos, $y - X\hat{\beta}$, √© ortogonal ao espa√ßo gerado pelas colunas de $X$. Isso significa que o vetor de res√≠duos tem um √¢ngulo de 90 graus com todos os vetores formados por combina√ß√µes lineares das colunas de $X$.

A interpreta√ß√£o geom√©trica facilita a compreens√£o que a solu√ß√£o de m√≠nimos quadrados busca o modelo linear que melhor se ajusta aos dados no sentido da dist√¢ncia euclidiana. O modelo minimiza a dist√¢ncia entre o valor observado e a sua proje√ß√£o no espa√ßo gerado pelos preditores.

> üí° **Exemplo Num√©rico:**
>
> Vamos usar os dados do exemplo anterior para ilustrar a interpreta√ß√£o geom√©trica.
>
> Temos $y = \begin{bmatrix} 5 \\ 8 \\ 10 \end{bmatrix}$ e $X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}$.
>
> No exemplo anterior, usamos $\beta = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$ e obtivemos $X\beta = \begin{bmatrix} 5 \\ 7 \\ 9 \end{bmatrix}$ e $r = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix}$.
>
>  Embora n√£o seja a solu√ß√£o √≥tima, podemos visualizar que $X\beta$ √© um vetor no espa√ßo gerado pelas colunas de $X$. Geometricamente, o objetivo √© encontrar o vetor $X\hat{\beta}$ que esteja o mais pr√≥ximo poss√≠vel de $y$. O res√≠duo $r$ representa o vetor que conecta $X\beta$ a $y$. Para a solu√ß√£o de m√≠nimos quadrados, este vetor seria ortogonal ao espa√ßo gerado pelas colunas de X.
>
> ```mermaid
> graph LR
>     y(y) --> r(r);
>     Xb(XŒ≤) --> r;
>     X(Subespa√ßo de X) --> Xb;
>     style y fill:#f9f,stroke:#333,stroke-width:2px
>     style Xb fill:#ccf,stroke:#333,stroke-width:2px
>     style X fill:#cfc,stroke:#333,stroke-width:2px
>     style r fill:#fcc,stroke:#333,stroke-width:2px
>     linkStyle 0,1,2 stroke:#333,stroke-width:2px
> ```
>
> Neste diagrama, $y$ √© o vetor de observa√ß√µes, $X\beta$ √© o vetor de predi√ß√µes, e $r$ √© o vetor de res√≠duos. A seta entre $X$ e $X\beta$ representa o espa√ßo gerado pelas colunas de $X$. A solu√ß√£o de m√≠nimos quadrados encontra o vetor $X\hat{\beta}$ neste espa√ßo que minimiza a dist√¢ncia a $y$, tornando o res√≠duo $r$ ortogonal ao espa√ßo $X$.

**Lemma 17:**  Propriedades da Transposi√ß√£o

A transposi√ß√£o de vetores e matrizes tem propriedades que s√£o fundamentais na manipula√ß√£o da fun√ß√£o RSS:
    1.  $(A^T)^T = A$
    2.  $(AB)^T = B^T A^T$
    3. $(a^T b) = (b^T a)$, onde $a$ e $b$ s√£o vetores.
Estas propriedades permitem simplificar express√µes, manipular matrizes e vetores, e derivar resultados como a solu√ß√£o de m√≠nimos quadrados.

**Corol√°rio 17:** Solu√ß√£o de M√≠nimos Quadrados por Deriva√ß√£o Matricial

A solu√ß√£o de m√≠nimos quadrados, $\hat{\beta} = (X^T X)^{-1} X^T y$, pode ser encontrada derivando a fun√ß√£o RSS em termos matriciais [^11]. O processo de deriva√ß√£o utiliza as propriedades da transposi√ß√£o e as opera√ß√µes de matrizes. Este resultado permite obter uma solu√ß√£o de forma anal√≠tica e eficiente.
A transposta de um produto de matrizes, $(AB)^T = B^T A^T$, √© usada na deriva√ß√£o para expandir a fun√ß√£o $RSS(\beta)$. A transposta tamb√©m √© utilizada para garantir que o resultado do produto de matrizes seja um escalar, ou seja, para passar do produto interno de dois vetores √† sua norma quadrada.

> üí° **Exemplo Num√©rico:**
>
> Vamos demonstrar a deriva√ß√£o da solu√ß√£o de m√≠nimos quadrados usando as propriedades da transposi√ß√£o.
>
> Partimos de $RSS(\beta) = (y - X\beta)^T (y - X\beta)$.
>
> **Passo 1:** Expandir a express√£o usando a propriedade $(AB)^T = B^T A^T$:
>
> $$RSS(\beta) = (y^T - (X\beta)^T)(y - X\beta) = (y^T - \beta^T X^T)(y - X\beta)$$
>
> **Passo 2:** Distribuir os termos:
>
> $$RSS(\beta) = y^T y - y^T X\beta - \beta^T X^T y + \beta^T X^T X \beta$$
>
> **Passo 3:**  Usando a propriedade $a^T b = b^T a$, podemos reescrever o termo do meio:
>
> $$RSS(\beta) = y^T y - 2\beta^T X^T y + \beta^T X^T X \beta$$
>
> **Passo 4:** Para encontrar o m√≠nimo, derivamos $RSS(\beta)$ em rela√ß√£o a $\beta$ e igualamos a zero:
>
> $$\frac{\partial RSS(\beta)}{\partial \beta} = -2X^T y + 2X^T X \beta = 0$$
>
> **Passo 5:** Resolver para $\beta$:
>
> $$2X^T X \beta = 2X^T y$$
>
> $$X^T X \beta = X^T y$$
>
> $$\beta = (X^T X)^{-1} X^T y$$
>
> Portanto, a solu√ß√£o de m√≠nimos quadrados √© $\hat{\beta} = (X^T X)^{-1} X^T y$.
>
> Este exemplo mostra como as propriedades da transposi√ß√£o s√£o utilizadas para derivar a solu√ß√£o de m√≠nimos quadrados de forma concisa e eficiente.

### Implica√ß√µes da Formula√ß√£o Matricial

A formula√ß√£o $RSS(\beta) = (y - X\beta)^T (y - X\beta)$ tem implica√ß√µes importantes na teoria e pr√°tica da regress√£o linear:

1.  **Deriva√ß√£o da Solu√ß√£o:** A formula√ß√£o matricial simplifica a deriva√ß√£o da solu√ß√£o de m√≠nimos quadrados. Ao derivar a fun√ß√£o RSS com rela√ß√£o a $\beta$, chegamos √† solu√ß√£o $\hat{\beta} = (X^T X)^{-1} X^T y$ de forma concisa e elegante.
2.  **Efici√™ncia Computacional:** A representa√ß√£o matricial √© facilmente implement√°vel em computadores, usando bibliotecas de √°lgebra linear eficientes. O uso de bibliotecas de √°lgebra linear permite computar as solu√ß√µes de m√≠nimos quadrados com alta efici√™ncia e precis√£o.
3.  **Generaliza√ß√µes:** A representa√ß√£o matricial da RSS se generaliza facilmente para modelos com v√°rias vari√°veis dependentes, problemas de m√≠nimos quadrados ponderados, e modelos com estruturas de erro mais complexas, permitindo o uso de m√©todos mais avan√ßados de modelagem.
4.  **Conex√£o com Decomposi√ß√£o de Matrizes:** A formula√ß√£o matricial da RSS tem uma conex√£o com m√©todos de decomposi√ß√£o de matrizes, como a fatoriza√ß√£o QR e a decomposi√ß√£o em valores singulares (SVD). Estas decomposi√ß√µes podem ser usadas para calcular a solu√ß√£o de m√≠nimos quadrados, especialmente em casos onde a matriz $X^T X$ √© mal condicionada.

> üí° **Exemplo Num√©rico:**
>
> Vamos usar os dados do exemplo inicial e calcular a solu√ß√£o de m√≠nimos quadrados $\hat{\beta}$ utilizando a f√≥rmula $\hat{\beta} = (X^T X)^{-1} X^T y$.
>
> Temos:
>
> $$y = \begin{bmatrix} 5 \\ 8 \\ 10 \end{bmatrix}, \quad X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}$$
>
> **Passo 1:** Calcular $X^T$:
>
> $$X^T = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 4 \end{bmatrix}$$
>
> **Passo 2:** Calcular $X^T X$:
>
> $$X^T X = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix} = \begin{bmatrix} 3 & 9 \\ 9 & 29 \end{bmatrix}$$
>
> **Passo 3:** Calcular $(X^T X)^{-1}$:
>
> $$(X^T X)^{-1} = \frac{1}{(3*29 - 9*9)} \begin{bmatrix} 29 & -9 \\ -9 & 3 \end{bmatrix} = \frac{1}{6} \begin{bmatrix} 29 & -9 \\ -9 & 3 \end{bmatrix} = \begin{bmatrix} 29/6 & -3/2 \\ -3/2 & 1/2 \end{bmatrix}$$
>
> **Passo 4:** Calcular $X^T y$:
>
> $$X^T y = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} 5 \\ 8 \\ 10 \end{bmatrix} = \begin{bmatrix} 23 \\ 74 \end{bmatrix}$$
>
> **Passo 5:** Calcular $\hat{\beta} = (X^T X)^{-1} X^T y$:
>
> $$\hat{\beta} = \begin{bmatrix} 29/6 & -3/2 \\ -3/2 & 1/2 \end{bmatrix} \begin{bmatrix} 23 \\ 74 \end{bmatrix} = \begin{bmatrix} (29/6)*23 + (-3/2)*74 \\ (-3/2)*23 + (1/2)*74 \end{bmatrix} = \begin{bmatrix} -0.333 \\ 6.5 \end{bmatrix}$$
>
> Portanto, a solu√ß√£o de m√≠nimos quadrados √© $\hat{\beta} \approx \begin{bmatrix} -0.333 \\ 6.5 \end{bmatrix}$.
>
> Este exemplo demonstra como a formula√ß√£o matricial permite calcular a solu√ß√£o de m√≠nimos quadrados de forma direta, utilizando opera√ß√µes de √°lgebra linear. A solu√ß√£o obtida minimiza a soma dos quadrados dos res√≠duos para os dados fornecidos.

> ‚ö†Ô∏è **Nota Importante**: A formula√ß√£o $RSS(\beta) = (y - X\beta)^T(y - X\beta)$ expressa a soma dos quadrados dos res√≠duos em nota√ß√£o matricial, onde $y$ √© o vetor de respostas, $X$ a matriz de design e $\beta$ o vetor de par√¢metros. **Refer√™ncia ao contexto [^11]**.

> ‚ùó **Ponto de Aten√ß√£o**: A minimiza√ß√£o desta fun√ß√£o de custo em rela√ß√£o a $\beta$ leva √† solu√ß√£o de m√≠nimos quadrados, que corresponde √† proje√ß√£o ortogonal de y no subespa√ßo gerado pelas colunas de X. **Conforme indicado no contexto [^12]**.

> ‚úîÔ∏è **Destaque**: A nota√ß√£o matricial simplifica a deriva√ß√£o matem√°tica da solu√ß√£o, sua implementa√ß√£o e permite uma conex√£o clara com o processo geom√©trico. **Baseado no contexto [^11]**.

### Conclus√£o

A formula√ß√£o matricial da soma dos quadrados dos res√≠duos, $RSS(\beta) = (y - X\beta)^T(y - X\beta)$, √© uma representa√ß√£o fundamental na regress√£o linear, com importantes implica√ß√µes te√≥ricas e pr√°ticas. A sua utiliza√ß√£o na obten√ß√£o da solu√ß√£o de m√≠nimos quadrados, bem como sua import√¢ncia para m√©todos de regulariza√ß√£o, a tornam um conceito essencial no aprendizado de m√°quina e an√°lise estat√≠stica. Atrav√©s da compreens√£o desta formula√ß√£o e dos seus componentes, podemos entender o funcionamento interno dos m√©todos de regress√£o linear e a sua conex√£o com a √°lgebra linear.

### Refer√™ncias

[^10]: "The most popular estimation method is least squares, in which we pick the coefficients $\beta = (\beta_0, \beta_1, \ldots, \beta_p)^T$ to minimize the residual sum of squares" *(Trecho de Linear Methods for Regression)*
[^11]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^12]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Methods for Regression)*
[^46]: "The predicted values at an input vector $x_o$ are given by $f(x_o) = (1 : x_o)^T\beta$;" *(Trecho de Linear Methods for Regression)*
