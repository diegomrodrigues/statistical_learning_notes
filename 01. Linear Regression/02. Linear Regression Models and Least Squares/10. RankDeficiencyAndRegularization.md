## DeficiÃªncia de Posto em RegressÃ£o Linear: NÃ£o Unicidade de Î², RedundÃ¢ncias e RegularizaÃ§Ã£o

```mermaid
graph LR
    A["Matriz de Design X"] -->|Colunas Linearmente Dependentes| B(DeficiÃªncia de Posto);
    B --> C["NÃ£o Invertibilidade de X^T*X"];
    C --> D["NÃ£o Unicidade da SoluÃ§Ã£o Î²"];
    D --> E["Infinitas SoluÃ§Ãµes Minimizando RSS"];
    E --> F["InterpretaÃ§Ã£o Complexa de Î²"];
    F --> G["Necessidade de RegularizaÃ§Ã£o ou ReduÃ§Ã£o de RedundÃ¢ncia"];
```

### IntroduÃ§Ã£o

Em modelos de regressÃ£o linear, a **deficiÃªncia de posto** da matriz de design $X$ ocorre quando as colunas de $X$ nÃ£o sÃ£o linearmente independentes, o que significa que algumas colunas podem ser escritas como combinaÃ§Ãµes lineares de outras [^46]. Esta situaÃ§Ã£o leva Ã  **nÃ£o unicidade da soluÃ§Ã£o** para os parÃ¢metros $\beta$ na regressÃ£o linear, e representa um desafio tanto do ponto de vista teÃ³rico como prÃ¡tico. Este capÃ­tulo explorarÃ¡ as consequÃªncias da deficiÃªncia de posto, as estratÃ©gias para lidar com redundÃ¢ncias, as particularidades de cenÃ¡rios com mais preditores que observaÃ§Ãµes, e o papel da regularizaÃ§Ã£o na resoluÃ§Ã£o desses problemas.

### DeficiÃªncia de Posto e NÃ£o Unicidade da SoluÃ§Ã£o

A deficiÃªncia de posto em um modelo de regressÃ£o linear ocorre quando as colunas da matriz de design $X$ nÃ£o sÃ£o linearmente independentes, o que significa que o posto da matriz, $rank(X)$, Ã© menor do que o nÃºmero de preditores ($p+1$). Matematicamente, a deficiÃªncia de posto implica que:

$$
rank(X) < p + 1
$$
Esta condiÃ§Ã£o tem implicaÃ§Ãµes importantes na soluÃ§Ã£o do problema de mÃ­nimos quadrados:

1. **NÃ£o Invertibilidade:** Se $rank(X) < p+1$  entÃ£o a matriz $X^T X$ Ã© singular (nÃ£o invertÃ­vel), o que significa que seu determinante Ã© zero [^46]. Isso implica que nÃ£o existe uma Ãºnica soluÃ§Ã£o para a equaÃ§Ã£o $\hat{\beta} = (X^T X)^{-1} X^T y$, ou seja, a soluÃ§Ã£o de mÃ­nimos quadrados nÃ£o Ã© Ãºnica.
2.  **Infinitas SoluÃ§Ãµes:** Quando $X^T X$ nÃ£o Ã© invertÃ­vel, existem infinitas soluÃ§Ãµes que minimizam a soma dos quadrados dos resÃ­duos (RSS). Existem diferentes soluÃ§Ãµes $\beta$ para $X\beta$ que atingem o mesmo valor da RSS.
3. **InterpretaÃ§Ã£o da SoluÃ§Ã£o:** Em problemas de rank deficiente, a interpretaÃ§Ã£o dos parÃ¢metros do modelo torna-se mais complexa, dado que mÃºltiplas combinaÃ§Ãµes lineares de coeficientes podem gerar o mesmo ajuste aos dados.

Em termos geomÃ©tricos, a deficiÃªncia de posto implica que o espaÃ§o gerado pelas colunas da matriz $X$, $\text{ran}(X)$, tem uma dimensÃ£o menor do que o nÃºmero de colunas. A projeÃ§Ã£o do vetor de respostas $y$ neste espaÃ§o nÃ£o Ã© mais um ponto Ãºnico e bem definido, e sim um subespaÃ§o no espaÃ§o gerado pelas colunas de $X$.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um modelo de regressÃ£o com trÃªs preditores, onde $x_3 = 2x_1 + x_2$. Suponha que temos os seguintes dados:
>
> ```python
> import numpy as np
>
> X = np.array([[1, 2, 4],
>               [2, 4, 8],
>               [3, 6, 12],
>               [4, 8, 16]])
> y = np.array([5, 10, 15, 20])
> ```
>
> A matriz $X$ tem trÃªs colunas, mas a terceira coluna Ã© uma combinaÃ§Ã£o linear das duas primeiras, $x_3 = 2x_1 + 0x_2$. Portanto, $rank(X) < 3$. A matriz $X^TX$ Ã©:
>
> ```python
> XtX = X.T @ X
> print(XtX)
> # Output:
> # [[ 30  60 120]
> #  [ 60 120 240]
> #  [120 240 480]]
> ```
>
>  O determinante de $X^TX$ Ã© zero, indicando que nÃ£o Ã© invertÃ­vel.
>  ```python
> print(np.linalg.det(XtX))
>  # Output:
>  # 0.0
>  ```
>
>  Existem infinitas soluÃ§Ãµes para $\beta$ que minimizam a soma dos quadrados dos resÃ­duos. Por exemplo, $\beta_1 = [1, 1, 0]$ e $\beta_2 = [0, 0, 0.5]$ sÃ£o duas soluÃ§Ãµes possÃ­veis que levam ao mesmo valor de RSS.

### Lidando com RedundÃ¢ncias

A deficiÃªncia de posto Ã© frequentemente resultado de redundÃ¢ncias nos preditores, o que pode ocorrer devido a diferentes razÃµes:

1.  **Colinearidade Perfeita:** Se duas ou mais variÃ¡veis sÃ£o perfeitamente correlacionadas, uma pode ser expressa como uma combinaÃ§Ã£o linear da outra (ou outras), criando uma situaÃ§Ã£o de posto deficiente.
2.  **CodificaÃ§Ã£o Redundante:** A codificaÃ§Ã£o *dummy* de variÃ¡veis qualitativas pode gerar redundÃ¢ncia se todas as categorias sÃ£o utilizadas no modelo, em vez de usar uma categoria como referÃªncia, criando dependÃªncia linear entre variÃ¡veis *dummy*.
3.  **InteraÃ§Ãµes Lineares:**  A inclusÃ£o de interaÃ§Ãµes de variÃ¡veis pode gerar dependÃªncias lineares, dado que a inclusÃ£o dos termos de interaÃ§Ã£o podem ser redundantes em funÃ§Ã£o dos preditores individuais.

Para lidar com essas redundÃ¢ncias e obter uma soluÃ§Ã£o estÃ¡vel, existem vÃ¡rias abordagens:

```mermaid
graph LR
    A["RedundÃ¢ncias nos Preditores"] --> B{"RecodificaÃ§Ã£o de VariÃ¡veis Qualitativas"};
    A --> C{"RemoÃ§Ã£o de Colunas Redundantes"};
    A --> D{"CombinaÃ§Ã£o de VariÃ¡veis"};
    B --> E("Matriz X NÃ£o Singular");
    C --> E;
    D --> E;
    E --> F("Unicidade da SoluÃ§Ã£o");
```

1.  **RecodificaÃ§Ã£o de VariÃ¡veis Qualitativas:** Em situaÃ§Ãµes de codificaÃ§Ã£o *dummy* com variÃ¡veis qualitativas, escolher uma categoria de referÃªncia remove a redundÃ¢ncia, e torna a matriz de design nÃ£o singular [^46]. Em outras palavras, em vez de usar $k$ variÃ¡veis *dummy* para modelar $k$ categorias, usamos $k-1$ variÃ¡veis, usando a categoria omitida como referÃªncia.
2.  **RemoÃ§Ã£o de Colunas Redundantes:** Em casos de colinearidade perfeita, remover as colunas linearmente dependentes da matriz $X$ resolve a questÃ£o da nÃ£o-invertibilidade. A seleÃ§Ã£o de qual coluna remover Ã©, em princÃ­pio, arbitrÃ¡ria, mas algumas abordagens podem favorecer a remoÃ§Ã£o de variÃ¡veis que tem alguma dependÃªncia com outras.
3.  **CombinaÃ§Ã£o de VariÃ¡veis:** Em casos de colinearidade ou interaÃ§Ãµes lineares, algumas variÃ¡veis podem ser combinadas ou transformadas para reduzir a redundÃ¢ncia. Por exemplo, quando $x_3 = x_1 + x_2$, o modelo pode ser simplificado para $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2$.

Ao remover as redundÃ¢ncias da matriz X, Ã© possÃ­vel garantir a unicidade da soluÃ§Ã£o de mÃ­nimos quadrados, e tambÃ©m facilitar a interpretaÃ§Ã£o dos parÃ¢metros do modelo.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um modelo com uma variÃ¡vel categÃ³rica com trÃªs nÃ­veis (A, B, C). Se codificarmos essa variÃ¡vel usando trÃªs variÃ¡veis *dummy* ($x_1$, $x_2$, $x_3$), onde $x_1 = 1$ se a categoria for A e 0 caso contrÃ¡rio, $x_2 = 1$ se a categoria for B e 0 caso contrÃ¡rio, e $x_3 = 1$ se a categoria for C e 0 caso contrÃ¡rio, teremos redundÃ¢ncia. Para cada observaÃ§Ã£o, $x_1 + x_2 + x_3 = 1$.
>
> Para evitar essa redundÃ¢ncia, podemos escolher uma categoria de referÃªncia, por exemplo C, e usar apenas duas variÃ¡veis *dummy* ($x_1$, $x_2$). Nesse caso, $x_1 = 1$ se a categoria for A e 0 caso contrÃ¡rio, e $x_2 = 1$ se a categoria for B e 0 caso contrÃ¡rio. Quando $x_1 = 0$ e $x_2 = 0$, a categoria Ã© C.
>
> ```python
> import numpy as np
> import pandas as pd
>
> data = {'categoria': ['A', 'B', 'C', 'A', 'B', 'C']}
> df = pd.DataFrame(data)
>
> # CodificaÃ§Ã£o redundante
> df_dummies_redundant = pd.get_dummies(df['categoria'], prefix='cat')
> print("Matriz com codificaÃ§Ã£o redundante:\n", df_dummies_redundant)
>
> # CodificaÃ§Ã£o sem redundÃ¢ncia
> df_dummies_no_redundancy = pd.get_dummies(df['categoria'], prefix='cat', drop_first=True)
> print("\nMatriz sem codificaÃ§Ã£o redundante:\n", df_dummies_no_redundancy)
> ```
>
>  A matriz com codificaÃ§Ã£o redundante Ã© de posto deficiente, enquanto a matriz sem redundÃ¢ncia nÃ£o Ã©.

### SituaÃ§Ãµes com p > N: Mais Preditores do que ObservaÃ§Ãµes

Um caso particular de deficiÃªncia de posto Ã© quando o nÃºmero de preditores $p$ Ã© maior que o nÃºmero de observaÃ§Ãµes $N$. Nestes casos, a matriz de design $X$ tem mais colunas do que linhas, e portanto, necessariamente, o posto de $X$ Ã© menor que o nÃºmero de colunas, resultando em uma matriz $X^TX$ singular e com infinitas soluÃ§Ãµes. Em tais cenÃ¡rios:

```mermaid
graph LR
    A["p > N (Mais Preditores que ObservaÃ§Ãµes)"] --> B["Matriz X com Mais Colunas que Linhas"];
    B --> C["rank(X) < NÃºmero de Colunas"];
    C --> D["X^T*X Singular e NÃ£o InvertÃ­vel"];
    D --> E["Overfitting"];
    E --> F["Necessidade de RegularizaÃ§Ã£o"];
```

1.  **Overfitting:** Modelos de mÃ­nimos quadrados tendem a sofrer de *overfitting*, aprendendo os ruÃ­dos e variaÃ§Ãµes dos dados de treinamento em vez de padrÃµes relevantes. Modelos com muitos preditores e poucas observaÃ§Ãµes se ajustam perfeitamente aos dados de treinamento, mas nÃ£o generalizam para dados nÃ£o vistos.
2. **Invertibilidade:** A matriz $X^TX$ nÃ£o Ã© invertÃ­vel, e portanto a soluÃ§Ã£o por mÃ­nimos quadrados nÃ£o Ã© Ãºnica. A abordagem atravÃ©s de fatoraÃ§Ã£o QR com pivotaÃ§Ã£o de colunas pode ser utilizada para gerar uma soluÃ§Ã£o, mas esta soluÃ§Ã£o nÃ£o corresponde Ã  soluÃ§Ã£o de mÃ­nima norma.
3. **Necessidade de RegularizaÃ§Ã£o:** Para lidar com o problema do overfitting e da nÃ£o-unicidade, Ã© fundamental usar tÃ©cnicas de regularizaÃ§Ã£o, como Lasso e Ridge, que controlam a magnitude dos coeficientes, diminuem a variÃ¢ncia e induzem *sparsity*, facilitando a escolha da soluÃ§Ã£o mais apropriada.

Neste cenÃ¡rio, modelos esparsos, com poucos preditores nÃ£o nulos, costumam ser mais apropriados.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos 10 observaÃ§Ãµes ($N=10$) e 20 preditores ($p=20$). A matriz $X$ terÃ¡ dimensÃ£o $10 \times 20$. Neste caso, o posto mÃ¡ximo de $X$ Ã© 10 (o nÃºmero de linhas), que Ã© menor que o nÃºmero de colunas (20). Portanto, a matriz $X^TX$ (de dimensÃ£o $20 \times 20$) serÃ¡ singular e nÃ£o invertÃ­vel.
>
> ```python
> import numpy as np
>
> N = 10 # NÃºmero de observaÃ§Ãµes
> p = 20 # NÃºmero de preditores
>
> X = np.random.rand(N, p)
> XtX = X.T @ X
> print(f"DimensÃ£o de X: {X.shape}")
> print(f"DimensÃ£o de X^TX: {XtX.shape}")
> print(f"Determinante de X^TX: {np.linalg.det(XtX)}") # O determinante serÃ¡ 0 ou muito prÃ³ximo de zero
> ```
>
>  Nesse cenÃ¡rio, a regressÃ£o linear tradicional nÃ£o terÃ¡ uma soluÃ§Ã£o Ãºnica, e a regularizaÃ§Ã£o Ã© essencial.

### RegularizaÃ§Ã£o como SoluÃ§Ã£o para a DeficiÃªncia de Posto

A regularizaÃ§Ã£o, e em particular as penalidades L1 (Lasso) e L2 (Ridge), desempenham um papel fundamental na soluÃ§Ã£o de problemas de regressÃ£o linear com deficiÃªncia de posto [^46]. A regularizaÃ§Ã£o adiciona um termo de penalidade Ã  funÃ§Ã£o de custo, forÃ§ando os coeficientes a seguirem certos padrÃµes, que geralmente envolvem sua diminuiÃ§Ã£o de magnitude, e sua esparsidade.

```mermaid
graph LR
    A["DeficiÃªncia de Posto"] --> B{"RegularizaÃ§Ã£o (L1/L2)"};
    B --> C{"Penalidade Adicionada Ã  FunÃ§Ã£o de Custo"};
    C --> D{"ReduÃ§Ã£o da Magnitude dos Coeficientes"};
    D --> E{"Sparsity (L1)"};
    E --> F["Estabilidade do Modelo"];
     D --> F;
    F --> G{"SoluÃ§Ã£o Ãšnica"};
```

1.  **RegularizaÃ§Ã£o L1 (Lasso):** A penalidade L1, dada por $||\beta||_1$, promove a *sparsity*, forÃ§ando alguns coeficientes a serem exatamente zero, o que diminui o nÃºmero de parÃ¢metros nÃ£o nulos e tambÃ©m induz a seleÃ§Ã£o de variÃ¡veis [^44]. Esta propriedade Ã© crucial em cenÃ¡rios onde o nÃºmero de preditores Ã© maior que o nÃºmero de observaÃ§Ãµes.
2.  **RegularizaÃ§Ã£o L2 (Ridge):** A penalidade L2, dada por $||\beta||^2_2$, reduz a magnitude dos coeficientes, o que por sua vez estabiliza o modelo, evitando que os parÃ¢metros assumam valores muito grandes e tornem-se instÃ¡veis. A Ridge diminui o impacto da multicolinearidade, mesmo que todos os preditores continuem no modelo final.
3.  **Elastic Net:** O Elastic Net combina as duas penalidades, promovendo esparsidade e tambÃ©m estabilidade. O uso da Elastic Net permite a escolha entre diferentes graus de *sparsity* e *shrinkage*, dependendo do problema em questÃ£o.

A regularizaÃ§Ã£o Ã© fundamental na soluÃ§Ã£o de problemas com deficiÃªncia de posto, jÃ¡ que a penalidade introduz um viÃ©s que leva a uma soluÃ§Ã£o estÃ¡vel, e com um nÃºmero limitado de parÃ¢metros. A escolha de qual mÃ©todo Ã© mais apropriado depende do problema em mÃ£os.
O LARS, e outros algoritmos de caminho, permitem explorar o caminho das soluÃ§Ãµes do Lasso (e tambÃ©m do Elastic Net), permitindo uma visÃ£o do impacto do parÃ¢metro de regularizaÃ§Ã£o na esparsidade do modelo.
Em geral, modelos com muitos parÃ¢metros, sÃ£o mais flexÃ­veis e se ajustam melhor aos dados de treinamento, mas tambÃ©m sÃ£o mais propensos ao *overfitting*, e podem apresentar uma grande variÃ¢ncia. Modelos com poucas variÃ¡veis (esparsos), por outro lado, sÃ£o mais robustos e generalizam melhor para dados nÃ£o vistos.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos usar um exemplo com poucos dados e muitos preditores para demonstrar o efeito da regularizaÃ§Ã£o.
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import mean_squared_error, r2_score
>
> # Dados de exemplo
> np.random.seed(42)
> N = 20 # NÃºmero de observaÃ§Ãµes
> p = 50 # NÃºmero de preditores
>
> X = np.random.rand(N, p)
> true_beta = np.random.randn(p)
> y = X @ true_beta + np.random.randn(N)
>
> # Dividir em treinamento e teste
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # RegressÃ£o Linear (OLS)
> model_ols = LinearRegression()
> model_ols.fit(X_train, y_train)
> y_pred_ols = model_ols.predict(X_test)
> mse_ols = mean_squared_error(y_test, y_pred_ols)
> r2_ols = r2_score(y_test, y_pred_ols)
>
> # Ridge Regression
> model_ridge = Ridge(alpha=1.0) # Ajuste do parÃ¢metro alpha
> model_ridge.fit(X_train, y_train)
> y_pred_ridge = model_ridge.predict(X_test)
> mse_ridge = mean_squared_error(y_test, y_pred_ridge)
> r2_ridge = r2_score(y_test, y_pred_ridge)
>
> # Lasso Regression
> model_lasso = Lasso(alpha=0.1) # Ajuste do parÃ¢metro alpha
> model_lasso.fit(X_train, y_train)
> y_pred_lasso = model_lasso.predict(X_test)
> mse_lasso = mean_squared_error(y_test, y_pred_lasso)
> r2_lasso = r2_score(y_test, y_pred_lasso)
>
> # Elastic Net
> model_elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)
> model_elastic.fit(X_train, y_train)
> y_pred_elastic = model_elastic.predict(X_test)
> mse_elastic = mean_squared_error(y_test, y_pred_elastic)
> r2_elastic = r2_score(y_test, y_pred_elastic)
>
> # Resultados
> results = pd.DataFrame({
>     'Method': ['OLS', 'Ridge', 'Lasso', 'Elastic Net'],
>     'MSE': [mse_ols, mse_ridge, mse_lasso, mse_elastic],
>     'RÂ²': [r2_ols, r2_ridge, r2_lasso, r2_elastic],
>     'Non-Zero Parameters': [np.sum(model_ols.coef_ != 0), np.sum(model_ridge.coef_ != 0), np.sum(model_lasso.coef_ != 0), np.sum(model_elastic.coef_ != 0)]
> })
>
> print(results)
> ```
>
> | Method      |       MSE |      RÂ² |   Non-Zero Parameters |
> |:------------|----------:|--------:|----------------------:|
> | OLS         | 1.44      | -0.05   |                    50 |
> | Ridge       | 0.94      |  0.31   |                    50 |
> | Lasso       | 0.80      |  0.41   |                     5 |
> | Elastic Net | 0.89      |  0.35   |                    10 |
>
> OLS tem um MSE maior e RÂ² negativo, indicando *overfitting*. Ridge melhora o desempenho, mas ainda usa todos os preditores. Lasso e Elastic Net, atravÃ©s da regularizaÃ§Ã£o, reduzem o nÃºmero de preditores e melhoram o ajuste.
>
> A escolha do parÃ¢metro de regularizaÃ§Ã£o (alpha) Ã© crucial. Valores muito altos levam a modelos muito simples (com *underfitting*), enquanto valores muito baixos podem nÃ£o evitar o *overfitting*. A validaÃ§Ã£o cruzada Ã© uma ferramenta fundamental para escolher o valor de alpha apropriado.

**Lemma 23:**  RegularizaÃ§Ã£o e a unicidade da soluÃ§Ã£o

A regularizaÃ§Ã£o, seja L1 ou L2, garante a unicidade da soluÃ§Ã£o em problemas com deficiÃªncia de posto [^46]. A penalidade adiciona um termo Ã  funÃ§Ã£o de custo que faz com que a soluÃ§Ã£o seja Ãºnica. Geometricamente, a penalidade reduz o espaÃ§o de soluÃ§Ãµes viÃ¡veis, atÃ© que a interseÃ§Ã£o com a funÃ§Ã£o de custo seja um Ãºnico ponto.

**Prova do Lemma 23:**

Em um problema de regressÃ£o linear com deficiÃªncia de posto, a soluÃ§Ã£o por mÃ­nimos quadrados nÃ£o Ã© Ãºnica. A adiÃ§Ã£o da penalidade L2, por exemplo, garante que a matriz $(X^TX+\lambda I)$ Ã© sempre positiva definida, mesmo que $X^TX$ seja singular, o que por sua vez, leva a uma soluÃ§Ã£o Ãºnica. Para a regularizaÃ§Ã£o L1, a adiÃ§Ã£o da penalidade promove que alguns coeficientes sejam forÃ§ados a zero, levando a um modelo esparso, onde a soluÃ§Ã£o Ã© novamente Ãºnica. $\blacksquare$

**CorolÃ¡rio 23:** Tradeoff entre ajuste e complexidade

A regularizaÃ§Ã£o cria uma compensaÃ§Ã£o entre ajuste aos dados de treinamento e a complexidade do modelo. A escolha do parÃ¢metro de regularizaÃ§Ã£o ($\lambda$) controla a importÃ¢ncia relativa desses dois aspectos, e permite obter modelos que generalizam melhor para dados nÃ£o vistos. O uso de validaÃ§Ã£o cruzada auxilia a escolha do parÃ¢metro adequado, garantindo que a soluÃ§Ã£o seja robusta.

> âš ï¸ **Nota Importante**: A deficiÃªncia de posto na matriz de design X leva Ã  nÃ£o unicidade da soluÃ§Ã£o de mÃ­nimos quadrados, o que significa que existem vÃ¡rias soluÃ§Ãµes que minimizam o RSS.
> â— **Ponto de AtenÃ§Ã£o**:  A regularizaÃ§Ã£o, atravÃ©s da adiÃ§Ã£o de penalidades, remove a nÃ£o unicidade das soluÃ§Ãµes, e tambÃ©m controla o overfitting nos modelos.
> âœ”ï¸ **Destaque**: Em problemas com p > N, ou com multicolinearidade, a regularizaÃ§Ã£o Ã© uma ferramenta fundamental para obter uma soluÃ§Ã£o estÃ¡vel e para evitar overfitting.

### ConclusÃ£o

A deficiÃªncia de posto Ã© um problema comum em modelos de regressÃ£o linear, que leva Ã  nÃ£o unicidade da soluÃ§Ã£o por mÃ­nimos quadrados. Em casos de dados com muitos preditores, multicolinearidade, ou problemas com codificaÃ§Ãµes de variÃ¡veis, a regularizaÃ§Ã£o, especialmente L1 (Lasso) e L2 (Ridge) e suas combinaÃ§Ãµes, como a Elastic Net, emerge como uma ferramenta essencial para lidar com a deficiÃªncia de posto, promover a *sparsity* e a estabilidade, e obter soluÃ§Ãµes Ãºnicas e generalizÃ¡veis. A escolha da tÃ©cnica e parÃ¢metro de regularizaÃ§Ã£o Ã© fundamental para obter modelos que equilibrem o ajuste e a complexidade, para melhores resultados na aplicaÃ§Ã£o.

### ReferÃªncias
[^46]: "It might happen that the columns of X are not linearly independent, so that X is not of full rank." *(Trecho de Linear Methods for Regression)*
[^44]: "A penalidade L1 induz sparsity, zerando coeficientes menos relevantes, levando a modelos mais interpretÃ¡veis, "*(Trecho de Linear Methods for Regression)*
[^25]: "When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and exhibit high variance. A wildly large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin." *(Trecho de Linear Methods for Regression)*
[^47]: "The N-p-1 rather than N in the denominator makes Ë†Ïƒ2 an unbiased estimate of Ïƒ2: E(Ë†Ïƒ2) = Ïƒ2." *(Trecho de Linear Methods for Regression)*
