## Estima√ß√£o da Vari√¢ncia do Erro: $\sigma^2 = 1/(N-p-1) \sum(y_i-\hat{y}_i)^2$ em Modelos de Regress√£o Linear

```mermaid
flowchart TD
    A["Dados de entrada: 'x_i' e 'y_i'"] --> B{"Ajustar Modelo de Regress√£o Linear"};
    B --> C{"Calcular '$\hat{y}_i$' (valores preditos)"};
    C --> D{"Calcular '$r_i$' (res√≠duos): $y_i - \hat{y}_i$"};
    D --> E{"Calcular '$r_i^2$' (res√≠duos ao quadrado)"};
    E --> F{"Somar '$r_i^2$': $\sum(y_i-\hat{y}_i)^2$"};
    F --> G{"Calcular Graus de Liberdade: $N - p - 1$"};
    G --> H{"Estimar Vari√¢ncia do Erro: $\sigma^2 = 1/(N-p-1) \sum(y_i-\hat{y}_i)^2$"};
    H --> I["Vari√¢ncia do Erro Estimada: '$\sigma^2$'"];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style I fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o
A estimativa da **vari√¢ncia do erro**, denotada por **$\sigma^2$**, √© um componente essencial em modelos de regress√£o linear, pois quantifica a variabilidade do erro aleat√≥rio que n√£o √© explicado pelo modelo. A f√≥rmula **$\sigma^2 = 1/(N-p-1) \sum(y_i-\hat{y}_i)^2$** fornece uma estimativa n√£o viesada da vari√¢ncia do erro, onde a soma dos quadrados dos res√≠duos √© dividida pelo grau de liberdade, $(N-p-1)$, ao inv√©s de apenas $N$. Esta se√ß√£o detalhar√° cada componente da formula√ß√£o, seu significado estat√≠stico, e o papel dos graus de liberdade na estimativa.

### Formula√ß√£o Matem√°tica da Vari√¢ncia do Erro

Em um modelo de regress√£o linear, a vari√°vel resposta $y_i$ √© modelada como uma fun√ß√£o linear dos preditores, mais um erro aleat√≥rio, $\epsilon_i$:
$$
y_i = \beta_0 + \sum_{j=1}^p x_{ij}\beta_j + \epsilon_i
$$
onde:

-  $y_i$ √© o valor observado para a i-√©sima observa√ß√£o.
-  $\beta_0$ √© o *intercept*.
-   $x_{ij}$ √© o valor da j-√©sima vari√°vel preditora na i-√©sima observa√ß√£o.
-   $\beta_j$ s√£o os par√¢metros do modelo linear.
-   $\epsilon_i$ √© o erro aleat√≥rio para a i-√©sima observa√ß√£o.

O m√©todo dos m√≠nimos quadrados busca encontrar os par√¢metros $\beta_j$ que minimizam a soma dos quadrados dos res√≠duos, que √© dada por:

$$
RSS(\beta) = \sum_{i=1}^N (y_i - \hat{y_i})^2 = \sum_{i=1}^N r_i^2
$$

onde:
- $\hat{y_i}$ √© o valor predito para a i-√©sima observa√ß√£o, $\hat{y_i} = \beta_0 + \sum_{j=1}^p x_{ij}\beta_j$.
- $r_i$ √© o res√≠duo, ou a diferen√ßa entre o valor observado e o predito, $y_i - \hat{y_i}$.

A estimativa da vari√¢ncia do erro, $\sigma^2$, √© dada pela seguinte f√≥rmula:
$$
\hat{\sigma}^2 = \frac{1}{N-p-1} \sum_{i=1}^N (y_i - \hat{y}_i)^2 = \frac{1}{N-p-1} \sum_{i=1}^N r_i^2
$$
onde:
-   $N$ √© o n√∫mero total de observa√ß√µes.
-   $p$ √© o n√∫mero de preditores no modelo.
-   $\hat{y}_i$ √© o valor predito pelo modelo para a i-√©sima observa√ß√£o.
-   $N-p-1$ √© o n√∫mero de graus de liberdade associados ao modelo.
O termo $1/(N-p-1)$ √© usado no c√°lculo da estimativa para garantir que a estimativa da vari√¢ncia seja n√£o viesada.

> üí° **Exemplo Num√©rico:**
> Suponha que temos um modelo de regress√£o linear com uma vari√°vel preditora ($p=1$) e 5 observa√ß√µes ($N=5$). Os dados s√£o:
>
> | Observa√ß√£o (i) | $x_i$ | $y_i$ |
> |-----------------|-------|-------|
> | 1               | 1     | 2     |
> | 2               | 2     | 3     |
> | 3               | 3     | 5     |
> | 4               | 4     | 6     |
> | 5               | 5     | 8     |
>
> Ap√≥s ajustar o modelo de regress√£o linear, obtemos os seguintes valores preditos ($\hat{y}_i$):
>
> | Observa√ß√£o (i) | $x_i$ | $y_i$ | $\hat{y}_i$ | $r_i = y_i - \hat{y}_i$ | $r_i^2$ |
> |-----------------|-------|-------|-------------|------------------------|---------|
> | 1               | 1     | 2     | 2.0         | 0.0                    | 0.0     |
> | 2               | 2     | 3     | 3.2         | -0.2                   | 0.04    |
> | 3               | 3     | 5     | 4.4         | 0.6                    | 0.36    |
> | 4               | 4     | 6     | 5.6         | 0.4                    | 0.16    |
> | 5               | 5     | 8     | 6.8         | 1.2                    | 1.44    |
>
> A soma dos quadrados dos res√≠duos (RSS) √©:
> $$
> RSS = \sum_{i=1}^5 r_i^2 = 0.0 + 0.04 + 0.36 + 0.16 + 1.44 = 2.0
> $$
>
> Os graus de liberdade s√£o $N - p - 1 = 5 - 1 - 1 = 3$. Portanto, a estimativa da vari√¢ncia do erro √©:
>
> $$
> \hat{\sigma}^2 = \frac{RSS}{N-p-1} = \frac{2.0}{3} \approx 0.667
> $$
>
> Este valor de $\hat{\sigma}^2$ representa a variabilidade dos erros em torno da linha de regress√£o ajustada.

### Import√¢ncia dos Graus de Liberdade

O termo ($N - p - 1$) no denominador da estimativa de $\sigma^2$ representa os graus de liberdade do modelo. Os graus de liberdade correspondem ao n√∫mero de observa√ß√µes independentes que est√£o dispon√≠veis para estimar a vari√¢ncia do erro, ap√≥s a estima√ß√£o dos par√¢metros do modelo.  A ideia √© que os res√≠duos est√£o sujeitos a $N$ restri√ß√µes, uma para cada par√¢metro, e portanto a vari√¢ncia √© calculada com $N-p-1$ graus de liberdade.

```mermaid
flowchart TD
    A["N√∫mero total de observa√ß√µes (N)"] --> B{"N√∫mero de par√¢metros a serem estimados (p+1)"};
    B --> C{"Restri√ß√µes impostas aos dados para estimar os par√¢metros"};
    C --> D{"Graus de liberdade (N - p - 1)"};
    D --> E["N√∫mero de res√≠duos que podem variar livremente"];
    style D fill:#aaf,stroke:#333,stroke-width:2px
```

Em termos mais detalhados:
    -  $N$ representa o n√∫mero total de observa√ß√µes,
    -  $p+1$ √© o n√∫mero de par√¢metros a serem estimados (incluindo o *intercept*), que por sua vez, correspondem ao n√∫mero de restri√ß√µes impostas nos dados para estimar os par√¢metros.
    - $N-p-1$ √© o n√∫mero de res√≠duos que podem variar livremente, dados os par√¢metros do modelo.
A utiliza√ß√£o de $N-p-1$ no denominador, ao inv√©s de apenas $N$, garante que a estimativa de $\sigma^2$ seja **n√£o viesada**, ou seja, que o valor esperado da estimativa corresponde ao verdadeiro valor da vari√¢ncia do erro [^47].

**Lemma 25:**  N√£o Viesamento da Estimativa da Vari√¢ncia do Erro
A estimativa da vari√¢ncia do erro $\hat{\sigma}^2 = \frac{1}{N-p-1} \sum_{i=1}^N (y_i - \hat{y}_i)^2$ √© um estimador n√£o viesado da vari√¢ncia do erro $\sigma^2$, ou seja, que $E[\hat{\sigma}^2] = \sigma^2$ , onde $E$ representa a expectativa.

**Prova do Lemma 25:**
Para mostrar o n√£o viesamento da estimativa, vamos assumir que o modelo √©:
$$
Y = X \beta + \epsilon
$$
onde $\epsilon \sim N(0,\sigma^2 I)$ e a solu√ß√£o por m√≠nimos quadrados √© dada por $\hat{\beta} = (X^T X)^{-1} X^T y$
O res√≠duo √© ent√£o
$$
\hat{\epsilon} = y - X\hat{\beta} = y - X(X^TX)^{-1}X^T y= (I-H)y
$$
onde H √© a matriz Hat.
Substituindo a express√£o de $y$ temos
$$ \hat{\epsilon} = (I-H)(X\beta + \epsilon) = (I-H)\epsilon $$
Ent√£o, a soma dos quadrados dos res√≠duos pode ser escrita como:
$$ \hat{\epsilon}^T \hat{\epsilon} = \epsilon^T(I-H)^T(I-H)\epsilon=\epsilon^T(I-H)\epsilon$$
Usando a propriedade da tra√ßa de matrizes ( a tra√ßa √© a soma dos elementos da diagonal de uma matriz),
$$E[\hat{\epsilon}^T \hat{\epsilon}] = E[\text{tr}(\epsilon^T(I-H)\epsilon)]=E[\text{tr}((I-H)\epsilon \epsilon^T)]=\text{tr}((I-H)E[\epsilon \epsilon^T])=\sigma^2\text{tr}(I-H)$$
Note que o tra√ßo da matriz identidade de $N$ dimens√µes √© $N$. O tra√ßo da matriz H, tamb√©m chamado de grau de liberdade, √© dado pelo n√∫mero de par√¢metros do modelo, $p+1$. Assim, a esperan√ßa da soma dos res√≠duos √©:
$$E[\hat{\epsilon}^T \hat{\epsilon}]=\sigma^2 (N - p - 1)$$
e dividindo pelo n√∫mero de graus de liberdade, temos que:
$$E[\hat{\sigma}^2] = E\frac{\hat{\epsilon}^T \hat{\epsilon}}{N-p-1} = \sigma^2$$
o que demonstra que a estimativa da vari√¢ncia do erro √© n√£o viesada. $\blacksquare$

**Corol√°rio 25:** Viesamento da Estimativa quando se usa N
Ao dividir a soma dos quadrados dos res√≠duos por $N$, em vez de $N-p-1$, obtemos um estimador da vari√¢ncia do erro que √© viesado, e que tende a subestimar o valor da vari√¢ncia do erro, dado que $N>N-p-1$.

> üí° **Exemplo Num√©rico:**
> Usando o mesmo exemplo anterior, se divid√≠ssemos o RSS por $N$ em vez de $N-p-1$, obter√≠amos:
>
> $$
> \hat{\sigma}^2_{viesado} = \frac{RSS}{N} = \frac{2.0}{5} = 0.4
> $$
>
> Este valor de 0.4 √© menor que a estimativa n√£o viesada de 0.667. Isso ilustra como dividir por $N$ subestima a verdadeira vari√¢ncia do erro.

### Interpreta√ß√£o Estat√≠stica e Geom√©trica da Estimativa da Vari√¢ncia

A estimativa da vari√¢ncia do erro tem uma interpreta√ß√£o estat√≠stica e geom√©trica:

1.  **Variabilidade dos Erros:** A estimativa $\hat{\sigma}^2$ quantifica a dispers√£o do erro em torno da fun√ß√£o de regress√£o, e representa a quantidade de varia√ß√£o na vari√°vel resposta que n√£o √© explicada pelos preditores [^10].
2.  **Distribui√ß√£o do Erro:** Em muitos casos, o erro aleat√≥rio $\epsilon$ √© assumido como tendo distribui√ß√£o normal com m√©dia zero e vari√¢ncia $\sigma^2$. Essa hip√≥tese permite infer√™ncias estat√≠sticas e construir intervalos de confian√ßa para os par√¢metros.
3.  **Conex√£o com a Soma dos Quadrados dos Res√≠duos:** A estimativa da vari√¢ncia do erro √© diretamente proporcional √† soma dos quadrados dos res√≠duos, e um modelo com menor RSS tem menor vari√¢ncia do erro, o que significa que os pontos de dados est√£o mais pr√≥ximos do hiperplano de regress√£o.
4.  **Incerteza da Estimativa:** A vari√¢ncia do erro, assim como a vari√¢ncia dos par√¢metros, indica o quanto incerta √© a estimativa do modelo. A estimativa da vari√¢ncia do erro pode ser combinada com os erros padr√£o dos par√¢metros para obter intervalos de confian√ßa e realizar testes de hip√≥tese.

> üí° **Exemplo Num√©rico:**
> Considere dois modelos de regress√£o ajustados aos mesmos dados. O Modelo A tem um RSS de 10 e o Modelo B tem um RSS de 5. Ambos t√™m 10 observa√ß√µes e 2 preditores.
>
> Para o Modelo A:
>   - $N = 10$, $p = 2$
>   - $RSS_A = 10$
>   - $\hat{\sigma}^2_A = \frac{10}{10 - 2 - 1} = \frac{10}{7} \approx 1.43$
>
> Para o Modelo B:
>   - $N = 10$, $p = 2$
>   - $RSS_B = 5$
>   - $\hat{\sigma}^2_B = \frac{5}{10 - 2 - 1} = \frac{5}{7} \approx 0.71$
>
> O Modelo B tem uma vari√¢ncia do erro estimada menor, indicando que ele se ajusta melhor aos dados do que o Modelo A. Isso significa que a variabilidade dos erros em torno da linha de regress√£o no Modelo B √© menor do que no Modelo A.

A estimativa n√£o viesada da vari√¢ncia do erro √© um componente essencial para quantificar a incerteza do modelo, e √© um elemento fundamental para a qualidade das previs√µes do modelo.

```mermaid
flowchart TD
    A["Alto RSS"] --> B["Alta Vari√¢ncia do Erro ($\sigma^2$)"];
    B --> C["Menor Qualidade de Ajuste do Modelo"];
    D["Baixo RSS"] --> E["Baixa Vari√¢ncia do Erro ($\sigma^2$)"];
    E --> F["Maior Qualidade de Ajuste do Modelo"];
    style B fill:#fbb,stroke:#333,stroke-width:2px
    style E fill:#bfb,stroke:#333,stroke-width:2px
```

‚ö†Ô∏è **Nota Importante**:  A estimativa da vari√¢ncia do erro √© dada por $\sigma^2 = 1/(N-p-1) \sum(y_i-\hat{y}_i)^2$, onde a divis√£o por $N-p-1$ garante que a estimativa seja n√£o viesada.
 
‚ùó **Ponto de Aten√ß√£o**: A estima√ß√£o de par√¢metros usa a soma dos quadrados dos res√≠duos, e a divis√£o pelos graus de liberdade resulta num estimador n√£o viesado da vari√¢ncia do erro.
 
‚úîÔ∏è **Destaque**: O conceito dos graus de liberdade √© essencial para a estimativa de $\sigma^2$, e a utiliza√ß√£o do termo $N-p-1$ garante que a estimativa seja n√£o viesada.

### Conclus√£o

A estimativa da vari√¢ncia do erro, expressa por $\hat{\sigma}^2 = \frac{1}{N-p-1} \sum_{i=1}^N (y_i - \hat{y}_i)^2$, √© fundamental para a compreens√£o e a utiliza√ß√£o de modelos de regress√£o linear. A utiliza√ß√£o do termo de graus de liberdade no denominador garante que o estimador seja n√£o viesado, fornecendo uma representa√ß√£o precisa da incerteza associada √†s previs√µes do modelo. O uso adequado desta estimativa √© crucial para a valida√ß√£o e infer√™ncia nos modelos lineares.

### Refer√™ncias

[^10]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Methods for Regression)*
[^11]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^47]: "The N-p-1 rather than N in the denominator makes ÀÜœÉ2 an unbiased estimate of œÉ2: E(ÀÜœÉ2) = œÉ2." *(Trecho de Linear Methods for Regression)*
[^12]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Methods for Regression)*
