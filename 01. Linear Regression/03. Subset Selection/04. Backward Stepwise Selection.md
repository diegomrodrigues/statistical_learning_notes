## Backward-Stepwise Selection em Regressão Linear

### Introdução
Este capítulo explora o método de seleção de variáveis **Backward-Stepwise Selection** no contexto de modelos de regressão linear. A seleção de subconjuntos de variáveis é uma etapa crucial na modelagem estatística, especialmente quando lidamos com um grande número de preditores [^57]. A Backward-Stepwise Selection oferece uma abordagem sistemática para simplificar o modelo, removendo iterativamente as variáveis menos significativas. Este método, junto com o Forward-Stepwise Selection, é uma alternativa para encontrar um bom caminho entre todos os subconjuntos de variáveis, especialmente quando a busca exaustiva se torna inviável [^58].

### Conceitos Fundamentais

**Backward-Stepwise Selection** [^59]
Começa com o modelo completo, que inclui todos os preditores disponíveis. Em cada etapa, o algoritmo avalia o impacto de cada preditor no ajuste do modelo. A variável que menos contribui para o ajuste é removida. O processo continua até que um critério de parada seja satisfeito, como um nível de significância predefinido ou um número máximo de variáveis removidas.

**Z-score** [^59]
A métrica usada para avaliar o impacto de cada preditor é o Z-score, calculado como a razão entre o coeficiente estimado e seu erro padrão [^48]:
$$Z_j = \frac{\hat{\beta}_j}{\hat{\sigma}_j}$$.
O preditor com o menor Z-score (em valor absoluto) é o candidato para remoção [^59].

**Condições de Aplicabilidade** [^59]
Uma limitação importante da Backward-Stepwise Selection é que ela só pode ser usada quando o número de observações ($N$) é maior que o número de preditores ($p$), ou seja, $N > p$. Isso se deve à necessidade de estimar os coeficientes do modelo completo no início do processo. Quando $N \leq p$, o modelo completo é superparametrizado, e a matriz $X^TX$ não é inversível [^4].

**Algoritmo**
O processo de Backward-Stepwise Selection pode ser resumido nos seguintes passos:
1.  Começar com o modelo completo, incluindo todos os $p$ preditores.
2.  Ajustar o modelo completo aos dados.
3.  Calcular o Z-score para cada preditor.
4.  Identificar o preditor com o menor Z-score em valor absoluto.
5.  Remover o preditor identificado do modelo.
6.  Repetir os passos 2-5 até que um critério de parada seja atingido.

O critério de parada pode ser baseado em:
*   Um nível de significância predefinido (e.g., $\alpha = 0.05$).
*   Um número máximo de variáveis a serem removidas.
*   Uma métrica de desempenho do modelo, como o AIC (Akaike Information Criterion) [^60] ou o BIC (Bayesian Information Criterion).

**Comparação com Forward-Stepwise Selection**
Em contraste com a Backward-Stepwise Selection, o **Forward-Stepwise Selection** [^58] começa com um modelo nulo (apenas o intercepto) e adiciona preditores sequencialmente, selecionando aquele que mais melhora o ajuste do modelo em cada etapa. Uma vantagem do Forward-Stepwise Selection é que ele pode ser usado mesmo quando $N \leq p$ [^59].

**Exemplo Prático**
Considere o exemplo dos dados de câncer de próstata [^49], onde ajustamos um modelo linear para o log do antígeno prostático específico (*lpsa*) em função de diversas medidas clínicas. A tabela 3.2 [^50] apresenta os coeficientes, erros padrão e Z-scores dos preditores. No contexto da Backward-Stepwise Selection, começaríamos com o modelo completo e removeríamos o preditor com o menor Z-score em valor absoluto. Neste caso, *gleason* tem o menor Z-score (-0.15), sendo o primeiro candidato para remoção.

### Conclusão

A Backward-Stepwise Selection é uma técnica útil para simplificar modelos de regressão linear, especialmente quando há um grande número de preditores. No entanto, é importante estar ciente de suas limitações e considerar outras abordagens, como o Forward-Stepwise Selection e métodos de regularização, dependendo das características dos dados e dos objetivos da análise. A escolha do método de seleção de variáveis deve ser guiada por uma combinação de critérios estatísticos e considerações práticas, como interpretabilidade e desempenho preditivo do modelo.

### Referências
[^4]:  "Most regression software packages detect these redundancies and automatically implement."
[^48]: "To test the hypothesis that a particular coefficient β; = 0, we form the standardized coefficient or Z-score $Z_j = \frac{\beta_j}{\hat{\sigma}_j}$."
[^49]: "The data for this example come from a study by Stamey et al. (1989)."
[^50]: "TABLE 3.2. Linear model fit to the prostate cancer data. The Z score is the coefficient divided by its standard error (3.12). Roughly a Z score larger than two in absolute value is significantly nonzero at the p = 0.05 level."
[^57]: "With subset selection we retain only a subset of the variables, and eliminate the rest from the model."
[^58]: "Forward-stepwise selection starts with the intercept, and then sequentially adds into the model the predictor that most improves the fit."
[^59]: "Backward-stepwise selection starts with the full model, and sequentially deletes the predictor that has the least impact on the fit; backward selection can only be used when N > p, while forward stepwise can always be used."
[^60]: "For example in the R package the step function uses the AIC criterion for weighing the choices, which takes proper account of the number of parameters fit; at each step an add or drop will be performed that minimizes the AIC score."
<!-- END -->