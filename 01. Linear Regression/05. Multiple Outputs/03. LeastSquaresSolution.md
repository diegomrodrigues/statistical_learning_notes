## Solu√ß√£o de M√≠nimos Quadrados: $B = (X^TX)^{-1}X^TY$

```mermaid
graph LR
    A["Problema: Minimizar RSS(B)"] --> B("Derivar RSS(B) em rela√ß√£o a B");
    B --> C("Igualar a derivada a zero: -2X·µÄ(Y - XB) = 0");
    C --> D("Resolver para B");
    D --> E("Solu√ß√£o: B = (X·µÄX)‚Åª¬πX·µÄY");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Em modelos de regress√£o linear m√∫ltipla, a solu√ß√£o de **m√≠nimos quadrados** para a matriz de coeficientes $B$ √© dada pela express√£o $B = (X^TX)^{-1}X^TY$ [^1]. Essa solu√ß√£o √© fundamental na estima√ß√£o dos par√¢metros do modelo e representa a matriz de coeficientes que minimiza a soma dos quadrados dos res√≠duos para todas as vari√°veis respostas. Nesta se√ß√£o, vamos explorar a deriva√ß√£o dessa solu√ß√£o em detalhes, analisando a sua interpreta√ß√£o e as condi√ß√µes para sua exist√™ncia.

### Deriva√ß√£o da Solu√ß√£o de M√≠nimos Quadrados

O objetivo do m√©todo dos m√≠nimos quadrados, em modelos com m√∫ltiplas vari√°veis respostas, √© encontrar a matriz de coeficientes $B$ que minimize a fun√ß√£o de perda multivariada, dada por [^3]:
$$ RSS(B) = tr[(Y - XB)^T (Y - XB)] $$

Onde:
-   $Y$ √© uma matriz $N \times K$ de respostas.
-   $X$ √© uma matriz $N \times (p+1)$ de preditores, incluindo o intercepto.
-   $B$ √© uma matriz $(p+1) \times K$ de coeficientes [^4].

Para encontrar a matriz $B$ que minimiza essa fun√ß√£o, precisamos deriv√°-la em rela√ß√£o a $B$ e igual√°-la a zero [^5].

**Lemma 1:** *A derivada da fun√ß√£o de perda $RSS(B)$ em rela√ß√£o a $B$ √© dada por $-2X^T(Y-XB)$* [^6].

**Prova do Lemma 1:**
A fun√ß√£o de perda √© expressa como
$$ RSS(B) = tr[(Y-XB)^T (Y-XB)] = tr[Y^TY - Y^TXB - B^TX^TY + B^TX^TXB]  $$

A derivada do tra√ßo em rela√ß√£o a B √© uma generaliza√ß√£o da derivada de uma fun√ß√£o escalar com rela√ß√£o a um vetor.
Utilizando as propriedades de tra√ßo, como $tr(A) = tr(A^T)$ e $tr(ABC)=tr(BCA)=tr(CAB)$, podemos derivar:
$$ \frac{\partial RSS(B)}{\partial B} = \frac{\partial tr(Y^TY)}{\partial B} - \frac{\partial tr(Y^TXB)}{\partial B} - \frac{\partial tr(B^TX^TY)}{\partial B} + \frac{\partial tr(B^TX^TXB)}{\partial B} $$
Os termos $tr(Y^TY)$ e $tr(Y^TXB)$ n√£o dependem de $B$ e o seu derivado √© 0. Podemos ainda usar as propriedades de matrizes que dizem que $ \frac{\partial tr(C B)}{\partial B} = C^T $ e $\frac{\partial tr(B^TDB)}{\partial B} = D^T B + DB $, resultando em
```mermaid
flowchart LR
    A["RSS(B)"] --> B{"Derivar em rela√ß√£o a B"};
    B --> C{"Aplicar propriedades do tra√ßo e matrizes"};
    C --> D{"Resultado: -2X·µÄ(Y-XB)"};
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

$$ \frac{\partial RSS(B)}{\partial B} = - X^TY - (X^TY)^T + (X^TX)B + (X^TX)^TB = -2X^TY + 2X^TXB $$
$$ \frac{\partial RSS(B)}{\partial B} = -2X^T(Y-XB) $$
$\blacksquare$

Igualando a derivada a zero para encontrar o ponto de m√≠nimo, temos [^7]:

$$ -2X^T(Y - XB) = 0 $$

$$ X^TY - X^TXB = 0 $$

$$ X^TXB = X^TY $$
Se a matriz $X^TX$ for invert√≠vel, podemos multiplicar ambos os lados pela sua inversa, obtendo a solu√ß√£o de m√≠nimos quadrados [^8]:

$$ B = (X^TX)^{-1} X^TY $$
> üí° **Exemplo Num√©rico:**
> Vamos considerar um exemplo simples com duas vari√°veis preditoras e uma vari√°vel resposta. Suponha que temos os seguintes dados:
>
> ```python
> import numpy as np
>
> # Matriz de preditores X (incluindo uma coluna de 1s para o intercepto)
> X = np.array([[1, 2, 3],
>              [1, 3, 5],
>              [1, 4, 2],
>              [1, 5, 6],
>              [1, 6, 4]])
>
> # Vetor de respostas y
> y = np.array([[5],
>              [6],
>              [7],
>              [8],
>              [9]])
> ```
>
> Aqui, $N=5$ (n√∫mero de amostras), $p=2$ (n√∫mero de preditores), e $K=1$ (n√∫mero de respostas).
>
> **C√°lculo de $X^TX$:**
>
> ```python
> XtX = np.dot(X.T, X)
> print("X^TX:\n", XtX)
> ```
>
> Resultado:
> ```
> X^TX:
> [[ 5 20 20]
>  [20 90 80]
>  [20 80 70]]
> ```
>
> **C√°lculo de $(X^TX)^{-1}$:**
>
> ```python
> XtX_inv = np.linalg.inv(XtX)
> print("(X^TX)^-1:\n", XtX_inv)
> ```
>
> Resultado:
> ```
> (X^TX)^-1:
> [[ 1.925 -0.25  -0.2  ]
>  [-0.25   0.0625  0.   ]
>  [-0.2    0.     0.05 ]]
> ```
>
> **C√°lculo de $X^Ty$:**
>
> ```python
> XtY = np.dot(X.T, y)
> print("X^Ty:\n", XtY)
> ```
>
> Resultado:
> ```
> X^Ty:
> [[ 35]
>  [150]
>  [140]]
> ```
>
> **C√°lculo de $B = (X^TX)^{-1}X^Ty$:**
>
> ```python
> B = np.dot(XtX_inv, XtY)
> print("B:\n", B)
> ```
>
> Resultado:
> ```
> B:
> [[2.3 ]
>  [0.6 ]
>  [0.2 ]]
> ```
> Os coeficientes estimados s√£o aproximadamente $\beta_0 = 2.3$, $\beta_1 = 0.6$ e $\beta_2 = 0.2$. Isso significa que a equa√ß√£o de regress√£o estimada √© $\hat{y} = 2.3 + 0.6x_1 + 0.2x_2$.

**Lemma 2:** *A matriz $X^TX$ √© invert√≠vel se, e somente se, as colunas da matriz de preditores $X$ s√£o linearmente independentes*. Esta condi√ß√£o √© crucial para a exist√™ncia de uma solu√ß√£o √∫nica de m√≠nimos quadrados [^9].

**Prova do Lemma 2:**
Se $X$ tem colunas linearmente independentes, isso significa que nenhuma coluna de $X$ pode ser escrita como combina√ß√£o linear das demais colunas. A matriz $X^TX$ √© uma matriz sim√©trica. Se as colunas de $X$ s√£o linearmente independentes, a matrix $X^TX$ √© positiva definida e, portanto, invert√≠vel, o que garante a unicidade da solu√ß√£o de m√≠nimos quadrados. A condi√ß√£o da invertibilidade de $X^TX$ √© equivalente a dizer que  $X$ tem posto completo. $\blacksquare$

### Interpreta√ß√£o da Solu√ß√£o de M√≠nimos Quadrados

A solu√ß√£o $B = (X^TX)^{-1}X^TY$ representa a matriz de coeficientes que minimiza a soma dos quadrados dos res√≠duos para todas as vari√°veis respostas. Cada coluna de $B$ corresponde aos coeficientes da regress√£o para cada vari√°vel resposta [^10]:

```mermaid
graph LR
    A["B"] --> B["[Œ≤ÃÇ‚ÇÅ, Œ≤ÃÇ‚ÇÇ, ..., Œ≤ÃÇ‚Çñ]"]
    B --> C("Cada Œ≤ÃÇ‚Çñ √© um vetor (p+1)x1");
    style A fill:#ccf,stroke:#333,stroke-width:2px
```

$$ B = [ \hat{\beta_1}, \hat{\beta_2}, \ldots, \hat{\beta_K}] $$

Onde cada $\hat{\beta_k}$ √© um vetor $(p+1) \times 1$ que cont√©m os coeficientes para a vari√°vel resposta $Y_k$ [^11]. Assim:

$$ \hat{\beta_k} = (X^TX)^{-1}X^T y_k $$

Onde $y_k$ √© a k-√©sima coluna da matriz $Y$.

**Corol√°rio 1:** *A solu√ß√£o para cada coluna de B √© exatamente a mesma obtida resolvendo um problema de m√≠nimos quadrados separado para cada resposta, ou seja, como uma regress√£o linear m√∫ltipla com apenas uma vari√°vel resposta*. Isso ocorre na aus√™ncia de correla√ß√£o entre os erros [^12].

### Implica√ß√µes Pr√°ticas da Solu√ß√£o de M√≠nimos Quadrados

A solu√ß√£o de m√≠nimos quadrados $B = (X^TX)^{-1}X^TY$ √© amplamente utilizada na pr√°tica devido a sua simplicidade e ao fato de que, em muitos casos, fornece estimativas razo√°veis dos par√¢metros [^13]. No entanto, √© importante estar ciente de suas limita√ß√µes, especialmente quando as premissas subjacentes ao modelo n√£o s√£o satisfeitas [^14].
*A condi√ß√£o para a solu√ß√£o de m√≠nimos quadrados existir √© que $X^TX$ seja invert√≠vel, o que significa que as colunas de $X$ devem ser linearmente independentes*. Na pr√°tica, isso significa que n√£o podemos ter preditores que sejam combina√ß√µes lineares de outros preditores, a colinearidade, o que tornaria a matrix $X^TX$ singular e impossibilitaria a obten√ß√£o de uma solu√ß√£o √∫nica [^15].

Al√©m disso, em situa√ß√µes com alto n√∫mero de preditores e um n√∫mero limitado de observa√ß√µes, a matriz $X^TX$ pode se tornar mal-condicionada e a solu√ß√£o de m√≠nimos quadrados pode ser muito inst√°vel e apresentar grande vari√¢ncia [^16].
*Nessas situa√ß√µes, t√©cnicas de regulariza√ß√£o e sele√ß√£o de vari√°veis s√£o usadas para lidar com os problemas da multicolinearidade e reduzir a vari√¢ncia das estimativas*.

> üí° **Exemplo Num√©rico:**
> Considere um cen√°rio com 50 observa√ß√µes e 10 preditores, onde os preditores s√£o gerados aleatoriamente e a vari√°vel resposta √© uma combina√ß√£o linear dos preditores mais um erro aleat√≥rio:
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import mean_squared_error, r2_score
>
> np.random.seed(42)
> N = 50  # Number of observations
> p = 10  # Number of predictors
>
> # Generate random predictors
> X = np.random.rand(N, p)
>
> # True coefficients
> true_coef = np.random.rand(p)
>
> # Generate response variable with some noise
> y = np.dot(X, true_coef) + np.random.normal(0, 0.5, N)
>
> # Add a column of ones for the intercept
> X = np.concatenate((np.ones((N, 1)), X), axis=1)
>
> # Calculate B using the normal equation
> XtX = np.dot(X.T, X)
> XtX_inv = np.linalg.inv(XtX)
> XtY = np.dot(X.T, y)
> B_normal = np.dot(XtX_inv, XtY)
>
> print("Estimated coefficients using normal equation:\n", B_normal)
>
> # Split data into train and test sets
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
>
> # Fit using sklearn's LinearRegression
> model_sklearn = LinearRegression()
> model_sklearn.fit(X_train, y_train)
>
> # Make predictions
> y_pred_sklearn = model_sklearn.predict(X_test)
>
> # Calculate MSE and R^2
> mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)
> r2_sklearn = r2_score(y_test, y_pred_sklearn)
>
> print(f'MSE from sklearn: {mse_sklearn}')
> print(f'R¬≤ from sklearn: {r2_sklearn}')
>
> # Print coefficients
> print("Estimated coefficients using sklearn's LinearRegression:\n", model_sklearn.coef_)
> print("Intercept using sklearn's LinearRegression:\n", model_sklearn.intercept_)
> ```
>
> Aqui, podemos ver que os coeficientes estimados pela equa√ß√£o normal e pelo `LinearRegression` do scikit-learn s√£o muito pr√≥ximos, confirmando a validade da equa√ß√£o normal. O MSE e o R¬≤ s√£o usados para avaliar o ajuste do modelo aos dados.

### Considera√ß√µes sobre a Invertibilidade da Matriz (X·µÄX)

A invertibilidade da matriz $X^TX$ √© uma condi√ß√£o crucial para a unicidade da solu√ß√£o de m√≠nimos quadrados [^18]. Quando a matriz $X^TX$ n√£o √© invert√≠vel (ou seja, √© singular), existem infinitas solu√ß√µes para $B$ que minimizam o RSS [^19]. Isso ocorre quando as colunas da matriz $X$ n√£o s√£o linearmente independentes, isto √©, quando existe multicolinearidade entre os preditores. Em termos pr√°ticos, isso pode ocorrer quando:
1.  H√° redund√¢ncia nos preditores, ou seja, um preditor √© uma combina√ß√£o linear de outros.
2.  O n√∫mero de preditores √© maior do que o n√∫mero de observa√ß√µes, levando a uma matriz $X^TX$ com posto deficiente.
3.  Os preditores s√£o quase colineares, resultando em uma matriz $X^TX$ mal condicionada.

Em casos onde a matriz $X^TX$ n√£o √© invert√≠vel, m√©todos como a pseudo-inversa e a decomposi√ß√£o em valores singulares (SVD) podem ser usados para obter uma solu√ß√£o de m√≠nimos quadrados com norma m√≠nima [^20].

> üí° **Exemplo Num√©rico: Multicolinearidade**
> Para ilustrar a multicolinearidade, vamos criar um exemplo onde um preditor √© uma combina√ß√£o linear de outro:
> ```python
> import numpy as np
>
> # Matrix X with multicollinearity
> X = np.array([[1, 2, 4],
>               [1, 3, 6],
>               [1, 4, 8],
>               [1, 5, 10]])
>
> # Response variable y
> y = np.array([[5],
>               [6],
>               [7],
>               [8]])
>
> # Calculate X^TX
> XtX = np.dot(X.T, X)
>
> # Attempt to calculate the inverse
> try:
>     XtX_inv = np.linalg.inv(XtX)
>     print("Inverse of X^TX:\n", XtX_inv)
> except np.linalg.LinAlgError:
>     print("X^TX is singular (not invertible) due to multicollinearity.")
> ```
> Resultado:
> ```
> X^TX is singular (not invertible) due to multicollinearity.
> ```
> Neste caso, a matriz $X^TX$ n√£o √© invert√≠vel porque a terceira coluna de $X$ √© exatamente duas vezes a segunda coluna. Isso demonstra que a multicolinearidade impede a obten√ß√£o de uma solu√ß√£o √∫nica de m√≠nimos quadrados.

### Pergunta Te√≥rica Avan√ßada: Como a Solu√ß√£o de M√≠nimos Quadrados e as Premissas do Modelo Influenciam a Distribui√ß√£o Amostral das Estimativas de Par√¢metros em Regress√£o Linear M√∫ltipla?

**Resposta:**

A solu√ß√£o de m√≠nimos quadrados $B = (X^TX)^{-1}X^TY$ √© obtida atrav√©s de um processo de otimiza√ß√£o que minimiza o RSS, mas a distribui√ß√£o amostral dos estimadores $\hat{B}$ depende crucialmente das premissas assumidas sobre a matriz de erros $E$ [^21].
Assumindo que os erros s√£o n√£o-correlacionados, t√™m m√©dia zero e vari√¢ncia constante $\sigma^2$, ou seja, $E \sim N(0, \sigma^2I)$, a distribui√ß√£o amostral das estimativas $\hat{B}$ segue uma distribui√ß√£o normal multivariada:

$$ \hat{B} \sim N(B, \sigma^2 (X^TX)^{-1}) $$

Essa distribui√ß√£o assume que a matriz $X$ √© fixa (isto √©, que os preditores n√£o s√£o aleat√≥rios) e que os erros t√™m distribui√ß√£o normal [^22]. As vari√¢ncias dos coeficientes $\beta_j$ s√£o dadas pelos elementos da diagonal de  $(X^TX)^{-1}$ multiplicados por $\sigma^2$. Portanto, a estrutura da matriz $X^TX$ tem um papel fundamental na variabilidade dos par√¢metros estimados [^23]. *Em especial, na presen√ßa de multicolinearidade, a vari√¢ncia dos coeficientes aumenta, resultando em estimativas menos precisas e est√°veis*.

```mermaid
graph LR
    A["Premissas sobre os erros (E)"] --> B{"E ~ N(0, œÉ¬≤I)"};
    B --> C{"Distribui√ß√£o amostral de BÃÇ"};
    C --> D{"BÃÇ ~ N(B, œÉ¬≤(X·µÄX)‚Åª¬π)"};
    D --> E{"Vari√¢ncia dos coeficientes depende de (X·µÄX)‚Åª¬π"};
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico: Vari√¢ncia dos Estimadores**
> Vamos ilustrar como a estrutura de $X^TX$ afeta a vari√¢ncia dos estimadores. Considere dois cen√°rios: um com preditores ortogonais e outro com preditores correlacionados.
>
> **Cen√°rio 1: Preditores Ortogonais**
> ```python
> import numpy as np
>
> # Preditores ortogonais
> X1 = np.array([[1, 0],
>                [0, 1],
>                [1, 1]])
>
> # Adiciona coluna de 1s para o intercepto
> X1 = np.concatenate((np.ones((X1.shape[0], 1)), X1), axis=1)
>
> XtX1 = np.dot(X1.T, X1)
> XtX1_inv = np.linalg.inv(XtX1)
> print("X^TX para preditores ortogonais:\n", XtX1)
> print("Diagonal de (X^TX)^-1 para preditores ortogonais:\n", np.diag(XtX1_inv))
> ```
>
> **Cen√°rio 2: Preditores Correlacionados**
> ```python
> # Preditores correlacionados
> X2 = np.array([[1, 2],
>                [1, 3],
>                [1, 4]])
>
> # Adiciona coluna de 1s para o intercepto
> X2 = np.concatenate((np.ones((X2.shape[0], 1)), X2), axis=1)
>
> XtX2 = np.dot(X2.T, X2)
> XtX2_inv = np.linalg.inv(XtX2)
> print("X^TX para preditores correlacionados:\n", XtX2)
> print("Diagonal de (X^TX)^-1 para preditores correlacionados:\n", np.diag(XtX2_inv))
> ```
>
> No cen√°rio com preditores ortogonais, os elementos da diagonal de $(X^TX)^{-1}$ s√£o menores, o que implica em menor vari√¢ncia para os estimadores. No cen√°rio com preditores correlacionados, a vari√¢ncia dos estimadores √© maior.

A suposi√ß√£o da normalidade dos erros tamb√©m √© crucial para o uso de testes de hip√≥teses e intervalos de confian√ßa associados √†s estimativas [^24]. Se essa suposi√ß√£o n√£o √© v√°lida, ent√£o as infer√™ncias baseadas nessas distribui√ß√µes podem n√£o ser confi√°veis. Nestes casos, o uso de m√©todos n√£o-param√©tricos ou alternativas robustas podem ser prefer√≠veis.
A matriz de covari√¢ncia das estimativas de par√¢metros, $\sigma^2 (X^TX)^{-1}$ tem  depend√™ncia da inversa da matriz $X^TX$. *Assim, a condi√ß√£o de $X^TX$ √© um indicador de qu√£o bem-comportada √© a distribui√ß√£o amostral dos estimadores*, sendo  que valores muito altos de condi√ß√£o  levam a grande variabilidade e instabilidade das estimativas de par√¢metros. As pressuposi√ß√µes assumidas para o res√≠duo, bem como a estrutura da matrix X, tem uma forte influencia na qualidade do resultado final.

### Conclus√£o

A solu√ß√£o de m√≠nimos quadrados $B = (X^TX)^{-1}X^TY$ √© uma das ferramentas fundamentais em modelos de regress√£o linear m√∫ltipla [^25]. Sua deriva√ß√£o envolve a minimiza√ß√£o da soma dos quadrados dos res√≠duos, levando a uma express√£o expl√≠cita para a matriz de coeficientes. A compreens√£o da sua interpreta√ß√£o, das condi√ß√µes para a sua exist√™ncia e do seu comportamento sob as premissas do modelo s√£o essenciais para uma an√°lise completa e rigorosa da regress√£o linear m√∫ltipla e do entendimento dos seus resultados [^26].

### Refer√™ncias
[^1]: "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,..., Xp." *(Trecho de Linear Methods for Regression)*
[^2]: "Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them." *(Trecho de Linear Methods for Regression)*
[^3]: "In this chapter we describe linear methods for regression..." *(Trecho de Linear Methods for Regression)*
[^4]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^5]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Regression Models and Least Squares)*
[^6]: "The linear model has the form f(x) = Œ≤0 + Œ£j=1 pXjŒ≤j." *(Trecho de Linear Regression Models and Least Squares)*
[^7]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their population." *(Trecho de Linear Regression Models and Least Squares)*
[^8]: "Even if the xi's were not drawn randomly, the criterion is still valid if the yi's are conditionally independent given the inputs xi." *(Trecho de Linear Regression Models and Least Squares)*
[^9]: "Figure 3.1 illustrates the geometry of least-squares fitting in the IRp+1-dimensional space occupied by the pairs (X, Y)." *(Trecho de Linear Regression Models and Least Squares)*
[^10]: "Note that (3.2) makes no assumptions about the validity of model (3.1); it simply finds the best linear fit to the data." *(Trecho de Linear Regression Models and Least Squares)*
[^11]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Regression Models and Least Squares)*
[^12]: "How do we minimize (3.2)? Denote by X the N x (p + 1) matrix with each row an input vector (with a 1 in the first position), and similarly let y be the N-vector of outputs in the training set." *(Trecho de Linear Regression Models and Least Squares)*
[^13]: "Then we can write the residual sum-of-squares as RSS(Œ≤) = (y - XŒ≤)T(y - XŒ≤)." *(Trecho de Linear Regression Models and Least Squares)*
[^14]: "This is a quadratic function in the p + 1 parameters. Differentiating with respect to Œ≤ we obtain" *(Trecho de Linear Regression Models and Least Squares)*
[^15]: "Assuming (for the moment) that X has full column rank, and hence XTX is positive definite, we set the first derivative to zero XTY - XTXŒ≤ = 0." *(Trecho de Linear Regression Models and Least Squares)*
[^16]: "To obtain the unique solution Œ≤ = (XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^17]: "The predicted values at an input vector x0 are given by f(x0) = (1 x0)TŒ≤; the fitted values at the training inputs are ≈∑ = XŒ≤ = X(XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^18]: "The matrix H = X(XTX)-1XT appearing in equation (3.7) is sometimes called the ‚Äúhat‚Äù matrix because it puts the hat on y." *(Trecho de Linear Regression Models and Least Squares)*
[^19]: "Figure 3.2 shows a different geometrical representation of the least squares estimate, this time in IRN." *(Trecho de Linear Regression Models and Least Squares)*
[^20]: "We denote the column vectors of X by x0, x1,..., xp, with x0 = 1. For much of what follows, this first column is treated like any other. These vectors span a subspace of IRN, also referred to as the column space of X." *(Trecho de Linear Regression Models and Least Squares)*
[^21]: "We minimize RSS(Œ≤) = ||y - XŒ≤||2 by choosing Œ≤ so that the residual vector y - ≈∑ is orthogonal to this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^22]: "This orthogonality is expressed in (3.5), and the resulting estimate ≈∑ is hence the orthogonal pro- jection of y onto this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^23]: "The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix." *(Trecho de Linear Regression Models and Least Squares)*
[^24]: "The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion." *(Trecho de Linear Regression Models and Least Squares)*
[^25]: "There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X." *(Trecho de Linear Regression Models and Least Squares)*
[^26]: "Up to now we have made minimal assumptions about the true distribution of the data." *(Trecho de Linear Regression Models and Least Squares)*
