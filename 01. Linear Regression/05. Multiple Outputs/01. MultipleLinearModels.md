## Modelos Lineares M√∫ltiplos: $Y = XB + E$

```mermaid
flowchart LR
    Y("Matriz de Respostas (Y)")
    X("Matriz de Preditores (X)")
    B("Matriz de Coeficientes (B)")
    E("Matriz de Erros (E)")
    
    X --> B
    B --> |Multiplica√ß√£o Matricial| XB
    XB --> |Soma| Y
    E --> |Soma| Y
    
    style Y fill:#f9f,stroke:#333,stroke-width:2px
    style X fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#9f9,stroke:#333,stroke-width:2px
    style E fill:#ffc,stroke:#333,stroke-width:2px

```

### Introdu√ß√£o

Os **modelos lineares m√∫ltiplos** s√£o uma extens√£o dos modelos de regress√£o linear simples, permitindo modelar m√∫ltiplas vari√°veis respostas em fun√ß√£o de m√∫ltiplos preditores [^1]. A representa√ß√£o matricial $Y = XB + E$ oferece uma forma concisa e elegante de descrever esses modelos, onde $Y$ √© a matriz de respostas, $X$ √© a matriz de preditores, $B$ √© a matriz de coeficientes e $E$ √© a matriz de erros [^2]. Nesta se√ß√£o, vamos explorar essa representa√ß√£o matricial em detalhes, analisando sua estrutura e as implica√ß√µes para a an√°lise e solu√ß√£o de modelos de regress√£o linear m√∫ltipla.

### Formula√ß√£o Matricial do Modelo Linear M√∫ltiplo

Para modelar m√∫ltiplas respostas $Y_1, Y_2, ..., Y_k$ a partir de m√∫ltiplos preditores $X_0, X_1, ..., X_p$, onde $X_0 = 1$ representa o intercepto, assumimos um modelo linear para cada resposta [^4]:

$$
\begin{aligned}
    Y_1 &= \beta_{01} + X_1\beta_{11} + X_2\beta_{21} + \ldots + X_p\beta_{p1} + \epsilon_1 \\
    Y_2 &= \beta_{02} + X_1\beta_{12} + X_2\beta_{22} + \ldots + X_p\beta_{p2} + \epsilon_2 \\
    &\ldots \\
    Y_k &= \beta_{0k} + X_1\beta_{1k} + X_2\beta_{2k} + \ldots + X_p\beta_{pk} + \epsilon_k
\end{aligned}
$$
onde $\beta_{jk}$ s√£o os coeficientes e $\epsilon_k$ s√£o os erros [^5]. Este sistema de equa√ß√µes pode ser escrito de forma concisa em nota√ß√£o matricial como [^6]:

$$ Y = XB + E $$

Onde:
- $Y$ √© uma matriz $N \times K$ de respostas, com cada coluna representando uma das $K$ respostas [^7]. Cada elemento $y_{ik}$ da matriz $Y$ representa a observa√ß√£o $i$ da resposta $k$.
- $X$ √© uma matriz $N \times (p+1)$ de preditores, incluindo uma coluna de 1 para o intercepto, com cada coluna representando um preditor, e $N$ representando o n√∫mero de observa√ß√µes. Cada elemento $x_{ij}$ da matriz $X$ representa a observa√ß√£o $i$ do preditor $j$ [^8].
- $B$ √© uma matriz $(p+1) \times K$ de coeficientes, com cada coluna representando os coeficientes para a predi√ß√£o de uma das $K$ respostas [^9]. Cada elemento $b_{jk}$ da matriz $B$ representa o efeito do preditor $j$ na resposta $k$.
- $E$ √© uma matriz $N \times K$ de erros, onde $\epsilon_{ik}$ representa o erro associado √† observa√ß√£o $i$ da resposta $k$ [^10].

**Lemma 1:** *A formula√ß√£o matricial $Y = XB + E$ representa um sistema conciso de $N \times K$ equa√ß√µes, onde cada equa√ß√£o descreve a rela√ß√£o linear entre os preditores $X$ e uma das $K$ vari√°veis respostas $Y_k$* [^11].

**Prova do Lemma 1:** Cada elemento $y_{ik}$ da matriz $Y$ √© igual √† soma dos produtos de elementos correspondentes das linhas $i$ de $X$ e as colunas $k$ de $B$ somado ao erro $\epsilon_{ik}$.
$$ y_{ik} = \sum_{j=0}^{p} x_{ij} b_{jk} + \epsilon_{ik}$$
Esta equa√ß√£o representa a linearidade do modelo na forma matricial. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos 3 observa√ß√µes (N=3), 2 preditores (p=2) e 2 respostas (K=2). A matriz X incluir√° uma coluna de 1s para o intercepto. As matrizes $X$, $Y$, $B$, e $E$ podem ser representadas como:
>
> $$
> X = \begin{bmatrix}
> 1 & 2 & 3 \\
> 1 & 4 & 5 \\
> 1 & 6 & 7
> \end{bmatrix}
> $$
>
> $$
> Y = \begin{bmatrix}
> 5 & 10 \\
> 7 & 12 \\
> 9 & 14
> \end{bmatrix}
> $$
>
> $$
> B = \begin{bmatrix}
> \beta_{01} & \beta_{02} \\
> \beta_{11} & \beta_{12} \\
> \beta_{21} & \beta_{22}
> \end{bmatrix}
> $$
>
> $$
> E = \begin{bmatrix}
> \epsilon_{11} & \epsilon_{12} \\
> \epsilon_{21} & \epsilon_{22} \\
> \epsilon_{31} & \epsilon_{32}
> \end{bmatrix}
> $$
>
>  A primeira linha da matriz Y, por exemplo, √© obtida por:
>
>  $y_{11} = 1*\beta_{01} + 2*\beta_{11} + 3*\beta_{21} + \epsilon_{11} = 5$
>
>  $y_{12} = 1*\beta_{02} + 2*\beta_{12} + 3*\beta_{22} + \epsilon_{12} = 10$
>
>  Este exemplo ilustra como cada linha de Y √© uma combina√ß√£o linear das colunas de X, ponderada pelos coeficientes em B, mais um termo de erro.

### Solu√ß√£o de M√≠nimos Quadrados em Modelos Lineares M√∫ltiplos

O objetivo em modelos lineares m√∫ltiplos √© encontrar a matriz de coeficientes $B$ que minimize a soma dos quadrados dos res√≠duos (RSS) para todas as respostas [^12]. Isso pode ser expresso matematicamente como:

$$ RSS(B) = \sum_{k=1}^{K} \sum_{i=1}^{N} (y_{ik} - f_k(x_i))^2 $$

onde $f_k(x_i)$ √© a resposta predita para a observa√ß√£o $i$ da resposta $k$ pelo modelo linear [^13]. Em nota√ß√£o matricial, o RSS √© dado por:
$$ RSS(B) = tr[(Y - XB)^T (Y - XB)] $$

onde $tr$ denota o tra√ßo da matriz [^14].

**Lemma 2:** *A solu√ß√£o de m√≠nimos quadrados para a matriz de coeficientes B √© dada por $ B = (X^TX)^{-1}X^TY$, assuming que $(X^TX)$ √© invert√≠vel* [^15].

**Prova do Lemma 2:** Para encontrar a matriz B que minimize o RSS, derivamos a equa√ß√£o acima com rela√ß√£o a B e igualamos a zero:
$$ \frac{\partial RSS(B)}{\partial B} = -2X^T(Y - XB) = 0 $$
Isso implica que $X^T(Y - XB) = 0$. Se a matriz $X^TX$ √© invert√≠vel, multiplicando ambos os lados por $(X^TX)^{-1}$ temos:
$$ (X^TX)^{-1} X^T(Y - XB) = 0 \Rightarrow (X^TX)^{-1} X^TY - (X^TX)^{-1} X^TXB = 0 \Rightarrow B = (X^TX)^{-1} X^TY  $$
$\blacksquare$

```mermaid
flowchart LR
    X("Matriz de Preditores (X)")
    Y("Matriz de Respostas (Y)")
    XT("X Transposta (X^T)")
    XTX("X^T * X")
    XTX_inv("(X^TX)^-1")
    XTY("X^T * Y")
    B("Matriz de Coeficientes (B)")
    
    X --> XT
    XT --> XTX
    XTX --> XTX_inv
    XT --> XTY
    Y --> XTY
    XTX_inv --> B
    XTY --> B
    
    style X fill:#ccf,stroke:#333,stroke-width:2px
    style Y fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#9f9,stroke:#333,stroke-width:2px
    style XT fill:#ddd,stroke:#333,stroke-width:2px
    style XTX fill:#ddd,stroke:#333,stroke-width:2px
    style XTX_inv fill:#ddd,stroke:#333,stroke-width:2px
    style XTY fill:#ddd,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o exemplo num√©rico anterior para calcular a matriz de coeficientes $B$. Para isso, vamos gerar os dados em Python usando numpy e calcular $B$ usando a formula√ß√£o de m√≠nimos quadrados.
> ```python
> import numpy as np
>
> # Matrizes X e Y do exemplo anterior
> X = np.array([[1, 2, 3],
>               [1, 4, 5],
>               [1, 6, 7]])
> Y = np.array([[5, 10],
>               [7, 12],
>               [9, 14]])
>
> # C√°lculo de B usando a f√≥rmula de m√≠nimos quadrados
> X_transpose = np.transpose(X)
> XTX = np.dot(X_transpose, X)
> XTX_inverse = np.linalg.inv(XTX)
> XTY = np.dot(X_transpose, Y)
> B = np.dot(XTX_inverse, XTY)
>
> print("Matriz de coeficientes B estimada:")
> print(B)
> ```
>
> O c√≥digo acima calcula $B$ usando a f√≥rmula $B = (X^TX)^{-1}X^TY$. O resultado impresso no console ser√° a matriz de coeficientes $B$ estimada pelo m√©todo de m√≠nimos quadrados.
>
> **Interpreta√ß√£o:** Cada coluna da matriz $B$ representa os coeficientes do modelo linear para cada vari√°vel resposta. Por exemplo, a primeira coluna de $B$ s√£o os coeficientes para prever $Y_1$ e a segunda coluna s√£o os coeficientes para prever $Y_2$.

**Corol√°rio 1:** *A solu√ß√£o de m√≠nimos quadrados para cada coluna de B, isto √©, cada vetor de coeficientes para cada vari√°vel resposta, √© equivalente √† solu√ß√£o de m√≠nimos quadrados para um problema de regress√£o linear com uma √∫nica vari√°vel resposta* [^16]. Isso significa que, neste caso, podemos resolver cada problema de regress√£o separadamente.

**Prova do Corol√°rio 1:** Da Lemma 2, sabemos que $ B = (X^TX)^{-1}X^TY$.  A matrix $Y$ pode ser particionada em colunas $Y=[y_1, y_2, ..., y_k]$ e  $B = [b_1, b_2,..., b_k]$. Assim
$b_k = (X^TX)^{-1}X^T y_k$ , onde $b_k$ √© a coluna $k$ de $B$ e $y_k$ √© a coluna $k$ de $Y$, o que prova que cada coluna de $B$ √© a solu√ß√£o de m√≠nimos quadrados para a regress√£o de cada resposta individual [^17]. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar o Corol√°rio 1, vamos calcular os coeficientes de $B$ para cada resposta separadamente e comparar com o resultado anterior.
>
> ```python
> import numpy as np
>
> # Matrizes X e Y do exemplo anterior
> X = np.array([[1, 2, 3],
>               [1, 4, 5],
>               [1, 6, 7]])
> Y = np.array([[5, 10],
>               [7, 12],
>               [9, 14]])
>
> # C√°lculo de B para cada resposta separadamente
> X_transpose = np.transpose(X)
> XTX = np.dot(X_transpose, X)
> XTX_inverse = np.linalg.inv(XTX)
>
> # Calculando para a primeira resposta (Y1)
> Y1 = Y[:, 0] # Primeira coluna de Y
> B1 = np.dot(XTX_inverse, np.dot(X_transpose, Y1))
>
> # Calculando para a segunda resposta (Y2)
> Y2 = Y[:, 1] # Segunda coluna de Y
> B2 = np.dot(XTX_inverse, np.dot(X_transpose, Y2))
>
> # Imprimindo resultados
> print("Coeficientes B para Y1:", B1)
> print("Coeficientes B para Y2:", B2)
>
> # Comparando com o resultado anterior
> B_total = np.dot(XTX_inverse, np.dot(X_transpose, Y))
> print("Matriz B completa:", B_total)
> ```
>
> Este c√≥digo calcula $B_1$ (coeficientes para $Y_1$) e $B_2$ (coeficientes para $Y_2$) separadamente e os compara com a matriz $B$ calculada anteriormente. Os resultados ser√£o id√™nticos, confirmando o Corol√°rio 1.
>
> **Interpreta√ß√£o:**  Isso demonstra que o c√°lculo da matriz de coeficientes $B$ para m√∫ltiplas respostas, quando os erros s√£o independentes, √© equivalente a calcular os coeficientes para cada resposta individualmente.

### Interpreta√ß√µes e Considera√ß√µes sobre o Modelo Matricial

O modelo $Y = XB + E$ apresenta diversas vantagens em rela√ß√£o √† representa√ß√£o escalar, como [^18]:
1. **Concis√£o:** A nota√ß√£o matricial expressa m√∫ltiplos modelos lineares de maneira compacta, simplificando a apresenta√ß√£o e an√°lise de sistemas complexos [^19].
2. **Generalidade:** A formula√ß√£o matricial pode ser aplicada a qualquer n√∫mero de preditores e respostas, tornando-se uma ferramenta vers√°til para diferentes cen√°rios [^20].
3. **Efici√™ncia Computacional:** Os c√°lculos e algoritmos da regress√£o linear m√∫ltipla podem ser eficientemente expressos e implementados utilizando opera√ß√µes matriciais, o que facilita o uso de bibliotecas computacionais otimizadas [^21].
4. **Entendimento da Estrutura:** A nota√ß√£o matricial permite melhor visualiza√ß√£o e entendimento da estrutura do modelo linear e de suas propriedades geom√©tricas [^22].

**Conceito 2: Independ√™ncia das Respostas**

Uma observa√ß√£o crucial no contexto fornecido √© que, *na aus√™ncia de erros correlacionados entre as respostas, a estimativa de cada coluna de B (isto √©, cada conjunto de coeficientes para cada resposta) pode ser realizada independentemente das outras colunas* [^23]. Isso simplifica o c√°lculo de $B$, mas o modelo pode se tornar ineficiente em casos onde h√° correla√ß√£o entre os erros das diferentes respostas [^24].

### Considera√ß√µes sobre a Generaliza√ß√£o para M√∫ltiplas Sa√≠das

O contexto menciona que "Multiple outputs do not affect one another's least squares estimates", o que √© uma consequ√™ncia da independ√™ncia dos erros entre as diferentes respostas. *Isso simplifica o problema de estima√ß√£o porque permite resolver problemas de regress√£o separados para cada resposta*. No entanto, a independ√™ncia dos erros √© uma premissa forte, e quando essa premissa √© violada, ou seja, quando os erros entre as diferentes respostas s√£o correlacionados, m√©todos mais sofisticados que consideram essas correla√ß√µes podem ser necess√°rios [^25].

A formula√ß√£o do problema de m√≠nimos quadrados para m√∫ltiplas respostas, como expresso no contexto, √© a soma das somas dos quadrados dos res√≠duos para cada resposta. Isso leva √† solu√ß√£o descrita acima que n√£o utiliza correla√ß√µes entre as respostas. √â importante salientar que, conforme descrito no contexto, se os erros entre as respostas s√£o correlacionados, isso pode levar a um modelo sub-√≥timo.

### Pergunta Te√≥rica Avan√ßada: Como a Suposi√ß√£o de Erros Independentes em Modelos de Respostas M√∫ltiplas Afeta a Estabilidade e a Efici√™ncia das Estimativas de Par√¢metros?

**Resposta:**

A suposi√ß√£o de erros independentes entre as diferentes respostas em modelos lineares m√∫ltiplos simplifica significativamente o processo de estima√ß√£o dos par√¢metros, mas pode levar a perdas de efici√™ncia e estabilidade se esta suposi√ß√£o n√£o for v√°lida [^26].
Quando os erros s√£o independentes, o RSS pode ser calculado como a soma das somas dos quadrados dos res√≠duos para cada resposta separadamente, o que leva √† solu√ß√£o de m√≠nimos quadrados separada para cada resposta. Esta abordagem ignora a possibilidade de informa√ß√µes conjuntas sobre os par√¢metros das diferentes respostas, o que pode ser sub√≥timo quando os erros s√£o correlacionados [^27].

*Se os erros s√£o correlacionados, as estimativas de m√≠nimos quadrados, que consideram as respostas como independentes, podem n√£o ser t√£o eficientes (isto √©, podem ter maior vari√¢ncia) quanto as estimativas que levam em considera√ß√£o as correla√ß√µes entre os erros* [^28]. Por exemplo, em situa√ß√µes onde as vari√°veis respostas compartilham informa√ß√µes, ou s√£o influenciadas por fatores n√£o observados comuns, a ignorar essas correla√ß√µes leva a uma perda de precis√£o na estima√ß√£o dos par√¢metros.
A presen√ßa de erros correlacionados tamb√©m pode levar a estimativas de par√¢metros com vi√©s, na medida em que a correla√ß√£o afeta a estrutura de covari√¢ncia dos dados [^29]. Nesses casos, abordagens como modelos de equa√ß√µes estruturais, ou outras t√©cnicas multivariadas que consideram a correla√ß√£o entre as respostas, podem levar a resultados mais precisos e confi√°veis, conforme apontado no contexto [^30]. A escolha entre modelagem separada e conjunta depende, ent√£o, da validade da premissa da independ√™ncia dos erros, e as ferramentas estat√≠sticas devem ser escolhidas com base nos dados e na natureza do problema a ser modelado.

### Conclus√£o

A formula√ß√£o matricial $Y=XB+E$ fornece uma maneira poderosa e concisa de representar modelos lineares m√∫ltiplos [^31]. Ela destaca a estrutura linear do modelo, simplifica a deriva√ß√£o da solu√ß√£o de m√≠nimos quadrados e facilita a aplica√ß√£o de algoritmos computacionais eficientes [^32]. A compreens√£o das implica√ß√µes dessa formula√ß√£o, incluindo a import√¢ncia da independ√™ncia entre os erros e a possibilidade de generaliza√ß√£o para o caso multivariado, √© crucial para o desenvolvimento e an√°lise de modelos de regress√£o linear avan√ßados [^33].

### Refer√™ncias

[^1]: "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,..., Xp." *(Trecho de Linear Methods for Regression)*
[^2]: "Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them." *(Trecho de Linear Methods for Regression)*
[^3]: "In this chapter we describe linear methods for regression..." *(Trecho de Linear Methods for Regression)*
[^4]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^5]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Regression Models and Least Squares)*
[^6]: "The linear model has the form f(x) = Œ≤0 + Œ£j=1 pXjŒ≤j." *(Trecho de Linear Regression Models and Least Squares)*
[^7]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their population." *(Trecho de Linear Regression Models and Least Squares)*
[^8]: "Even if the xi's were not drawn randomly, the criterion is still valid if the yi's are conditionally independent given the inputs xi." *(Trecho de Linear Regression Models and Least Squares)*
[^9]: "Figure 3.1 illustrates the geometry of least-squares fitting in the IRp+1-dimensional space occupied by the pairs (X, Y)." *(Trecho de Linear Regression Models and Least Squares)*
[^10]: "Note that (3.2) makes no assumptions about the validity of model (3.1); it simply finds the best linear fit to the data." *(Trecho de Linear Regression Models and Least Squares)*
[^11]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Regression Models and Least Squares)*
[^12]: "How do we minimize (3.2)? Denote by X the N x (p + 1) matrix with each row an input vector (with a 1 in the first position), and similarly let y be the N-vector of outputs in the training set." *(Trecho de Linear Regression Models and Least Squares)*
[^13]: "Then we can write the residual sum-of-squares as RSS(Œ≤) = (y - XŒ≤)T(y - XŒ≤)." *(Trecho de Linear Regression Models and Least Squares)*
[^14]: "This is a quadratic function in the p + 1 parameters. Differentiating with respect to Œ≤ we obtain" *(Trecho de Linear Regression Models and Least Squares)*
[^15]: "Assuming (for the moment) that X has full column rank, and hence XTX is positive definite, we set the first derivative to zero XTY - XTXŒ≤ = 0." *(Trecho de Linear Regression Models and Least Squares)*
[^16]: "To obtain the unique solution Œ≤ = (XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^17]: "The predicted values at an input vector x0 are given by f(x0) = (1 x0)TŒ≤; the fitted values at the training inputs are ≈∑ = XŒ≤ = X(XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^18]: "The matrix H = X(XTX)-1XT appearing in equation (3.7) is sometimes called the ‚Äúhat‚Äù matrix because it puts the hat on y." *(Trecho de Linear Regression Models and Least Squares)*
[^19]: "Figure 3.2 shows a different geometrical representation of the least squares estimate, this time in IRN." *(Trecho de Linear Regression Models and Least Squares)*
[^20]: "We denote the column vectors of X by x0, x1,..., xp, with x0 = 1. For much of what follows, this first column is treated like any other. These vectors span a subspace of IRN, also referred to as the column space of X." *(Trecho de Linear Regression Models and Least Squares)*
[^21]: "We minimize RSS(Œ≤) = ||y - XŒ≤||2 by choosing Œ≤ so that the residual vector y - ≈∑ is orthogonal to this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^22]: "This orthogonality is expressed in (3.5), and the resulting estimate ≈∑ is hence the orthogonal pro- jection of y onto this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^23]: "The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix." *(Trecho de Linear Regression Models and Least Squares)*
[^24]: "The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion." *(Trecho de Linear Regression Models and Least Squares)*
[^25]: "There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X." *(Trecho de Linear Regression Models and Least Squares)*
[^26]: "Up to now we have made minimal assumptions about the true distribution of the data." *(Trecho de Linear Regression Models and Least Squares)*
[^27]: "In order to pin down the sampling properties of Œ≤, we now assume that the observations yi are uncorrelated and have constant variance œÉ¬≤, and that the xi are fixed (non random)." *(Trecho de Linear Regression Models and Least Squares)*
[^28]: "The variance-covariance matrix of the least squares parameter estimates is easily derived from (3.6) and is given by Var(Œ≤) = (XTX)-1œÉ2." *(Trecho de Linear Regression Models and Least Squares)*
[^29]: "Typically one estimates the variance œÉ¬≤ by √¥¬≤ = (1/(N-p-1)) Œ£i(Yi-≈∂i)¬≤." *(Trecho de Linear Regression Models and Least Squares)*
[^30]: "To test the hypothesis that a particular coefficient Œ≤j = 0, we form the standardized coefficient or Z-score Zj = Œ≤j /√¥‚àövj, where vj is the jth diagonal element of (XTX)-1." *(Trecho de Linear Regression Models and Least Squares)*
[^31]: "Under the null hypothesis that Œ≤j = 0, zj is distributed as tN-p-1 (a t distribution with N ‚Äì p ‚Äì 1 degrees of freedom)" *(Trecho de Linear Regression Models and Least Squares)*
[^32]: "Often we need to test for the significance of groups of coefficients simultaneously." *(Trecho de Linear Regression Models and Least Squares)*
[^33]: "For example, to test if a categorical variable with k levels can be excluded from a model, we need to test whether the coefficients of the dummy variables used to represent the levels can all be set to zero." *(Trecho de Linear Regression Models and Least Squares)*
