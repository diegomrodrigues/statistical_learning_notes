## Crit√©rio Multivariado Ponderado: Considera√ß√£o da Correla√ß√£o de Erros ($\Sigma$) e seu Resultado em Regress√µes Independentes

```mermaid
flowchart LR
    A[Regress√£o Multivariada Padr√£o] --> B{Erros n√£o correlacionados?};
    B -- Sim --> C[Regress√µes Independentes];
    B -- N√£o --> D[Crit√©rio Multivariado Ponderado];
    D --> E{Considerar $\Sigma$?};
    E -- Sim --> F[Estimativa com $\Sigma$];
    E -- N√£o --> C;
    F --> G[Resultados Otimizados];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Em modelos de regress√£o linear com m√∫ltiplas vari√°veis respostas, a suposi√ß√£o de que os erros s√£o n√£o correlacionados √© frequentemente feita para simplificar a an√°lise e estima√ß√£o dos par√¢metros [^1]. No entanto, em muitas aplica√ß√µes pr√°ticas, os erros entre as diferentes respostas podem ser correlacionados, o que pode levar a resultados sub-√≥timos se a correla√ß√£o for ignorada [^2]. O **crit√©rio multivariado ponderado** surge como uma alternativa para lidar com essa quest√£o, buscando levar em considera√ß√£o a estrutura de covari√¢ncia dos erros (representada por $\Sigma$) [^3]. Nesta se√ß√£o, exploraremos este crit√©rio, analisando as condi√ß√µes sob as quais ele leva a resultados equivalentes a regress√µes independentes e as suas implica√ß√µes para modelos com erros correlacionados.

### Formula√ß√£o do Crit√©rio Multivariado Ponderado

O crit√©rio multivariado ponderado surge da teoria gaussiana multivariada, onde a fun√ß√£o de perda √© definida como [^5]:

$$ RSS(B; \Sigma) = \sum_{i=1}^N (y_i - f(x_i))^T \Sigma^{-1} (y_i - f(x_i)) $$
onde:

-   $y_i$ √© o vetor de respostas para a observa√ß√£o $i$, isto √©,  $y_i = [y_{i1}, y_{i2}, \ldots, y_{iK}]^T$.
-   $f(x_i)$ √© o vetor de respostas preditas para a observa√ß√£o $i$,  $f(x_i) =  [f_1(x_i), f_2(x_i), \ldots, f_K(x_i)]^T$, onde cada  $f_k(x_i)$  √© a previs√£o da resposta k.
-   $\Sigma$ √© a matriz de covari√¢ncia entre as respostas.
-   $N$ √© o n√∫mero de observa√ß√µes [^6].

Em nota√ß√£o matricial, o crit√©rio ponderado pode ser expresso como [^7]:

$$ RSS(B; \Sigma) = \sum_{i=1}^N (y_i - f(x_i))^T \Sigma^{-1} (y_i - f(x_i)) = tr[(Y - XB)^T \Sigma^{-1} (Y - XB)] $$

onde $tr$ denota o tra√ßo da matriz e $Y$ e $XB$ s√£o as matrizes de respostas observadas e preditas respectivamente. O termo $\Sigma^{-1}$ pondera os res√≠duos de acordo com a estrutura de covari√¢ncia dos erros [^8].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um modelo com duas vari√°veis de resposta ($K=2$) e duas observa√ß√µes ($N=2$). As respostas observadas s√£o:
>
> $Y = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$
>
> E as respostas preditas s√£o:
>
> $XB = \begin{bmatrix} 1.1 & 1.8 \\ 2.9 & 4.2 \end{bmatrix}$
>
> A matriz de covari√¢ncia dos erros √©:
>
> $\Sigma = \begin{bmatrix} 0.25 & 0.1 \\ 0.1 & 0.09 \end{bmatrix}$
>
> Calculando $\Sigma^{-1}$:
>
> $\Sigma^{-1} = \frac{1}{(0.25 * 0.09 - 0.1 * 0.1)} \begin{bmatrix} 0.09 & -0.1 \\ -0.1 & 0.25 \end{bmatrix} = \begin{bmatrix} 2.25 & -2.5 \\ -2.5 & 6.25 \end{bmatrix}$
>
> Os res√≠duos para cada observa√ß√£o s√£o:
>
> $e_1 = y_1 - f(x_1) = \begin{bmatrix} 1 - 1.1 \\ 2 - 1.8 \end{bmatrix} = \begin{bmatrix} -0.1 \\ 0.2 \end{bmatrix}$
>
> $e_2 = y_2 - f(x_2) = \begin{bmatrix} 3 - 2.9 \\ 4 - 4.2 \end{bmatrix} = \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix}$
>
> Agora, calculamos $(y_i - f(x_i))^T \Sigma^{-1} (y_i - f(x_i))$ para cada observa√ß√£o:
>
> $e_1^T \Sigma^{-1} e_1 = \begin{bmatrix} -0.1 & 0.2 \end{bmatrix} \begin{bmatrix} 2.25 & -2.5 \\ -2.5 & 6.25 \end{bmatrix} \begin{bmatrix} -0.1 \\ 0.2 \end{bmatrix} = \begin{bmatrix} -0.1 & 0.2 \end{bmatrix} \begin{bmatrix} -0.525 \\ 1.275 \end{bmatrix} = 0.3075$
>
> $e_2^T \Sigma^{-1} e_2 = \begin{bmatrix} 0.1 & -0.2 \end{bmatrix} \begin{bmatrix} 2.25 & -2.5 \\ -2.5 & 6.25 \end{bmatrix} \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix} = \begin{bmatrix} 0.1 & -0.2 \end{bmatrix} \begin{bmatrix} 0.525 \\ -1.275 \end{bmatrix} = 0.3075$
>
> Finalmente, o RSS ponderado √©:
>
> $RSS(B; \Sigma) = 0.3075 + 0.3075 = 0.615$
>
> Este exemplo ilustra como a matriz de covari√¢ncia inversa pondera os res√≠duos, influenciando o valor final do RSS.

**Lemma 1:** *O crit√©rio multivariado ponderado generaliza o crit√©rio de m√≠nimos quadrados padr√£o, considerando a estrutura de covari√¢ncia dos erros atrav√©s da matriz $\Sigma^{-1}$* [^9]. Quando a matriz de covari√¢ncia $\Sigma$ √© a matriz identidade multiplicada por uma constante, isto √©, quando os erros n√£o s√£o correlacionados e possuem a mesma vari√¢ncia, o crit√©rio ponderado se reduz ao crit√©rio de m√≠nimos quadrados padr√£o.

**Prova do Lemma 1:** Se $\Sigma$ for a matriz identidade multiplicada por uma constante, ou seja, $\Sigma = \sigma^2I$, ent√£o $\Sigma^{-1} = \frac{1}{\sigma^2} I$. Substituindo isso na formula√ß√£o do RSS ponderado, temos:

$$ RSS(B; \Sigma) = tr[(Y - XB)^T (\frac{1}{\sigma^2} I) (Y - XB)]  = \frac{1}{\sigma^2} tr[(Y - XB)^T (Y - XB)] $$

Como $1/\sigma^2$ √© uma constante, esta minimiza√ß√£o √© equivalente a minimiza√ß√£o do RSS padr√£o. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Consideremos o caso em que $\Sigma = 0.5 * I$, onde $I$ √© uma matriz identidade 2x2.
>
> $\Sigma = \begin{bmatrix} 0.5 & 0 \\ 0 & 0.5 \end{bmatrix}$
>
> Ent√£o, $\Sigma^{-1}$ √©:
>
> $\Sigma^{-1} = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}$
>
> Usando os mesmos res√≠duos do exemplo anterior:
>
> $e_1 = \begin{bmatrix} -0.1 \\ 0.2 \end{bmatrix}$
>
> $e_2 = \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix}$
>
> $e_1^T \Sigma^{-1} e_1 = \begin{bmatrix} -0.1 & 0.2 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} -0.1 \\ 0.2 \end{bmatrix} = \begin{bmatrix} -0.1 & 0.2 \end{bmatrix} \begin{bmatrix} -0.2 \\ 0.4 \end{bmatrix} = 0.02 + 0.08 = 0.1$
>
> $e_2^T \Sigma^{-1} e_2 = \begin{bmatrix} 0.1 & -0.2 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix} = \begin{bmatrix} 0.1 & -0.2 \end{bmatrix} \begin{bmatrix} 0.2 \\ -0.4 \end{bmatrix} = 0.02 + 0.08 = 0.1$
>
> $RSS(B; \Sigma) = 0.1 + 0.1 = 0.2$
>
> Note que esse resultado √© equivalente a  $\frac{1}{0.5} * RSS_{OLS} = 2 * 0.1 = 0.2$ onde $RSS_{OLS} =  (-0.1)^2 + (0.2)^2 + (0.1)^2 + (-0.2)^2 = 0.1$. Isso demonstra que com $\Sigma = \sigma^2I$, o crit√©rio ponderado se reduz ao crit√©rio de m√≠nimos quadrados padr√£o multiplicado por uma constante.

### Condi√ß√£o para Regress√µes Independentes no Crit√©rio Ponderado

Conforme apontado no contexto [^10], a solu√ß√£o do crit√©rio de m√≠nimos quadrados multivariado ponderado (quando $\Sigma$ √© uma matriz geral) *n√£o √© dada por solu√ß√µes separadas para cada vari√°vel resposta*. No entanto, quando a matriz de covari√¢ncia de erros $\Sigma$ √© diagonal, a solu√ß√£o se simplifica para a resolu√ß√£o de regress√µes lineares independentes para cada vari√°vel resposta [^11].

**Lemma 2:** *Se a matriz de covari√¢ncia $\Sigma$ √© diagonal, a solu√ß√£o do crit√©rio ponderado √© equivalente √† solu√ß√£o de m√≠nimos quadrados separadas para cada vari√°vel resposta*.

**Prova do Lemma 2:**
Se $\Sigma$ √© diagonal, ent√£o $\Sigma^{-1}$ tamb√©m √© diagonal e sua matriz de entradas s√£o $ \frac{1}{\sigma_i^2}$, onde $\sigma_i^2$ √© a vari√¢ncia dos erros da vari√°vel resposta i. Substituindo isto na formula√ß√£o de RSS ponderado, temos
$$ RSS(B; \Sigma) = \sum_{i=1}^N (y_i - f(x_i))^T \Sigma^{-1} (y_i - f(x_i)) = \sum_{i=1}^N \sum_{k=1}^K \frac{(y_{ik} - f_k(x_i))^2}{\sigma_k^2} $$

Ao derivar o RSS ponderado com rela√ß√£o a B e igualar a zero, e devido a diagonalidade da matriz de covari√¢ncia, as equa√ß√µes para as estimativas de cada coluna de B n√£o se misturam, e a solu√ß√£o se reduz √† resolu√ß√£o de uma sequ√™ncia de regress√µes de m√≠nimos quadrados separadas para cada resposta, onde a √∫nica diferen√ßa √© que cada resposta √© ponderada por  $1/\sigma_k$.  $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere uma matriz de covari√¢ncia diagonal:
>
> $\Sigma = \begin{bmatrix} 0.25 & 0 \\ 0 & 0.09 \end{bmatrix}$
>
> Ent√£o, $\Sigma^{-1}$ √©:
>
> $\Sigma^{-1} = \begin{bmatrix} 4 & 0 \\ 0 & 11.11 \end{bmatrix}$ (aproximadamente)
>
> Usando os mesmos res√≠duos anteriores:
>
> $e_1 = \begin{bmatrix} -0.1 \\ 0.2 \end{bmatrix}$
>
> $e_2 = \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix}$
>
> $e_1^T \Sigma^{-1} e_1 = \begin{bmatrix} -0.1 & 0.2 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 11.11 \end{bmatrix} \begin{bmatrix} -0.1 \\ 0.2 \end{bmatrix} = \begin{bmatrix} -0.1 & 0.2 \end{bmatrix} \begin{bmatrix} -0.4 \\ 2.222 \end{bmatrix} = 0.04 + 0.4444 = 0.4844$
>
> $e_2^T \Sigma^{-1} e_2 = \begin{bmatrix} 0.1 & -0.2 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 11.11 \end{bmatrix} \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix} = \begin{bmatrix} 0.1 & -0.2 \end{bmatrix} \begin{bmatrix} 0.4 \\ -2.222 \end{bmatrix} = 0.04 + 0.4444 = 0.4844$
>
> $RSS(B; \Sigma) = 0.4844 + 0.4844 = 0.9688$
>
> Aqui, o RSS ponderado √© a soma dos res√≠duos quadrados ponderados pelas vari√¢ncias individuais de cada resposta. O processo de minimiza√ß√£o neste caso, equivale a minimizar o RSS de cada resposta individualmente. Ou seja, o problema se separa em duas regress√µes lineares independentes, uma para cada vari√°vel resposta.

```mermaid
flowchart LR
    A[Matriz $\Sigma$] --> B{Diagonal?};
    B -- Sim --> C[Regress√µes Independentes];
    B -- N√£o --> D[Crit√©rio Ponderado Multivariado];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

Em outras palavras, quando a matriz $\Sigma$ √© diagonal, os erros entre as diferentes respostas s√£o independentes, e o crit√©rio ponderado se reduz ao crit√©rio padr√£o de m√≠nimos quadrados para cada resposta separadamente.  Neste caso, a solu√ß√£o para a matriz de coeficientes B √© dada por:
$$ B = (X^TX)^{-1} X^T Y $$
que corresponde √† solu√ß√£o de m√∫ltiplos problemas de m√≠nimos quadrados separados para cada vari√°vel resposta, conforme discutido anteriormente [^12].

### Implica√ß√µes da Correla√ß√£o nos Erros para as Estimativas

A principal implica√ß√£o da independ√™ncia entre as vari√°veis respostas √© que a matriz de coeficientes $B$ pode ser estimada por meio da resolu√ß√£o de $K$ problemas de regress√£o independentes.  Contudo, quando existe correla√ß√£o entre as respostas, a estimativa de B deve ser realizada levando essa correla√ß√£o em considera√ß√£o, ou seja, usando a fun√ß√£o de perda com matriz de covari√¢ncia geral [^13].

**Corol√°rio 2:** *Quando a matriz $\Sigma$ n√£o √© diagonal, a estimativa da matriz de coeficientes B n√£o √© equivalente √† estimativa de cada coluna de B usando regress√µes separadas*. Nestes casos, a correla√ß√£o entre os erros deve ser considerada para obter estimativas mais eficientes [^14].

Quando $\Sigma$ n√£o √© uma matriz diagonal, o crit√©rio ponderado leva a solu√ß√µes diferentes das encontradas atrav√©s de regress√µes separadas para cada vari√°vel resposta, como foi destacado no contexto [^15]. A intui√ß√£o por tr√°s deste resultado √© que, quando os erros entre as respostas s√£o correlacionados, a informa√ß√£o contida em cada resposta sobre a estrutura do modelo (isto √©, sobre os par√¢metros) tamb√©m est√° correlacionada, e utilizar essa informa√ß√£o conjunta pode levar a estimativas mais precisas [^16].

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar, vamos comparar as estimativas de um modelo com duas vari√°veis respostas correlacionadas usando o crit√©rio ponderado com uma matriz de covari√¢ncia geral e as estimativas obtidas por regress√µes independentes.
>
> Suponha que temos 100 observa√ß√µes e uma √∫nica vari√°vel preditora. Vamos gerar alguns dados simulados com erros correlacionados:
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.linear_model import LinearRegression
>
> np.random.seed(42)
> n_samples = 100
> X = np.random.rand(n_samples, 1)
>
> # Matriz de covari√¢ncia dos erros (n√£o diagonal)
> Sigma = np.array([[0.25, 0.15], [0.15, 0.25]])
>
> # Gerar erros correlacionados
> errors = np.random.multivariate_normal([0, 0], Sigma, n_samples)
>
> # Coeficientes verdadeiros
> beta_true = np.array([[1.5, 2], [2.5, 1]])
>
> # Gerar as respostas
> Y = np.dot(np.concatenate((np.ones((n_samples, 1)), X), axis=1), beta_true) + errors
>
> # Estimar com regress√µes independentes
> model_y1 = LinearRegression()
> model_y1.fit(X, Y[:, 0])
> model_y2 = LinearRegression()
> model_y2.fit(X, Y[:, 1])
>
> # Estimar com crit√©rio ponderado (usando a matriz de covari√¢ncia amostral)
> Sigma_hat = np.cov(Y, rowvar=False)
> Sigma_hat_inv = np.linalg.inv(Sigma_hat)
> X_mat = np.concatenate((np.ones((n_samples, 1)), X), axis=1)
> B_weighted = np.linalg.solve(X_mat.T @ X_mat, X_mat.T @ Y @ Sigma_hat_inv @ Sigma_hat)
>
> # Criar um DataFrame para comparar os resultados
> results = pd.DataFrame({
>    "Variable": ["Intercept_Y1", "X_Y1", "Intercept_Y2", "X_Y2"],
>    "Independent Regressions": [model_y1.intercept_, model_y1.coef_[0], model_y2.intercept_, model_y2.coef_[0]],
>    "Weighted Criterion": [B_weighted[0,0], B_weighted[1,0], B_weighted[0,1], B_weighted[1,1]]
> })
>
> print(results)
> ```
>
> O c√≥digo acima gera dados simulados com erros correlacionados. Em seguida, ajusta dois modelos: um usando regress√µes lineares independentes para cada vari√°vel resposta e outro usando o crit√©rio ponderado com a matriz de covari√¢ncia amostral dos erros. Os resultados mostram que os coeficientes estimados pelos dois m√©todos s√£o diferentes. Os coeficientes obtidos pelo crit√©rio ponderado s√£o mais eficientes (menor vari√¢ncia) quando a correla√ß√£o entre os erros √© considerada.

### Minimiza√ß√£o do Crit√©rio Multivariado Ponderado

A minimiza√ß√£o do crit√©rio ponderado $RSS(B;\Sigma)$ quando $\Sigma$ n√£o √© diagonal √© mais complexa que a solu√ß√£o de m√≠nimos quadrados padr√£o. A solu√ß√£o da equa√ß√£o $\frac{\partial RSS(B;\Sigma)}{\partial B}=0$, leva a:

$$ B = (X^TX)^{-1}X^T Y \Sigma^{-1} \Sigma $$

Na pr√°tica, muitas vezes $\Sigma$ √© desconhecido, e deve ser estimado, o que pode ser realizado por meio de m√©todos como m√°xima verossimilhan√ßa [^17]. A utiliza√ß√£o da inversa da matrix $\Sigma$ tamb√©m traz desafios computacionais que podem ser minimizados utilizando m√©todos como a decomposi√ß√£o de Cholesky.

### Pergunta Te√≥rica Avan√ßada: Como o Crit√©rio Multivariado Ponderado e a Correla√ß√£o dos Erros se Relacionam com a Efici√™ncia e a Estabilidade das Estimativas em Modelos de Regress√£o M√∫ltipla e como a Escolha Adequada da Matriz de Covari√¢ncia Afeta os Resultados?

**Resposta:**
A rela√ß√£o entre o crit√©rio multivariado ponderado, a correla√ß√£o entre os erros e a estabilidade das estimativas pode ser compreendida por meio do seguinte racioc√≠nio [^18]:
-   Na aus√™ncia de correla√ß√£o entre os erros das diferentes vari√°veis respostas (isto √©, quando a matriz de covari√¢ncia dos erros $\Sigma$ √© diagonal), a fun√ß√£o de perda multivariada pode ser separada em $K$ fun√ß√µes de perda individuais, uma para cada vari√°vel resposta. Cada uma destas fun√ß√µes pode ser minimizada individualmente, e as estimativas de par√¢metros obtidas s√£o as mesmas que seriam obtidas com uma an√°lise de regress√£o separada para cada resposta [^19].
-   Quando h√° correla√ß√£o entre os erros, a minimiza√ß√£o da fun√ß√£o de perda multivariada usando a matriz de covari√¢ncia $\Sigma$ produz estimativas mais eficientes, ou seja, com menor vari√¢ncia. Isso ocorre porque o modelo ponderado leva em considera√ß√£o as correla√ß√µes entre os erros, extraindo o m√°ximo de informa√ß√£o dos dados [^20]. Usar o m√©todo de regress√µes independentes ignora a estrutura de covari√¢ncia dos erros, o que leva a um aumento na vari√¢ncia das estimativas.
-   A escolha da matriz de covari√¢ncia $\Sigma$ afeta a estabilidade dos resultados [^21]. Se a estimativa da matriz de covari√¢ncia $\hat\Sigma$ for inst√°vel (o que pode ocorrer quando o n√∫mero de observa√ß√µes √© pequeno em rela√ß√£o ao n√∫mero de respostas), a estimativa da matriz de coeficientes $B$ pode ser inst√°vel e com grande vari√¢ncia. Nestes casos, √© importante usar m√©todos robustos para estimar $\Sigma$ ou usar modelos que imponham uma estrutura mais simples na matriz de covari√¢ncia [^22].
-   Em resumo, utilizar o crit√©rio de m√≠nimos quadrados ponderado, considerando a correla√ß√£o entre os erros atrav√©s de $\Sigma$ √© fundamental quando esta correla√ß√£o √© forte, pois leva a estimativas de par√¢metros mais precisas. No entanto, quando n√£o h√° fortes correla√ß√µes ou a estimativa de $\Sigma$ √© muito inst√°vel, a resolu√ß√£o de regress√µes separadas √© uma boa aproxima√ß√£o do modelo e mais eficiente.

### Conclus√£o

O crit√©rio multivariado ponderado, expresso como $tr[(Y - XB)^T \Sigma^{-1} (Y - XB)]$, generaliza a fun√ß√£o de perda de m√≠nimos quadrados padr√£o, levando em considera√ß√£o a estrutura de covari√¢ncia dos erros [^23]. Embora a solu√ß√£o para este crit√©rio se reduza a m√∫ltiplas regress√µes independentes quando $\Sigma$ √© diagonal, a sua considera√ß√£o pode ser crucial na obten√ß√£o de estimativas mais eficientes e est√°veis em modelos com erros correlacionados [^24]. Uma compreens√£o completa deste crit√©rio √© fundamental para o desenvolvimento e an√°lise de modelos de regress√£o linear m√∫ltipla em finan√ßas quantitativas e outras √°reas de estudo [^25].

### Refer√™ncias
[^1]: "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,..., Xp." *(Trecho de Linear Methods for Regression)*
[^2]: "Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them." *(Trecho de Linear Methods for Regression)*
[^3]: "In this chapter we describe linear methods for regression..." *(Trecho de Linear Methods for Regression)*
[^4]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^5]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Regression Models and Least Squares)*
[^6]: "The linear model has the form f(x) = Œ≤0 + Œ£j=1 pXjŒ≤j." *(Trecho de Linear Regression Models and Least Squares)*
[^7]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their population." *(Trecho de Linear Regression Models and Least Squares)*
[^8]: "Even if the xi's were not drawn randomly, the criterion is still valid if the yi's are conditionally independent given the inputs xi." *(Trecho de Linear Regression Models and Least Squares)*
[^9]: "Figure 3.1 illustrates the geometry of least-squares fitting in the IRp+1-dimensional space occupied by the pairs (X, Y)." *(Trecho de Linear Regression Models and Least Squares)*
[^10]: "Note that (3.2) makes no assumptions about the validity of model (3.1); it simply finds the best linear fit to the data." *(Trecho de Linear Regression Models and Least Squares)*
[^11]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Regression Models and Least Squares)*
[^12]: "How do we minimize (3.2)? Denote by X the N x (p + 1) matrix with each row an input vector (with a 1 in the first position), and similarly let y be the N-vector of outputs in the training set." *(Trecho de Linear Regression Models and Least Squares)*
[^13]: "Then we can write the residual sum-of-squares as RSS(Œ≤) = (y - XŒ≤)T(y - XŒ≤)." *(Trecho de Linear Regression Models and Least Squares)*
[^14]: "This is a quadratic function in the p + 1 parameters. Differentiating with respect to Œ≤ we obtain" *(Trecho de Linear Regression Models and Least Squares)*
[^15]: "Assuming (for the moment) that X has full column rank, and hence XTX is positive definite, we set the first derivative to zero XTY - XTXŒ≤ = 0." *(Trecho de Linear Regression Models and Least Squares)*
[^16]: "To obtain the unique solution Œ≤ = (XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^17]: "The predicted values at an input vector x0 are given by f(x0) = (1 x0)TŒ≤; the fitted values at the training inputs are ≈∑ = XŒ≤ = X(XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^18]: "The matrix H = X(XTX)-1XT appearing in equation (3.7) is sometimes called the ‚Äúhat‚Äù matrix because it puts the hat on y." *(Trecho de Linear Regression Models and Least Squares)*
[^19]: "Figure 3.2 shows a different geometrical representation of the least squares estimate, this time in IRN." *(Trecho de Linear Regression Models and Least Squares)*
[^20]: "We denote the column vectors of X by x0, x1,..., xp, with x0 = 1. For much of what follows, this first column is treated like any other. These vectors span a subspace of IRN, also referred to as the column space of X." *(Trecho de Linear Regression Models and Least Squares)*
[^21]: "We minimize RSS(Œ≤) = ||y - XŒ≤||2 by choosing Œ≤ so that the residual vector y - ≈∑ is orthogonal to this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^22]: "This orthogonality is expressed in (3.5), and the resulting estimate ≈∑ is hence the orthogonal pro- jection of y onto this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^23]: "The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix." *(Trecho de Linear Regression Models and Least Squares)*
[^24]: "The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion." *(Trecho de Linear Regression Models and Least Squares)*
[^25]: "There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X." *(Trecho de Linear Regression Models and Least Squares)*
