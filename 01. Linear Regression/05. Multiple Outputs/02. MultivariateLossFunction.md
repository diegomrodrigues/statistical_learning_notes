## Perda Multivariada: tr[(Y - XB)·µÄ(Y - XB)]

```mermaid
flowchart LR
    A["Y (N √ó K)\nObserved Responses"] --> B["Y - XB"]
    C["X (N √ó (p+1))\nPredictors"] --> D["B ((p+1) √ó K)\nCoefficients"]
    D --> E["XB\nPredicted Responses"]
    E --> B
    B --> F["(Y - XB)·µÄ"]
    F --> G["(Y - XB)·µÄ(Y - XB)"]
    G --> H["tr[(Y - XB)·µÄ(Y - XB)]\nMultivariate Loss"]
    style H fill:#f9f,stroke:#333,stroke-width:4px
```

### Introdu√ß√£o

Em modelos de regress√£o linear com m√∫ltiplas vari√°veis respostas, o objetivo √© encontrar os coeficientes que melhor se ajustem aos dados observados para todas as respostas simultaneamente. A fun√ß√£o de perda multivariada, expressa como **$tr[(Y - XB)^T(Y - XB)]$**, √© uma generaliza√ß√£o da soma dos quadrados dos res√≠duos (RSS) para o caso de m√∫ltiplas sa√≠das [^1]. Esta se√ß√£o tem como objetivo analisar essa fun√ß√£o de perda em detalhes, explorando sua deriva√ß√£o, significado e rela√ß√£o com a otimiza√ß√£o dos modelos de regress√£o linear m√∫ltipla.

### Defini√ß√£o e Formula√ß√£o da Perda Multivariada

No contexto de regress√£o linear m√∫ltipla, temos uma matriz de respostas $Y$ de dimens√£o $N \times K$, uma matriz de preditores $X$ de dimens√£o $N \times (p+1)$, e uma matriz de coeficientes $B$ de dimens√£o $(p+1) \times K$. O objetivo √© minimizar a diferen√ßa entre as respostas observadas ($Y$) e as respostas preditas ($XB$). Em um modelo com m√∫ltiplas respostas, a fun√ß√£o de perda precisa levar em considera√ß√£o os erros em todas as respostas [^4]. Isso √© alcan√ßado atrav√©s da fun√ß√£o de perda multivariada:
$$ RSS(B) = tr[(Y - XB)^T (Y - XB)] $$

Onde:
- $Y$ √© a matriz $N \times K$ das respostas observadas.
- $X$ √© a matriz $N \times (p+1)$ dos preditores.
- $B$ √© a matriz $(p+1) \times K$ dos coeficientes.
- O termo $(Y-XB)$ representa a matriz dos res√≠duos.
- O termo $(Y-XB)^T$ representa a transposta da matriz dos res√≠duos.
- O operador $tr$ denota o tra√ßo de uma matriz, isto √©, a soma dos elementos da sua diagonal principal [^5].

> üí° **Exemplo Num√©rico:**
> Vamos considerar um cen√°rio simplificado com 3 observa√ß√µes (N=3), 2 preditores (p=2, mais o intercepto) e 2 vari√°veis respostas (K=2). Suponha que temos as seguintes matrizes:
>
> $$ X = \begin{bmatrix} 1 & 2 & 3 \\ 1 & 4 & 5 \\ 1 & 6 & 7 \end{bmatrix}, \quad Y = \begin{bmatrix} 4 & 6 \\ 8 & 10 \\ 12 & 14 \end{bmatrix} $$
>
> E uma matriz de coeficientes inicial $B$:
>
> $$ B = \begin{bmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \\ 0.5 & 0.5 \end{bmatrix} $$
>
> **Passo 1: Calcular XB**
>
> $$ XB = \begin{bmatrix} 1 & 2 & 3 \\ 1 & 4 & 5 \\ 1 & 6 & 7 \end{bmatrix} \begin{bmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \\ 0.5 & 0.5 \end{bmatrix} = \begin{bmatrix} 3 & 3 \\ 5 & 5 \\ 7 & 7 \end{bmatrix} $$
>
> **Passo 2: Calcular a matriz de res√≠duos E = Y - XB**
>
> $$ E = \begin{bmatrix} 4 & 6 \\ 8 & 10 \\ 12 & 14 \end{bmatrix} - \begin{bmatrix} 3 & 3 \\ 5 & 5 \\ 7 & 7 \end{bmatrix} = \begin{bmatrix} 1 & 3 \\ 3 & 5 \\ 5 & 7 \end{bmatrix} $$
>
> **Passo 3: Calcular E·µÄE**
>
> $$ E^T E = \begin{bmatrix} 1 & 3 & 5 \\ 3 & 5 & 7 \end{bmatrix} \begin{bmatrix} 1 & 3 \\ 3 & 5 \\ 5 & 7 \end{bmatrix} = \begin{bmatrix} 35 & 53 \\ 53 & 83 \end{bmatrix} $$
>
> **Passo 4: Calcular o tra√ßo de E·µÄE**
>
> $$ tr(E^T E) = 35 + 83 = 118 $$
>
> Portanto, a perda multivariada para essa matriz $B$ inicial √© 118. Este valor representa a soma dos quadrados dos res√≠duos para todas as observa√ß√µes e todas as vari√°veis respostas. O objetivo √© encontrar uma matriz $B$ que minimize este valor.

**Conceito 1: Matriz dos Res√≠duos**
```mermaid
flowchart LR
    Y["'Y' (N x K) - Observed Responses"]
    XB["'XB' (N x K) - Predicted Responses"]
    subgraph "Calculation of Residual Matrix E"
    Y -->|Subtract| E["'E = Y - XB' (N x K) - Residuals"]
    XB --> E
    end
```

A matriz $(Y-XB)$ representa a matriz dos res√≠duos, com cada elemento $e_{ik} = y_{ik} - \sum_{j=0}^p x_{ij}\beta_{jk}$ representando a diferen√ßa entre o valor observado e o valor predito para a observa√ß√£o $i$ e a vari√°vel resposta $k$ [^6].

**Conceito 2: Tra√ßo de uma Matriz**
```mermaid
flowchart LR
    A["'A' (n x n) - Square Matrix"]
    subgraph "Trace Calculation"
    A -->|Sum of Diagonal Elements| trA["tr(A) = Œ£ a<sub>ii</sub> - Trace"]
    end
```
O tra√ßo de uma matriz quadrada $A$ de dimens√£o $n \times n$, denotado por $tr(A)$, √© a soma dos elementos em sua diagonal principal [^7]:
$$ tr(A) = \sum_{i=1}^n a_{ii} $$
O tra√ßo √© um operador linear e tem a propriedade importante de que $tr(AB) = tr(BA)$, onde $A$ e $B$ s√£o matrizes tais que $AB$ e $BA$ sejam definidos [^8].

**Lemma 1:** *A fun√ß√£o de perda multivariada $tr[(Y-XB)^T(Y-XB)]$ √© equivalente √† soma dos quadrados dos res√≠duos para todas as vari√°veis respostas, ou seja, a soma de todos os elementos ao quadrado na matriz $(Y-XB)$* [^9].

**Prova do Lemma 1:**
Seja $E = Y - XB$ a matriz de res√≠duos. Ent√£o, $(Y-XB)^T(Y-XB) = E^TE$. O produto da matriz transposta $E^T$ pela matriz $E$ resulta em uma matriz cujos elementos diagonais s√£o a soma dos quadrados dos elementos das colunas de E. O tra√ßo √© a soma dos elementos na diagonal, que correspondem, neste caso, √† soma dos quadrados dos res√≠duos para cada uma das vari√°veis respostas. Portanto
$$tr[(Y-XB)^T (Y-XB)] = tr(E^TE) = \sum_{i=1}^N\sum_{k=1}^K e_{ik}^2$$
$\blacksquare$

> üí° **Exemplo Num√©rico (Continua√ß√£o):**
> Usando a matriz de res√≠duos $E$ do exemplo anterior:
>
> $$ E = \begin{bmatrix} 1 & 3 \\ 3 & 5 \\ 5 & 7 \end{bmatrix} $$
>
> A soma dos quadrados de todos os elementos de $E$ √©:
>
> $$ 1^2 + 3^2 + 3^2 + 5^2 + 5^2 + 7^2 = 1 + 9 + 9 + 25 + 25 + 49 = 118 $$
>
> Este valor √© igual ao tra√ßo de $E^T E$ calculado anteriormente, demonstrando o Lemma 1.

### Rela√ß√£o com a Perda em Modelos com Resposta √önica

Em modelos com resposta √∫nica, a fun√ß√£o de perda √© a soma dos quadrados dos res√≠duos, dada por
$$ RSS(\beta) = ||y - X\beta||^2 = \sum_{i=1}^{N} (y_i - \sum_{j=0}^{p} x_{ij}\beta_j)^2 $$
A fun√ß√£o de perda multivariada √© uma generaliza√ß√£o desta fun√ß√£o para o caso de m√∫ltiplas respostas [^10].

**Corol√°rio 1:** *Se houver apenas uma vari√°vel resposta, ou seja, $K=1$, a fun√ß√£o de perda multivariada se reduz √† soma dos quadrados dos res√≠duos da regress√£o linear com uma √∫nica vari√°vel resposta, ou seja, $RSS(B) = ||y - X\beta||^2$* [^11].

**Prova do Corol√°rio 1:**
Se K=1, a matrix $Y$ e $B$ se tornam vetores colunas $y$ e $\beta$ respectivamente e a matriz $E$ se torna um vetor coluna de res√≠duos. Neste caso:
$$ RSS(B) = tr[(y-X\beta)^T(y-X\beta)] = (y-X\beta)^T(y-X\beta) = ||y - X\beta||^2 $$
$\blacksquare$

> üí° **Exemplo Num√©rico:**
> Vamos considerar o caso onde K=1. Usaremos os mesmos dados de $X$ do exemplo anterior, e apenas a primeira coluna de $Y$ como a resposta $y$:
>
> $$ X = \begin{bmatrix} 1 & 2 & 3 \\ 1 & 4 & 5 \\ 1 & 6 & 7 \end{bmatrix}, \quad y = \begin{bmatrix} 4 \\ 8 \\ 12 \end{bmatrix} $$
>
> E vamos usar um vetor de coeficientes $\beta$:
>
> $$ \beta = \begin{bmatrix} 0.5 \\ 0.5 \\ 0.5 \end{bmatrix} $$
>
> **Passo 1: Calcular XŒ≤**
>
> $$ X\beta = \begin{bmatrix} 1 & 2 & 3 \\ 1 & 4 & 5 \\ 1 & 6 & 7 \end{bmatrix} \begin{bmatrix} 0.5 \\ 0.5 \\ 0.5 \end{bmatrix} = \begin{bmatrix} 3 \\ 5 \\ 7 \end{bmatrix} $$
>
> **Passo 2: Calcular o vetor de res√≠duos e = y - XŒ≤**
>
> $$ e = \begin{bmatrix} 4 \\ 8 \\ 12 \end{bmatrix} - \begin{bmatrix} 3 \\ 5 \\ 7 \end{bmatrix} = \begin{bmatrix} 1 \\ 3 \\ 5 \end{bmatrix} $$
>
> **Passo 3: Calcular a soma dos quadrados dos res√≠duos**
>
> $$ ||e||^2 = 1^2 + 3^2 + 5^2 = 1 + 9 + 25 = 35 $$
>
> Neste caso, a fun√ß√£o de perda multivariada com K=1 √© igual √† soma dos quadrados dos res√≠duos da regress√£o linear simples.

### Minimiza√ß√£o da Perda Multivariada
```mermaid
flowchart LR
    subgraph "Minimization"
    direction TB
    A["Minimize tr[(Y-XB)<sup>T</sup>(Y-XB)]"] --> B["Find optimal B"]
    end
    B --> C["B = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>Y"]
    C --> D["Optimal Coefficients Matrix"]
```

O objetivo √© encontrar a matriz $B$ que minimize a fun√ß√£o de perda multivariada [^12]:

$$ \underset{B}{\text{min}} \quad tr[(Y - XB)^T (Y - XB)] $$

Como demonstrado no contexto [^13], a solu√ß√£o para $B$ que minimiza a fun√ß√£o de perda √© dada por:
$$ B = (X^TX)^{-1}X^TY $$

**Lemma 3:** *A solu√ß√£o de m√≠nimos quadrados para a matriz de coeficientes B, obtida atrav√©s da minimiza√ß√£o do tra√ßo da matriz de res√≠duos transposta pela matriz de res√≠duos, √© equivalente √† solu√ß√£o de m√∫ltiplos problemas de m√≠nimos quadrados separados para cada uma das vari√°veis respostas* [^14].

**Prova do Lemma 3:**
A derivada da fun√ß√£o de perda multivariada em rela√ß√£o a $B$ √© dada por $-2X^T(Y-XB)$. Ao igualar essa derivada a zero, encontramos a solu√ß√£o para a qual o RSS √© m√≠nimo. Isolando a matriz $B$, temos:
$$ X^T(Y-XB) = 0 $$
$$ X^TXB = X^TY $$
$$ B = (X^TX)^{-1} X^TY $$
Que √© equivalente a solu√ß√£o de m√≠nimos quadrados por m√≠nimos quadrados separados. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Usando as matrizes $X$ e $Y$ do primeiro exemplo, vamos calcular a matriz $B$ que minimiza a perda multivariada.
>
> $$ X = \begin{bmatrix} 1 & 2 & 3 \\ 1 & 4 & 5 \\ 1 & 6 & 7 \end{bmatrix}, \quad Y = \begin{bmatrix} 4 & 6 \\ 8 & 10 \\ 12 & 14 \end{bmatrix} $$
>
> **Passo 1: Calcular X·µÄX**
>
> $$ X^T X = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 4 & 6 \\ 3 & 5 & 7 \end{bmatrix} \begin{bmatrix} 1 & 2 & 3 \\ 1 & 4 & 5 \\ 1 & 6 & 7 \end{bmatrix} = \begin{bmatrix} 3 & 12 & 15 \\ 12 & 56 & 72 \\ 15 & 72 & 83 \end{bmatrix} $$
>
> **Passo 2: Calcular (X·µÄX)‚Åª¬π**
>
> $$ (X^T X)^{-1} = \begin{bmatrix} 16 & -4.5 & 0.5 \\ -4.5 & 2.0 & -0.5 \\ 0.5 & -0.5 & 0.167 \end{bmatrix} $$
>
> **Passo 3: Calcular X·µÄY**
>
> $$ X^T Y = \begin{bmatrix} 1 & 1 & 1 \\ 2 & 4 & 6 \\ 3 & 5 & 7 \end{bmatrix} \begin{bmatrix} 4 & 6 \\ 8 & 10 \\ 12 & 14 \end{bmatrix} = \begin{bmatrix} 24 & 30 \\ 96 & 120 \\ 120 & 150 \end{bmatrix} $$
>
> **Passo 4: Calcular B = (X·µÄX)‚Åª¬πX·µÄY**
>
> $$ B = \begin{bmatrix} 16 & -4.5 & 0.5 \\ -4.5 & 2.0 & -0.5 \\ 0.5 & -0.5 & 0.167 \end{bmatrix} \begin{bmatrix} 24 & 30 \\ 96 & 120 \\ 120 & 150 \end{bmatrix} = \begin{bmatrix} -2 & 0 \\ 1 & 1 \\ 1 & 1 \end{bmatrix} $$
>
> Esta matriz $B$ √© a que minimiza a fun√ß√£o de perda multivariada para os dados fornecidos.

**Corol√°rio 2:** *Na aus√™ncia de correla√ß√£o entre os erros das diferentes respostas, o problema de m√≠nimos quadrados para m√∫ltiplas respostas pode ser resolvido de forma independente para cada resposta, sem perda de otimalidade* [^15].

### Implica√ß√µes da Perda Multivariada para a An√°lise de Modelos

A fun√ß√£o de perda multivariada oferece uma forma elegante e concisa de expressar o objetivo de ajuste em modelos lineares com m√∫ltiplas respostas [^16]. Ela resume a soma dos erros para todas as respostas em uma √∫nica m√©trica, que √© facilmente implementada e otimizada. O uso do tra√ßo garante que a fun√ß√£o de perda seja um escalar, facilitando sua manipula√ß√£o e minimiza√ß√£o [^17].
Al√©m disso, a solu√ß√£o obtida atrav√©s da minimiza√ß√£o da perda multivariada coincide com a solu√ß√£o obtida resolvendo separadamente problemas de m√≠nimos quadrados para cada resposta [^18]. Essa propriedade simplifica a an√°lise em muitas situa√ß√µes pr√°ticas, mas destaca tamb√©m as limita√ß√µes da abordagem quando a premissa de independ√™ncia entre os erros n√£o √© v√°lida [^19].

### Pergunta Te√≥rica Avan√ßada: Como a Correla√ß√£o entre as Vari√°veis Respostas Afeta a Escolha e a Estabilidade das Estimativas na Regress√£o Linear M√∫ltipla e como a Fun√ß√£o de Perda Multivariada Aborda Essa Quest√£o?

**Resposta:**

A correla√ß√£o entre as vari√°veis respostas em modelos de regress√£o linear m√∫ltipla introduz complexidades que podem afetar a estabilidade e efici√™ncia das estimativas de par√¢metros. A fun√ß√£o de perda multivariada $tr[(Y-XB)^T(Y-XB)]$ tratada neste contexto,  assume explicitamente que os erros entre as diferentes respostas s√£o independentes e, portanto, n√£o leva em conta tais correla√ß√µes.
Na aus√™ncia de correla√ß√£o entre os erros, a minimiza√ß√£o da perda multivariada leva √† mesma solu√ß√£o que minimizar individualmente as fun√ß√µes de perda para cada resposta, ou seja, a matriz de coeficientes $B$ √© estimada resolvendo problemas separados de m√≠nimos quadrados para cada vari√°vel resposta [^20]. No entanto, quando os erros s√£o correlacionados, a independ√™ncia das respostas n√£o √© uma premissa v√°lida. A ignorar essa correla√ß√£o podemos levar a modelos menos precisos, com par√¢metros inst√°veis e com maior vari√¢ncia. *Modelos que levam em conta as correla√ß√µes entre os erros das respostas (e.g. modelos lineares mistos e modelos multivariados), podem resultar em estimativas mais eficientes (isto √©, com menor vari√¢ncia), ao compartilhar informa√ß√µes entre as diferentes respostas* [^21].
A fun√ß√£o de perda multivariada apresentada aqui, embora n√£o leve em conta a correla√ß√£o entre as respostas, n√£o √© inerentemente "errada" se a independ√™ncia entre os erros for uma boa aproxima√ß√£o do problema, o que ocorre em diversas situa√ß√µes. Entretanto, em situa√ß√µes onde as correla√ß√µes entre os erros s√£o significativas, uma formula√ß√£o que incorpore essa estrutura de correla√ß√£o, geralmente atrav√©s da inclus√£o da matriz de covari√¢ncia dos erros na fun√ß√£o de perda, poderia resultar em estimativas mais robustas e eficientes [^22]. A formula√ß√£o matricial permite uma generaliza√ß√£o desta abordagem em fun√ß√£o de uma matriz de covari√¢ncia dos erros.

### Conclus√£o

A fun√ß√£o de perda multivariada, expressa como $tr[(Y-XB)^T(Y-XB)]$, oferece uma forma concisa e eficiente de definir o objetivo de ajuste em modelos de regress√£o linear com m√∫ltiplas respostas [^23]. Sua minimiza√ß√£o leva √† solu√ß√£o de m√≠nimos quadrados, que coincide com a solu√ß√£o de m√≠nimos quadrados obtida individualmente para cada resposta, desde que os erros sejam independentes entre si [^24]. Embora esta formula√ß√£o seja uma ferramenta poderosa e amplamente utilizada, a compreens√£o de suas limita√ß√µes, particularmente em situa√ß√µes com correla√ß√£o entre as respostas, √© crucial para a escolha de um m√©todo de modelagem apropriado e para a an√°lise de resultados mais confi√°veis [^25].

### Refer√™ncias
[^1]: "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,..., Xp." *(Trecho de Linear Methods for Regression)*
[^2]: "Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them." *(Trecho de Linear Methods for Regression)*
[^3]: "In this chapter we describe linear methods for regression..." *(Trecho de Linear Methods for Regression)*
[^4]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^5]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Regression Models and Least Squares)*
[^6]: "The linear model has the form f(x) = Œ≤0 + Œ£j=1 pXjŒ≤j." *(Trecho de Linear Regression Models and Least Squares)*
[^7]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their population." *(Trecho de Linear Regression Models and Least Squares)*
[^8]: "Even if the xi's were not drawn randomly, the criterion is still valid if the yi's are conditionally independent given the inputs xi." *(Trecho de Linear Regression Models and Least Squares)*
[^9]: "Figure 3.1 illustrates the geometry of least-squares fitting in the IRp+1-dimensional space occupied by the pairs (X, Y)." *(Trecho de Linear Regression Models and Least Squares)*
[^10]: "Note that (3.2) makes no assumptions about the validity of model (3.1); it simply finds the best linear fit to the data." *(Trecho de Linear Regression Models and Least Squares)*
[^11]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Regression Models and Least Squares)*
[^12]: "How do we minimize (3.2)? Denote by X the N x (p + 1) matrix with each row an input vector (with a 1 in the first position), and similarly let y be the N-vector of outputs in the training set." *(Trecho de Linear Regression Models and Least Squares)*
[^13]: "Then we can write the residual sum-of-squares as RSS(Œ≤) = (y - XŒ≤)T(y - XŒ≤)." *(Trecho de Linear Regression Models and Least Squares)*
[^14]: "This is a quadratic function in the p + 1 parameters. Differentiating with respect to Œ≤ we obtain" *(Trecho de Linear Regression Models and Least Squares)*
[^15]: "Assuming (for the moment) that X has full column rank, and hence XTX is positive definite, we set the first derivative to zero XTY - XTXŒ≤ = 0." *(Trecho de Linear Regression Models and Least Squares)*
[^16]: "To obtain the unique solution Œ≤ = (XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^17]: "The predicted values at an input vector x0 are given by f(x0) = (1 x0)TŒ≤; the fitted values at the training inputs are ≈∑ = XŒ≤ = X(XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^18]: "The matrix H = X(XTX)-1XT appearing in equation (3.7) is sometimes called the ‚Äúhat‚Äù matrix because it puts the hat on y." *(Trecho de Linear Regression Models and Least Squares)*
[^19]: "Figure 3.2 shows a different geometrical representation of the least squares estimate, this time in IRN." *(Trecho de Linear Regression Models and Least Squares)*
[^20]: "We denote the column vectors of X by x0, x1,..., xp, with x0 = 1. For much of what follows, this first column is treated like any other. These vectors span a subspace of IRN, also referred to as the column space of X." *(Trecho de Linear Regression Models and Least Squares)*
[^21]: "We minimize RSS(Œ≤) = ||y - XŒ≤||2 by choosing Œ≤ so that the residual vector y - ≈∑ is orthogonal to this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^22]: "This orthogonality is expressed in (3.5), and the resulting estimate ≈∑ is hence the orthogonal pro- jection of y onto this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^23]: "The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix." *(Trecho de Linear Regression Models and Least Squares)*
[^24]: "The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion." *(Trecho de Linear Regression Models and Least Squares)*
[^25]: "There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X." *(Trecho de Linear Regression Models and Least Squares)*
[^26]: "Up to now we have made minimal assumptions about the true distribution of the data." *(Trecho de Linear Regression Models and Least Squares)*
[^27]: "In order to pin down the sampling properties of Œ≤, we now assume that the observations yi are uncorrelated and have constant variance œÉ¬≤, and that the xi are fixed (non random)." *(Trecho de Linear Regression Models and Least Squares)*
[^28]: "The variance-covariance matrix of the least squares parameter estimates is easily derived from (3.6) and is given by Var(Œ≤) = (XTX)-1œÉ2." *(Trecho de Linear Regression Models and Least Squares)*
[^29]: "Typically one estimates the variance œÉ¬≤ by √¥¬≤ = (1/(N-p-1)) Œ£i(Yi-≈∂i)¬≤." *(Trecho de Linear Regression Models and Least Squares)*
[^30]: "To test the hypothesis that a particular coefficient Œ≤j = 0, we form the standardized coefficient or Z-score Zj = Œ≤j /√¥‚àövj, where vj is the jth diagonal element of (XTX)-1." *(Trecho de Linear Regression Models and Least Squares)*
[^31]: "Under the null hypothesis that Œ≤j = 0, zj is distributed as tN-p-1 (a t distribution with N ‚Äì p ‚Äì 1 degrees of freedom)" *(Trecho de Linear Regression Models and Least Squares)*
[^32]: "Often we need to test for the significance of groups of coefficients simultaneously." *(Trecho de Linear Regression Models and Least Squares)*
[^33]: "For example, to test if a categorical variable with k levels can be excluded from a model, we need to test whether the coefficients of the dummy variables used to represent the levels can all be set to zero." *(Trecho de Linear Regression Models and Least Squares)*
