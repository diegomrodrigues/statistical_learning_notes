## Orthogonal Predictors and Their Effect on Parameter Estimates

```mermaid
graph LR
    A["Preditores"] -->|Ortogonalidade| B("Modelo de Regress√£o Linear")
    B --> C("An√°lise Simplificada")
    B --> D("Interpreta√ß√£o Direta")
    B --> E("Estimativas Est√°veis")
    C-->F("C√°lculo Simplificado de Coeficientes")
    D-->G("Efeito Marginal Puro")
    E-->H("Redu√ß√£o de Multicolinearidade")
```

### Introdu√ß√£o

A **ortogonalidade dos preditores** em um modelo de regress√£o linear √© uma condi√ß√£o especial que simplifica significativamente a an√°lise e interpreta√ß√£o dos resultados [^1]. Quando os preditores s√£o ortogonais, seus efeitos sobre a vari√°vel resposta podem ser avaliados independentemente, sem a influ√™ncia de multicolinearidade, tornando a interpreta√ß√£o dos coeficientes mais direta e est√°vel [^2]. Nesta se√ß√£o, exploraremos os benef√≠cios e as implica√ß√µes da ortogonalidade dos preditores no contexto da regress√£o linear.

### Defini√ß√µes e Propriedades da Ortogonalidade

Antes de nos aprofundarmos em suas implica√ß√µes para a regress√£o linear, vamos definir formalmente o conceito de ortogonalidade e apresentar algumas propriedades relevantes [^4]:

**Conceito 1: Ortogonalidade de Vetores**

Dois vetores $u$ e $v$ s√£o **ortogonais** se seu produto interno √© igual a zero [^5]:
$$ <u, v> = u^T v = \sum_{i=1}^{n} u_i v_i = 0 $$
Geometricamente, isso significa que os vetores formam um √¢ngulo de 90 graus entre si [^6].

> üí° **Exemplo Num√©rico:**
> Seja $u = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$ e $v = \begin{bmatrix} -4 \\ 2 \end{bmatrix}$. Vamos verificar se eles s√£o ortogonais:
>
> $ <u, v> = (1 \times -4) + (2 \times 2) = -4 + 4 = 0 $
>
> Como o produto interno √© zero, os vetores $u$ e $v$ s√£o ortogonais.

**Lemma 1:** *Um conjunto de vetores $\{v_1, v_2, \ldots, v_k\}$ √© dito ortogonal se, para todo par $i \neq j$, temos $ <v_i, v_j> = 0 $*. Se al√©m de ortogonal, todos os vetores tiverem norma 1, o conjunto √© chamado de ortonormal [^7].

**Prova do Lemma 1:** A defini√ß√£o de ortogonalidade para um conjunto de vetores exige que todos os pares de vetores sejam ortogonais entre si. Ou seja, o produto interno de qualquer par de vetores $v_i$ e $v_j$ deve ser igual a zero quando $i$ √© diferente de $j$. $\blacksquare$

**Conceito 2: Ortogonalidade de Preditores**

Em regress√£o linear, as colunas da matriz de preditores $X$ correspondem aos preditores. Dizemos que os preditores s√£o **ortogonais** se as colunas correspondentes de $X$ s√£o ortogonais entre si, ou seja, o produto interno entre quaisquer duas colunas distintas √© zero [^8].

> üí° **Exemplo Num√©rico:**
> Considere a matriz de preditores $X$:
>
> $ X = \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 0 \end{bmatrix} $
>
> Seja $x_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$ e $x_2 = \begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix}$ as colunas de $X$.
>
> $ <x_1, x_2> = (1 \times 1) + (1 \times -1) + (1 \times 0) = 1 - 1 + 0 = 0 $
>
> Como o produto interno das colunas √© zero, os preditores s√£o ortogonais.

**Corol√°rio 1:** *Se as colunas da matriz $X$ s√£o ortogonais, a matriz $X^TX$ resultante √© uma matriz diagonal*. Isso simplifica significativamente as opera√ß√µes matriciais na regress√£o linear [^9].

> üí° **Exemplo Num√©rico:**
> Usando a matriz $X$ do exemplo anterior:
>
> $ X^T = \begin{bmatrix} 1 & 1 & 1 \\ 1 & -1 & 0 \end{bmatrix} $
>
> $ X^TX = \begin{bmatrix} 1 & 1 & 1 \\ 1 & -1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix} $
>
> Como resultado, $X^TX$ √© uma matriz diagonal, confirmando o Corol√°rio 1.

### Impacto da Ortogonalidade nas Estimativas de Par√¢metros

```mermaid
graph LR
    A("Matriz X") --> B("Ortogonalidade")
    B --> C("Matriz X^T * X (Diagonal)")
    C --> D("C√°lculo Simplificado da Inversa")
    D --> E("Solu√ß√£o de M√≠nimos Quadrados")
```

Quando os preditores s√£o ortogonais, diversas propriedades importantes emergem em rela√ß√£o √†s estimativas de par√¢metros [^10]:

**1. C√°lculo Simplificado de Coeficientes**
A solu√ß√£o de m√≠nimos quadrados para o vetor de coeficientes $\beta$ √© dada por [^11]:

$$ \hat{\beta} = (X^TX)^{-1}X^Ty $$

Se as colunas de $X$ s√£o ortogonais, ent√£o $X^TX$ √© uma matriz diagonal [^12]. Isso significa que a inversa de $X^TX$ √© tamb√©m diagonal, com as entradas na diagonal sendo os inversos dos produtos internos de cada coluna de $X$ consigo mesma. Assim, a solu√ß√£o de m√≠nimos quadrados simplifica-se para:

$$ \hat{\beta}_j = \frac{<x_j, y>}{<x_j, x_j>} $$

onde $x_j$ representa a j-√©sima coluna da matriz $X$ [^13]. Cada coeficiente $\hat{\beta}_j$ √© computado independentemente, sem necessidade de c√°lculos matriciais complexos ou invers√£o de matrizes [^14].

> üí° **Exemplo Num√©rico:**
> Vamos usar a matriz $X$ do exemplo anterior e um vetor de respostas $y = \begin{bmatrix} 3 \\ -1 \\ 2 \end{bmatrix}$.
>
> $x_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$, $x_2 = \begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix}$
>
> $ <x_1, y> = (1 \times 3) + (1 \times -1) + (1 \times 2) = 3 - 1 + 2 = 4 $
>
> $ <x_1, x_1> = (1 \times 1) + (1 \times 1) + (1 \times 1) = 3 $
>
> $ \hat{\beta}_1 = \frac{4}{3} $
>
> $ <x_2, y> = (1 \times 3) + (-1 \times -1) + (0 \times 2) = 3 + 1 + 0 = 4 $
>
> $ <x_2, x_2> = (1 \times 1) + (-1 \times -1) + (0 \times 0) = 2 $
>
> $ \hat{\beta}_2 = \frac{4}{2} = 2 $
>
> Portanto, o vetor de coeficientes √© $\hat{\beta} = \begin{bmatrix} 4/3 \\ 2 \end{bmatrix}$. Note que cada coeficiente foi calculado de forma independente, sem invers√£o de matriz.

**Lemma 2:** *Quando os preditores s√£o ortogonais, o c√°lculo dos coeficientes em uma regress√£o linear se reduz a uma s√©rie de opera√ß√µes de produto interno e divis√£o, sem a necessidade de invers√£o matricial*. Isso torna o processo mais eficiente e est√°vel do ponto de vista num√©rico [^15].

**Prova do Lemma 2:**
Quando as colunas de X s√£o ortogonais, ou seja, se  $<x_i, x_j> = 0$  para  $i \neq j$, a matriz  $X^TX$ √© diagonal. A matriz $X^TX$ √© composta pelos produtos internos de todas as combina√ß√µes das colunas de $X$. Se as colunas de $X$ s√£o ortogonais, $X^TX$ s√≥ ter√° elementos diferentes de zero na diagonal. Os elementos da diagonal s√£o, neste caso, os produtos internos de cada coluna consigo mesma. Assim,
$$ (X^TX) = \begin{bmatrix}
    <x_1, x_1> & 0 & \ldots & 0 \\
    0 & <x_2, x_2> & \ldots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \ldots & <x_p, x_p>
    \end{bmatrix} $$
A matriz inversa de uma matriz diagonal √© tamb√©m diagonal, onde os elementos da diagonal s√£o os inversos dos elementos da diagonal da matriz original. Assim, $(X^TX)^{-1}$ √© uma matriz diagonal com os elementos da forma  $1/<x_j,x_j>$.

Portanto, $\hat{\beta} = (X^TX)^{-1} X^Ty$  simplifica-se a $\hat{\beta_j} = \frac{<x_j,y>}{<x_j,x_j>}$.
$\blacksquare$

**2. Interpreta√ß√£o Direta dos Coeficientes**

Em um modelo com preditores ortogonais, cada coeficiente $\hat{\beta}_j$ representa o efeito marginal puro da vari√°vel $x_j$ na vari√°vel resposta $y$, sem qualquer influ√™ncia da colinearidade. Isso significa que o coeficiente $\hat{\beta}_j$ indica a mudan√ßa em $y$ para um aumento de uma unidade em $x_j$, mantendo todas as outras vari√°veis constantes, dado que os outros preditores s√£o ortogonais [^16].

> üí° **Exemplo Num√©rico:**
> No exemplo anterior, $\hat{\beta}_1 = 4/3$. Isso significa que, para cada unidade de aumento em $x_1$, espera-se um aumento de $4/3$ na vari√°vel resposta $y$, mantendo $x_2$ constante. Similarmente, $\hat{\beta}_2 = 2$, indicando que para cada unidade de aumento em $x_2$, espera-se um aumento de 2 em $y$, mantendo $x_1$ constante. A interpreta√ß√£o √© direta porque n√£o h√° colinearidade.

**3. Estabilidade das Estimativas**

A ortogonalidade entre os preditores leva a uma maior estabilidade das estimativas dos par√¢metros [^17]. *Na presen√ßa de multicolinearidade, pequenas mudan√ßas nos dados podem levar a grandes varia√ß√µes nas estimativas dos coeficientes*. Com preditores ortogonais, a vari√¢ncia das estimativas dos par√¢metros √© reduzida, tornando o modelo mais robusto a flutua√ß√µes nos dados [^18].

**Corol√°rio 2:** *A ortogonalidade dos preditores reduz a multicolinearidade, estabilizando as estimativas dos par√¢metros e facilitando a interpreta√ß√£o do modelo* [^19]. A aus√™ncia de colinearidade significa que as informa√ß√µes contidas em um preditor n√£o s√£o redundantes com as informa√ß√µes de outros preditores, levando a uma maior confiabilidade nos resultados.

### Obtendo Preditores Ortogonais

Embora a ortogonalidade entre os preditores seja uma situa√ß√£o ideal, ela raramente ocorre em conjuntos de dados observacionais [^20]. M√©todos como a **ortogonaliza√ß√£o de Gram-Schmidt** e a **decomposi√ß√£o QR** s√£o ferramentas que permitem transformar um conjunto de preditores em um conjunto ortogonal, preservando o espa√ßo gerado pelos preditores originais [^21]. Esses m√©todos s√£o fundamentais em muitos algoritmos de regress√£o, incluindo a obten√ß√£o de componentes principais, e facilitam a aplica√ß√£o dos resultados anteriormente descritos.

> üí° **Exemplo Num√©rico (Gram-Schmidt):**
>
> Vamos aplicar o processo de Gram-Schmidt em um conjunto de preditores n√£o ortogonais, $x_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ e $x_2 = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}$.
>
> **Passo 1: Normalizar o primeiro vetor**
>
> $v_1 = x_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$
>
> **Passo 2: Ortogonalizar o segundo vetor**
>
> $ v_2 = x_2 - \frac{<x_2, v_1>}{<v_1, v_1>}v_1 $
>
> $ <x_2, v_1> = (4 \times 1) + (5 \times 2) + (6 \times 3) = 4 + 10 + 18 = 32 $
>
> $ <v_1, v_1> = (1 \times 1) + (2 \times 2) + (3 \times 3) = 1 + 4 + 9 = 14 $
>
> $ v_2 = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix} - \frac{32}{14} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix} - \begin{bmatrix} 2.2857 \\ 4.5714 \\ 6.8571 \end{bmatrix} = \begin{bmatrix} 1.7143 \\ 0.4286 \\ -0.8571 \end{bmatrix}$
>
> Agora, $v_1$ e $v_2$ s√£o ortogonais. Podemos normaliz√°-los para obter um conjunto ortonormal.
>
> Este exemplo ilustra como o processo de Gram-Schmidt transforma vetores n√£o ortogonais em vetores ortogonais.

### An√°lise Geom√©trica da Ortogonalidade

```mermaid
graph LR
    A("Preditores Ortogonais") --> B("Sistema de Coordenadas Perpendiculares")
    B --> C("Proje√ß√£o Ortogonal da Resposta")
    C --> D("Soma Ponderada dos Preditores")
    D --> E("Interpreta√ß√£o Facilitada")
```

Geometricamente, quando os preditores s√£o ortogonais, eles formam um sistema de coordenadas perpendicular entre si no espa√ßo dos preditores [^22]. A resposta predita para cada observa√ß√£o √© obtida atrav√©s da proje√ß√£o ortogonal da resposta observada no espa√ßo gerado pelos preditores ortogonais. Essa proje√ß√£o √© uma soma ponderada dos preditores, onde os pesos s√£o os coeficientes do modelo de regress√£o [^23]. A ortogonalidade garante que cada preditor contribui de forma √∫nica e independente para a resposta, o que torna o modelo mais f√°cil de visualizar e interpretar [^24].

### Pergunta Te√≥rica Avan√ßada: Como a Decomposi√ß√£o QR se Relaciona com a Ortogonaliza√ß√£o de Preditores e a Solu√ß√£o de M√≠nimos Quadrados?

**Resposta:**

A decomposi√ß√£o QR √© uma t√©cnica fundamental na regress√£o linear que conecta a ortogonaliza√ß√£o de preditores com a solu√ß√£o de m√≠nimos quadrados [^25]. Dado uma matriz de preditores $X$, a decomposi√ß√£o QR expressa $X$ como o produto de uma matriz ortogonal $Q$ e uma matriz triangular superior $R$ [^26]:
$$ X = QR $$
onde $Q^TQ = I$, o que significa que as colunas de $Q$ s√£o ortonormais, e R √© uma matriz triangular superior.
Ao aplicar a decomposi√ß√£o QR na equa√ß√£o de m√≠nimos quadrados, ela se transforma em um sistema equivalente com preditores ortogonais, o que facilita a solu√ß√£o [^27]. Se $X = QR$, a solu√ß√£o de m√≠nimos quadrados √©:

$$ \hat{\beta} = (X^TX)^{-1}X^Ty = (R^TQ^TQR)^{-1}R^TQ^Ty = R^{-1}Q^Ty $$

*A matriz $Q$ tem colunas ortonormais, e assim $Q^TQ = I$*. Isso simplifica a solu√ß√£o e garante que a matrix $R$ seja triangular superior. O problema original de minimizar  || y - XŒ≤ ||¬≤  torna-se o problema de encontrar Œ≤ que minimize || Q^T y - RŒ≤||¬≤ , com preditores ortogonais e um sistema linear mais f√°cil de resolver [^28]. Essa decomposi√ß√£o  transforma o problema original em um novo problema  com uma estrutura que facilita o c√°lculo da solu√ß√£o de m√≠nimos quadrados.
Al√©m disso, o res√≠duo $r$ que resulta da regress√£o tamb√©m pode ser descrito como $r = y - X\beta = Q (Q^T y - R\beta)$. A proje√ß√£o ortogonal dos dados no espa√ßo dos preditores ortogonalizados e o c√°lculo dos res√≠duos, s√£o feitos de maneira eficiente e est√°vel usando a decomposi√ß√£o QR [^29]. A decomposi√ß√£o QR, portanto, n√£o s√≥ transforma os preditores em ortogonais, mas tamb√©m simplifica a obten√ß√£o da solu√ß√£o de m√≠nimos quadrados, atrav√©s da transforma√ß√£o do sistema linear original em um problema triangular superior mais f√°cil de resolver [^30].

> üí° **Exemplo Num√©rico (Decomposi√ß√£o QR):**
>
> Vamos usar a matriz $X$ do exemplo anterior, $X = \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 0 \end{bmatrix}$ e realizar a decomposi√ß√£o QR usando `numpy`.
>
> ```python
> import numpy as np
>
> X = np.array([[1, 1], [1, -1], [1, 0]])
> Q, R = np.linalg.qr(X)
>
> print("Matriz Q:\n", Q)
> print("Matriz R:\n", R)
> ```
>
> A sa√≠da do c√≥digo mostra a matriz $Q$ (ortogonal) e a matriz $R$ (triangular superior) resultantes.
>
> ```
> Matriz Q:
> [[-0.57735027  0.70710678]
> [-0.57735027 -0.70710678]
> [-0.57735027  0.        ]]
> Matriz R:
> [[-1.73205081  0.        ]
> [ 0.         -1.41421356]]
> ```
>
> Este exemplo ilustra como a decomposi√ß√£o QR transforma a matriz $X$ em um produto de uma matriz ortogonal $Q$ e uma matriz triangular superior $R$, facilitando a solu√ß√£o de m√≠nimos quadrados.

### Conclus√£o

A ortogonalidade dos preditores em um modelo de regress√£o linear √© uma propriedade valiosa, embora raramente encontrada em conjuntos de dados observacionais [^31]. Ela simplifica o c√°lculo das estimativas de par√¢metros, tornando-as mais interpret√°veis e est√°veis. T√©cnicas como Gram-Schmidt e decomposi√ß√£o QR s√£o essenciais para transformar conjuntos de dados complexos em formas mais trat√°veis, o que leva a an√°lises mais robustas e confi√°veis. A compreens√£o desses conceitos e suas implica√ß√µes √© fundamental para o desenvolvimento de modelos de regress√£o linear eficientes e bem-fundamentados em diversas √°reas, incluindo finan√ßas quantitativas [^32].

### Refer√™ncias
[^1]: "Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them." *(Trecho de Linear Methods for Regression)*
[^2]: "They are simple and often provide an adequate and interpretable description of how the inputs affect the output." *(Trecho de Linear Methods for Regression)*
[^3]: "In this chapter we describe linear methods for regression..." *(Trecho de Linear Methods for Regression)*
[^4]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^5]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Regression Models and Least Squares)*
[^6]: "The linear model has the form f(x) = Œ≤0 + Œ£j=1 pXjŒ≤j." *(Trecho de Linear Regression Models and Least Squares)*
[^7]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their population." *(Trecho de Linear Regression Models and Least Squares)*
[^8]: "Even if the xi's were not drawn randomly, the criterion is still valid if the yi's are conditionally independent given the inputs xi." *(Trecho de Linear Regression Models and Least Squares)*
[^9]: "Figure 3.1 illustrates the geometry of least-squares fitting in the IRp+1-dimensional space occupied by the pairs (X, Y)." *(Trecho de Linear Regression Models and Least Squares)*
[^10]: "Note that (3.2) makes no assumptions about the validity of model (3.1); it simply finds the best linear fit to the data." *(Trecho de Linear Regression Models and Least Squares)*
[^11]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Regression Models and Least Squares)*
[^12]: "How do we minimize (3.2)? Denote by X the N x (p + 1) matrix with each row an input vector (with a 1 in the first position), and similarly let y be the N-vector of outputs in the training set." *(Trecho de Linear Regression Models and Least Squares)*
[^13]: "Then we can write the residual sum-of-squares as RSS(Œ≤) = (y - XŒ≤)T(y - XŒ≤)." *(Trecho de Linear Regression Models and Least Squares)*
[^14]: "This is a quadratic function in the p + 1 parameters. Differentiating with respect to Œ≤ we obtain" *(Trecho de Linear Regression Models and Least Squares)*
[^15]: "Assuming (for the moment) that X has full column rank, and hence XTX is positive definite, we set the first derivative to zero XTY - XTXŒ≤ = 0." *(Trecho de Linear Regression Models and Least Squares)*
[^16]: "To obtain the unique solution Œ≤ = (XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^17]: "The predicted values at an input vector x0 are given by f(x0) = (1 x0)TŒ≤; the fitted values at the training inputs are ≈∑ = XŒ≤ = X(XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^18]: "The matrix H = X(XTX)-1XT appearing in equation (3.7) is sometimes called the ‚Äúhat‚Äù matrix because it puts the hat on y." *(Trecho de Linear Regression Models and Least Squares)*
[^19]: "Figure 3.2 shows a different geometrical representation of the least squares estimate, this time in IRN." *(Trecho de Linear Regression Models and Least Squares)*
[^20]: "We denote the column vectors of X by x0, x1,..., xp, with x0 = 1. For much of what follows, this first column is treated like any other. These vectors span a subspace of IRN, also referred to as the column space of X." *(Trecho de Linear Regression Models and Least Squares)*
[^21]: "We minimize RSS(Œ≤) = ||y - XŒ≤||2 by choosing Œ≤ so that the residual vector y - ≈∑ is orthogonal to this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^22]: "This orthogonality is expressed in (3.5), and the resulting estimate ≈∑ is hence the orthogonal pro- jection of y onto this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^23]: "The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix." *(Trecho de Linear Regression Models and Least Squares)*
[^24]: "The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion." *(Trecho de Linear Regression Models and Least Squares)*
[^25]: "There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X." *(Trecho de Linear Regression Models and Least Squares)*
[^26]: "Up to now we have made minimal assumptions about the true distribution of the data." *(Trecho de Linear Regression Models and Least Squares)*
[^27]: "In order to pin down the sampling properties of Œ≤, we now assume that the observations yi are uncorrelated and have constant variance œÉ¬≤, and that the xi are fixed (non random)." *(Trecho de Linear Regression Models and Least Squares)*
[^28]: "The variance-covariance matrix of the least squares parameter estimates is easily derived from (3.6) and is given by Var(Œ≤) = (XTX)-1œÉ2." *(Trecho de Linear Regression Models and Least Squares)*
[^29]: "Typically one estimates the variance œÉ¬≤ by √¥¬≤ = (1/(N-p-1)) Œ£i(Yi-≈∂i)¬≤." *(Trecho de Linear Regression Models and Least Squares)*
[^30]: "To test the hypothesis that a particular coefficient Œ≤j = 0, we form the standardized coefficient or Z-score Zj = Œ≤j /√¥‚àövj, where vj is the jth diagonal element of (XTX)-1." *(Trecho de Linear Regression Models and Least Squares)*
[^31]: "Under the null hypothesis that Œ≤j = 0, zj is distributed as tN-p-1 (a t distribution with N ‚Äì p ‚Äì 1 degrees of freedom)" *(Trecho de Linear Regression Models and Least Squares)*
[^32]: "Often we need to test for the significance of groups of coefficients simultaneously." *(Trecho de Linear Regression Models and Least Squares)*
