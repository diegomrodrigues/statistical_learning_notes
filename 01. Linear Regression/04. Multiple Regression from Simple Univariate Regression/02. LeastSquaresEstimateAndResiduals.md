## Estimativa por M√≠nimos Quadrados e Res√≠duos em Modelos de Regress√£o Linear

```mermaid
flowchart LR
    A["Dados Observados (y)"] --> B("Modelo de Regress√£o Linear (f(x))");
    B --> C("Valores Preditos (≈∑)");
    A -- "Diferen√ßa (Res√≠duo)" --> D("Res√≠duos (r)");
    C --> D;
    D --> E("Soma dos Quadrados dos Res√≠duos (RSS)");
    E --> F("Minimizar RSS");
    F --> G("Estimativa de Par√¢metros (Œ≤)");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ffc,stroke:#333,stroke-width:2px
    style E fill:#cff,stroke:#333,stroke-width:2px
    style F fill:#cfc,stroke:#333,stroke-width:2px
    style G fill:#fcc,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Em modelos de regress√£o linear, a estima√ß√£o dos par√¢metros √© tipicamente realizada atrav√©s do m√©todo dos **m√≠nimos quadrados**, que busca minimizar a **soma dos quadrados dos res√≠duos (RSS)**. Os **res√≠duos** representam a diferen√ßa entre os valores observados e os valores preditos pelo modelo. Este cap√≠tulo explorar√° em profundidade a obten√ß√£o da estimativa por m√≠nimos quadrados, bem como a interpreta√ß√£o e o uso dos res√≠duos na avalia√ß√£o e valida√ß√£o do modelo. Vamos demonstrar, tamb√©m, o m√©todo de *orthogonalization by successive regression* que auxilia a entender as rela√ß√µes entre os preditores, bem como a formula√ß√£o do algoritmo QR.

### Estima√ß√£o por M√≠nimos Quadrados

O m√©todo de **m√≠nimos quadrados** tem como objetivo encontrar os par√¢metros $\beta$ de um modelo de regress√£o linear que minimizem a soma dos quadrados dos res√≠duos. Dado o modelo linear:
$$
y_i = f(x_i) + \epsilon_i = \beta_0 + \sum_{j=1}^{p} x_{ij}\beta_j + \epsilon_i
$$
onde:
   - $y_i$ √© o valor observado para a i-√©sima observa√ß√£o.
        - $f(x_i)$ √© o valor predito pelo modelo para a i-√©sima observa√ß√£o.
        - $x_{ij}$ √© o valor do j-√©simo preditor para a i-√©sima observa√ß√£o.
        - $\beta_0$ √© o intercepto.
        - $\beta_j$ s√£o os par√¢metros do modelo.
   -  $\epsilon_i$ √© o erro aleat√≥rio.
A fun√ß√£o RSS, a ser minimizada pelo m√©todo de m√≠nimos quadrados, √© dada por:
$$
RSS(\beta) = \sum_{i=1}^N (y_i - f(x_i))^2 = \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2
$$
onde $N$ √© o n√∫mero de observa√ß√µes.

A solu√ß√£o de m√≠nimos quadrados √© obtida derivando a fun√ß√£o RSS em rela√ß√£o a cada par√¢metro $\beta_j$ e igualando a zero, o que resulta nas equa√ß√µes normais. Estas equa√ß√µes podem ser resolvidas de forma anal√≠tica atrav√©s da forma matricial:
```mermaid
flowchart LR
    A["Matriz de Design (X)"] --> B["X<sup>T</sup>"];
    B --> C["X<sup>T</sup>X"];
    C --> D["(X<sup>T</sup>X)<sup>-1</sup>"];
    D --> E["(X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>"];
    F["Vetor de Respostas (y)"] --> G["X<sup>T</sup>y"];
    E & G --> H["Œ≤ÃÇ = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y"];
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#fcc,stroke:#333,stroke-width:2px
```
$$
\hat{\beta} = (X^T X)^{-1} X^T y
$$
onde:
   - $\hat{\beta}$ √© o vetor de par√¢metros estimados por m√≠nimos quadrados.
   - $X$ √© a matriz de design.
   - $y$ √© o vetor de respostas.

Essa solu√ß√£o minimiza a dist√¢ncia euclidiana entre o vetor de respostas $y$ e o espa√ßo gerado pelos preditores, garantindo que o modelo de regress√£o linear seja o mais pr√≥ximo poss√≠vel dos dados.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simples com duas observa√ß√µes e um preditor para ilustrar o c√°lculo de $\hat{\beta}$. Suponha que temos os seguintes dados:
>
>  $X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \end{bmatrix}$ (matriz de design com um intercepto e um preditor)
>
>  $y = \begin{bmatrix} 5 \\ 8 \end{bmatrix}$ (vetor de respostas)
>
> Primeiro, calculamos $X^T$:
>
> $X^T = \begin{bmatrix} 1 & 1 \\ 2 & 3 \end{bmatrix}$
>
> Em seguida, calculamos $X^T X$:
>
> $X^T X = \begin{bmatrix} 1 & 1 \\ 2 & 3 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 1 & 3 \end{bmatrix} = \begin{bmatrix} 2 & 5 \\ 5 & 13 \end{bmatrix}$
>
> Agora, calculamos a inversa de $X^T X$:
>
> $(X^T X)^{-1} = \frac{1}{(2*13 - 5*5)} \begin{bmatrix} 13 & -5 \\ -5 & 2 \end{bmatrix} = \begin{bmatrix} 13 & -5 \\ -5 & 2 \end{bmatrix}$
>
> Calculamos $X^T y$:
>
> $X^T y = \begin{bmatrix} 1 & 1 \\ 2 & 3 \end{bmatrix} \begin{bmatrix} 5 \\ 8 \end{bmatrix} = \begin{bmatrix} 13 \\ 34 \end{bmatrix}$
>
> Finalmente, calculamos $\hat{\beta}$:
>
> $\hat{\beta} = (X^T X)^{-1} X^T y = \begin{bmatrix} 13 & -5 \\ -5 & 2 \end{bmatrix} \begin{bmatrix} 13 \\ 34 \end{bmatrix} = \begin{bmatrix} 13*13 - 5*34 \\ -5*13 + 2*34 \end{bmatrix} = \begin{bmatrix} -29 \\ 3 \end{bmatrix}$
>
> Portanto, $\hat{\beta} = \begin{bmatrix} -29 \\ 13 \end{bmatrix}$, o que significa que o intercepto √© -29 e o coeficiente do preditor √© 13. O modelo estimado √© $\hat{y} = -29 + 13x$.

### Propriedades e Interpreta√ß√£o dos Res√≠duos

O res√≠duo, denotado por $r_i$, √© a diferen√ßa entre o valor observado $y_i$ e o valor predito $\hat{y}_i$ de cada observa√ß√£o, ou seja:
```mermaid
flowchart LR
    A["Valor Observado (y<sub>i</sub>)"]
    B["Valor Predito (≈∑<sub>i</sub>)"]
    A -- "Diferen√ßa" --> C["Res√≠duo (r<sub>i</sub>)"]
    B --> C
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ffc,stroke:#333,stroke-width:2px
```
$$
r_i = y_i - \hat{y}_i
$$
Os res√≠duos t√™m diversas propriedades importantes e s√£o fundamentais para avalia√ß√£o da adequa√ß√£o do modelo.

1. **Ortogonalidade:** Os res√≠duos s√£o ortogonais aos preditores,  o que significa que o vetor de res√≠duos $r$ √© ortogonal a todas as colunas da matriz de design $X$, ou seja, $X^T(y-X\hat{\beta})=0$ [^13]. Esta propriedade √© uma consequ√™ncia direta da solu√ß√£o de m√≠nimos quadrados, e a sua visualiza√ß√£o geom√©trica √© fundamental para entender o problema.
2.  **M√©dia Zero:** Sob as hip√≥teses do modelo linear, a m√©dia dos res√≠duos √© zero, $E[r_i]=0$ (assumindo que o modelo esteja especificado corretamente).
3.  **Varia√ß√£o:** A vari√¢ncia dos res√≠duos √© dada por  $\sigma^2$ que √© estimada por $\hat{\sigma}^2 = \frac{1}{N-p-1} \sum_{i=1}^N (y_i - \hat{y}_i)^2$.
4.  **Avalia√ß√£o do Ajuste:**  Os res√≠duos s√£o fundamentais para avaliar o ajuste do modelo. A an√°lise gr√°fica dos res√≠duos, procurando por padr√µes ou estruturas, pode indicar se o modelo √© apropriado para os dados.

A an√°lise dos res√≠duos permite verificar se a hip√≥tese de linearidade e de erros aleat√≥rios com m√©dia zero e vari√¢ncia constante s√£o v√°lidas. Se os res√≠duos apresentarem padr√µes, como heteroscedasticidade ou n√£o-linearidade, a validade do modelo pode estar em quest√£o.

> üí° **Exemplo Num√©rico:**
>
> Continuando com o exemplo anterior, vamos calcular os res√≠duos.
>
> As predi√ß√µes $\hat{y}$ s√£o:
>
> $\hat{y}_1 = -29 + 13 * 2 = -29 + 26 = -3$
>
> $\hat{y}_2 = -29 + 13 * 3 = -29 + 39 = 10$
>
> Os res√≠duos s√£o:
>
> $r_1 = y_1 - \hat{y}_1 = 5 - (-3) = 8$
>
> $r_2 = y_2 - \hat{y}_2 = 8 - 10 = -2$
>
> O vetor de res√≠duos √© $r = \begin{bmatrix} 8 \\ -2 \end{bmatrix}$.
>
> Vamos verificar a ortogonalidade dos res√≠duos com os preditores (incluindo o intercepto):
>
> $X^T r = \begin{bmatrix} 1 & 1 \\ 2 & 3 \end{bmatrix} \begin{bmatrix} 8 \\ -2 \end{bmatrix} = \begin{bmatrix} 1*8 + 1*(-2) \\ 2*8 + 3*(-2) \end{bmatrix} = \begin{bmatrix} 6 \\ 10 \end{bmatrix}$
>
> Note que o resultado n√£o √© zero, o que √© esperado pois os res√≠duos s√£o ortogonais ao espa√ßo gerado pelas colunas de X *ap√≥s a inclus√£o do intercepto*. Para verificar a ortogonalidade corretamente, precisamos centralizar o vetor $x$ e verificar a ortogonalidade dos res√≠duos com o vetor preditor centrado (veja o exemplo de ortogonaliza√ß√£o a seguir). A m√©dia dos res√≠duos √© $(8 - 2)/2 = 3$, o que n√£o √© zero devido a este pequeno exemplo com poucos dados. Em geral, com mais dados, a m√©dia dos res√≠duos se aproximar√° de zero.

### Algoritmo de Ortogonaliza√ß√£o por Regress√£o Sucessiva
```mermaid
flowchart TD
    A[Inicio] --> B{Inicializa√ß√£o: Intercepto x0 = 1};
    B --> C{"Para cada preditor x<sub>j</sub>"};
    C --> D{Projetar x<sub>j</sub> no espa√ßo ortogonal aos preditores anteriores};
    D --> E{Res√≠duo z<sub>j</sub>};
    E --> F{Regredir y em rela√ß√£o a z<sub>j</sub>};
    F --> G{Estimativa Œ≤<sub>j</sub>};
    G --> H{Fim};
    C -- "Pr√≥ximo Preditor" --> D
    style A fill:#cfc,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
    style G fill:#fcc,stroke:#333,stroke-width:2px
    style H fill:#cfc,stroke:#333,stroke-width:2px
```
A intui√ß√£o por tr√°s dos m√©todos de regress√£o linear podem ser mais bem entendidas atrav√©s da an√°lise da ortogonaliza√ß√£o sucessiva dos preditores, usando o m√©todo da *regress√£o sucessiva* [^53]. O processo de *regress√£o sucessiva* pode ser descrito da seguinte forma:
    1. **Inicializa√ß√£o:** Come√ßamos com um intercepto $x_0=1$.
        2. **Regress√£o:**  Para cada preditor $x_j$, projetamos o vetor $x_j$ no espa√ßo ortogonal a todos os preditores anteriores, $x_0, x_1, \ldots, x_{j-1}$, o que resulta no res√≠duo $z_j$

   3. **Nova Proje√ß√£o:** Em seguida, fazemos a regress√£o de $y$ em rela√ß√£o ao novo preditor $z_j$, obtendo a estimativa $\beta_j$.
    O algoritmo de regress√£o sucessiva pode ser descrito com os seguintes passos:
        1.  Regredir $x$ em $1$ para gerar $z=x-1\bar{x}$, onde $\bar{x}$ √© a m√©dia de $x$.
        2. Regredir $y$ em $z$ para obter $\hat{\beta_1}$.

O coeficiente de regress√£o obtido nesse processo √© id√™ntico ao coeficiente obtido no modelo linear com todos os preditores. Este m√©todo mostra como a influ√™ncia de cada preditor √© separada dos preditores anteriores. Esta ideia pode ser generalizada para v√°rios preditores [^53].

Para um modelo com $p$ preditores, seja $j = 1,\ldots,p$:

1. Inicializa√ß√£o:
   $$
   \begin{align*}
   x_0 &= \mathbf{1} \text{ (vetor de uns para o intercepto)} \\
   z_0 &= x_0
   \end{align*}
   $$

2. Para cada preditor $j$:
   $$
   \begin{align*}
   \text{Seja } x_j &\text{ o vetor do preditor atual} \\
   z_j &= x_j - \sum_{i=0}^{j-1} \frac{x_j^T z_i}{z_i^T z_i}z_i
   \end{align*}
   $$

3. Obtenha o coeficiente $\beta_j$ pela regress√£o de $y$ em $z_j$:
   $$
   \begin{align*}
   \beta_j &= \frac{z_j^T y}{z_j^T z_j}
   \end{align*}
   $$

Este processo √© equivalente ao m√©todo de Gram-Schmidt e produz o mesmo resultado que a solu√ß√£o de m√≠nimos quadrados:

$$
\hat{\beta} = (X^T X)^{-1}X^T y
$$

> üí° **Exemplo Num√©rico:**
>
> Vamos ilustrar a ortogonaliza√ß√£o por regress√£o sucessiva com os dados do exemplo anterior.
>
> $x = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$
>
> $y = \begin{bmatrix} 5 \\ 8 \end{bmatrix}$
>
> 1. **Centraliza√ß√£o do Preditores:**
>
> Calculamos a m√©dia de $x$: $\bar{x} = (2 + 3) / 2 = 2.5$
>
> Centralizamos $x$: $z = x - \bar{x} = \begin{bmatrix} 2 - 2.5 \\ 3 - 2.5 \end{bmatrix} = \begin{bmatrix} -0.5 \\ 0.5 \end{bmatrix}$
>
> 2. **Regress√£o de $y$ em $z$:**
>
> Agora, vamos realizar a regress√£o de $y$ em $z$. Como temos apenas um preditor (ap√≥s a centraliza√ß√£o), o coeficiente $\beta_1$ √© calculado como:
>
> $\beta_1 = \frac{z^T y}{z^T z} = \frac{\begin{bmatrix} -0.5 & 0.5 \end{bmatrix} \begin{bmatrix} 5 \\ 8 \end{bmatrix}}{\begin{bmatrix} -0.5 & 0.5 \end{bmatrix} \begin{bmatrix} -0.5 \\ 0.5 \end{bmatrix}} = \frac{-2.5 + 4}{0.25 + 0.25} = \frac{1.5}{0.5} = 3$
>
> Observe que este valor de $\beta_1 = 3$ √© exatamente o mesmo que obtivemos anteriormente para o coeficiente do preditor no m√©todo de m√≠nimos quadrados.
>
> 3. **Verifica√ß√£o da Ortogonalidade:**
>
> Vamos verificar a ortogonalidade dos res√≠duos da regress√£o de y em z com o preditor z. Primeiro, calculamos os valores preditos:
>
> $\hat{y} = \beta_1 z = 3 \begin{bmatrix} -0.5 \\ 0.5 \end{bmatrix} = \begin{bmatrix} -1.5 \\ 1.5 \end{bmatrix}$
>
> Os res√≠duos da regress√£o de y em z s√£o:
>
> $r = y - \hat{y} = \begin{bmatrix} 5 \\ 8 \end{bmatrix} - \begin{bmatrix} -1.5 \\ 1.5 \end{bmatrix} = \begin{bmatrix} 6.5 \\ 6.5 \end{bmatrix}$
>
> Agora, verificamos a ortogonalidade:
>
> $z^T r = \begin{bmatrix} -0.5 & 0.5 \end{bmatrix} \begin{bmatrix} 6.5 \\ 6.5 \end{bmatrix} = -3.25 + 3.25 = 0$
>
> Os res√≠duos s√£o ortogonais ao preditor centrado, conforme esperado.

**Lemma 28:** Proje√ß√£o Ortogonal nos Res√≠duos
No processo de regress√£o sucessiva, cada preditor √© ortogonalizado aos preditores anteriores. O res√≠duo de uma regress√£o de $x_j$ sobre preditores anteriores $x_0,\ldots,x_{j-1}$ corresponde a um vetor $z_j$ que √© ortogonal ao subespa√ßo gerado pelos vetores anteriores. A regress√£o de $y$ sobre $z_j$ fornece um coeficiente que corresponde ao do mesmo preditor em um modelo com todos os preditores.

**Prova do Lemma 28:**
O algoritmo de regress√£o sucessiva implementa o processo de ortogonaliza√ß√£o de Gram-Schmidt. A cada passo do algoritmo, os preditores s√£o transformados de forma a que sejam ortogonais aos preditores anteriores. A proje√ß√£o ortogonal de um vetor $x_j$ em um espa√ßo gerado por outros vetores resulta num res√≠duo que √© perpendicular ao espa√ßo gerado pelos vetores. Este processo se repete a cada passo do algoritmo, garantindo que os vetores resultantes sejam ortogonais. $\blacksquare$

**Corol√°rio 28:** Rela√ß√£o com o M√©todo de M√≠nimos Quadrados
O Corol√°rio 28 demonstra que cada passo do algoritmo de regress√£o sucessiva computa coeficientes que s√£o iguais aos coeficientes da regress√£o linear com todos os preditores. Ao usar a regress√£o sucessiva, tamb√©m podemos notar que os coeficientes s√£o computados ao projetar $y$ nos res√≠duos ortogonalizados $z$, e os coeficientes s√£o portanto tamb√©m aqueles da regress√£o linear com todos os preditores.

### Conex√£o com a Fatoriza√ß√£o QR
```mermaid
flowchart LR
    A["Matriz de Design (X)"] --> B["Fatora√ß√£o QR"];
    B --> C["Matriz Ortogonal (Q)"];
    B --> D["Matriz Triangular Superior (R)"];
    C & D --> E["X = QR"];
    E --> F["Res√≠duos Ortogonalizados (z)"];
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#ffc,stroke:#333,stroke-width:2px
```
O m√©todo da regress√£o sucessiva tem uma liga√ß√£o direta com a fatora√ß√£o QR da matriz de design [^55]. O algoritmo QR, em ess√™ncia, ortogonaliza os preditores da matriz $X$, encontrando uma matriz ortogonal $Q$ e uma matriz triangular superior $R$ tais que $X=QR$. Os vetores ortogonalizados resultantes da fatora√ß√£o QR s√£o exatamente os res√≠duos $z$ obtidos no processo de regress√£o sucessiva.
O algoritmo QR √© uma forma eficiente de ortogonalizar os preditores e calcular as solu√ß√µes de m√≠nimos quadrados. O algoritmo LARS, tamb√©m utiliza o mesmo tipo de racioc√≠nio, atrav√©s da ortogonaliza√ß√£o do res√≠duo com os preditores no conjunto ativo.

> üí° **Exemplo Num√©rico:**
>
> Embora o c√°lculo completo da fatoriza√ß√£o QR seja mais complexo, podemos ilustrar o conceito com um exemplo simples. No nosso caso, a matriz X √©:
>
> $X = \begin{bmatrix} 1 & 2 \\ 1 & 3 \end{bmatrix}$
>
> A fatora√ß√£o QR decomp√µe X em $X = QR$, onde Q √© uma matriz ortogonal e R √© uma matriz triangular superior. O processo de Gram-Schmidt √© usado para obter Q e R.
>
> Vamos simplificar e apenas ilustrar o primeiro passo de ortogonaliza√ß√£o (que √© equivalente √† primeira etapa de regress√£o sucessiva):
>
> 1. **Primeira Coluna de Q:**
>
> A primeira coluna de Q √© obtida normalizando a primeira coluna de X:
>
> $q_1 = \frac{x_1}{||x_1||} = \frac{\begin{bmatrix} 1 \\ 1 \end{bmatrix}}{\sqrt{1^2 + 1^2}} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{bmatrix}$
>
> 2. **Proje√ß√£o e Ortogonaliza√ß√£o:**
>
> A segunda coluna de X √© projetada na primeira coluna de Q, e o res√≠duo √© ortogonalizado:
>
> $x_2 = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$
>
> $proj_{q_1}(x_2) = (q_1^T x_2) q_1 = (\begin{bmatrix} 1/\sqrt{2} & 1/\sqrt{2} \end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix}) \begin{bmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{bmatrix} = \frac{5}{\sqrt{2}} \begin{bmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{bmatrix} = \begin{bmatrix} 2.5 \\ 2.5 \end{bmatrix}$
>
> O res√≠duo ortogonalizado √©:
>
> $z = x_2 - proj_{q_1}(x_2) = \begin{bmatrix} 2 \\ 3 \end{bmatrix} - \begin{bmatrix} 2.5 \\ 2.5 \end{bmatrix} = \begin{bmatrix} -0.5 \\ 0.5 \end{bmatrix}$
>
> Observe que esse res√≠duo $z$ √© o mesmo que obtivemos no exemplo de regress√£o sucessiva, mostrando a liga√ß√£o entre as duas abordagens.
>
> Este √© apenas o primeiro passo do algoritmo QR. O algoritmo continua para encontrar a matriz ortogonal completa Q e a matriz triangular superior R.

> ‚ö†Ô∏è **Nota Importante**: A estima√ß√£o por m√≠nimos quadrados busca o conjunto de par√¢metros que minimiza a soma dos quadrados dos res√≠duos.
>
> ‚ùó **Ponto de Aten√ß√£o**: Os res√≠duos s√£o a diferen√ßa entre os valores observados e preditos pelo modelo, e s√£o ortogonais aos preditores.
>
> ‚úîÔ∏è **Destaque**:  O algoritmo de ortogonaliza√ß√£o por regress√£o sucessiva √© intimamente relacionado com a fatoriza√ß√£o QR e com o processo de m√≠nimos quadrados.

### Conclus√£o
A estima√ß√£o por m√≠nimos quadrados, aliada √† an√°lise dos res√≠duos, forma um conjunto de ferramentas essenciais para a constru√ß√£o e avalia√ß√£o de modelos de regress√£o linear. Os res√≠duos, que representam a parte dos dados n√£o capturada pelo modelo linear, s√£o um guia importante para validar as suposi√ß√µes do modelo e identificar poss√≠veis problemas no ajuste do mesmo. A conex√£o entre o m√©todo de m√≠nimos quadrados, os res√≠duos e o processo de ortogonaliza√ß√£o por regress√£o sucessiva fornece uma vis√£o clara de como os modelos de regress√£o linear s√£o constru√≠dos e interpretados.

### Refer√™ncias
[^10]: "The most popular estimation method is least squares, in which we pick the coefficients $\beta = (\beta_0, \beta_1, \ldots, \beta_p)^T$ to minimize the residual sum of squares" *(Trecho de Linear Methods for Regression)*
[^13]: "We minimize $RSS(3) = ||y ‚Äì X\beta||^2$ by choosing $\beta$ so that the residual vector $y - \hat{y}$ is orthogonal to this subspace." *(Trecho de Linear Methods for Regression)*
[^53]: "Then the least squares coefficient of $x$‚Äã has the form  $\beta_1 =  (x-x_1,y)/(x-x_1, x - 1)$‚Äã  where $\bar{x} = \sum x_i/N$‚Äã, and $1 = x_0$‚Äã, the vector of N ones. We can view the estimate (3.27) as the result of two applications of the simple regression (3.26)." *(Trecho de Linear Methods for Regression)*
[^55]: "We can represent step 2 of Algorithm 3.1 in matrix form:" *(Trecho de Linear Methods for Regression)*
