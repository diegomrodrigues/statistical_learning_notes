## Ortogonaliza√ß√£o de Preditores: Ajuste para x‚ÇÄ, Etapas para Regress√£o Simples e Interpreta√ß√µes em Regress√£o M√∫ltipla

```mermaid
graph LR
    A["Dados de Entrada (X, y)"] --> B{"Ajuste para x0 (centraliza√ß√£o)"};
    B --> C{"Preditores Ortogonalizados (z)"};
    C --> D{"Regress√£o Linear"};
    D --> E["Coeficientes (Œ≤)"];
    E --> F["Modelo Ajustado"];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

A ortogonaliza√ß√£o de preditores √© um processo crucial em regress√£o linear, permitindo que os efeitos das vari√°veis preditoras sejam analisados de forma mais clara e independente. As etapas envolvidas na ortogonaliza√ß√£o, particularmente o ajuste para o termo constante $x_0$ e a sua extens√£o da regress√£o simples para a m√∫ltipla, s√£o fundamentais para uma compreens√£o aprofundada de como a independ√™ncia entre os preditores afeta a solu√ß√£o do modelo [^1]. Esta se√ß√£o ir√° detalhar esses procedimentos.

### Ajuste para o Termo Constante (x‚ÇÄ) em Regress√£o Linear

Em um modelo de regress√£o linear, frequentemente inclu√≠mos um termo constante, que representamos como $x_0 = 1$ para todas as observa√ß√µes [^3]. Para garantir a ortogonalidade, devemos ajustar os outros preditores em rela√ß√£o a este termo constante [^4]. Este ajuste √© crucial para remover a correla√ß√£o entre os preditores e a constante, o que simplifica a interpreta√ß√£o dos coeficientes do modelo [^5].

**Conceito 1: Ajuste para x‚ÇÄ**

Em um modelo de regress√£o linear com um termo constante, o primeiro passo para ortogonalizar os preditores √© centraliz√°-los em rela√ß√£o √† sua m√©dia [^6]. O ajuste para o termo constante $x_0$ pode ser visto como um processo de proje√ß√£o ortogonal, removendo a componente de cada preditor que √© paralela a $x_0$ [^7].

Matematicamente, dado um vetor de preditor $x_j$ e um vetor constante $x_0 = 1$, o ajuste para $x_0$ √© obtido pela subtra√ß√£o da m√©dia de $x_j$ ($ \bar{x_j}$ ) do vetor $x_j$, resultando em um vetor residual $z_j$ [^8]:

$$ z_j = x_j - \bar{x_j}x_0 = x_j - \bar{x_j} $$

Onde $\bar{x_j} = \frac{1}{N} \sum_{i=1}^{N} x_{ij}$, com N sendo o n√∫mero de observa√ß√µes [^9]. O vetor $z_j$ √© agora ortogonal ao vetor constante $x_0$.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um preditor $x_1$ com os seguintes valores para 5 observa√ß√µes: $x_1 = [2, 4, 6, 8, 10]$. O vetor constante $x_0$ √© $[1, 1, 1, 1, 1]$.
>
> 1. **Calcular a m√©dia de $x_1$**:
>    $\bar{x_1} = \frac{2+4+6+8+10}{5} = \frac{30}{5} = 6$
>
> 2. **Ajustar $x_1$**:
>    $z_1 = x_1 - \bar{x_1} = [2-6, 4-6, 6-6, 8-6, 10-6] = [-4, -2, 0, 2, 4]$
>
> O vetor resultante $z_1 = [-4, -2, 0, 2, 4]$ √© ortogonal a $x_0 = [1, 1, 1, 1, 1]$. Para verificar a ortogonalidade, calculamos o produto interno:
>
> $<z_1, x_0> = (-4*1) + (-2*1) + (0*1) + (2*1) + (4*1) = -4 -2 + 0 + 2 + 4 = 0$
>
> O resultado √© 0, confirmando a ortogonalidade.

**Lemma 1:** *O vetor residual $z_j$ √© ortogonal ao vetor constante $x_0$, ou seja, $<z_j, x_0> = 0$* [^10].

**Prova do Lemma 1:**
O produto interno entre o vetor ajustado $z_j$ e o vetor constante $x_0 = 1$ √© dado por:
$$ <z_j, x_0> = <x_j - \bar{x_j}, 1> = \sum_{i=1}^{N} (x_{ij} - \bar{x_j}) = \sum_{i=1}^{N} x_{ij} - \sum_{i=1}^{N} \bar{x_j} = N\bar{x_j} - N\bar{x_j} = 0 $$
$\blacksquare$

**Conceito 2: Interpreta√ß√£o do Ajuste**

O ajuste para o termo constante $x_0$ garante que o intercepto do modelo de regress√£o linear capture apenas a m√©dia da vari√°vel resposta quando todos os preditores s√£o zero (ou em suas m√©dias) e que as demais vari√°veis capturem desvios em rela√ß√£o a essa m√©dia [^11]. Este ajuste tamb√©m tem implica√ß√µes computacionais, evitando problemas num√©ricos ao calcular a inversa da matriz $X^TX$, que se torna mais est√°vel e bem-condicionada ap√≥s a ortogonaliza√ß√£o [^12].

### Etapas para Ortogonaliza√ß√£o de Preditores na Regress√£o Simples

Em uma **regress√£o simples** com um √∫nico preditor $x$, o processo de ortogonaliza√ß√£o envolve duas etapas principais:

1. **Ajuste de x:** O primeiro passo √© ajustar o preditor $x$ em rela√ß√£o ao vetor constante $x_0=1$, resultando no vetor residual $z$ [^14]:

   $$ z = x - \bar{x} $$
2. **Regress√£o de y em z:** O segundo passo √© usar este preditor ajustado $z$ para regredir a vari√°vel resposta $y$, obtendo o coeficiente $\beta_1$ [^15]:

   $$ \hat{\beta}_1 = \frac{<z, y>}{<z, z>} $$

Onde $\beta_0$ √© a m√©dia de $y$, sendo $ \bar{y}$ [^16].

```mermaid
flowchart TD
    A[In√≠cio] --> B{Calcular m√©dia de x (xÃÑ)};
    B --> C{Ajustar x: z = x - xÃÑ};
    C --> D{Calcular produto interno <z, y>};
    D --> E{Calcular produto interno <z, z>};
    E --> F{Calcular Œ≤ÃÇ‚ÇÅ = <z, y> / <z, z>};
    F --> G{Calcular Œ≤ÃÇ‚ÇÄ = yÃÑ};
    G --> H[Fim: Modelo ajustado com Œ≤ÃÇ‚ÇÄ e Œ≤ÃÇ‚ÇÅ];
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style H fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um preditor $x = [2, 4, 6, 8, 10]$ e uma vari√°vel resposta $y = [5, 7, 9, 11, 13]$.
>
> 1. **Calcular a m√©dia de $x$ e $y$:**
>    $\bar{x} = \frac{2+4+6+8+10}{5} = 6$
>    $\bar{y} = \frac{5+7+9+11+13}{5} = 9$
>
> 2. **Ajustar $x$:**
>    $z = x - \bar{x} = [2-6, 4-6, 6-6, 8-6, 10-6] = [-4, -2, 0, 2, 4]$
>
> 3. **Calcular o produto interno $<z, y>$:**
>    $<z, y> = (-4*5) + (-2*7) + (0*9) + (2*11) + (4*13) = -20 - 14 + 0 + 22 + 52 = 40$
>
> 4. **Calcular o produto interno $<z, z>$:**
>    $<z, z> = (-4)^2 + (-2)^2 + 0^2 + 2^2 + 4^2 = 16 + 4 + 0 + 4 + 16 = 40$
>
> 5. **Calcular $\hat{\beta}_1$:**
>    $\hat{\beta}_1 = \frac{<z, y>}{<z, z>} = \frac{40}{40} = 1$
>
> O coeficiente $\hat{\beta}_1 = 1$ indica que, para cada unidade de aumento em $x$ (ap√≥s o ajuste pela m√©dia), $y$ aumenta em 1 unidade. O intercepto $\hat{\beta}_0$ √© simplesmente a m√©dia de $y$, $\bar{y} = 9$.
>
> O modelo ajustado √©: $\hat{y} = 9 + 1*(x - 6)$.

**Lemma 2:** *Em regress√£o simples com um preditor ajustado, o coeficiente $\hat{\beta}_1$ captura o efeito puro da vari√°vel preditora na vari√°vel resposta, independentemente do termo constante* [^17].

**Prova do Lemma 2:**
O ajuste de $x$ usando $z = x - \bar{x}$ garante que o novo preditor  $z$ seja ortogonal ao termo constante $1$. Assim, o coeficiente  $\beta_1$ obtido atrav√©s da regress√£o de $y$ em $z$ quantifica a influ√™ncia de $x$ em $y$ que √© independente da influ√™ncia da m√©dia da vari√°vel preditora $x$. $\blacksquare$

### Etapas para Ortogonaliza√ß√£o de Preditores na Regress√£o M√∫ltipla

```mermaid
flowchart TD
    A[In√≠cio] --> B{Inicializar z‚ÇÄ = x‚ÇÄ = 1};
    B --> C{Para cada preditor x‚±º (j=1 a p)};
    C --> D{Ajustar x‚±º em rela√ß√£o a z‚ÇÄ, z‚ÇÅ,..., z‚±º‚Çã‚ÇÅ: z‚±º = x‚±º - Œ£(<x‚±º,z‚Çñ>/<z‚Çñ,z‚Çñ>)*z‚Çñ};
    D --> E{Regredir y em z‚±º: Œ≤ÃÇ‚±º = <z‚±º, y> / <z‚±º, z‚±º>};
    E --> F{Pr√≥ximo preditor};
    F -- Se houver mais preditores --> C;
     F-- Se n√£o houver mais preditores --> G[Fim: Modelo ajustado com Œ≤ÃÇ‚ÇÄ, Œ≤ÃÇ‚ÇÅ,..., Œ≤ÃÇ‚Çö];
      style A fill:#ccf,stroke:#333,stroke-width:2px
     style G fill:#ccf,stroke:#333,stroke-width:2px
```

Na **regress√£o m√∫ltipla**, com m√∫ltiplos preditores $x_1, x_2, ..., x_p$, o processo de ortogonaliza√ß√£o √© um pouco mais complexo [^18]. √â comum o uso da ortogonaliza√ß√£o sucessiva, onde cada preditor √© ajustado em rela√ß√£o aos preditores j√° ortogonalizados anteriormente:
1. **Inicializa√ß√£o:** Inicialize $z_0 = x_0 = 1$ [^20].
2. **Itera√ß√£o:** Para cada preditor $x_j$, de 1 at√© $p$, realize as seguintes etapas:
    -   **Ajuste:** Ajuste $x_j$ em rela√ß√£o a todos os vetores ortogonalizados anteriores $z_0, z_1,..., z_{j-1}$, obtendo o res√≠duo $z_j$ [^21]:

        $$ z_j = x_j - \sum_{k=0}^{j-1} \frac{<x_j, z_k>}{<z_k, z_k>} z_k $$

    -   **Regress√£o:** Regrida a vari√°vel resposta $y$ sobre o res√≠duo $z_j$, obtendo o coeficiente $\beta_j$ [^22]:
        $$ \hat{\beta_j} = \frac{<z_j, y>}{<z_j, z_j>} $$

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos dois preditores $x_1 = [2, 4, 6, 8, 10]$ e $x_2 = [3, 5, 7, 9, 11]$ e a vari√°vel resposta $y = [6, 8, 10, 12, 14]$.
>
> 1. **Inicializa√ß√£o:** $z_0 = x_0 = [1, 1, 1, 1, 1]$
>
> 2. **Ajustar $x_1$ em rela√ß√£o a $z_0$:**
>    - $\bar{x_1} = \frac{2+4+6+8+10}{5} = 6$
>    - $z_1 = x_1 - \bar{x_1} = [2-6, 4-6, 6-6, 8-6, 10-6] = [-4, -2, 0, 2, 4]$
>
> 3. **Ajustar $x_2$ em rela√ß√£o a $z_0$ e $z_1$:**
>    - $\bar{x_2} = \frac{3+5+7+9+11}{5} = 7$
>    - $z_2' = x_2 - \bar{x_2} = [3-7, 5-7, 7-7, 9-7, 11-7] = [-4, -2, 0, 2, 4]$
>    - Calcular a proje√ß√£o de $z_2'$ em $z_1$:
>        - $<z_2', z_1> = (-4)*(-4) + (-2)*(-2) + (0*0) + (2*2) + (4*4) = 16 + 4 + 0 + 4 + 16 = 40$
>        - $<z_1, z_1> = (-4)^2 + (-2)^2 + 0^2 + 2^2 + 4^2 = 40$
>        - $proj_{z_1}(z_2') = \frac{<z_2', z_1>}{<z_1, z_1>} z_1 = \frac{40}{40}z_1 = z_1 = [-4, -2, 0, 2, 4]$
>    - Ajustar $z_2'$ em rela√ß√£o a $z_1$:
>        - $z_2 = z_2' - proj_{z_1}(z_2') = [-4, -2, 0, 2, 4] - [-4, -2, 0, 2, 4] = [0, 0, 0, 0, 0]$
>        - Como $z_2$ √© nulo, isso indica que $x_2$ √© uma combina√ß√£o linear de $x_1$ e o termo constante e ap√≥s a ortogonaliza√ß√£o, n√£o adiciona nenhuma nova informa√ß√£o ao modelo.
>
> 4. **Regredir $y$ em $z_1$:**
>   -  $<z_1, y> = (-4*6) + (-2*8) + (0*10) + (2*12) + (4*14) = -24 - 16 + 0 + 24 + 56 = 40$
>   -  $<z_1, z_1> = 40$ (calculado anteriormente)
>   - $\hat{\beta}_1 = \frac{<z_1, y>}{<z_1, z_1>} = \frac{40}{40} = 1$
>
> 5. **Regredir $y$ em $z_2$:**
>    - Como $z_2$ √© um vetor nulo, $\hat{\beta}_2$ n√£o ser√° calculado.
>
> O modelo final, ap√≥s a ortogonaliza√ß√£o, √© $\hat{y} = \bar{y} + \hat{\beta_1}z_1= 10 + 1*(x_1 - 6)$. Observamos que como $x_2$ √© uma combina√ß√£o linear de $x_1$, ele n√£o contribui para o modelo ap√≥s a ortogonaliza√ß√£o.

**Lemma 3:** *Em regress√£o m√∫ltipla, a ortogonaliza√ß√£o sucessiva garante que cada vetor res√≠duo $z_j$ seja ortogonal a todos os vetores res√≠duos anteriores, ou seja, $<z_i, z_j> = 0 $ para $i \neq j$ e $i, j= 0,1,...,p$. Isso permite que cada coeficiente $\beta_j$ capture o efeito puro do preditor xj na resposta y* [^23].

**Prova do Lemma 3:**
No passo j, o vetor $x_j$ √© ajustado usando uma combina√ß√£o linear dos res√≠duos ortogonais  $z_0,z_1, \ldots, z_{j-1}$. Assim, por constru√ß√£o, o novo vetor $z_j$ √© ortogonal a todos os vetores anteriores, j√° que a subtra√ß√£o da combina√ß√£o linear remove as proje√ß√µes sobre os vetores anteriores.
$\blacksquare$

**Interpreta√ß√µes em Regress√£o M√∫ltipla:**

Ap√≥s a ortogonaliza√ß√£o, os coeficientes $\hat{\beta_j}$ representam o efeito √∫nico do preditor correspondente sobre a vari√°vel resposta, mantendo os outros preditores constantes, e que este efeito n√£o √© influenciado pela colinearidade entre os preditores. Ou seja, a mudan√ßa em y para um incremento de uma unidade em $x_j$ √© avaliada independentemente dos outros preditores [^24]. Este processo promove uma interpreta√ß√£o mais limpa e precisa do modelo [^25].

**Corol√°rio 3:** *A ortogonaliza√ß√£o sucessiva permite que as informa√ß√µes de cada preditor sejam avaliadas sem sobreposi√ß√£o ou confus√£o causada pela colinearidade*. Isso leva a uma melhor interpreta√ß√£o do modelo e a uma maior confiabilidade das estimativas de par√¢metros [^26].

### Conclus√£o

As etapas de ortogonaliza√ß√£o, incluindo o ajuste para o termo constante e a aplica√ß√£o de m√©todos sucessivos, s√£o vitais para a constru√ß√£o de modelos de regress√£o linear que forne√ßam interpreta√ß√µes claras e robustas. O produto interno √© o pilar dessas transforma√ß√µes, fornecendo uma ferramenta matem√°tica concisa para entender a rela√ß√£o entre vetores e o processo de ajuste [^27]. A compreens√£o desses procedimentos capacita a an√°lise de dados mais eficiente e eficaz, permitindo que as informa√ß√µes de cada preditor sejam interpretadas com precis√£o e seguran√ßa [^28].

### Refer√™ncias
[^1]: "Linear models were largely developed in the precomputer age of statistics, but even in today's computer era there are still good reasons to study and use them." *(Trecho de Linear Methods for Regression)*
[^2]: "They are simple and often provide an adequate and interpretable description of how the inputs affect the output." *(Trecho de Linear Methods for Regression)*
[^3]: "In this chapter we describe linear methods for regression..." *(Trecho de Linear Methods for Regression)*
[^4]: "The linear model either assumes that the regression function E(Y|X) is linear, or that the linear model is a reasonable approximation." *(Trecho de Linear Methods for Regression)*
[^5]: "The most popular estimation method is least squares, in which we pick the coefficients Œ≤ = (Œ≤0, Œ≤1, ..., Œ≤p)T to minimize the residual sum of squares" *(Trecho de Linear Regression Models and Least Squares)*
[^6]: "The linear model has the form f(x) = Œ≤0 + Œ£j=1 pXjŒ≤j." *(Trecho de Linear Regression Models and Least Squares)*
[^7]: "From a statistical point of view, this criterion is reasonable if the training observations (xi, Yi) represent independent random draws from their population." *(Trecho de Linear Regression Models and Least Squares)*
[^8]: "Even if the xi's were not drawn randomly, the criterion is still valid if the yi's are conditionally independent given the inputs xi." *(Trecho de Linear Regression Models and Least Squares)*
[^9]: "Figure 3.1 illustrates the geometry of least-squares fitting in the IRp+1-dimensional space occupied by the pairs (X, Y)." *(Trecho de Linear Regression Models and Least Squares)*
[^10]: "Note that (3.2) makes no assumptions about the validity of model (3.1); it simply finds the best linear fit to the data." *(Trecho de Linear Regression Models and Least Squares)*
[^11]: "Least squares fitting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of fit." *(Trecho de Linear Regression Models and Least Squares)*
[^12]: "How do we minimize (3.2)? Denote by X the N x (p + 1) matrix with each row an input vector (with a 1 in the first position), and similarly let y be the N-vector of outputs in the training set." *(Trecho de Linear Regression Models and Least Squares)*
[^13]: "Then we can write the residual sum-of-squares as RSS(Œ≤) = (y - XŒ≤)T(y - XŒ≤)." *(Trecho de Linear Regression Models and Least Squares)*
[^14]: "This is a quadratic function in the p + 1 parameters. Differentiating with respect to Œ≤ we obtain" *(Trecho de Linear Regression Models and Least Squares)*
[^15]: "Assuming (for the moment) that X has full column rank, and hence XTX is positive definite, we set the first derivative to zero XTY - XTXŒ≤ = 0." *(Trecho de Linear Regression Models and Least Squares)*
[^16]: "To obtain the unique solution Œ≤ = (XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^17]: "The predicted values at an input vector x0 are given by f(x0) = (1 x0)TŒ≤; the fitted values at the training inputs are ≈∑ = XŒ≤ = X(XTX)-1XTY." *(Trecho de Linear Regression Models and Least Squares)*
[^18]: "The matrix H = X(XTX)-1XT appearing in equation (3.7) is sometimes called the ‚Äúhat‚Äù matrix because it puts the hat on y." *(Trecho de Linear Regression Models and Least Squares)*
[^19]: "Figure 3.2 shows a different geometrical representation of the least squares estimate, this time in IRN." *(Trecho de Linear Regression Models and Least Squares)*
[^20]: "We denote the column vectors of X by x0, x1,..., xp, with x0 = 1. For much of what follows, this first column is treated like any other. These vectors span a subspace of IRN, also referred to as the column space of X." *(Trecho de Linear Regression Models and Least Squares)*
[^21]: "We minimize RSS(Œ≤) = ||y - XŒ≤||2 by choosing Œ≤ so that the residual vector y - ≈∑ is orthogonal to this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^22]: "This orthogonality is expressed in (3.5), and the resulting estimate ≈∑ is hence the orthogonal pro- jection of y onto this subspace." *(Trecho de Linear Regression Models and Least Squares)*
[^23]: "The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix." *(Trecho de Linear Regression Models and Least Squares)*
[^24]: "The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion." *(Trecho de Linear Regression Models and Least Squares)*
[^25]: "There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X." *(Trecho de Linear Regression Models and Least Squares)*
[^26]: "Up to now we have made minimal assumptions about the true distribution of the data." *(Trecho de Linear Regression Models and Least Squares)*
[^27]: "In order to pin down the sampling properties of Œ≤, we now assume that the observations yi are uncorrelated and have constant variance œÉ¬≤, and that the xi are fixed (non random)." *(Trecho de Linear Regression Models and Least Squares)*
[^28]: "The variance-covariance matrix of the least squares parameter estimates is easily derived from (3.6) and is given by Var(Œ≤) = (XTX)-1œÉ2." *(Trecho de Linear Regression Models and Least Squares)*
[^29]: "Typically one estimates the variance œÉ¬≤ by √¥¬≤ = (1/(N-p-1)) Œ£i(Yi-≈∂i)¬≤." *(Trecho de Linear Regression Models and Least Squares)*
