## Efici√™ncia da Decomposi√ß√£o Cholesky QR em M√≠nimos Quadrados

<imagem: Diagrama de fluxo detalhado mostrando os passos para decomposi√ß√£o Cholesky QR, desde a entrada da matriz X at√© a obten√ß√£o dos par√¢metros Œ≤, com destaque para as etapas de decomposi√ß√£o e retrosubstitui√ß√£o>

**Introdu√ß√£o**

O m√©todo dos **m√≠nimos quadrados** √© uma ferramenta fundamental em estat√≠stica e aprendizado de m√°quina para ajustar modelos lineares a dados observados. A solu√ß√£o desse problema geralmente envolve a resolu√ß√£o de equa√ß√µes normais, que podem ser resolvidas de forma eficiente atrav√©s da **decomposi√ß√£o Cholesky QR** [^3.2].  Esta t√©cnica, embora consolidada, √© fundamental para a compreens√£o e otimiza√ß√£o de modelos mais avan√ßados. Este cap√≠tulo explorar√° em profundidade a aplica√ß√£o e efici√™ncia da Cholesky QR, enfatizando suas propriedades matem√°ticas e suas implica√ß√µes pr√°ticas no contexto de regress√£o linear, baseando-se nos conceitos apresentados nos t√≥picos [^3.1] e [^3.2].

### Conceitos Fundamentais

**Conceito 1:** O **problema de regress√£o linear** busca encontrar um modelo linear que minimize a soma dos quadrados dos erros entre as previs√µes do modelo e os valores observados [^3.1]. Matematicamente, este problema pode ser expresso como encontrar um vetor $\beta$ que minimize a fun√ß√£o custo:

$$ RSS(\beta) = \sum_{i=1}^{N} (y_i - f(x_i))^2 = ||y - X\beta||^2 $$

onde $y$ √© o vetor de respostas, $X$ √© a matriz de design (ou matriz de caracter√≠sticas), e $\beta$ √© o vetor de par√¢metros a serem estimados [^3.2]. A regress√£o linear √© um m√©todo simples e interpretabil, mas que estabelece a base para t√©cnicas mais complexas, como as discutidas em cap√≠tulos posteriores.

> üí° **Exemplo Num√©rico:** Suponha que temos um conjunto de dados com duas vari√°veis preditoras ($x_1$ e $x_2$) e uma vari√°vel resposta $y$. As observa√ß√µes s√£o:
>
> | Observa√ß√£o | $x_1$ | $x_2$ | $y$ |
> |------------|-------|-------|-----|
> | 1          | 1     | 2     | 5   |
> | 2          | 2     | 3     | 8   |
> | 3          | 3     | 5     | 12  |
> | 4          | 4     | 6     | 15  |
>
> A matriz de design $X$ e o vetor de respostas $y$ ser√£o:
>
> $$ X = \begin{bmatrix} 1 & 1 & 2 \\ 1 & 2 & 3 \\ 1 & 3 & 5 \\ 1 & 4 & 6 \end{bmatrix}, \quad y = \begin{bmatrix} 5 \\ 8 \\ 12 \\ 15 \end{bmatrix} $$
>
> O objetivo √© encontrar $\beta = [\beta_0, \beta_1, \beta_2]^T$ que minimize $||y - X\beta||^2$. Aqui, $\beta_0$ √© o intercepto, $\beta_1$ √© o coeficiente de $x_1$ e $\beta_2$ √© o coeficiente de $x_2$.

**Lemma 1:** A solu√ß√£o para o problema de m√≠nimos quadrados, obtida por meio das **equa√ß√µes normais**, √© dada por $\hat{\beta} = (X^TX)^{-1}X^Ty$, assumindo que a matriz $X^TX$ √© invert√≠vel [^3.2]. Essa solu√ß√£o √© obtida ao igualar a zero a primeira derivada da fun√ß√£o RSS em rela√ß√£o a $\beta$, que √© um ponto de m√≠nimo se a segunda derivada for definida positiva.

```mermaid
graph TB
    subgraph "Derivation of Normal Equations"
        direction TB
        A["Cost Function: RSS(Œ≤) = ||y - XŒ≤||¬≤"]
        B["First Derivative: ‚àÇRSS/‚àÇŒ≤ = -2X·µÄ(y - XŒ≤)"]
        C["Setting to Zero: -2X·µÄ(y - XŒ≤) = 0"]
        D["Normal Equation: X·µÄXŒ≤ = X·µÄy"]
        E["Solution: Œ≤ÃÇ = (X·µÄX)‚Åª¬πX·µÄy"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```

$$ \frac{\partial RSS}{\partial \beta} = -2X^T(y - X\beta) = 0 $$

$$ X^TX\beta = X^Ty $$

$$ \hat{\beta} = (X^TX)^{-1}X^Ty $$

A condi√ß√£o de invertibilidade de $X^TX$ √© fundamental e geralmente satisfeita quando as colunas de $X$ s√£o linearmente independentes [^3.2].

> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, podemos calcular $X^TX$ e $X^Ty$:
>
> $$ X^TX = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \\ 2 & 3 & 5 & 6 \end{bmatrix} \begin{bmatrix} 1 & 1 & 2 \\ 1 & 2 & 3 \\ 1 & 3 & 5 \\ 1 & 4 & 6 \end{bmatrix} = \begin{bmatrix} 4 & 10 & 16 \\ 10 & 30 & 49 \\ 16 & 49 & 86 \end{bmatrix} $$
>
> $$ X^Ty = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \\ 2 & 3 & 5 & 6 \end{bmatrix} \begin{bmatrix} 5 \\ 8 \\ 12 \\ 15 \end{bmatrix} = \begin{bmatrix} 40 \\ 111 \\ 180 \end{bmatrix} $$
>
>  Para encontrar $\hat{\beta}$, calculamos $(X^TX)^{-1}$ e multiplicamos por $X^Ty$:
> ```python
> import numpy as np
>
> X = np.array([[1, 1, 2], [1, 2, 3], [1, 3, 5], [1, 4, 6]])
> y = np.array([5, 8, 12, 15])
> XtX = X.T @ X
> XtX_inv = np.linalg.inv(XtX)
> XtY = X.T @ y
> beta_hat = XtX_inv @ XtY
> print(f"Beta estimado: {beta_hat}")
> ```
>
> O resultado ser√° aproximadamente $\hat{\beta} = \begin{bmatrix} -0.03 \\ 1.06 \\ 1.96 \end{bmatrix}$. Isso significa que o modelo ajustado √© aproximadamente $y \approx -0.03 + 1.06x_1 + 1.96x_2$.

**Conceito 2:** A **decomposi√ß√£o Cholesky QR** √© uma t√©cnica para resolver sistemas de equa√ß√µes lineares, especialmente √∫til quando a matriz de design √© mal condicionada ou quando a matriz $X^TX$ √© singular. Essa decomposi√ß√£o expressa a matriz $X$ como o produto de uma matriz ortogonal $Q$ e uma matriz triangular superior $R$:  $X = QR$ [^3.2].

**Corol√°rio 1:**  Usando a decomposi√ß√£o QR, o problema de m√≠nimos quadrados se torna:

$$ ||y - X\beta||^2 = ||y - QR\beta||^2 = ||Q^Ty - R\beta||^2 $$

Dado que $Q$ √© ortogonal ($Q^TQ = I$), a norma √© preservada. Assim, a solu√ß√£o $\beta$ pode ser encontrada ao resolver $R\beta = Q^Ty$ por meio de retrosubstitui√ß√£o, uma vez que $R$ √© triangular superior [^3.2].

```mermaid
graph TB
    subgraph "QR Decomposition and Least Squares"
        direction TB
        A["Original Problem: minimize ||y - XŒ≤||¬≤"]
        B["QR Decomposition: X = QR"]
        C["Substitute: minimize ||y - QRŒ≤||¬≤"]
        D["Orthogonal Property: ||Q·µÄz|| = ||z||"]
         E["Transformed Problem: minimize ||Q·µÄy - RŒ≤||¬≤"]
         F["Solve: RŒ≤ = Q·µÄy"]
         A --> B
        B --> C
        C --> D
        D --> E
        E --> F
    end
```

> üí° **Exemplo Num√©rico:** Usando o mesmo $X$ do exemplo anterior, podemos realizar a decomposi√ß√£o QR:
>
> ```python
> import numpy as np
> import scipy.linalg
>
> X = np.array([[1, 1, 2], [1, 2, 3], [1, 3, 5], [1, 4, 6]])
> y = np.array([5, 8, 12, 15])
>
> Q, R = scipy.linalg.qr(X)
>
> print("Matriz Q:\n", Q)
> print("Matriz R:\n", R)
>
> QT_y = Q.T @ y
> beta_hat_qr = scipy.linalg.solve_triangular(R, QT_y)
> print("Beta estimado via QR:\n", beta_hat_qr)
> ```
> O c√≥digo acima calcula a matriz ortogonal $Q$ e a matriz triangular superior $R$ tal que $X = QR$. Em seguida, encontra a solu√ß√£o para $\beta$ usando a retrosubstitui√ß√£o, resolvendo $R\beta = Q^Ty$. O resultado para $\beta$ √© o mesmo (aproximadamente) que o obtido pelas equa√ß√µes normais, mas o processo √© numericamente mais est√°vel.

**Conceito 3:** A **regress√£o linear** pode ser extendida para cen√°rios em que os inputs s√£o transforma√ß√µes dos dados originais, como em expans√µes de base ou intera√ß√µes entre vari√°veis [^3.2]. Em tais casos, as matrizes $X$ e $y$ podem ser adaptadas de acordo, e o processo de otimiza√ß√£o, seja por equa√ß√µes normais ou QR, pode ser aplicado da mesma maneira.

> üí° **Exemplo Num√©rico:** Suponha que queremos modelar a rela√ß√£o entre $x$ e $y$ usando um polin√¥mio de grau 2. Os dados s√£o:
>
> |  $x$  |  $y$  |
> |-------|-------|
> |  1    |  2.8  |
> |  2    |  5.2  |
> |  3    |  8.1  |
> |  4    |  11.3 |
>
> A matriz de design agora incluir√° termos para $x^2$:
>
> $$ X = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 4 \\ 1 & 3 & 9 \\ 1 & 4 & 16 \end{bmatrix}, \quad y = \begin{bmatrix} 2.8 \\ 5.2 \\ 8.1 \\ 11.3 \end{bmatrix} $$
>
> Podemos usar tanto as equa√ß√µes normais quanto a decomposi√ß√£o QR para encontrar $\beta = [\beta_0, \beta_1, \beta_2]^T$, onde o modelo ajustado ser√° $y \approx \beta_0 + \beta_1x + \beta_2x^2$.
>
>  ```python
> import numpy as np
> import scipy.linalg
>
> x = np.array([1, 2, 3, 4])
> y = np.array([2.8, 5.2, 8.1, 11.3])
> X = np.vstack([np.ones(len(x)), x, x**2]).T
>
> # M√©todo das equa√ß√µes normais
> XtX = X.T @ X
> XtX_inv = np.linalg.inv(XtX)
> XtY = X.T @ y
> beta_hat_normal = XtX_inv @ XtY
> print("Beta via equa√ß√µes normais:", beta_hat_normal)
>
> # M√©todo da decomposi√ß√£o QR
> Q, R = scipy.linalg.qr(X)
> QT_y = Q.T @ y
> beta_hat_qr = scipy.linalg.solve_triangular(R, QT_y)
> print("Beta via QR:", beta_hat_qr)
> ```
>
> Os coeficientes encontrados mostram o ajuste do polin√¥mio aos dados.

> ‚ö†Ô∏è **Nota Importante**: A escolha do m√©todo de otimiza√ß√£o (equa√ß√µes normais vs. decomposi√ß√£o QR) pode afetar a precis√£o e a estabilidade da solu√ß√£o, especialmente quando a matriz de design √© mal condicionada [^3.2].
> ‚ùó **Ponto de Aten√ß√£o**: A decomposi√ß√£o QR √© geralmente prefer√≠vel em cen√°rios de instabilidade num√©rica, pois evita a invers√£o direta de $X^TX$, que pode ser inst√°vel [^3.2].
> ‚úîÔ∏è **Destaque**: A efici√™ncia computacional e estabilidade num√©rica da decomposi√ß√£o Cholesky QR a tornam uma ferramenta poderosa na regress√£o linear e para compreender os fundamentos das solu√ß√µes de m√≠nimos quadrados.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

<imagem: Mapa mental mostrando as etapas da regress√£o linear aplicada √† classifica√ß√£o, incluindo codifica√ß√£o das classes, estimativa de coeficientes, aplica√ß√£o da regra de decis√£o e avalia√ß√£o da performance>

A aplica√ß√£o da regress√£o linear √† classifica√ß√£o, embora menos comum do que a regress√£o log√≠stica, pode ser obtida por meio da **regress√£o linear de uma matriz indicadora** [^4.2]. Este processo envolve a codifica√ß√£o das classes de resposta como vari√°veis dummy (ou indicadores), onde cada vari√°vel representa uma classe [^4.1], [^4.2].  A regress√£o linear √© ent√£o aplicada a cada uma dessas vari√°veis, utilizando as mesmas t√©cnicas de m√≠nimos quadrados e decomposi√ß√£o QR abordadas anteriormente.

**Lemma 2:**  A regress√£o linear de uma matriz indicadora pode ser usada para produzir um classificador linear. Para cada observa√ß√£o, a classe predita ser√° aquela com a maior valor de resposta estimada. No entanto, essa abordagem pode apresentar algumas limita√ß√µes em rela√ß√£o √† regress√£o log√≠stica e LDA, como a extrapola√ß√£o de previs√µes fora do intervalo [0,1], e a n√£o-otimiza√ß√£o direta da probabilidade da classe [^4.1].

**Corol√°rio 2:** Apesar das limita√ß√µes, a regress√£o de uma matriz indicadora pode fornecer resultados compar√°veis em certos cen√°rios, especialmente quando o interesse principal √© a obten√ß√£o da fronteira de decis√£o linear, e n√£o a estimativa precisa de probabilidades de classe. Em alguns casos, pode ser mais computacionalmente eficiente do que outros m√©todos probabil√≠sticos, como a regress√£o log√≠stica [^4.2].

> üí° **Exemplo Num√©rico:** Considere um problema de classifica√ß√£o com 3 classes. Os dados s√£o:
>
> | $x_1$ | $x_2$ | Classe |
> |-------|-------|--------|
> | 1     | 1     | 0      |
> | 2     | 1     | 0      |
> | 1     | 2     | 1      |
> | 2     | 2     | 1      |
> | 3     | 3     | 2      |
> | 4     | 3     | 2      |
>
> Criamos uma matriz indicadora $Y$, onde cada coluna representa uma classe:
>
> $$ Y = \begin{bmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 1 \end{bmatrix} $$
>
> A matriz de design $X$ ser√°:
>
> $$ X = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 1 \\ 1 & 1 & 2 \\ 1 & 2 & 2 \\ 1 & 3 & 3 \\ 1 & 4 & 3 \end{bmatrix} $$
>
> Realizamos a regress√£o linear de $Y$ sobre $X$ para obter uma matriz de coeficientes $\hat{B}$. Para classificar uma nova observa√ß√£o $x_{new}$, calculamos $\hat{y} = x_{new}^T\hat{B}$. A classe predita ser√° a classe correspondente √† coluna de $\hat{y}$ com maior valor.
>
>  ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> X = np.array([[1, 1, 1], [1, 2, 1], [1, 1, 2], [1, 2, 2], [1, 3, 3], [1, 4, 3]])
> y = np.array([0, 0, 1, 1, 2, 2])
>
> # Converter y para one-hot encoding
> Y = np.eye(3)[y]
>
> model = LinearRegression()
> model.fit(X, Y)
>
> x_new = np.array([1, 3, 2]) # Novo ponto
> y_pred = model.predict(x_new.reshape(1, -1)) # A classe ser√° aquela com maior valor
> predicted_class = np.argmax(y_pred) # A classe predita
> print(f"Classe predita para [1, 3, 2]: {predicted_class}")
> ```

Em suma, a regress√£o linear pode ser aplicada a problemas de classifica√ß√£o por meio da regress√£o da matriz indicadora, onde cada classe √© representada por uma vari√°vel dummy. A decis√£o da classe √© feita pela classe que apresentar a maior resposta predita pela regress√£o. No entanto, a principal vantagem da regress√£o linear reside na sua simplicidade e interpretabilidade, mas ela apresenta limita√ß√µes e, em alguns cen√°rios, outros m√©todos s√£o mais adequados [^4.1], [^4.2].
‚ÄúA regress√£o log√≠stica fornece estimativas mais est√°veis da probabilidade, e deve ser prefer√≠vel quando o objetivo √© a estimativa de probabilidades. A regress√£o de indicadores √© mais adequada quando o principal objetivo √© a fronteira de decis√£o linear.‚Äù [^4.4]

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

<imagem: Diagrama mostrando as penaliza√ß√µes L1 e L2 aplicadas na regress√£o log√≠stica, com √™nfase na influ√™ncia de cada uma na sparsity e na estabilidade dos par√¢metros>

No contexto da classifica√ß√£o, a sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o cruciais para lidar com a complexidade do modelo, evitar o sobreajuste e melhorar a interpretabilidade [^4.5].  A **regress√£o log√≠stica**, que pode ser ajustada atrav√©s de otimiza√ß√£o por descida do gradiente, √© um bom exemplo para demonstrar como podemos usar essas t√©cnicas.

A **penaliza√ß√£o L1** (Lasso) adiciona um termo de penaliza√ß√£o √† fun√ß√£o custo, que √© a soma dos valores absolutos dos coeficientes. Esse tipo de penaliza√ß√£o tende a tornar os coeficientes esparsos, for√ßando alguns deles a serem exatamente zero [^4.4.4]. J√° a **penaliza√ß√£o L2** (Ridge) adiciona a soma dos quadrados dos coeficientes, o que tende a encolher os coeficientes em dire√ß√£o a zero, mas sem zer√°-los completamente. Ambas as regulariza√ß√µes podem ser aplicadas em conjunto, atrav√©s do **Elastic Net** [^4.5].

**Lemma 3:** A penaliza√ß√£o L1 na regress√£o log√≠stica leva a coeficientes esparsos devido √† sua natureza n√£o-diferenci√°vel em zero, o que faz com que o otimizador muitas vezes atinja valores exatamente iguais a zero.  A penaliza√ß√£o L2, por outro lado, √© diferenci√°vel em zero, apenas encolhendo o valor dos coeficientes.

**Prova do Lemma 3:** A fun√ß√£o de custo da regress√£o log√≠stica com penaliza√ß√£o L1 √© dada por:

$$ J(\beta) = - \sum_{i=1}^N [y_i \log(\sigma(x_i^T\beta)) + (1-y_i)\log(1-\sigma(x_i^T\beta))] + \lambda\sum_{j=1}^p |\beta_j| $$

onde $\sigma(x_i^T\beta)$ √© a fun√ß√£o log√≠stica (sigmoid). A derivada do termo de penaliza√ß√£o com respeito a $\beta_j$ √© $\lambda sign(\beta_j)$, que √© descont√≠nua em $\beta_j = 0$. O otimizador tentar√°, ao chegar pr√≥ximo a 0, "saltar" esse ponto, fazendo com que os coeficientes sejam 0.  Para a penaliza√ß√£o L2, que √© dada por  $\lambda\sum_{j=1}^p \beta_j^2$, a derivada √© $2\lambda\beta_j$, que √© cont√≠nua em zero, apenas reduzindo a magnitude de $\beta_j$.  $\blacksquare$
```mermaid
graph LR
    subgraph "L1 and L2 Regularization"
        direction LR
        A["Cost Function"] --> B["Loss Term: Cross-Entropy"]
        A --> C["L1 Penalty: Œª‚àë|Œ≤j|"]
        A --> D["L2 Penalty: Œª‚àëŒ≤j¬≤"]
        B --> E["Optimization Objective"]
        C --> E
        D --> E
        C --> F["Sparsity"]
         D --> G["Coefficient Shrinkage"]
    end
```

> üí° **Exemplo Num√©rico:** Para ilustrar o efeito das regulariza√ß√µes L1 e L2, consideremos um conjunto de dados de classifica√ß√£o com duas vari√°veis preditoras e uma vari√°vel resposta bin√°ria. Usaremos regress√£o log√≠stica com e sem regulariza√ß√£o:
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.model_selection import train_test_split
> from sklearn.linear_model import LogisticRegression
> from sklearn.metrics import accuracy_score
>
> # Gerar dados de exemplo
> np.random.seed(0)
> n_samples = 200
> X = np.random.randn(n_samples, 2)
> y = (X[:, 0] + X[:, 1] > 0).astype(int)
>
> # Separar dados em treino e teste
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # Regress√£o Log√≠stica sem regulariza√ß√£o
> model_no_reg = LogisticRegression(penalty=None)
> model_no_reg.fit(X_train, y_train)
> y_pred_no_reg = model_no_reg.predict(X_test)
> acc_no_reg = accuracy_score(y_test, y_pred_no_reg)
>
> # Regress√£o Log√≠stica com regulariza√ß√£o L1 (Lasso)
> model_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.5) # C √© inverso de lambda
> model_l1.fit(X_train, y_train)
> y_pred_l1 = model_l1.predict(X_test)
> acc_l1 = accuracy_score(y_test, y_pred_l1)
>
> # Regress√£o Log√≠stica com regulariza√ß√£o L2 (Ridge)
> model_l2 = LogisticRegression(penalty='l2', C=0.5) # C √© inverso de lambda
> model_l2.fit(X_train, y_train)
> y_pred_l2 = model_l2.predict(X_test)
> acc_l2 = accuracy_score(y_test, y_pred_l2)
>
> # Exibir resultados
> results = pd.DataFrame({
>     'Method': ['No Regularization', 'L1 Regularization', 'L2 Regularization'],
>     'Accuracy': [acc_no_reg, acc_l1, acc_l2],
>     'Coefficients': [model_no_reg.coef_, model_l1.coef_, model_l2.coef_]
> })
>
> print(results)
> ```
>
> A tabela de resultados exibir√° a acur√°cia e os coeficientes do modelo. Observe como a regulariza√ß√£o L1 pode zerar alguns coeficientes, enquanto a L2 reduz a magnitude dos coeficientes sem zer√°-los completamente. A acur√°cia tamb√©m pode mudar, dependendo dos dados e da for√ßa da regulariza√ß√£o.

**Corol√°rio 3:** A esparsidade induzida pela penaliza√ß√£o L1 em modelos classificat√≥rios melhora a interpretabilidade, pois resulta em modelos que dependem apenas de um subconjunto das vari√°veis de entrada. Isso √© √∫til para entender quais features s√£o mais relevantes para a classifica√ß√£o e para simplificar modelos complexos [^4.4.5].

> ‚ö†Ô∏è **Ponto Crucial**: A escolha entre penaliza√ß√£o L1, L2 ou Elastic Net depende das caracter√≠sticas do problema. L1 favorece modelos esparsos, L2 favorece estabilidade e Elastic Net √© uma combina√ß√£o dos dois [^4.5].

### Separating Hyperplanes e Perceptrons

<imagem: Diagrama mostrando a representa√ß√£o geom√©trica de um hiperplano separador em um espa√ßo de caracter√≠sticas, com destaque para a margem de separa√ß√£o e os vetores de suporte>

O conceito de **hiperplanos separadores** est√° fundamentalmente ligado √† classifica√ß√£o linear, visando encontrar um hiperplano que separe as classes de forma otimizada [^4.5.2]. Essa ideia leva ao conceito de **margem de separa√ß√£o**, que √© a dist√¢ncia m√≠nima entre o hiperplano e os pontos de dados mais pr√≥ximos de cada classe [^4.5.2]. O objetivo √© maximizar essa margem para obter um classificador robusto e generaliz√°vel. A otimiza√ß√£o da margem pode ser feita atrav√©s da formula√ß√£o do problema dual de Wolfe e do uso de combina√ß√µes lineares dos pontos de suporte, que s√£o os pontos de dados mais pr√≥ximos do hiperplano.

O **Perceptron de Rosenblatt** [^4.5.1] √© um algoritmo simples que busca iterativamente um hiperplano que separa as classes, corrigindo seus par√¢metros a cada erro de classifica√ß√£o encontrado. O Perceptron tem como diferencial a sua atualiza√ß√£o iterativa da fronteira de decis√£o com base nos erros de classifica√ß√£o cometidos. Sua converg√™ncia √© garantida sob a condi√ß√£o de que os dados sejam linearmente separ√°veis.
Se os dados n√£o s√£o linearmente separ√°veis, o Perceptron pode n√£o convergir. Nestes casos, t√©cnicas como Regulariza√ß√£o e m√©todos de otimiza√ß√£o mais robustos s√£o necess√°rios.
```mermaid
graph TB
    subgraph "Perceptron Algorithm"
        direction TB
        A["Initialize: weights and bias"]
        B["For each sample x_i:"]
        C["Predict: yÃÇ_i = sign(w·µÄx_i + b)"]
        D["If yÃÇ_i ‚â† y_i:"]
        E["Update weights: w = w + Œ∑y_i x_i"]
        F["Update bias: b = b + Œ∑y_i"]
        A --> B
        B --> C
        C --> D
        D -- "Error" --> E
        E --> F
        D -- "Correct" --> B
    end
```

> üí° **Exemplo Num√©rico:** Para ilustrar o funcionamento do Perceptron, considere o seguinte conjunto de dados linearmente separ√°veis:
>
> | $x_1$ | $x_2$ | Classe |
> |-------|-------|--------|
> | 1     | 1     | -1     |
> | 2     | 1     | -1     |
> | 1     | 2     | -1     |
> | 3     | 3     | 1      |
> | 4     | 3     | 1      |
> | 4     | 4     | 1      |
>
> O Perceptron come√ßa com um hiperplano aleat√≥rio (representado por um vetor de pesos e um bias) e atualiza esses pesos a cada itera√ß√£o, at√© que todos os pontos sejam corretamente classificados. Abaixo, um exemplo de implementa√ß√£o:
>
> ```python
> import numpy as np
>
> def perceptron(X, y, learning_rate=0.1, epochs=100):
>     """Implementa√ß√£o do Perceptron."""
>     n_samples, n_features = X.shape
>     weights = np.zeros(n_features)
>     bias = 0
>     errors_history = []
>
>     for _ in range(epochs):
>         errors = 0
>         for i in range(n_samples):
>             y_predicted = np.sign(np.dot(X[i], weights) + bias)
>             if y_predicted != y[i]:
>                 weights += learning_rate * y[i] * X[i]
>                 bias += learning_rate * y[i]
>                 errors += 1
>         errors_history.append(errors)
>         if errors == 0:
>           break # Convergiu, dados linearmente separ√°veis
>     return weights, bias, errors_history
>
> X = np.array([[1, 1], [2, 1], [1, 2], [3, 3], [4, 3], [4, 4]])
> y = np.array([-1, -1, -1, 1, 1, 1])
>
> weights, bias, errors_history = perceptron(X, y)
> print("Pesos:", weights)
> print("Bias:", bias)
> print("Hist√≥rico de erros:", errors_history)
> ```
> O c√≥digo mostra a evolu√ß√£o dos pesos e bias do Perceptron a cada itera√ß√£o. O hist√≥rico de erros demonstra como o modelo aprende at√© convergir, ou seja, encontrar um hiperplano que separa as classes. Caso os dados n√£o sejam linearmente separ√°veis, o Perceptron pode n√£o convergir (isto √©, os erros n√£o se reduziriam a zero), sendo necess√°rio, nesses casos, um n√∫mero m√°ximo de itera√ß√µes.

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

**Resposta:**
A **Linear Discriminant Analysis (LDA)** e a **Regra de Decis√£o Bayesiana** s√£o ambas abordagens para classifica√ß√£o, mas diferem em seus pressupostos e como modelam a probabilidade de cada classe.  A LDA assume que as classes s√£o Gaussianas e que todas t√™m a mesma matriz de covari√¢ncia [^4.3].  Desta forma, as fronteiras de decis√£o s√£o lineares. A Regra de Decis√£o Bayesiana tamb√©m utiliza distribui√ß√µes Gaussianas para modelar as classes, mas n√£o requer covari√¢ncias iguais [^4.3], [^4.3.3].  Se a covari√¢ncia entre as classes for considerada igual, o discriminante de Bayes tamb√©m ser√° linear.

Sob a condi√ß√£o de que as covari√¢ncias sejam iguais (como na LDA), as fronteiras de decis√£o Bayesiana se tornam lineares, e se tornam equivalentes √†s da LDA [^4.3]. Isso ocorre porque o discriminante de Bayes se reduz a uma fun√ß√£o linear das caracter√≠sticas, similar ao discriminante de Fisher. No entanto, se as covari√¢ncias forem diferentes, a Regra de Decis√£o Bayesiana resulta em fronteiras quadr√°ticas, que s√£o modeladas por meio da **Quadratic Discriminant Analysis (QDA)**, que n√£o possui restri√ß√£o das covari√¢ncias serem iguais [^4.3.1].
```mermaid
graph LR
    subgraph "LDA vs Bayesian Decision Rule (Gaussian)"
        direction LR
         A["LDA: Gaussian with equal covariance"] --> B["Linear Decision Boundary"]
        C["Bayesian Decision: Gaussian"] --> D["Equal Covariances: Linear Boundary"]
        C --> E["Unequal Covariances: Quadratic Boundary"]
        D --> B
    end
```

**Lemma 4:** Em condi√ß√µes de covari√¢ncias iguais e distribui√ß√µes gaussianas, o discriminante de Bayes se torna equivalente a uma proje√ß√£o linear, como feito na LDA.
Formalmente, o discriminante de Bayes √© dado por:

$$ \delta_k(x) = - \frac{1}{2} (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) + log(\pi_k) $$

onde $\mu_k$ √© a m√©dia da classe k, $\Sigma$ √© a matriz de covari√¢ncia e $\pi_k$ √© a probabilidade a priori da classe k.  Quando as covari√¢ncias s√£o iguais ($\Sigma_k = \Sigma$ para todas as classes), o discriminante se reduz a:

$$ \delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + log(\pi_k) $$

que √© uma fun√ß√£o linear de $x$.  $\blacksquare$

> üí° **Exemplo Num√©rico:** Para ilustrar a diferen√ßa entre LDA e QDA, considere um conjunto de dados simulado com duas classes, em que uma classe possui uma vari√¢ncia maior do que a outra:
> ```python
> import numpy as np
> import pandas as pd
> import matplotlib.pyplot as plt
> from sklearn.model_selection import train_test_split
> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
> from sklearn.metrics import accuracy_score
>
> # Simular dados
> np.random.seed(0)
> n_samples = 200
> mean1 = [2, 2]
> cov1 = [[1, 0.5], [0.5, 1]]
> mean2 = [4, 4]
> cov2 = [[2, -0.8], [-0.8, 2]]
> X1 = np.random.multivariate_normal(mean1, cov1, n_samples//2)
> X2 = np.random.multivariate_normal(mean2, cov2, n_samples//2)
> X = np.vstack((X1, X2))
> y = np.array([0] * (n_samples//2) + [1] * (n_samples//2))
>
> # Dividir dados
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # LDA
> lda = LinearDiscriminantAnalysis()
> lda.fit(X_train, y_train)
> y_pred_lda = lda.predict(X_test)
> acc_lda = accuracy_score(y_test, y_pred_lda)
>
> # QDA
> qda = QuadraticDiscriminantAnalysis()
> qda.fit(X_train, y_train)
> y_pred_qda = qda.predict(X_test)
> acc_qda = accuracy_score(y_test, y_pred_qda)
>
> # Exibir Resultados
> print(f"Acur√°cia LDA: {acc_lda}")
> print(f"Acur√°cia QDA: {acc_qda}")
>
> # Criar gr√°ficos
> plt.figure(figsize=(8, 6))
> plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap