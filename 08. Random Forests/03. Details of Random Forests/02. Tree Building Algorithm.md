## √Årvores de Decis√£o e Constru√ß√£o de Random Forests
<imagem: Um diagrama de √°rvore complexo que ilustra a constru√ß√£o de uma √°rvore de decis√£o, desde a sele√ß√£o das vari√°veis e pontos de divis√£o at√© a forma√ß√£o dos n√≥s terminais. Os passos do algoritmo de Random Forest devem ser tamb√©m destacados.>

### Introdu√ß√£o
O conceito de **√°rvores de decis√£o**, e mais especificamente a constru√ß√£o de **Random Forests**, representam uma abordagem poderosa e flex√≠vel para modelagem preditiva, tanto em classifica√ß√£o quanto em regress√£o [^15.1]. A t√©cnica de *bagging*, ou *bootstrap aggregation* [^15.1], desempenha um papel crucial na redu√ß√£o da vari√¢ncia de modelos preditivos, e as √°rvores de decis√£o, conhecidas por sua alta vari√¢ncia e baixo vi√©s, s√£o candidatas ideais para este procedimento [^15.1]. Random forests, uma extens√£o sofisticada do bagging, constroem uma cole√ß√£o de √°rvores descorrelacionadas, resultando em um modelo robusto e com alta capacidade preditiva [^15.1]. Este cap√≠tulo explora em detalhes a constru√ß√£o dessas √°rvores, os mecanismos por tr√°s do Random Forest, e suas aplica√ß√µes pr√°ticas.

### Conceitos Fundamentais

**Conceito 1: Bagging e a Redu√ß√£o da Vari√¢ncia**
O *bagging* √© uma t√©cnica que visa reduzir a vari√¢ncia de modelos preditivos por meio da agrega√ß√£o de m√∫ltiplas vers√µes do modelo ajustadas em amostras bootstrap [^15.1]. Em √°rvores de decis√£o, essa t√©cnica consiste em criar v√°rias √°rvores, cada uma treinada em uma amostra bootstrap do conjunto de dados original, e ent√£o agregar suas previs√µes. A ideia central √© que, ao promediar muitos modelos ruidosos, mas aproximadamente n√£o viesados, a vari√¢ncia do modelo preditivo final diminui significativamente [^15.2]. Para problemas de regress√£o, as predi√ß√µes s√£o simplesmente promediadas, enquanto para classifica√ß√£o, as √°rvores "votam" na classe predita [^15.1].

**Lemma 1:** Se temos *B* √°rvores de decis√£o i.i.d. com vari√¢ncia $\sigma^2$, a vari√¢ncia da m√©dia das predi√ß√µes √© $\frac{\sigma^2}{B}$. Quando as √°rvores n√£o s√£o i.i.d., mas apenas identicamente distribu√≠das com correla√ß√£o par-a-par $\rho$, a vari√¢ncia da m√©dia √© dada por $\rho\sigma^2 + \frac{1-\rho}{B}\sigma^2$ [^15.2].
$$ Var\left(\frac{1}{B}\sum_{b=1}^B T_b(x)\right) = \rho\sigma^2 + \frac{1-\rho}{B}\sigma^2 $$
*Prova:* A vari√¢ncia da soma de vari√°veis aleat√≥rias √© dada por
$$Var(\sum_{b=1}^B T_b(x)) = \sum_{b=1}^B Var(T_b(x)) + 2\sum_{i < j} Cov(T_i(x), T_j(x)).$$
Se as √°rvores s√£o i.i.d., $Var(T_b(x)) = \sigma^2$ e $Cov(T_i(x), T_j(x)) = 0$. Portanto, a vari√¢ncia da m√©dia, $\frac{1}{B}\sum_{b=1}^B T_b(x)$, √© $\frac{\sigma^2}{B}$. Se elas s√£o identicamente distribu√≠das com correla√ß√£o $\rho$, $Cov(T_i(x), T_j(x)) = \rho\sigma^2$ e temos $$ Var\left(\frac{1}{B}\sum_{b=1}^B T_b(x)\right) = \frac{1}{B^2} (B\sigma^2 + B(B-1)\rho\sigma^2) = \frac{\sigma^2}{B} + \frac{B-1}{B} \rho\sigma^2 \approx \rho\sigma^2 + \frac{1-\rho}{B}\sigma^2 .$$
$\blacksquare$
```mermaid
graph TD
    subgraph "Variance Decomposition with Bagging"
    direction TB
        A["Variance of average prediction: Var(1/B * Œ£ Tb(x))"]
        B["Trees are i.i.d.: Var(1/B * Œ£ Tb(x)) = œÉ¬≤/B"]
        C["Trees are identically distributed with correlation œÅ"]
        D["Variance with correlation: Var(1/B * Œ£ Tb(x)) = œÅœÉ¬≤ + (1-œÅ)œÉ¬≤/B"]
        A --> B
        A --> C
        C --> D
    end
```

> üí° **Exemplo Num√©rico:** Vamos supor que temos 10 √°rvores de decis√£o (*B* = 10), cada uma com vari√¢ncia nas suas predi√ß√µes de $\sigma^2 = 4$.
>
> 1. **Caso i.i.d.:** Se as √°rvores s√£o independentes e identicamente distribu√≠das (i.i.d.), a vari√¢ncia da m√©dia das predi√ß√µes √© $\frac{\sigma^2}{B} = \frac{4}{10} = 0.4$. Isso mostra que a m√©dia das predi√ß√µes √© menos vari√°vel do que as predi√ß√µes individuais.
>
> 2. **Caso com Correla√ß√£o:** Se as √°rvores s√£o identicamente distribu√≠das, mas com uma correla√ß√£o par-a-par de $\rho = 0.2$, a vari√¢ncia da m√©dia √© $\rho\sigma^2 + \frac{1-\rho}{B}\sigma^2 = 0.2 * 4 + \frac{1-0.2}{10} * 4 = 0.8 + 0.32 = 1.12$. Aqui, a vari√¢ncia √© maior devido √† correla√ß√£o entre as √°rvores, o que demonstra como a correla√ß√£o prejudica a redu√ß√£o da vari√¢ncia.
>
> Este exemplo ilustra numericamente como o *bagging* reduz a vari√¢ncia quando as √°rvores s√£o independentes, e como a correla√ß√£o entre as √°rvores limita essa redu√ß√£o. Random Forests buscam descorrelacionar as √°rvores para obter o m√°ximo benef√≠cio do *bagging*.

**Conceito 2: Random Forests e a Descorrela√ß√£o de √Årvores**
Random Forests, introduzido por Breiman em 2001 [^15.1], aprimora o *bagging* atrav√©s da descorrela√ß√£o das √°rvores. O processo de constru√ß√£o de cada √°rvore envolve uma sele√ß√£o aleat√≥ria de vari√°veis em cada n√≥ de divis√£o. Em vez de considerar todas as *p* vari√°veis de entrada para a divis√£o, apenas um subconjunto *m ‚â§ p* √© considerado [^15.2]. Este processo reduz a correla√ß√£o entre as √°rvores, o que √© essencial para obter uma redu√ß√£o significativa da vari√¢ncia [^15.2]. A escolha de *m* √© um hiperpar√¢metro importante que afeta o desempenho do modelo [^15.3].
```mermaid
graph LR
    subgraph "Random Forest Variable Selection"
    direction LR
        A["At each node split"] --> B["Randomly select 'm' variables (m ‚â§ p)"]
         B --> C["Evaluate split on 'm' variables"]
        C --> D["Split based on best variable in 'm'"]
    end
```

**Corol√°rio 1:** Ao selecionar um n√∫mero *m* de vari√°veis em cada n√≥, o limite da redu√ß√£o da vari√¢ncia pelo m√©todo de bagging √© aumentado. A descorrela√ß√£o entre as √°rvores geradas pelo Random Forest leva a uma redu√ß√£o da vari√¢ncia maior em rela√ß√£o ao bagging [^15.2]. O termo $\rho\sigma^2$  na vari√¢ncia da m√©dia de um conjunto de *B* √°rvores identicamente distribu√≠das diminui, levando a um melhor resultado em termos de redu√ß√£o da vari√¢ncia.

**Conceito 3: Constru√ß√£o de uma √Årvore de Random Forest**
O processo de constru√ß√£o de uma √°rvore de Random Forest envolve os seguintes passos [^15.2]:
1.  **Amostragem Bootstrap:** Criar uma amostra bootstrap *Z*** do tamanho *N* do conjunto de dados de treino.
2.  **Crescimento da √Årvore:** Para cada n√≥ terminal da √°rvore:
    *   Selecionar aleatoriamente *m* vari√°veis dentre as *p* vari√°veis dispon√≠veis.
    *   Escolher a melhor vari√°vel e ponto de divis√£o entre as *m* vari√°veis selecionadas.
    *   Dividir o n√≥ em dois n√≥s filhos.
3.  **Repetir o processo** at√© atingir um tamanho m√≠nimo de n√≥ *nmin*.
4.  **Ensemble de √Årvores:** O resultado final √© um conjunto de *B* √°rvores {T<sub>b</sub>}<sub>1</sub><sup>B</sup>.

Para classifica√ß√£o, cada √°rvore prediz uma classe, e a classe final √© determinada por voto majorit√°rio. Para regress√£o, a predi√ß√£o final √© obtida pela m√©dia das predi√ß√µes de todas as √°rvores [^15.2].

> üí° **Exemplo Num√©rico:** Considere um dataset com *p* = 5 vari√°veis (A, B, C, D, E) e queremos construir uma √°rvore de Random Forest.
>
> 1.  **Amostragem Bootstrap:** Uma amostra bootstrap do conjunto de dados √© criada (com poss√≠vel repeti√ß√£o de inst√¢ncias).
> 2.  **Crescimento da √Årvore:**
>     *   **N√≥ Raiz:** Selecionamos aleatoriamente *m* = 2 vari√°veis, digamos, B e D. Avaliamos qual dessas vari√°veis proporciona a melhor divis√£o, vamos supor que seja a vari√°vel D no ponto de corte 0.5. O n√≥ raiz √© ent√£o dividido com base em D <= 0.5.
>     *   **N√≥ Filho Esquerdo:** Em seguida, para o n√≥ filho esquerdo (D <= 0.5), selecionamos novamente *m* = 2 vari√°veis aleatoriamente, digamos A e C. Avaliamos a melhor divis√£o e continuamos o processo recursivamente.
>     *   **N√≥ Filho Direito:**  Similarmente, no n√≥ filho direito (D > 0.5) selecionamos outras 2 vari√°veis aleatoriamente e prosseguimos.
> 3.  **Repeti√ß√£o:** O processo √© repetido at√© que cada n√≥ tenha um n√∫mero m√≠nimo de amostras *nmin*.
> 4.  **Ensemble de √Årvores:** O processo √© repetido para *B* = 100 √°rvores.
>
> Este exemplo ilustra como cada √°rvore √© constru√≠da com amostras bootstrap e sele√ß√£o aleat√≥ria de vari√°veis, resultando em um ensemble diversificado e descorrelacionado.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o
<imagem: Diagrama de fluxo detalhando o processo de constru√ß√£o de √°rvores no Random Forest, desde o bootstrap at√© a predi√ß√£o final. Incluir os passos de sele√ß√£o aleat√≥ria de vari√°veis em cada split e a agrega√ß√£o das predi√ß√µes.>
```mermaid
flowchart TD
    A[Conjunto de Dados] --> B(Bootstrap);
    B --> C{Selecionar Subconjunto de Vari√°veis (m)};
    C --> D[Encontrar Melhor Split];
    D --> E{N√≥ Terminal?};
    E -- Sim --> F[Adicionar N√≥ √† √Årvore];
    E -- N√£o --> C;
    F --> G{Repetir para B √Årvores};
     G --> H[Agrega√ß√£o de Predi√ß√µes];
     H --> I[Predi√ß√£o Final];
```
A regress√£o linear, quando utilizada diretamente para classifica√ß√£o atrav√©s da regress√£o em uma matriz de indicadores, pode apresentar limita√ß√µes devido √† sua natureza linear. No entanto, o conceito de minimiza√ß√£o dos quadrados dos res√≠duos √© um princ√≠pio fundamental no treinamento de muitos modelos, incluindo √°rvores de decis√£o e suas varia√ß√µes [^15.1].
A regress√£o de indicadores pode ser entendida como uma aproxima√ß√£o linear da fun√ß√£o de decis√£o, onde os coeficientes s√£o estimados por m√≠nimos quadrados. Embora seja simples de implementar, essa abordagem pode ser inadequada quando a rela√ß√£o entre as vari√°veis de entrada e as classes √© n√£o linear [^15.1]. A complexidade de modelos como √°rvores de decis√£o e random forests permite o aprendizado de rela√ß√µes n√£o lineares.

**Lemma 2:** A regress√£o linear aplicada a uma matriz de indicadores busca encontrar hiperplanos que separam as classes. Se as classes forem linearmente separ√°veis, a regress√£o linear pode funcionar bem. No entanto, para problemas n√£o linearmente separ√°veis, a regress√£o linear pode gerar limites de decis√£o inadequados, com altas taxas de erro [^15.2]. √Årvores, por sua vez, usam parti√ß√µes lineares (ortogonais), mas s√£o capazes de aproximar limites n√£o lineares por meio da combina√ß√£o de m√∫ltiplos splits.
```mermaid
graph LR
    subgraph "Linear Regression for Classification"
    direction LR
        A["Input Data"] --> B["Indicator Matrix"]
         B --> C["Linear Regression: find hyperplane"]
        C --> D["Decision Boundary"]
    end
```

*Prova:* A regress√£o linear na forma matricial √© dada por  $\hat{\beta} = (X^T X)^{-1}X^Ty$ onde $X$ √© a matriz de design e $y$ o vetor de respostas. Se $y$ for uma codifica√ß√£o bin√°ria de classes (por exemplo, 0 e 1), o resultado √© um hiperplano de separa√ß√£o linear. Para dados n√£o linearmente separ√°veis, esse hiperplano pode n√£o se adequar bem √†s separa√ß√µes das classes. √Årvores, por outro lado, constroem parti√ß√µes retangulares que aproximam regi√µes de decis√£o n√£o lineares.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos um problema de classifica√ß√£o bin√°ria com duas vari√°veis preditoras, *x1* e *x2*, e duas classes, 0 e 1. As inst√¢ncias s√£o distribu√≠das de forma que n√£o podem ser separadas por uma √∫nica linha reta.
>
> 1.  **Regress√£o Linear:** Ao aplicar regress√£o linear diretamente, o modelo pode gerar um hiperplano (neste caso, uma linha) que tenta separar as classes, mas comete muitos erros, j√° que os dados n√£o s√£o linearmente separ√°veis. O resultado √© um limite de decis√£o linear, que n√£o captura a complexidade dos dados.
> 2.  **√Årvore de Decis√£o:** Uma √°rvore de decis√£o, por outro lado, pode criar parti√ß√µes retangulares no espa√ßo, dividindo o espa√ßo em regi√µes mais complexas. Por exemplo, um primeiro split pode dividir os dados com base em *x1* <= 0.5, criando dois n√≥s filhos. Cada um desses n√≥s pode ser dividido novamente, resultando em um limite de decis√£o n√£o linear, que se ajusta melhor aos dados.
>
> Este exemplo demonstra a limita√ß√£o da regress√£o linear em problemas n√£o linearmente separ√°veis e como √°rvores de decis√£o podem aproximar limites de decis√£o mais complexos.

**Corol√°rio 2:** O Random Forest, ao combinar m√∫ltiplas √°rvores treinadas em diferentes subespa√ßos de vari√°veis e subamostras, pode contornar as limita√ß√µes impostas pela linearidade da regress√£o de indicadores. Isso leva a fronteiras de decis√£o mais complexas e adapt√°veis, melhorando a precis√£o do modelo em problemas de classifica√ß√£o mais desafiadores [^15.1]. Random forests usam *bagging* e sele√ß√£o aleat√≥ria de vari√°veis para reduzir correla√ß√£o entre √°rvores e aumentar a robustez do modelo.

> ‚ö†Ô∏è **Nota Importante**: A regress√£o linear na matriz de indicadores pode levar a problemas de extrapola√ß√£o fora do intervalo [0,1] para as probabilidades, o que n√£o ocorre na regress√£o log√≠stica ou em √°rvores, que se adaptam aos dados de forma mais eficaz. [^15.1]

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

A sele√ß√£o aleat√≥ria de vari√°veis no Random Forest √© um tipo de regulariza√ß√£o. Ao selecionar um subconjunto de *m* vari√°veis em cada split, o modelo evita o overfitting, j√° que cada √°rvore n√£o utiliza todas as vari√°veis [^15.2]. Essa abordagem reduz a correla√ß√£o entre as √°rvores, que √© fundamental para o desempenho do Random Forest.

A regulariza√ß√£o, em geral, visa penalizar a complexidade do modelo, reduzindo a vari√¢ncia, mas √†s vezes aumentando o vi√©s. Em modelos como a regress√£o log√≠stica, a regulariza√ß√£o L1 e L2 adicionam termos de penaliza√ß√£o √† fun√ß√£o de custo, controlando o tamanho dos coeficientes e evitando o overfitting [^15.1].

**Lemma 3:** A sele√ß√£o de um subconjunto aleat√≥rio de vari√°veis *m*, com *m < p*, em cada split da √°rvore, introduz uma forma de regulariza√ß√£o que evita o overfitting e reduz a vari√¢ncia do modelo final. Isso ocorre pois as √°rvores se tornam mais descorrelacionadas e o resultado da m√©dia √© mais est√°vel [^15.2].
```mermaid
graph TB
 subgraph "Regularization by Random Subspace"
  direction TB
    A["Random Forest: Select 'm' < 'p' variables"]
    B["Each tree sees a subspace of the feature set"]
    C["Reduced correlation between trees"]
    D["Reduced model variance"]
    A --> B
    B --> C
    C --> D
 end
```
*Prova:*  A vari√¢ncia da m√©dia das predi√ß√µes de *B* √°rvores √© dada por $Var(\frac{1}{B}\sum_{b=1}^B T_b(x)) = \rho\sigma^2 + \frac{1-\rho}{B}\sigma^2$. Reduzir a correla√ß√£o $\rho$ entre as √°rvores leva a uma diminui√ß√£o na vari√¢ncia do modelo agregado. A sele√ß√£o aleat√≥ria de vari√°veis *m*, onde $m < p$, for√ßa as √°rvores a aprender diferentes aspectos dos dados, reduzindo $\rho$ e, portanto, a vari√¢ncia final. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos um dataset com *p* = 10 vari√°veis e constru√≠mos um Random Forest com 100 √°rvores (*B* = 100).
>
> 1.  **Sele√ß√£o de Vari√°veis:** Se escolhermos *m* = 2 vari√°veis em cada n√≥, a cada split, a √°rvore ter√° acesso apenas a uma pequena parte da informa√ß√£o, for√ßando-a a aprender rela√ß√µes espec√≠ficas nos dados. Isso cria √°rvores mais diversas e descorrelacionadas, diminuindo a vari√¢ncia do modelo final.
> 2.  **Compara√ß√£o:** Se escolhermos *m* = 9 vari√°veis, as √°rvores seriam muito mais semelhantes entre si, pois teriam acesso a quase todas as informa√ß√µes. A correla√ß√£o entre elas aumentaria, diminuindo o efeito do *bagging* e n√£o reduzindo a vari√¢ncia na mesma magnitude.
>
> Este exemplo ilustra como a escolha de *m* afeta a descorrela√ß√£o entre as √°rvores e, portanto, a vari√¢ncia do modelo. Valores menores de *m* promovem maior regulariza√ß√£o, descorrelacionando as √°rvores.

**Corol√°rio 3:** Em Random Forests, a escolha do hiperpar√¢metro *m* tem um efeito direto na complexidade e desempenho do modelo. Valores menores de *m* aumentam a descorrela√ß√£o entre as √°rvores, o que reduz a vari√¢ncia, mas pode aumentar o vi√©s. Valores maiores de *m*, por outro lado, diminuem a descorrela√ß√£o, levando a uma vari√¢ncia maior, mas menor vi√©s. A sele√ß√£o de *m* √© um compromisso entre vi√©s e vari√¢ncia, que deve ser ajustado de acordo com a complexidade do problema [^15.2]. Em geral, um valor $m=\sqrt{p}$ √© uma boa escolha para problemas de classifica√ß√£o, e *m = p/3* para regress√£o [^15.3].

> ‚ùó **Ponto de Aten√ß√£o**: A escolha do valor de *m* (n√∫mero de vari√°veis selecionadas em cada split) √© crucial para o desempenho do Random Forest. Um valor muito baixo pode levar a um modelo de alto vi√©s, enquanto um valor muito alto pode resultar em √°rvores correlacionadas e menor redu√ß√£o da vari√¢ncia [^15.2].

### Separating Hyperplanes e Perceptrons

A ideia de encontrar hiperplanos √≥timos para separa√ß√£o de classes √© fundamental em muitos algoritmos de classifica√ß√£o. Em Random Forests, embora cada split seja um hiperplano no espa√ßo das vari√°veis selecionadas, a combina√ß√£o de v√°rios splits em diferentes √°rvores leva a um particionamento muito mais complexo do espa√ßo de entrada [^15.2]. O Perceptron de Rosenblatt, um m√©todo simples de classifica√ß√£o linear, busca um hiperplano que separa linearmente as classes. Random Forests, ao contr√°rio, usam uma estrat√©gia n√£o linear, construindo parti√ß√µes hier√°rquicas do espa√ßo de entrada [^15.2].

### Pergunta Te√≥rica Avan√ßada (Exemplo): Como a escolha do hiperpar√¢metro *m* influencia o vi√©s e a vari√¢ncia no Random Forest?
**Resposta:**
O hiperpar√¢metro *m*, que controla o n√∫mero de vari√°veis selecionadas aleatoriamente em cada split, tem um efeito crucial no compromisso entre vi√©s e vari√¢ncia em Random Forests [^15.2]. Um valor baixo de *m* aumenta a descorrela√ß√£o entre as √°rvores, o que leva a uma redu√ß√£o na vari√¢ncia. No entanto, ao limitar as vari√°veis dispon√≠veis para cada divis√£o, o modelo se torna mais restrito, o que pode aumentar o vi√©s. Por outro lado, um valor alto de *m* faz com que cada √°rvore tenha acesso a mais informa√ß√£o, resultando em √°rvores mais correlacionadas e uma redu√ß√£o menor da vari√¢ncia, mas com um vi√©s menor.
```mermaid
graph LR
    subgraph "Bias-Variance Tradeoff in Random Forest"
     direction LR
        A["Low 'm'"] --> B["High tree diversity"]
        A --> C["High bias"]
        B --> D["Low variance"]
         E["High 'm'"] --> F["Low tree diversity"]
          E --> G["Low Bias"]
          F --> H["High variance"]
    end
```

**Lemma 4:** Em Random Forests, a vari√¢ncia do estimador √© influenciada pela correla√ß√£o entre as √°rvores, conforme dado por $Var(\frac{1}{B}\sum_{b=1}^B T_b(x)) = \rho\sigma^2 + \frac{1-\rho}{B}\sigma^2$ [^15.2]. Reduzir o valor de *m* diminui a correla√ß√£o $\rho$, diminuindo a vari√¢ncia total do modelo. No entanto, o vi√©s do modelo tamb√©m aumenta.

*Prova:* √Årvores com *m* pequeno, selecionando poucas vari√°veis, s√£o menos propensas a capturar as rela√ß√µes existentes entre as vari√°veis relevantes. O aumento do vi√©s √© devido √† maior restri√ß√£o nas vari√°veis dispon√≠veis a cada n√≥ de divis√£o, levando a modelos mais simples. Com *m* maior, as √°rvores se tornam mais complexas, com menos vi√©s, por√©m mais correlacionadas.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Considere novamente o dataset com *p* = 10 vari√°veis e o objetivo de construir um Random Forest.
>
> 1.  **Baixo *m* (ex: *m* = 1):** Ao usar *m* = 1, cada √°rvore s√≥ pode usar uma vari√°vel para cada split. Isso torna as √°rvores muito simples e pouco adapt√°veis a rela√ß√µes complexas nos dados, resultando em alto vi√©s. No entanto, como as √°rvores s√£o muito diferentes entre si, a correla√ß√£o entre elas √© baixa, reduzindo a vari√¢ncia do conjunto final de √°rvores.
> 2.  **Alto *m* (ex: *m* = 9):** Ao usar *m* = 9, as √°rvores s√£o muito mais parecidas entre si, pois em cada split elas podem usar quase todas as vari√°veis. Isso leva a um vi√©s menor, pois as √°rvores podem modelar rela√ß√µes mais complexas, mas como as √°rvores s√£o mais correlacionadas, a vari√¢ncia do modelo final √© maior.
>
> Este exemplo ilustra o compromisso entre vi√©s e vari√¢ncia ao variar o hiperpar√¢metro *m*. Valores muito baixos aumentam o vi√©s, enquanto valores muito altos aumentam a vari√¢ncia, sendo necess√°rio encontrar um valor ideal que equilibre os dois.

**Corol√°rio 4:** A escolha de *m* √© um problema de otimiza√ß√£o que depende da complexidade do problema. Em problemas com muitas vari√°veis irrelevantes, um valor menor de *m* pode ser prefer√≠vel para evitar overfitting. Em problemas com muitas vari√°veis relevantes, um valor maior de *m* pode levar a um melhor desempenho [^15.2]. A sele√ß√£o √≥tima de *m* frequentemente envolve a avalia√ß√£o do desempenho do modelo em um conjunto de valida√ß√£o e pode ser obtida por m√©todos como valida√ß√£o cruzada [^15.1].

> ‚úîÔ∏è **Destaque**: Em geral, a escolha de $m=\sqrt{p}$ para classifica√ß√£o e $m=\frac{p}{3}$ para regress√£o s√£o boas heur√≠sticas, mas ajustes podem ser necess√°rios para cada problema espec√≠fico [^15.3].

### Conclus√£o

Random Forests representam uma ferramenta poderosa no campo do aprendizado de m√°quina, oferecendo uma solu√ß√£o robusta para problemas de classifica√ß√£o e regress√£o [^15.1]. Atrav√©s da combina√ß√£o de *bagging* com sele√ß√£o aleat√≥ria de vari√°veis, Random Forests s√£o capazes de construir modelos com alta precis√£o e generaliza√ß√£o, mesmo em conjuntos de dados complexos e com alta dimensionalidade. A capacidade de lidar com dados n√£o lineares e a robustez ao overfitting tornam o Random Forest uma das abordagens mais populares e eficazes para modelagem preditiva [^15.1].

### Footnotes
[^15.1]: "Bagging or bootstrap aggregation (section 8.7) is a technique for reducing the variance of an estimated prediction function. Bagging seems to work especially well for high-variance, low-bias procedures, such as trees. For regression, we simply fit the same regression tree many times to bootstrap-sampled versions of the training data, and average the result. For classification, a committee of trees each cast a vote for the predicted class. Boosting in Chapter 10 was initially proposed as a committee method as well, although unlike bagging, the committee of weak learners evolves over time, and the members cast a weighted vote. Boosting appears to dominate bagging on most problems, and became the preferred choice. Random forests (Breiman, 2001) is a substantial modification of bagging that builds a large collection of de-correlated trees, and then averages them. On many problems the performance of random forests is very similar to boosting, and they are simpler to train and tune. As a consequence, random forests are popular, and are implemented in a variety of packages." *[Trecho de Random Forests]*

[^15.2]: "The essential idea in bagging (Section 8.7) is to average many noisy but approximately unbiased models, and hence reduce the variance. Trees are ideal candidates for bagging, since they can capture complex interaction structures in the data, and if grown sufficiently deep, have relatively low bias. Since trees are notoriously noisy, they benefit greatly from the averaging. Moreover, since each tree generated in bagging is identically distributed (i.d.), the expectation of an average of B such trees is the same as the expectation of any one of them. This means the bias of bagged trees is the same as that of the individual trees, and the only hope of improvement is through variance reduction. This is in contrast to boosting, where the trees are grown in an adaptive way to remove bias, and hence are not i.d. An average of B i.i.d. random variables, each with variance œÉ¬≤, has variance œÉ¬≤/B. If the variables are simply i.d. (identically distributed, but not necessarily independent) with positive pairwise correlation p, the variance of the average is (Exercise 15.1)... As B increases, the second term disappears, but the first remains, and hence the size of the correlation of pairs of bagged trees limits the benefits of averaging. The idea in random forests (Algorithm 15.1) is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much. This is achieved in the tree-growing process through random selection of the input variables. Specifically, when growing a tree on a bootstrapped dataset: Before each split, select m ‚â§ p of the input variables at random as candidates for splitting." *[Trecho de Random Forests]*

[^15.3]: "Typically values for m are ‚àöp or even as low as 1. After B such trees {T(x; Œò‚ÇÅ)}B b=1 are grown, the random forest (regression) predictor is ...For classification, the default value for m is [‚àöp] and the minimum node size is one. For regression, the default value for m is [p/3] and the minimum node size is five." *[Trecho de Random Forests]*

<!-- END DOCUMENT -->
