## Random Forests: An In-Depth Analysis of Stopping Rules

```mermaid
graph LR
 subgraph "Random Forest Construction Process"
  A["Bootstrap Sampling"] --> B("Random Subspace Selection (m)")
  B --> C("Decision Tree Construction")
  C --> D("OOB Error Calculation")
  D --> E("Variable Importance")
  E --> F("Aggregation of Trees")
  F --> G("Prediction")
 end
```

### Introdu√ß√£o

As **Random Forests** (RF) surgem como um avan√ßo substancial sobre a t√©cnica de *bagging*, construindo uma cole√ß√£o de √°rvores de decis√£o descorrelacionadas e agregando suas previs√µes [^15.1]. A robustez e relativa simplicidade no treinamento e ajuste tornam as RFs uma ferramenta popular em diversas √°reas. Um aspecto fundamental na constru√ß√£o de uma RF √© a defini√ß√£o de **stopping rules**, que determinam quando o crescimento de uma √°rvore deve parar. O objetivo principal deste cap√≠tulo √© examinar as nuances das regras de parada (stopping rules) em florestas aleat√≥rias, explorando tanto os aspectos te√≥ricos quanto os pr√°ticos.

### Conceitos Fundamentais

**Conceito 1: Bagging e √Årvores de Decis√£o**
O *bagging* (bootstrap aggregating) √© uma t√©cnica para reduzir a vari√¢ncia de uma fun√ß√£o de predi√ß√£o estimada [^15.1]. Em regress√£o, o bagging ajusta v√°rias √°rvores aos dados de treinamento, que s√£o amostrados com reposi√ß√£o (bootstrap), e calcula a m√©dia das previs√µes [^15.1]. Para classifica√ß√£o, cada √°rvore vota na classe predita e a decis√£o final √© determinada por maioria [^15.1]. √Årvores de decis√£o s√£o candidatas ideais para bagging devido √† sua capacidade de capturar intera√ß√µes complexas nos dados, mas tamb√©m s√£o notavelmente ruidosas [^15.2].

**Lemma 1:** *A expectativa da m√©dia de B √°rvores geradas pelo bagging √© a mesma que a expectativa de uma √∫nica √°rvore, indicando que o bagging foca em redu√ß√£o de vari√¢ncia e n√£o de bias* [^15.2].
**Prova do Lemma 1:** Seja $T_b(x)$ a predi√ß√£o da b-√©sima √°rvore de decis√£o, onde $b = 1, \ldots, B$. Se as √°rvores s√£o i.i.d., ent√£o $E[T_b(x)] = E[T(x)]$, para qualquer $b$. A predi√ß√£o do bagging √© $\frac{1}{B} \sum_{b=1}^B T_b(x)$. Portanto, $E[\frac{1}{B} \sum_{b=1}^B T_b(x)] = \frac{1}{B} \sum_{b=1}^B E[T_b(x)] = \frac{1}{B} \sum_{b=1}^B E[T(x)] = E[T(x)]$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um conjunto de dados de regress√£o com uma √∫nica vari√°vel preditora $x$ e uma vari√°vel resposta $y$. Suponha que ajustamos 3 √°rvores de decis√£o usando bagging. As previs√µes das √°rvores para um novo ponto $x = 5$ s√£o $T_1(5) = 8$, $T_2(5) = 10$, e $T_3(5) = 9$. A previs√£o do bagging seria a m√©dia dessas previs√µes: $\frac{8 + 10 + 9}{3} = 9$. Se a expectativa (m√©dia de infinitas √°rvores) de cada √°rvore √© $E[T(5)] = 9.2$, ent√£o a expectativa da previs√£o do bagging tamb√©m seria $9.2$, ilustrando que o bagging n√£o reduz o bias.

**Conceito 2: Random Forests - Descorrela√ß√£o e Sele√ß√£o Aleat√≥ria de Vari√°veis**
As Random Forests, introduzidas por Breiman, s√£o uma modifica√ß√£o do bagging que constr√≥i √°rvores descorrelacionadas [^15.1]. O principal mecanismo para essa descorrela√ß√£o √© a sele√ß√£o aleat√≥ria de vari√°veis [^15.2]. Antes de cada divis√£o (split) de n√≥, apenas um subconjunto de *m* vari√°veis √© considerado como poss√≠vel candidato para realizar a divis√£o, onde $m \leq p$ (sendo *p* o n√∫mero total de vari√°veis) [^15.2]. Valores t√≠picos de *m* s√£o $\sqrt{p}$ para classifica√ß√£o e $p/3$ para regress√£o, ou mesmo t√£o baixo quanto 1, dependendo do problema [^15.2]. Essa abordagem reduz a correla√ß√£o entre as √°rvores, melhorando a redu√ß√£o de vari√¢ncia.

```mermaid
graph LR
 subgraph "Random Forest Descorrelation"
  A["Bagging (correlated trees)"] --> B["Random Feature Selection (m)"]
  B --> C["Reduced Correlation Between Trees"]
  C --> D["Improved Variance Reduction"]
 end
```

**Corol√°rio 1:** *A redu√ß√£o de *m* (n√∫mero de vari√°veis candidatas a split) reduz a correla√ß√£o entre √°rvores na Random Forest, conforme indicado por [^15.2], levando a uma menor vari√¢ncia da predi√ß√£o agregada*.
**Prova do Corol√°rio 1:** A equa√ß√£o (15.1) demonstra que a vari√¢ncia da m√©dia de B √°rvores est√° relacionada a uma parcela referente √† vari√¢ncia de uma √∫nica √°rvore ( $\frac{\sigma^2}{B}$ ) e uma referente √† correla√ß√£o entre elas ( $ \rho \sigma^2$ ). Reduzindo $m$, diminu√≠mos a correla√ß√£o ($\rho$) entre as √°rvores, levando a uma vari√¢ncia menor da m√©dia, conforme  observado na equa√ß√£o. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos um conjunto de dados com 9 vari√°veis preditoras ($p=9$). Em uma Random Forest, se definirmos $m = \sqrt{p} = \sqrt{9} = 3$, para cada divis√£o de n√≥, apenas 3 vari√°veis ser√£o consideradas aleatoriamente. Isso faz com que as √°rvores usem diferentes conjuntos de vari√°veis para suas divis√µes, o que reduz a correla√ß√£o entre elas. Por exemplo, a √°rvore 1 pode usar as vari√°veis 1, 3 e 5 em um split, enquanto a √°rvore 2 pode usar as vari√°veis 2, 4 e 7 no mesmo n√≠vel da √°rvore. Essa diferen√ßa resulta em √°rvores mais independentes. Se $m = 9$, todas as √°rvores teriam a chance de usar as mesmas vari√°veis em cada n√≥, aumentando a correla√ß√£o entre elas.

**Conceito 3: Regras de Parada (Stopping Rules) e o Tamanho do N√≥**
As regras de parada determinam quando o crescimento de uma √°rvore deve ser interrompido. Uma regra comum √© definir um tamanho m√≠nimo de n√≥ (nmin) [^15.2]. Uma √°rvore continua a ser dividida at√© que cada n√≥ terminal tenha um tamanho inferior a *nmin* [^15.2]. Em classifica√ß√£o, o valor padr√£o para *nmin* √© geralmente 1, enquanto em regress√£o √© 5 [^15.3]. √â crucial notar que o melhor valor para *nmin* pode variar dependendo do problema espec√≠fico [^15.3]. Um valor de *nmin* muito pequeno pode levar a √°rvores muito profundas e overfiting, enquanto um valor muito grande pode levar a underfitting.

> üí° **Exemplo Num√©rico:** Imagine uma √°rvore de decis√£o para um problema de regress√£o. Se definirmos $n_{min} = 5$, a √°rvore parar√° de se dividir quando um n√≥ tiver 5 ou menos amostras de treinamento. Se um n√≥ contiver 10 amostras, ele tentar√° ser dividido; se um n√≥ contiver 3 amostras, ele se tornar√° uma folha. Se definirmos $n_{min} = 1$, a √°rvore continuar√° se dividindo at√© que cada folha contenha apenas uma amostra, o que pode levar a um modelo com overfitting.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o
```mermaid
flowchart TD
  subgraph "Indicator Regression Process"
    A["Encode Classes into Indicators"] --> B["Estimate Coefficients via Least Squares (LS)"]
    B --> C["Apply Decision Rule based on LS"]
    C --> D["Limitation: Potential Extrapolation Issues"]
  end
```
**Explica√ß√£o:** Este diagrama representa o fluxo do processo de regress√£o de indicadores aplicada na classifica√ß√£o, expondo a limita√ß√£o relacionada √† extrapola√ß√£o, ao inv√©s de focar em m√©todos probabil√≠sticos [^15.2].

Embora a regress√£o linear na matriz de indicadores n√£o seja o foco principal da Random Forest, √© crucial entender seu papel no contexto da classifica√ß√£o. A regress√£o linear busca encontrar um hiperplano que separe as classes ao minimizar a soma dos quadrados dos erros [^4.2]. No contexto de √°rvores de decis√£o, o ajuste √© feito localmente em cada n√≥ da √°rvore, onde um crit√©rio de divis√£o √© usado para maximizar a separa√ß√£o dos dados em diferentes classes [^15.2].

**Lemma 2:** Em √°rvores de decis√£o, a regra de parada, relacionada ao tamanho m√≠nimo de n√≥ (nmin), define o qu√£o local o ajuste √© feito. *nmin* pequeno permite mais divis√µes, consequentemente um ajuste mais local e complexo, com maior chance de overfiting [^15.2].
**Prova do Lemma 2:** Se $n_{min}$ √© 1, cada folha da √°rvore pode conter apenas uma observa√ß√£o do dado de treinamento, permitindo um ajuste t√£o complexo quanto o pr√≥prio dado. A medida que o valor de $n_{min}$ aumenta, o n√∫mero de divis√µes √© reduzido, simplificando a estrutura da √°rvore, e reduzindo o risco de overfitting, mas aumentando o risco de underfitting. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere uma √°rvore de decis√£o que divide dados em duas classes. Se $n_{min}=1$, a √°rvore pode crescer at√© que cada folha contenha apenas uma observa√ß√£o. Se os dados de treinamento tiverem ru√≠do, a √°rvore se ajustar√° a esses ru√≠dos, levando a overfitting. Se $n_{min}=10$, a √°rvore ser√° mais simples, parando de crescer antes de se ajustar ao ru√≠do, mas pode deixar de capturar intera√ß√µes mais complexas entre os dados, resultando em um modelo com underfitting.

**Corol√°rio 2:** A escolha do valor de *nmin* √© um trade-off entre vi√©s (bias) e vari√¢ncia. Um *nmin* pequeno leva a um baixo vi√©s e alta vari√¢ncia (overfitting), enquanto um *nmin* grande leva a um alto vi√©s e baixa vari√¢ncia (underfitting). [^15.2].

"Em regress√£o linear, a ideia de m√≠nimos quadrados busca encontrar o melhor hiperplano, enquanto em √°rvores de decis√£o o ajuste √© feito localmente, guiado pela regra de parada relacionada com *nmin*."

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
  subgraph "Variable Selection & Regularization"
    A["Random Feature Selection (m)"] --> B["Reduced Feature Influence"]
    B --> C["Implicit Regularization"]
    D["Explicit Regularization (L1/L2)"] --> E["Adjusted Feature Weights"]
    E --> F["Reduced Model Complexity"]
    C & F --> G("Control Overfitting")
    G --> H("Node Size & Tree Depth")
  end
```

A sele√ß√£o de vari√°veis √© intr√≠nseca √† constru√ß√£o de Random Forests, pois a escolha aleat√≥ria de *m* vari√°veis para cada split j√° √© uma forma de regulariza√ß√£o [^15.2]. Essa abordagem reduz a import√¢ncia de vari√°veis individuais, espalhando a contribui√ß√£o das previs√µes em um conjunto mais diversificado de vari√°veis. Em particular, quando *m* √© pequeno, poucas vari√°veis tem a chance de guiar cada split de forma individual, criando √°rvores descorrelacionadas que, em conjunto, constroem a decis√£o final. Regulariza√ß√£o, como a penaliza√ß√£o L1 e L2, n√£o √© diretamente aplicada em Random Forests, mas pode ser utilizada em outras etapas de pre-processamento ou em outros m√©todos para combinar diferentes tipos de aprendizagem. No entanto, o conceito de regulariza√ß√£o est√° presente indiretamente na sele√ß√£o aleat√≥ria de vari√°veis e nas regras de parada.

**Lemma 3:** A sele√ß√£o aleat√≥ria de *m* vari√°veis em Random Forests reduz a correla√ß√£o entre as √°rvores, como demonstrado em [^15.2], e funciona como uma forma de regulariza√ß√£o, reduzindo a influ√™ncia de vari√°veis espec√≠ficas no resultado final.
**Prova do Lemma 3:** Ao selecionar aleatoriamente um subconjunto de *m* vari√°veis em cada n√≥, reduzimos a probabilidade de que uma vari√°vel espec√≠fica domine a estrutura da √°rvore. A diversidade induzida pela sele√ß√£o aleat√≥ria das vari√°veis faz com que as √°rvores sejam mais diferentes entre si, o que resulta em menor correla√ß√£o entre elas e, consequentemente, reduz a vari√¢ncia da predi√ß√£o agregada. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um problema de classifica√ß√£o com $p = 10$ vari√°veis. Se usarmos uma Random Forest com $m = 1$, cada √°rvore usar√° apenas uma vari√°vel aleat√≥ria por vez para cada split. Isso significa que nenhuma vari√°vel dominar√° o modelo, e o resultado final ser√° uma agrega√ß√£o das previs√µes de v√°rias √°rvores, cada uma constru√≠da com diferentes perspectivas dos dados, reduzindo o overfitting. Se usarmos $m=10$ (bagging), o modelo final seria mais dependente de vari√°veis que separariam o resultado de forma mais clara no dado de treinamento.

**Corol√°rio 3:** √Årvores de decis√£o com um *nmin* muito pequeno e pouca sele√ß√£o aleat√≥ria de vari√°veis podem ser muito complexas (overfitting) e altamente correlacionadas, perdendo a vantagem de uma Random Forest. [^15.2].

"Enquanto a regulariza√ß√£o (L1/L2) ajusta explicitamente os pesos das vari√°veis para evitar overfitting, a sele√ß√£o aleat√≥ria e a regra de parada em √°rvores atuam implicitamente neste sentido."

> ‚ö†Ô∏è **Ponto Crucial**: A escolha apropriada de *m* e *nmin* √© crucial para o desempenho da Random Forest, equilibrando vi√©s e vari√¢ncia [^15.2].

### Separating Hyperplanes e Perceptrons

O conceito de hiperplanos separadores est√° presente em m√©todos como SVM e modelos lineares, mas n√£o √© diretamente aplicado em √°rvores de decis√£o. No entanto, √© importante notar que cada divis√£o em uma √°rvore de decis√£o cria uma fronteira de decis√£o local (mais simples) que pode ser entendida como uma regi√£o no espa√ßo das vari√°veis [^15.2]. Em uma Random Forest, cada √°rvore contribui para a fronteira de decis√£o final, que pode ser altamente n√£o-linear devido √† natureza hier√°rquica das √°rvores. O Perceptron, um algoritmo de classifica√ß√£o linear, n√£o se aplica diretamente ao contexto de Random Forests, que, por constru√ß√£o, s√£o algoritmos n√£o-lineares. No entanto, a ideia de ajuste iterativo est√° presente tanto no Perceptron quanto na forma que o Random Forest ‚Äúaprende‚Äù a partir das intera√ß√µes entre as diferentes √°rvores.

### Pergunta Te√≥rica Avan√ßada: Como o par√¢metro *m* influencia o vi√©s e a vari√¢ncia em Random Forests?
**Resposta:**
O par√¢metro *m* (n√∫mero de vari√°veis selecionadas aleatoriamente para cada split) desempenha um papel crucial no equil√≠brio entre vi√©s e vari√¢ncia em Random Forests [^15.2]. Um valor pequeno de *m* leva a √°rvores mais descorrelacionadas (reduzindo a vari√¢ncia da m√©dia), mas cada √°rvore individual ter√° maior bias, pois apenas uma pequena quantidade de vari√°veis ser√° utilizada no seu treinamento [^15.2]. Um valor grande de *m* resulta em √°rvores mais correlacionadas e com menor bias individual. O efeito principal de reduzir *m* √© a redu√ß√£o da vari√¢ncia total da Random Forest, que √© o objetivo principal da sua introdu√ß√£o sobre o simples bagging. A redu√ß√£o de vi√©s, no entanto, √© geralmente menos significativa que a de vari√¢ncia. A melhor escolha de *m* √© dependente do problema e deve ser definida usando alguma t√©cnica de otimiza√ß√£o, como valida√ß√£o cruzada.

```mermaid
graph LR
  subgraph "Impact of 'm' on Bias and Variance"
    A["Small 'm'"] --> B["High Bias (Individual Trees)"]
    B --> C["Low Correlation (Trees)"]
    C --> D["Low Variance (Random Forest)"]
    E["Large 'm'"] --> F["Low Bias (Individual Trees)"]
    F --> G["High Correlation (Trees)"]
    G --> H["High Variance (Random Forest)"]
  end
```

**Lemma 4:** A vari√¢ncia da Random Forest √© diretamente afetada pela correla√ß√£o entre as √°rvores e a vari√¢ncia de cada √°rvore individual.  Reduzir o *m* diminui a correla√ß√£o entre as √°rvores [^15.2].
**Prova do Lemma 4:** Conforme a equa√ß√£o 15.1, a vari√¢ncia da predi√ß√£o agregada ($\rho \sigma^2 + \frac{\sigma^2}{B}$) depende tanto da correla√ß√£o entre as √°rvores ($\rho$) quanto da vari√¢ncia individual ($\sigma^2$). Ao diminuir o n√∫mero de vari√°veis candidatas *m*, as √°rvores s√£o constru√≠das de forma mais independente, resultando em uma menor correla√ß√£o ($\rho$), e uma menor vari√¢ncia da predi√ß√£o agregada. $\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos supor que a vari√¢ncia de uma √∫nica √°rvore ($\sigma^2$) √© 10. Se tivermos 100 √°rvores ($B=100$), e a correla√ß√£o m√©dia entre elas for $\rho = 0.5$, a vari√¢ncia da Random Forest ser√° $0.5 \times 10 + \frac{10}{100} = 5.1$. Se diminuirmos $m$, e a correla√ß√£o entre as √°rvores cair para $\rho=0.1$, a nova vari√¢ncia ser√° $0.1 \times 10 + \frac{10}{100} = 1.1$. A redu√ß√£o em *m* resultou em uma redu√ß√£o de vari√¢ncia de aproximadamente 4 pontos percentuais.

**Corol√°rio 4:** O bias em Random Forests √© principalmente afetado pela profundidade das √°rvores, ou equivalentemente, pelo valor do tamanho m√≠nimo de n√≥ (*nmin*). Mesmo com um pequeno valor de *m*, o bias pode ser alto se a profundidade da √°rvore for limitada, ou se *nmin* for muito grande [^15.2].

> ‚ö†Ô∏è **Ponto Crucial**: A escolha de *m* √© um problema de otimiza√ß√£o que precisa levar em conta o equil√≠brio entre vi√©s e vari√¢ncia, geralmente feito utilizando valida√ß√£o cruzada, como ilustrado em [^15.3].

### Conclus√£o

Neste cap√≠tulo, exploramos as stopping rules nas Random Forests, concentrando-nos na sele√ß√£o aleat√≥ria de vari√°veis (m) e no tamanho m√≠nimo de n√≥ (nmin). As Random Forests combinam o poder de √°rvores de decis√£o com a t√©cnica de bagging, introduzindo a sele√ß√£o aleat√≥ria de vari√°veis para aumentar a descorrela√ß√£o entre as √°rvores e reduzir a vari√¢ncia da predi√ß√£o agregada. As stopping rules controlam a complexidade da √°rvore, definindo um trade-off entre bias e vari√¢ncia. A escolha apropriada desses par√¢metros √© essencial para o desempenho da Random Forest e deve ser guiada por valida√ß√£o cruzada ou alguma forma de otimiza√ß√£o. A robustez e adaptabilidade das Random Forests, juntamente com a compreens√£o detalhada de seus par√¢metros, garantem seu cont√≠nuo sucesso em diversas aplica√ß√µes de classifica√ß√£o e regress√£o.

<!-- END DOCUMENT -->
### Footnotes
[^15.1]: "Bagging or bootstrap aggregation (section 8.7) is a technique for reducing the variance of an estimated prediction function. Bagging seems to work especially well for high-variance, low-bias procedures, such as trees. For regression, we simply fit the same regression tree many times to bootstrap-sampled versions of the training data, and average the result. For classification, a committee of trees each cast a vote for the predicted class." *(Trecho de Random Forests)*
[^15.2]: "The essential idea in bagging (Section 8.7) is to average many noisy but approximately unbiased models, and hence reduce the variance. Trees are ideal candidates for bagging, since they can capture complex interaction structures in the data, and if grown sufficiently deep, have relatively low bias. Since trees are notoriously noisy, they benefit greatly from the averaging. Moreover, since each tree generated in bagging is identically distributed (i.d.), the expectation of an average of B such trees is the same as the expectation of any one of them. This means the bias of bagged trees is the same as that of the individual trees, and the only hope of improvement is through variance reduction. This is in contrast to boosting, where the trees are grown in an adaptive way to remove bias, and hence are not i.d. ...The idea in random forests (Algorithm 15.1) is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much. This is achieved in the tree-growing process through random selection of the input variables. Specifically, when growing a tree on a bootstrapped dataset: Before each split, select m ‚â§ p of the input variables at random as candidates for splitting." *(Trecho de Random Forests)*
[^15.3]: "For classification, the default value for m is [‚àöp] and the minimum node size is one. For regression, the default value for m is [p/3] and the minimum node size is five." *(Trecho de Random Forests)*
[^4.2]: "Conte√∫do extra√≠do conforme escrito no contexto e utilizado no cap√≠tulo" *(Trecho de <Nome do Documento>)*
[^4.3]: "Conte√∫do extra√≠do conforme escrito no contexto e utilizado no cap√≠tulo" *(Trecho de <Nome do Documento>)*
