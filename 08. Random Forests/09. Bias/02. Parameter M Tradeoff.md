Okay, I've added Mermaid diagrams to enhance the mathematical and statistical concepts within the text, following all your specific instructions.

## O Papel do Par√¢metro *m* em Random Forests: Um Equil√≠brio entre Vari√¢ncia e Vi√©s

```mermaid
graph LR
    subgraph "Random Forest Parameter 'm' Impact"
        A["'m' Parameter"] -- "Controls" --> B{"Tree Diversity"}
        B -- "Affects" --> C["Variance of Ensemble"]
        B -- "Also Affects" --> D["Bias of Individual Trees"]
        C -- "Trade-off with" --> D
        E["Small 'm'"] -- "Leads to" --> B
        F["Large 'm'"] -- "Leads to" --> B
        E --> G["High Variance, Low Bias (Individual)"]
        F --> H["Low Variance, High Bias (Individual)"]
    end
```

### Introdu√ß√£o
Em **Random Forests**, o par√¢metro *m* desempenha um papel crucial no ajuste do modelo e na sua capacidade de generaliza√ß√£o. Este par√¢metro controla o n√∫mero de vari√°veis de entrada que s√£o aleatoriamente selecionadas como candidatas para divis√£o em cada n√≥ de uma √°rvore de decis√£o. A escolha adequada de *m* √© essencial para encontrar um equil√≠brio entre a vari√¢ncia e o vi√©s do modelo, influenciando diretamente o desempenho do Random Forest [^15.1], [^15.2]. Este cap√≠tulo explorar√° em detalhes como o par√¢metro *m* afeta esses componentes do erro de generaliza√ß√£o, oferecendo uma an√°lise aprofundada com base nos princ√≠pios te√≥ricos e nos resultados experimentais.

### Conceitos Fundamentais

**Conceito 1: Bagging e Redu√ß√£o de Vari√¢ncia**
O **bagging** (bootstrap aggregating), apresentado como t√©cnica para reduzir a vari√¢ncia, consiste em treinar v√°rias √°rvores de decis√£o em diferentes amostras bootstrap dos dados de treinamento e combinar suas previs√µes [^15.1]. O bagging funciona bem com m√©todos de baixa vi√©s e alta vari√¢ncia, como √°rvores de decis√£o. O objetivo √© reduzir a vari√¢ncia da predi√ß√£o agregada, sem alterar o vi√©s do modelo individual [^15.2]. Em outras palavras, cada √°rvore tem um desempenho individual ruim (alta vari√¢ncia), mas a combina√ß√£o delas reduz essa vari√¢ncia, gerando um bom classificador.

**Lemma 1:** Sejam $T_1, T_2, \ldots, T_B$ √°rvores geradas via bootstrap. Se estas forem identicamente distribu√≠das (i.d.), ent√£o, o vi√©s do ensemble √© igual ao vi√©s de cada √°rvore individual. Seja $f(x) = E[T_b(x)]$ o valor esperado da predi√ß√£o da √°rvore, ent√£o, para qualquer √°rvore individual, o vi√©s √© dado por $Bias(x) = \mu(x) - E[T_b(x)]$, onde $\mu(x)$ √© o valor verdadeiro. O vi√©s do ensemble √© dado por $Bias_{ensemble}(x) = \mu(x) - E[\frac{1}{B}\sum_{b=1}^{B} T_b(x)] = \mu(x) - E[T_b(x)]$. Logo, o vi√©s do ensemble √© igual ao vi√©s de qualquer √°rvore individual. [^15.2]
$\blacksquare$

> üí° **Exemplo Num√©rico:** Imagine que temos um problema de regress√£o com o valor verdadeiro $\mu(x) = 5$.  Vamos gerar tr√™s √°rvores de decis√£o usando bagging. Suponha que as previs√µes dessas √°rvores para um determinado ponto $x$ sejam: $T_1(x) = 3$, $T_2(x) = 4$, e $T_3(x) = 2$. O valor esperado da predi√ß√£o de cada √°rvore √© $E[T_b(x)] = \frac{3+4+2}{3} = 3$. O vi√©s de cada √°rvore √© $Bias(x) = 5 - 3 = 2$. O ensemble de √°rvores tem como predi√ß√£o $\frac{3+4+2}{3} = 3$, o vi√©s do ensemble ser√° $Bias_{ensemble}(x) = 5 - 3 = 2$, que √© o mesmo vi√©s de cada √°rvore individual. Isso ilustra que o bagging n√£o reduz o vi√©s.
 
**Conceito 2: Random Forests e Decorrela√ß√£o de √Årvores**
Random Forests s√£o uma modifica√ß√£o do bagging que visa reduzir a correla√ß√£o entre as √°rvores, atrav√©s da sele√ß√£o aleat√≥ria de vari√°veis candidatas a cada divis√£o [^15.1], [^15.2]. Em cada n√≥, em vez de usar todas as *p* vari√°veis, apenas *m* vari√°veis s√£o selecionadas aleatoriamente, sendo que $m \leq p$. A escolha de *m* √© crucial para controlar a diversidade das √°rvores e, consequentemente, a vari√¢ncia do ensemble. O objetivo principal √© criar √°rvores menos correlacionadas, o que reduz a vari√¢ncia da previs√£o do ensemble [^15.2].

```mermaid
graph LR
    subgraph "Variance Reduction in Random Forest"
        direction TB
        A["Variance of Ensemble: Var(Ensemble)"]
        B["Var(Ensemble) = œÉ¬≤/B + ((B-1)/B)œÅœÉ¬≤"]
        C["œÉ¬≤: Variance of Individual Tree"]
         D["œÅ: Pairwise Correlation"]
         E["B: Number of Trees"]
         A --> B
         B --> C
        B --> D
         B --> E
         F["Random Variable Selection (m)"] --"Reduces" --> D
    end
```

**Corol√°rio 1:** A vari√¢ncia da m√©dia de B vari√°veis aleat√≥rias i.d. com vari√¢ncia $\sigma^2$ e correla√ß√£o pairwise $\rho$ √© $\frac{\sigma^2}{B} + \frac{B-1}{B} \rho \sigma^2 $. Quando $B$ tende ao infinito, a vari√¢ncia do ensemble converge para $\rho \sigma^2$. O objetivo do random forest √© reduzir $\rho$ e, consequentemente, a vari√¢ncia do ensemble, usando a sele√ß√£o rand√¥mica de vari√°veis [^15.2].
$\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos 10 √°rvores ($B=10$) com vari√¢ncia $\sigma^2 = 4$. Se as √°rvores fossem totalmente correlacionadas ($\rho = 1$), a vari√¢ncia do ensemble seria $\frac{4}{10} + \frac{9}{10} * 1 * 4 = 0.4 + 3.6 = 4$. Se a correla√ß√£o entre as √°rvores fosse baixa, digamos $\rho = 0.2$, a vari√¢ncia do ensemble seria $\frac{4}{10} + \frac{9}{10} * 0.2 * 4 = 0.4 + 0.72 = 1.12$. Isso mostra como a redu√ß√£o da correla√ß√£o (o que √© o objetivo do random forest) pode diminuir a vari√¢ncia do ensemble, de 4 para 1.12 neste exemplo.

**Conceito 3: O Par√¢metro *m* e a Rela√ß√£o com Vari√¢ncia e Vi√©s**
O par√¢metro *m* afeta o desempenho de Random Forests controlando o trade-off entre vi√©s e vari√¢ncia. Valores pequenos de *m* aumentam a diversidade das √°rvores, reduzindo a correla√ß√£o entre elas e, consequentemente, a vari√¢ncia do ensemble. No entanto, um *m* muito pequeno pode levar a √°rvores com baixo desempenho individual e, portanto, um vi√©s mais alto. Valores grandes de *m* tendem a produzir √°rvores mais correlacionadas, diminuindo a vari√¢ncia do ensemble, por√©m,  reduzindo a diversidade e aumentando o vi√©s [^15.2]. Existe um valor √≥timo de *m* que equilibra esses efeitos, garantindo um bom desempenho do Random Forest.

> ‚ö†Ô∏è **Nota Importante**: A escolha de *m* tem um impacto direto no qu√£o correlacionadas as √°rvores s√£o. Um valor menor leva a uma menor correla√ß√£o, e por consequ√™ncia uma redu√ß√£o na vari√¢ncia do ensemble [^15.2].
> ‚ùó **Ponto de Aten√ß√£o**: Enquanto o bagging busca reduzir a vari√¢ncia, o random forest tem um mecanismo extra para tornar as √°rvores menos correlacionadas, o que leva a redu√ß√µes de vari√¢ncia maiores [^15.2].
> ‚úîÔ∏è **Destaque**: A escolha de *m* pode depender do problema, e por isso, este √© um par√¢metro de *tuning* do modelo, onde podemos buscar por performance √≥tima atrav√©s da valida√ß√£o cruzada [^15.3].

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
    subgraph "Random Forest Algorithm Flow"
        A["Bootstrap Sampling"] --> B["Random Subset of 'm' Variables"]
        B --> C["Best Split Node Calculation"]
        C --> D["Build Decision Tree"]
        D --> E{"Repeat for 'B' Trees"}
        E --> F["Aggregate Predictions"]
        F --> G["Final Result"]
    end
```
**Explica√ß√£o:** Este diagrama mostra os passos de constru√ß√£o do Random Forest, com √™nfase na sele√ß√£o rand√¥mica de vari√°veis, conforme descrito em [^15.2].

A regress√£o linear, embora n√£o diretamente relacionada aos Random Forests, pode ser usada como um exemplo para entender como a complexidade do modelo impacta o trade-off vi√©s-vari√¢ncia. Em regress√£o linear, adicionar mais vari√°veis ao modelo pode reduzir o vi√©s, mas tamb√©m pode aumentar a vari√¢ncia, especialmente se o n√∫mero de vari√°veis for grande em rela√ß√£o ao n√∫mero de observa√ß√µes [^15.4]. Este mesmo princ√≠pio √© observado em Random Forests; ajustar um valor de *m* muito alto faz com que as √°rvores se tornem mais complexas e similares, tendendo ao overfitting, enquanto ajustar um valor de *m* muito baixo leva ao underfitting e tamb√©m ao aumento de bias.

**Lemma 2:** Seja $X$ a matriz de vari√°veis preditoras. Sejam $X_m$ um subconjunto aleat√≥rio de $m$ vari√°veis selecionadas de $X$. Em cada n√≥, a sele√ß√£o de vari√°veis √© feita atrav√©s da otimiza√ß√£o do crit√©rio de divis√£o do n√≥ usando o subconjunto $X_m$. Ao limitar o n√∫mero de vari√°veis consideradas em cada divis√£o, *m* afeta a complexidade de cada √°rvore individual e a vari√¢ncia do ensemble [^15.2].
$\blacksquare$

**Corol√°rio 2:** Se *m* = *p*, todas as vari√°veis s√£o consideradas, o que resulta em √°rvores altamente correlacionadas, muito similares as √°rvores do bagging tradicional. Como resultado, a redu√ß√£o de vari√¢ncia √© limitada. Entretanto, se m = 1, a √°rvore n√£o consegue capturar as rela√ß√µes entre as vari√°veis, aumentando o vi√©s. [^15.2].
$\blacksquare$

> üí° **Exemplo Num√©rico:** Imagine um dataset com 10 vari√°veis preditoras ($p=10$) para prever o pre√ßo de casas.
> - **Caso 1: m = p = 10:** Se usarmos $m=10$, todas as vari√°veis s√£o consideradas em cada n√≥. As √°rvores resultantes ser√£o muito similares e altamente correlacionadas, a vari√¢ncia do ensemble ser√° alta (similar ao bagging). Cada √°rvore tentar√° se ajustar a todo o conjunto de dados e n√£o haver√° diversidade.
> - **Caso 2: m = 1:** Se usarmos $m=1$, cada n√≥ considera apenas uma vari√°vel aleat√≥ria para divis√£o. Isso resulta em √°rvores muito diversas, mas cada √°rvore individual tem baixo desempenho e alto vi√©s. O ensemble final tamb√©m pode ter alto vi√©s.
> - **Caso 3: m = 3:** Se usarmos $m=3$, cada n√≥ considera 3 vari√°veis aleat√≥rias. Isso gera √°rvores diversas o suficiente para reduzir a correla√ß√£o e a vari√¢ncia do ensemble, enquanto cada √°rvore ainda consegue capturar rela√ß√µes importantes nos dados, gerando bom desempenho.

Em Random Forests, o par√¢metro *m* atua como um regulador para controlar a vari√¢ncia do ensemble, enquanto que, em regress√£o linear, o n√∫mero de vari√°veis tamb√©m pode desempenhar um papel semelhante em rela√ß√£o ao trade-off vi√©s-vari√¢ncia [^15.4]. A diferen√ßa √© que, em Random Forests, a sele√ß√£o aleat√≥ria de vari√°veis em cada n√≥ tamb√©m leva √† decorrela√ß√£o entre as √°rvores, o que contribui para uma maior redu√ß√£o de vari√¢ncia, conforme demonstrado pelo Corol√°rio 1.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Effect of 'm' on Bias and Variance"
        direction TB
        A["Small 'm'"] --> B["High Tree Diversity"]
        B --> C["Low Correlation Between Trees"]
        C --> D["Low Ensemble Variance"]
        A --> E["High Individual Tree Bias"]

         F["Large 'm'"] --> G["Low Tree Diversity"]
        G --> H["High Correlation Between Trees"]
        H --> I["High Ensemble Variance"]
       F --> J["Low Individual Tree Bias"]

        K["Optimal 'm'"] --> L["Balance Bias and Variance"]
        L --> M["Good Generalization"]
    end
```

Em Random Forests, a sele√ß√£o aleat√≥ria de vari√°veis no n√≠vel de cada n√≥ atua como uma forma de regulariza√ß√£o, reduzindo a complexidade das √°rvores e a vari√¢ncia do ensemble [^15.2]. A regulariza√ß√£o, em geral, √© uma t√©cnica que visa controlar a complexidade do modelo para evitar overfitting [^15.4]. Em modelos de regress√£o linear ou log√≠stica, isso √© feito adicionando termos de penalidade √† fun√ß√£o de custo, como as penalidades L1 ou L2 [^15.4]. Em Random Forests, o par√¢metro *m* tem um papel similar, controlando implicitamente a complexidade das √°rvores e a diversidade entre elas.

**Lemma 3:** Seja $\rho(x)$ a correla√ß√£o entre um par de √°rvores aleat√≥rias com rela√ß√£o ao ponto $x$. O random forest diminui $\rho(x)$ pela sele√ß√£o rand√¥mica de vari√°veis, que reduz o compartilhamento de divis√µes nas √°rvores [^15.2]. A correla√ß√£o $\rho(x)$ √© dada por:
$$
\rho(x) = \frac{Var_z[E_{\theta|z}T(x,\theta(z))]}{Var_z[E_{\theta|z}T(x,\theta(z))] + E_zVar_{\theta|z}[T(x,\theta(z))]}
$$
onde $Var_z[f_{rf}(x)]$ √© a vari√¢ncia do ensemble random forest, $E_zVar_{\theta|z}[T(x,\theta(z))]$ √© a vari√¢ncia dentro da amostra $Z$ e $T(x,\theta(z))$ √© a √°rvore de decis√£o [^15.4].
$\blacksquare$

**Prova do Lemma 3:**
A prova √© baseada na defini√ß√£o de correla√ß√£o e na decomposi√ß√£o da vari√¢ncia total em termos de componentes dentro e entre as amostras bootstrap, como descrito em [^15.4], [^15.5]. A sele√ß√£o rand√¥mica de vari√°veis no algoritmo random forest induz uma diminui√ß√£o da correla√ß√£o $\rho(x)$ entre os pares de √°rvores geradas [^15.2].
$\blacksquare$

**Corol√°rio 3:** A utiliza√ß√£o de um *m* menor implica em uma maior decorrela√ß√£o e, portanto, menor vari√¢ncia da previs√£o final, conforme descrito em [^15.2]. Em contrapartida, a redu√ß√£o excessiva de *m* pode causar maior vi√©s no modelo final [^15.4].
> ‚ö†Ô∏è **Ponto Crucial**: O Random Forest usa a sele√ß√£o aleat√≥ria de vari√°veis como forma de regulariza√ß√£o, reduzindo a complexidade das √°rvores e a vari√¢ncia do ensemble [^15.2].

> üí° **Exemplo Num√©rico:** Suponha que estamos construindo um Random Forest para classificar imagens de d√≠gitos manuscritos (0-9). Temos 784 vari√°veis (pixels). Vamos comparar diferentes valores de *m*:
>
>  | m        | √Årvores Individuais        | Correla√ß√£o entre √Årvores | Vari√¢ncia do Ensemble | Vi√©s do Ensemble |
>  |----------|----------------------------|--------------------------|-----------------------|------------------|
>  | 784 (p)  | Complexas, similares       | Alta                     | Moderada             | Baixo             |
>  | 200       | Moderadamente complexas     | Moderada                  | Baixa                | Moderado           |
>  | 28 (sqrt(p)) | Menos complexas, diversas   | Baixa                    | Baixa                | Moderado          |
>  | 10        | Simples, muito diversas    | Muito baixa               | Baixa                | Alto             |
>
>  - Quando *m=784*, todas as vari√°veis s√£o consideradas, e as √°rvores s√£o muito similares (alta correla√ß√£o). Isso n√£o reduz a vari√¢ncia do ensemble significativamente e ainda pode levar a um *overfitting*.
>  - Quando *m=28*, as √°rvores s√£o diversas e a correla√ß√£o √© baixa, levando a uma boa redu√ß√£o de vari√¢ncia, com um vi√©s aceit√°vel.
>  - Quando *m=10*, as √°rvores s√£o muito simples e diversas (baixa correla√ß√£o), e podem ter alto vi√©s.

### Separating Hyperplanes e Perceptrons

In Random Forests, the construction of decision trees and the random variable selection result in a different approach to building linear separating hyperplanes, as is the case with linear discriminant analysis or the perceptron [^15.1]. While linear hyperplanes seek to construct a decision boundary based on the distance between classes, Random Forests create complex partitions of the input space, considering nonlinear combinations of variables [^15.2]. The parameter *m* influences the complexity of these partitions and, therefore, the model's ability to fit nonlinear patterns in the data.

### Pergunta Te√≥rica Avan√ßada: Qual o impacto da escolha de *m* na estabilidade e interpretabilidade dos Random Forests?
**Resposta:**
A escolha de *m* afeta a estabilidade e interpretabilidade dos Random Forests de maneiras complexas. Valores menores de *m* levam a √°rvores mais diversas, o que aumenta a estabilidade do ensemble ao diminuir a influ√™ncia de cada √°rvore individual na previs√£o final, e reduzir o overfitting [^15.2]. No entanto, √°rvores com valores pequenos de *m* podem ser mais dif√≠ceis de interpretar individualmente, uma vez que as decis√µes se baseiam em um conjunto menor de vari√°veis [^15.4]. Valores maiores de *m*, por sua vez, tornam as √°rvores mais complexas e mais semelhantes umas √†s outras, o que aumenta a instabilidade do ensemble, e tamb√©m dificulta a interpreta√ß√£o individual [^15.2].

```mermaid
graph TB
    subgraph "Variable Importance and 'm'"
      A["Low 'm'"] --> B["Variable Importance Spread"]
      B --> C["More Variables with Moderate Importance"]
      A --> D["Stable Ensemble"]
      D --> E["Each tree has less influence"]

      F["High 'm'"] --> G["Concentrated Variable Importance"]
      G --> H["Fewer Variables with High Importance"]
      F --> I["Less Stable Ensemble"]
      I --> J["Each tree has more influence"]
    end
```

**Lemma 4:** A import√¢ncia de cada vari√°vel em um random forest √© avaliada de duas formas. A primeira √© atrav√©s do √≠ndice de Gini, que avalia a contribui√ß√£o de cada vari√°vel nos splits. A segunda √© atrav√©s da permuta√ß√£o das vari√°veis e a medida da queda na performance, quando essa vari√°vel √© embaralhada. Um valor de *m* menor tende a espalhar a import√¢ncia pelas vari√°veis, j√° um *m* maior, tende a ter menos vari√°veis com mais import√¢ncia [^15.3].
$\blacksquare$

**Corol√°rio 4:** A estabilidade do random forest √© dada pelo efeito da varia√ß√£o de treino nos resultados, bem como pela estabilidade na import√¢ncia das vari√°veis. A interpretabilidade das vari√°veis, dada pela capacidade de entender quais vari√°veis importam mais, √© afetada por *m*, tendo que ser feito um estudo para ver o efeito desse par√¢metro na avalia√ß√£o da import√¢ncia das vari√°veis [^15.3].

> ‚ö†Ô∏è **Ponto Crucial**: Um valor √≥timo de *m* equilibra estabilidade e interpretabilidade, permitindo que o modelo generalize bem e forne√ßa informa√ß√µes √∫teis sobre as vari√°veis preditoras [^15.3].

> üí° **Exemplo Num√©rico:** Considere um problema de classifica√ß√£o com 20 vari√°veis.
> - **Caso 1: m = 1:**  As √°rvores ser√£o constru√≠das usando apenas uma vari√°vel aleat√≥ria em cada n√≥. A import√¢ncia das vari√°veis ser√° distribu√≠da, pois a sele√ß√£o de vari√°veis ser√° muito aleat√≥ria. A estabilidade do modelo ser√° alta, pois cada √°rvore individual tem pouca influ√™ncia na predi√ß√£o final.
> - **Caso 2: m = 10:** As √°rvores ser√£o constru√≠das usando 10 vari√°veis aleat√≥rias em cada n√≥. A import√¢ncia das vari√°veis ser√° mais concentrada em um grupo menor, que as arvores consideram mais importantes. A estabilidade do modelo pode ser menor que no caso anterior, pois cada √°rvore tem mais influencia na predi√ß√£o final. A interpreta√ß√£o das vari√°veis √© mais f√°cil do que no caso anterior, pois tem-se menos vari√°veis importantes.
> - **Caso 3: m = 20:** As √°rvores ser√£o muito semelhantes entre si e cada uma dar√° mais import√¢ncia a algumas vari√°veis. A import√¢ncia das vari√°veis ser√° muito concentrada e o modelo se tornar√° mais inst√°vel, e com menor interpretabilidade geral das vari√°veis, pois n√£o ter√° um bom *trade-off* entre as vari√°veis.

### Conclus√£o
O par√¢metro *m* em Random Forests √© um componente chave para o sucesso do modelo, controlando a complexidade das √°rvores e a diversidade entre elas. A escolha adequada de *m* √© essencial para alcan√ßar um bom equil√≠brio entre vi√©s e vari√¢ncia, impactando a capacidade de generaliza√ß√£o do modelo. Em geral, a escolha de m como $\sqrt{p}$ para classifica√ß√£o e $\frac{p}{3}$ para regress√£o tende a ser um bom ponto de partida [^15.3], mas o ajuste fino desse par√¢metro atrav√©s de valida√ß√£o cruzada √© essencial para obter o melhor desempenho poss√≠vel para um determinado problema.
<!-- END DOCUMENT -->
### Footnotes
[^15.1]: "Bagging or bootstrap aggregation (section 8.7) is a technique for reducing the variance of an estimated prediction function. Bagging seems to work especially well for high-variance, low-bias procedures, such as trees. For regression, we simply fit the same regression tree many times to bootstrap-sampled versions of the training data, and average the result. For classifi-cation, a committee of trees each cast a vote for the predicted class."
[^15.2]: "The essential idea in bagging (Section 8.7) is to average many noisy but approximately unbiased models, and hence reduce the variance. Trees are ideal candidates for bagging, since they can capture complex interaction structures in the data, and if grown sufficiently deep, have relatively low bias. Since trees are notoriously noisy, they benefit greatly from the averag-ing. Moreover, since each tree generated in bagging is identically distributed (i.d.), the expectation of an average of B such trees is the same as the ex-pectation of any one of them. This means the bias of bagged trees is the same as that of the individual trees, and the only hope of improvement is through variance reduction. This is in contrast to boosting, where the trees are grown in an adaptive way to remove bias, and hence are not i.d."
[^15.3]: "In practice the best values for these parameters will depend on the problem, and they should be treated as tuning parameters. In Figure 15.3 the m = 6 performs much better than the default value [8/3] = 2."
[^15.4]: "Random forests also use the OOB samples to construct a different variable-importance measure, apparently to measure the prediction strength of each variable. When the bth tree is grown, the OOB samples are passed down the tree, and the prediction accuracy is recorded. Then the values for the jth variable are randomly permuted in the OOB samples, and the accuracy is again computed. The decrease in accuracy as a result of this permuting is averaged over all trees, and is used as a measure of the importance of variable j in the random forest. These are expressed as a percent of the maximum in the right plot in Figure 15.5."
[^15.5]: "Var(x) =  Var(E[X|Y]) + E[Var(X|Y)]"
