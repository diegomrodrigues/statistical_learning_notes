## Multidimensional Scaling Plots: Visualiza√ß√£o de Similaridade em Espa√ßos de Alta Dimensionalidade

```mermaid
graph LR
    subgraph "MDS Process"
        direction TB
        A["Input: 'Dissimilarity Matrix' (D)"] --> B["MDS Algorithm"]
        B --> C["Output: 'Low-Dimensional Coordinates'"]
        style A fill:#ccf,stroke:#333,stroke-width:2px
        style B fill:#cfc,stroke:#333,stroke-width:2px
        style C fill:#fcc,stroke:#333,stroke-width:2px

    end
    D["High Dimensional Data"] -- "Dissimilarity Calculation" --> A
    C --> E["Visualization & Analysis"]
```

### Introdu√ß√£o
O **Multidimensional Scaling (MDS)**, ou Escalonamento Multidimensional, √© um conjunto de t√©cnicas estat√≠sticas e de aprendizado de m√°quina que visa visualizar a estrutura de dados de alta dimensionalidade em um espa√ßo de baixa dimensionalidade, geralmente 2D ou 3D [^15.3.3]. O MDS busca preservar as similaridades ou dissimilaridades entre os dados originais, permitindo que rela√ß√µes complexas sejam interpretadas visualmente.  Diferentemente de m√©todos de redu√ß√£o de dimensionalidade como PCA (Principal Component Analysis), o MDS n√£o depende necessariamente de uma representa√ß√£o vetorial dos dados, sendo capaz de operar a partir de matrizes de dist√¢ncia ou dissimilaridade [^15.3.3]. MDS √© particularmente √∫til em situa√ß√µes onde as caracter√≠sticas subjacentes dos dados s√£o dif√≠ceis de obter diretamente e onde a rela√ß√£o entre os objetos √© mais importante do que as dimens√µes originais [^15.3.3].

### Conceitos Fundamentais

**Conceito 1: Matriz de Dissimilaridade (ou Similaridade)**
O ponto de partida para o MDS √© uma matriz que quantifica a dissimilaridade (ou similaridade) entre todos os pares de objetos. Esta matriz pode ser derivada de diversas fontes, como dist√¢ncias euclidianas, correla√ß√µes, ou medidas customizadas de dissimilaridade [^15.3.3].
  *   Uma matriz de dissimilaridade $D$ de tamanho $n \times n$ √© definida tal que o elemento $d_{ij}$ representa a dissimilaridade entre o objeto $i$ e o objeto $j$. Quanto maior o valor de $d_{ij}$, menos similares s√£o os objetos.

> üí° **Exemplo Num√©rico:** Vamos considerar um conjunto de 4 objetos (A, B, C, D) e uma matriz de dissimilaridade calculada atrav√©s de alguma m√©trica (e.g., dist√¢ncia euclidiana entre dados originais que n√£o temos acesso).  A matriz $D$ poderia ser:
>
> ```
>       A    B    C    D
>   A   0.0  2.1  3.5  5.0
>   B   2.1  0.0  1.8  3.2
>   C   3.5  1.8  0.0  1.5
>   D   5.0  3.2  1.5  0.0
> ```
>
> Nesta matriz, $d_{AB} = 2.1$ indica que os objetos A e B s√£o menos similares do que os objetos C e D ($d_{CD} = 1.5$). Note que a diagonal principal √© sempre zero ($d_{ii} = 0$) porque a dissimilaridade de um objeto consigo mesmo √© nula. Os valores fora da diagonal mostram o qu√£o diferentes os objetos s√£o entre si.

**Lemma 1:** A matriz de dissimilaridade $D$ √© uma representa√ß√£o de dados flex√≠vel, podendo ser utilizada quando os dados originais n√£o s√£o vetoriais.
*   **Prova:** A matriz de dissimilaridade $D$ pode ser definida usando v√°rias m√©tricas, como a dist√¢ncia de Mahalanobis ou dist√¢ncia de correla√ß√£o, que n√£o necessariamente requerem representa√ß√£o vetorial dos objetos [^15.3.3]. A √∫nica exig√™ncia √© que $d_{ij} = d_{ji}$ e $d_{ii} = 0$. $\blacksquare$
```mermaid
graph LR
    subgraph "Dissimilarity Matrix Properties"
        direction TB
         A["Dissimilarity Matrix D"]
        B["d_ij: Dissimilarity between object i and j"]
        C["d_ij = d_ji (Symmetry)"]
        D["d_ii = 0 (Self-Dissimilarity is Zero)"]
        A --> B
        A --> C
        A --> D

    end

```

**Conceito 2: Mapeamento para Espa√ßo de Baixa Dimensionalidade**
O objetivo principal do MDS √© encontrar uma configura√ß√£o de pontos em um espa√ßo de baixa dimensionalidade (tipicamente $k=2$ ou $k=3$) tal que as dist√¢ncias entre esses pontos reflitam as dissimilaridades originais. As coordenadas dos pontos no novo espa√ßo s√£o as sa√≠das do MDS.

**Corol√°rio 1:** O MDS pode ser visto como um problema de otimiza√ß√£o, onde se busca minimizar uma fun√ß√£o de custo que quantifica a diferen√ßa entre as dist√¢ncias no novo espa√ßo e as dissimilaridades originais [^15.3.3].
*  *A minimiza√ß√£o desta fun√ß√£o de custo √© crucial para garantir a representatividade do mapeamento.*

> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, o MDS busca mapear os objetos A, B, C e D para um espa√ßo 2D (por exemplo, um gr√°fico) de tal forma que as dist√¢ncias entre os pontos no gr√°fico sejam aproximadamente iguais √†s dissimilaridades da matriz D. Imagine que ap√≥s a aplica√ß√£o do MDS (usando, por exemplo, o m√©todo Classical MDS) obtivemos as seguintes coordenadas 2D para cada objeto:
>
>   *   A: (0.5, 1.0)
>   *   B: (2.5, 1.2)
>   *   C: (3.0, 2.8)
>   *   D: (4.5, 2.5)
>
> Calculando as dist√¢ncias euclidianas entre esses pontos, ter√≠amos, por exemplo,  $d'(A,B) = \sqrt{(2.5 - 0.5)^2 + (1.2 - 1.0)^2} \approx 2.01 $.  O objetivo do MDS √© que esta dist√¢ncia $d'(A,B)$ se aproxime da dissimilaridade original $d_{AB} = 2.1$.  A qualidade do mapeamento √© medida pela minimiza√ß√£o da fun√ß√£o de custo (stress function).
>
> ```mermaid
>   graph LR
>       A(A(0.5, 1.0)) --> B(B(2.5, 1.2))
>       A --> C(C(3.0, 2.8))
>       A --> D(D(4.5, 2.5))
>       B --> C
>       B --> D
>       C --> D
> ```
>  Visualmente, os pontos B e C est√£o mais pr√≥ximos entre si, refletindo a menor dissimilaridade na matriz D.

**Conceito 3: Algoritmos de MDS (Classical MDS e Non-Metric MDS)**
Existem diferentes algoritmos para realizar o MDS. O **Classical MDS** ou MDS m√©trico, assume que as dissimilaridades s√£o aproximadamente dist√¢ncias euclidianas. J√° o **Non-Metric MDS** ou MDS n√£o-m√©trico, relaxa essa suposi√ß√£o e se concentra em preservar a ordem das dissimilaridades [^15.3.3].
  *  O Classical MDS utiliza a decomposi√ß√£o em autovalores da matriz de produtos internos, enquanto o Non-Metric MDS usa m√©todos de otimiza√ß√£o iterativos.

```mermaid
graph LR
    subgraph "MDS Algorithms"
        direction TB
        A["MDS"] --> B["Classical MDS ('Metric')"]
        A --> C["Non-Metric MDS"]
         B --> D["Assumes dissimilarities are approx. Euclidean distances"]
        C --> E["Preserves the order of dissimilarities"]
        B --> F["Uses eigenvalue decomposition"]
        C --> G["Uses iterative optimization"]
    end
```

> üí° **Exemplo Num√©rico:** Para ilustrar a diferen√ßa entre os dois m√©todos, imagine que as dissimilaridades da matriz D anterior n√£o representam dist√¢ncias euclidianas, mas apenas a ordem de dissimilaridade.  Nesse caso, o Non-Metric MDS seria mais apropriado, pois ele n√£o se preocupa em manter as dist√¢ncias exatas, mas sim em garantir que os pontos mais similares (menores valores em D) apare√ßam mais pr√≥ximos no espa√ßo de baixa dimensionalidade. O Classical MDS, por outro lado, tentaria preservar os valores num√©ricos das dissimilaridades como se fossem dist√¢ncias.

> ‚ö†Ô∏è **Nota Importante**: A escolha entre Classical e Non-Metric MDS depende da natureza dos dados e das suposi√ß√µes que se podem fazer sobre eles. **Refer√™ncia ao t√≥pico [^15.3.3]**.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

O MDS n√£o √© um m√©todo de classifica√ß√£o, e sim uma t√©cnica de redu√ß√£o e visualiza√ß√£o de dimensionalidade. No entanto, os dados visualizados pelo MDS podem ser usados como input para classificadores. No contexto do cap√≠tulo, vamos explorar como a matriz de proximidade utilizada pelo Random Forest, um m√©todo de classifica√ß√£o, se relaciona com o MDS. O t√≥pico [^15.3.3] nos apresenta um tipo de visualiza√ß√£o chamada *Proximity Plot*, que √© gerado a partir da matriz de proximidade de um Random Forest.

```mermaid
graph LR
    subgraph "Proximity Matrix & MDS"
      direction TB
      A["Random Forest"] --> B["Proximity Matrix"]
      B --> C["MDS Application"]
      C --> D["Proximity Plot (Low-dimensional visualization)"]
    end
```

**Lemma 2:** A matriz de proximidade de um Random Forest pode ser interpretada como uma matriz de similaridade, onde a proximidade entre dois pontos √© dada pelo n√∫mero de vezes que eles compartilharam o mesmo n√≥ terminal nas √°rvores do Random Forest.
* **Prova:** Conforme descrito em [^15.3.3], a matriz de proximidade √© constru√≠da contando quantas vezes cada par de amostras termina no mesmo n√≥ em cada √°rvore. Assim, quanto mais vezes dois pontos terminam juntos, mais semelhantes eles s√£o do ponto de vista do Random Forest. $\blacksquare$
```mermaid
graph LR
    subgraph "Random Forest Proximity"
        direction TB
        A["Random Forest Tree"] --> B["Samples sharing terminal nodes"]
        B --> C["Proximity Count (p_ij)"]
        C --> D["Proximity Matrix"]
        D --> E["Interpretation: p_ij =  # of times samples i and j end in the same terminal node"]
    end
```
**Corol√°rio 2:** O MDS aplicado √† matriz de proximidade de um Random Forest pode revelar agrupamentos e estruturas nos dados que podem n√£o ser evidentes no espa√ßo original de alta dimensionalidade, como mostrado em [^15.3.3].
 *  *Essa visualiza√ß√£o permite entender melhor como o Random Forest est√° processando os dados, mesmo sem conhecer as vari√°veis originais.*

> üí° **Exemplo Num√©rico:** Imagine que temos um conjunto de dados de pacientes com duas classes: Doente e Saud√°vel. Ap√≥s construir um Random Forest, obtemos uma matriz de proximidade. Se dois pacientes frequentemente terminam no mesmo n√≥ das √°rvores, o valor correspondente na matriz de proximidade √© alto. Ao aplicar MDS nesta matriz, esperamos que os pacientes da mesma classe se agrupem no espa√ßo 2D gerado pelo MDS, e que pacientes de classes distintas se encontrem mais distantes, o que facilita a interpreta√ß√£o e an√°lise da efic√°cia do Random Forest.

√â crucial notar que, embora o MDS possa ser utilizado para visualizar dados usados em classifica√ß√£o, ele n√£o √©, em si, um m√©todo de classifica√ß√£o. O MDS √© uma ferramenta que auxilia na compreens√£o das rela√ß√µes entre os dados e pode complementar os m√©todos de classifica√ß√£o como o Random Forest.

> ‚ùó **Ponto de Aten√ß√£o**: O MDS n√£o deve ser usado como substituto para m√©todos de classifica√ß√£o, mas sim como um auxiliar para a an√°lise e compreens√£o dos dados. **Conforme indicado em [^15.3.3]**.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

Embora o MDS em si n√£o realize sele√ß√£o de vari√°veis, ele permite a visualiza√ß√£o dos dados, o que pode guiar na escolha das vari√°veis mais importantes para um problema de classifica√ß√£o. A regulariza√ß√£o, por sua vez, √© uma t√©cnica para simplificar modelos e reduzir o overfitting, impactando indiretamente a visualiza√ß√£o MDS, j√° que dados processados por m√©todos de regulariza√ß√£o podem levar a uma estrutura diferente da matriz de dissimilaridades.
No contexto do cap√≠tulo, a sele√ß√£o de vari√°veis no random forest por meio da escolha aleat√≥ria de m vari√°veis candidatas a cada divis√£o [^15.2], e a regulariza√ß√£o por meio do controle da profundidade da √°rvore, t√™m um impacto indireto na estrutura de proximidade utilizada no MDS.

```mermaid
graph LR
    subgraph "Random Forest Parameters Impact on MDS"
    direction TB
    A["Parameter m (variables per split)"] -- "affects tree correlation"--> B["Proximity Matrix"]
    C["Tree Depth (minimum node size)"] --"affects tree complexity"--> B
    B --> D["MDS Visualization"]
    end
```

**Lemma 3:** A escolha de *m*, o n√∫mero de vari√°veis candidatas em cada split em um Random Forest, afeta a correla√ß√£o entre as √°rvores e, consequentemente, a matriz de proximidade, impactando a visualiza√ß√£o MDS resultante.
*   **Prova:** Como discutido em [^15.2], a escolha de um valor menor de *m* leva a √°rvores menos correlacionadas, e tamb√©m aumenta o vi√©s de cada √°rvore. Uma matriz de proximidade calculada com essas √°rvores menos correlacionadas deve refletir um padr√£o diferente dos dados. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que em um Random Forest, temos 10 vari√°veis (features) dispon√≠veis. Se escolhermos *m* = 2, cada √°rvore considerar√° apenas 2 dessas 10 vari√°veis para cada split. Isso levar√° a √°rvores mais diversas, e a matriz de proximidade refletir√° essa diversidade, o que pode levar a um MDS com agrupamentos diferentes do que seria obtido se us√°ssemos *m* = 8, por exemplo. Se as features s√£o redundantes, um valor baixo de *m* pode ser bom para evitar √°rvores muito parecidas.
```mermaid
graph LR
    subgraph "Effect of 'm' on Tree Correlation"
      direction TB
        A["Smaller 'm'"] --> B["Less Correlated Trees"]
        B --> C["More Diverse RF"]
         A --> D["Higher Bias"]

         E["Larger 'm'"] --> F["More Correlated Trees"]
         F --> G["Less Diverse RF"]
          E --> H["Lower Bias"]

    end
```

**Prova do Lemma 3:** A redu√ß√£o de *m* faz com que as √°rvores base do Random Forest se tornem menos correlacionadas, devido a uma menor probabilidade de que elas escolham as mesmas vari√°veis em cada split. Isso impacta a matriz de proximidade porque ela √© constru√≠da a partir de √°rvores menos semelhantes, resultando em uma visualiza√ß√£o MDS diferente.
$\blacksquare$

**Corol√°rio 3:** A profundidade m√°xima das √°rvores do Random Forest, que pode ser controlada por um m√≠nimo tamanho do n√≥, tamb√©m afeta a matriz de proximidade e a visualiza√ß√£o MDS, similarmente √† regulariza√ß√£o.
*  *A profundidade das √°rvores controla a complexidade do modelo e a matriz de proximidade refletir√° essa complexidade na visualiza√ß√£o MDS.*

> üí° **Exemplo Num√©rico:** Limitar a profundidade da √°rvore, por exemplo, definindo um tamanho m√≠nimo de n√≥ de 10, faz com que cada √°rvore se torne mais simples, menos propensa ao overfitting. A matriz de proximidade gerada por um Random Forest com √°rvores rasas ser√° diferente da matriz obtida com √°rvores mais profundas, afetando a visualiza√ß√£o MDS. Uma √°rvore muito profunda pode fazer com que a proximidade entre pontos seja muito "localizada", e √°rvores mais rasas geram matrizes com mais pontos "conectados".
```mermaid
graph LR
    subgraph "Tree Depth & Model Complexity"
        direction TB
        A["Shallow Trees"] --> B["Simpler Model"]
        B --> C["Reduced Overfitting"]
        A --> D["Proximity Matrix reflects global similarities"]
         E["Deep Trees"] --> F["More Complex Model"]
        F --> G["Increased Overfitting"]
          E --> H["Proximity Matrix reflect local similarities"]
    end
```

### Separating Hyperplanes e Perceptrons
O MDS √© uma t√©cnica de visualiza√ß√£o e n√£o est√° diretamente relacionado com m√©todos de classifica√ß√£o como Separating Hyperplanes ou Perceptrons. No entanto, podemos analisar como as visualiza√ß√µes geradas pelo MDS se relacionam com a separabilidade das classes em um espa√ßo de baixa dimens√£o. Por exemplo, um MDS poderia ser aplicado √† matriz de dist√¢ncia entre dados de diferentes classes. Uma visualiza√ß√£o em que os pontos de diferentes classes estejam bem separados sugere que um modelo linear como um Separating Hyperplane pode ser efetivo.

> üí° **Exemplo Num√©rico:** Suponha que aplicamos MDS a dados de duas classes (A e B) e obtemos o seguinte plot:
>
> ```mermaid
>   graph LR
>       A1(A) --> A2(A)
>       A1 --> A3(A)
>       B1(B) --> B2(B)
>       B1 --> B3(B)
>       style A1 fill:#f9f,stroke:#333,stroke-width:2px
>       style A2 fill:#f9f,stroke:#333,stroke-width:2px
>       style A3 fill:#f9f,stroke:#333,stroke-width:2px
>       style B1 fill:#ccf,stroke:#333,stroke-width:2px
>        style B2 fill:#ccf,stroke:#333,stroke-width:2px
>        style B3 fill:#ccf,stroke:#333,stroke-width:2px
>
> ```
>
>  Se no gr√°fico gerado pelo MDS, os pontos da classe A (roxos) se agrupam em uma regi√£o e os pontos da classe B (azuis) em outra, com uma clara separa√ß√£o, isso sugere que um hiperplano separador pode ser eficiente para classificar esses dados, o que possibilita o uso de m√©todos lineares como um perceptron.

### Pergunta Te√≥rica Avan√ßada: Como a escolha da m√©trica de dissimilaridade impacta o resultado do MDS e qual a rela√ß√£o entre essa escolha e o tipo de estrutura que se espera nos dados?
**Resposta:** A escolha da m√©trica de dissimilaridade √© crucial para o MDS, j√° que essa m√©trica define a base da estrutura a ser preservada na visualiza√ß√£o. Diferentes m√©tricas ressaltam diferentes aspectos dos dados:
*   **Dist√¢ncia Euclidiana:** Adequada para dados onde as dist√¢ncias lineares s√£o relevantes. Assume que os dados est√£o distribu√≠dos aproximadamente em um espa√ßo Euclidiano, e √© utilizada para casos como o Classical MDS.
*   **Dist√¢ncia de Mahalanobis:** Considera a vari√¢ncia e covari√¢ncia dos dados, sendo mais apropriada quando as vari√°veis n√£o s√£o independentes. √ötil quando os dados est√£o em diferentes escalas e n√£o devem ser interpretados de forma linear.
*   **Dist√¢ncia de Correla√ß√£o:** Foca na similaridade de padr√µes ou tend√™ncias, ignorando a escala. Adequada para dados que representam s√©ries temporais ou padr√µes de express√£o g√™nica.

```mermaid
graph LR
    subgraph "Dissimilarity Metrics"
        direction TB
        A["Euclidean Distance"] --> B["Linear Distances, Assumes Euclidean Space"]
        C["Mahalanobis Distance"] --> D["Considers variance and covariance"]
        E["Correlation Distance"] --> F["Focuses on pattern similarity"]
    end
```
A escolha da m√©trica afeta a estrutura do mapa MDS. Se a m√©trica escolhida n√£o for adequada para os dados, o mapa pode apresentar distor√ß√µes e n√£o refletir as verdadeiras rela√ß√µes entre os objetos. Por exemplo, usar a dist√¢ncia euclidiana em dados de texto, onde a semelhan√ßa de palavras pode ser n√£o-linear, pode levar a resultados ruins. A escolha deve refletir o tipo de estrutura que se espera nos dados. Se se espera que grupos sejam bem separados, uma m√©trica que enfatize a separa√ß√£o entre grupos √© mais adequada. Se a estrutura √© cont√≠nua, uma m√©trica que capture as varia√ß√µes gradativas pode ser mais √∫til.
**Lemma 4:** A escolha da m√©trica de dissimilaridade afeta a fun√ß√£o de custo que o MDS busca minimizar e, por consequ√™ncia, a configura√ß√£o final dos pontos no espa√ßo de baixa dimens√£o.
*   **Prova:** A fun√ß√£o de custo do MDS compara as dist√¢ncias no espa√ßo de baixa dimens√£o com as dissimilaridades originais. Ao mudar a m√©trica, mudamos o valor dessas dissimilaridades e o √≥timo da fun√ß√£o de custo, levando a uma configura√ß√£o diferente dos pontos. $\blacksquare$
```mermaid
graph LR
    subgraph "Metric Impact on MDS"
        direction TB
        A["Choice of Dissimilarity Metric"] --> B["Impacts Dissimilarity Values in Matrix D"]
        B --> C["Modifies MDS Cost Function"]
         C --> D["Results in different Low-dimensional Configurations"]
    end
```

> üí° **Exemplo Num√©rico:** Considere um conjunto de dados de s√©ries temporais de a√ß√µes. Se utilizarmos a dist√¢ncia euclidiana, enfatizaremos diferen√ßas na amplitude das s√©ries. Ao passo que se utilizarmos a dist√¢ncia de correla√ß√£o, vamos focar nas diferen√ßas dos padr√µes de varia√ß√£o, mesmo que as amplitudes sejam diferentes. Assim, a escolha da m√©trica depender√° do que se considera "similar". O MDS com a dist√¢ncia euclidiana pode agrupar a√ß√µes com amplitude semelhante, e com dist√¢ncia de correla√ß√£o pode agrupar a√ß√µes com padr√µes semelhantes de varia√ß√£o.

**Corol√°rio 4:**  Diferentes m√©tricas podem levar a interpreta√ß√µes distintas sobre a estrutura dos dados. √â essencial escolher a m√©trica com base no conhecimento do problema e nas caracter√≠sticas dos dados, conforme discutido em [^15.3.3].

> ‚ö†Ô∏è **Ponto Crucial**: A interpreta√ß√£o dos resultados do MDS deve levar em conta a m√©trica de dissimilaridade utilizada, pois diferentes m√©tricas podem enfatizar aspectos distintos dos dados. **Conforme discutido em [^15.3.3]**.

### Conclus√£o
O Multidimensional Scaling √© uma poderosa t√©cnica para visualizar a estrutura de dados de alta dimensionalidade em um espa√ßo de baixa dimens√£o. Embora n√£o seja um m√©todo de classifica√ß√£o em si, ele pode complementar outros m√©todos, como o Random Forest. A escolha da m√©trica de dissimilaridade, a interpreta√ß√£o dos resultados e a sua rela√ß√£o com outros m√©todos devem ser feitas com cuidado, seguindo os fundamentos estat√≠sticos apresentados. O Proximity Plot de Random Forests, explorado no cap√≠tulo, fornece um excelente exemplo de como usar MDS para visualizar as rela√ß√µes de dados.

### Footnotes
[^15.1]: *Conte√∫do extra√≠do conforme escrito no contexto e utilizado no cap√≠tulo* (Trecho de <Random Forests>)
[^15.2]: *Conte√∫do extra√≠do conforme escrito no contexto e utilizado no cap√≠tulo* (Trecho de <Random Forests>)
[^15.3]: *Conte√∫do extra√≠do conforme escrito no contexto e utilizado no cap√≠tulo* (Trecho de <Random Forests>)
[^15.3.1]: *Conte√∫do extra√≠do conforme escrito no contexto e utilizado no cap√≠tulo* (Trecho de <Random Forests>)
[^15.3.2]: *Conte√∫do extra√≠do conforme escrito no contexto e utilizado no cap√≠tulo* (Trecho de <Random Forests>)
[^15.3.3]: *Conte√∫do extra√≠do conforme escrito no contexto e utilizado no cap√≠tulo* (Trecho de <Random Forests>)
<!-- END DOCUMENT -->
