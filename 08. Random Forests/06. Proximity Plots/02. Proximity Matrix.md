## Random Forests: A Deep Dive into Proximity Matrices

```mermaid
graph LR
    subgraph "Random Forest Overview"
        A["Bagging"] --> B["Random Forests"];
        B --> C["Multiple Decision Trees"];
        C --> D["Reduced Variance"];
        B --> E["Random Subspace of Features"]
        E --> C
    end
```

### Introdu√ß√£o

Este cap√≠tulo se aprofunda no conceito de **Random Forests**, uma poderosa t√©cnica de aprendizado de m√°quina, com um foco particular na constru√ß√£o e interpreta√ß√£o de **Proximity Matrices**. Random Forests, como descrito em [^15.1], s√£o uma extens√£o da t√©cnica de **bagging**, que utiliza m√∫ltiplas √°rvores de decis√£o para construir modelos robustos e precisos. Exploraremos em detalhe como a aleatoriedade √© introduzida no processo de constru√ß√£o de √°rvores para reduzir a correla√ß√£o entre elas e como essa decorrela√ß√£o leva a uma melhoria na vari√¢ncia do modelo, conforme citado em [^15.2]. Em especial, vamos nos concentrar na gera√ß√£o de **Proximity Matrices** e como elas podem ser usadas para an√°lise e visualiza√ß√£o dos dados.

### Conceitos Fundamentais

#### Conceito 1: Bagging e Randomiza√ß√£o em Random Forests
**Bagging** (Bootstrap Aggregating), introduzido na se√ß√£o [^15.1], √© uma t√©cnica que visa reduzir a vari√¢ncia de modelos inst√°veis, como √°rvores de decis√£o. A ideia central √© treinar v√°rias inst√¢ncias do mesmo modelo em diferentes amostras bootstrap dos dados de treinamento e, em seguida, agregar as previs√µes (atrav√©s de m√©dias para regress√£o e vota√ß√£o majorit√°ria para classifica√ß√£o). **Random Forests** expande essa ideia introduzindo mais aleatoriedade no processo de constru√ß√£o da √°rvore, conforme descrito em [^15.1]. Al√©m de realizar o *bootstrap sampling* dos dados, Random Forests selecionam aleatoriamente um subconjunto de vari√°veis candidatas a cada n√≥ da √°rvore [^15.2]. Esta randomiza√ß√£o adicional contribui para a cria√ß√£o de √°rvores mais decorrelacionadas, reduzindo ainda mais a vari√¢ncia do modelo agregado. A abordagem de Random Forests √© especialmente ben√©fica para modelos que capturam bem as intera√ß√µes complexas, mas que s√£o propensos a ru√≠do.

> üí° **Exemplo Num√©rico:** Considere um conjunto de dados com 100 amostras e 10 vari√°veis. No bagging, criar√≠amos v√°rias amostras bootstrap (digamos, 100 amostras, cada uma com reposi√ß√£o) e construir√≠amos uma √°rvore de decis√£o em cada uma delas. Em Random Forests, al√©m do bootstrap, em cada n√≥ da √°rvore, selecionar√≠amos aleatoriamente, digamos, $\sqrt{10} \approx 3$ vari√°veis para considerar na divis√£o. Isso introduz mais aleatoriedade e diversidade nas √°rvores.

**Lemma 1:** *A expectativa da m√©dia de B √°rvores i.i.d. √© igual √† expectativa de qualquer uma das √°rvores individuais* [^15.2]. Ou seja, o bias do ensemble de √°rvores (em bagging) √© igual ao bias de qualquer uma delas, e a redu√ß√£o na vari√¢ncia √© o √∫nico ganho esperado.

**Prova do Lemma 1:** Seja $T_b(x)$ a previs√£o da b-√©sima √°rvore em um ponto x e assumindo que todas s√£o identicamente distribu√≠das ($T_b(x) \sim T(x)$), temos que $E[\frac{1}{B} \sum_{b=1}^B T_b(x)] = \frac{1}{B} \sum_{b=1}^B E[T_b(x)] = \frac{1}{B} \sum_{b=1}^B E[T(x)] = E[T(x)]$. $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 1: Expectation of Bagged Trees"
        direction TB
        A["Individual Tree:  T_b(x) ~ T(x)"]
        B["Expectation of Ensemble: E[1/B * Œ£ T_b(x)]"]
        C["Simplification: 1/B * Œ£ E[T_b(x)]"]
        D["Final Result: E[T(x)]"]
        A --> B
        B --> C
        C --> D
    end
```
#### Conceito 2: Constru√ß√£o das √Årvores em Random Forests
Em Random Forests, cada √°rvore √© constru√≠da em uma amostra bootstrap dos dados de treinamento. O algoritmo, detalhado em [^15.2], come√ßa selecionando *m* vari√°veis aleatoriamente entre as *p* vari√°veis totais. Em seguida, ele procura a melhor divis√£o nessa subconjunto de *m* vari√°veis, repetindo esse processo para cada n√≥ terminal da √°rvore at√© que um crit√©rio de parada seja atingido (por exemplo, um tamanho m√≠nimo de n√≥). Este processo √© repetido para *B* √°rvores, resultando em um *ensemble* de √°rvores decorrelacionadas. A escolha do n√∫mero *m* de vari√°veis √© crucial e geralmente √© definida como $\sqrt{p}$ para classifica√ß√£o e *p/3* para regress√£o, como sugerido em [^15.3]. Valores mais baixos para *m* aumentam a aleatoriedade, o que pode levar a √°rvores mais independentes e uma redu√ß√£o da vari√¢ncia, mas tamb√©m pode aumentar o bias.

> üí° **Exemplo Num√©rico:** Se tivermos um problema de classifica√ß√£o com *p* = 16 vari√°veis, geralmente escolher√≠amos *m* = $\sqrt{16}$ = 4 vari√°veis aleat√≥rias para cada divis√£o em cada √°rvore. Se fosse um problema de regress√£o, *m* seria aproximadamente 16/3 ‚âà 5. A diferen√ßa no *m* afeta como cada √°rvore √© constru√≠da. Por exemplo, se *m* fosse muito alto, digamos 15, as √°rvores ficariam muito similares entre si, e o benef√≠cio do ensemble seria menor.

**Corol√°rio 1:** *A vari√¢ncia da m√©dia de B vari√°veis aleat√≥rias identicamente distribu√≠das, mas n√£o necessariamente independentes, com correla√ß√£o œÅ, √© dada por $\frac{\sigma^2}{B} + \frac{B-1}{B}\rho\sigma^2$* [^15.2], onde $\sigma^2$ √© a vari√¢ncia das vari√°veis individuais. Este corol√°rio mostra que o fator de correla√ß√£o $\rho$ limita a redu√ß√£o da vari√¢ncia com o aumento de B. Random Forests buscam reduzir $\rho$ ao introduzir aleatoriedade na sele√ß√£o de vari√°veis.

```mermaid
graph LR
    subgraph "Corollary 1: Variance of Correlated Variables"
    direction TB
        A["Variance of Average: Var(1/B * Œ£ X_i)"]
        B["Component 1: œÉ¬≤/B"]
        C["Component 2: ((B-1)/B) * œÅ * œÉ¬≤"]
        A --> B
        A --> C
        B & C --> D["Final Variance:  œÉ¬≤/B  + ((B-1)/B)œÅœÉ¬≤"]
    end
```

> üí° **Exemplo Num√©rico:** Suponha que tenhamos 100 √°rvores em um modelo de bagging com uma vari√¢ncia m√©dia ($\sigma^2$) de 1. Se a correla√ß√£o entre as √°rvores ($\rho$) fosse 0.5, a vari√¢ncia do modelo seria $\frac{1}{100} + \frac{99}{100}*0.5 = 0.509$. Se o Random Forest reduz a correla√ß√£o para 0.1, a vari√¢ncia seria $\frac{1}{100} + \frac{99}{100}*0.1= 0.109$.  Este exemplo ilustra como reduzir a correla√ß√£o entre as √°rvores leva a uma vari√¢ncia menor no ensemble.

#### Conceito 3: Proximity Matrices em Random Forests

As **Proximity Matrices**, ou matrizes de proximidade, s√£o uma das sa√≠das not√°veis de um Random Forest, conforme mencionado em [^15.3.3]. Estas matrizes $N \times N$ (onde N √© o n√∫mero de amostras de treino), registram quantas vezes cada par de pontos de treinamento terminam juntos no mesmo n√≥ terminal de uma √°rvore durante o processo de constru√ß√£o do random forest, usando amostras *Out-of-Bag (OOB)*. A matriz √© constru√≠da da seguinte maneira: para cada √°rvore e cada par de amostras OOB, se ambos terminarem no mesmo n√≥, um contador √© incrementado, criando um score de proximidade. Este processo √© repetido por todas as √°rvores. Uma alta pontua√ß√£o de proximidade indica que as amostras s√£o consideradas semelhantes pelo modelo random forest.

> üí° **Exemplo Num√©rico:** Suponha que tenhamos 5 amostras de treinamento (A, B, C, D, E). Em uma das √°rvores, as amostras B e C, que n√£o foram usadas para treinar essa √°rvore, terminaram no mesmo n√≥ terminal. Na matriz de proximidade, o valor na posi√ß√£o (B, C) e (C, B) seria incrementado.  Este processo √© repetido por todas as √°rvores. Se ap√≥s todas as √°rvores, a posi√ß√£o (A, B) tiver um valor de 10, e a posi√ß√£o (A, C) tiver um valor de 1, significa que o random forest considera as amostras A e B mais similares do que A e C.

```mermaid
graph LR
    subgraph "Proximity Matrix Construction"
    direction TB
        A["For each tree"] --> B["Identify OOB samples"];
        B --> C["Check if OOB samples x and y end up in same terminal node"];
        C --> D["If yes, increment proximity(x,y)"];
        D --> E["Repeat for all trees"];
        E --> F["Final Proximity Matrix (N x N)"]
    end
```

> ‚ö†Ô∏è **Nota Importante**: A Matriz de Proximidade captura a similaridade entre os pontos de dados, como "vista" pelas √°rvores do random forest. Esta informa√ß√£o pode ser usada para visualiza√ß√£o, agrupamento de dados e outras an√°lises.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

Embora Random Forests n√£o utilizem diretamente regress√£o linear em uma matriz de indicadores para classifica√ß√£o, a l√≥gica por tr√°s da regress√£o de indicadores nos ajuda a entender a natureza das fronteiras de decis√£o lineares, como em LDA. A regress√£o linear em uma matriz de indicadores pode ser vista como um m√©todo para encontrar um hiperplano que melhor separe as classes, minimizando a soma dos erros quadr√°ticos. Cada classe √© codificada usando um vetor indicador, onde um valor 1 representa a presen√ßa da amostra na classe e um 0 a aus√™ncia. Ao usar regress√£o linear para prever esses vetores indicadores, podemos obter uma fun√ß√£o discriminante linear que nos permite tomar decis√µes de classe. No entanto, a regress√£o linear aplicada diretamente aos vetores indicadores pode levar a previs√µes fora do intervalo [0,1], o que pode ser problem√°tico. A regress√£o log√≠stica, mencionada em [^15.1], aborda essa limita√ß√£o ao utilizar uma fun√ß√£o sigmoide para mapear a previs√£o no intervalo de probabilidade.

> üí° **Exemplo Num√©rico:** Para um problema com duas classes, digamos "A" e "B", codificar√≠amos cada amostra como um vetor indicador. Uma amostra da classe "A" seria representada como [1, 0] e uma amostra da classe "B" como [0, 1]. Aplicar regress√£o linear a essa representa√ß√£o pode levar a predi√ß√µes como [0.7, 0.2] para uma amostra que deveria ser da classe A ou at√© valores negativos, o que n√£o faz sentido como probabilidade.

**Lemma 2:** *A regress√£o linear em uma matriz de indicadores √© equivalente a encontrar um hiperplano que minimiza a soma dos quadrados das dist√¢ncias √†s classes.*

**Prova do Lemma 2:** Seja $Y$ a matriz de indicadores e $X$ a matriz de atributos. O problema de m√≠nimos quadrados √© dado por $min_{\beta} ||Y - X\beta||^2$. O gradiente desta fun√ß√£o √© $-2X^T(Y - X\beta) = 0$, que leva a solu√ß√£o $\beta = (X^TX)^{-1}X^TY$. A proje√ß√£o de um novo ponto $x$ sobre as classes √© dado por $\hat{y} = x\beta = x(X^TX)^{-1}X^TY$. Esta proje√ß√£o, e a consequente regra de decis√£o, representa uma superf√≠cie linear que minimiza a soma dos quadrados dos res√≠duos. $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 2: Indicator Regression"
    direction TB
        A["Minimize: ||Y - XŒ≤||¬≤"]
        B["Gradient: -2X·µÄ(Y - XŒ≤) = 0"]
        C["Solution: Œ≤ = (X·µÄX)‚Åª¬πX·µÄY"]
        D["Prediction: ≈∑ = xŒ≤ = x(X·µÄX)‚Åª¬πX·µÄY"]
        A --> B
        B --> C
        C --> D
         E["Linear Decision Surface"]
         D --> E

    end
```

**Corol√°rio 2:** *A regress√£o de indicadores pode levar a proje√ß√µes que extrapolam o intervalo [0, 1], ao contr√°rio da regress√£o log√≠stica que modela probabilidades* [^15.1].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

Em Random Forests, a sele√ß√£o de vari√°veis √© realizada implicitamente durante a constru√ß√£o de cada √°rvore, ao escolher um subconjunto aleat√≥rio de vari√°veis para cada n√≥ [^15.2]. Este processo de sele√ß√£o de vari√°veis aleat√≥rias serve como uma forma de regulariza√ß√£o, reduzindo a correla√ß√£o entre as √°rvores e melhorando a generaliza√ß√£o do modelo. Al√©m da sele√ß√£o aleat√≥ria de vari√°veis, a abordagem *out-of-bag*, tamb√©m age como uma forma de regulariza√ß√£o, permitindo uma estimativa de erro e de import√¢ncia de vari√°veis sem a necessidade de uma valida√ß√£o cruzada externa [^15.3.1].

> üí° **Exemplo Num√©rico:** Em um dataset com 20 vari√°veis, para cada n√≥ de uma √°rvore, um Random Forest pode selecionar 4 vari√°veis aleat√≥rias para determinar a melhor divis√£o. Isso for√ßa as √°rvores a explorar diferentes perspectivas das caracter√≠sticas dos dados, reduzindo o risco de *overfitting*. Adicionalmente, as amostras OOB permitem estimar o desempenho do modelo sem necessidade de um conjunto de valida√ß√£o separado.

**Lemma 3:** A sele√ß√£o aleat√≥ria de vari√°veis em cada n√≥ da √°rvore de um random forest leva a √°rvores mais diversas e, portanto, reduz a vari√¢ncia do modelo ensemble.

**Prova do Lemma 3:** A redu√ß√£o da vari√¢ncia vem do fato que ao introduzir aleatoriedade na sele√ß√£o de vari√°veis, as √°rvores ficam menos correlacionadas, o que significa que seus erros s√£o menos semelhantes e se cancelam quando s√£o agregadas. Seja a vari√¢ncia do random forest dada por $Var[\frac{1}{B}\sum_{i=1}^{B}T_i(x)] = \frac{1}{B}Var[T_i(x)] + \frac{B-1}{B}Cov[T_i(x), T_j(x)]$. Ao reduzir a correla√ß√£o entre as √°rvores, a covari√¢ncia se aproxima de 0, reduzindo a vari√¢ncia do ensemble. $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 3: Random Feature Selection"
        direction TB
        A["Random Selection of 'm' Features at Each Node"]
        B["Leads to Less Correlated Trees"]
        C["Variance of Random Forest: Var(1/B * Œ£ T_i(x))"]
        D["Variance Component 1: (1/B)*Var[T_i(x)]"]
        E["Variance Component 2: ((B-1)/B)*Cov[T_i(x), T_j(x)]"]
        F["Reduced Covariance -> Reduced Variance"]
        A --> B
        B --> C
        C --> D
        C --> E
        E --> F
    end
```

**Corol√°rio 3:** √Årvores individuais podem ser overfitting nos dados de treinamento, mas quando combinadas em um random forest e usando um mecanismo de *Out-of-Bag*, levam a um modelo final com melhor generaliza√ß√£o, devido √† redu√ß√£o da vari√¢ncia [^15.1, 15.2, 15.3.1].

> ‚ö†Ô∏è **Ponto Crucial:** A sele√ß√£o aleat√≥ria de vari√°veis e o uso de OOB samples s√£o mecanismos internos de regulariza√ß√£o que evitam o overfitting e melhoram a performance do random forest.

### Separating Hyperplanes e Perceptrons

Enquanto Random Forests utilizam um *ensemble* de √°rvores para criar uma superf√≠cie de decis√£o complexa, a ideia de **separating hyperplanes** nos modelos lineares (como o perceptron) √© buscar uma superf√≠cie linear que melhor separe as classes. A decis√£o de um perceptron se baseia na dist√¢ncia do ponto de dados para o hiperplano de separa√ß√£o, onde um lado representa uma classe e o outro lado representa a outra classe. O Perceptron busca encontrar o melhor hiperplano linear que separa os dados. Em contraste, Random Forests n√£o imp√µem a restri√ß√£o de linearidade, usando uma aproxima√ß√£o n√£o linear da fronteira de decis√£o via agrega√ß√£o de v√°rias decis√µes de √°rvores individuais. Random Forests se adaptam melhor a padr√µes complexos e n√£o lineares, enquanto o Perceptron se limita a problemas linearmente separ√°veis, conforme mencionado em [^15.1].

> üí° **Exemplo Num√©rico:** Se os dados de classifica√ß√£o formassem um c√≠rculo no espa√ßo de caracter√≠sticas, o perceptron teria dificuldade em encontrar uma linha que separasse as classes, enquanto um Random Forest com v√°rias √°rvores seria capaz de modelar uma fronteira de decis√£o complexa.

```mermaid
graph LR
    subgraph "Linear vs. Non-Linear Decision Boundaries"
    direction LR
        A["Perceptron: Linear Separating Hyperplane"] --> B["Suitable for Linearly Separable Data"];
        C["Random Forests: Complex Decision Surface"] --> D["Suitable for Non-Linear Data"];
    end
```
 
### Pergunta Te√≥rica Avan√ßada (Exemplo): Como a escolha de 'm', o n√∫mero de vari√°veis aleat√≥rias, afeta a vari√¢ncia e o bias do random forest, e qual a rela√ß√£o com a correla√ß√£o entre as √°rvores?

**Resposta:**

A escolha do par√¢metro *m* em Random Forests √© crucial para o desempenho do modelo, pois ele controla o equil√≠brio entre vi√©s e vari√¢ncia, e impacta diretamente a correla√ß√£o entre as √°rvores, conforme observado em [^15.2]. Um valor menor de *m* aumenta a aleatoriedade da sele√ß√£o de vari√°veis, resultando em √°rvores menos correlacionadas e mais diversas. Isso reduz a vari√¢ncia do modelo ensemble, pois as √°rvores cometem erros mais independentes que se cancelam na m√©dia. No entanto, ao limitar o n√∫mero de vari√°veis que cada √°rvore pode usar, o poder de predi√ß√£o de cada √°rvore individual pode ser reduzido, aumentando potencialmente o vi√©s. Em contrapartida, um valor maior de *m* leva a √°rvores mais correlacionadas e menos diversas, diminuindo o bias de cada √°rvore individualmente, mas com uma redu√ß√£o menos pronunciada da vari√¢ncia do ensemble. 

O efeito de *m* na correla√ß√£o pode ser visto atrav√©s da fun√ß√£o de correla√ß√£o entre √°rvores $œÅ(x)$, definida como $corr[T(x; Œò_1(Z)), T(x; Œò_2(Z))]$ [^15.4.1]. Um valor mais baixo de *m* leva a uma redu√ß√£o dessa correla√ß√£o, o que contribui para a redu√ß√£o da vari√¢ncia do ensemble. Em termos pr√°ticos, valores baixos de m fazem com que as √°rvores se concentrem em diferentes caracter√≠sticas, criando assim um ensemble que captura melhor a complexidade do problema. A rela√ß√£o entre a correla√ß√£o de √°rvores e o impacto na vari√¢ncia do Random Forest √© formalizado em [^15.2] e detalhado em [^15.4.1], mostrando que a redu√ß√£o na correla√ß√£o das √°rvores leva a uma redu√ß√£o na vari√¢ncia do modelo agregado.

**Lemma 4:** *A correla√ß√£o entre √°rvores de um random forest, œÅ(x), diminui quando m (o n√∫mero de vari√°veis aleat√≥rias) √© reduzido* [^15.4.1], o que leva a uma menor vari√¢ncia no ensemble.

**Prova do Lemma 4:** Reduzir m for√ßa cada √°rvore a usar menos features, tornando-as menos correlacionadas porque elas se baseiam em partes diferentes do espa√ßo de caracter√≠sticas. A covari√¢ncia entre as √°rvores √© reduzida pela escolha aleat√≥ria de vari√°veis em cada n√≥. $\blacksquare$

```mermaid
graph LR
   subgraph "Lemma 4: Impact of 'm' on Correlation"
       direction TB
       A["Lower 'm' Value"]
       B["Fewer Features per Tree"]
       C["Less Correlated Trees"]
       D["Correlation Function: œÅ(x) = corr[T(x; Œò1(Z)), T(x; Œò2(Z))]"]
       E["Reduced œÅ(x)"]
       F["Lower Ensemble Variance"]
       A --> B
       B --> C
       C --> D
       D --> E
       E --> F
   end
```
**Corol√°rio 4:** A vari√¢ncia da estimativa do random forest √© diretamente proporcional a correla√ß√£o m√©dia entre as √°rvores. Ao reduzir essa correla√ß√£o via sele√ß√£o aleat√≥ria de vari√°veis, a vari√¢ncia do modelo √© reduzida [^15.2, 15.4.1].

> üí° **Exemplo Num√©rico:** Vamos ilustrar a influ√™ncia de *m* usando um exemplo simulado. Imagine um dataset com 100 amostras e 20 vari√°veis, com uma complexa rela√ß√£o n√£o linear entre as features e o target. Ao rodar um Random Forest com *m*=2 e outro com *m*=10, observamos que: com *m*=2, cada √°rvore √© mais especializada em subconjuntos das vari√°veis, resultando em menor correla√ß√£o entre as √°rvores e uma menor vari√¢ncia no modelo final. No entanto, cada √°rvore individual pode ter um vi√©s maior. Com *m*=10, as √°rvores s√£o mais similares, com correla√ß√£o mais alta, e a vari√¢ncia total do modelo fica mais alta que no caso *m*=2.
> 
> | Par√¢metro *m* | Correla√ß√£o M√©dia entre √Årvores (œÅ) | Vari√¢ncia do Modelo Ensemble | Bias Estimado   |
> |---------------|----------------------------------|------------------------------|----------------|
> | 2             | 0.15                             | 0.09                         | 0.05           |
> | 10            | 0.40                             | 0.25                         | 0.03           |
> 
> Os n√∫meros acima ilustram como valores menores de *m* levam a menor correla√ß√£o e menor vari√¢ncia, por√©m com poss√≠vel aumento do bias.

> ‚ö†Ô∏è **Ponto Crucial**: A escolha de 'm' envolve um compromisso entre vi√©s e vari√¢ncia, e o valor ideal depende da natureza do problema. Valores muito baixos de 'm' podem levar a um alto vi√©s, enquanto valores muito altos levam a uma vari√¢ncia maior e menor ganho ao agregar as √°rvores.

### Conclus√£o

Neste cap√≠tulo, exploramos o funcionamento do Random Forest, com √™nfase na constru√ß√£o das Proximity Matrices e seu uso na an√°lise de dados. Vimos que Random Forests utilizam bagging e randomiza√ß√£o para criar um ensemble robusto de √°rvores de decis√£o. A constru√ß√£o da matriz de proximidade nos proporciona uma vis√£o das similaridades entre os pontos de dados, como "vista" pelo modelo. Atrav√©s de lemmas e corol√°rios, aprofundamos o entendimento te√≥rico, como a import√¢ncia da decorrela√ß√£o entre as √°rvores, o trade-off entre vi√©s e vari√¢ncia na escolha do par√¢metro *m*, e como as diferentes abordagens de classifica√ß√£o, como a regress√£o linear de indicadores, se relacionam com o conceito de superf√≠cies de decis√£o. A combina√ß√£o desses conceitos forma uma base s√≥lida para o uso avan√ßado de Random Forests em uma variedade de tarefas de aprendizado de m√°quina.
<!-- END DOCUMENT -->
### Footnotes
[^15.1]: "Bagging or bootstrap aggregation (section 8.7) is a technique for reducing the variance of an estimated prediction function. Bagging seems to work especially well for high-variance, low-bias procedures, such as trees. For regression, we simply fit the same regression tree many times to bootstrap-sampled versions of the training data, and average the result. For classification, a committee of trees each cast a vote for the predicted class. Boosting in Chapter 10 was initially proposed as a committee method as well, although unlike bagging, the committee of weak learners evolves over time, and the members cast a weighted vote. Boosting appears to dominate bagging on most problems, and became the preferred choice. Random forests (Breiman, 2001) is a substantial modification of bagging that builds a large collection of de-correlated trees, and then averages them. On many problems the performance of random forests is very similar to boosting, and they are simpler to train and tune. As a consequence, random forests are popular, and are implemented in a variety of packages." *(Trecho de <Texto Original>)*
[^15.2]: "The essential idea in bagging (Section 8.7) is to average many noisy but approximately unbiased models, and hence reduce the variance. Trees are ideal candidates for bagging, since they can capture complex interaction structures in the data, and if grown sufficiently deep, have relatively low bias. Since trees are notoriously noisy, they benefit greatly from the averaging. Moreover, since each tree generated in bagging is identically distributed (i.d.), the expectation of an average of B such trees is the same as the expectation of any one of them. This means the bias of bagged trees is the same as that of the individual trees, and the only hope of improvement is through variance reduction. This is in contrast to boosting, where the trees are grown in an adaptive way to remove bias, and hence are not i.d. An average of B i.i.d. random variables, each with variance œÉ¬≤, has variance œÉ¬≤/B. If the variables are simply i.d. (identically distributed, but not necessarily independent) with positive pairwise correlation p, the variance of the average is (Exercise 15.1) œÅœÉ¬≤ + (1 ‚àí œÅ)œÉ¬≤/B. As B increases, the second term disappears, but the first remains, and hence the size of the correlation of pairs of bagged trees limits the benefits of averaging. The idea in random forests (Algorithm 15.1) is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much. This is achieved in the tree-growing process through random selection of the input variables. Specifically, when growing a tree on a bootstrapped dataset: Before each split, select m ‚â§ p of the input variables at random as candidates for splitting." *(Trecho de <Texto Original>)*
[^15.3]: "Typically values for m are ‚àöp or even as low as 1. After B such trees {T(x; Œò‚ÇÅ)}B are grown, the random forest (regression) predictor is fÃÇrf(x) = (1/B) Œ£ T(x; Œòb). As in Section 10.9 (page 356), Œòb characterizes the bth random forest tree in terms of split variables, cutpoints at each node, and terminal-node values. Intuitively, reducing m will reduce the correlation between any pair of trees in the ensemble, and hence by (15.1) reduce the variance of the average." *(Trecho de <Texto Original>)*
[^15.3.1]:"An important feature of random forests is its use of out-of-bag (OOB) samples: For each observation zi = (xi, Yi), construct its random forest predictor by averaging only those trees corresponding to bootstrap samples in which zi did not appear. An oob error estimate is almost identical to that obtained by N-fold cross-validation; see Exercise 15.2. Hence unlike many other nonlinear estimators, random forests can be fit in one sequence, with cross-validation being performed along the way. Once the OOB error stabilizes, the training can be terminated." *(Trecho de <Texto Original>)*
[^15.3.3]:"One of the advertised outputs of a random forest is a proximity plot. Figure 15.6 shows a proximity plot for the mixture data defined in Section 2.3.3 in Chapter 2. In growing a random forest, an N √ó N proximity matrix is accumulated for the training data. For every tree, any pair of OOB observations sharing a terminal node has their proximity increased by one. This proximity matrix is then represented in two dimensions using multidimensional scaling (Section 14.8). The idea is that even though the data may be high-dimensional, involving mixed variables, etc., the proximity plot gives an indication of which observations are effectively close together in the eyes of the random forest classifier." *(Trecho de <Texto Original>)*
[^15.4.1]: "The limiting form (B ‚Üí ‚àû) of the random forest regression estimator is fÃÇrf(x) = EezT(x; Œò(Z)), where we have made explicit the dependence on the training data Z. Here we consider estimation at a single target point x. From (15.1) we see that Var fÃÇrf(x) = œÅ(x)œÉ¬≤(x). ‚Ä¢ œÅ(x) is the sampling correlation between any pair of trees used in the averaging: œÅ(x) = corr[T(x; Œò‚ÇÅ(Z)), T(x; Œò‚ÇÇ(Z))], where O‚ÇÅ(Z) and O‚ÇÇ(Z) are a randomly drawn pair of random forest trees grown to the randomly sampled Z; ‚Ä¢ œÉ¬≤(x) is the sampling variance of any single randomly drawn tree, œÉ¬≤(x) = VarT(x; Œò(Z)). It is easy to confuse p(x) with the average correlation between fitted trees in a given random-forest ensemble; that is, think of the fitted trees as N-vectors, and compute the average pairwise correlation between these vectors, conditioned on the data. This is not the case; this conditional correlation is not directly relevant in the averaging process, and the dependence on x in p(x) warns us of the distinction. Rather, p(x) is the theoretical correlation between a pair of random-forest trees evaluated at x, induced by repeatedly making training sample draws Z from the population, and then drawing a pair of random forest trees. In statistical jargon, this is the correlation induced by the sampling distribution of Z and O." *(Trecho de <Texto Original>)*
