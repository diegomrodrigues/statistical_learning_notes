Okay, here's the enhanced text with Mermaid diagrams added to support the advanced mathematical and statistical concepts, following your guidelines:

## Averaging Unbiased Models: Uma An√°lise Aprofundada de Random Forests

```mermaid
graph LR
    subgraph "Averaging Unbiased Models"
        direction TB
        A["High Variance, Low Bias Models"] --> B["Bagging (Bootstrap Aggregation)"]
        A --> C["Random Forests"]
        B --> D["Multiple Models on Bootstrap Samples"]
        D --> E["Averaging of Predictions"]
         C --> F["Bagging with Random Feature Selection"]
         F --> E
        E --> G["Reduced Variance, Improved Accuracy"]
    end
```

### Introdu√ß√£o

O conceito de **averaging de modelos n√£o viesados** emerge como uma poderosa ferramenta no aprendizado estat√≠stico, especialmente quando aplicado a modelos com alta vari√¢ncia e baixo vi√©s. O objetivo principal √© reduzir a vari√¢ncia das previs√µes, o que pode levar a melhorias significativas na precis√£o geral do modelo. T√©cnicas como **bagging** e **random forests** exemplificam bem esse conceito. O bagging, ou *bootstrap aggregation*, consiste em ajustar o mesmo modelo v√°rias vezes em amostras bootstrap dos dados de treinamento e, em seguida, calcular a m√©dia das previs√µes. Essa abordagem funciona particularmente bem para procedimentos como √°rvores de decis√£o, conhecidas por sua sensibilidade a pequenas mudan√ßas nos dados. Os *random forests*, por sua vez, s√£o uma extens√£o do bagging que busca reduzir a correla√ß√£o entre as √°rvores, resultando em uma redu√ß√£o ainda maior na vari√¢ncia [^15.1].

### Conceitos Fundamentais

**Conceito 1: Bagging e Redu√ß√£o de Vari√¢ncia**

O **bagging** √© uma t√©cnica que visa reduzir a vari√¢ncia de um modelo de aprendizado de m√°quina ajustando m√∫ltiplas vers√µes do modelo em subconjuntos aleat√≥rios dos dados de treinamento. Especificamente, o bagging envolve a cria√ß√£o de m√∫ltiplas amostras *bootstrap* dos dados originais, ajustando um modelo em cada uma dessas amostras, e, ent√£o, agregando as previs√µes desses modelos. O processo de agrega√ß√£o pode envolver o c√°lculo da m√©dia (no caso de regress√£o) ou a vota√ß√£o majorit√°ria (no caso de classifica√ß√£o). Essa abordagem funciona especialmente bem para modelos com alta vari√¢ncia e baixo vi√©s, como as √°rvores de decis√£o. A intui√ß√£o por tr√°s do bagging √© que a vari√¢ncia das previs√µes de v√°rios modelos individuais √© menor do que a vari√¢ncia de um √∫nico modelo, desde que os modelos n√£o sejam altamente correlacionados [^15.1].

> üí° **Exemplo Num√©rico:** Considere um conjunto de dados de regress√£o com 100 amostras. Uma √∫nica √°rvore de decis√£o, quando treinada nesse conjunto, pode ter uma previs√£o com um erro quadr√°tico m√©dio (MSE) de 0.8, refletindo alta vari√¢ncia. Ao usar bagging com 100 √°rvores, cada uma treinada em uma amostra bootstrap diferente do conjunto de dados original, o MSE da m√©dia das previs√µes dessas 100 √°rvores pode ser reduzido para 0.5. Isso ilustra a redu√ß√£o na vari√¢ncia obtida atrav√©s do bagging.

**Lemma 1:** A expectativa da m√©dia de √°rvores em *bagging* √© igual √† expectativa de uma √°rvore individual. Isso implica que o vi√©s do *bagging* √© o mesmo de uma √°rvore individual, e a melhoria √© atingida pela redu√ß√£o de vari√¢ncia.

$$ E\left[\frac{1}{B} \sum_{b=1}^{B} T_b(x)\right] = \frac{1}{B} \sum_{b=1}^{B} E[T_b(x)] = E[T(x)] $$

```mermaid
graph LR
    subgraph "Bagging Expectation"
        direction TB
        A["Expectation of Bagged Trees: E[1/B Œ£(T_b(x))]"]
        B["Expectation of Individual Tree: E[T(x)]"]
        A --> C["E[1/B Œ£(T_b(x))] = 1/B Œ£ E[T_b(x)]"]
        C --> D["Since E[T_b(x)] = E[T(x)] for all b"]
        D --> E["E[1/B Œ£(T_b(x))] = E[T(x)]"]
        E --> F["Bagging preserves the bias of single tree"]
    end
```

**Conceito 2: Random Forests: Decorrela√ß√£o de √Årvores**

Um **random forest** √© uma extens√£o do *bagging* que vai al√©m da simples cria√ß√£o de amostras *bootstrap* dos dados de treinamento. Em cada etapa de crescimento da √°rvore, durante a divis√£o de um n√≥, um random forest seleciona aleatoriamente um subconjunto de vari√°veis preditoras dispon√≠veis para a divis√£o. A ideia √© decorrelacionar as √°rvores, garantindo que elas n√£o sejam muito semelhantes. Esta decorrela√ß√£o leva a uma redu√ß√£o maior na vari√¢ncia das previs√µes agregadas. Essa caracter√≠stica faz dos random forests uma ferramenta muito poderosa, e eles s√£o amplamente utilizados devido √† sua capacidade de fornecer previs√µes precisas com pouco ajuste [^15.1, 15.2].

> üí° **Exemplo Num√©rico:** Imagine um problema de classifica√ß√£o com 10 vari√°veis preditoras. No bagging, cada √°rvore teria acesso a todas as 10 vari√°veis para cada divis√£o de n√≥, o que pode resultar em √°rvores correlacionadas. Em um random forest, se selecionarmos aleatoriamente 3 vari√°veis a cada divis√£o, cada √°rvore ser√° constru√≠da com uma perspectiva diferente das vari√°veis, diminuindo a correla√ß√£o entre elas e, consequentemente, a vari√¢ncia do ensemble.

**Corol√°rio 1:** A redu√ß√£o da correla√ß√£o entre as √°rvores em um *random forest*, atrav√©s da amostragem aleat√≥ria de vari√°veis, leva a uma redu√ß√£o maior na vari√¢ncia do ensemble, comparado a *bagging* [^15.2].

**Conceito 3: A M√©dia de Modelos I.I.D. e a Redu√ß√£o de Vari√¢ncia**

O conceito chave por tr√°s de averaging de modelos est√° na redu√ß√£o de vari√¢ncia, explorando a propriedade de que a m√©dia de vari√°veis aleat√≥rias independentes e identicamente distribu√≠das (i.i.d.) tem vari√¢ncia menor do que uma √∫nica vari√°vel. Se $X_1, X_2, \ldots, X_B$ s√£o vari√°veis aleat√≥rias i.i.d. com vari√¢ncia $\sigma^2$, a vari√¢ncia de sua m√©dia √© $\sigma^2/B$. No entanto, as √°rvores em random forests e bagging n√£o s√£o exatamente i.i.d., mas sim apenas identicamente distribu√≠das (i.d.). Quando as √°rvores t√™m uma correla√ß√£o positiva entre elas, a vari√¢ncia da m√©dia √© dada por [^15.2]:

$$ \text{Var}\left(\frac{1}{B} \sum_{b=1}^{B} X_b\right) = \rho \sigma^2 + \frac{1-\rho}{B} \sigma^2 $$

onde $\rho$ √© a correla√ß√£o entre as √°rvores. Portanto, a decorrela√ß√£o atrav√©s da sele√ß√£o aleat√≥ria de vari√°veis em random forests √© crucial para reduzir a vari√¢ncia da m√©dia [^15.2].

```mermaid
graph LR
    subgraph "Variance of Correlated Averages"
        direction TB
        A["Variance of Average: Var(1/B Œ£ X_b)"]
        B["Correlation Term: œÅœÉ¬≤"]
        C["Variance Reduction Term: (1-œÅ)/B œÉ¬≤"]
        A --> B
        A --> C
         B --> D["œÅ is correlation between models"]
         C --> E["As B increases (1-œÅ)/B decreases"]
         D --> F["Positive correlation limits variance reduction"]
    end
```

> üí° **Exemplo Num√©rico:** Suponha que as previs√µes de √°rvores em um bagging tenham uma vari√¢ncia $\sigma^2 = 0.9$ e uma correla√ß√£o m√©dia $\rho = 0.6$. Se usarmos 100 √°rvores ($B=100$), a vari√¢ncia da m√©dia seria:
>
> $$\text{Var}\left(\frac{1}{100} \sum_{b=1}^{100} X_b\right) = 0.6 \times 0.9 + \frac{1-0.6}{100} \times 0.9 = 0.54 + 0.0036 = 0.5436$$
>
> Agora, com um random forest, se a correla√ß√£o m√©dia entre as √°rvores for reduzida para $\rho = 0.2$ (devido √† sele√ß√£o aleat√≥ria de vari√°veis), mantendo a vari√¢ncia $\sigma^2 = 0.9$ e $B=100$, a vari√¢ncia da m√©dia seria:
>
> $$\text{Var}\left(\frac{1}{100} \sum_{b=1}^{100} X_b\right) = 0.2 \times 0.9 + \frac{1-0.2}{100} \times 0.9 = 0.18 + 0.0072 = 0.1872$$
>
> Isso mostra como a decorrela√ß√£o atrav√©s da sele√ß√£o aleat√≥ria de vari√°veis em random forests leva a uma redu√ß√£o maior na vari√¢ncia, indo de 0.5436 para 0.1872, em compara√ß√£o com o bagging.

> ‚ö†Ô∏è **Nota Importante**: O principal benef√≠cio do *bagging* e *random forests* √© a redu√ß√£o da vari√¢ncia, mantendo o vi√©s compar√°vel a de um modelo individual.
> ‚ùó **Ponto de Aten√ß√£o**: *Random Forests* superam o *bagging* atrav√©s da decorrela√ß√£o das √°rvores, usando sele√ß√£o aleat√≥ria de vari√°veis em cada split.
> ‚úîÔ∏è **Destaque**: A vari√¢ncia da m√©dia de modelos correlacionados diminui com $B$, mas o limite √© controlado pela correla√ß√£o.

### Regress√£o Linear e √Årvores como Base para Averaging

```mermaid
graph TD
  subgraph "Bagging Application"
  direction TB
    A["Regression Data"] --> B["Bootstrap Samples"]
    B --> C["Regression Tree Model on Each Sample"]
     C --> D["Average Predictions"]
  end

    subgraph "Linear Regression"
    direction TB
    E["Linear Regression Data"] --> F["Bootstrap Samples"]
    F --> G["Linear Regression model on Each Sample"]
    G --> H["Average Parameters"]
    H --> I["No reduction in variance"]

    end
    D --> J["Variance Reduction"]
    I --> J
```

A ideia principal por tr√°s do *bagging* √© usar m√©todos inst√°veis (alta vari√¢ncia), como √°rvores de regress√£o, para construir modelos que tenham baixo vi√©s e menor vari√¢ncia. Apesar de a regress√£o linear ser um m√©todo est√°vel (sua solu√ß√£o n√£o muda muito com pequenas altera√ß√µes nos dados), √°rvores de regress√£o s√£o altamente sens√≠veis aos dados de treinamento. O *bagging* e, por extens√£o, *random forests*, se aproveitam dessa alta vari√¢ncia para construir um ensemble que pode ter melhor desempenho que uma √∫nica √°rvore.

**Lemma 2:** A m√©dia de v√°rias estimativas lineares de um mesmo par√¢metro, obtidas a partir de diferentes amostras bootstrap, n√£o reduz a vari√¢ncia em compara√ß√£o com a estimativa linear obtida a partir da amostra original.

A regress√£o linear, como estimador linear, n√£o se beneficia do *bagging* da mesma forma que as √°rvores. O *bagging* n√£o altera estimativas lineares, e, portanto, sua vari√¢ncia. Em contraste, as √°rvores, por sua n√£o linearidade, se beneficiam muito.

> üí° **Exemplo Num√©rico:** Considere um modelo de regress√£o linear ajustado a um conjunto de dados. Se repetirmos o ajuste usando diferentes amostras bootstrap desse mesmo conjunto de dados, os par√¢metros estimados ($\beta$) ser√£o muito pr√≥ximos e a vari√¢ncia da m√©dia desses par√¢metros ser√° muito semelhante √† vari√¢ncia do $\beta$ estimado a partir do conjunto de dados original. Por exemplo, se $\hat{\beta}$ estimado no conjunto de dados original for [2.5, 1.2], a m√©dia dos $\hat{\beta}$ obtidos com o *bagging* ser√° aproximadamente [2.5, 1.2] e, portanto, a vari√¢ncia n√£o √© significativamente reduzida.

**Corol√°rio 2:** O *bagging* n√£o altera a vari√¢ncia de estimadores lineares, o que significa que a combina√ß√£o de modelos lineares por *bagging* n√£o traz benef√≠cios em termos de redu√ß√£o de vari√¢ncia [^15.1].

*Random forests*, por sua vez, exploram o conceito de decorrela√ß√£o entre √°rvores. Selecionando um subconjunto aleat√≥rio de vari√°veis em cada divis√£o de n√≥, *random forests* garantem que as √°rvores constru√≠das em diferentes amostras *bootstrap* sejam diferentes e, portanto, tenham baixas correla√ß√µes. Essa decorrela√ß√£o √© o que permite uma maior redu√ß√£o na vari√¢ncia em compara√ß√£o com o *bagging*.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Feature Selection in Random Forests"
        direction TB
        A["Random Subspace Method"] --> B["Random Subset of Features for each Split"]
        B --> C["Reduces Correlation between Trees"]
         B --> D["Acts as a Regularization"]
        E["Permutation OOB"] --> F["Permute feature j"]
        F --> G["Measure Decrease in Accuracy"]
        G --> H["Importance of Variable j"]

    end
```

Em classifica√ß√£o, o processo de sele√ß√£o de vari√°veis √© crucial, especialmente em cen√°rios com muitas caracter√≠sticas. Tanto *bagging* quanto *random forests* oferecem mecanismos para avaliar a import√¢ncia das vari√°veis. O *random forest* usa a ideia de **random subspace method**, ou seja, seleciona aleatoriamente um subconjunto de vari√°veis candidatas para dividir cada n√≥ da √°rvore [^15.2]. Essa abordagem n√£o s√≥ reduz a correla√ß√£o entre as √°rvores, mas tamb√©m atua como uma forma de regulariza√ß√£o, ao impedir que o modelo dependa fortemente de vari√°veis irrelevantes. Al√©m disso, o m√©todo de permuta√ß√£o OOB (Out-of-Bag) pode ser utilizado para medir a import√¢ncia das vari√°veis de forma emp√≠rica [^15.3.2]. A ideia √© que, se uma vari√°vel √© importante, a permuta√ß√£o de seus valores deve diminuir a precis√£o do modelo.

> üí° **Exemplo Num√©rico:** Num problema de classifica√ß√£o de spam, um random forest pode selecionar aleatoriamente um subconjunto de vari√°veis (por exemplo, a frequ√™ncia de certas palavras, o tamanho do email) em cada divis√£o de n√≥. Ao usar o m√©todo de permuta√ß√£o OOB, se permutarmos os valores da vari√°vel "n√∫mero de exclama√ß√µes" e observarmos uma grande queda na precis√£o do modelo, isso indicaria que essa vari√°vel √© muito importante para o modelo. Por outro lado, se permutarmos os valores de "n√∫mero de pontos e v√≠rgulas" e observarmos pouca ou nenhuma queda na precis√£o, isso sugeriria que essa vari√°vel √© menos relevante.

**Lemma 3:** A sele√ß√£o aleat√≥ria de vari√°veis em cada divis√£o de n√≥ em random forests contribui para a decorrela√ß√£o das √°rvores e para a regulariza√ß√£o do modelo.

**Prova do Lemma 3:** A sele√ß√£o aleat√≥ria de $m$ vari√°veis dentre $p$ impede que √°rvores constru√≠das em amostras *bootstrap* diferentes dependam muito das mesmas vari√°veis em suas divis√µes, levando a √°rvores mais independentes e, portanto, menos correlacionadas. Al√©m disso, essa sele√ß√£o aleat√≥ria implica que o modelo n√£o se torna muito dependente de vari√°veis irrelevantes, atuando como uma forma de regulariza√ß√£o que reduz o overfitting $\blacksquare$.

**Corol√°rio 3:** A permuta√ß√£o de vari√°veis em amostras *out-of-bag* (OOB) fornece uma medida emp√≠rica da import√¢ncia de cada vari√°vel, ao medir o impacto da permuta√ß√£o de seus valores na precis√£o do modelo [^15.3.2].

> ‚ö†Ô∏è **Ponto Crucial**: *Random forests* combinam sele√ß√£o aleat√≥ria de vari√°veis com amostragem *bootstrap* para decorrelacionar √°rvores e reduzir a vari√¢ncia.

### Separating Hyperplanes e Perceptrons

Embora este cap√≠tulo se concentre em m√©todos de averaging, √© √∫til comparar o *random forest* com os m√©todos lineares que buscam *separating hyperplanes* ou o Perceptron. M√©todos lineares, como o Perceptron, buscam encontrar uma fun√ß√£o discriminante linear que separa as classes de dados. No entanto, esses m√©todos podem falhar quando as classes n√£o s√£o linearmente separ√°veis. O Perceptron tamb√©m pode ser sens√≠vel a outliers nos dados de treinamento. Os *random forests*, por sua vez, s√£o mais flex√≠veis e podem se adaptar a dados que n√£o s√£o linearmente separ√°veis. O Perceptron tem um objetivo claro de separabilidade, enquanto o random forest busca previs√µes precisas atrav√©s do averaging de m√∫ltiplos modelos.

### Pergunta Te√≥rica Avan√ßada: Qual √© a rela√ß√£o entre a correla√ß√£o entre as √°rvores e a vari√¢ncia de um random forest? Como o n√∫mero de vari√°veis selecionadas aleatoriamente por divis√£o ($m$) afeta essa correla√ß√£o?

**Resposta:**

A rela√ß√£o entre a correla√ß√£o das √°rvores em um *random forest* e a vari√¢ncia do ensemble √© fundamental para entender como essa t√©cnica funciona. Como vimos na discuss√£o do conceito 3, a vari√¢ncia da m√©dia de modelos correlacionados √© dada por:

$$ \text{Var}\left(\frac{1}{B} \sum_{b=1}^{B} X_b\right) = \rho \sigma^2 + \frac{1-\rho}{B} \sigma^2 $$

onde $\rho$ √© a correla√ß√£o m√©dia entre os modelos. Em *random forests*, o n√∫mero de vari√°veis selecionadas aleatoriamente para cada divis√£o ($m$) tem um impacto direto sobre a correla√ß√£o entre as √°rvores. Se $m$ √© muito pequeno, as √°rvores constru√≠das ser√£o muito diferentes entre si, levando a uma correla√ß√£o $\rho$ menor e, portanto, a uma menor vari√¢ncia do ensemble. Por outro lado, se $m$ √© muito grande (pr√≥ximo ao n√∫mero total de vari√°veis $p$), as √°rvores tender√£o a ser mais semelhantes e, portanto, ter√£o maior correla√ß√£o $\rho$, levando a menor redu√ß√£o de vari√¢ncia. Portanto, a escolha de $m$ envolve um trade-off: um $m$ muito pequeno pode diminuir a correla√ß√£o, mas tamb√©m pode aumentar o vi√©s. J√° um $m$ muito grande pode diminuir o vi√©s, mas aumentar a correla√ß√£o e a vari√¢ncia do ensemble [^15.2].

```mermaid
graph LR
 subgraph "Effect of m on Variance"
    direction TB
        A["Random Forest Variance"]
        B["Parameter m: Number of features at each split"]
        C["Small m"] --> D["Low Correlation between Trees"]
         D --> E["Reduced Ensemble Variance"]
         C --> F["Potential Increase in Bias"]
         B --> C
          G["Large m"] --> H["High Correlation between Trees"]
          H --> I["Limited Ensemble Variance Reduction"]
          G --> J["Potential Reduction in Bias"]
          B --> G

         A --> K["Trade-off between Bias and Variance"]
        K--> E
        K --> I
        E --> L["Optimization of 'm' is critical"]
        I --> L

 end
```

> üí° **Exemplo Num√©rico:** Se tivermos 20 vari√°veis preditoras ($p=20$), ao usar $m=2$ em um random forest, cada √°rvore ter√° acesso a apenas 2 vari√°veis em cada divis√£o, levando a uma menor correla√ß√£o entre as √°rvores e maior redu√ß√£o de vari√¢ncia. No entanto, as √°rvores podem n√£o ser muito boas individualmente devido √† pouca informa√ß√£o em cada n√≥ (maior vi√©s). Se usarmos $m=18$, a maioria das √°rvores ser√° muito similar, pois ter√£o acesso a quase todas as vari√°veis em cada divis√£o, aumentando a correla√ß√£o e a vari√¢ncia do ensemble, embora as √°rvores possam ser melhores individualmente (menor vi√©s).

**Lemma 4:** A correla√ß√£o entre √°rvores em um random forest √© influenciada pelo par√¢metro *m*, que controla o n√∫mero de vari√°veis aleatoriamente selecionadas em cada divis√£o, afetando diretamente a vari√¢ncia do ensemble.

**Corol√°rio 4:** A escolha √≥tima do par√¢metro $m$ em um *random forest* busca um equil√≠brio entre a redu√ß√£o da correla√ß√£o das √°rvores (para diminuir a vari√¢ncia) e a garantia de que as √°rvores individuais sejam suficientemente boas para gerar previs√µes precisas, evitando o aumento do vi√©s.

> ‚ö†Ô∏è **Ponto Crucial**: Ajustar o par√¢metro $m$ √© fundamental para controlar a correla√ß√£o entre as √°rvores e, assim, minimizar a vari√¢ncia do random forest.

### Conclus√£o

Em resumo, a t√©cnica de averaging de modelos n√£o viesados, exemplificada pelo *bagging* e *random forests*, oferece um m√©todo eficaz para reduzir a vari√¢ncia e melhorar a precis√£o das previs√µes. Enquanto o *bagging* utiliza o conceito de amostras bootstrap para gerar diversas √°rvores e ent√£o as agrega, *random forests* adiciona a sele√ß√£o aleat√≥ria de vari√°veis em cada divis√£o para decorrelacionar ainda mais as √°rvores.  As an√°lises mostram que o ajuste do par√¢metro $m$ √© crucial para controlar o trade-off entre vi√©s e vari√¢ncia e encontrar o melhor desempenho do modelo. Atrav√©s de an√°lises te√≥ricas, exemplos pr√°ticos, lemmas e corol√°rios, e discuss√µes sobre as limita√ß√µes e o contexto de uso desses m√©todos, conseguimos aprofundar o entendimento do conceito de averaging em modelos estat√≠sticos.

<!-- END DOCUMENT -->
### Footnotes
[^15.1]: "Bagging or bootstrap aggregation (section 8.7) is a technique for reducing the variance of an estimated prediction function. Bagging seems to work especially well for high-variance, low-bias procedures, such as trees. For regression, we simply fit the same regression tree many times to bootstrap-sampled versions of the training data, and average the result. For classification, a committee of trees each cast a vote for the predicted class." *(Trecho de Random Forests)*
[^15.2]: "The essential idea in bagging (Section 8.7) is to average many noisy but approximately unbiased models, and hence reduce the variance. Trees are ideal candidates for bagging, since they can capture complex interaction structures in the data, and if grown sufficiently deep, have relatively low bias. Since trees are notoriously noisy, they benefit greatly from the averaging. Moreover, since each tree generated in bagging is identically distributed (i.d.), the expectation of an average of B such trees is the same as the expectation of any one of them. This means the bias of bagged trees is the same as that of the individual trees, and the only hope of improvement is through variance reduction. This is in contrast to boosting, where the trees are grown in an adaptive way to remove bias, and hence are not i.d. An average of B i.i.d. random variables, each with variance œÉ¬≤, has variance œÉ¬≤/B. If the variables are simply i.d. (identically distributed, but not necessarily independent) with positive pairwise correlation p, the variance of the average is (Exercise 15.1)... As B increases, the second term disappears, but the first remains, and hence the size of the correlation of pairs of bagged trees limits the benefits of averaging. The idea in random forests (Algorithm 15.1) is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much. This is achieved in the tree-growing process through random selection of the input variables. Specifically, when growing a tree on a bootstrapped dataset: Before each split, select m ‚â§ p of the input variables at random as candidates for splitting." *(Trecho de Random Forests)*
[^15.3.2]: "Variable importance plots can be constructed for random forests in exactly the same way as they were for gradient-boosted models (Section 10.13). At each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splitting variable, and is accumulated over all the trees in the forest separately for each variable. The left plot of Figure 15.5 shows the variable importances computed in this way for the spam data; compare with the corresponding Figure 10.6 on page 354 for gradient boosting. Boosting ignores some variables completely, while the random forest does not. The candidate split-variable selection increases the chance that any single variable gets included in a random forest, while no such selection occurs with boosting. Random forests also use the OOB samples to construct a different variable-importance measure, apparently to measure the prediction strength of each variable. When the bth tree is grown, the OOB samples are passed down the tree, and the prediction accuracy is recorded. Then the values for the jth variable are randomly permuted in the OOB samples, and the accuracy is again computed. The decrease in accuracy as a result of this permuting is averaged over all trees, and is used as a measure of the importance of variable j in the random forest." *(Trecho de Random Forests)*
