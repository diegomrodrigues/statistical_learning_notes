## Random Forests: Feature Selection at Split Points

```mermaid
graph LR
    subgraph "Random Forest Process"
        A["Bootstrap Sampling"] --> B["Multiple Decision Trees"]
        B --> C["Random Feature Selection at Each Split"]
        C --> D["Prediction Aggregation"]
    end
```

### Introdu√ß√£o

O **Random Forest** [^15.1] √© um algoritmo de aprendizado de m√°quina vers√°til, usado tanto para classifica√ß√£o quanto para regress√£o. Ele se baseia na t√©cnica de **bagging** (bootstrap aggregation) [^15.1], que visa reduzir a vari√¢ncia de estimadores inst√°veis, como √°rvores de decis√£o. Em ess√™ncia, o Random Forest constr√≥i m√∫ltiplas √°rvores de decis√£o a partir de diferentes amostras de bootstrap dos dados de treinamento e, em seguida, combina as previs√µes dessas √°rvores por meio de um processo de vota√ß√£o (para classifica√ß√£o) ou por meio da m√©dia (para regress√£o). Uma caracter√≠stica fundamental do Random Forest que o distingue do bagging simples √© a **sele√ß√£o aleat√≥ria de um subconjunto de vari√°veis (features) em cada n√≥ de cada √°rvore** [^15.1]. Esta caracter√≠stica, que chamamos de **sele√ß√£o de *m* features**, √© o foco principal deste cap√≠tulo. A capacidade do Random Forest de reduzir a correla√ß√£o entre as √°rvores e, assim, reduzir a vari√¢ncia da previs√£o final, depende crucialmente dessa sele√ß√£o aleat√≥ria de features [^15.2].

### Conceitos Fundamentais

**Conceito 1: O Problema da Classifica√ß√£o e Regress√£o com √Årvores de Decis√£o**
O uso de √°rvores de decis√£o para classifica√ß√£o ou regress√£o √© intuitivo. A √°rvore particiona o espa√ßo de entrada em regi√µes menores, de forma hier√°rquica, onde a predi√ß√£o √© feita com base na maioria da classe predominante (classifica√ß√£o) ou pela m√©dia dos valores da vari√°vel resposta (regress√£o) [^15.2]. √Årvores, no entanto, s√£o estimadores *high-variance and low-bias*, o que significa que elas podem se ajustar muito bem aos dados de treinamento, mas podem ter um desempenho ruim em novos dados. Em outras palavras, o modelo pode ser inst√°vel com pequenas varia√ß√µes nos dados de treinamento. A sele√ß√£o de *m* features a cada split visa controlar essa instabilidade [^15.2].

> üí° **Exemplo Num√©rico:** Imagine que temos um conjunto de dados de treinamento com 100 amostras e 5 vari√°veis preditoras ($p=5$). Uma √∫nica √°rvore de decis√£o pode criar um modelo complexo, ajustando-se excessivamente aos dados de treinamento, o que resulta em alta vari√¢ncia. Por exemplo, uma pequena mudan√ßa em alguns pontos de dados de treinamento pode levar a uma √°rvore muito diferente. O Random Forest aborda isso construindo v√°rias √°rvores, cada uma com um subconjunto aleat√≥rio de vari√°veis (digamos, $m=2$) em cada split. Isso reduz a sensibilidade a pontos de dados individuais e, portanto, a vari√¢ncia do modelo.

```mermaid
graph LR
    subgraph "Decision Tree Characteristics"
        direction TB
        A["Single Decision Tree"] --> B["High Variance"]
        A --> C["Low Bias"]
        B --> D["Susceptible to Overfitting"]
    end
    subgraph "Random Forest Approach"
        E["Multiple Decision Trees"] --> F["Feature Randomization at Each Split"]
        F --> G["Reduced Variance"]
    end
```

**Lemma 1:** A esperan√ßa da m√©dia de um conjunto de √°rvores geradas por bagging √© igual √† esperan√ßa de uma √∫nica √°rvore, ou seja, o vi√©s do conjunto √© igual ao vi√©s de uma √∫nica √°rvore [^15.2].
$$E\left[\frac{1}{B}\sum_{b=1}^{B}T_b(x)\right] = E[T(x)]$$
onde $T_b(x)$ √© a predi√ß√£o da *b*-√©sima √°rvore e $T(x)$ √© a predi√ß√£o de uma √°rvore aleat√≥ria. Esta propriedade √© essencial para justificar o uso de bagging, dado que o objetivo n√£o √© reduzir o vi√©s, mas sim a vari√¢ncia [^15.2]. $\blacksquare$

**Conceito 2: Linear Discriminant Analysis (LDA)**
O LDA, discutido em outros contextos, busca encontrar um hiperplano que separe diferentes classes. Ele assume que os dados s√£o normalmente distribu√≠dos com covari√¢ncias iguais para cada classe [^15.2]. O LDA pode ser comparado a m√©todos baseados em √°rvores, pois ambos visam encontrar fronteiras de decis√£o. No entanto, o LDA √© um m√©todo param√©trico que imp√µe uma forma funcional sobre os dados, enquanto o random forest √© um m√©todo n√£o param√©trico.

**Corol√°rio 1:** A combina√ß√£o de √°rvores (bagging) em Random Forests n√£o altera o vi√©s, mas reduz a vari√¢ncia se as √°rvores forem menos correlacionadas. A sele√ß√£o de *m* vari√°veis √© o mecanismo essencial para reduzir a correla√ß√£o entre as √°rvores. [^15.2]

```mermaid
graph LR
    subgraph "Bagging and Random Forest"
       A["Bagging: Multiple Trees"] --> B["Averaging Predictions"]
       B --> C["Reduces Variance"]
       C --> D["Requires Low Correlation"]
       D --> E["Random Feature Selection at Split Point"]
       E --> C
    end
```

**Conceito 3: Regress√£o Log√≠stica**
A regress√£o log√≠stica, outro m√©todo para classifica√ß√£o, modela a probabilidade de uma vari√°vel bin√°ria usando uma fun√ß√£o log√≠stica. Ao contr√°rio das √°rvores de decis√£o, ela √© um modelo linear. A sele√ß√£o de vari√°veis em modelos lineares, como regress√£o log√≠stica, √© importante para reduzir complexidade e evitar overfitting. No random forest, essa sele√ß√£o √© feita a cada split, tornando o processo mais robusto [^15.2].

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
    subgraph "Regression for Classification"
        A["Classes encoded as Binary Variables"] --> B["Linear Regression for each class"]
        B --> C["Predicted Class is the one with the highest prediction"]
    end
```

A regress√£o linear pode ser adaptada para problemas de classifica√ß√£o usando uma matriz de indicadores. As classes s√£o codificadas como vari√°veis bin√°rias e, em seguida, uma regress√£o linear √© feita em cada vari√°vel indicadora. A classe prevista √© aquela que corresponde √† vari√°vel indicadora com a maior predi√ß√£o. Embora seja uma abordagem simples, ela apresenta limita√ß√µes quando as classes s√£o muito correlacionadas [^15.1], [^15.2]. O random forest, com a sele√ß√£o de *m* features, √© uma alternativa mais robusta para lidar com este problema.

**Lemma 2:** A vari√¢ncia da m√©dia de vari√°veis aleat√≥rias *identicamente distribu√≠das*, mas *n√£o independentes*, com vari√¢ncia $\sigma^2$ e correla√ß√£o par a par $\rho$, √© dada por:
$$Var\left[\frac{1}{B}\sum_{b=1}^B X_b\right] = \frac{\sigma^2}{B} + \frac{B-1}{B}\rho \sigma^2$$
Esta equa√ß√£o demonstra explicitamente como a correla√ß√£o entre as √°rvores limita a redu√ß√£o da vari√¢ncia por meio da m√©dia [^15.2]. $\blacksquare$

```mermaid
graph TB
    subgraph "Variance of Averaged Correlated Variables"
        A["Var(Average of X_b)"] --> B["Term 1: œÉ¬≤/B"]
        A --> C["Term 2: (B-1)/B * œÅ * œÉ¬≤"]
        B & C --> D["Combined Variance"]
    end
```

> üí° **Exemplo Num√©rico:** Suponha que temos 10 √°rvores ($B=10$) com vari√¢ncia $\sigma^2 = 1$ e correla√ß√£o m√©dia $\rho = 0.6$. Sem a sele√ß√£o de features, a vari√¢ncia da m√©dia das previs√µes seria:
> $$Var\left[\frac{1}{10}\sum_{b=1}^{10} X_b\right] = \frac{1}{10} + \frac{10-1}{10}(0.6)(1) = 0.1 + 0.54 = 0.64$$
> Se pudermos reduzir a correla√ß√£o entre as √°rvores, digamos para $\rho=0.2$, usando a sele√ß√£o de *m* features, a vari√¢ncia da m√©dia seria:
> $$Var\left[\frac{1}{10}\sum_{b=1}^{10} X_b\right] = \frac{1}{10} + \frac{10-1}{10}(0.2)(1) = 0.1 + 0.18 = 0.28$$
> Claramente, a redu√ß√£o na correla√ß√£o levou a uma redu√ß√£o significativa na vari√¢ncia da m√©dia das previs√µes.

**Corol√°rio 2:** Quando *B* tende ao infinito, o termo $\frac{\sigma^2}{B}$ desaparece, e a vari√¢ncia da m√©dia torna-se $\rho \sigma^2$. Portanto, quanto menor a correla√ß√£o $\rho$, menor a vari√¢ncia da m√©dia. O objetivo da sele√ß√£o de *m* features √© reduzir a correla√ß√£o $\rho$ entre as √°rvores de decis√£o. [^15.2]

‚ÄúA regress√£o linear para classifica√ß√£o pode levar a extrapola√ß√µes fora do intervalo \[0,1], enquanto a regress√£o log√≠stica fornece probabilidades entre 0 e 1, sendo esta uma vantagem da regress√£o log√≠stica em problemas de classifica√ß√£o [^15.1], [^15.2]. No entanto, a regress√£o linear ainda pode ser vantajosa quando o principal objetivo √© a fronteira de decis√£o linear e o foco n√£o √© a estimativa de probabilidades. [^15.2]‚Äù

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Variable Selection Methods"
        direction TB
        A["Random Forest: Feature selection at each split"]
        B["Regularization (L1, L2): Penalize Coefficients"]
        A --> C["Reduces Overfitting"]
        B --> C
    end
    subgraph "Regularization for Logistic Regression"
        D["L1 Regularization"] --> E["Sparsity (Zero Coefficients)"]
        E --> F["Feature Selection"]
    end
```

A sele√ß√£o de vari√°veis em modelos de classifica√ß√£o visa identificar as features mais relevantes para a predi√ß√£o. No contexto do random forest, essa sele√ß√£o ocorre a cada split da √°rvore, escolhendo aleatoriamente *m* features dentre as *p* dispon√≠veis. Essa abordagem √© diferente da regulariza√ß√£o (L1 ou L2), que penaliza os coeficientes de modelos lineares para reduzir a complexidade e evitar overfitting [^15.2].
Em regress√£o log√≠stica, a regulariza√ß√£o L1 pode levar a solu√ß√µes esparsas, ou seja, alguns coeficientes ser√£o exatamente zero, o que promove a sele√ß√£o de vari√°veis mais relevantes para a classifica√ß√£o [^15.2]. A sele√ß√£o de *m* features em random forest atinge objetivo similar de redu√ß√£o de overfitting mas por um processo diferente, atrav√©s da aleatoriza√ß√£o da sele√ß√£o de features e subsequente m√©dia.

**Lemma 3:** Em regress√£o log√≠stica, a penaliza√ß√£o L1 leva a solu√ß√µes esparsas, o que significa que alguns coeficientes s√£o exatamente zero [^15.2]. Essa propriedade pode ser derivada da formula√ß√£o do problema de otimiza√ß√£o, que busca o m√≠nimo da soma da *log-likelihood* negativa com o termo de regulariza√ß√£o L1:
$$ \min_{\beta} \left( -\sum_{i=1}^{n} \left[ y_i \log(\sigma(\mathbf{x}_i^T\beta)) + (1-y_i) \log(1-\sigma(\mathbf{x}_i^T\beta)) \right] + \lambda \sum_{j=1}^p |\beta_j| \right) $$
onde $\sigma(\cdot)$ √© a fun√ß√£o log√≠stica, $\lambda$ √© o par√¢metro de regulariza√ß√£o, e $\beta_j$ s√£o os coeficientes [^15.2]. $\blacksquare$

```mermaid
graph LR
    subgraph "L1 Regularization Optimization"
        A["Cost Function with L1 Penalty"]
        B["Negative Log-Likelihood Term"]
        C["Œª * Sum of Absolute Values of Coefficients"]
         A --> B
        A --> C
    end
```

> üí° **Exemplo Num√©rico:** Considere um problema de classifica√ß√£o bin√°ria com 3 features ($p=3$) usando regress√£o log√≠stica. Sem regulariza√ß√£o, a regress√£o log√≠stica pode atribuir pesos n√£o nulos a todas as 3 features. Aplicando regulariza√ß√£o L1 com um certo $\lambda$, a otimiza√ß√£o pode for√ßar um dos coeficientes, digamos $\beta_2$, para exatamente zero, resultando em um modelo que apenas usa as features 1 e 3. Em contraste, um random forest com a sele√ß√£o de *m* features realiza a sele√ß√£o de vari√°veis *em cada split*, de forma estoc√°stica, resultando em um modelo que, em m√©dia, d√° menos peso √†s vari√°veis menos importantes e com menor frequ√™ncia na divis√£o das √°rvores.

**Prova do Lemma 3:** A prova envolve mostrar que, ao minimizar a fun√ß√£o de custo com penaliza√ß√£o L1, alguns coeficientes s√£o levados a zero. Isto ocorre porque a penaliza√ß√£o L1 n√£o √© diferenci√°vel na origem, e a otimiza√ß√£o leva a solu√ß√µes em que alguns coeficientes s√£o zerados exatamente, promovendo sparsity. [^15.2] $\blacksquare$
**Corol√°rio 3:** A esparsidade induzida pela regulariza√ß√£o L1 em modelos de regress√£o log√≠stica leva a modelos mais interpret√°veis e robustos ao overfitting, j√° que as vari√°veis menos relevantes s√£o efetivamente exclu√≠das do modelo. [^15.2]

> ‚ö†Ô∏è **Ponto Crucial**: A combina√ß√£o de regulariza√ß√£o L1 e L2 (Elastic Net) oferece um balan√ßo entre sparsity e estabilidade, combinando as vantagens de ambos os tipos de regulariza√ß√£o. [^15.2]
### Separating Hyperplanes e Perceptrons

Hiperplanos separadores s√£o usados em algoritmos como SVM (Support Vector Machines) para encontrar uma fronteira linear √≥tima que maximize a margem entre as classes [^15.2]. O Perceptron, por sua vez, √© um algoritmo de aprendizado mais antigo que busca um hiperplano capaz de separar as classes. Random forests, ao contr√°rio desses m√©todos, n√£o dependem diretamente de hiperplanos, mas sim de particionamentos hier√°rquicos do espa√ßo, e a sele√ß√£o de *m* features auxilia na diversifica√ß√£o dessas parti√ß√µes [^15.2].

### Pergunta Te√≥rica Avan√ßada: Qual o impacto da escolha do valor de *m* (n√∫mero de features a serem consideradas em cada split) no desempenho do Random Forest?
**Resposta:**
A escolha do valor de *m* √© crucial para o desempenho de um Random Forest [^15.2]. Se *m* for muito pequeno, as √°rvores ser√£o pouco correlacionadas, levando a uma redu√ß√£o na vari√¢ncia. Por outro lado, se *m* for muito grande (pr√≥ximo a *p*), as √°rvores ser√£o mais correlacionadas, e a redu√ß√£o na vari√¢ncia ser√° menor. Em casos extremos, se $m=p$, o m√©todo se torna equivalente a bagging simples, sem a aleatoriedade na sele√ß√£o de features [^15.2]. Normalmente, o valor de $m$ para classifica√ß√£o √© dado como $m=\lfloor \sqrt{p} \rfloor$ e para regress√£o como $m= \lfloor \frac{p}{3} \rfloor$, mas a melhor escolha de $m$ depende dos dados [^15.2].

```mermaid
graph LR
    subgraph "Impact of m on Random Forest"
        direction TB
        A["Small m"] --> B["Low Correlation between Trees"]
        B --> C["High Variance Reduction"]
        A --> D["Potentially Higher Bias"]

        E["Large m (close to p)"] --> F["High Correlation between Trees"]
        F --> G["Lower Variance Reduction"]
        E --> H["Similar to Bagging"]
        C & G --> I["Optimal m as a trade-off"]
    end
```

> üí° **Exemplo Num√©rico:** Suponha que temos um problema de classifica√ß√£o com $p=16$ vari√°veis preditoras. Usando a regra geral, o valor de *m* para classifica√ß√£o seria $\lfloor \sqrt{16} \rfloor = 4$. Isso significa que em cada n√≥ de cada √°rvore, o algoritmo escolher√° aleatoriamente 4 features para decidir qual usar para a divis√£o. Se us√°ssemos $m=1$, as √°rvores seriam menos correlacionadas, mas tamb√©m menos capazes de capturar relacionamentos entre as vari√°veis, potencialmente aumentando o vi√©s. Se us√°ssemos $m=16$, estar√≠amos usando todas as vari√°veis em cada split, e isso seria equivalente a um bagging simples, sem a redu√ß√£o na correla√ß√£o entre as √°rvores, e consequentemente menos redu√ß√£o de vari√¢ncia.

**Lemma 4:** A correla√ß√£o entre duas √°rvores aleat√≥rias no Random Forest, em fun√ß√£o da escolha de *m* √©:
$$ \rho(x) = \text{corr}[T(x; \Theta_1(Z)), T(x; \Theta_2(Z))]$$
onde $\Theta_1(Z)$ e $\Theta_2(Z)$ s√£o √°rvores geradas a partir de amostras bootstrap aleat√≥rias, e a correla√ß√£o $\rho(x)$ √© uma fun√ß√£o de *m* [^15.2].

**Corol√°rio 4:** Reduzindo o valor de *m*, a correla√ß√£o entre as √°rvores diminui, o que leva a uma maior redu√ß√£o de vari√¢ncia do modelo Random Forest. Entretanto, ao reduzir excessivamente *m*, as √°rvores podem se tornar muito fracas e apresentar vieses maiores, resultando em performance sub-√≥tima. A escolha de *m* envolve um trade-off entre vi√©s e vari√¢ncia. [^15.2]
> ‚ö†Ô∏è **Ponto Crucial**: A escolha de *m* √© um par√¢metro de ajuste que pode afetar significativamente o desempenho do Random Forest, sendo necess√°ria aten√ß√£o especial para otimizar esse valor para cada problema espec√≠fico. [^15.2]
### Conclus√£o

A sele√ß√£o de *m* features em cada split √© uma caracter√≠stica central do Random Forest, que contribui significativamente para a sua capacidade de reduzir a vari√¢ncia e evitar o overfitting. Essa abordagem, combinada com a aleatoriedade do bootstrap e a m√©dia das predi√ß√µes, torna o Random Forest um m√©todo de aprendizado de m√°quina robusto e eficaz, tanto para problemas de classifica√ß√£o quanto de regress√£o [^15.2]. <!-- END DOCUMENT -->

### Footnotes
[^15.1]: "Bagging or bootstrap aggregation (section 8.7) is a technique for reducing the variance of an estimated prediction function. Bagging seems to work especially well for high-variance, low-bias procedures, such as trees. For regression, we simply fit the same regression tree many times to bootstrap-sampled versions of the training data, and average the result. For classification, a committee of trees each cast a vote for the predicted class. Random forests (Breiman, 2001) is a substantial modification of bagging that builds a large collection of de-correlated trees, and then averages them. On many problems the performance of random forests is very similar to boosting, and they are simpler to train and tune. As a consequence, random forests are popular, and are implemented in a variety of packages." *[Trecho do documento Random Forests]*
[^15.2]: "The essential idea in bagging (Section 8.7) is to average many noisy but approximately unbiased models, and hence reduce the variance. Trees are ideal candidates for bagging, since they can capture complex interaction structures in the data, and if grown sufficiently deep, have relatively low bias. Since trees are notoriously noisy, they benefit greatly from the averaging. Moreover, since each tree generated in bagging is identically distributed (i.d.), the expectation of an average of B such trees is the same as the expectation of any one of them. This means the bias of bagged trees is the same as that of the individual trees, and the only hope of improvement is through variance reduction. This is in contrast to boosting, where the trees are grown in an adaptive way to remove bias, and hence are not i.d. As B increases, the second term disappears, but the first remains, and hence the size of the correlation of pairs of bagged trees limits the benefits of averaging. The idea in random forests (Algorithm 15.1) is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much. This is achieved in the tree-growing process through random selection of the input variables." *[Trecho do documento Random Forests]*
[^15.3]: "For each observation zi = (xi, Yi), construct its random forest predictor by averaging only those trees corresponding to bootstrap samples in which zi did not appear. An oob error estimate is almost identical to that obtained by N-fold cross-validation; see Exercise 15.2." *[Trecho do documento Random Forests]*
[^15.4]: "When the number of variables is large, but the fraction of relevant variables small, random forests are likely to perform poorly with small m. At each split the chance can be small that the relevant variables will be selected." *[Trecho do documento Random Forests]*
[^15.5]: "The limiting form (B ‚Üí ‚àû) of the random forest regression estimator is frf(x) = EezT(x; Œò(Œñ)), where we have made explicit the dependence on the training data Z. Here we consider estimation at a single target point x. From (15.1) we see that Varfrf(x) = p(x)œÉ¬≤(x)." *[Trecho do documento Random Forests]*
[^15.6]: "Bias(x) = Œº(x) ‚Äì Ezfrf(x) = Œº(x) ‚Äì ŒïŒñŒïezT(x; Œò(Œñ)). This is also typically greater (in absolute terms) than the bias of an un- pruned tree grown to Z, since the randomization and reduced sample space impose restrictions. Hence the improvements in prediction obtained by bagging or random forests are solely a result of variance reduction." *[Trecho do documento Random Forests]*
[^15.7]: "In practice the best values for these parameters will depend on the problem, and they should be treated as tuning parameters." *[Trecho do documento Random Forests]*
[^15.8]: "Typically values for m are ‚àöp or even as low as 1." *[Trecho do documento Random Forests]*
[^15.9]: "For classification, the default value for m is [‚àöp] and the minimum node size is one." *[Trecho do documento Random Forests]*
[^15.10]: "For regression, the default value for m is [p/3] and the minimum node size is five." *[Trecho do documento Random Forests]*
[^15.11]: "At each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splitting variable, and is accumulated over all the trees in the forest separately for each variable." *[Trecho do documento Random Forests]*
[^15.12]: "Random forests also use the OOB samples to construct a different variable- importance measure, apparently to measure the prediction strength of each variable." *[Trecho do documento Random Forests]*
[^15.13]: "The tree-growing algorithm finds an "optimal" path to that observation, choosing the most informative predictors from those at its disposal. The averaging process assigns weights to these training responses, which ultimately vote for the prediction. Hence via the random-forest voting mechanism, those observations close to the target point get assigned weights‚Äîan equivalent kernel-which combine to form the classification decision." *[Trecho do documento Random Forests]*
[^15.14]: "It is easy to confuse p(x) with the average correlation between fitted trees in a given random-forest ensemble; that is, think of the fitted trees as N-vectors, and compute the average pairwise correlation between these vectors, conditioned on the data. This is not the case; this conditional correlation is not directly relevant in the averaging process, and the dependence on x in p(x) warns us of the distinction. Rather, p(x) is the theoretical correlation between a pair of random-forest trees evaluated at x, induced by repeatedly making training sample draws Z from the population, and then drawing a pair of random forest trees. In statistical jargon, this is the correlation induced by the sampling distribution of Z and O." *[Trecho do documento Random Forests]*
