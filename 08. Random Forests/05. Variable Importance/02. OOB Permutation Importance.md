## Random Forests: OOB Permutation Importance and Beyond

```mermaid
graph LR
    subgraph "Random Forest Overview"
        A["Bagging"] --> B["Random Forests"];
        B --> C["Out-of-Bag Samples"];
        B --> D["Variable Importance"];
        D --> E["OOB permutation importance"];
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#fcc,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o
Os **Random Forests** s√£o um m√©todo poderoso de aprendizado de m√°quina, amplamente utilizado tanto para classifica√ß√£o quanto para regress√£o, como apresentado em [^15.1]. Sua popularidade decorre de sua capacidade de construir modelos complexos com relativamente pouco ajuste, atrav√©s da combina√ß√£o de √°rvores de decis√£o descorrelacionadas [^15.1]. O princ√≠pio fundamental por tr√°s do Random Forest √© a **agrega√ß√£o bootstrap**, ou *bagging* [^15.1], uma t√©cnica que visa reduzir a vari√¢ncia de estimativas combinando v√°rias √°rvores ajustadas a diferentes amostras dos dados. No entanto, o Random Forest aprimora o bagging introduzindo um elemento adicional de aleatoriedade durante a constru√ß√£o das √°rvores, especificamente na sele√ß√£o de vari√°veis candidatas a cada divis√£o [^15.2]. A **OOB (Out-of-Bag) permutation importance** emerge como uma ferramenta crucial para entender a import√¢ncia relativa das vari√°veis no modelo, sem a necessidade de valida√ß√£o cruzada adicional [^15.3.1]. Este cap√≠tulo aprofundar√° esses conceitos, focando especialmente na OOB permutation importance e como ela complementa outros m√©todos de avalia√ß√£o de import√¢ncia vari√°vel.

### Conceitos Fundamentais
**Conceito 1: Bagging e o Problema de Vari√¢ncia**
O **bagging** √© uma t√©cnica que visa reduzir a vari√¢ncia de modelos, como √°rvores de decis√£o, que s√£o suscet√≠veis a *overfitting* [^15.1]. Ele opera criando v√°rias amostras de bootstrap a partir dos dados de treinamento e, em seguida, ajustando um modelo a cada uma dessas amostras [^15.1]. A previs√£o final √© obtida agregando as previs√µes de todos os modelos, como por exemplo, por meio de uma m√©dia para regress√£o ou vota√ß√£o majorit√°ria para classifica√ß√£o [^15.1]. Essa abordagem √© especialmente √∫til para modelos com baixo vi√©s e alta vari√¢ncia [^15.1]. A vari√¢ncia da m√©dia de vari√°veis aleat√≥rias i.i.d (independentes e identicamente distribu√≠das) √© dada por $\frac{\sigma^2}{B}$ onde $\sigma^2$ √© a vari√¢ncia individual, e $B$ o n√∫mero de amostras [^15.2]. No entanto, quando as vari√°veis s√£o apenas identicamente distribu√≠das e t√™m correla√ß√£o positiva $ \rho$, essa vari√¢ncia √© dada por $\rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2$ [^15.2], mostrando que a correla√ß√£o entre os modelos limita o benef√≠cio da m√©dia, portanto, reduzir essa correla√ß√£o √© essencial [^15.2].

```mermaid
graph LR
    subgraph "Bagging Variance"
        A["Variance of i.i.d. trees: œÉ¬≤/B"] --> B["Independent Trees"]
        C["Variance of correlated trees: œÅœÉ¬≤ + (1-œÅ)œÉ¬≤/B"] --> D["Correlated Trees"]
    end
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#fcc,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#fcc,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Suponha que temos √°rvores de decis√£o com vari√¢ncia $\sigma^2 = 4$. Se as √°rvores forem i.i.d e usarmos $B=100$ √°rvores no bagging, a vari√¢ncia da m√©dia ser√° $\frac{4}{100} = 0.04$. No entanto, se as √°rvores tiverem uma correla√ß√£o de $\rho = 0.5$, a vari√¢ncia da m√©dia ser√° $0.5 \cdot 4 + \frac{1 - 0.5}{100} \cdot 4 = 2 + 0.02 = 2.02$. Isso ilustra como a correla√ß√£o aumenta a vari√¢ncia, demonstrando a necessidade de t√©cnicas para descorrelacionar as √°rvores, como no Random Forest.
**Lemma 1:** *A expectativa da m√©dia de √°rvores de decis√£o geradas por bagging √© igual √† expectativa de uma √∫nica √°rvore*. Isso ocorre porque as √°rvores s√£o i.d., garantindo que a redu√ß√£o da vari√¢ncia seja o √∫nico mecanismo de melhoria do modelo, conforme apresentado em [^15.2].

**Conceito 2: Random Forests e a Descorrela√ß√£o de √Årvores**
Os **Random Forests** levam a ideia de bagging um passo adiante, introduzindo aleatoriedade na constru√ß√£o das √°rvores [^15.2]. Al√©m de usar amostras de bootstrap, o Random Forest tamb√©m seleciona um subconjunto aleat√≥rio de vari√°veis como candidatos para cada divis√£o em cada √°rvore [^15.2]. Essa aleatoriedade tem como objetivo descorrelacionar as √°rvores, o que, por sua vez, reduz a vari√¢ncia da m√©dia do conjunto [^15.2]. O n√∫mero de vari√°veis selecionadas aleatoriamente, $m$, √© um hiperpar√¢metro crucial para ajustar o modelo [^15.2]. Tipicamente, $m$ √© $\sqrt{p}$ para classifica√ß√£o e $\frac{p}{3}$ para regress√£o, onde $p$ √© o n√∫mero total de vari√°veis [^15.3].
> üí° **Exemplo Num√©rico:** Se tivermos um dataset com $p=16$ vari√°veis, um valor comum para $m$ em um problema de classifica√ß√£o seria $\sqrt{16} = 4$. Isso significa que para cada divis√£o em cada √°rvore, apenas 4 vari√°veis aleat√≥rias ser√£o consideradas como poss√≠veis candidatas. Para um problema de regress√£o com $p=15$, $m$ seria tipicamente $\frac{15}{3} = 5$.
**Corol√°rio 1:** Reduzir o n√∫mero de vari√°veis $m$ resulta em √°rvores mais descorrelacionadas, o que, por sua vez, diminui a vari√¢ncia da m√©dia do conjunto, de acordo com [^15.2] e a f√≥rmula (15.1).

```mermaid
graph LR
    subgraph "Random Forest Decorrelation"
        A["Bootstrap Samples"] --> B["Random Subsets of Features (m)"]
        B --> C["Less Correlated Trees"]
        C --> D["Reduced Variance"]
    end
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#fcc,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#fcc,stroke:#333,stroke-width:2px
```

**Conceito 3: OOB Samples e sua Utilidade**
Uma das caracter√≠sticas not√°veis dos Random Forests √© a utiliza√ß√£o de amostras **Out-of-Bag (OOB)** [^15.3.1]. Durante o processo de bootstrap, algumas observa√ß√µes n√£o s√£o inclu√≠das em cada amostra e essas s√£o as amostras OOB [^15.3.1]. Essas amostras podem ser usadas para avaliar o desempenho do modelo sem a necessidade de valida√ß√£o cruzada [^15.3.1]. Cada observa√ß√£o tem um preditor de floresta aleat√≥ria constru√≠do a partir das √°rvores em que essa observa√ß√£o n√£o foi usada [^15.3.1]. Essa abordagem fornece uma estimativa de erro quase id√™ntica √† valida√ß√£o cruzada N-fold, mas sem custo computacional adicional, como mencionado em [^15.3.1].
> ‚ö†Ô∏è **Nota Importante:** O erro OOB √© uma estimativa de desempenho do modelo muito confi√°vel, compar√°vel √† valida√ß√£o cruzada, e √© calculada internamente no processo de treinamento.
> üí° **Exemplo Num√©rico:** Imagine que temos 100 amostras de dados. Ao criar amostras de bootstrap para 100 √°rvores, cada amostra ter√°, aproximadamente, 63.2% das amostras originais (devido √† amostragem com reposi√ß√£o). As amostras restantes, cerca de 36.8%, s√£o as amostras OOB para cada √°rvore. Para cada observa√ß√£o, podemos calcular sua predi√ß√£o usando apenas as √°rvores para as quais ela estava fora da amostra.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o
```mermaid
graph LR
    subgraph "Random Forest for Classification"
        A["Bootstrap Sampling"] --> B["Random Feature Selection"];
        B --> C["Decision Tree Construction"];
        C --> D["Aggregation (Majority Vote)"];
        D --> E["OOB Sample Validation"];
        E --> F["Variable Importance Calculation"];
    end
     style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#fcc,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#fcc,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
        style F fill:#fcc,stroke:#333,stroke-width:2px
```

A regress√£o linear tradicionalmente n√£o √© usada diretamente para classifica√ß√£o, mas √© importante entender como o Random Forest contorna essa limita√ß√£o, utilizando os princ√≠pios de bagging e aleatoriza√ß√£o na constru√ß√£o de √°rvores de decis√£o. As √°rvores de decis√£o, por sua vez, s√£o capazes de capturar intera√ß√µes complexas entre as vari√°veis e, quando combinadas em um Random Forest, tornam-se um preditor eficiente para problemas de classifica√ß√£o [^15.2].

Ao contr√°rio dos m√©todos lineares que estimam coeficientes diretamente a partir de todos os dados de treinamento, o Random Forest usa amostras bootstrap e subconjuntos de vari√°veis para construir cada √°rvore, o que √© um desvio fundamental da abordagem linear cl√°ssica [^15.2]. Isso permite que o modelo capture rela√ß√µes n√£o-lineares e reduza o risco de *overfitting* [^15.2]. Em cada n√≥ de uma √°rvore, a escolha da vari√°vel de divis√£o ideal √© realizada atrav√©s de crit√©rios que medem a redu√ß√£o da impureza, como o √≠ndice de Gini [^15.3.2]. No entanto, ao utilizar apenas um subconjunto de vari√°veis candidatas, o Random Forest garante que nenhuma vari√°vel domine a estrutura da √°rvore, promovendo maior diversidade no conjunto [^15.2].
**Lemma 2:** *A vari√¢ncia da m√©dia de √°rvores geradas por bagging √© sempre menor ou igual √† vari√¢ncia de uma √∫nica √°rvore*. Essa desigualdade √© estrita se houver correla√ß√£o entre as √°rvores [^15.2], demonstrando a import√¢ncia de descorrelacionar as √°rvores com Random Forests.

**Corol√°rio 2:** *A escolha aleat√≥ria de vari√°veis em Random Forests reduz a correla√ß√£o entre as √°rvores*. Isso leva a uma redu√ß√£o maior da vari√¢ncia quando comparado com bagging tradicional, conforme em [^15.2] e na equa√ß√£o (15.1).

> ‚ùó **Ponto de Aten√ß√£o:** Embora a regress√£o linear n√£o seja diretamente usada em Random Forests para classifica√ß√£o, entender seus princ√≠pios ajuda a contrastar os mecanismos de cada m√©todo.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o
```mermaid
graph LR
 subgraph "Variable Importance Methods"
    A["Gini Importance (Reduction in Impurity)"] --> B["Node Splitting"];
    C["OOB Permutation Importance"] --> D["Permuting OOB Values"]
        D --> E["Measuring Performance Impact"];
    end
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#fcc,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#fcc,stroke:#333,stroke-width:2px
        style E fill:#fcc,stroke:#333,stroke-width:2px
```

A sele√ß√£o de vari√°veis em Random Forests n√£o √© feita atrav√©s de m√©todos de regulariza√ß√£o, como em regress√£o log√≠stica ou SVM, mas sim atrav√©s da sele√ß√£o aleat√≥ria de vari√°veis candidatas em cada divis√£o e atrav√©s do uso de m√©tricas de import√¢ncia de vari√°vel, incluindo a **OOB permutation importance** [^15.2]. Esta √∫ltima mede a import√¢ncia de uma vari√°vel avaliando o quanto a performance do modelo diminui quando os valores dessa vari√°vel s√£o permutados nas amostras OOB [^15.3.2]. Esta abordagem √© diferente da penaliza√ß√£o L1 ou L2 usada em modelos lineares, pois ela avalia diretamente o impacto preditivo de cada vari√°vel e n√£o apenas o valor dos seus coeficientes [^15.2]. A sele√ß√£o aleat√≥ria de vari√°veis em cada n√≥ da √°rvore e o uso de amostras OOB, atuam como m√©todos de regulariza√ß√£o impl√≠citos dentro do Random Forest, controlando *overfitting* e evitando que uma √∫nica vari√°vel domine a √°rvore [^15.2].

O c√°lculo da OOB permutation importance envolve as seguintes etapas: para cada √°rvore, a precis√£o preditiva √© avaliada nas amostras OOB. Em seguida, para cada vari√°vel, os valores dessa vari√°vel nas amostras OOB s√£o permutados aleatoriamente e a precis√£o preditiva √© reavaliada. A diminui√ß√£o na precis√£o (geralmente medida como erro ou acur√°cia), quando a vari√°vel √© permutada, √© usada como uma medida de import√¢ncia da vari√°vel [^15.3.2].
> üí° **Exemplo Num√©rico:** Considere um Random Forest com 100 √°rvores ajustado a um conjunto de dados. Para uma vari√°vel espec√≠fica, digamos 'idade', e para uma √°rvore espec√≠fica,  calculamos a precis√£o da √°rvore em suas amostras OOB. Vamos supor que a precis√£o seja 0.85. Em seguida, permutamos aleatoriamente os valores de 'idade' nas amostras OOB e recalculamos a precis√£o, que agora √© 0.78. A diminui√ß√£o da precis√£o, 0.85 - 0.78 = 0.07, √© a medida de import√¢ncia para esta √°rvore e esta vari√°vel. Fazemos isso para todas as √°rvores e vari√°veis, calculando a m√©dia das diminui√ß√µes na acur√°cia para obter a import√¢ncia final de cada vari√°vel.

**Lemma 3:** *A permuta√ß√£o aleat√≥ria de valores de uma vari√°vel em amostras OOB invalida a contribui√ß√£o dessa vari√°vel para a predi√ß√£o*. Portanto, a diminui√ß√£o da acur√°cia na predi√ß√£o reflete a import√¢ncia dessa vari√°vel no modelo, conforme apresentado em [^15.3.2].
**Prova do Lemma 3:** Dado que o Random Forest √© constru√≠do para aprender as rela√ß√µes entre vari√°veis preditoras e a vari√°vel resposta, ao embaralharmos uma vari√°vel em particular, n√≥s quebramos a rela√ß√£o aprendida pelo modelo para essa vari√°vel espec√≠fica. Se esta vari√°vel for importante para a predi√ß√£o, a performance do modelo ser√° reduzida, j√° que agora os padr√µes aprendidos para essa vari√°vel s√£o inv√°lidos. $\blacksquare$
**Corol√°rio 3:** Vari√°veis mais importantes resultam em uma maior diminui√ß√£o da acur√°cia quando seus valores s√£o permutados aleatoriamente nas amostras OOB. Isso permite comparar a import√¢ncia de diferentes vari√°veis no mesmo modelo, conforme discutido em [^15.3.2].
> üí° **Exemplo Num√©rico:** Suponha que, ap√≥s o processo de permuta√ß√£o OOB, obtivemos as seguintes redu√ß√µes m√©dias de acur√°cia para tr√™s vari√°veis: 'renda' = 0.15, 'idade' = 0.05 e 'educa√ß√£o' = 0.02. Isso sugere que 'renda' √© a vari√°vel mais importante para o modelo, seguida por 'idade', e 'educa√ß√£o' tem a menor import√¢ncia entre as tr√™s.

### Separating Hyperplanes e Perceptrons
A abordagem dos **Separating Hyperplanes** e Perceptrons √© fundamentalmente diferente do Random Forest. M√©todos como SVM (Support Vector Machines) buscam encontrar um hiperplano que maximize a margem de separa√ß√£o entre diferentes classes [^15.2]. O Random Forest, por outro lado, n√£o se baseia na busca por hiperplanos, mas sim na constru√ß√£o de m√∫ltiplas √°rvores de decis√£o, que s√£o agregadas para formar uma previs√£o final. Enquanto os m√©todos de hiperplano se focam na geometria dos dados, Random Forest foca na constru√ß√£o de modelos que capturam a complexidade das rela√ß√µes entre vari√°veis por meio de divis√µes sucessivas em √°rvores [^15.1].
Os perceptrons, uma forma simples de redes neurais, buscam aprender um separador linear atrav√©s de um processo de atualiza√ß√£o iterativo dos pesos. Embora sejam capazes de separar dados linearmente separ√°veis, perceptrons n√£o conseguem lidar com problemas complexos. Por outro lado, Random Forests, com sua capacidade de modelar rela√ß√µes n√£o-lineares, s√£o geralmente mais adequados para problemas de classifica√ß√£o e regress√£o do mundo real [^15.2].

> ‚úîÔ∏è **Destaque**: Random Forests n√£o buscam hiperplanos separadores, mas constroem v√°rias √°rvores de decis√£o agregadas. M√©todos de hiperplanos buscam a melhor separa√ß√£o geom√©trica entre as classes.

### Pergunta Te√≥rica Avan√ßada:
#### Qual a rela√ß√£o entre a OOB permutation importance e a import√¢ncia de vari√°veis calculada usando o √≠ndice de Gini em Random Forests?
**Resposta:**
A import√¢ncia das vari√°veis calculada pelo √≠ndice de Gini √© baseada em avaliar a redu√ß√£o da impureza de um n√≥ em cada √°rvore quando uma determinada vari√°vel √© usada para a divis√£o [^15.3.2]. A import√¢ncia √© ent√£o acumulada sobre todas as √°rvores [^15.3.2]. J√° a OOB permutation importance avalia a import√¢ncia das vari√°veis diretamente atrav√©s da sua influ√™ncia na performance do modelo [^15.3.2]. Ao embaralhar os valores de uma vari√°vel nas amostras OOB e recalcular o erro, a import√¢ncia √© diretamente medida pelo quanto essa vari√°vel contribui para o poder preditivo do modelo. A import√¢ncia do Gini √© intr√≠nseca √† constru√ß√£o da √°rvore, enquanto a OOB permutation importance avalia o efeito da vari√°vel no resultado final do modelo [^15.3.2].
**Lemma 4:** A import√¢ncia do Gini mede o qu√£o bem uma vari√°vel separa as classes dentro de uma √°rvore, enquanto a OOB permutation importance mede diretamente o impacto de uma vari√°vel na acur√°cia do modelo.
**Corol√°rio 4:** Vari√°veis com alta import√¢ncia Gini podem n√£o ter necessariamente alta OOB permutation importance, e vice-versa, pois as duas medidas est√£o avaliando aspectos diferentes da contribui√ß√£o de uma vari√°vel no modelo.
> üí° **Exemplo Num√©rico:** Uma vari√°vel 'X' pode ter uma alta import√¢ncia Gini porque ela causa uma redu√ß√£o significativa na impureza dos n√≥s onde ela √© utilizada para divis√£o em v√°rias √°rvores. No entanto, sua OOB permutation importance pode ser baixa se, globalmente, embaralhar os valores dessa vari√°vel n√£o impacta muito a acur√°cia do modelo em amostras OOB. Isso pode acontecer se essa vari√°vel estiver correlacionada com outras vari√°veis e a informa√ß√£o que ela carrega √© redundante. Por outro lado, outra vari√°vel 'Y' pode ter baixa import√¢ncia Gini, mas uma alta OOB permutation importance, indicando que, apesar de n√£o ser frequentemente utilizada em divis√µes, ela √© crucial para a acur√°cia do modelo quando a informa√ß√£o que ela fornece √© perturbada por embaralhamento.
```mermaid
graph LR
    subgraph "Gini vs OOB Importance"
        A["Gini Importance"] --> B["Node Impurity Reduction"]
        C["OOB Permutation Importance"] --> D["Performance Impact on OOB"];
    end
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#fcc,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#fcc,stroke:#333,stroke-width:2px
```

### Conclus√£o
O Random Forest √© uma t√©cnica poderosa que utiliza princ√≠pios de bagging e aleatoriza√ß√£o para construir um modelo robusto e com alta capacidade preditiva, como indicado em [^15.1] e [^15.2]. O uso de amostras OOB e a t√©cnica de OOB permutation importance s√£o recursos importantes que permitem avaliar o desempenho e a import√¢ncia das vari√°veis sem a necessidade de valida√ß√£o cruzada adicional [^15.3.1] e [^15.3.2]. Este cap√≠tulo aprofundou os conceitos fundamentais do Random Forest, destacando as diferen√ßas entre essa abordagem e m√©todos lineares ou baseados em hiperplanos. A an√°lise da OOB permutation importance e sua compara√ß√£o com o √≠ndice de Gini fornece uma vis√£o mais completa do comportamento do Random Forest, e ajuda a entender as nuances de seus mecanismos de funcionamento, complementando a an√°lise fornecida no contexto e apresentando uma base s√≥lida para futuros estudos e aplica√ß√µes.
<!-- END DOCUMENT -->
### Footnotes
[^15.1]: *‚ÄúBagging or bootstrap aggregation (section 8.7) is a technique for reducing the variance of an estimated prediction function. Bagging seems to work especially well for high-variance, low-bias procedures, such as trees. For regression, we simply fit the same regression tree many times to bootstrap-sampled versions of the training data, and average the result. For classifi- cation, a committee of trees each cast a vote for the predicted class. Random forests (Breiman, 2001) is a substantial modification of bagging that builds a large collection of de-correlated trees, and then averages them.‚Äù* (Trecho de *Random Forests*)
[^15.2]: *‚ÄúThe essential idea in bagging (Section 8.7) is to average many noisy but approximately unbiased models, and hence reduce the variance. Trees are ideal candidates for bagging, since they can capture complex interaction structures in the data, and if grown sufficiently deep, have relatively low bias. Since trees are notoriously noisy, they benefit greatly from the averaging. Moreover, since each tree generated in bagging is identically distributed (i.d.), the expectation of an average of B such trees is the same as the ex- pectation of any one of them. This means the bias of bagged trees is the same as that of the individual trees, and the only hope of improvement is through variance reduction.‚Äù* (Trecho de *Random Forests*)
[^15.3]: *‚ÄúTypically values for m are ‚àöp or even as low as 1. After B such trees {T(x; Œò‚ÇÅ)}f are grown, the random forest (regression) predictor is f_rf(x) = (1/B) Œ£ T(x; Œò_b). As in Section 10.9 (page 356), Œò_b characterizes the bth random forest tree in terms of split variables, cutpoints at each node, and terminal-node values. Intuitively, reducing m will reduce the correlation between any pair of trees in the ensemble, and hence by (15.1) reduce the variance of the average.‚Äù* (Trecho de *Random Forests*)
[^15.3.1]: *‚ÄúFor each observation zi = (xi, Yi), construct its random forest predictor by averaging only those trees corresponding to bootstrap samples in which zi did not appear. An oob error estimate is almost identical to that obtained by N-fold cross- validation; see Exercise 15.2. Hence unlike many other nonlinear estimators, random forests can be fit in one sequence, with cross-validation being per- formed along the way. Once the OOB error stabilizes, the training can be terminated.‚Äù* (Trecho de *Random Forests*)
[^15.3.2]: *‚ÄúAt each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splitting variable, and is accumulated over all the trees in the forest separately for each variable. Random forests also use the OOB samples to construct a different variable- importance measure, apparently to measure the prediction strength of each variable. When the bth tree is grown, the OOB samples are passed down the tree, and the prediction accuracy is recorded. Then the values for the jth variable are randomly permuted in the OOB samples, and the accuracy is again computed. The decrease in accuracy as a result of this permuting is averaged over all trees, and is used as a measure of the importance of variable j in the random forest.‚Äù* (Trecho de *Random Forests*)
