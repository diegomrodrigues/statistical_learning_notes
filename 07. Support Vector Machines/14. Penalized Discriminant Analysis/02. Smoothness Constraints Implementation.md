Okay, let's enhance this text with Mermaid diagrams to visualize the concepts.

## T√≠tulo: An√°lise Discriminante Penalizada: Imposi√ß√£o de Suavidade com Dist√¢ncia de Mahalanobis Regularizada

```mermaid
graph LR
    subgraph "Penalized Discriminant Analysis (PDA)"
        direction TB
        A["Input Data"] --> B{"Feature Transformation h(x)"}
        B --> C{"Compute Regularized Mahalanobis Distance"}
        C --> D["Classification Decision"]
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Na **An√°lise Discriminante Penalizada (PDA)**, como explorado em cap√≠tulos anteriores, a regulariza√ß√£o desempenha um papel fundamental no controle da complexidade do modelo e na estabilidade das solu√ß√µes, especialmente em problemas de classifica√ß√£o com dados de alta dimensionalidade. Uma abordagem comum para impor regulariza√ß√£o em PDA √© atrav√©s da modifica√ß√£o da **dist√¢ncia de Mahalanobis**, utilizando um termo de penaliza√ß√£o que incentiva a suavidade dos coeficientes do modelo e imp√µe restri√ß√µes sobre a forma da fronteira de decis√£o.

Neste cap√≠tulo, exploraremos em detalhes como a **dist√¢ncia de Mahalanobis regularizada** √© utilizada em PDA para impor restri√ß√µes de suavidade nos coeficientes, analisando como a escolha da matriz de penaliza√ß√£o influencia o resultado da otimiza√ß√£o e a forma da fronteira de decis√£o. Discutiremos como a regulariza√ß√£o leva a modelos mais robustos e com melhor capacidade de generaliza√ß√£o, e como ela pode ser utilizada para lidar com dados correlacionados ou com *outliers*.

A compreens√£o da utiliza√ß√£o da dist√¢ncia de Mahalanobis regularizada e da sua aplica√ß√£o na PDA fornece uma base te√≥rica e pr√°tica para a constru√ß√£o de modelos de classifica√ß√£o eficientes e robustos, especialmente em cen√°rios com dados complexos de alta dimens√£o.

### A Dist√¢ncia de Mahalanobis Regularizada

**Conceito 1: A Dist√¢ncia de Mahalanobis e a LDA**

A **dist√¢ncia de Mahalanobis** √© uma m√©trica de dist√¢ncia que leva em conta a covari√¢ncia das vari√°veis. A dist√¢ncia de Mahalanobis entre uma amostra $x$ e a m√©dia da classe $k$, $\mu_k$, √© dada por:

$$ d(x, \mu_k) = \sqrt{(x - \mu_k)^T \Sigma^{-1} (x - \mu_k)} $$

onde $\Sigma$ √© a matriz de covari√¢ncia (comum para todas as classes, como assumido na LDA).

```mermaid
graph LR
    subgraph "Mahalanobis Distance"
    direction LR
        A["Sample x"]
        B["Class Mean Œº_k"]
        C["Covariance Matrix Œ£"]
        D["Inverse Covariance Œ£‚Åª¬π"]
        E["(x - Œº_k)"]
        F["(x - Œº_k)·µÄ"]
        G["d(x, Œº_k) = sqrt((x - Œº_k)·µÄ Œ£‚Åª¬π (x - Œº_k))"]
        A --> E
        B --> E
        E --> F
        C --> D
        F & D & E --> G
    end
    style G fill:#aaf,stroke:#333,stroke-width:2px
```

Na **An√°lise Discriminante Linear (LDA)**, a classifica√ß√£o √© feita atribuindo a cada amostra a classe cujo centroide tem a menor dist√¢ncia de Mahalanobis. A utiliza√ß√£o da dist√¢ncia de Mahalanobis na LDA garante que a dist√¢ncia entre classes seja calculada de forma a ter em conta a variabilidade dos dados.

> üí° **Exemplo Num√©rico:**
> Suponha que temos duas classes, com m√©dias $\mu_1 = [1, 1]$ e $\mu_2 = [3, 3]$, e uma matriz de covari√¢ncia $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$. Vamos calcular a dist√¢ncia de Mahalanobis de um ponto $x = [2, 2]$ a cada uma das m√©dias.
>
> Primeiro, calculamos a inversa de $\Sigma$:
>  $\Sigma^{-1} = \frac{1}{1 - 0.5^2} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \frac{4}{3}\begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix}$
>
> Agora, calculamos a dist√¢ncia para $\mu_1$:
> $d(x, \mu_1) = \sqrt{([2-1, 2-1]) \begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix} [2-1, 2-1]^T} = \sqrt{[1, 1] \begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix} [1, 1]^T} = \sqrt{[1, 1] [2/3, 2/3]^T} = \sqrt{4/3} \approx 1.15$
>
> E para $\mu_2$:
> $d(x, \mu_2) = \sqrt{([2-3, 2-3]) \begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix} [2-3, 2-3]^T} = \sqrt{[-1, -1] \begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix} [-1, -1]^T} = \sqrt{[-1, -1] [-2/3, -2/3]^T} = \sqrt{4/3} \approx 1.15$
>
> Note que o ponto [2,2] est√° a mesma dist√¢ncia de Mahalanobis das duas m√©dias. A dist√¢ncia de Mahalanobis considera a covari√¢ncia dos dados, resultando em uma dist√¢ncia diferente da dist√¢ncia euclidiana simples.

**Lemma 1:** A dist√¢ncia de Mahalanobis leva em conta a variabilidade dos dados e √© utilizada para calcular a dist√¢ncia entre as amostras e as m√©dias das classes na LDA, o que influencia diretamente o processo de classifica√ß√£o e a separabilidade das classes.

A demonstra√ß√£o desse lemma se baseia na an√°lise da formula√ß√£o da dist√¢ncia de Mahalanobis e como ela considera a matriz de covari√¢ncia para o c√°lculo da dist√¢ncia entre os pontos.

**Conceito 2: Regulariza√ß√£o da Dist√¢ncia de Mahalanobis em PDA**

Na **An√°lise Discriminante Penalizada (PDA)**, a dist√¢ncia de Mahalanobis √© modificada com a introdu√ß√£o de um termo de regulariza√ß√£o, que tem como objetivo impor restri√ß√µes de suavidade aos coeficientes do modelo, e, por consequ√™ncia, nas fronteiras de decis√£o. A **dist√¢ncia de Mahalanobis regularizada** √© definida como:

$$ D(x, \mu) = (h(x) - h(\mu))^T (\Sigma_W + \lambda \Omega)^{-1} (h(x) - h(\mu)) $$

onde:

*   $h(x)$ √© a transforma√ß√£o dos dados, por exemplo, aplicando fun√ß√µes de base;
*   $\Sigma_W$ √© a matriz de covari√¢ncia dentro das classes, calculada sobre as *features* transformadas $h(x)$;
*   $\Omega$ √© a matriz de penaliza√ß√£o, que define como as diferen√ßas nos coeficientes s√£o penalizadas;
*   $\lambda$ √© o par√¢metro de regulariza√ß√£o, que controla a intensidade da penaliza√ß√£o.

```mermaid
graph LR
    subgraph "Regularized Mahalanobis Distance"
    direction LR
        A["Transformed Sample h(x)"]
        B["Transformed Class Mean h(Œº)"]
        C["Within-class Covariance Matrix Œ£_W"]
        D["Penalty Matrix Œ©"]
        E["Regularization Parameter Œª"]
        F["Œ£_W + ŒªŒ©"]
        G["(Œ£_W + ŒªŒ©)‚Åª¬π"]
        H["(h(x) - h(Œº))"]
        I["(h(x) - h(Œº))·µÄ"]
        J["D(x, Œº) = (h(x) - h(Œº))·µÄ (Œ£_W + ŒªŒ©)‚Åª¬π (h(x) - h(Œº))"]
        A --> H
        B --> H
        C --> F
        D --> F
        E --> F
        F --> G
        H --> I
        I & G & H --> J
    end
      style J fill:#aaf,stroke:#333,stroke-width:2px
```

A matriz de penaliza√ß√£o $\Omega$ √© utilizada para induzir suavidade nos coeficientes do modelo, evitando varia√ß√µes abruptas e instabilidades. A escolha da matriz $\Omega$ e do par√¢metro $\lambda$ √© crucial para o desempenho do modelo, e depende da natureza dos dados.

> üí° **Exemplo Num√©rico:**
>
> Suponha que ap√≥s a transforma√ß√£o $h(x)$, temos $\Sigma_W = \begin{bmatrix} 2 & 0.3 \\ 0.3 & 1 \end{bmatrix}$. Vamos usar $\Omega = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ (matriz identidade) e $\lambda = 0.5$.
>
> Ent√£o, a matriz regularizada de covari√¢ncia √©:
>
> $\Sigma_W + \lambda \Omega = \begin{bmatrix} 2 & 0.3 \\ 0.3 & 1 \end{bmatrix} + 0.5 \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 2.5 & 0.3 \\ 0.3 & 1.5 \end{bmatrix}$
>
> Calculando a inversa:
>
> $(\Sigma_W + \lambda \Omega)^{-1} = \frac{1}{(2.5)(1.5) - (0.3)^2}\begin{bmatrix} 1.5 & -0.3 \\ -0.3 & 2.5 \end{bmatrix} = \frac{1}{3.66}\begin{bmatrix} 1.5 & -0.3 \\ -0.3 & 2.5 \end{bmatrix} \approx \begin{bmatrix} 0.41 & -0.08 \\ -0.08 & 0.68 \end{bmatrix}$
>
> Agora, se $h(x) = [2, 1]$ e $h(\mu) = [1, 0]$, podemos calcular a dist√¢ncia de Mahalanobis regularizada:
>
> $D(x, \mu) = ([2-1, 1-0]) \begin{bmatrix} 0.41 & -0.08 \\ -0.08 & 0.68 \end{bmatrix} [2-1, 1-0]^T = [1, 1] \begin{bmatrix} 0.41 & -0.08 \\ -0.08 & 0.68 \end{bmatrix} [1, 1]^T = [1, 1] [0.33, 0.60]^T = 0.93$
>
> A regulariza√ß√£o altera a matriz de covari√¢ncia, o que afeta a dist√¢ncia calculada.

**Corol√°rio 1:** A dist√¢ncia de Mahalanobis regularizada incorpora um termo de penaliza√ß√£o que controla a complexidade do modelo, induzindo suavidade nos coeficientes e evitando o *overfitting* em dados de alta dimensionalidade.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise da formula√ß√£o da dist√¢ncia de Mahalanobis regularizada e como o termo de penaliza√ß√£o, que inclui a matriz $\Omega$ e o par√¢metro $\lambda$, afeta a solu√ß√£o do problema.

### A Matriz de Penaliza√ß√£o $\Omega$ e a Suavidade da Fronteira de Decis√£o

```mermaid
graph LR
    subgraph "Penalty Matrix Œ© and Decision Boundary"
        direction TB
        A["Different Penalty Matrices Œ©"]
        B["Identity Matrix Œ© = I"]
        C["Finite Difference Matrix"]
        D["Graph-based Matrix"]
        E["Effect on Decision Boundary Smoothness"]
        A --> B
        A --> C
        A --> D
        B --> E
        C --> E
        D --> E
    end
    style E fill:#ffc,stroke:#333,stroke-width:2px
```

A matriz de penaliza√ß√£o $\Omega$ desempenha um papel crucial na defini√ß√£o da suavidade da fronteira de decis√£o em PDA. A escolha da matriz $\Omega$ afeta como os coeficientes do modelo s√£o penalizados e, consequentemente, como a fronteira de decis√£o se adapta aos dados.

Algumas escolhas comuns para a matriz $\Omega$ incluem:

1.  **Matriz Identidade:** Utilizando a matriz identidade, a penaliza√ß√£o corresponde √† norma L2 dos coeficientes, o que reduz sua magnitude e leva a fronteiras mais est√°veis, com os coeficientes menores:
    $$ \Omega = I$$
2.  **Matriz de Diferen√ßas Finitas:** A matriz de diferen√ßas finitas penaliza as diferen√ßas entre coeficientes de *features* vizinhas, induzindo suavidade na fronteira de decis√£o:
   $$ \Omega_{ij} = \begin{cases}
     1  & \text{se } i = j \\
    -0.5  & \text{se } |i-j| = 1\\
      0  & \text{caso contr√°rio}
  \end{cases} $$

    Essa abordagem √© especialmente √∫til em problemas com dados espaciais ou temporais, onde *features* adjacentes est√£o correlacionadas e √© desej√°vel que os seus coeficientes n√£o apresentem grandes varia√ß√µes.

3.   **Matrizes Baseadas em Grafos:** Nesses casos a matriz $\Omega$ √© constru√≠da com base em um grafo, que representa as rela√ß√µes entre as vari√°veis originais, onde os n√≥s representam as vari√°veis e as arestas representam as rela√ß√µes entre elas. Essa abordagem leva a uma penaliza√ß√£o que leva em considera√ß√£o a estrutura do grafo, com penalidades maiores para componentes mais distantes.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um caso com 4 features.
>
> 1. **Matriz Identidade:** $\Omega = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}$. Esta matriz penaliza a magnitude de cada coeficiente individualmente.
>
> 2.  **Matriz de Diferen√ßas Finitas:**
>
>  $\Omega = \begin{bmatrix} 1 & -0.5 & 0 & 0 \\ -0.5 & 1 & -0.5 & 0 \\ 0 & -0.5 & 1 & -0.5 \\ 0 & 0 & -0.5 & 1 \end{bmatrix}$
>   Esta matriz penaliza as diferen√ßas entre os coeficientes de features adjacentes. Por exemplo, ela penaliza se o coeficiente da feature 1 √© muito diferente do coeficiente da feature 2.
>
> Suponha que temos coeficientes $w = [0.8, 0.2, 0.7, 0.3]$.
> A penaliza√ß√£o usando a matriz identidade seria: $w^T \Omega w = 0.8^2 + 0.2^2 + 0.7^2 + 0.3^2 = 1.26$.
> A penaliza√ß√£o usando a matriz de diferen√ßas finitas seria:
> $w^T \Omega w = [0.8, 0.2, 0.7, 0.3] \begin{bmatrix} 1 & -0.5 & 0 & 0 \\ -0.5 & 1 & -0.5 & 0 \\ 0 & -0.5 & 1 & -0.5 \\ 0 & 0 & -0.5 & 1 \end{bmatrix} [0.8, 0.2, 0.7, 0.3]^T = 0.665$.
> A penaliza√ß√£o com diferen√ßas finitas √© menor, pois os coeficientes n√£o s√£o t√£o diferentes entre features adjacentes.

A escolha da matriz $\Omega$ depende da natureza dos dados e do tipo de suavidade desejada. A matriz identidade √© utilizada para regularizar a magnitude dos coeficientes, enquanto as matrizes de diferen√ßas finitas e baseadas em grafos s√£o utilizadas para impor suavidade nas fronteiras de decis√£o.

**Lemma 3:** A escolha da matriz de penaliza√ß√£o Œ© em PDA afeta a suavidade da fronteira de decis√£o, onde a matriz identidade penaliza os coeficientes com norma L2, a matriz de diferen√ßas finitas penaliza as mudan√ßas bruscas entre os coeficientes e a matriz baseada em grafos penaliza os coeficientes com base na estrutura do grafo.

A demonstra√ß√£o desse lemma se baseia na an√°lise da forma das diferentes matrizes de penaliza√ß√£o e como elas se relacionam com a forma da fun√ß√£o de decis√£o resultante do processo de otimiza√ß√£o.

### O Par√¢metro de Regulariza√ß√£o $\lambda$ e a Generaliza√ß√£o

```mermaid
graph LR
    subgraph "Regularization Parameter Œª and Generalization"
        direction TB
        A["Regularization Parameter Œª"]
        B["High Œª"]
        C["Low Œª"]
        D["Model Complexity"]
        E["Generalization Ability"]
        A --> B
        A --> C
        B --> D
        C --> D
        D --> E
    end
    style E fill:#ffc,stroke:#333,stroke-width:2px
```

O par√¢metro de regulariza√ß√£o $\lambda$ controla a intensidade da penaliza√ß√£o na dist√¢ncia de Mahalanobis regularizada e tem um impacto direto na **generaliza√ß√£o** do modelo. Valores altos de $\lambda$ imp√µem uma penaliza√ß√£o forte sobre os coeficientes, o que leva a modelos mais simples e com fronteiras de decis√£o mais suaves. Modelos mais simples s√£o menos propensos ao *overfitting*, mas podem apresentar um alto vi√©s, caso n√£o capturem adequadamente as rela√ß√µes entre as *features* e as classes.

Valores baixos de $\lambda$, por outro lado, diminuem a penaliza√ß√£o sobre os coeficientes, o que leva a modelos mais complexos e com fronteiras de decis√£o mais detalhadas. Modelos mais complexos podem se ajustar demais aos dados de treinamento e apresentar *overfitting*.

A escolha do valor apropriado de $\lambda$ √© feita atrav√©s de t√©cnicas de valida√ß√£o cruzada, onde o desempenho do modelo √© avaliado com diferentes valores de $\lambda$ e o valor que maximiza o desempenho em dados n√£o vistos √© selecionado.

> üí° **Exemplo Num√©rico:**
>
> Suponha que estamos treinando um modelo PDA com um conjunto de dados e temos as seguintes m√©tricas de valida√ß√£o cruzada para diferentes valores de $\lambda$:
>
> | $\lambda$ | Acur√°cia (Valida√ß√£o) |
> |----------|----------------------|
> | 0.01     | 0.82                 |
> | 0.1      | 0.88                 |
> | 1        | 0.91                 |
> | 10       | 0.85                 |
> | 100      | 0.78                 |
>
>  Observamos que a acur√°cia aumenta at√© $\lambda = 1$ e depois come√ßa a cair. Isso indica que um valor de $\lambda$ em torno de 1 oferece o melhor equil√≠brio entre ajuste e generaliza√ß√£o para este conjunto de dados. Um $\lambda$ muito baixo leva ao overfitting, enquanto um $\lambda$ muito alto leva a um modelo com alto vi√©s.

A escolha de $\lambda$ adequado √© um compromisso entre o ajuste aos dados de treinamento e a capacidade de generaliza√ß√£o, e √© crucial para obter um modelo robusto e com boa capacidade de lidar com novos dados.

**Corol√°rio 3:** O par√¢metro de regulariza√ß√£o $\lambda$ controla a intensidade da penaliza√ß√£o e impacta a complexidade da fronteira de decis√£o e a capacidade de generaliza√ß√£o da PDA, e a escolha desse par√¢metro depende do equil√≠brio entre vi√©s e vari√¢ncia que se deseja obter.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise do impacto do par√¢metro $\lambda$ na fun√ß√£o de custo da PDA e como a escolha desse par√¢metro afeta a estrutura da solu√ß√£o √≥tima do modelo, com a busca por um equil√≠brio entre ajuste e complexidade.

### Conex√£o com o Problema Dual e a Interpreta√ß√£o Geom√©trica

```mermaid
graph LR
    subgraph "Dual Problem and Geometric Interpretation"
        direction TB
        A["Original PDA Problem"]
        B["Dual Formulation via Wolfe Duality"]
        C["Solution in Terms of Inner Products"]
        D["Linear Projections in Transformed Space"]
        E["Role of Penalty Matrix Œ© in Projections"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
      style E fill:#ffc,stroke:#333,stroke-width:2px
```

A aplica√ß√£o da dualidade de Wolfe na formula√ß√£o da PDA leva a um problema dual, que permite expressar a solu√ß√£o em termos de produtos internos e de um espa√ßo de proje√ß√µes lineares sobre os dados. No entanto, a formula√ß√£o e solu√ß√£o expl√≠cita do problema dual de PDA √© mais complexa do que em SVMs.

A matriz de penaliza√ß√£o $\Omega$ desempenha um papel importante na formula√ß√£o do problema dual e define como os autovetores generalizados s√£o calculados, e como o termo de penaliza√ß√£o afeta o espa√ßo transformado. A solu√ß√£o do problema dual da PDA envolve a obten√ß√£o das proje√ß√µes que maximizam a separabilidade entre classes, ao mesmo tempo em que respeitam a penaliza√ß√£o definida pela matriz $\Omega$ e o par√¢metro $\lambda$.

Essa an√°lise permite entender a import√¢ncia da matriz de penaliza√ß√£o na constru√ß√£o da solu√ß√£o do problema e como a interpreta√ß√£o geom√©trica da regulariza√ß√£o na PDA √© importante para entender o comportamento do modelo. A interpreta√ß√£o geom√©trica das proje√ß√µes que a PDA gera, mostra como os autovalores definem a separabilidade das classes sobre o espa√ßo projetado, o que √© an√°logo ao que ocorre na LDA.

**Corol√°rio 4:** A formula√ß√£o dual da PDA permite expressar o problema em termos de produtos internos e revela a import√¢ncia da matriz de penaliza√ß√£o Œ© na defini√ß√£o das proje√ß√µes e na forma da fronteira de decis√£o.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise da formula√ß√£o dual da PDA e como a matriz de penaliza√ß√£o √© incorporada no processo de otimiza√ß√£o, definindo um espa√ßo de proje√ß√£o que √© sens√≠vel √† estrutura dos dados.

### Conclus√£o

Neste cap√≠tulo, exploramos em detalhes a utiliza√ß√£o da **dist√¢ncia de Mahalanobis regularizada** na **An√°lise Discriminante Penalizada (PDA)**, e como a matriz de penaliza√ß√£o e o par√¢metro de regulariza√ß√£o s√£o utilizados para controlar a complexidade do modelo e a forma da fronteira de decis√£o. Vimos como a PDA surge como uma extens√£o da LDA que visa lidar com os problemas de instabilidade e *overfitting* em espa√ßos de alta dimensionalidade, e como a utiliza√ß√£o de um termo de regulariza√ß√£o na dist√¢ncia de Mahalanobis leva a modelos mais robustos e com melhor capacidade de generaliza√ß√£o.

Analisamos tamb√©m como a escolha da matriz de penaliza√ß√£o $\Omega$ permite induzir diferentes tipos de suavidade na fronteira de decis√£o, e como o par√¢metro de regulariza√ß√£o $\lambda$ controla a intensidade da penaliza√ß√£o, o que leva a diferentes modelos.

A compreens√£o da dist√¢ncia de Mahalanobis regularizada e do papel da matriz de penaliza√ß√£o √© fundamental para a aplica√ß√£o bem-sucedida da PDA em problemas de classifica√ß√£o com dados complexos e de alta dimensionalidade.

### Footnotes

[^12.1]: "In this chapter we describe generalizations of linear decision boundaries for classification. Optimal separating hyperplanes are introduced in Chapter 4 for the case when two classes are linearly separable. Here we cover extensions to the nonseparable case, where the classes overlap. These techniques are then generalized to what is known as the support vector machine, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.4]: "Often LDA produces the best classification results, because of its simplicity and low variance. LDA was among the top three classifiers for 11 of the 22 datasets studied in the STATLOG project (Michie et al., 1994)3." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.5]:  "In this section we describe a method for performing LDA using linear re-gression on derived responses." *(Trecho de "Support Vector Machines and Flexible Discriminants")*

[^12.6]: "Although FDA is motivated by generalizing optimal scoring, it can also be viewed directly as a form of regularized discriminant analysis. Suppose the regression procedure used in FDA amounts to a linear regression onto a basis expansion h(X), with a quadratic penalty on the coefficients" *(Trecho de "Support Vector Machines and Flexible Discriminants")*
