Okay, here's the enhanced text with Mermaid diagrams, focusing on mathematical and statistical clarity:

## T√≠tulo: An√°lise de Desempenho de SVMs N√£o Lineares com Kernels Polinomiais e Radiais em Dados de Mistura

```mermaid
graph LR
    subgraph "SVM with Kernels"
    A["Input Data"] --> B{"Feature Transformation (Kernel)"};
    B --> C["Decision Boundary"];
    C --> D["Classification Output"];
    end
    subgraph "Kernel Types"
    E["Polynomial Kernel"] -.-> B
    F["Radial (RBF) Kernel"] -.-> B
    end
    subgraph "Data"
        G["Mixture Data"] --> A
    end
```

### Introdu√ß√£o

Em cap√≠tulos anteriores, exploramos os fundamentos te√≥ricos das **Support Vector Machines (SVMs)**, incluindo a formula√ß√£o do problema de otimiza√ß√£o, a utiliza√ß√£o de **kernels** e o conceito de **vetores de suporte**. Neste cap√≠tulo, vamos analisar o desempenho pr√°tico das SVMs n√£o lineares em um conjunto de dados espec√≠fico, conhecido como **dados de mistura**. Analisaremos o impacto da escolha do *kernel* (polinomial e radial) e de seus par√¢metros associados na complexidade da fronteira de decis√£o e na capacidade de generaliza√ß√£o dos modelos.

Os **dados de mistura** s√£o um conjunto de dados sint√©tico que apresenta caracter√≠sticas de sobreposi√ß√£o entre as classes e uma distribui√ß√£o complexa, que n√£o pode ser facilmente separada por um hiperplano linear. A an√°lise do desempenho de modelos SVM nesse conjunto de dados nos fornecer√° *insights* sobre a capacidade dos diferentes tipos de *kernels* em modelar rela√ß√µes n√£o lineares complexas e sobre a necessidade de ajustar cuidadosamente os par√¢metros dos modelos para obter um bom desempenho.

Este cap√≠tulo apresentar√° exemplos pr√°ticos, com resultados obtidos a partir da aplica√ß√£o das SVMs com *kernels* polinomial e radial aos dados de mistura, o que nos permitir√° avaliar as vantagens e desvantagens de cada tipo de *kernel*. Analisaremos tamb√©m o efeito da regulariza√ß√£o (par√¢metro $C$) sobre os resultados obtidos.

### O Conjunto de Dados de Mistura

**Conceito 1: Descri√ß√£o dos Dados de Mistura**

Os **dados de mistura** s√£o um conjunto de dados sint√©tico, amplamente utilizado para avaliar o desempenho de algoritmos de classifica√ß√£o n√£o lineares. O conjunto de dados consiste em duas classes que se sobrep√µem, formando uma estrutura complexa que n√£o pode ser separada por uma fronteira de decis√£o linear.

```mermaid
graph LR
    subgraph "Mixture Data Generation"
        A["Class 1: Gaussian Distribution (Œº1, Œ£1)"]
        B["Class 2: Gaussian Distribution (Œº2, Œ£2)"]
        A --> C["Overlapping Region"]
        B --> C
        C --> D["Complex Structure"]
    end
```

Em geral, cada classe √© gerada a partir de uma distribui√ß√£o gaussiana bivariada, com m√©dias e covari√¢ncias espec√≠ficas. Os centros das distribui√ß√µes s√£o escolhidos de forma a gerar uma √°rea de sobreposi√ß√£o entre as classes. A distribui√ß√£o dos dados de mistura cria um desafio para modelos lineares, que t√™m dificuldade de construir uma fronteira de decis√£o que separe as duas classes de forma adequada.

Os dados de mistura s√£o, por isso, um conjunto de dados apropriado para testar modelos n√£o lineares como SVMs com *kernels* polinomiais e radiais, como mencionado em [^12.2]. A utiliza√ß√£o dos dados de mistura permite verificar como a escolha do *kernel* e seus par√¢metros influenciam na capacidade do modelo de se adaptar a dados complexos e com sobreposi√ß√£o de classes.

> üí° **Exemplo Num√©rico:**
> Vamos criar um exemplo simplificado de dados de mistura em 2D. Imagine que temos duas classes, cada uma com 100 pontos. A Classe 1 tem uma m√©dia de (1, 1) e a Classe 2 tem uma m√©dia de (2, 2), ambas com desvio padr√£o de 0.7 em ambas as dimens√µes.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.svm import SVC
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import accuracy_score
>
> # Gerar dados de mistura
> np.random.seed(42)
> class1_mean = [1, 1]
> class2_mean = [2, 2]
> class_std = 0.7
> class1_data = np.random.normal(class1_mean, class_std, size=(100, 2))
> class2_data = np.random.normal(class2_mean, class_std, size=(100, 2))
> X = np.concatenate((class1_data, class2_data))
> y = np.concatenate((np.zeros(100), np.ones(100)))
>
> # Dividir em treino e teste
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', edgecolors='k')
> plt.title('Dados de Mistura (Treino)')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
> plt.show()
> ```
> Este c√≥digo gera e visualiza os dados de treino. A sobreposi√ß√£o entre as classes √© vis√≠vel.

**Lemma 1:** O conjunto de dados de mistura apresenta uma estrutura complexa, com sobreposi√ß√£o entre as classes, que exige modelos n√£o lineares para uma separa√ß√£o adequada.

A demonstra√ß√£o desse lemma se baseia na descri√ß√£o das caracter√≠sticas dos dados de mistura, que s√£o gerados de forma a criar uma regi√£o de sobreposi√ß√£o entre as classes, dificultando a separa√ß√£o linear dos dados.

### An√°lise de Desempenho com Kernel Polinomial

```mermaid
graph LR
    subgraph "Polynomial Kernel"
        direction TB
        A["Input Data (x)"]
        B["Degree 'd'"]
        C["Polynomial Kernel Function: K(x, x') = (x^T x' + c)^d"]
        A --> C
        B --> C
        D["Transformed Feature Space"]
        C --> D
    end
```

O **kernel polinomial** permite que a SVM construa fronteiras de decis√£o n√£o lineares, mapeando os dados para um espa√ßo de *features* de maior dimens√£o, que inclui todos os termos como produtos e pot√™ncias das *features* originais, at√© um certo grau $d$. A fun√ß√£o *kernel* polinomial √© dada por:

$$ K(x, x') = (x^T x' + c)^d $$

onde $c$ √© uma constante e $d$ √© o grau do polin√¥mio.

A aplica√ß√£o da SVM com *kernel* polinomial aos dados de mistura mostra como o par√¢metro $d$ influencia a complexidade da fronteira de decis√£o. Para valores baixos de $d$ (por exemplo, $d=2$), a fronteira de decis√£o tende a ser relativamente simples e n√£o consegue capturar todas as nuances da distribui√ß√£o dos dados, o que pode levar a classifica√ß√µes incorretas em √°reas com sobreposi√ß√£o das classes. Para valores mais altos de $d$ (por exemplo, $d=5$ ou $d=10$), a fronteira de decis√£o se torna mais complexa, ajustando-se aos detalhes dos dados de treinamento e reduzindo o n√∫mero de erros de classifica√ß√£o nesse conjunto, mas aumentando o risco de *overfitting* e de redu√ß√£o do desempenho em novos dados.

A escolha do valor apropriado de $d$ envolve um compromisso entre a complexidade do modelo e sua capacidade de generaliza√ß√£o. A utiliza√ß√£o da valida√ß√£o cruzada √© uma pr√°tica para ajustar o par√¢metro $d$ de forma a obter um modelo com boa capacidade de generalizar para novos dados.

> üí° **Exemplo Num√©rico:**
> Vamos treinar SVMs com kernel polinomial com diferentes graus (d=2 e d=5) nos dados de mistura que geramos.
> ```python
> # Treinar SVM com kernel polinomial (d=2)
> svm_poly_2 = SVC(kernel='poly', degree=2, C=1)
> svm_poly_2.fit(X_train, y_train)
> y_pred_poly_2 = svm_poly_2.predict(X_test)
> accuracy_poly_2 = accuracy_score(y_test, y_pred_poly_2)
>
> # Treinar SVM com kernel polinomial (d=5)
> svm_poly_5 = SVC(kernel='poly', degree=5, C=1)
> svm_poly_5.fit(X_train, y_train)
> y_pred_poly_5 = svm_poly_5.predict(X_test)
> accuracy_poly_5 = accuracy_score(y_test, y_pred_poly_5)
>
> print(f"Acur√°cia (Kernel Polinomial, d=2): {accuracy_poly_2:.4f}")
> print(f"Acur√°cia (Kernel Polinomial, d=5): {accuracy_poly_5:.4f}")
>
> # Visualizar as fronteiras de decis√£o (simplificado)
> # Note: Visualiza√ß√£o completa da fronteira de decis√£o seria mais complexa
>
> plt.figure(figsize=(10, 5))
> plt.subplot(1, 2, 1)
> plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_poly_2, cmap='viridis', edgecolors='k')
> plt.title('Kernel Polinomial (d=2)')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
>
> plt.subplot(1, 2, 2)
> plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_poly_5, cmap='viridis', edgecolors='k')
> plt.title('Kernel Polinomial (d=5)')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
> plt.tight_layout()
> plt.show()
> ```
> Executando o c√≥digo, podemos observar que com `d=2` a acur√°cia √© menor e a fronteira de decis√£o √© mais simples, enquanto com `d=5` a acur√°cia aumenta e a fronteira de decis√£o se torna mais complexa. Os valores de acur√°cia podem variar um pouco devido √† aleatoriedade da divis√£o de treino/teste, mas a tend√™ncia √© que um valor maior de `d` se ajuste melhor aos dados de treino, mas pode n√£o generalizar t√£o bem quanto um menor valor de `d`.

**Lemma 2:** O par√¢metro $d$ do *kernel* polinomial controla a complexidade da fronteira de decis√£o e a capacidade do modelo de capturar rela√ß√µes n√£o lineares complexas, e a escolha adequada de $d$ envolve um compromisso entre vi√©s e vari√¢ncia.

A demonstra√ß√£o desse lemma se baseia na an√°lise do *kernel* polinomial e como o par√¢metro $d$ controla a dimens√£o do espa√ßo de *features* transformado e, consequentemente, a complexidade da fronteira de decis√£o e sua capacidade de se ajustar aos dados de treinamento.

### An√°lise de Desempenho com Kernel Radial (RBF)

```mermaid
graph LR
    subgraph "RBF Kernel"
        direction TB
        A["Input Data (x)"]
        B["Gamma 'Œ≥'"]
         C["RBF Kernel Function: K(x, x') = exp(-Œ≥||x - x'||¬≤)" ]
         A --> C
         B --> C
        D["Transformed Feature Space"]
        C --> D
    end
```

O **kernel radial ou Gaussiano (RBF)** √© um *kernel* amplamente utilizado em SVMs, pois ele tem a capacidade de modelar rela√ß√µes n√£o lineares complexas e oferece um bom desempenho em uma variedade de problemas de classifica√ß√£o. O *kernel* RBF √© dado por:

$$ K(x, x') = \exp(-\gamma ||x - x'||^2) $$

onde $\gamma > 0$ √© um par√¢metro que controla a largura da fun√ß√£o gaussiana.

A aplica√ß√£o da SVM com *kernel* RBF aos dados de mistura mostra como o par√¢metro $\gamma$ influencia a complexidade da fronteira de decis√£o. Valores altos de $\gamma$ levam a modelos com alta vari√¢ncia e fronteiras de decis√£o complexas, com grande capacidade de ajustar-se aos dados de treinamento, o que pode resultar em *overfitting*. Valores baixos de $\gamma$, por outro lado, levam a modelos mais simples, com baixa vari√¢ncia e fronteiras de decis√£o mais suaves, o que pode resultar em alto vi√©s.

A escolha do valor apropriado de $\gamma$ envolve o ajuste do compromisso entre vi√©s e vari√¢ncia, com a necessidade de balancear a complexidade do modelo com sua capacidade de generalizar para dados n√£o vistos. A valida√ß√£o cruzada √© uma t√©cnica √∫til para escolher um valor de $\gamma$ que equilibre esses fatores.

> üí° **Exemplo Num√©rico:**
> Vamos treinar SVMs com kernel RBF com diferentes valores de gamma (Œ≥=0.1 e Œ≥=10) nos mesmos dados de mistura.
> ```python
> # Treinar SVM com kernel RBF (gamma=0.1)
> svm_rbf_01 = SVC(kernel='rbf', gamma=0.1, C=1)
> svm_rbf_01.fit(X_train, y_train)
> y_pred_rbf_01 = svm_rbf_01.predict(X_test)
> accuracy_rbf_01 = accuracy_score(y_test, y_pred_rbf_01)
>
> # Treinar SVM com kernel RBF (gamma=10)
> svm_rbf_10 = SVC(kernel='rbf', gamma=10, C=1)
> svm_rbf_10.fit(X_train, y_train)
> y_pred_rbf_10 = svm_rbf_10.predict(X_test)
> accuracy_rbf_10 = accuracy_score(y_test, y_pred_rbf_10)
>
> print(f"Acur√°cia (Kernel RBF, gamma=0.1): {accuracy_rbf_01:.4f}")
> print(f"Acur√°cia (Kernel RBF, gamma=10): {accuracy_rbf_10:.4f}")
>
> # Visualizar as fronteiras de decis√£o (simplificado)
> plt.figure(figsize=(10, 5))
> plt.subplot(1, 2, 1)
> plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_rbf_01, cmap='viridis', edgecolors='k')
> plt.title('Kernel RBF (gamma=0.1)')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
>
> plt.subplot(1, 2, 2)
> plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_rbf_10, cmap='viridis', edgecolors='k')
> plt.title('Kernel RBF (gamma=10)')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
> plt.tight_layout()
> plt.show()
> ```
> Ao executar o c√≥digo, observamos que com `gamma=0.1` a fronteira de decis√£o √© mais suave, enquanto com `gamma=10`, a fronteira √© mais complexa e se ajusta mais aos dados de treino. A acur√°cia pode variar um pouco devido √† aleatoriedade, mas em geral, um valor de `gamma` menor pode apresentar um vi√©s maior, enquanto um valor maior pode apresentar um overfitting, especialmente se `C` for alto tamb√©m.

**Lemma 3:** O par√¢metro $\gamma$ do *kernel* RBF controla a largura da fun√ß√£o gaussiana e a complexidade da fronteira de decis√£o, e a escolha apropriada de $\gamma$ envolve a busca por um equil√≠brio entre vi√©s e vari√¢ncia.

A demonstra√ß√£o desse lemma se baseia na an√°lise do efeito de $\gamma$ no *kernel* RBF, e como esse par√¢metro afeta a capacidade do modelo de modelar detalhes dos dados de treinamento. Valores baixos de $\gamma$ levam a uma fun√ß√£o mais "larga", com menos precis√£o, e valores altos, a uma fun√ß√£o mais "estreita", com mais precis√£o, o que resulta em modelos mais ou menos complexos, respectivamente.

### Compara√ß√£o do Desempenho e Impacto de C

```mermaid
graph LR
    subgraph "SVM Performance & Regularization"
        A["Kernel Choice (Poly/RBF)"]
        B["Kernel Parameters (d/Œ≥)"]
        C["Regularization Parameter 'C'"]
        A & B --> D["Model Complexity"];
        C --> D
        D --> E["Performance on Test Data"];
    end
```

A compara√ß√£o do desempenho das SVMs com *kernels* polinomial e RBF nos dados de mistura revela que o *kernel* RBF geralmente oferece uma performance melhor, especialmente quando as rela√ß√µes entre as *features* e as classes s√£o complexas, e n√£o podem ser modeladas por polin√¥mios. O *kernel* polinomial √© mais adequado para problemas onde existe uma estrutura polinomial entre as *features*. No entanto, o RBF oferece uma maior flexibilidade e capacidade de adapta√ß√£o a diferentes tipos de dados.

Al√©m da escolha do *kernel*, o ajuste do par√¢metro de regulariza√ß√£o **C** √© crucial para o desempenho do modelo. Como discutido em cap√≠tulos anteriores, o par√¢metro $C$ controla o equil√≠brio entre a maximiza√ß√£o da margem e a penaliza√ß√£o por erros de classifica√ß√£o [^12.2]. Valores altos de $C$ tendem a levar a modelos com margens menores e um maior n√∫mero de vetores de suporte, e esses modelos s√£o mais propensos a *overfitting*. Valores baixos de $C$, por outro lado, levam a modelos com margem maior, menos vetores de suporte e mais robustos, com uma melhor capacidade de generalizar.

A escolha do valor apropriado de $C$ e dos par√¢metros do *kernel* √© um processo emp√≠rico, que envolve a experimenta√ß√£o e a utiliza√ß√£o de t√©cnicas de valida√ß√£o cruzada para obter modelos SVM com bom desempenho em dados n√£o vistos.

> üí° **Exemplo Num√©rico:**
> Vamos treinar SVMs com kernel RBF usando diferentes valores de C (C=0.1 e C=10) para analisar o efeito da regulariza√ß√£o. Vamos usar um valor fixo para gamma (Œ≥=1) para simplificar.
> ```python
> # Treinar SVM com kernel RBF (C=0.1, gamma=1)
> svm_rbf_c01 = SVC(kernel='rbf', gamma=1, C=0.1)
> svm_rbf_c01.fit(X_train, y_train)
> y_pred_rbf_c01 = svm_rbf_c01.predict(X_test)
> accuracy_rbf_c01 = accuracy_score(y_test, y_pred_rbf_c01)
>
> # Treinar SVM com kernel RBF (C=10, gamma=1)
> svm_rbf_c10 = SVC(kernel='rbf', gamma=1, C=10)
> svm_rbf_c10.fit(X_train, y_train)
> y_pred_rbf_c10 = svm_rbf_c10.predict(X_test)
> accuracy_rbf_c10 = accuracy_score(y_test, y_pred_rbf_c10)
>
> print(f"Acur√°cia (Kernel RBF, C=0.1, gamma=1): {accuracy_rbf_c01:.4f}")
> print(f"Acur√°cia (Kernel RBF, C=10, gamma=1): {accuracy_rbf_c10:.4f}")
>
> # Visualizar as fronteiras de decis√£o (simplificado)
> plt.figure(figsize=(10, 5))
> plt.subplot(1, 2, 1)
> plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_rbf_c01, cmap='viridis', edgecolors='k')
> plt.title('Kernel RBF (C=0.1)')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
>
> plt.subplot(1, 2, 2)
> plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_rbf_c10, cmap='viridis', edgecolors='k')
> plt.title('Kernel RBF (C=10)')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
> plt.tight_layout()
> plt.show()
> ```
> Executando o c√≥digo, podemos observar que com `C=0.1` a fronteira √© mais suave e h√° uma maior toler√¢ncia a erros de classifica√ß√£o, enquanto com `C=10` a fronteira tenta classificar corretamente todos os dados de treino, o que pode levar a um overfitting. A acur√°cia nos dados de teste pode variar um pouco, mas um valor menor de `C` tende a generalizar melhor.

**Corol√°rio 3:** A escolha do *kernel* e de seus par√¢metros, juntamente com o ajuste do par√¢metro de regulariza√ß√£o C, s√£o cruciais para o desempenho das SVMs e a escolha apropriada depende das caracter√≠sticas espec√≠ficas dos dados e do problema de classifica√ß√£o.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise do impacto da escolha do *kernel* e de seus par√¢metros na fun√ß√£o de decis√£o e no desempenho do modelo, onde a valida√ß√£o cruzada se torna um mecanismo para selecionar os melhores par√¢metros para a generaliza√ß√£o e performance.

### Conclus√£o

Neste cap√≠tulo, analisamos o desempenho de **SVMs n√£o lineares** utilizando **kernels polinomial e radial** em um conjunto de dados de mistura. Vimos como a escolha do *kernel* e de seus par√¢metros influencia a complexidade da fronteira de decis√£o, o n√∫mero de vetores de suporte e a capacidade de generaliza√ß√£o do modelo.

Exploramos a import√¢ncia do par√¢metro $C$ e como ele controla o equil√≠brio entre a maximiza√ß√£o da margem e a toler√¢ncia a erros de classifica√ß√£o, e como a escolha apropriada dos par√¢metros do modelo √© crucial para a obten√ß√£o de um bom desempenho em dados n√£o vistos.

A utiliza√ß√£o de dados de mistura ilustra as vantagens de SVMs com kernels, especialmente o RBF, para a modelagem de dados complexos com sobreposi√ß√£o de classes. A experimenta√ß√£o com diferentes valores de par√¢metros (C e os par√¢metros do *kernel*) permite que o profissional tenha mais *insights* sobre o problema e a resposta do modelo. A capacidade de ajustar os par√¢metros do modelo de acordo com as caracter√≠sticas do conjunto de dados √© um dos pontos fortes das SVMs, o que permite criar modelos flex√≠veis e robustos.

### Footnotes

[^12.1]: "In this chapter we describe generalizations of linear decision boundaries for classification. Optimal separating hyperplanes are introduced in Chapter 4 for the case when two classes are linearly separable. Here we cover extensions to the nonseparable case, where the classes overlap. These techniques are then generalized to what is known as the support vector machine, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.2]: "In Chapter 4 we discussed a technique for constructing an optimal separating hyperplane between two perfectly separated classes. We review this and generalize to the nonseparable case, where the classes may not be separable by a linear boundary." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*
