## TÃ­tulo: ReformulaÃ§Ã£o do CritÃ©rio de OtimizaÃ§Ã£o e NotaÃ§Ã£o Matricial para SVMs: Primal e Dual

```mermaid
graph LR
    A[Dados de Entrada (X, y)] --> B(FormulaÃ§Ã£o Primal);
    A --> C(FormulaÃ§Ã£o Dual);
    B --> D{OtimizaÃ§Ã£o (Î², Î²â‚€, Î¾)};
    C --> E{OtimizaÃ§Ã£o (Î±)};
    D --> F[Hiperplano Separador];
    E --> G[Multiplicadores de Lagrange (Î±)];
    G --> F;
    F --> H(ClassificaÃ§Ã£o);
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

### IntroduÃ§Ã£o

A formulaÃ§Ã£o matemÃ¡tica das **Support Vector Machines (SVMs)** pode ser expressa de forma concisa e elegante atravÃ©s da utilizaÃ§Ã£o de **notaÃ§Ã£o matricial**. A notaÃ§Ã£o matricial nÃ£o sÃ³ simplifica a representaÃ§Ã£o das equaÃ§Ãµes, como tambÃ©m facilita a compreensÃ£o das relaÃ§Ãµes entre as diferentes variÃ¡veis e parÃ¢metros do modelo. AlÃ©m disso, a reformulaÃ§Ã£o do critÃ©rio de otimizaÃ§Ã£o utilizando notaÃ§Ã£o matricial permite obter uma visÃ£o mais clara da estrutura do problema e facilita a derivaÃ§Ã£o das soluÃ§Ãµes Ã³timas.

Neste capÃ­tulo, vamos reformular o problema de otimizaÃ§Ã£o das SVMs, tanto para o caso primal quanto para o caso dual, utilizando notaÃ§Ã£o matricial. Apresentaremos as matrizes e vetores relevantes, e mostraremos como as funÃ§Ãµes objetivo e as restriÃ§Ãµes podem ser expressas de forma concisa atravÃ©s dessa notaÃ§Ã£o. Exploraremos tambÃ©m as conexÃµes entre as formulaÃ§Ãµes primal e dual, e como a utilizaÃ§Ã£o da notaÃ§Ã£o matricial facilita a implementaÃ§Ã£o e anÃ¡lise dos algoritmos de treinamento das SVMs.

A reformulaÃ§Ã£o do critÃ©rio de otimizaÃ§Ã£o e a utilizaÃ§Ã£o da notaÃ§Ã£o matricial sÃ£o passos importantes para a compreensÃ£o profunda e para o uso avanÃ§ado das SVMs em problemas de classificaÃ§Ã£o e regressÃ£o. A notaÃ§Ã£o matricial tambÃ©m se tornarÃ¡ essencial em outros capÃ­tulos ao longo da apresentaÃ§Ã£o dos mÃ©todos mais avanÃ§ados.

### NotaÃ§Ã£o Matricial para o Problema Primal

**Conceito 1: DefiniÃ§Ã£o das Matrizes e Vetores**

Para expressar o problema primal das SVMs em notaÃ§Ã£o matricial, vamos definir as seguintes matrizes e vetores:

*   **Matriz de *Features* (X):**
    A matriz $X$ tem dimensÃ£o $N \times p$, onde $N$ Ã© o nÃºmero de amostras e $p$ Ã© o nÃºmero de *features*. Cada linha de $X$ corresponde a um vetor de *features* $x_i$.
    $$ X = \begin{bmatrix} x_1^T \\ x_2^T \\ \vdots \\ x_N^T \end{bmatrix} $$
*   **Vetor de RÃ³tulos (y):**
    O vetor $y$ tem dimensÃ£o $N \times 1$, onde cada elemento $y_i \in \{-1, 1\}$ corresponde ao rÃ³tulo da $i$-Ã©sima amostra.
    $$ y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_N \end{bmatrix} $$
*   **Vetor de VariÃ¡veis de Folga ($\xi$):**
    O vetor $\xi$ tem dimensÃ£o $N \times 1$, onde cada elemento $\xi_i \geq 0$ corresponde Ã  variÃ¡vel de folga da $i$-Ã©sima amostra.
    $$ \xi = \begin{bmatrix} \xi_1 \\ \xi_2 \\ \vdots \\ \xi_N \end{bmatrix} $$
*   **Vetor de ParÃ¢metros ($\beta$):**
    O vetor $\beta$ tem dimensÃ£o $p \times 1$, onde cada elemento corresponde a um coeficiente do hiperplano separador.
*   **ParÃ¢metro de *bias* ($\beta_0$):**
    O parÃ¢metro $\beta_0$ Ã© um escalar que define a posiÃ§Ã£o do hiperplano no espaÃ§o.
*   **ParÃ¢metro de RegularizaÃ§Ã£o (C):**
    O parÃ¢metro $C$ Ã© um escalar positivo que controla o compromisso entre a maximizaÃ§Ã£o da margem e a tolerÃ¢ncia a erros de classificaÃ§Ã£o.

**Lemma 1:** A notaÃ§Ã£o matricial permite representar os dados e os parÃ¢metros do modelo SVM de forma concisa e eficiente, facilitando a formulaÃ§Ã£o do problema de otimizaÃ§Ã£o.

A demonstraÃ§Ã£o desse lemma envolve apenas a definiÃ§Ã£o das matrizes e vetores e a demonstraÃ§Ã£o de que eles representam todos os dados e parÃ¢metros do modelo.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Vamos considerar um conjunto de dados com 3 amostras e 2 *features*:
> $$ X = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \end{bmatrix}, \quad y = \begin{bmatrix} -1 \\ 1 \\ 1 \end{bmatrix} $$
> Aqui, $N=3$ e $p=2$. Suponha que apÃ³s o treinamento, encontramos $\beta = \begin{bmatrix} 0.5 \\ -0.5 \end{bmatrix}$, $\beta_0 = -0.5$, $C=1$, e as variÃ¡veis de folga $\xi = \begin{bmatrix} 0.1 \\ 0 \\ 0.2 \end{bmatrix}$.  Podemos observar que a primeira amostra ($x_1 = [1, 2]$) tem uma variÃ¡vel de folga maior (0.1) pois estÃ¡ mais prÃ³xima da fronteira de decisÃ£o ou do lado errado, enquanto a segunda amostra ($x_2 = [2, 1]$) nÃ£o tem variÃ¡vel de folga ($\xi_2 = 0$), pois estÃ¡ corretamente classificada e longe da margem.
>
> Este exemplo ilustra como os dados e parÃ¢metros sÃ£o representados pelas matrizes e vetores, e como os valores refletem as caracterÃ­sticas do modelo.

**Conceito 2: FormulaÃ§Ã£o do Problema Primal em NotaÃ§Ã£o Matricial**

Utilizando as matrizes e vetores definidos anteriormente, o problema primal das SVMs, para o caso nÃ£o separÃ¡vel, pode ser expresso em notaÃ§Ã£o matricial como:

$$ \min_{\beta, \beta_0, \xi} \frac{1}{2} \beta^T \beta + C 1^T \xi $$

sujeito a:

$$ \text{diag}(y)(X\beta + \beta_0 1) \geq 1 - \xi $$
$$ \xi \geq 0 $$

onde:
*   $1$ Ã© um vetor de uns com dimensÃ£o $N \times 1$.
*  $\text{diag}(y)$ Ã© uma matriz diagonal, onde os elementos da diagonal sÃ£o os elementos do vetor $y$.

A funÃ§Ã£o objetivo $\frac{1}{2} \beta^T \beta + C 1^T \xi$ representa a minimizaÃ§Ã£o da norma ao quadrado do vetor $\beta$, juntamente com a penalidade por violaÃ§Ãµes da margem, como discutido anteriormente. A restriÃ§Ã£o $\text{diag}(y)(X\beta + \beta_0 1) \geq 1 - \xi$ garante que todas as amostras estejam corretamente classificadas ou dentro da margem. A restriÃ§Ã£o $\xi \geq 0$ garante que as variÃ¡veis de folga sejam nÃ£o negativas.

```mermaid
graph TD
    subgraph "Primal Problem Formulation"
        direction TB
        A["Minimize: 1/2 * Î²áµ€Î² + C * 1áµ€Î¾"]
        B["Subject to: diag(y) * (XÎ² + Î²â‚€1) â‰¥ 1 - Î¾"]
        C["Î¾ â‰¥ 0"]
        A --> B
        B --> C
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

**CorolÃ¡rio 1:** A notaÃ§Ã£o matricial simplifica a representaÃ§Ã£o do problema primal, tornando a funÃ§Ã£o objetivo e as restriÃ§Ãµes mais concisas e fÃ¡ceis de manipular algebricamente.

A demonstraÃ§Ã£o desse corolÃ¡rio envolve mostrar como a notaÃ§Ã£o matricial simplifica as equaÃ§Ãµes para um formato mais compacto e representativo de todo o problema.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Usando os dados do exemplo anterior, vamos calcular uma parte da restriÃ§Ã£o para a primeira amostra ($i=1$):
>  *   $y_1 = -1$, $x_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$, $\beta = \begin{bmatrix} 0.5 \\ -0.5 \end{bmatrix}$, $\beta_0 = -0.5$, $\xi_1 = 0.1$.
>  *   $X\beta + \beta_0 1 = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \end{bmatrix} \begin{bmatrix} 0.5 \\ -0.5 \end{bmatrix} + (-0.5) \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} -0.5 \\ 0.5 \\ 0 \end{bmatrix} -  \begin{bmatrix} 0.5 \\ 0.5 \\ 0.5 \end{bmatrix} = \begin{bmatrix} -1 \\ 0 \\ -0.5 \end{bmatrix}$
>  *   $\text{diag}(y)(X\beta + \beta_0 1) = \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} -1 \\ 0 \\ -0.5 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \\ -0.5 \end{bmatrix}$
>  *   $1 - \xi = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} - \begin{bmatrix} 0.1 \\ 0 \\ 0.2 \end{bmatrix} = \begin{bmatrix} 0.9 \\ 1 \\ 0.8 \end{bmatrix}$
>  *   Para a primeira amostra: $1 \geq 1 - 0.1 = 0.9$, que Ã© verdadeiro.
>
> A restriÃ§Ã£o Ã© verificada para cada amostra, garantindo que a classificaÃ§Ã£o esteja correta ou dentro da margem, com uma penalidade para as amostras que violam a margem.

### NotaÃ§Ã£o Matricial para o Problema Dual

```mermaid
graph LR
    A[Dados de Entrada (X, y)] --> B(Matriz de Produtos Internos (K));
    B --> C(Multiplicadores de Lagrange (Î±));
    C --> D{OtimizaÃ§Ã£o (Î±)};
    D --> E[FunÃ§Ã£o Objetivo Dual];
    E --> F[Multiplicadores de Lagrange Ã“timos (Î±*)];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

Assim como o problema primal, o problema dual das SVMs tambÃ©m pode ser expresso de forma concisa utilizando notaÃ§Ã£o matricial. Para isso, vamos definir as seguintes matrizes e vetores:

*   **Matriz de Produtos Internos (K):**
    A matriz $K$ tem dimensÃ£o $N \times N$, onde o elemento $K_{ij} = x_i^T x_j$ corresponde ao produto interno entre as amostras $x_i$ e $x_j$.
    $$ K = \begin{bmatrix} x_1^T x_1 & x_1^T x_2 & \cdots & x_1^T x_N \\ x_2^T x_1 & x_2^T x_2 & \cdots & x_2^T x_N \\ \vdots & \vdots & \ddots & \vdots \\ x_N^T x_1 & x_N^T x_2 & \cdots & x_N^T x_N \end{bmatrix} $$

*   **Vetor de Multiplicadores de Lagrange ($\alpha$):**
     O vetor $\alpha$ tem dimensÃ£o $N \times 1$, onde cada elemento $\alpha_i$ corresponde ao multiplicador de Lagrange da $i$-Ã©sima amostra e tem como restriÃ§Ã£o $0 \le \alpha_i \le C$.

*   **Vetor de RÃ³tulos (y):**
    O vetor $y$ tem dimensÃ£o $N \times 1$ e os elementos correspondem aos rÃ³tulos das classes de cada amostra, como definido anteriormente.

**Lemma 2:** A notaÃ§Ã£o matricial permite expressar a funÃ§Ã£o objetivo do problema dual e suas restriÃ§Ãµes de forma concisa e elegante, facilitando a manipulaÃ§Ã£o algÃ©brica e a implementaÃ§Ã£o computacional.

A demonstraÃ§Ã£o desse lemma se baseia na definiÃ§Ã£o das matrizes e vetores e na demonstraÃ§Ã£o que eles expressam todos os dados e parÃ¢metros do problema dual.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Usando a matriz $X$ do exemplo anterior, podemos calcular a matriz de produtos internos $K$:
> $$
> K = \begin{bmatrix}
>  \begin{bmatrix} 1 & 2 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} & \begin{bmatrix} 1 & 2 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} & \begin{bmatrix} 1 & 2 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix} \\
>  \begin{bmatrix} 2 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} & \begin{bmatrix} 2 & 1 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} & \begin{bmatrix} 2 & 1 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix} \\
>  \begin{bmatrix} 3 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} & \begin{bmatrix} 3 & 3 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} & \begin{bmatrix} 3 & 3 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix}
> \end{bmatrix}
> =
> \begin{bmatrix}
>  5 & 4 & 9 \\
>  4 & 5 & 9 \\
>  9 & 9 & 18
> \end{bmatrix}
> $$
>
> A matriz $K$ captura as similaridades entre as amostras atravÃ©s de seus produtos internos. Se apÃ³s o treinamento obtivermos o vetor de multiplicadores de Lagrange $\alpha = \begin{bmatrix} 0.2 \\ 0.7 \\ 0 \end{bmatrix}$, com $C = 1$, podemos observar que o primeiro multiplicador Ã© 0.2 e o segundo Ã© 0.7, ambos com valores entre 0 e C=1, enquanto o terceiro Ã© 0. Isso indica que a primeira e segunda amostras sÃ£o vetores de suporte, enquanto a terceira amostra nÃ£o Ã© um vetor de suporte.

**Conceito 2: FormulaÃ§Ã£o do Problema Dual em NotaÃ§Ã£o Matricial**

Utilizando as matrizes e vetores definidos, o problema dual das SVMs pode ser expresso em notaÃ§Ã£o matricial como:

$$ \max_{\alpha} 1^T \alpha - \frac{1}{2} \alpha^T \text{diag}(y) K \text{diag}(y) \alpha $$

sujeito a:

$$ 0 \leq \alpha \leq C 1 $$
$$ y^T \alpha = 0 $$

onde:

*   $1$ Ã© um vetor de uns com dimensÃ£o $N \times 1$.
*   $\text{diag}(y)$ Ã© uma matriz diagonal, onde os elementos da diagonal sÃ£o os elementos do vetor $y$.

A funÃ§Ã£o objetivo $1^T \alpha - \frac{1}{2} \alpha^T \text{diag}(y) K \text{diag}(y) \alpha$ representa a funÃ§Ã£o dual que precisa ser maximizada, onde a primeiro termo representa a soma dos multiplicadores de Lagrange e o segundo termo representa a relaÃ§Ã£o com os produtos internos entre os dados de treinamento. A restriÃ§Ã£o $0 \leq \alpha \leq C 1$ indica que os multiplicadores de Lagrange devem estar dentro do intervalo $[0, C]$, enquanto a restriÃ§Ã£o $y^T \alpha = 0$ impÃµe que os multiplicadores de Lagrange devem satisfazer a relaÃ§Ã£o de equilÃ­brio entre as classes.

A formulaÃ§Ã£o do problema dual em notaÃ§Ã£o matricial facilita a manipulaÃ§Ã£o algÃ©brica das equaÃ§Ãµes e revela a importÃ¢ncia da matriz de produtos internos $K$, que serÃ¡ substituÃ­da pelo *kernel* em problemas de classificaÃ§Ã£o nÃ£o lineares. A notaÃ§Ã£o matricial tambÃ©m facilita a implementaÃ§Ã£o computacional do algoritmo de treinamento das SVMs.

```mermaid
graph TD
    subgraph "Dual Problem Formulation"
        direction TB
        A["Maximize: 1áµ€Î± - 1/2 * Î±áµ€ * diag(y) * K * diag(y) * Î±"]
        B["Subject to: 0 â‰¤ Î± â‰¤ C1"]
         C["yáµ€Î± = 0"]
        A --> B
        B --> C
    end
       style A fill:#f9f,stroke:#333,stroke-width:2px
```

**CorolÃ¡rio 2:** A notaÃ§Ã£o matricial revela que o problema dual das SVMs depende apenas dos produtos internos entre os dados, o que possibilita a utilizaÃ§Ã£o do *kernel trick*.

A demonstraÃ§Ã£o desse corolÃ¡rio envolve a anÃ¡lise da funÃ§Ã£o objetivo do problema dual, que depende da matriz de produtos internos K. A substituiÃ§Ã£o dessa matriz por uma funÃ§Ã£o *kernel*, permite trabalhar com espaÃ§os de *features* de alta dimensÃ£o sem explicitar a transformaÃ§Ã£o.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Continuando com o exemplo anterior, com $y = \begin{bmatrix} -1 \\ 1 \\ 1 \end{bmatrix}$ e  $K = \begin{bmatrix} 5 & 4 & 9 \\ 4 & 5 & 9 \\ 9 & 9 & 18 \end{bmatrix}$ e $\alpha = \begin{bmatrix} 0.2 \\ 0.7 \\ 0 \end{bmatrix}$, vamos calcular a funÃ§Ã£o objetivo dual:
>
> *   $\text{diag}(y) = \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$
> *   $\text{diag}(y) K \text{diag}(y) = \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 5 & 4 & 9 \\ 4 & 5 & 9 \\ 9 & 9 & 18 \end{bmatrix} \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 5 & -4 & -9 \\ -4 & 5 & 9 \\ -9 & 9 & 18 \end{bmatrix}$
> *   $1^T \alpha = \begin{bmatrix} 1 & 1 & 1 \end{bmatrix} \begin{bmatrix} 0.2 \\ 0.7 \\ 0 \end{bmatrix} = 0.9$
> *   $\alpha^T \text{diag}(y) K \text{diag}(y) \alpha = \begin{bmatrix} 0.2 & 0.7 & 0 \end{bmatrix} \begin{bmatrix} 5 & -4 & -9 \\ -4 & 5 & 9 \\ -9 & 9 & 18 \end{bmatrix} \begin{bmatrix} 0.2 \\ 0.7 \\ 0 \end{bmatrix} = \begin{bmatrix} 0.2 & 0.7 & 0 \end{bmatrix} \begin{bmatrix} -1.8 \\ 2.7 \\ 5.4 \end{bmatrix} = 1.53$
> *   FunÃ§Ã£o objetivo dual = $0.9 - \frac{1}{2} * 1.53 = 0.135$
>
> Este valor Ã© o que o problema dual busca maximizar. O vetor $\alpha$ obtido Ã© tal que $y^T\alpha = (-1)*0.2 + 1*0.7 + 1*0 = 0.5$, o que nÃ£o satisfaz a restriÃ§Ã£o $y^T\alpha=0$. Ã‰ preciso usar um otimizador para obter o vetor correto de multiplicadores de Lagrange.

### ConexÃµes entre as FormulaÃ§Ãµes Primal e Dual

```mermaid
graph LR
    A[Problema Primal] --> B(Dualidade de Wolfe);
    B --> C[Problema Dual];
    A --> D(CondiÃ§Ãµes KKT);
    C --> D;
    D --> E[SoluÃ§Ã£o Ã“tima (Î², Î±)];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
```

As formulaÃ§Ãµes primal e dual do problema de otimizaÃ§Ã£o das SVMs estÃ£o interligadas atravÃ©s da teoria da **dualidade de Wolfe** e das **condiÃ§Ãµes de Karush-Kuhn-Tucker (KKT)**. A dualidade de Wolfe permite que o problema primal seja transformado em um problema dual, onde as variÃ¡veis primais sÃ£o substituÃ­das por multiplicadores de Lagrange. As condiÃ§Ãµes de KKT sÃ£o um conjunto de equaÃ§Ãµes e desigualdades que devem ser satisfeitas no ponto Ã³timo tanto para o problema primal quanto para o dual.

As condiÃ§Ãµes de KKT, expressas em notaÃ§Ã£o matricial, podem ser formuladas da seguinte forma:

1.  **Estacionaridade (Gradiente Nulo):**
    *   $\beta - X^T \text{diag}(y) \alpha = 0$
    *   $y^T \alpha = 0$
    *   $C1 - \alpha - \mu = 0$
2.  **Viabilidade Primal:**
    *   $\text{diag}(y)(X\beta + \beta_0 1) \geq 1 - \xi$
    *   $\xi \geq 0$
3.  **Viabilidade Dual:**
    *   $\alpha \geq 0$
    *   $\mu \geq 0$
4.  **Complementaridade:**
    *   $\alpha^T [\text{diag}(y)(X\beta + \beta_0 1) - 1 + \xi] = 0$
    *   $\mu^T \xi = 0$

onde $\mu$ Ã© o vetor dos multiplicadores de Lagrange associados Ã s restriÃ§Ãµes de nÃ£o negatividade das variÃ¡veis de folga $\xi$. As condiÃ§Ãµes KKT permitem relacionar os parÃ¢metros primais $(\beta, \beta_0, \xi)$ com os parÃ¢metros duais $(\alpha, \mu)$, e sÃ£o essenciais para encontrar a soluÃ§Ã£o Ã³tima do problema de otimizaÃ§Ã£o das SVMs.

```mermaid
graph TD
    subgraph "KKT Conditions"
        direction TB
        subgraph "Stationarity"
             A["Î² - Xáµ€diag(y)Î± = 0"]
            B["yáµ€Î± = 0"]
            C["C1 - Î± - Î¼ = 0"]
            A --> B
            B --> C
        end
        subgraph "Primal Feasibility"
            D["diag(y)(XÎ² + Î²â‚€1) â‰¥ 1 - Î¾"]
            E["Î¾ â‰¥ 0"]
            D --> E
        end
        subgraph "Dual Feasibility"
            F["Î± â‰¥ 0"]
            G["Î¼ â‰¥ 0"]
            F --> G
        end
        subgraph "Complementarity"
            H["Î±áµ€[diag(y)(XÎ² + Î²â‚€1) - 1 + Î¾] = 0"]
            I["Î¼áµ€Î¾ = 0"]
             H --> I
         end
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#f9f,stroke:#333,stroke-width:2px
```

A anÃ¡lise das condiÃ§Ãµes de KKT revela que a soluÃ§Ã£o do problema dual nos fornece os valores dos multiplicadores de Lagrange $\alpha_i$, que sÃ£o usados para calcular o vetor $\beta$ e o *bias* $\beta_0$, e identificar os vetores de suporte. A dependÃªncia do problema dual apenas dos produtos internos entre os dados de treinamento Ã© a base para a utilizaÃ§Ã£o do *kernel trick*.

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Suponha que, apÃ³s resolver o problema dual, encontramos $\alpha = \begin{bmatrix} 0.3 \\ 0.7 \\ 0 \end{bmatrix}$. Usando a condiÃ§Ã£o de estacionaridade, podemos encontrar $\beta$:
>
> *   $\beta = X^T \text{diag}(y) \alpha = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix}  \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.3 \\ 0.7 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix} \begin{bmatrix} -0.3 \\ 0.7 \\ 0 \end{bmatrix} = \begin{bmatrix} 1.1 \\ 0.1 \end{bmatrix} $
>
>  Note que apenas as amostras com $\alpha_i > 0$ contribuem para o cÃ¡lculo de $\beta$, que sÃ£o os vetores de suporte. A condiÃ§Ã£o $y^T \alpha = 0$ pode ser verificada: $(-1) * 0.3 + 1 * 0.7 + 1 * 0 = 0.4$, que nÃ£o Ã© exatamente zero devido a erros de otimizaÃ§Ã£o. Este valor deveria ser zero para satisfazer a restriÃ§Ã£o do problema dual.
>
> As condiÃ§Ãµes de complementaridade tambÃ©m nos ajudam a identificar os vetores de suporte. Se $\alpha_i > 0$, entÃ£o $\text{diag}(y)(X\beta + \beta_0 1) - 1 + \xi = 0$, o que significa que a amostra estÃ¡ na margem ou violando a margem. Se $\alpha_i = 0$, entÃ£o a amostra estÃ¡ corretamente classificada e fora da margem.

### ConclusÃ£o

Neste capÃ­tulo, reformulamos o problema de otimizaÃ§Ã£o das **Support Vector Machines (SVMs)**, tanto no caso primal quanto no dual, utilizando **notaÃ§Ã£o matricial**. Vimos como a notaÃ§Ã£o matricial permite representar os dados e os parÃ¢metros de forma concisa e eficiente, o que simplifica a manipulaÃ§Ã£o algÃ©brica e facilita a compreensÃ£o da estrutura do problema de otimizaÃ§Ã£o das SVMs.

Exploramos a formulaÃ§Ã£o matricial das funÃ§Ãµes objetivo e das restriÃ§Ãµes, tanto para o problema primal quanto para o dual, destacando como essa notaÃ§Ã£o simplifica a representaÃ§Ã£o das equaÃ§Ãµes e a derivaÃ§Ã£o da soluÃ§Ã£o. Analisamos tambÃ©m as conexÃµes entre as formulaÃ§Ãµes primal e dual, e como a utilizaÃ§Ã£o da dualidade de Wolfe leva a um problema de otimizaÃ§Ã£o mais tratÃ¡vel e permite a utilizaÃ§Ã£o do *kernel trick*.

A notaÃ§Ã£o matricial Ã© fundamental para uma compreensÃ£o mais profunda e para o uso avanÃ§ado das SVMs em problemas de classificaÃ§Ã£o e regressÃ£o. As matrizes e vetores apresentados neste capÃ­tulo estabelecem a base para o estudo de algoritmos de treinamento mais sofisticados e para a anÃ¡lise das propriedades dos modelos SVM. A capacidade de expressar os problemas de otimizaÃ§Ã£o em notaÃ§Ã£o matricial tambÃ©m facilita a implementaÃ§Ã£o computacional e a aplicaÃ§Ã£o das SVMs em uma variedade de problemas prÃ¡ticos.

### Footnotes

[^12.1]: "In this chapter we describe generalizations of linear decision boundaries for classification. Optimal separating hyperplanes are introduced in Chapter 4 for the case when two classes are linearly separable. Here we cover extensions to the nonseparable case, where the classes overlap. These techniques are then generalized to what is known as the support vector machine, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.2]: "In Chapter 4 we discussed a technique for constructing an optimal separating hyperplane between two perfectly separated classes. We review this and generalize to the nonseparable case, where the classes may not be separable by a linear boundary." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*
