## T√≠tulo: Reformula√ß√£o do Crit√©rio de Otimiza√ß√£o e Nota√ß√£o Matricial para SVMs: Primal e Dual

```mermaid
graph LR
    A[Dados de Entrada (X, y)] --> B(Formula√ß√£o Primal);
    A --> C(Formula√ß√£o Dual);
    B --> D{Otimiza√ß√£o (Œ≤, Œ≤‚ÇÄ, Œæ)};
    C --> E{Otimiza√ß√£o (Œ±)};
    D --> F[Hiperplano Separador];
    E --> G[Multiplicadores de Lagrange (Œ±)];
    G --> F;
    F --> H(Classifica√ß√£o);
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

A formula√ß√£o matem√°tica das **Support Vector Machines (SVMs)** pode ser expressa de forma concisa e elegante atrav√©s da utiliza√ß√£o de **nota√ß√£o matricial**. A nota√ß√£o matricial n√£o s√≥ simplifica a representa√ß√£o das equa√ß√µes, como tamb√©m facilita a compreens√£o das rela√ß√µes entre as diferentes vari√°veis e par√¢metros do modelo. Al√©m disso, a reformula√ß√£o do crit√©rio de otimiza√ß√£o utilizando nota√ß√£o matricial permite obter uma vis√£o mais clara da estrutura do problema e facilita a deriva√ß√£o das solu√ß√µes √≥timas.

Neste cap√≠tulo, vamos reformular o problema de otimiza√ß√£o das SVMs, tanto para o caso primal quanto para o caso dual, utilizando nota√ß√£o matricial. Apresentaremos as matrizes e vetores relevantes, e mostraremos como as fun√ß√µes objetivo e as restri√ß√µes podem ser expressas de forma concisa atrav√©s dessa nota√ß√£o. Exploraremos tamb√©m as conex√µes entre as formula√ß√µes primal e dual, e como a utiliza√ß√£o da nota√ß√£o matricial facilita a implementa√ß√£o e an√°lise dos algoritmos de treinamento das SVMs.

A reformula√ß√£o do crit√©rio de otimiza√ß√£o e a utiliza√ß√£o da nota√ß√£o matricial s√£o passos importantes para a compreens√£o profunda e para o uso avan√ßado das SVMs em problemas de classifica√ß√£o e regress√£o. A nota√ß√£o matricial tamb√©m se tornar√° essencial em outros cap√≠tulos ao longo da apresenta√ß√£o dos m√©todos mais avan√ßados.

### Nota√ß√£o Matricial para o Problema Primal

**Conceito 1: Defini√ß√£o das Matrizes e Vetores**

Para expressar o problema primal das SVMs em nota√ß√£o matricial, vamos definir as seguintes matrizes e vetores:

*   **Matriz de *Features* (X):**
    A matriz $X$ tem dimens√£o $N \times p$, onde $N$ √© o n√∫mero de amostras e $p$ √© o n√∫mero de *features*. Cada linha de $X$ corresponde a um vetor de *features* $x_i$.
    $$ X = \begin{bmatrix} x_1^T \\ x_2^T \\ \vdots \\ x_N^T \end{bmatrix} $$
*   **Vetor de R√≥tulos (y):**
    O vetor $y$ tem dimens√£o $N \times 1$, onde cada elemento $y_i \in \{-1, 1\}$ corresponde ao r√≥tulo da $i$-√©sima amostra.
    $$ y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_N \end{bmatrix} $$
*   **Vetor de Vari√°veis de Folga ($\xi$):**
    O vetor $\xi$ tem dimens√£o $N \times 1$, onde cada elemento $\xi_i \geq 0$ corresponde √† vari√°vel de folga da $i$-√©sima amostra.
    $$ \xi = \begin{bmatrix} \xi_1 \\ \xi_2 \\ \vdots \\ \xi_N \end{bmatrix} $$
*   **Vetor de Par√¢metros ($\beta$):**
    O vetor $\beta$ tem dimens√£o $p \times 1$, onde cada elemento corresponde a um coeficiente do hiperplano separador.
*   **Par√¢metro de *bias* ($\beta_0$):**
    O par√¢metro $\beta_0$ √© um escalar que define a posi√ß√£o do hiperplano no espa√ßo.
*   **Par√¢metro de Regulariza√ß√£o (C):**
    O par√¢metro $C$ √© um escalar positivo que controla o compromisso entre a maximiza√ß√£o da margem e a toler√¢ncia a erros de classifica√ß√£o.

**Lemma 1:** A nota√ß√£o matricial permite representar os dados e os par√¢metros do modelo SVM de forma concisa e eficiente, facilitando a formula√ß√£o do problema de otimiza√ß√£o.

A demonstra√ß√£o desse lemma envolve apenas a defini√ß√£o das matrizes e vetores e a demonstra√ß√£o de que eles representam todos os dados e par√¢metros do modelo.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um conjunto de dados com 3 amostras e 2 *features*:
> $$ X = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \end{bmatrix}, \quad y = \begin{bmatrix} -1 \\ 1 \\ 1 \end{bmatrix} $$
> Aqui, $N=3$ e $p=2$. Suponha que ap√≥s o treinamento, encontramos $\beta = \begin{bmatrix} 0.5 \\ -0.5 \end{bmatrix}$, $\beta_0 = -0.5$, $C=1$, e as vari√°veis de folga $\xi = \begin{bmatrix} 0.1 \\ 0 \\ 0.2 \end{bmatrix}$.  Podemos observar que a primeira amostra ($x_1 = [1, 2]$) tem uma vari√°vel de folga maior (0.1) pois est√° mais pr√≥xima da fronteira de decis√£o ou do lado errado, enquanto a segunda amostra ($x_2 = [2, 1]$) n√£o tem vari√°vel de folga ($\xi_2 = 0$), pois est√° corretamente classificada e longe da margem.
>
> Este exemplo ilustra como os dados e par√¢metros s√£o representados pelas matrizes e vetores, e como os valores refletem as caracter√≠sticas do modelo.

**Conceito 2: Formula√ß√£o do Problema Primal em Nota√ß√£o Matricial**

Utilizando as matrizes e vetores definidos anteriormente, o problema primal das SVMs, para o caso n√£o separ√°vel, pode ser expresso em nota√ß√£o matricial como:

$$ \min_{\beta, \beta_0, \xi} \frac{1}{2} \beta^T \beta + C 1^T \xi $$

sujeito a:

$$ \text{diag}(y)(X\beta + \beta_0 1) \geq 1 - \xi $$
$$ \xi \geq 0 $$

onde:
*   $1$ √© um vetor de uns com dimens√£o $N \times 1$.
*  $\text{diag}(y)$ √© uma matriz diagonal, onde os elementos da diagonal s√£o os elementos do vetor $y$.

A fun√ß√£o objetivo $\frac{1}{2} \beta^T \beta + C 1^T \xi$ representa a minimiza√ß√£o da norma ao quadrado do vetor $\beta$, juntamente com a penalidade por viola√ß√µes da margem, como discutido anteriormente. A restri√ß√£o $\text{diag}(y)(X\beta + \beta_0 1) \geq 1 - \xi$ garante que todas as amostras estejam corretamente classificadas ou dentro da margem. A restri√ß√£o $\xi \geq 0$ garante que as vari√°veis de folga sejam n√£o negativas.

```mermaid
graph TD
    subgraph "Primal Problem Formulation"
        direction TB
        A["Minimize: 1/2 * Œ≤·µÄŒ≤ + C * 1·µÄŒæ"]
        B["Subject to: diag(y) * (XŒ≤ + Œ≤‚ÇÄ1) ‚â• 1 - Œæ"]
        C["Œæ ‚â• 0"]
        A --> B
        B --> C
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

**Corol√°rio 1:** A nota√ß√£o matricial simplifica a representa√ß√£o do problema primal, tornando a fun√ß√£o objetivo e as restri√ß√µes mais concisas e f√°ceis de manipular algebricamente.

A demonstra√ß√£o desse corol√°rio envolve mostrar como a nota√ß√£o matricial simplifica as equa√ß√µes para um formato mais compacto e representativo de todo o problema.

> üí° **Exemplo Num√©rico:**
> Usando os dados do exemplo anterior, vamos calcular uma parte da restri√ß√£o para a primeira amostra ($i=1$):
>  *   $y_1 = -1$, $x_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$, $\beta = \begin{bmatrix} 0.5 \\ -0.5 \end{bmatrix}$, $\beta_0 = -0.5$, $\xi_1 = 0.1$.
>  *   $X\beta + \beta_0 1 = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \end{bmatrix} \begin{bmatrix} 0.5 \\ -0.5 \end{bmatrix} + (-0.5) \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} -0.5 \\ 0.5 \\ 0 \end{bmatrix} -  \begin{bmatrix} 0.5 \\ 0.5 \\ 0.5 \end{bmatrix} = \begin{bmatrix} -1 \\ 0 \\ -0.5 \end{bmatrix}$
>  *   $\text{diag}(y)(X\beta + \beta_0 1) = \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} -1 \\ 0 \\ -0.5 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \\ -0.5 \end{bmatrix}$
>  *   $1 - \xi = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} - \begin{bmatrix} 0.1 \\ 0 \\ 0.2 \end{bmatrix} = \begin{bmatrix} 0.9 \\ 1 \\ 0.8 \end{bmatrix}$
>  *   Para a primeira amostra: $1 \geq 1 - 0.1 = 0.9$, que √© verdadeiro.
>
> A restri√ß√£o √© verificada para cada amostra, garantindo que a classifica√ß√£o esteja correta ou dentro da margem, com uma penalidade para as amostras que violam a margem.

### Nota√ß√£o Matricial para o Problema Dual

```mermaid
graph LR
    A[Dados de Entrada (X, y)] --> B(Matriz de Produtos Internos (K));
    B --> C(Multiplicadores de Lagrange (Œ±));
    C --> D{Otimiza√ß√£o (Œ±)};
    D --> E[Fun√ß√£o Objetivo Dual];
    E --> F[Multiplicadores de Lagrange √ìtimos (Œ±*)];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

Assim como o problema primal, o problema dual das SVMs tamb√©m pode ser expresso de forma concisa utilizando nota√ß√£o matricial. Para isso, vamos definir as seguintes matrizes e vetores:

*   **Matriz de Produtos Internos (K):**
    A matriz $K$ tem dimens√£o $N \times N$, onde o elemento $K_{ij} = x_i^T x_j$ corresponde ao produto interno entre as amostras $x_i$ e $x_j$.
    $$ K = \begin{bmatrix} x_1^T x_1 & x_1^T x_2 & \cdots & x_1^T x_N \\ x_2^T x_1 & x_2^T x_2 & \cdots & x_2^T x_N \\ \vdots & \vdots & \ddots & \vdots \\ x_N^T x_1 & x_N^T x_2 & \cdots & x_N^T x_N \end{bmatrix} $$

*   **Vetor de Multiplicadores de Lagrange ($\alpha$):**
     O vetor $\alpha$ tem dimens√£o $N \times 1$, onde cada elemento $\alpha_i$ corresponde ao multiplicador de Lagrange da $i$-√©sima amostra e tem como restri√ß√£o $0 \le \alpha_i \le C$.

*   **Vetor de R√≥tulos (y):**
    O vetor $y$ tem dimens√£o $N \times 1$ e os elementos correspondem aos r√≥tulos das classes de cada amostra, como definido anteriormente.

**Lemma 2:** A nota√ß√£o matricial permite expressar a fun√ß√£o objetivo do problema dual e suas restri√ß√µes de forma concisa e elegante, facilitando a manipula√ß√£o alg√©brica e a implementa√ß√£o computacional.

A demonstra√ß√£o desse lemma se baseia na defini√ß√£o das matrizes e vetores e na demonstra√ß√£o que eles expressam todos os dados e par√¢metros do problema dual.

> üí° **Exemplo Num√©rico:**
> Usando a matriz $X$ do exemplo anterior, podemos calcular a matriz de produtos internos $K$:
> $$
> K = \begin{bmatrix}
>  \begin{bmatrix} 1 & 2 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} & \begin{bmatrix} 1 & 2 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} & \begin{bmatrix} 1 & 2 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix} \\
>  \begin{bmatrix} 2 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} & \begin{bmatrix} 2 & 1 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} & \begin{bmatrix} 2 & 1 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix} \\
>  \begin{bmatrix} 3 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} & \begin{bmatrix} 3 & 3 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} & \begin{bmatrix} 3 & 3 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix}
> \end{bmatrix}
> =
> \begin{bmatrix}
>  5 & 4 & 9 \\
>  4 & 5 & 9 \\
>  9 & 9 & 18
> \end{bmatrix}
> $$
>
> A matriz $K$ captura as similaridades entre as amostras atrav√©s de seus produtos internos. Se ap√≥s o treinamento obtivermos o vetor de multiplicadores de Lagrange $\alpha = \begin{bmatrix} 0.2 \\ 0.7 \\ 0 \end{bmatrix}$, com $C = 1$, podemos observar que o primeiro multiplicador √© 0.2 e o segundo √© 0.7, ambos com valores entre 0 e C=1, enquanto o terceiro √© 0. Isso indica que a primeira e segunda amostras s√£o vetores de suporte, enquanto a terceira amostra n√£o √© um vetor de suporte.

**Conceito 2: Formula√ß√£o do Problema Dual em Nota√ß√£o Matricial**

Utilizando as matrizes e vetores definidos, o problema dual das SVMs pode ser expresso em nota√ß√£o matricial como:

$$ \max_{\alpha} 1^T \alpha - \frac{1}{2} \alpha^T \text{diag}(y) K \text{diag}(y) \alpha $$

sujeito a:

$$ 0 \leq \alpha \leq C 1 $$
$$ y^T \alpha = 0 $$

onde:

*   $1$ √© um vetor de uns com dimens√£o $N \times 1$.
*   $\text{diag}(y)$ √© uma matriz diagonal, onde os elementos da diagonal s√£o os elementos do vetor $y$.

A fun√ß√£o objetivo $1^T \alpha - \frac{1}{2} \alpha^T \text{diag}(y) K \text{diag}(y) \alpha$ representa a fun√ß√£o dual que precisa ser maximizada, onde a primeiro termo representa a soma dos multiplicadores de Lagrange e o segundo termo representa a rela√ß√£o com os produtos internos entre os dados de treinamento. A restri√ß√£o $0 \leq \alpha \leq C 1$ indica que os multiplicadores de Lagrange devem estar dentro do intervalo $[0, C]$, enquanto a restri√ß√£o $y^T \alpha = 0$ imp√µe que os multiplicadores de Lagrange devem satisfazer a rela√ß√£o de equil√≠brio entre as classes.

A formula√ß√£o do problema dual em nota√ß√£o matricial facilita a manipula√ß√£o alg√©brica das equa√ß√µes e revela a import√¢ncia da matriz de produtos internos $K$, que ser√° substitu√≠da pelo *kernel* em problemas de classifica√ß√£o n√£o lineares. A nota√ß√£o matricial tamb√©m facilita a implementa√ß√£o computacional do algoritmo de treinamento das SVMs.

```mermaid
graph TD
    subgraph "Dual Problem Formulation"
        direction TB
        A["Maximize: 1·µÄŒ± - 1/2 * Œ±·µÄ * diag(y) * K * diag(y) * Œ±"]
        B["Subject to: 0 ‚â§ Œ± ‚â§ C1"]
         C["y·µÄŒ± = 0"]
        A --> B
        B --> C
    end
       style A fill:#f9f,stroke:#333,stroke-width:2px
```

**Corol√°rio 2:** A nota√ß√£o matricial revela que o problema dual das SVMs depende apenas dos produtos internos entre os dados, o que possibilita a utiliza√ß√£o do *kernel trick*.

A demonstra√ß√£o desse corol√°rio envolve a an√°lise da fun√ß√£o objetivo do problema dual, que depende da matriz de produtos internos K. A substitui√ß√£o dessa matriz por uma fun√ß√£o *kernel*, permite trabalhar com espa√ßos de *features* de alta dimens√£o sem explicitar a transforma√ß√£o.

> üí° **Exemplo Num√©rico:**
> Continuando com o exemplo anterior, com $y = \begin{bmatrix} -1 \\ 1 \\ 1 \end{bmatrix}$ e  $K = \begin{bmatrix} 5 & 4 & 9 \\ 4 & 5 & 9 \\ 9 & 9 & 18 \end{bmatrix}$ e $\alpha = \begin{bmatrix} 0.2 \\ 0.7 \\ 0 \end{bmatrix}$, vamos calcular a fun√ß√£o objetivo dual:
>
> *   $\text{diag}(y) = \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$
> *   $\text{diag}(y) K \text{diag}(y) = \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 5 & 4 & 9 \\ 4 & 5 & 9 \\ 9 & 9 & 18 \end{bmatrix} \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 5 & -4 & -9 \\ -4 & 5 & 9 \\ -9 & 9 & 18 \end{bmatrix}$
> *   $1^T \alpha = \begin{bmatrix} 1 & 1 & 1 \end{bmatrix} \begin{bmatrix} 0.2 \\ 0.7 \\ 0 \end{bmatrix} = 0.9$
> *   $\alpha^T \text{diag}(y) K \text{diag}(y) \alpha = \begin{bmatrix} 0.2 & 0.7 & 0 \end{bmatrix} \begin{bmatrix} 5 & -4 & -9 \\ -4 & 5 & 9 \\ -9 & 9 & 18 \end{bmatrix} \begin{bmatrix} 0.2 \\ 0.7 \\ 0 \end{bmatrix} = \begin{bmatrix} 0.2 & 0.7 & 0 \end{bmatrix} \begin{bmatrix} -1.8 \\ 2.7 \\ 5.4 \end{bmatrix} = 1.53$
> *   Fun√ß√£o objetivo dual = $0.9 - \frac{1}{2} * 1.53 = 0.135$
>
> Este valor √© o que o problema dual busca maximizar. O vetor $\alpha$ obtido √© tal que $y^T\alpha = (-1)*0.2 + 1*0.7 + 1*0 = 0.5$, o que n√£o satisfaz a restri√ß√£o $y^T\alpha=0$. √â preciso usar um otimizador para obter o vetor correto de multiplicadores de Lagrange.

### Conex√µes entre as Formula√ß√µes Primal e Dual

```mermaid
graph LR
    A[Problema Primal] --> B(Dualidade de Wolfe);
    B --> C[Problema Dual];
    A --> D(Condi√ß√µes KKT);
    C --> D;
    D --> E[Solu√ß√£o √ìtima (Œ≤, Œ±)];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
```

As formula√ß√µes primal e dual do problema de otimiza√ß√£o das SVMs est√£o interligadas atrav√©s da teoria da **dualidade de Wolfe** e das **condi√ß√µes de Karush-Kuhn-Tucker (KKT)**. A dualidade de Wolfe permite que o problema primal seja transformado em um problema dual, onde as vari√°veis primais s√£o substitu√≠das por multiplicadores de Lagrange. As condi√ß√µes de KKT s√£o um conjunto de equa√ß√µes e desigualdades que devem ser satisfeitas no ponto √≥timo tanto para o problema primal quanto para o dual.

As condi√ß√µes de KKT, expressas em nota√ß√£o matricial, podem ser formuladas da seguinte forma:

1.  **Estacionaridade (Gradiente Nulo):**
    *   $\beta - X^T \text{diag}(y) \alpha = 0$
    *   $y^T \alpha = 0$
    *   $C1 - \alpha - \mu = 0$
2.  **Viabilidade Primal:**
    *   $\text{diag}(y)(X\beta + \beta_0 1) \geq 1 - \xi$
    *   $\xi \geq 0$
3.  **Viabilidade Dual:**
    *   $\alpha \geq 0$
    *   $\mu \geq 0$
4.  **Complementaridade:**
    *   $\alpha^T [\text{diag}(y)(X\beta + \beta_0 1) - 1 + \xi] = 0$
    *   $\mu^T \xi = 0$

onde $\mu$ √© o vetor dos multiplicadores de Lagrange associados √†s restri√ß√µes de n√£o negatividade das vari√°veis de folga $\xi$. As condi√ß√µes KKT permitem relacionar os par√¢metros primais $(\beta, \beta_0, \xi)$ com os par√¢metros duais $(\alpha, \mu)$, e s√£o essenciais para encontrar a solu√ß√£o √≥tima do problema de otimiza√ß√£o das SVMs.

```mermaid
graph TD
    subgraph "KKT Conditions"
        direction TB
        subgraph "Stationarity"
             A["Œ≤ - X·µÄdiag(y)Œ± = 0"]
            B["y·µÄŒ± = 0"]
            C["C1 - Œ± - Œº = 0"]
            A --> B
            B --> C
        end
        subgraph "Primal Feasibility"
            D["diag(y)(XŒ≤ + Œ≤‚ÇÄ1) ‚â• 1 - Œæ"]
            E["Œæ ‚â• 0"]
            D --> E
        end
        subgraph "Dual Feasibility"
            F["Œ± ‚â• 0"]
            G["Œº ‚â• 0"]
            F --> G
        end
        subgraph "Complementarity"
            H["Œ±·µÄ[diag(y)(XŒ≤ + Œ≤‚ÇÄ1) - 1 + Œæ] = 0"]
            I["Œº·µÄŒæ = 0"]
             H --> I
         end
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#f9f,stroke:#333,stroke-width:2px
```

A an√°lise das condi√ß√µes de KKT revela que a solu√ß√£o do problema dual nos fornece os valores dos multiplicadores de Lagrange $\alpha_i$, que s√£o usados para calcular o vetor $\beta$ e o *bias* $\beta_0$, e identificar os vetores de suporte. A depend√™ncia do problema dual apenas dos produtos internos entre os dados de treinamento √© a base para a utiliza√ß√£o do *kernel trick*.

> üí° **Exemplo Num√©rico:**
> Suponha que, ap√≥s resolver o problema dual, encontramos $\alpha = \begin{bmatrix} 0.3 \\ 0.7 \\ 0 \end{bmatrix}$. Usando a condi√ß√£o de estacionaridade, podemos encontrar $\beta$:
>
> *   $\beta = X^T \text{diag}(y) \alpha = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix}  \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0.3 \\ 0.7 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \end{bmatrix} \begin{bmatrix} -0.3 \\ 0.7 \\ 0 \end{bmatrix} = \begin{bmatrix} 1.1 \\ 0.1 \end{bmatrix} $
>
>  Note que apenas as amostras com $\alpha_i > 0$ contribuem para o c√°lculo de $\beta$, que s√£o os vetores de suporte. A condi√ß√£o $y^T \alpha = 0$ pode ser verificada: $(-1) * 0.3 + 1 * 0.7 + 1 * 0 = 0.4$, que n√£o √© exatamente zero devido a erros de otimiza√ß√£o. Este valor deveria ser zero para satisfazer a restri√ß√£o do problema dual.
>
> As condi√ß√µes de complementaridade tamb√©m nos ajudam a identificar os vetores de suporte. Se $\alpha_i > 0$, ent√£o $\text{diag}(y)(X\beta + \beta_0 1) - 1 + \xi = 0$, o que significa que a amostra est√° na margem ou violando a margem. Se $\alpha_i = 0$, ent√£o a amostra est√° corretamente classificada e fora da margem.

### Conclus√£o

Neste cap√≠tulo, reformulamos o problema de otimiza√ß√£o das **Support Vector Machines (SVMs)**, tanto no caso primal quanto no dual, utilizando **nota√ß√£o matricial**. Vimos como a nota√ß√£o matricial permite representar os dados e os par√¢metros de forma concisa e eficiente, o que simplifica a manipula√ß√£o alg√©brica e facilita a compreens√£o da estrutura do problema de otimiza√ß√£o das SVMs.

Exploramos a formula√ß√£o matricial das fun√ß√µes objetivo e das restri√ß√µes, tanto para o problema primal quanto para o dual, destacando como essa nota√ß√£o simplifica a representa√ß√£o das equa√ß√µes e a deriva√ß√£o da solu√ß√£o. Analisamos tamb√©m as conex√µes entre as formula√ß√µes primal e dual, e como a utiliza√ß√£o da dualidade de Wolfe leva a um problema de otimiza√ß√£o mais trat√°vel e permite a utiliza√ß√£o do *kernel trick*.

A nota√ß√£o matricial √© fundamental para uma compreens√£o mais profunda e para o uso avan√ßado das SVMs em problemas de classifica√ß√£o e regress√£o. As matrizes e vetores apresentados neste cap√≠tulo estabelecem a base para o estudo de algoritmos de treinamento mais sofisticados e para a an√°lise das propriedades dos modelos SVM. A capacidade de expressar os problemas de otimiza√ß√£o em nota√ß√£o matricial tamb√©m facilita a implementa√ß√£o computacional e a aplica√ß√£o das SVMs em uma variedade de problemas pr√°ticos.

### Footnotes

[^12.1]: "In this chapter we describe generalizations of linear decision boundaries for classification. Optimal separating hyperplanes are introduced in Chapter 4 for the case when two classes are linearly separable. Here we cover extensions to the nonseparable case, where the classes overlap. These techniques are then generalized to what is known as the support vector machine, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.2]: "In Chapter 4 we discussed a technique for constructing an optimal separating hyperplane between two perfectly separated classes. We review this and generalize to the nonseparable case, where the classes may not be separable by a linear boundary." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*
