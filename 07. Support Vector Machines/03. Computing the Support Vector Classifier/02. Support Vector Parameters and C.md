## T√≠tulo: C√°lculo dos Par√¢metros dos Vetores de Suporte e o Papel do Par√¢metro C na Solu√ß√£o das SVMs

```mermaid
graph LR
    subgraph "SVM Parameter Calculation"
        direction TB
        A["Training Data"]
        A --> B("Optimization Problem")
        B --> C("Lagrange Multipliers (Œ±)")
        C --> D("Support Vectors")
        D --> E("Calculate Œ≤")
        D --> F("Calculate Œ≤‚ÇÄ")
        E --> G("Decision Hyperplane")
        F --> G
        H("Parameter C") --> B
    end
    style H fill:#f9f,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

No estudo das **Support Vector Machines (SVMs)**, a compreens√£o do c√°lculo dos par√¢metros dos **vetores de suporte** e do papel do par√¢metro de regulariza√ß√£o **C** √© crucial para a constru√ß√£o de modelos eficientes e robustos. Os vetores de suporte, como j√° discutimos, s√£o as amostras de treinamento que est√£o localizadas na margem ou a violam, e s√£o esses pontos que determinam a posi√ß√£o e orienta√ß√£o do hiperplano separador √≥timo. O par√¢metro $C$, por sua vez, controla o compromisso entre a maximiza√ß√£o da margem e a toler√¢ncia a erros de classifica√ß√£o [^12.2].

Neste cap√≠tulo, vamos explorar em detalhe o processo de c√°lculo dos par√¢metros associados aos vetores de suporte, incluindo o c√°lculo do vetor $\beta$ e do *bias* $\beta_0$ a partir dos multiplicadores de Lagrange. Analisaremos tamb√©m o impacto do par√¢metro $C$ na localiza√ß√£o dos vetores de suporte, no n√∫mero de vetores de suporte e na complexidade do modelo resultante. Entenderemos como o ajuste de $C$ influencia a capacidade de generaliza√ß√£o da SVM e o equil√≠brio entre vi√©s e vari√¢ncia.

A compreens√£o desses aspectos √© fundamental para a aplica√ß√£o pr√°tica das SVMs e para a escolha adequada dos par√¢metros do modelo, a fim de obter o melhor desempenho em diferentes conjuntos de dados. A import√¢ncia dos vetores de suporte e a maneira como eles s√£o usados para o c√°lculo da fun√ß√£o de decis√£o √© uma caracter√≠stica chave dos modelos SVM, e compreend√™-la √© central.

### C√°lculo do Vetor $\beta$ a partir dos Multiplicadores de Lagrange

**Conceito 1: A Rela√ß√£o entre $\beta$ e os Multiplicadores de Lagrange**

Como vimos no cap√≠tulo anterior, a dualidade de Wolfe nos leva √† seguinte express√£o para o vetor normal ao hiperplano, $\beta$, em termos dos multiplicadores de Lagrange $\alpha_i$:

$$ \beta = \sum_{i=1}^{N} \alpha_i y_i x_i $$

Essa equa√ß√£o mostra que o vetor $\beta$ √© uma combina√ß√£o linear dos vetores de *features* $x_i$, ponderados pelos multiplicadores de Lagrange $\alpha_i$ e pelos r√≥tulos das classes $y_i$. Essa express√£o √© fundamental, pois ela demonstra que o vetor $\beta$, que define a orienta√ß√£o do hiperplano separador, √© determinado apenas pelos vetores de suporte.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados com 4 amostras, onde $x_1 = [1, 2]$, $x_2 = [2, 1]$, $x_3 = [4, 5]$, $x_4 = [5, 4]$, com r√≥tulos $y_1 = 1$, $y_2 = 1$, $y_3 = -1$, $y_4 = -1$. Ap√≥s a otimiza√ß√£o do problema dual, obtivemos os seguintes multiplicadores de Lagrange: $\alpha_1 = 0.2$, $\alpha_2 = 0.5$, $\alpha_3 = 0.7$, e $\alpha_4 = 0$.
>
> O vetor $\beta$ √© calculado como:
>
> $\beta = \alpha_1 y_1 x_1 + \alpha_2 y_2 x_2 + \alpha_3 y_3 x_3 + \alpha_4 y_4 x_4$
>
> $\beta = (0.2)(1)[1, 2] + (0.5)(1)[2, 1] + (0.7)(-1)[4, 5] + (0)(-1)[5, 4]$
>
> $\beta = [0.2, 0.4] + [1, 0.5] + [-2.8, -3.5] + [0, 0]$
>
> $\beta = [-1.6, -2.6]$
>
> Neste exemplo, apenas as amostras $x_1$, $x_2$ e $x_3$ s√£o vetores de suporte, pois seus $\alpha_i$ s√£o diferentes de zero. A amostra $x_4$ n√£o √© um vetor de suporte, e n√£o contribui para o c√°lculo de $\beta$.

```mermaid
graph LR
    subgraph "Calculation of Œ≤"
        direction TB
        A["Œ≤ = Œ£ Œ±·µ¢ y·µ¢ x·µ¢"]
        B["Œ±·µ¢: Lagrange multipliers"]
        C["y·µ¢: Class labels"]
        D["x·µ¢: Feature vectors"]
        A --> B
        A --> C
        A --> D
    end
```

**Lemma 1:** O vetor $\beta$ √© uma combina√ß√£o linear dos vetores de *features* dos vetores de suporte, ponderados pelos multiplicadores de Lagrange e pelos r√≥tulos das classes.

A demonstra√ß√£o desse lemma se baseia na deriva√ß√£o da solu√ß√£o do problema dual das SVMs. Ao minimizar a fun√ß√£o Lagrangiana com rela√ß√£o aos par√¢metros primais $\beta$, $\beta_0$ e $\xi$, e utilizar as condi√ß√µes de otimalidade de KKT, obtemos a rela√ß√£o entre o vetor $\beta$ e os multiplicadores de Lagrange.

**Conceito 2: Vetores de Suporte e o C√°lculo de $\beta$**

Como vimos anteriormente, os **vetores de suporte** s√£o as amostras de treinamento para as quais os multiplicadores de Lagrange s√£o diferentes de zero $(\alpha_i > 0)$. Isso significa que apenas os vetores de suporte contribuem para o c√°lculo do vetor $\beta$. Em outras palavras, as amostras que est√£o longe da margem n√£o t√™m influ√™ncia na defini√ß√£o do hiperplano separador, e sua informa√ß√£o √© descartada. O c√°lculo de $\beta$ pode ser restringido para apenas as amostras para as quais $\alpha_i > 0$, que s√£o as amostras que mais afetam a separa√ß√£o linear.

O c√°lculo de $\beta$ pode ser expresso como:

$$ \beta = \sum_{i \in SV} \alpha_i y_i x_i $$

onde SV √© o conjunto dos √≠ndices dos vetores de suporte. Essa equa√ß√£o enfatiza o fato de que apenas os vetores de suporte s√£o relevantes para a defini√ß√£o do hiperplano separador.

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, o conjunto de vetores de suporte SV √© $\{1, 2, 3\}$. Ent√£o, o c√°lculo de $\beta$ pode ser reescrito como:
>
> $\beta = \sum_{i \in \{1,2,3\}} \alpha_i y_i x_i $
>
> $\beta = \alpha_1 y_1 x_1 + \alpha_2 y_2 x_2 + \alpha_3 y_3 x_3$
>
> $\beta = (0.2)(1)[1, 2] + (0.5)(1)[2, 1] + (0.7)(-1)[4, 5]$
>
> $\beta = [-1.6, -2.6]$
>
> Observe que este resultado √© id√™ntico ao resultado anterior, o que demonstra que apenas os vetores de suporte contribuem para o c√°lculo de $\beta$.

```mermaid
graph LR
 subgraph "Support Vectors' Influence on Œ≤"
  direction TB
  A["Œ≤ = Œ£ Œ±·µ¢ y·µ¢ x·µ¢  (for i ‚àà SV)"]
  B["SV: Support Vectors"]
  C["Œ±·µ¢ > 0 for Support Vectors"]
  A --> B
  B --> C
  end
```

**Corol√°rio 1:** O vetor $\beta$, que define a orienta√ß√£o do hiperplano separador, √© determinado unicamente pelos vetores de suporte.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise da equa√ß√£o para o c√°lculo de $\beta$. Como os multiplicadores de Lagrange $\alpha_i$ s√£o zero para todas as amostras que n√£o s√£o vetores de suporte, o c√°lculo de $\beta$ √© determinado unicamente pelas amostras que s√£o vetores de suporte.

### C√°lculo do Par√¢metro de *bias* $\beta_0$

```mermaid
graph LR
    subgraph "Bias Parameter (Œ≤‚ÇÄ) Calculation"
        direction TB
        A["Support Vectors on the Margin"]
        A --> B("KKT Condition: Œ±·µ¢[y·µ¢(Œ≤·µÄx·µ¢ + Œ≤‚ÇÄ) - 1 + Œæ·µ¢] = 0")
        B --> C("If 0 < Œ±·µ¢ < C, then y·µ¢(Œ≤·µÄx·µ¢ + Œ≤‚ÇÄ) = 1")
        C --> D("Œ≤‚ÇÄ = y·µ¢ - Œ≤·µÄx·µ¢ for SV on Margin")
        D --> E("Average for Stability: Œ≤‚ÇÄ = (1/N_SV) Œ£ (y·µ¢ - Œ≤·µÄx·µ¢)")
        E --> F("Bias Parameter (Œ≤‚ÇÄ)")
    end
```

O par√¢metro de *bias* $\beta_0$ define a posi√ß√£o do hiperplano separador no espa√ßo. Ao contr√°rio do vetor $\beta$, que √© calculado a partir de todos os vetores de suporte, o par√¢metro $\beta_0$ √© calculado atrav√©s dos vetores de suporte que est√£o exatamente sobre a margem.

As condi√ß√µes de Karush-Kuhn-Tucker (KKT) nos fornecem a seguinte condi√ß√£o de complementaridade:

$$ \alpha_i [y_i(\beta^T x_i + \beta_0) - 1 + \xi_i] = 0 $$

Essa condi√ß√£o implica que:

*   Se $0 < \alpha_i < C$, ent√£o $\xi_i = 0$, e $y_i(\beta^T x_i + \beta_0) = 1$. Isso indica que o ponto $x_i$ √© um vetor de suporte e est√° exatamente sobre a margem.
*   Se $\alpha_i = C$, ent√£o $y_i(\beta^T x_i + \beta_0) \leq 1$, e o ponto $x_i$ √© um vetor de suporte que viola a margem.
*   Se $\alpha_i = 0$, o ponto $x_i$ n√£o √© um vetor de suporte, e portanto n√£o influencia a defini√ß√£o do hiperplano.

Para calcular $\beta_0$, podemos utilizar qualquer vetor de suporte que esteja exatamente sobre a margem $(0 < \alpha_i < C)$:

$$ \beta_0 = y_i - \beta^T x_i $$

No entanto, para garantir maior estabilidade, √© comum calcular $\beta_0$ como a m√©dia dos valores obtidos a partir de todos os vetores de suporte que est√£o na margem:

$$ \beta_0 = \frac{1}{N_{SV}} \sum_{i \in SV} (y_i - \beta^T x_i) $$

onde $N_{SV}$ √© o n√∫mero de vetores de suporte que est√£o exatamente na margem.

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, suponha que o par√¢metro $C$ seja 1 e que, ap√≥s a otimiza√ß√£o, tenhamos $\alpha_1 = 0.2$, $\alpha_2 = 0.5$, e $\alpha_3 = 0.7$. Isso significa que todos os vetores de suporte est√£o na margem, pois nenhum $\alpha_i$ atingiu o valor de C. Podemos usar qualquer um dos vetores de suporte para calcular $\beta_0$, mas para maior estabilidade, vamos usar a m√©dia.
>
> J√° calculamos $\beta = [-1.6, -2.6]$. Vamos calcular $\beta_0$ usando os vetores de suporte $x_1$, $x_2$, e $x_3$.
>
> Para $x_1$:
>
> $\beta_0 = y_1 - \beta^T x_1 = 1 - ([-1.6, -2.6] \cdot [1, 2]) = 1 - (-1.6 - 5.2) = 1 - (-6.8) = 7.8$
>
> Para $x_2$:
>
> $\beta_0 = y_2 - \beta^T x_2 = 1 - ([-1.6, -2.6] \cdot [2, 1]) = 1 - (-3.2 - 2.6) = 1 - (-5.8) = 6.8$
>
> Para $x_3$:
>
> $\beta_0 = y_3 - \beta^T x_3 = -1 - ([-1.6, -2.6] \cdot [4, 5]) = -1 - (-6.4 - 13) = -1 - (-19.4) = 18.4$
>
> Agora, calculamos a m√©dia:
>
> $\beta_0 = \frac{7.8 + 6.8 + 18.4}{3} = \frac{33}{3} = 11$
>
> Portanto, o valor do par√¢metro de *bias* $\beta_0$ √© 11.

**Lemma 2:** O par√¢metro de *bias* $\beta_0$ pode ser calculado atrav√©s dos vetores de suporte que est√£o exatamente sobre a margem, e a utiliza√ß√£o de uma m√©dia das solu√ß√µes obtidas a partir de cada um desses vetores aumenta a estabilidade do c√°lculo.

A demonstra√ß√£o desse lemma se baseia na an√°lise das condi√ß√µes de KKT e na rela√ß√£o entre os multiplicadores de Lagrange e a posi√ß√£o das amostras em rela√ß√£o √† margem. A condi√ß√£o de complementaridade garante que as amostras com $0 < \alpha_i < C$ est√£o exatamente sobre a margem, e s√£o portanto os pontos usados para calcular o par√¢metro $\beta_0$.

### Impacto do Par√¢metro $C$ na Solu√ß√£o

```mermaid
graph LR
 subgraph "Impact of Parameter C"
  direction TB
  A["Parameter C: Regularization"]
  A --> B("High C: Small Margin, Many SVs, Overfitting")
  A --> C("Low C: Large Margin, Few SVs, Generalization")
  B --> D("Complex Model")
  C --> E("Simple Model")
 end
```

O par√¢metro de regulariza√ß√£o **C** desempenha um papel crucial na determina√ß√£o da solu√ß√£o das SVMs, pois ele controla o compromisso entre a maximiza√ß√£o da margem e a toler√¢ncia a erros de classifica√ß√£o, como discutimos anteriormente [^12.2]. A escolha do valor de C impacta o n√∫mero de vetores de suporte, a complexidade do modelo e sua capacidade de generaliza√ß√£o.

*   **Valores Altos de C:** Quando $C$ √© alto, o termo de penalidade por viola√ß√µes da margem se torna mais forte. Nesse caso, a SVM tende a construir um modelo mais complexo, com uma margem menor e com um maior n√∫mero de vetores de suporte, inclusive, um n√∫mero maior de vetores de suporte que violam a margem. O modelo se ajusta mais aos dados de treinamento e √© mais propenso ao *overfitting*.

*   **Valores Baixos de C:** Quando $C$ √© baixo, o modelo permite que mais pontos violem a margem, e busca uma solu√ß√£o com uma margem maior, mesmo que isso signifique a classifica√ß√£o incorreta de alguns pontos. Nesses casos, o modelo √© mais simples, com menos vetores de suporte e mais tolerante a *outliers*. O modelo tamb√©m √© menos propenso ao *overfitting* e geralmente apresenta melhor capacidade de generalizar para novos dados.

> üí° **Exemplo Num√©rico:**
>
> Imagine que temos um conjunto de dados com algumas amostras que se sobrep√µem.
>
> **C alto (ex: C=100):**  Com um C alto, o modelo tentar√° classificar o m√°ximo de amostras corretamente, mesmo que isso resulte em uma margem pequena e muitos vetores de suporte, alguns deles violando a margem. Isso pode levar a um modelo que se ajusta muito bem aos dados de treino, mas que generaliza mal para novos dados (overfitting). Nesse cen√°rio, a fronteira de decis√£o poder√° ter uma forma irregular, seguindo de perto os dados de treino.
>
> **C baixo (ex: C=0.1):** Com um C baixo, o modelo prioriza uma margem grande e permite que alguns pontos sejam classificados incorretamente ou que violem a margem. Isso leva a um modelo mais simples, com menos vetores de suporte, e uma fronteira de decis√£o mais suave. O modelo √© mais tolerante a *outliers* e tem maior chance de generalizar bem para novos dados. Aqui, a fronteira de decis√£o ser√° mais regular e menos influenciada por amostras individuais.
>
> Para visualizar o efeito de C, considere o seguinte cen√°rio simplificado em 2D:
>
> ```mermaid
> graph LR
>     A[Dados com sobreposi√ß√£o] --> B(C alto: Margem pequena, muitos SVs, overfitting);
>     A --> C(C baixo: Margem grande, poucos SVs, generaliza√ß√£o);
>     B -->|Fronteira de decis√£o irregular| D(Modelo complexo);
>     C -->|Fronteira de decis√£o suave| E(Modelo simples);
> ```
>
> O gr√°fico acima ilustra a ideia de como a escolha de C afeta a complexidade do modelo e a forma da fronteira de decis√£o.
>
> | C Value | Margin Size | Number of Support Vectors | Model Complexity | Risk of Overfitting |
> | -------- | ----------- | ------------------------- | ----------------- | -------------------- |
> | High     | Small       | High                      | High              | High                 |
> | Low      | Large       | Low                       | Low               | Low                  |

```mermaid
graph LR
 subgraph "Parameter C and Model Characteristics"
  direction TB
  A["C Value"]
  A --> B("High C")
  A --> C("Low C")
  B --> D("Small Margin")
  B --> E("Many Support Vectors")
  B --> F("High Model Complexity")
   B --> G("High Overfitting Risk")
  C --> H("Large Margin")
    C --> I("Few Support Vectors")
   C --> J("Low Model Complexity")
   C --> K("Low Overfitting Risk")
 end
```

A rela√ß√£o entre $C$ e a localiza√ß√£o dos vetores de suporte tamb√©m √© importante. Quando $C$ √© alto, muitos vetores de suporte estar√£o localizados dentro da margem ou classificados erroneamente, enquanto quando $C$ √© baixo, a maioria dos vetores de suporte estar√° localizada sobre a margem.

A escolha do valor adequado para o par√¢metro $C$ √© um passo crucial no treinamento de uma SVM, e √© frequentemente feita atrav√©s de t√©cnicas de valida√ß√£o cruzada. A valida√ß√£o cruzada permite avaliar o desempenho do modelo com diferentes valores de $C$ em um conjunto de dados de valida√ß√£o e escolher o valor que maximiza o desempenho no conjunto de dados n√£o visto.

**Lemma 3:** O par√¢metro $C$ controla a complexidade do modelo SVM, o n√∫mero de vetores de suporte e a toler√¢ncia a erros de classifica√ß√£o, e a escolha apropriada de $C$ depende do equil√≠brio entre vi√©s e vari√¢ncia e da necessidade de generaliza√ß√£o do modelo.

A demonstra√ß√£o desse lemma se baseia na an√°lise da fun√ß√£o de custo da SVM e como o par√¢metro C afeta o valor dos multiplicadores de Lagrange e, consequentemente, a determina√ß√£o dos vetores de suporte e da fronteira de decis√£o.

### Conclus√£o

Neste cap√≠tulo, exploramos em detalhes o processo de c√°lculo dos par√¢metros dos **vetores de suporte** e o papel do par√¢metro de regulariza√ß√£o **C** na solu√ß√£o das **Support Vector Machines (SVMs)**. Vimos como o vetor $\beta$, que define a orienta√ß√£o do hiperplano separador, √© calculado a partir de uma combina√ß√£o linear dos vetores de *features* dos vetores de suporte, ponderados pelos multiplicadores de Lagrange e os r√≥tulos das classes.

Analisamos tamb√©m o c√°lculo do par√¢metro de *bias* $\beta_0$, que √© obtido atrav√©s dos vetores de suporte que est√£o exatamente sobre a margem, e como a m√©dia dessas estimativas aumenta a estabilidade do c√°lculo. Discutimos o impacto crucial do par√¢metro $C$ na complexidade do modelo, no n√∫mero de vetores de suporte e na toler√¢ncia a erros de classifica√ß√£o.

A compreens√£o desses conceitos √© fundamental para a aplica√ß√£o pr√°tica das SVMs e para a escolha apropriada dos par√¢metros do modelo. Os vetores de suporte, com seu papel central na defini√ß√£o da fronteira de decis√£o, e o par√¢metro $C$, com sua influ√™ncia na complexidade do modelo, s√£o elementos chave para a constru√ß√£o de modelos robustos e com boa capacidade de generaliza√ß√£o.

### Footnotes

[^12.1]: "In this chapter we describe generalizations of linear decision boundaries for classification. Optimal separating hyperplanes are introduced in Chapter 4 for the case when two classes are linearly separable. Here we cover extensions to the nonseparable case, where the classes overlap. These techniques are then generalized to what is known as the support vector machine, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.2]: "In Chapter 4 we discussed a technique for constructing an optimal separating hyperplane between two perfectly separated classes. We review this and generalize to the nonseparable case, where the classes may not be separable by a linear boundary." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*
