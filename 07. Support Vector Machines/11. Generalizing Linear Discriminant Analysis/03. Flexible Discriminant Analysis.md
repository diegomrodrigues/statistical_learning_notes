Okay, let's enhance the text with practical numerical examples where appropriate, following your guidelines.

## T√≠tulo: An√°lise Discriminante Flex√≠vel (FDA): Uma Generaliza√ß√£o da LDA para Fronteiras N√£o Lineares

```mermaid
graph LR
    subgraph "FDA vs LDA"
        direction TB
        A["Linear Discriminant Analysis (LDA)"]
        B["Flexible Discriminant Analysis (FDA)"]
        A --> C["Linear Decision Boundaries"]
        B --> D["Non-Linear Decision Boundaries"]
        C --> E["Limitations with Complex Data"]
        D --> F["Handles Complex Data Effectively"]
    end
```

### Introdu√ß√£o

A **An√°lise Discriminante Linear (LDA)**, apesar de sua simplicidade e efici√™ncia, apresenta limita√ß√µes na modelagem de problemas de classifica√ß√£o onde a rela√ß√£o entre as *features* e as classes n√£o √© linear, ou onde as classes apresentam distribui√ß√µes n√£o gaussianas ou com covari√¢ncias distintas. A **An√°lise Discriminante Flex√≠vel (FDA)** surge como uma generaliza√ß√£o da LDA, que visa superar essas limita√ß√µes atrav√©s da utiliza√ß√£o de **regress√£o flex√≠vel** para modelar as rela√ß√µes entre as *features* e as classes.

Neste cap√≠tulo, exploraremos em detalhe a formula√ß√£o da FDA e como ela se diferencia da LDA, com foco em como a utiliza√ß√£o da regress√£o flex√≠vel permite construir fronteiras de decis√£o n√£o lineares e lidar com dados complexos. Analisaremos os passos do algoritmo FDA, incluindo a codifica√ß√£o das classes, a regress√£o n√£o param√©trica, o c√°lculo de *scores* √≥timos e o uso desses *scores* na classifica√ß√£o. Discutiremos tamb√©m as vantagens e desvantagens da FDA em rela√ß√£o √† LDA e como a FDA se relaciona com outras t√©cnicas de aprendizado de m√°quina, incluindo as SVMs.

A compreens√£o da formula√ß√£o da FDA e de como ela se diferencia da LDA √© fundamental para a escolha apropriada do m√©todo de classifica√ß√£o em diferentes cen√°rios, permitindo construir modelos mais robustos e com melhor capacidade de generaliza√ß√£o em dados complexos.

### Formula√ß√£o da An√°lise Discriminante Flex√≠vel (FDA)

**Conceito 1: A LDA como Regress√£o Linear**

Uma das ideias centrais da FDA √© reformular o problema da LDA como um problema de regress√£o linear. Em LDA, o objetivo √© encontrar uma proje√ß√£o linear que maximize a separa√ß√£o entre as classes, e isso pode ser obtido de forma equivalente atrav√©s da regress√£o linear nas vari√°veis indicadoras da classe, como discutido em cap√≠tulos anteriores.

No entanto, na FDA, a abordagem √© generalizada ao utilizar uma regress√£o linear em vari√°veis transformadas das classes. A FDA busca encontrar uma fun√ß√£o $\theta$ que atribui scores √†s classes, de forma que esses scores possam ser preditos por regress√£o linear no espa√ßo das *features*.

Formalmente, suponha que temos $K$ classes, e $\theta$ √© uma fun√ß√£o que atribui um score a cada classe ($ \theta : G \rightarrow \mathbb{R}$), onde $G = \{1,..., K \}$. A FDA busca encontrar uma fun√ß√£o $\eta(x)$,  que seja uma regress√£o de $\theta(g)$ sobre os dados $x$, minimizando o seguinte crit√©rio de otimiza√ß√£o:

$$ \min_{\beta, \theta} \sum_{i=1}^N  [\theta(g_i) - \eta(x_i)]^2 $$

onde $g_i$ √© a classe da $i$-√©sima amostra, e $\eta(x_i)$ √© uma fun√ß√£o de regress√£o flex√≠vel dos dados $x_i$.

> üí° **Exemplo Num√©rico:**
> Imagine um cen√°rio de classifica√ß√£o com 3 classes (K=3) e um conjunto de dados com duas *features* ($x_1$ e $x_2$). As classes s√£o codificadas usando vari√°veis indicadoras, como descrito no texto. A fun√ß√£o $\theta$ atribui scores a cada classe, por exemplo, $\theta(1) = 0.2$, $\theta(2) = 0.5$, e $\theta(3) = 0.8$. O objetivo da FDA √© encontrar uma fun√ß√£o $\eta(x)$ (uma regress√£o sobre as *features*) que se aproxime desses scores. Em vez de tentar ajustar uma reta (como na LDA), a FDA pode usar uma fun√ß√£o n√£o linear, como um polin√¥mio ou um *spline*. Por exemplo, $\eta(x) = 0.1 + 0.3x_1 + 0.2x_2 + 0.1x_1^2 -0.05x_2^2$ poderia ser uma fun√ß√£o de regress√£o n√£o linear encontrada pelo FDA. O objetivo √© minimizar o erro quadr√°tico entre os scores $\theta(g_i)$ e as predi√ß√µes $\eta(x_i)$ para todas as amostras.

**Lemma 1:** A FDA reformula o problema da LDA como um problema de regress√£o linear, que busca encontrar scores para as classes que podem ser previstos linearmente a partir das *features*.

```mermaid
graph LR
    subgraph "LDA as Regression"
        direction TB
        A["LDA: Find Linear Projection"]
        B["FDA: Reformulate as Regression"]
        C["Minimize: Sum of Squared Errors"]
        D["Target: Predict Class Scores"]
        A --> B
        B --> C
        C --> D
    end
```

A demonstra√ß√£o desse lemma se baseia na an√°lise da formula√ß√£o matem√°tica da FDA, onde o objetivo √© encontrar fun√ß√µes que associem um valor √†s classes que possam ser preditas atrav√©s da regress√£o sobre as *features*, transformando o problema da LDA em um problema de regress√£o.

**Conceito 2: Regress√£o N√£o Param√©trica e a Flexibilidade da FDA**

Ao contr√°rio da LDA, que utiliza proje√ß√µes lineares no espa√ßo original de *features*, a FDA utiliza **regress√£o n√£o param√©trica** para modelar a rela√ß√£o entre as *features* e os *scores* das classes. A regress√£o n√£o param√©trica oferece maior flexibilidade na modelagem, permitindo construir fun√ß√µes de regress√£o n√£o lineares que se adaptam √† estrutura dos dados, e, consequentemente, a constru√ß√£o de fun√ß√µes de decis√£o que n√£o s√£o lineares.

Ao usar a regress√£o n√£o param√©trica para encontrar $\eta(x)$, a FDA n√£o se limita a proje√ß√µes lineares, e permite que o modelo capture as rela√ß√µes n√£o lineares entre as *features* e as classes. A escolha do m√©todo de regress√£o n√£o param√©trica √© um passo crucial na aplica√ß√£o da FDA, e diferentes m√©todos podem ser utilizados, como *splines*, modelos aditivos ou MARS.

> üí° **Exemplo Num√©rico:**
> Imagine que temos duas classes e uma *feature* ($x$) que tem uma rela√ß√£o n√£o linear com as classes. A LDA tentaria encontrar uma linha reta para separar as classes, o que seria ineficaz. No entanto, a FDA, ao usar uma regress√£o n√£o param√©trica, pode ajustar uma curva que separe melhor as classes. Por exemplo, se os dados da classe 1 est√£o centrados em torno de $x = 2$ e os dados da classe 2 est√£o centrados em torno de $x = 5$, com alguma sobreposi√ß√£o, a regress√£o n√£o param√©trica poderia aprender uma fun√ß√£o $\eta(x)$ que tem um valor baixo para valores de x pr√≥ximos de 2, e um valor alto para valores de x pr√≥ximos de 5, com uma transi√ß√£o suave entre eles. Uma fun√ß√£o *spline* c√∫bica seria capaz de modelar essa rela√ß√£o.

**Corol√°rio 1:** A utiliza√ß√£o de regress√£o n√£o param√©trica permite que a FDA construa fronteiras de decis√£o n√£o lineares, superando a limita√ß√£o da LDA de impor uma fronteira de decis√£o linear.

```mermaid
graph LR
    subgraph "Non-Parametric Regression"
        direction LR
        A["LDA: Linear Projections"] --> B["Linear Decision Boundary"]
        C["FDA: Non-Parametric Regression"] --> D["Non-Linear Decision Boundary"]
        D --> E["Adapts to Complex Relationships"]
    end
```

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise da formula√ß√£o da FDA, onde a utiliza√ß√£o de regress√£o n√£o param√©trica permite que a fun√ß√£o $\eta(x)$ seja n√£o linear, o que leva √† constru√ß√£o de fronteiras de decis√£o n√£o lineares.

### O Algoritmo FDA: Etapas do Processo

```mermaid
graph TD
    subgraph "FDA Algorithm"
        direction TB
        A["1. Class Encoding: Indicator Variables"]
        B["2. Non-Parametric Regression: Œ∑(x)"]
        C["3. Optimal Score Calculation: Œ∏(g)"]
        D["4. Classification: Using scores and regression"]
        A --> B
        B --> C
        C --> D
    end
```

O algoritmo da **An√°lise Discriminante Flex√≠vel (FDA)** pode ser resumido nas seguintes etapas:

1.  **Codifica√ß√£o das Classes:** As classes s√£o codificadas utilizando vari√°veis indicadoras. Se temos $K$ classes, criamos $K$ vari√°veis indicadoras, onde $Y_{ik} = 1$ se a $i$-√©sima amostra pertence √† classe $k$ e $Y_{ik} = 0$ caso contr√°rio.
2.  **Regress√£o N√£o Param√©trica:** Para cada classe, ajustamos uma fun√ß√£o de regress√£o n√£o param√©trica, $\eta_l(x)$, utilizando os dados de treinamento. Essas fun√ß√µes de regress√£o modelam a rela√ß√£o entre as *features* e os scores das classes. Podemos utilizar um modelo de regress√£o linear (como na LDA) ou, como √© comum na FDA, um modelo mais flex√≠vel como modelos aditivos, splines ou MARS.
3.  **C√°lculo dos Scores √ìtimos:** Calculamos os *scores* √≥timos $\theta_l(g)$, para $l = 1, ..., L$, onde $L$ √© o n√∫mero de scores calculados (que pode ser igual ou menor que $K-1$). Esses *scores* s√£o obtidos atrav√©s da decomposi√ß√£o espectral da matriz de respostas ajustadas. Os *scores* √≥timos s√£o a solu√ß√£o para o problema de otimiza√ß√£o que definimos anteriormente, onde buscamos valores que podem ser bem modelados a partir da regress√£o feita no passo anterior.
4.  **Classifica√ß√£o:** Para classificar uma nova amostra, calculamos os valores das fun√ß√µes de regress√£o para cada classe, $\eta_l(x)$, e utilizamos os *scores* √≥timos para definir o novo espa√ßo projetado e classificamos as amostras com base na proximidade ao centroide das classes nesse espa√ßo projetado.

> üí° **Exemplo Num√©rico:**
> Vamos detalhar as etapas com um exemplo. Suponha que temos 3 classes (K=3) e duas *features* ($x_1$ e $x_2$).
>
> **Etapa 1: Codifica√ß√£o das Classes:**
>
> | Amostra | Classe | $Y_1$ | $Y_2$ | $Y_3$ |
> | ------- | ------ | ----- | ----- | ----- |
> | 1       | 1      | 1     | 0     | 0     |
> | 2       | 2      | 0     | 1     | 0     |
> | 3       | 3      | 0     | 0     | 1     |
> | 4       | 1      | 1     | 0     | 0     |
> | 5       | 2      | 0     | 1     | 0     |
>
> **Etapa 2: Regress√£o N√£o Param√©trica:**
> Suponha que ajustamos fun√ß√µes de regress√£o n√£o param√©trica para cada classe usando *splines* c√∫bicos:
>
> $\eta_1(x) = 0.1 + 0.5x_1 - 0.2x_2 + 0.1x_1^2 - 0.05x_2^2$ (para a classe 1)
> $\eta_2(x) = 0.3 - 0.1x_1 + 0.4x_2 - 0.05x_1^2 + 0.1x_2^2$ (para a classe 2)
> $\eta_3(x) = 0.2 + 0.2x_1 + 0.3x_2 + 0.02x_1^2 - 0.01x_2^2$ (para a classe 3)
>
> **Etapa 3: C√°lculo dos Scores √ìtimos:**
> Suponha que a decomposi√ß√£o espectral nos d√° dois *scores* (L=2) representados por $\theta_1$ e $\theta_2$. Os valores desses *scores* s√£o calculados de forma a maximizar a separa√ß√£o das classes no espa√ßo transformado. Esses scores s√£o, por exemplo:
>
> $\theta_1(1) = 0.8$, $\theta_1(2) = -0.5$, $\theta_1(3) = -0.3$
> $\theta_2(1) = -0.2$, $\theta_2(2) = 0.7$, $\theta_2(3) = -0.5$
>
> **Etapa 4: Classifica√ß√£o:**
> Para classificar uma nova amostra com $x_1 = 1$ e $x_2 = 2$, calculamos:
>
> $\eta_1(1, 2) = 0.1 + 0.5(1) - 0.2(2) + 0.1(1)^2 - 0.05(2)^2 = 0.1 + 0.5 - 0.4 + 0.1 - 0.2 = 0.0$
> $\eta_2(1, 2) = 0.3 - 0.1(1) + 0.4(2) - 0.05(1)^2 + 0.1(2)^2 = 0.3 - 0.1 + 0.8 - 0.05 + 0.4 = 1.35$
> $\eta_3(1, 2) = 0.2 + 0.2(1) + 0.3(2) + 0.02(1)^2 - 0.01(2)^2 = 0.2 + 0.2 + 0.6 + 0.02 - 0.04 = 0.98$
>
> Os scores projetados para a amostra s√£o:
>
> $score_1 = 0.0 * 0.8 + 1.35 * (-0.5) + 0.98 * (-0.3) = -0.97$
> $score_2 = 0.0 * (-0.2) + 1.35 * 0.7 + 0.98 * (-0.5) = 0.455$
>
> A amostra √© classificada com base na proximidade aos centroides das classes no espa√ßo transformado definido pelos scores.

A escolha do tipo de regress√£o n√£o param√©trica e do n√∫mero de *scores* ($L$) afetam o desempenho do modelo. A valida√ß√£o cruzada √© utilizada para escolher os valores desses par√¢metros de forma a obter o melhor desempenho.

**Lemma 4:** O algoritmo FDA envolve a codifica√ß√£o das classes, a utiliza√ß√£o de regress√£o n√£o param√©trica para construir as fun√ß√µes de regress√£o, e o c√°lculo de *scores* √≥timos, que s√£o utilizados para a classifica√ß√£o.

A demonstra√ß√£o desse lemma se baseia na an√°lise dos passos do algoritmo FDA e como cada um dos passos contribui para a constru√ß√£o do modelo de classifica√ß√£o.

### Vantagens da FDA em Rela√ß√£o √† LDA

```mermaid
graph LR
    subgraph "FDA Advantages"
        direction TB
        A["FDA"] --> B["Non-Linear Decision Boundaries"]
        A --> C["Flexibility in Modeling"]
        A --> D["Handles Non-Gaussian Distributions"]
        E["LDA"] --> F["Linear Decision Boundaries"]
        B --> G["Better for Complex Data"]
        F --> H["Limitations with Complex Data"]
    end
```

A **An√°lise Discriminante Flex√≠vel (FDA)** oferece diversas vantagens em rela√ß√£o √† **An√°lise Discriminante Linear (LDA)**, especialmente quando os dados apresentam padr√µes de separa√ß√£o n√£o linear ou distribui√ß√µes n√£o gaussianas.

1.  **Fronteiras de Decis√£o N√£o Lineares:** A principal vantagem da FDA √© a capacidade de construir fronteiras de decis√£o n√£o lineares, atrav√©s da utiliza√ß√£o de modelos de regress√£o n√£o param√©tricos. Ao contr√°rio da LDA, que gera fronteiras lineares, a FDA pode se adaptar a conjuntos de dados com rela√ß√µes complexas entre as *features* e as classes.
2.  **Flexibilidade na Modelagem:** A FDA oferece maior flexibilidade na modelagem, pois permite a escolha de diferentes m√©todos de regress√£o n√£o param√©trica, o que permite ajustar o modelo √†s caracter√≠sticas espec√≠ficas dos dados. A escolha de um modelo de regress√£o espec√≠fico para cada classe ou a escolha do tipo de penaliza√ß√£o a ser utilizado tornam a FDA mais adapt√°vel a diferentes tipos de dados e problemas de classifica√ß√£o.
3.  **Lidar com Distribui√ß√µes N√£o Gaussianas:** A FDA n√£o assume que os dados seguem uma distribui√ß√£o gaussiana multivariada, o que a torna mais adequada para lidar com dados reais que frequentemente apresentam distribui√ß√µes n√£o gaussianas. A flexibilidade das fun√ß√µes de regress√£o n√£o param√©trica permite que a FDA se adapte a diferentes distribui√ß√µes das classes.

> üí° **Exemplo Num√©rico:**
> Imagine um conjunto de dados onde a classe 1 forma um c√≠rculo no centro do espa√ßo de *features*, e a classe 2 est√° ao redor desse c√≠rculo. A LDA tentaria encontrar uma linha reta para separar essas classes, o que seria imposs√≠vel. J√° a FDA, ao usar uma regress√£o n√£o param√©trica, poderia ajustar uma fun√ß√£o que modela essa forma circular, separando as classes de forma mais eficiente.
>
> Outro exemplo: suponha que temos um problema de classifica√ß√£o com duas *features* e duas classes, onde os dados da classe 1 t√™m uma distribui√ß√£o bimodal (dois picos) e os dados da classe 2 t√™m uma distribui√ß√£o gaussiana. A LDA, que assume que os dados seguem uma distribui√ß√£o gaussiana, ter√° dificuldades em modelar a distribui√ß√£o da classe 1. A FDA, por outro lado, pode usar modelos de regress√£o n√£o param√©tricos que se adaptam a essas distribui√ß√µes n√£o gaussianas, melhorando o desempenho da classifica√ß√£o.

As vantagens da FDA em rela√ß√£o √† LDA s√£o particularmente evidentes em problemas com dados n√£o lineares e de alta dimensionalidade, onde a LDA apresenta limita√ß√µes importantes. A FDA, no entanto, continua a ser um modelo linear sobre as vari√°veis transformadas, e, por isso, √© mais f√°cil de interpretar do que modelos totalmente n√£o lineares.

**Corol√°rio 2:** A FDA supera as principais limita√ß√µes da LDA atrav√©s da utiliza√ß√£o de proje√ß√µes n√£o lineares e do uso de modelos de regress√£o flex√≠veis, que lhe permitem modelar rela√ß√µes complexas entre as *features* e as classes.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise da formula√ß√£o da FDA e como a utiliza√ß√£o de regress√£o n√£o param√©trica e a proje√ß√£o nos *scores* das classes leva a um modelo que n√£o apresenta as limita√ß√µes da LDA.

### Conex√£o da FDA com Outras Abordagens de Aprendizado

```mermaid
graph LR
    subgraph "FDA Connections"
        direction TB
        A["Flexible Discriminant Analysis (FDA)"]
        A --> B["Non-Parametric Regression Methods"]
        A --> C["Support Vector Machines (SVMs)"]
        A --> D["Kernel PCA (KPCA)"]
    end
```

A **An√°lise Discriminante Flex√≠vel (FDA)** pode ser vista como uma abordagem que combina conceitos de **regress√£o n√£o param√©trica** com a ideia de encontrar proje√ß√µes discriminantes, utilizada na **An√°lise Discriminante Linear (LDA)**. A FDA se relaciona com outras abordagens de aprendizado de m√°quina, como:

1.  **M√©todos de Regress√£o N√£o Param√©trica:** A FDA utiliza modelos de regress√£o n√£o param√©tricos, como *splines*, modelos aditivos ou MARS, para modelar a rela√ß√£o entre as *features* e os scores das classes. A escolha do modelo de regress√£o apropriado permite a FDA se adaptar √†s caracter√≠sticas dos dados e modelar rela√ß√µes n√£o lineares com flexibilidade.

2.  **Support Vector Machines (SVMs):** A FDA e as SVMs podem ser vistas como abordagens para construir modelos n√£o lineares. Enquanto as SVMs utilizam o *kernel trick* para transformar os dados em um espa√ßo de *features* de alta dimens√£o, a FDA utiliza regress√£o n√£o param√©trica para modelar a rela√ß√£o entre as *features* e as classes. Ambos os m√©todos levam a modelos com alta capacidade de generaliza√ß√£o e se baseiam em conceitos de otimiza√ß√£o.

3.  **An√°lise de Componentes Principais (PCA) com Kernel (KPCA):** Assim como a SVD √© a base para o m√©todo PCA, a FDA pode utilizar um processo similar para realizar a redu√ß√£o de dimensionalidade de forma n√£o linear, ao projetar os dados nos *scores* das classes, que levam a um melhor poder de separa√ß√£o das classes.

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar a conex√£o com SVMs, podemos pensar no caso em que uma SVM usa um kernel radial (RBF) para mapear os dados para um espa√ßo de alta dimens√£o, onde uma separa√ß√£o linear √© poss√≠vel. A FDA, ao usar uma regress√£o n√£o param√©trica, tenta encontrar diretamente uma fun√ß√£o n√£o linear que separe as classes no espa√ßo original, sem a necessidade de mapeamento para um espa√ßo de alta dimens√£o. Em ambos os casos, o objetivo √© encontrar uma fronteira de decis√£o n√£o linear.
>
> Para a conex√£o com KPCA, podemos imaginar que, em vez de usar uma transforma√ß√£o linear para reduzir a dimensionalidade (como no PCA), o KPCA usa um kernel para mapear os dados para um espa√ßo de alta dimens√£o e depois realiza o PCA nesse espa√ßo. A FDA, de forma similar, projeta os dados nos *scores* das classes, que podem ser vistos como uma redu√ß√£o de dimensionalidade n√£o linear.

A conex√£o da FDA com outras abordagens de aprendizado de m√°quina ilustra como a FDA se situa no contexto geral do aprendizado n√£o linear e como suas ideias podem ser integradas com outras t√©cnicas para construir modelos mais poderosos e flex√≠veis.

**Corol√°rio 3:** A FDA representa uma abordagem que combina conceitos de regress√£o n√£o param√©trica com a ideia de encontrar proje√ß√µes discriminantes, se relacionando com outras abordagens de aprendizado de m√°quina como as SVMs, o PCA com kernel, e outros m√©todos de regress√£o n√£o param√©trica.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise da formula√ß√£o da FDA e como ela pode ser vista como uma generaliza√ß√£o da LDA que incorpora conceitos de regress√£o n√£o param√©trica e m√©todos de kernel, o que permite entender como a FDA se relaciona com outras t√©cnicas de aprendizado.

### Conclus√£o

Neste cap√≠tulo, exploramos em detalhe a **An√°lise Discriminante Flex√≠vel (FDA)**, uma generaliza√ß√£o da LDA que visa superar suas limita√ß√µes atrav√©s da utiliza√ß√£o da **regress√£o flex√≠vel** para modelar a rela√ß√£o entre as *features* e as classes. Vimos como a FDA utiliza modelos de regress√£o n√£o param√©tricos para construir fronteiras de decis√£o n√£o lineares e como o c√°lculo de *scores* √≥timos permite que a FDA combine a flexibilidade da regress√£o n√£o param√©trica com a busca por proje√ß√µes discriminantes, que levam a uma melhor capacidade de separa√ß√£o das classes.

Analisamos as vantagens da FDA em rela√ß√£o √† LDA, destacando sua capacidade de lidar com dados n√£o lineares e sua flexibilidade na modelagem. Discutimos tamb√©m como a FDA se relaciona com outras abordagens de aprendizado de m√°quina, como SVMs e m√©todos de regress√£o n√£o param√©trica, o que nos permite entender seu papel no contexto geral do aprendizado n√£o linear. A capacidade de modelar n√£o linearidades, juntamente com sua maior capacidade de adapta√ß√£o a dados complexos faz com que a FDA se torne uma alternativa interessante √† LDA.

### Footnotes

[^12.1]: "In this chapter we describe generalizations of linear decision boundaries for classification. Optimal separating hyperplanes are introduced in Chapter 4 for the case when two classes are linearly separable. Here we cover extensions to the nonseparable case, where the classes overlap. These techniques are then generalized to what is known as the support vector machine, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.2]: "In Chapter 4 we discussed a technique for constructing an optimal separating hyperplane between two perfectly separated classes. We review this and generalize to the nonseparable case, where the classes may not be separable by a linear boundary." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.3]: "The support vector machine classifier is an extension of this idea, where the dimension of the enlarged space is allowed to get very large, infinite in some cases. It might seem that the computations would become prohibitive. It would also seem that with sufficient basis functions, the data would be separable, and overfitting would occur. We first show how the SVM technology deals with these issues. We then see that in fact the SVM classifier is solving a function-fitting problem using a particular criterion and form of regularization, and is part of a much bigger class of problems that includes the smoothing splines of Chapter 5." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.4]: "In the remainder of this chapter we describe a class of techniques that attend to all these issues by generalizing the LDA model. This is achieved largely by three different ideas." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*
[^12.5]:  "In this section we describe a method for performing LDA using linear re-gression on derived responses." *(Trecho de "Support Vector Machines and Flexible Discriminants")*
