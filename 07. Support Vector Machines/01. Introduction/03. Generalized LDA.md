## TÃ­tulo: GeneralizaÃ§Ãµes da AnÃ¡lise Discriminante Linear: FDA, PDA e MDA

```mermaid
graph LR
    subgraph "Generalizations of LDA"
        direction TB
        A["Linear Discriminant Analysis (LDA)"]
        B["Flexible Discriminant Analysis (FDA)"]
        C["Penalized Discriminant Analysis (PDA)"]
        D["Mixture Discriminant Analysis (MDA)"]
        A --> B
        A --> C
        A --> D
    end
```

### IntroduÃ§Ã£o

No capÃ­tulo anterior, discutimos as **Support Vector Machines (SVMs)** como uma poderosa ferramenta para construÃ§Ã£o de fronteiras de decisÃ£o nÃ£o lineares. Neste capÃ­tulo, voltamos nossa atenÃ§Ã£o para outra abordagem de generalizaÃ§Ã£o, focando na **AnÃ¡lise Discriminante Linear (LDA)** e suas extensÃµes: **AnÃ¡lise Discriminante FlexÃ­vel (FDA)**, **AnÃ¡lise Discriminante Penalizada (PDA)** e **AnÃ¡lise Discriminante por Misturas (MDA)**. O objetivo principal destas tÃ©cnicas Ã© superar as limitaÃ§Ãµes da LDA, que assume que as classes sÃ£o Gaussianas com uma matriz de covariÃ¢ncia comum, uma premissa que raramente se mantÃ©m em dados reais [^12.4].

A **LDA**, apesar de sua simplicidade e interpretabilidade, apresenta limitaÃ§Ãµes importantes em problemas de classificaÃ§Ã£o complexos. Uma das principais limitaÃ§Ãµes Ã© a rigidez de suas fronteiras de decisÃ£o, que sÃ£o lineares e, portanto, inadequadas para dados com padrÃµes de separaÃ§Ã£o nÃ£o lineares. AlÃ©m disso, a suposiÃ§Ã£o de uma covariÃ¢ncia comum para todas as classes pode levar a resultados subÃ³timos quando as classes apresentam estruturas de covariÃ¢ncia diferentes.

As generalizaÃ§Ãµes da LDA, por outro lado, procuram endereÃ§ar estas limitaÃ§Ãµes de diferentes maneiras. A **AnÃ¡lise Discriminante FlexÃ­vel (FDA)** generaliza a LDA atravÃ©s da aplicaÃ§Ã£o de regressÃ£o linear em respostas transformadas, o que permite modelar relaÃ§Ãµes nÃ£o lineares entre as *features* e as classes, como discutido em [^12.5]. A **AnÃ¡lise Discriminante Penalizada (PDA)** foca em lidar com alta dimensionalidade, introduzindo um termo de penalizaÃ§Ã£o para controlar a complexidade do modelo, especialmente em conjuntos de dados onde o nÃºmero de *features* Ã© muito maior do que o nÃºmero de amostras. JÃ¡ a **AnÃ¡lise Discriminante por Misturas (MDA)** busca superar a limitaÃ§Ã£o de representar cada classe por um Ãºnico protÃ³tipo, utilizando modelos de mistura Gaussianos para modelar a distribuiÃ§Ã£o de cada classe, permitindo assim que cada classe seja representada por mÃºltiplos protÃ³tipos, como abordado em [^12.4].

Neste capÃ­tulo, exploraremos em detalhe cada uma dessas abordagens, analisando suas motivaÃ§Ãµes, formulaÃ§Ãµes matemÃ¡ticas e como cada mÃ©todo contribui para aprimorar a capacidade de classificaÃ§Ã£o em cenÃ¡rios complexos. AlÃ©m disso, discutiremos a relaÃ§Ã£o entre estas tÃ©cnicas e outros mÃ©todos de aprendizado de mÃ¡quina, como a regressÃ£o linear e a regressÃ£o logÃ­stica.

### Conceitos Fundamentais: LimitaÃ§Ãµes da LDA e Abordagens de GeneralizaÃ§Ã£o

**Conceito 1: As LimitaÃ§Ãµes da AnÃ¡lise Discriminante Linear (LDA)**

A **AnÃ¡lise Discriminante Linear (LDA)** Ã© um mÃ©todo de classificaÃ§Ã£o que busca projetar os dados em um subespaÃ§o de menor dimensÃ£o, maximizando a separaÃ§Ã£o entre as classes e minimizando a variÃ¢ncia dentro de cada classe [^12.4]. A LDA assume que os dados de cada classe seguem uma distribuiÃ§Ã£o Gaussiana com uma matriz de covariÃ¢ncia comum. A funÃ§Ã£o discriminante da LDA Ã© linear, o que a torna um mÃ©todo simples e eficiente para dados com estruturas lineares de separabilidade.

No entanto, a LDA apresenta limitaÃ§Ãµes importantes, como:

1.  **Fronteiras de DecisÃ£o Lineares:** A LDA gera fronteiras de decisÃ£o lineares, o que pode ser inadequado para dados com padrÃµes de separaÃ§Ã£o nÃ£o lineares ou com classes sobrepostas.
2.  **SuposiÃ§Ã£o de Normalidade:** A LDA assume que os dados seguem uma distribuiÃ§Ã£o normal, o que pode nÃ£o ser vÃ¡lido para muitos conjuntos de dados reais.
3.  **CovariÃ¢ncia Comum:** A LDA assume que todas as classes compartilham a mesma matriz de covariÃ¢ncia, o que pode levar a modelos inadequados quando as classes apresentam estruturas de covariÃ¢ncia distintas.
4.  **NÃºmero Limitado de ProjeÃ§Ãµes:** A LDA limita o nÃºmero de projeÃ§Ãµes discriminantes ao nÃºmero de classes menos 1, o que pode ser insuficiente em problemas com muitas classes.

```mermaid
graph TB
    subgraph "LDA Limitations"
        direction TB
        A["Linear Decision Boundaries"]
        B["Assumption of Normality"]
        C["Common Covariance Assumption"]
        D["Limited Number of Projections (K-1)"]
        A --> B
        B --> C
        C --> D
    end
```

**Lemma 1:** A suposiÃ§Ã£o de normalidade multivariada e covariÃ¢ncia comum em LDA sÃ£o restritivas e podem levar a resultados subÃ³timos quando as classes nÃ£o seguem estas distribuiÃ§Ãµes.

A demonstraÃ§Ã£o desse lemma envolve a anÃ¡lise da natureza da funÃ§Ã£o discriminante da LDA, que Ã© derivada da densidade gaussiana e da matriz de covariÃ¢ncia comum. Quando as classes nÃ£o seguem essa distribuiÃ§Ã£o ou compartilham covariÃ¢ncias distintas, a funÃ§Ã£o discriminante da LDA nÃ£o representa adequadamente as probabilidades *a posteriori*, resultando em decisÃµes de classificaÃ§Ã£o subÃ³timas.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um problema de classificaÃ§Ã£o com duas classes, onde os dados da classe 1 sÃ£o gerados por uma distribuiÃ§Ã£o normal com mÃ©dia $\mu_1 = [1, 1]^T$ e matriz de covariÃ¢ncia $\Sigma_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$, e os dados da classe 2 sÃ£o gerados por uma distribuiÃ§Ã£o normal com mÃ©dia $\mu_2 = [3, 3]^T$ e matriz de covariÃ¢ncia $\Sigma_2 = \begin{bmatrix} 2 & 0.5 \\ 0.5 & 2 \end{bmatrix}$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>
> # Dados de exemplo
> np.random.seed(42)
> mean1 = [1, 1]
> cov1 = [[1, 0], [0, 1]]
> mean2 = [3, 3]
> cov2 = [[2, 0.5], [0.5, 2]]
> X1 = np.random.multivariate_normal(mean1, cov1, 100)
> X2 = np.random.multivariate_normal(mean2, cov2, 100)
> X = np.concatenate((X1, X2))
> y = np.array([0] * 100 + [1] * 100)
>
> # Aplicar LDA
> lda = LinearDiscriminantAnalysis()
> lda.fit(X, y)
>
> # Plotar os dados e a fronteira de decisÃ£o
> x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
> y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
> xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
> Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])
> Z = Z.reshape(xx.shape)
>
> plt.contourf(xx, yy, Z, alpha=0.3)
> plt.scatter(X1[:, 0], X1[:, 1], c='blue', label='Classe 1')
> plt.scatter(X2[:, 0], X2[:, 1], c='red', label='Classe 2')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
> plt.title('LDA em Dados com CovariÃ¢ncias Diferentes')
> plt.legend()
> plt.show()
>
> print(f"Coeficientes LDA: {lda.coef_}")
> print(f"Intercepto LDA: {lda.intercept_}")
> ```
>
> Neste exemplo, as classes tÃªm covariÃ¢ncias diferentes, o que viola a suposiÃ§Ã£o da LDA. A fronteira de decisÃ£o obtida pela LDA Ã© linear, mas nÃ£o captura completamente a complexidade dos dados. A classe 2, devido Ã  sua maior variÃ¢ncia, Ã© mais espalhada e a fronteira linear da LDA nÃ£o separa as classes de forma ideal. Este cenÃ¡rio ilustra como a suposiÃ§Ã£o de covariÃ¢ncia comum da LDA pode levar a resultados subÃ³timos.
>
> A saÃ­da dos coeficientes e do intercepto da LDA permite visualizar a inclinaÃ§Ã£o e o posicionamento da fronteira de decisÃ£o linear no plano das *features*.

**Conceito 2: AnÃ¡lise Discriminante FlexÃ­vel (FDA)**

A **AnÃ¡lise Discriminante FlexÃ­vel (FDA)** generaliza a LDA atravÃ©s da utilizaÃ§Ã£o de regressÃ£o linear sobre respostas transformadas [^12.5]. A FDA busca encontrar uma funÃ§Ã£o $\theta$ que atribui scores Ã s classes, de tal forma que estes scores possam ser previstos por regressÃ£o linear. A motivaÃ§Ã£o por trÃ¡s da FDA Ã© encontrar um espaÃ§o onde as classes se separam linearmente, mas utilizando regressÃ£o flexÃ­vel sobre transformaÃ§Ãµes das classes. A FDA pode usar modelos de regressÃ£o mais flexÃ­veis do que a simples projeÃ§Ã£o linear da LDA, o que permite modelar relaÃ§Ãµes nÃ£o lineares entre as *features* e as classes.

```mermaid
graph LR
    subgraph "Flexible Discriminant Analysis (FDA)"
        direction LR
        A["Data Transformation"] --> B["Regression Model"]
        B --> C["Class Scores (Î¸)"]
        C --> D["Flexible Decision Boundary"]
    end
```

**CorolÃ¡rio 1:** A FDA pode usar modelos de regressÃ£o nÃ£o paramÃ©tricos para projetar as classes em um espaÃ§o onde a separaÃ§Ã£o linear Ã© maximizada, relaxando as restriÃ§Ãµes impostas pela LDA.

A demonstraÃ§Ã£o desse corolÃ¡rio se baseia na anÃ¡lise da formulaÃ§Ã£o da FDA, onde o problema de encontrar scores Ã³timos para cada classe Ã© transformado em um problema de regressÃ£o, com a possibilidade de usar modelos mais flexÃ­veis, como modelos aditivos, *splines* ou MARS, o que permite aproximar as classes por funÃ§Ãµes nÃ£o lineares.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um problema de classificaÃ§Ã£o binÃ¡ria com dados que seguem um padrÃ£o circular. A LDA nÃ£o conseguiria separar esses dados de forma eficaz, pois sua fronteira Ã© linear. A FDA, no entanto, pode usar um modelo de regressÃ£o nÃ£o paramÃ©trico, como um modelo aditivo com *splines*, para modelar a relaÃ§Ã£o entre as *features* e as classes.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
> from sklearn.preprocessing import PolynomialFeatures
> from patsy import dmatrix
>
> # Dados de exemplo (padrÃ£o circular)
> np.random.seed(42)
> n_samples = 200
> radius = np.random.uniform(0, 1, n_samples)
> angle = np.random.uniform(0, 2 * np.pi, n_samples)
> X = np.column_stack((radius * np.cos(angle), radius * np.sin(angle)))
> y = (radius > 0.5).astype(int)
>
> # Adicionar ruÃ­do
> X += np.random.normal(0, 0.1, X.shape)
>
> # Aplicar FDA com regressÃ£o polinomial
> poly = PolynomialFeatures(degree=3)
> X_poly = poly.fit_transform(X)
>
> model = LinearRegression()
> model.fit(X_poly, y)
>
> # Plotar a fronteira de decisÃ£o
> x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
> y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
> xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
> Z = model.predict(poly.transform(np.c_[xx.ravel(), yy.ravel()]))
> Z = Z.reshape(xx.shape)
>
> plt.contourf(xx, yy, Z, levels=[0.5], colors=['gray'], alpha=0.3)
> plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Classe 0')
> plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Classe 1')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
> plt.title('FDA com RegressÃ£o Polinomial')
> plt.legend()
> plt.show()
>
> print(f"Coeficientes do modelo FDA: {model.coef_}")
> print(f"Intercepto do modelo FDA: {model.intercept_}")
> ```
>
> Neste exemplo, a regressÃ£o polinomial, uma forma de regressÃ£o nÃ£o paramÃ©trica, Ã© usada para modelar a relaÃ§Ã£o entre as *features* e as classes. A fronteira de decisÃ£o resultante Ã© nÃ£o linear e consegue separar as classes com um padrÃ£o circular, algo que a LDA nÃ£o conseguiria. Os coeficientes mostram a importÃ¢ncia de cada termo polinomial na modelagem da fronteira de decisÃ£o.
>
> O uso de `patsy` para criar a matriz de design pode ser substituÃ­do por outras tÃ©cnicas de expansÃ£o de base, como *splines*, dependendo do modelo de regressÃ£o nÃ£o paramÃ©trico escolhido.

**Conceito 3: AnÃ¡lise Discriminante Penalizada (PDA)**

A **AnÃ¡lise Discriminante Penalizada (PDA)** busca lidar com problemas de alta dimensionalidade introduzindo um termo de penalizaÃ§Ã£o na funÃ§Ã£o de custo da LDA [^12.6]. Em problemas com um grande nÃºmero de *features* (alta dimensionalidade), a LDA pode sofrer com o problema do *overfitting*, o que leva a modelos com baixa capacidade de generalizaÃ§Ã£o. A PDA introduz um termo de penalizaÃ§Ã£o para controlar a complexidade do modelo e evitar a utilizaÃ§Ã£o de *features* redundantes ou irrelevantes. A penalizaÃ§Ã£o geralmente Ã© aplicada aos coeficientes do modelo, como em mÃ©todos de regressÃ£o regularizada, e a forma da penalizaÃ§Ã£o afeta a natureza da soluÃ§Ã£o, como o tipo de suavizaÃ§Ã£o da funÃ§Ã£o discriminante.

```mermaid
graph LR
    subgraph "Penalized Discriminant Analysis (PDA)"
        direction LR
        A["LDA Cost Function"] --> B["Penalty Term (Î»Î©)"]
        B --> C["Regularized Solution"]
        C --> D["Reduced Overfitting"]
        D --> E["Improved Generalization"]

    end
```

**CorolÃ¡rio 2:** A PDA usa penalizaÃ§Ãµes para obter modelos LDA mais estÃ¡veis e com menor variÃ¢ncia, especialmente em problemas de alta dimensionalidade.

A demonstraÃ§Ã£o desse corolÃ¡rio envolve a anÃ¡lise da funÃ§Ã£o de custo da PDA, que inclui um termo de penalizaÃ§Ã£o que restringe a complexidade do modelo. Ao adicionar essa penalizaÃ§Ã£o, os coeficientes do modelo sÃ£o reduzidos em magnitude, levando a funÃ§Ãµes discriminantes mais suaves e com maior capacidade de generalizaÃ§Ã£o em dados nÃ£o vistos.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um problema de classificaÃ§Ã£o com 100 *features*, onde apenas 10 sÃ£o realmente informativas e as outras 90 sÃ£o ruÃ­do. A LDA, sem regularizaÃ§Ã£o, pode superajustar os dados de treinamento, utilizando as 100 *features* para separar as classes, levando a um modelo com baixa capacidade de generalizaÃ§Ã£o. A PDA, com regularizaÃ§Ã£o $L_2$, penaliza os coeficientes das *features* nÃ£o informativas, levando a um modelo mais estÃ¡vel e com melhor desempenho em dados novos.
>
> ```python
> import numpy as np
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import accuracy_score
> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
> from sklearn.linear_model import RidgeClassifier
>
> # Gerar dados de exemplo com alta dimensionalidade
> np.random.seed(42)
> n_samples = 200
> n_informative_features = 10
> n_features = 100
>
> X = np.random.randn(n_samples, n_features)
> W = np.random.randn(n_features, 1)
>
> # Definir os coeficientes das features informativas
> W[0:n_informative_features] = np.random.randn(n_informative_features, 1) * 5
>
> # Gerar classes com base nas features informativas + ruÃ­do
> y = (X @ W + np.random.randn(n_samples, 1) > 0).astype(int).flatten()
>
> # Dividir em conjuntos de treino e teste
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # Aplicar LDA sem penalizaÃ§Ã£o
> lda = LinearDiscriminantAnalysis()
> lda.fit(X_train, y_train)
> y_pred_lda = lda.predict(X_test)
>
> # Aplicar PDA (Ridge) com penalizaÃ§Ã£o L2
> pda = RidgeClassifier(alpha=1.0) # alpha Ã© o parÃ¢metro de regularizaÃ§Ã£o lambda
> pda.fit(X_train, y_train)
> y_pred_pda = pda.predict(X_test)
>
> # Avaliar os modelos
> acc_lda = accuracy_score(y_test, y_pred_lda)
> acc_pda = accuracy_score(y_test, y_pred_pda)
>
> print(f"AcurÃ¡cia LDA: {acc_lda:.4f}")
> print(f"AcurÃ¡cia PDA (Ridge): {acc_pda:.4f}")
> print(f"Coeficientes LDA: {lda.coef_}")
> print(f"Coeficientes PDA: {pda.coef_}")
>
> # Comparar os coeficientes
> plt.figure(figsize=(10, 6))
> plt.plot(lda.coef_.flatten(), label='LDA')
> plt.plot(pda.coef_.flatten(), label='PDA (Ridge)')
> plt.xlabel('Feature Index')
> plt.ylabel('Coefficient Value')
> plt.title('ComparaÃ§Ã£o dos Coeficientes LDA vs PDA')
> plt.legend()
> plt.show()
> ```
>
> Neste exemplo, a PDA (implementada usando `RidgeClassifier`) demonstra melhor desempenho do que a LDA em um cenÃ¡rio de alta dimensionalidade com ruÃ­do. A regularizaÃ§Ã£o $L_2$ reduz a magnitude dos coeficientes das *features* nÃ£o informativas, resultando em um modelo mais generalizÃ¡vel. O grÃ¡fico compara os coeficientes dos dois modelos, mostrando como a PDA penaliza os coeficientes, tornando-os menores em valor absoluto. A acurÃ¡cia de teste Ã© um bom indicador da capacidade de generalizaÃ§Ã£o dos modelos.

**Conceito 4: AnÃ¡lise Discriminante por Misturas (MDA)**

A **AnÃ¡lise Discriminante por Misturas (MDA)** generaliza a LDA ao modelar a distribuiÃ§Ã£o de cada classe como uma mistura de gaussianas com uma matriz de covariÃ¢ncia comum. Em muitos cenÃ¡rios, representar uma classe com um Ãºnico centroide (como faz a LDA) Ã© insuficiente para capturar a complexidade da distribuiÃ§Ã£o. A MDA permite que cada classe seja representada por mÃºltiplos protÃ³tipos (centrÃ³ides), o que oferece maior flexibilidade na modelagem das classes e na construÃ§Ã£o das fronteiras de decisÃ£o. A MDA combina os conceitos de modelos de mistura gaussianas e LDA, e o ajuste dos parÃ¢metros Ã© tipicamente realizado utilizando o algoritmo EM, como abordado em [^12.4].

```mermaid
graph LR
    subgraph "Mixture Discriminant Analysis (MDA)"
        direction LR
        A["Single Gaussian (LDA)"] --> B["Single Prototype Per Class"]
        C["Gaussian Mixture Model (GMM)"] --> D["Multiple Prototypes Per Class"]
        B --> E["Limited Representation"]
        D --> F["Flexible Representation"]
        C--> A
    end
```

> âš ï¸ **Nota Importante**: A LDA assume que as classes sÃ£o Gaussianas com uma covariÃ¢ncia comum, enquanto a FDA usa modelos de regressÃ£o flexÃ­veis para modelar as relaÃ§Ãµes entre as *features* e as classes, como apontado em [^12.4] e [^12.5].

> â— **Ponto de AtenÃ§Ã£o**: A PDA aborda a questÃ£o da alta dimensionalidade adicionando um termo de penalizaÃ§Ã£o, evitando o *overfitting* e melhorando a generalizaÃ§Ã£o do modelo.

> âœ”ï¸ **Destaque**: A MDA generaliza a LDA ao modelar a distribuiÃ§Ã£o de cada classe como uma mistura de gaussianas, o que permite uma melhor adaptaÃ§Ã£o aos dados. **Baseado em [^12.7]**.

### AnÃ¡lise Discriminante FlexÃ­vel (FDA): FormulaÃ§Ã£o e ImplementaÃ§Ã£o

```mermaid
graph TB
 subgraph "FDA Data Flow"
  direction TB
    A["Class Encoding (Y)"]
    B["Non-parametric Regression Î·(x)"]
    C["Optimal Scores Î¸(g) (Spectral Decomposition)"]
    D["Classification"]
   A --> B
    B --> C
    C --> D
  end
```

A **AnÃ¡lise Discriminante FlexÃ­vel (FDA)**, como discutido anteriormente, generaliza a LDA atravÃ©s da utilizaÃ§Ã£o de regressÃ£o linear em respostas transformadas. O objetivo Ã© encontrar um conjunto de scores $\theta_l(g)$ para cada classe $g$, onde $l=1,\ldots,L$ e $L$ Ã© o nÃºmero de projeÃ§Ãµes discriminantes desejadas, de forma que a relaÃ§Ã£o entre as *features* e os scores seja modelada por funÃ§Ãµes de regressÃ£o flexÃ­veis, em vez de uma projeÃ§Ã£o linear direta, como na LDA. Formalmente, buscamos minimizar:

$$ ASR = \sum_{l=1}^L \sum_{i=1}^{N} [\theta_l(g_i) - \eta_l(x_i)]^2 $$

onde $\eta_l(x_i)$ sÃ£o as funÃ§Ãµes de regressÃ£o que dependem das *features* $x_i$ e que modelam os scores $\theta_l(g_i)$, e $ASR$ Ã© o *Average Squared Residual*.

Para construir o modelo FDA, realizamos os seguintes passos:

1.  **CodificaÃ§Ã£o das Classes:** Criamos uma matriz de respostas $Y$ com dimensÃµes $N \times K$, onde $N$ Ã© o nÃºmero de amostras e $K$ Ã© o nÃºmero de classes. Cada linha de $Y$ corresponde a uma amostra e cada coluna corresponde a uma classe. Se a $i$-Ã©sima amostra pertence Ã  classe $k$, entÃ£o $Y_{ik} = 1$, caso contrÃ¡rio $Y_{ik} = 0$.
2.  **RegressÃ£o NÃ£o ParamÃ©trica:** Aplicamos uma regressÃ£o nÃ£o paramÃ©trica sobre a matriz de respostas $Y$ em funÃ§Ã£o das *features*, o que nos leva a um conjunto de projeÃ§Ãµes $\eta_l(x)$. A regressÃ£o pode ser feita atravÃ©s de mÃ©todos como *splines* aditivas, MARS ou outras formas de modelos nÃ£o paramÃ©tricos.
3.  **Scores Ã“timos:** Calculamos os scores Ã³timos $\theta_l(g_i)$ atravÃ©s de uma decomposiÃ§Ã£o espectral da matriz de respostas ajustada. Os scores Ã³timos, em conjunto com a decomposiÃ§Ã£o, nos permitem encontrar novas representaÃ§Ãµes das classes, onde as classes sÃ£o otimamente separadas, de acordo com a mÃ©trica utilizada.

A FDA permite escolher o tipo de regressÃ£o mais adequado para o problema em questÃ£o, oferecendo flexibilidade e capacidade de modelar relaÃ§Ãµes nÃ£o lineares entre as *features* e as classes. O nÃºmero de projeÃ§Ãµes discriminantes $L$ pode ser menor que $K - 1$, o que Ã© uma forma de reduÃ§Ã£o de dimensionalidade, alÃ©m da projeÃ§Ã£o sobre componentes discriminantes.

A funÃ§Ã£o de regressÃ£o $\eta_l(x)$ pode ser expressa como uma funÃ§Ã£o linear sobre a base de funÃ§Ãµes gerada pelo modelo de regressÃ£o nÃ£o paramÃ©trica:

$$ \eta_l(x) = h(x)^T \beta_l  $$

Onde $h(x)$ sÃ£o as funÃ§Ãµes de base geradas pela regressÃ£o nÃ£o paramÃ©trica.

A escolha do mÃ©todo de regressÃ£o e do nÃºmero de componentes discriminantes sÃ£o decisÃµes importantes que impactam no desempenho do modelo FDA. A escolha do nÃºmero de componentes discriminantes pode ser feita por validaÃ§Ã£o cruzada ou outras tÃ©cnicas de seleÃ§Ã£o de modelos, enquanto a escolha do mÃ©todo de regressÃ£o deve considerar a complexidade dos dados e as relaÃ§Ãµes entre as *features* e as classes.

**Lemma 4:** A FDA generaliza LDA ao substituir a projeÃ§Ã£o linear por uma regressÃ£o nÃ£o paramÃ©trica, o que permite modelar relaÃ§Ãµes nÃ£o lineares entre as *features* e as classes e melhorar a capacidade de classificaÃ§Ã£o em problemas complexos.

A prova desse lemma reside na anÃ¡lise da formulaÃ§Ã£o da FDA, onde a projeÃ§Ã£o linear da LDA Ã© substituÃ­da por modelos de regressÃ£o nÃ£o paramÃ©tricos, que permitem uma representaÃ§Ã£o mais flexÃ­vel das classes e uma melhor adaptaÃ§Ã£o a dados nÃ£o lineares. Ao usar a regressÃ£o, a FDA nÃ£o sÃ³ projeta as classes em espaÃ§os lineares como faz a LDA, mas tambÃ©m modela cada uma das classes como funÃ§Ãµes em um espaÃ§o de *features*.

### AnÃ¡lise Discriminante Penalizada (PDA): RegularizaÃ§Ã£o e Alta Dimensionalidade

```mermaid
graph LR
    subgraph "PDA Regularization"
        direction LR
        A["Optimization Goal:  max Î²^T S_B Î²"]
         B["Penalization Term:  Î»Î²^T Î©Î²"]
        A --> C["Regularized Function: max (Î²^T S_B Î² - Î»Î²^T Î©Î²)"]
        B --> C
        C --> D["Reduced Overfitting"]
    end
```

A **AnÃ¡lise Discriminante Penalizada (PDA)** surge como uma resposta Ã  necessidade de lidar com a alta dimensionalidade em problemas de classificaÃ§Ã£o, onde o nÃºmero de *features* Ã© comparÃ¡vel ou maior do que o nÃºmero de amostras. Em cenÃ¡rios com alta dimensionalidade, a LDA, ao buscar encontrar um espaÃ§o discriminante, pode se ajustar demais aos dados de treinamento, resultando em *overfitting*. A PDA adiciona um termo de penalizaÃ§Ã£o Ã  funÃ§Ã£o de custo da LDA, controlando a complexidade do modelo e evitando a utilizaÃ§Ã£o de *features* redundantes ou irrelevantes [^12.6].

O problema de otimizaÃ§Ã£o da PDA pode ser expresso como:

$$ \max_{\beta} \beta^T S_B \beta - \lambda \beta^T \Omega \beta $$

onde $S_B$ Ã© a matriz de dispersÃ£o entre as classes, $\Omega$ Ã© uma matriz que penaliza os coeficientes, e $\lambda$ Ã© o parÃ¢metro de regularizaÃ§Ã£o. A penalidade $\lambda \beta^T \Omega \beta$ visa limitar a magnitude dos coeficientes do modelo, tornando-o mais estÃ¡vel e menos propenso a *overfitting*.

A matriz de penalizaÃ§Ã£o $\Omega$ pode ser uma matriz identidade, ou pode incorporar algum conhecimento sobre o problema em questÃ£o. Por exemplo, em problemas com dados de imagem, a matriz $\Omega$ pode ser construÃ­da para penalizar coeficientes que variam muito entre pixels adjacentes, resultando em uma suavizaÃ§Ã£o da projeÃ§Ã£o discriminante.

O termo de penalizaÃ§Ã£o na PDA tem um efeito similar Ã  regularizaÃ§Ã£o na regressÃ£o linear, onde penalizamos coeficientes excessivamente grandes para evitar o *overfitting*. Ao penalizar os coeficientes da projeÃ§Ã£o discriminante, a PDA busca encontrar uma projeÃ§Ã£o mais suave e estÃ¡vel, com melhor capacidade de generalizaÃ§Ã£o.

A escolha do parÃ¢metro de regularizaÃ§Ã£o $\lambda$ Ã© importante, e pode ser feita utilizando mÃ©todos como validaÃ§Ã£o cruzada. Valores altos de $\lambda$ levam a modelos mais simples, com menor variÃ¢ncia, mas potencialmente com maior viÃ©s, enquanto valores baixos de $\lambda$ permitem modelos mais complexos, com menor viÃ©s, mas com maior variÃ¢ncia.

A PDA oferece uma maneira flexÃ­vel de lidar com a alta dimensionalidade em problemas de classificaÃ§Ã£o, permitindo ajustar o nÃ­vel de regularizaÃ§Ã£o de acordo com as caracterÃ­sticas do conjunto de dados.

**Lemma 5:** A PDA, ao introduzir um termo de penalizaÃ§Ã£o, controla a complexidade do modelo LDA e evita o *overfitting*, especialmente em cenÃ¡rios de alta dimensionalidade.

A prova desse lemma envolve a anÃ¡lise da funÃ§Ã£o de custo da PDA, que inclui o termo de penalizaÃ§Ã£o $\beta^T \Omega \beta$. Esse termo restringe a magnitude dos coeficientes do modelo, limitando a influÃªncia de *features* redundantes ou irrelevantes, e forÃ§ando o modelo a construir fronteiras de decisÃ£o mais suaves, e menos dependentes de detalhes especÃ­ficos nos dados de treinamento.

**Prova do Lemma 5:**

A funÃ§Ã£o objetivo da PDA Ã©:

$$ J(\beta) = \beta^T S_B \beta - \lambda \beta^T \Omega \beta $$

Onde $\beta$ Ã© o vetor de coeficientes, $S_B$ Ã© a matriz de dispersÃ£o entre classes, $\Omega$ Ã© a matriz de penalizaÃ§Ã£o e $\lambda$ Ã© o parÃ¢metro de regularizaÃ§Ã£o. Se $\lambda = 0$, nÃ£o hÃ¡ regularizaÃ§Ã£o e o mÃ©todo Ã© equivalente Ã  LDA.

Para maximizar a funÃ§Ã£o objetivo, calculamos a derivada em relaÃ§Ã£o a $\beta$ e igualamos a zero:

$$ \frac{\partial J}{\partial \beta} = 2S_B\beta - 2\lambda\Omega\beta = 0 $$
$$ (S_B - \lambda\Omega)\beta = 0 $$

Essa equaÃ§Ã£o mostra que a soluÃ§Ã£o Ã³tima Ã© dada pelos autovetores da matriz $(S_B - \lambda\Omega)$. Se nÃ£o existisse a regularizaÃ§Ã£o ($\lambda=0$), a soluÃ§Ã£o seria dada pelos autovetores da matriz $S_B$, como na LDA. A regularizaÃ§Ã£o adiciona um termo que penaliza coeficientes grandes, e este efeito depende da matriz $\Omega$. Por exemplo, quando $\Omega$ Ã© a matriz identidade, a penalizaÃ§Ã£o corresponde Ã  norma $L_2$, reduzindo a magnitude dos coeficientes. A matriz $\Omega$ pode ser usada para adicionar mais estrutura Ã  penalizaÃ§Ã£o.

$\blacksquare$

**CorolÃ¡rio 5:** A PDA, atravÃ©s da regularizaÃ§Ã£o, obtÃ©m modelos LDA mais estÃ¡veis, que generalizam melhor e sÃ£o menos propensos a ruÃ­do.

A regularizaÃ§Ã£o leva a modelos menos sensÃ­veis a ruÃ­do nos dados de treinamento, o que garante uma maior capacidade de generalizaÃ§Ã£o em dados novos.

### AnÃ¡lise Discriminante por Misturas (MDA): Modelando Classes com MÃºltiplos ProtÃ³tipos

```mermaid
graph LR
    subgraph "MDA Mixture Modeling"
        direction LR
        A["Class k"] --> B["Mixture of Gaussians: p(x|G=k) = Î£ Ï€_kr Ï†(x; Î¼_kr, Î£)"]
        B --> C["Multiple Prototypes"]
        C --> D["Flexible Decision Boundaries"]
    end
```

A **AnÃ¡lise Discriminante por Misturas (MDA)** aborda uma limitaÃ§Ã£o fundamental da LDA, que consiste em representar cada classe com um Ãºnico protÃ³tipo (centrÃ³ide) [^12.7]. Em muitos problemas de classificaÃ§Ã£o, as classes podem ser compostas por mÃºltiplos subgrupos, e a utilizaÃ§Ã£o de um Ãºnico protÃ³tipo pode nÃ£o representar adequadamente sua distribuiÃ§Ã£o. A MDA modela cada classe como uma mistura de Gaussianas, permitindo que cada classe seja representada por mÃºltiplos protÃ³tipos (centrÃ³ides), com covariÃ¢ncias compartilhadas, e com isso oferece maior flexibilidade na modelagem das classes e na construÃ§Ã£o das fronteiras de decisÃ£o.

O modelo de mistura Gaussiano para a classe $k$ pode ser expresso como:

$$ p(x|G=k) = \sum_{r=1}^{R_k} \pi_{kr} \phi(x; \mu_{kr}, \Sigma) $$

onde $R_k$ Ã© o nÃºmero de componentes Gaussianos na mistura da classe $k$, $\pi_{kr}$ sÃ£o os pesos da mistura, $\mu_{kr}$ Ã© a mÃ©dia do componente $r$ da classe $k$, e $\Sigma$ Ã© a matriz de covariÃ¢ncia comum.

A MDA utiliza o algoritmo Expectation-Maximization (EM) para ajustar os parÃ¢metros do modelo. O algoritmo EM Ã© um mÃ©todo iterativo que alterna entre as seguintes etapas:

1.  **Etapa E (Expectation):** Calcula a probabilidade de que cada amostra pertenÃ§a a cada componente gaussiano, dadas as estimativas atuais dos parÃ¢metros do modelo. Essas probabilidades sÃ£o chamadas de "responsabilidades" e servem para estimar o peso de cada componente no processo de otimizaÃ§Ã£o.
2.  **Etapa M (Maximization):** Calcula as novas estimativas para as mÃ©dias, pesos da mistura e matriz de covariÃ¢ncia, maximizando a funÃ§Ã£o de verossimilhanÃ§a com base nas responsabilidades calculadas na etapa E.

ApÃ³s a convergÃªncia do algoritmo EM, as probabilidades *a posteriori* de cada classe sÃ£o calculadas como:

$$ P(G=k|x) = \frac{p(x|G=k) P(G=k)}{\sum_{j=1}^{K} p(x|G=j) P(G=j)} $$

onde $P(G=k)$ Ã© a probabilidade *a priori* da classe $k$.

A MDA oferece uma abordagem mais flexÃ­vel e robusta para classificaÃ§Ã£o, especialmente em conjuntos de dados onde as classes nÃ£o sÃ£o bem representadas por um Ãºnico protÃ³tipo. Ao utilizar modelos de mistura, a MDA pode modelar classes com mÃºltiplas regiÃµes de alta densidade, o que permite uma melhor adaptaÃ§Ã£o aos dados e, consequentemente, uma melhor classificaÃ§Ã£o. A escolha do nÃºmero de componentes na mistura Ã© um aspecto importante a ser considerado na aplicaÃ§Ã£o da MDA, e pode ser feita por meio de tÃ©cnicas de seleÃ§Ã£o de modelos ou validaÃ§Ã£o cruzada.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um problema de classificaÃ§Ã£o com duas classes, onde a classe 1 possui duas subpopulaÃ§Ãµes distintas e a classe 2 possui uma Ãºnica subpopulaÃ§Ã£o. A LDA, que assume um Ãºnico centrÃ³ide por classe, nÃ£o modelaria bem a classe 1. A MDA, ao modelar a classe 1 com duas gaussianas, captura melhor a estrutura dos dados.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.mixture import GaussianMixture
> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>
> # Gerar dados de exemplo
> np.random.seed(42)
> mean1_1 = [1, 1]
> cov1_1 = [[0.5, 0], [0, 0.5]]
> mean1_2 = [3, 3]
> cov1_2 = [[0.5, 0], [0, 0.5]]
> mean2 = [5, 1]
> cov2 = [[0.8, 0], [0, 0.8]]
>
> X1_1 = np.random.multivariate_normal(mean1_1, cov1_1, 50)
> X1_2 = np.random.multivariate_normal(mean1_2, cov1_2, 50)
> X2 = np.random.multivariate_normal(mean2, cov2, 100)
> X = np.concatenate((X1_1, X1_2, X2))
> y = np.array([0] * 100 + [1] * 100)
>
> # Aplicar MDA (GMM por classe)
> gmm_class0 = GaussianMixture(n_components=2, covariance_type='full', random_state=42)
> gmm_class0.fit(X[y==0])
> gmm_class1 = GaussianMixture(n_components=1, covariance_type='full', random_