## T√≠tulo: Support Vector Machines: Formula√ß√£o, Kernels e Aplica√ß√µes Avan√ßadas

```mermaid
graph LR
    subgraph "SVM Training Process"
        direction TB
        A["Input Data: 'x_i', 'y_i'"]
        B["Feature Space Transformation: 'Kernel'"]
        C["Optimization Problem Formulation: 'min ||Œ≤||¬≤ + CŒ£Œæ_i'"]
        D["Lagrange Multipliers: 'Œ±_i'"]
        E["Support Vectors: 'Œ±_i > 0'"]
        F["Optimal Hyperplane: 'Œ≤, Œ≤_0'"]
        A --> B
        B --> C
        C --> D
        D --> E
        E --> F
    end
```

### Introdu√ß√£o

No cap√≠tulo anterior, discutimos as limita√ß√µes das fronteiras de decis√£o lineares e apresentamos diversas abordagens para generalizar m√©todos de classifica√ß√£o. Neste cap√≠tulo, aprofundaremos nosso estudo sobre as **Support Vector Machines (SVMs)**, um m√©todo de aprendizado de m√°quina poderoso e vers√°til, capaz de construir fronteiras de decis√£o n√£o lineares atrav√©s da utiliza√ß√£o de **kernels** [^12.3]. As SVMs se destacam pela sua capacidade de encontrar o hiperplano √≥timo que separa classes, maximizando a margem e oferecendo robustez contra o *overfitting*.

A motiva√ß√£o principal para a utiliza√ß√£o de SVMs reside na sua capacidade de lidar com dados n√£o linearmente separ√°veis. Enquanto os m√©todos lineares, como a regress√£o log√≠stica e a An√°lise Discriminante Linear (LDA), constroem fronteiras de decis√£o lineares no espa√ßo original de *features*, as SVMs podem mapear os dados para um espa√ßo de *features* de maior dimens√£o, onde uma separa√ß√£o linear se torna poss√≠vel. A constru√ß√£o de modelos mais flex√≠veis por meio do uso de *kernels*, que calculam os produtos internos nesse espa√ßo de maior dimens√£o sem explicitamente realizar a transforma√ß√£o, √© uma das principais vantagens desse m√©todo, como abordado em [^12.3].

Al√©m disso, as SVMs oferecem uma formula√ß√£o matem√°tica elegante baseada em um problema de otimiza√ß√£o convexa, o que garante que uma solu√ß√£o global possa ser encontrada. A introdu√ß√£o de vari√°veis de folga permite lidar com dados sobrepostos e *outliers*, tornando as SVMs robustas em rela√ß√£o a dados ruidosos ou incompletos.

Neste cap√≠tulo, exploraremos em profundidade os fundamentos te√≥ricos das SVMs, desde a formula√ß√£o do problema de otimiza√ß√£o at√© a utiliza√ß√£o de *kernels* para modelar fronteiras n√£o lineares. Discutiremos os conceitos de vetores de suporte, o papel do par√¢metro de regulariza√ß√£o $C$ e as diferentes op√ß√µes de *kernels* dispon√≠veis. Al√©m disso, analisaremos como as SVMs podem ser adaptadas para problemas de regress√£o e como elas se relacionam com outros m√©todos de aprendizado de m√°quina.

### Conceitos Fundamentais

**Conceito 1: O Problema de Otimiza√ß√£o das SVMs**

As SVMs s√£o constru√≠das a partir da resolu√ß√£o de um problema de otimiza√ß√£o, cuja formula√ß√£o consiste em encontrar o hiperplano que maximiza a margem de separa√ß√£o entre as classes. Para o caso linearmente separ√°vel, o problema de otimiza√ß√£o pode ser escrito como [^12.2]:

$$ \min_{\beta, \beta_0} \frac{1}{2} ||\beta||^2 $$
$$ \text{sujeito a } y_i(x_i^T\beta + \beta_0) \ge 1, \text{ para todo } i $$

onde $\beta$ √© o vetor normal ao hiperplano, $\beta_0$ √© o *bias*, $x_i$ √© o vetor de *features* da $i$-√©sima amostra e $y_i \in \{-1,1\}$ √© o r√≥tulo da classe. A restri√ß√£o $y_i(x_i^T\beta + \beta_0) \ge 1$ garante que todas as amostras estejam corretamente classificadas e a uma dist√¢ncia de pelo menos 1 do hiperplano.

Para lidar com dados que n√£o s√£o linearmente separ√°veis, introduzimos as vari√°veis de folga $\xi_i$, que medem a viola√ß√£o da restri√ß√£o por cada amostra. O problema de otimiza√ß√£o para o caso n√£o separ√°vel se torna [^12.3]:

$$ \min_{\beta, \beta_0, \xi} \frac{1}{2} ||\beta||^2 + C \sum_i \xi_i $$
$$ \text{sujeito a } y_i(x_i^T\beta + \beta_0) \ge 1 - \xi_i, \text{ para todo } i $$
$$ \xi_i \ge 0, \text{ para todo } i $$

onde $C$ √© o par√¢metro de regulariza√ß√£o que controla o compromisso entre a maximiza√ß√£o da margem e a penalidade por classifica√ß√µes erradas. Valores altos de $C$ penalizam fortemente a classifica√ß√£o errada, enquanto valores baixos permitem mais erros para maximizar a margem.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simplificado com duas amostras. Suponha que tenhamos:
>
> - $x_1 = [1, 2]$, $y_1 = 1$
> - $x_2 = [2, 1]$, $y_2 = -1$
>
> Inicialmente, vamos considerar o caso linearmente separ√°vel, onde $C$ √© muito alto (ou seja, erros de classifica√ß√£o s√£o fortemente penalizados). O objetivo √© encontrar $\beta$ e $\beta_0$ que satisfa√ßam as restri√ß√µes $y_i(x_i^T\beta + \beta_0) \ge 1$.
>
> Para simplificar, vamos assumir que $\beta = [b_1, b_2]$ e que $\beta_0 = -1$. Uma poss√≠vel solu√ß√£o para este problema seria $\beta = [1, -1]$. Vamos verificar as restri√ß√µes:
>
> - Para $x_1$: $1 * (1*1 + 2*(-1) - 1) = 1 * (1 - 2 - 1) = -2$. Essa restri√ß√£o n√£o √© satisfeita, pois precisa ser maior ou igual a 1.
>
> Agora vamos considerar o caso n√£o separ√°vel, onde permitimos vari√°veis de folga ($\xi_i$). Se $C = 1$, o problema se torna:
>
> $$ \min_{\beta, \beta_0, \xi} \frac{1}{2} ||\beta||^2 +  \xi_1 + \xi_2 $$
>
> sujeito a:
>
> -  $1 * (1*b_1 + 2*b_2 + \beta_0) \ge 1 - \xi_1$
> -  $-1 * (2*b_1 + 1*b_2 + \beta_0) \ge 1 - \xi_2$
> -  $\xi_1 \ge 0, \xi_2 \ge 0$
>
> Uma solu√ß√£o poss√≠vel poderia ser: $\beta = [0.5, -0.5]$, $\beta_0 = -0.5$, $\xi_1 = 0.5$ e $\xi_2 = 0$. Verificando as restri√ß√µes:
>
> - Para $x_1$: $1 * (1*0.5 + 2*(-0.5) - 0.5) = 1 * (0.5 - 1 - 0.5) = -1 \ge 1 - 0.5 = 0.5$. Essa restri√ß√£o n√£o √© satisfeita.
> - Para $x_2$: $-1 * (2*0.5 + 1*(-0.5) - 0.5) = -1 * (1 - 0.5 - 0.5) = 0 \ge 1 - 0 = 1$. Essa restri√ß√£o tamb√©m n√£o √© satisfeita.
>
> Ajustando os valores, uma poss√≠vel solu√ß√£o poderia ser: $\beta = [0.5, -0.5]$, $\beta_0 = -0.2$, $\xi_1 = 0.3$ e $\xi_2 = 0.1$.
>
> - Para $x_1$: $1 * (1*0.5 + 2*(-0.5) - 0.2) = 1 * (0.5 - 1 - 0.2) = -0.7 \ge 1 - 0.3 = 0.7$. N√£o satisfeita.
> - Para $x_2$: $-1 * (2*0.5 + 1*(-0.5) - 0.2) = -1 * (1 - 0.5 - 0.2) = -0.3 \ge 1 - 0.1 = 0.9$. N√£o satisfeita.
>
> Este exemplo ilustra como as vari√°veis de folga $\xi_i$ permitem que as restri√ß√µes sejam violadas, com a penalidade controlada pelo par√¢metro $C$. A escolha de $\beta$, $\beta_0$ e $\xi_i$ √© feita pelo problema de otimiza√ß√£o para minimizar a fun√ß√£o objetivo, respeitando as restri√ß√µes.

**Lemma 1:** O problema de otimiza√ß√£o das SVMs √© um problema convexo, o que garante a exist√™ncia de um m√≠nimo global √∫nico, simplificando a busca pela solu√ß√£o √≥tima.

A demonstra√ß√£o desse lemma se baseia na observa√ß√£o de que a fun√ß√£o objetivo (a norma ao quadrado de $\beta$ mais a soma ponderada das vari√°veis de folga) √© convexa, e as restri√ß√µes s√£o lineares. Problemas de otimiza√ß√£o convexos t√™m a propriedade de que qualquer m√≠nimo local tamb√©m √© um m√≠nimo global, o que torna o processo de otimiza√ß√£o mais eficiente e confi√°vel.

```mermaid
graph TB
    subgraph "SVM Optimization Problem"
        direction TB
        A["Objective Function: 'min ||Œ≤||¬≤/2 + CŒ£Œæ_i'"]
        B["Convexity"]
        C["Constraints: 'y_i(x_i^TŒ≤ + Œ≤_0) ‚â• 1 - Œæ_i', 'Œæ_i ‚â• 0'"]
        D["Guaranteed Global Minimum"]
         A --> B
         A --> C
        B & C --> D
    end
```

**Conceito 2: Multiplicadores de Lagrange e o Problema Dual**

Para resolver o problema de otimiza√ß√£o das SVMs, √© comum utilizar o m√©todo dos **Multiplicadores de Lagrange**, que transforma o problema primal em um problema dual. A fun√ß√£o Lagrangiana para o problema de otimiza√ß√£o da SVM √© dada por [^12.2]:

$$ L_p = \frac{1}{2} ||\beta||^2 + C \sum_i \xi_i - \sum_i \alpha_i [y_i(x_i^T\beta + \beta_0) - 1 + \xi_i] - \sum_i \mu_i \xi_i $$

onde $\alpha_i$ e $\mu_i$ s√£o os multiplicadores de Lagrange n√£o negativos. Ao minimizar $L_p$ em rela√ß√£o a $\beta$, $\beta_0$ e $\xi_i$ e aplicando as condi√ß√µes de Karush-Kuhn-Tucker (KKT), obtemos o problema dual:

$$ \max_\alpha L_D = \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^T x_j $$
$$ \text{sujeito a } 0 \le \alpha_i \le C, \text{ para todo } i $$
$$ \sum_i \alpha_i y_i = 0 $$

O problema dual envolve encontrar os multiplicadores de Lagrange $\alpha_i$ que maximizam $L_D$ sujeitos √†s restri√ß√µes. A solu√ß√£o desse problema nos fornece os valores de $\alpha_i$ que determinam os **vetores de suporte** e, por conseguinte, o hiperplano √≥timo.

> üí° **Exemplo Num√©rico:**
>
> Continuando com o exemplo anterior, vamos supor que ap√≥s resolver o problema dual, obtivemos os seguintes valores para $\alpha$:
>
> - $\alpha_1 = 0.6$
> - $\alpha_2 = 0.2$
>
>  e $C = 1$.
>
>  As restri√ß√µes do problema dual s√£o:
>
>  1. $0 \le \alpha_i \le C$
>  2. $\sum_i \alpha_i y_i = 0$
>
>  Verificando a primeira restri√ß√£o:
>  - $0 \le 0.6 \le 1$ (V√°lido)
>  - $0 \le 0.2 \le 1$ (V√°lido)
>
>  Verificando a segunda restri√ß√£o:
>  - $0.6 * 1 + 0.2 * (-1) = 0.6 - 0.2 = 0.4 \ne 0$
>
>  Este resultado mostra que os valores de $\alpha$ precisam ser otimizados para satisfazer a restri√ß√£o $\sum_i \alpha_i y_i = 0$.
>
>  Suponha que, ap√≥s a otimiza√ß√£o, obtivemos:
>
> - $\alpha_1 = 0.5$
> - $\alpha_2 = 0.5$
>
> A restri√ß√£o agora √©:
>
> - $0.5 * 1 + 0.5 * (-1) = 0.5 - 0.5 = 0$ (V√°lido)
>
> A fun√ß√£o dual para este exemplo com $x_1 = [1, 2]$, $x_2 = [2, 1]$, $y_1 = 1$, $y_2 = -1$ e $\alpha_1=0.5$, $\alpha_2=0.5$ seria:
>
> $L_D = 0.5 + 0.5 - 0.5 * (0.5 * 0.5 * 1 * 1 * [1,2]^T[1,2] + 0.5 * 0.5 * 1 * -1 * [1,2]^T[2,1] + 0.5 * 0.5 * -1 * 1 * [2,1]^T[1,2] + 0.5 * 0.5 * -1 * -1 * [2,1]^T[2,1] )$
>
> $L_D = 1 - 0.5 * (0.25 * (1+4) + 0.25 * (-1) * (2+2) + 0.25 * (-1) * (2+2) + 0.25 * (4+1))$
>
> $L_D = 1 - 0.5 * (0.25 * 5 - 0.25 * 4 - 0.25 * 4 + 0.25 * 5)$
>
> $L_D = 1 - 0.5 * (1.25 - 1 - 1 + 1.25)$
>
> $L_D = 1 - 0.5 * 0.5 = 1 - 0.25 = 0.75$
>
> Note que o c√°lculo de $L_D$ envolve os produtos internos $x_i^T x_j$, que s√£o cruciais para o uso de kernels.

```mermaid
graph TB
 subgraph "Lagrange Multipliers"
    direction TB
    A["Primal Problem"]
    B["Lagrangian Function: 'L_p'"]
    C["Dual Problem: 'L_D(Œ±)'"]
    D["Lagrange Multipliers: 'Œ±_i', 'Œº_i'"]
    E["Dual Constraints: '0 ‚â§ Œ±_i ‚â§ C', 'Œ£Œ±_iy_i = 0'"]
    F["Solution: 'Œ±_i'"]
     A --> B
     B --> C
     B --> D
     C --> E
     E --> F
  end
```

**Corol√°rio 1:** A solu√ß√£o do problema dual das SVMs envolve apenas os produtos internos entre os dados de treinamento, o que permite a utiliza√ß√£o de *kernels* para lidar com dados n√£o linearmente separ√°veis.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise da fun√ß√£o dual, onde a √∫nica depend√™ncia dos dados $x_i$ √© atrav√©s dos produtos internos $x_i^Tx_j$. Essa propriedade √© fundamental, pois permite substituir os produtos internos $x_i^Tx_j$ por uma fun√ß√£o *kernel* $K(x_i, x_j)$, que calcula o produto interno em um espa√ßo de *features* de maior dimens√£o sem explicitar a transforma√ß√£o, conforme mencionado em [^12.3].

**Conceito 3: Vetores de Suporte e o Par√¢metro de Regulariza√ß√£o C**

Os **vetores de suporte** s√£o as amostras que ficam sobre ou dentro da margem e que possuem multiplicadores de Lagrange $\alpha_i > 0$ [^12.2]. S√£o esses pontos que determinam o hiperplano √≥timo, e a solu√ß√£o do problema dual depende apenas deles. Os vetores de suporte s√£o os pontos mais dif√≠ceis de classificar, e a SVM foca sua aten√ß√£o nesses pontos durante o treinamento.

O par√¢metro de regulariza√ß√£o $C$ controla a complexidade do modelo. Valores maiores de $C$ tendem a gerar modelos mais complexos, com margens menores e menos erros de classifica√ß√£o nos dados de treinamento, mas com maior risco de *overfitting*. Valores menores de $C$ tendem a gerar modelos mais simples, com margens maiores e mais erros nos dados de treinamento, mas com melhor capacidade de generaliza√ß√£o [^12.2].

```mermaid
graph LR
    subgraph "Support Vectors & Regularization"
       direction TB
        A["Lagrange Multipliers 'Œ±_i'"]
        B["Support Vectors: 'Œ±_i > 0'"]
        C["Regularization Parameter 'C'"]
        D["High 'C': Complex Model, Overfitting Risk"]
        E["Low 'C': Simple Model, Generalization"]
        A --> B
        C --> D
        C --> E
    end
```

> üí° **Exemplo Num√©rico:**
>
> Suponha que, ap√≥s treinar uma SVM em um conjunto de dados, tenhamos os seguintes valores para os multiplicadores de Lagrange $\alpha_i$:
>
> - $\alpha_1 = 0$
> - $\alpha_2 = 0.8$
> - $\alpha_3 = 0$
> - $\alpha_4 = 0.5$
> - $\alpha_5 = 0$
>
> Neste caso, apenas as amostras $x_2$ e $x_4$ s√£o vetores de suporte, pois seus multiplicadores de Lagrange s√£o maiores que zero. Isso significa que o hiperplano √≥timo √© definido apenas por essas duas amostras. As amostras $x_1$, $x_3$ e $x_5$ n√£o influenciam na constru√ß√£o do modelo final.
>
> Agora, vamos analisar o impacto do par√¢metro $C$. Se aumentarmos o valor de $C$, estamos penalizando mais os erros de classifica√ß√£o e, consequentemente, permitindo que mais amostras se tornem vetores de suporte. Em contrapartida, se diminuirmos o valor de $C$, estamos permitindo mais erros e, consequentemente, reduzindo o n√∫mero de vetores de suporte.
>
> Por exemplo, se $C=0.1$, poder√≠amos ter menos vetores de suporte (e margem maior), enquanto que se $C=100$, poder√≠amos ter mais vetores de suporte (e margem menor).

> ‚ö†Ô∏è **Nota Importante**: As condi√ß√µes de Karush-Kuhn-Tucker (KKT) s√£o necess√°rias para a otimiza√ß√£o do problema, relacionando os par√¢metros primais e duais. **Baseado em [^12.4]**.

> ‚ùó **Ponto de Aten√ß√£o**: O par√¢metro de regulariza√ß√£o C controla a complexidade do modelo, com valores maiores levando a modelos mais complexos e valores menores a modelos mais simples. **Conforme indicado em [^12.2]**.

> ‚úîÔ∏è **Destaque**: Os vetores de suporte determinam o hiperplano √≥timo e s√£o os pontos mais dif√≠ceis de classificar. **Baseado no t√≥pico [^12.3]**.

### Formula√ß√£o Matem√°tica Detalhada da SVM

```mermaid
graph TB
    subgraph "SVM Cost Function"
        direction TB
        A["'Hinge Loss': max(0, 1 - y_i(x_i^TŒ≤ + Œ≤_0))"]
        B["Regularization Term: '||Œ≤||¬≤/2'"]
        C["Cost Function: 'C * Loss + ||Œ≤||¬≤/2'"]
        D["Penalizes Misclassifications"]
        E["Prevents Overfitting"]
         A --> D
         B --> E
         A & B --> C
    end
```

Para uma compreens√£o mais profunda da formula√ß√£o da SVM, vamos detalhar os passos matem√°ticos envolvidos. O problema primal, com vari√°veis de folga, pode ser reescrito como:

$$ \min_{\beta, \beta_0, \xi} \frac{1}{2} ||\beta||^2 + C \sum_{i=1}^{N} \xi_i $$

sujeito a:

$$ y_i(x_i^T\beta + \beta_0) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad \forall i $$

onde $N$ √© o n√∫mero total de amostras.

**Formula√ß√£o do Problema Dual:**

A fun√ß√£o Lagrangiana √©:

$$ L_p(\beta, \beta_0, \xi, \alpha, \mu) = \frac{1}{2} ||\beta||^2 + C \sum_{i=1}^{N} \xi_i - \sum_{i=1}^{N} \alpha_i [y_i(x_i^T\beta + \beta_0) - 1 + \xi_i] - \sum_{i=1}^{N} \mu_i \xi_i $$

onde $\alpha_i \geq 0$ e $\mu_i \geq 0$ s√£o os multiplicadores de Lagrange.

Para obter o problema dual, precisamos minimizar $L_p$ em rela√ß√£o a $\beta$, $\beta_0$ e $\xi_i$, e aplicar as condi√ß√µes de KKT. As derivadas parciais de $L_p$ em rela√ß√£o a essas vari√°veis s√£o:

$$ \frac{\partial L_p}{\partial \beta} = \beta - \sum_{i=1}^{N} \alpha_i y_i x_i = 0 \implies \beta = \sum_{i=1}^{N} \alpha_i y_i x_i $$
$$ \frac{\partial L_p}{\partial \beta_0} = - \sum_{i=1}^{N} \alpha_i y_i = 0 \implies \sum_{i=1}^{N} \alpha_i y_i = 0 $$
$$ \frac{\partial L_p}{\partial \xi_i} = C - \alpha_i - \mu_i = 0 \implies \mu_i = C - \alpha_i $$

Substituindo as express√µes para $\beta$ e $\mu_i$ na fun√ß√£o Lagrangiana e usando a restri√ß√£o $\sum_{i=1}^{N} \alpha_i y_i = 0$, obtemos a fun√ß√£o dual:

$$ L_D(\alpha) = \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j x_i^T x_j $$

O problema dual da SVM √© ent√£o:

$$ \max_{\alpha} \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j x_i^T x_j $$

sujeito a:

$$ 0 \leq \alpha_i \leq C, \quad \forall i $$
$$ \sum_{i=1}^{N} \alpha_i y_i = 0 $$

A solu√ß√£o do problema dual nos fornece os valores de $\alpha_i$, que s√£o usados para calcular $\beta$ e $\beta_0$.

**Condi√ß√µes de Karush-Kuhn-Tucker (KKT):**

As condi√ß√µes de KKT s√£o:

1.  $\alpha_i [y_i(x_i^T\beta + \beta_0) - 1 + \xi_i] = 0, \quad \forall i$
2.  $\mu_i \xi_i = 0, \quad \forall i$
3.  $\alpha_i \geq 0, \quad \mu_i \geq 0, \quad \xi_i \geq 0, \quad \forall i$
4.  $y_i(x_i^T\beta + \beta_0) - 1 + \xi_i \geq 0, \quad \forall i$

As condi√ß√µes de KKT permitem analisar a rela√ß√£o entre as vari√°veis primais e duais, e s√£o essenciais para a solu√ß√£o do problema de otimiza√ß√£o da SVM.

### Kernels e a Extens√£o para N√£o Linearidade

```mermaid
graph LR
    subgraph "Kernel Transformation"
        direction LR
        A["Input Space: 'x' (Nonlinear)"]
        B["Mapping Function: 'Œ¶(x)'"]
        C["Feature Space: 'Œ¶(x)' (Linear)"]
        D["Kernel: 'K(x_i, x_j) = Œ¶(x_i)^TŒ¶(x_j)'"]
        A --> B
        B --> C
        C --> D
    end
```

Um dos principais trunfos das SVMs √© a capacidade de trabalhar em espa√ßos de alta dimens√£o atrav√©s do uso de **kernels**, o que permite construir fronteiras de decis√£o n√£o lineares no espa√ßo original das *features*. A fun√ß√£o *kernel* $K(x_i, x_j)$ calcula o produto interno entre duas amostras, $x_i$ e $x_j$, em um espa√ßo de *features* transformado, sem a necessidade de explicitamente computar a transforma√ß√£o. O problema dual da SVM, como visto anteriormente, envolve apenas os produtos internos entre as amostras:

$$ L_D(\alpha) = \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j x_i^T x_j $$

A substitui√ß√£o do produto interno $x_i^Tx_j$ pelo *kernel* $K(x_i, x_j)$ nos permite trabalhar em espa√ßos de *features* de alta dimens√£o, potencialmente infinitos, onde as classes podem ser linearmente separ√°veis:

$$ L_D(\alpha) = \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j K(x_i, x_j) $$

Tr√™s *kernels* amplamente utilizados s√£o:

1.  **Kernel Polinomial:**

    $$ K(x_i, x_j) = (x_i^T x_j + c)^d $$
    onde $c$ √© um constante e $d$ √© o grau do polin√¥mio.
2.  **Kernel Radial Basis Function (RBF) ou Gaussiano:**

    $$ K(x_i, x_j) = \exp \left( -\gamma ||x_i - x_j||^2 \right) $$
    onde $\gamma > 0$ √© um par√¢metro que controla a largura da fun√ß√£o gaussiana.
3.  **Kernel Sigm√≥ide:**

    $$ K(x_i, x_j) = \tanh(\kappa_1 x_i^T x_j + \kappa_2) $$
    onde $\kappa_1$ e $\kappa_2$ s√£o par√¢metros do *kernel*.

Cada um desses *kernels* mapeia os dados para um espa√ßo diferente e induz uma fronteira de decis√£o n√£o linear diferente no espa√ßo original. A escolha do *kernel* apropriado depende das caracter√≠sticas espec√≠ficas do conjunto de dados, como a complexidade das rela√ß√µes entre as *features* e as classes.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar duas amostras $x_1 = [1, 2]$ e $x_2 = [2, 1]$. Vamos calcular os valores dos *kernels* para esses pontos.
>
> 1.  **Kernel Polinomial (com $c=1$ e $d=2$):**
>
>     $K(x_1, x_2) = ([1, 2]^T [2, 1] + 1)^2 = (1*2 + 2*1 + 1)^2 = (2 + 2 + 1)^2 = 5^2 = 25$
>
>     $K(x_1, x_1) = ([1, 2]^T [1, 2] + 1)^2 = (1*1 + 2*2 + 1)^2 = (1 + 4 + 1)^2 = 6^2 = 36$
>
> 2.  **Kernel RBF (com $\gamma = 0.5$):**
>
>     $K(x_1, x_2) = \exp(-0.5 * ||[1, 2] - [2, 1]||^2) = \exp(-0.5 * ||[-1, 1]||^2) = \exp(-0.5 * (1 + 1)) = \exp(-1) \approx 0.368$
>
>     $K(x_1, x_1) = \exp(-0.5 * ||[1, 2] - [1, 2]||^2) = \exp(-0.5 * 0) = \exp(0) = 1$
>
> 3.  **Kernel Sigm√≥ide (com $\kappa_1 = 1$ e $\kappa_2 = 0$):**
>
>     $K(x_1, x_2) = \tanh(1 * [1, 2]^T [2, 1] + 0) = \tanh(1 * (1*2 + 2*1)) = \tanh(4) \approx 0.999$
>
>     $K(x_1, x_1) = \tanh(1 * [1, 2]^T [1, 2] + 0) = \tanh(1 * (1*1 + 2*2)) = \tanh(5) \approx 0.999$
>
> Esses exemplos mostram como diferentes *kernels* calculam diferentes valores de similaridade entre os pontos, o que levar√° a diferentes fronteiras de decis√£o no espa√ßo original.

```mermaid
graph LR
 subgraph "Common Kernels"
        direction TB
       A["Polynomial Kernel: '(x_i^Tx_j + c)^d'"]
       B["RBF/Gaussian Kernel: 'exp(-Œ≥||x_i - x_j||¬≤)'"]
       C["Sigmoid Kernel: 'tanh(Œ∫‚ÇÅx_i^Tx_j + Œ∫‚ÇÇ)'"]
    end
```

**Lemma 3:** O "kernel trick" permite que as SVMs trabalhem em espa√ßos de alta dimens√£o sem explicitamente calcular a transforma√ß√£o $\phi(x)$, contanto que a fun√ß√£o $K(x_i, x_j)$ seja um *kernel* v√°lido, o que √© definido matematicamente por ser uma fun√ß√£o sim√©trica e semidefinida positiva, como abordado em [^12.3].

A prova desse lemma envolve demonstrar que se $K(x_i, x_j)$ √© um *kernel* v√°lido, ent√£o existe uma transforma√ß√£o $\phi(x)$ tal que $K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$, embora a forma expl√≠cita da transforma√ß√£o n√£o seja conhecida ou necess√°ria para a otimiza√ß√£o. A condi√ß√£o de ser semidefinida positiva garante que a fun√ß√£o *kernel* √© um produto interno em algum espa√ßo de *features*.

**Prova do Lemma 3:**

A prova desse lemma se baseia no **Teorema de Mercer**, que afirma que uma fun√ß√£o $K(x, x')$ pode ser expressa como um produto interno em um espa√ßo de Hilbert se e somente se a fun√ß√£o √© sim√©trica e semidefinida positiva. A simetria √© garantida pela defini√ß√£o do produto interno, e a propriedade de ser semidefinida positiva √© o resultado de ser um produto interno em um espa√ßo de Hilbert.

Este teorema nos diz que se uma fun√ß√£o √© um *kernel* v√°lido, isto √©, se √© uma fun√ß√£o sim√©trica e semidefinida positiva, ent√£o existe um mapeamento $\phi$ para um espa√ßo de alta dimens√£o tal que:

$$K(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle$$

onde $\langle \cdot, \cdot \rangle$ representa o produto interno. O teorema garante a exist√™ncia do mapeamento $\phi$, mas n√£o nos diz como encontr√°-lo. A grande vantagem do *kernel trick* √© que n√£o precisamos conhecer o mapeamento expl√≠cito $\phi$, apenas a fun√ß√£o *kernel* $K$.

$\blacksquare$

**Corol√°rio 3:** A utiliza√ß√£o de kernels permite que as SVMs construam fronteiras de decis√£o n√£o lineares no espa√ßo original dos dados, transformando o problema em uma separa√ß√£o linear em um espa√ßo de *features* de maior dimens√£o [^12.3].

A utiliza√ß√£o de *kernels* transforma um problema de classifica√ß√£o n√£o linear em um problema linear em um espa√ßo de maior dimens√£o, que, ent√£o, pode ser resolvido usando t√©cnicas lineares, como as SVMs, e com a vantagem de n√£o ter que representar explicitamente a transforma√ß√£o.

### SVMs para Regress√£o

```mermaid
graph LR
    subgraph "SVR Loss Functions"
        direction LR
        A["Epsilon Insensitive Loss: 'max(0, |y - f(x)| - Œµ)'"]
        B["Quadratic Loss: '(y - f(x))¬≤'"]
        A --> C["SVR"]
        B --> D["Traditional Regression"]
    end
```

As **Support Vector Machines (SVMs)** podem ser adaptadas para problemas de regress√£o, um processo que √© conhecido como **Support Vector Regression (SVR)**. O objetivo da SVR √© encontrar uma fun√ß√£o $f(x)$ que se aproxima o m√°ximo poss√≠vel dos valores de resposta, dentro de uma certa margem de erro $\epsilon$. A fun√ß√£o de custo para SVR √© dada por:

$$  \min_{ \beta, \beta_0, \xi, \xi^*} \frac{1}{2} ||\beta||^2 + C \sum_{i=1}^N (\xi_i + \xi_i^*) $$

sujeito a:

$$ y_i - (x_i^T\beta + \beta_0) \leq \epsilon + \xi_i  $$
$$  (x_i^T\beta + \beta_0) - y_i  \leq \epsilon + \xi_i^*$$
$$ \xi_i, \xi_i^* \geq 0$$

onde $\epsilon$ √© a largura da margem, $\xi_i$ e $\xi_i^*$ s√£o as vari√°veis de folga que permitem erros maiores do que $\epsilon$, e $C$ √©