Okay, let's explore the connection between a **more general optimization criteria based on scores** and the link between **penalized canonical correlations** and **discriminant analysis**. This section will delve into the theoretical underpinnings of how FDA can be seen as a more flexible approach to finding optimal discriminant directions.

## TÃ­tulo: Scores Ã“timos Generalizados e AnÃ¡lise Discriminante: ConexÃ£o com CorrelaÃ§Ãµes CanÃ´nicas Penalizadas

```mermaid
graph LR
    subgraph "Optimization Criteria and Discriminant Analysis"
        direction TB
        A["Generalized Optimization Criterion"] --> B["Flexible Discriminant Analysis (FDA)"]
        A --> C["Penalized Canonical Correlations (PCCA)"]
        B --> D["Maximizing Class Separation via Scores"]
        C --> E["Regularization for Stable Solutions"]
        D --> F["More Robust Classification Models"]
        E --> F
    end
```

### IntroduÃ§Ã£o

A **AnÃ¡lise Discriminante Linear (LDA)** e suas generalizaÃ§Ãµes, como a **AnÃ¡lise Discriminante FlexÃ­vel (FDA)**, buscam encontrar projeÃ§Ãµes que maximizem a separaÃ§Ã£o entre as classes. A formulaÃ§Ã£o da LDA se baseia na maximizaÃ§Ã£o da razÃ£o entre a dispersÃ£o entre classes e a dispersÃ£o dentro das classes, enquanto a FDA utiliza a regressÃ£o para encontrar *scores* Ã³timos que maximizem a relaÃ§Ã£o entre as classes e as *features*. Neste capÃ­tulo, vamos explorar uma visÃ£o mais geral desse problema, utilizando um **critÃ©rio de otimizaÃ§Ã£o baseado em scores**, e como esse critÃ©rio se relaciona com o conceito de **correlaÃ§Ãµes canÃ´nicas penalizadas**.

A conexÃ£o entre a FDA e as correlaÃ§Ãµes canÃ´nicas penalizadas oferece uma perspectiva teÃ³rica mais profunda sobre o funcionamento da FDA e como ela generaliza a LDA. Analisaremos como a utilizaÃ§Ã£o de scores permite que o modelo se adapte a diferentes tipos de dados e como a regularizaÃ§Ã£o Ã© utilizada para obter soluÃ§Ãµes mais estÃ¡veis e com melhor capacidade de generalizaÃ§Ã£o. A compreensÃ£o dessas conexÃµes Ã© fundamental para o desenvolvimento e aplicaÃ§Ã£o de mÃ©todos de classificaÃ§Ã£o avanÃ§ados.

### Um CritÃ©rio de OtimizaÃ§Ã£o Generalizado Baseado em Scores

**Conceito 1: Scores e a MaximizaÃ§Ã£o da RelaÃ§Ã£o entre Classes e Features**

Em muitos problemas de classificaÃ§Ã£o, o objetivo Ã© encontrar uma representaÃ§Ã£o dos dados onde a separaÃ§Ã£o entre as classes seja maximizada. Uma abordagem geral para esse problema Ã© atravÃ©s da definiÃ§Ã£o de **scores** para cada classe, e encontrar uma transformaÃ§Ã£o dos dados que maximize a relaÃ§Ã£o entre esses *scores* e as *features*. A relaÃ§Ã£o entre os *scores* e as *features* pode ser medida como uma correlaÃ§Ã£o ou qualquer medida de associaÃ§Ã£o.

Formalmente, seja $\theta(g)$ um *score* associado Ã  classe $g$, e seja $\eta(x)$ uma funÃ§Ã£o que mapeia um vetor de *features* $x$ para um valor no espaÃ§o dos *scores*. O objetivo da otimizaÃ§Ã£o passa a ser:

$$ \max_{\eta, \theta} \text{RelaÃ§Ã£o}(\theta(g), \eta(x)) $$

onde $\text{RelaÃ§Ã£o}$ Ã© uma funÃ§Ã£o que mede a relaÃ§Ã£o entre os *scores* e a funÃ§Ã£o de mapeamento. Diferentes formas para a funÃ§Ã£o $\text{RelaÃ§Ã£o}$ levam a diferentes abordagens para a projeÃ§Ã£o das amostras.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos duas classes, $g_1$ e $g_2$, e definimos seus scores como $\theta(g_1) = -1$ e $\theta(g_2) = 1$. Temos tambÃ©m duas features, $x_1$ e $x_2$, e queremos encontrar uma funÃ§Ã£o $\eta(x) = w_1x_1 + w_2x_2$ que mapeie as features para o espaÃ§o de scores. O objetivo Ã© encontrar os pesos $w_1$ e $w_2$ que maximizem a relaÃ§Ã£o entre $\theta(g)$ e $\eta(x)$.
>
> Digamos que temos trÃªs amostras de cada classe:
>
> - Classe $g_1$: $x^{(1)} = [1, 2]$, $x^{(2)} = [2, 1]$, $x^{(3)} = [1, 1]$
> - Classe $g_2$: $x^{(4)} = [3, 4]$, $x^{(5)} = [4, 3]$, $x^{(6)} = [4, 4]$
>
> Vamos escolher pesos iniciais, por exemplo, $w = [0.5, 0.5]$. Calculamos entÃ£o os valores de $\eta(x)$ para cada amostra:
>
> - $\eta(x^{(1)}) = 0.5 * 1 + 0.5 * 2 = 1.5$
> - $\eta(x^{(2)}) = 0.5 * 2 + 0.5 * 1 = 1.5$
> - $\eta(x^{(3)}) = 0.5 * 1 + 0.5 * 1 = 1.0$
> - $\eta(x^{(4)}) = 0.5 * 3 + 0.5 * 4 = 3.5$
> - $\eta(x^{(5)}) = 0.5 * 4 + 0.5 * 3 = 3.5$
> - $\eta(x^{(6)}) = 0.5 * 4 + 0.5 * 4 = 4.0$
>
> Agora, queremos ajustar $w$ para que $\eta(x)$ se aproxime dos scores $\theta(g)$. Se usarmos a correlaÃ§Ã£o como a funÃ§Ã£o `RelaÃ§Ã£o`, o objetivo seria encontrar $w$ que maximize a correlaÃ§Ã£o entre os valores de $\eta(x)$ e os scores $\theta(g)$. Este Ã© o tipo de problema que FDA tenta resolver, buscando os pesos que melhor separam as classes no espaÃ§o de features transformado. Em uma iteraÃ§Ã£o do algoritmo de FDA, os pesos seriam ajustados de forma a aproximar os valores de $\eta(x)$ dos scores.

**Lemma 1:** Um critÃ©rio de otimizaÃ§Ã£o baseado em *scores* busca encontrar um espaÃ§o de representaÃ§Ã£o dos dados onde a relaÃ§Ã£o entre as classes e as *features* seja maximizada, e esta formulaÃ§Ã£o oferece uma generalizaÃ§Ã£o de outros modelos de classificaÃ§Ã£o.

A demonstraÃ§Ã£o desse lemma se baseia na anÃ¡lise da definiÃ§Ã£o do critÃ©rio de otimizaÃ§Ã£o e como ele busca encontrar funÃ§Ãµes de *scores* e *features* que estejam relacionadas de forma ideal, de forma que o problema de classificaÃ§Ã£o possa ser expresso em termos de uma funÃ§Ã£o de mapeamento que esteja bem relacionada com as classes.

**Conceito 2: A FDA e a MaximizaÃ§Ã£o de Scores via RegressÃ£o**

A **AnÃ¡lise Discriminante FlexÃ­vel (FDA)** utiliza o conceito de *scores* Ã³timos para generalizar a LDA. Como visto em capÃ­tulos anteriores, a FDA utiliza a regressÃ£o nÃ£o paramÃ©trica para modelar a relaÃ§Ã£o entre as *features* e uma matriz de resposta indicadora, onde cada coluna indica a pertinÃªncia a uma dada classe.

A FDA tambÃ©m busca encontrar scores $\theta_l(g)$, para $l = 1, ..., L$, onde $L$ Ã© o nÃºmero de projeÃ§Ãµes discriminantes desejadas, de forma a maximizar a associaÃ§Ã£o entre as classes e as *features*. Para isso, FDA utiliza uma projeÃ§Ã£o do espaÃ§o de *features* que Ã© funÃ§Ã£o de uma regressÃ£o:

$$ \eta_l(x) = W_l^T x $$

onde $W_l$ Ã© um vetor de coeficientes a serem otimizados, que tem o papel de projetar as amostras no novo espaÃ§o de *features*. O objetivo da FDA Ã© encontrar os scores $\theta_l(g)$ e os vetores de projeÃ§Ã£o $W_l$ que maximizem a relaÃ§Ã£o entre os scores e as projeÃ§Ãµes dos dados, que Ã© definido como a mÃ©dia do resÃ­duo quadrado (ASR) que deve ser minimizada:

$$ \text{ASR} = \sum_{l=1}^{L} \sum_{i=1}^{N} (\theta_l(g_i) - \eta_l(x_i))^2 $$

Essa formulaÃ§Ã£o mostra que a FDA, ao utilizar regressÃ£o, busca por um espaÃ§o de scores Ã³timos que melhor se relacionem com a projeÃ§Ã£o das amostras sobre um espaÃ§o transformado.

```mermaid
graph LR
    subgraph "FDA Score Optimization"
        direction TB
        A["Features: x"] --> B["Projection: Î·_l(x) = W_l^T x"]
        C["Class Scores: Î¸_l(g)"] --> D["Objective: Minimize ASR"]
        B --> D
        D --> E["Optimal Scores and Projection Vectors"]
    end
```

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Consideremos um problema de classificaÃ§Ã£o com trÃªs classes ($g_1, g_2, g_3$) e duas *features* ($x_1, x_2$).  Vamos usar $L=1$ para simplificar, ou seja, queremos encontrar uma Ãºnica projeÃ§Ã£o discriminante.
>
> Primeiro, criamos uma matriz de *scores* $\Theta$ onde cada linha representa uma amostra e cada coluna representa um score para cada classe. Para simplificar, vamos usar uma codificaÃ§Ã£o one-hot para as classes. Se tivermos 6 amostras, com as 2 primeiras na classe $g_1$, as 2 seguintes em $g_2$ e as 2 Ãºltimas em $g_3$, a matriz $\Theta$ seria:
>
> ```
>  [[1, 0, 0],
>   [1, 0, 0],
>   [0, 1, 0],
>   [0, 1, 0],
>   [0, 0, 1],
>   [0, 0, 1]]
> ```
>
>  Para $L=1$, os scores seriam $\theta_1(g_1)$, $\theta_1(g_2)$ e $\theta_1(g_3)$. Podemos inicializar com valores simples, por exemplo, $\theta_1(g_1) = 1$, $\theta_1(g_2) = 2$, e $\theta_1(g_3) = 3$. A matriz de scores $\Theta$ Ã© entÃ£o:
>
> ```
>  [[1],
>   [1],
>   [2],
>   [2],
>   [3],
>   [3]]
> ```
>
> Suponha que temos os seguintes dados de *features* $X$:
>
> ```
> [[1, 2],
>  [1.5, 2.5],
>  [3, 4],
>  [3.5, 4.5],
>  [5, 6],
>  [5.5, 6.5]]
> ```
>
> A FDA busca encontrar $W_1 = [w_1, w_2]$ que minimize o ASR. Inicializamos com valores aleatÃ³rios, por exemplo, $W_1 = [0.5, 0.5]$. Calculamos $\eta_1(x_i) = W_1^T x_i$ para cada amostra:
>
> - $\eta_1(x_1) = 0.5*1 + 0.5*2 = 1.5$
> - $\eta_1(x_2) = 0.5*1.5 + 0.5*2.5 = 2.0$
> - $\eta_1(x_3) = 0.5*3 + 0.5*4 = 3.5$
> - $\eta_1(x_4) = 0.5*3.5 + 0.5*4.5 = 4.0$
> - $\eta_1(x_5) = 0.5*5 + 0.5*6 = 5.5$
> - $\eta_1(x_6) = 0.5*5.5 + 0.5*6.5 = 6.0$
>
>  Agora podemos calcular o ASR:
>
> $ASR = (1-1.5)^2 + (1-2)^2 + (2-3.5)^2 + (2-4)^2 + (3-5.5)^2 + (3-6)^2 = 0.25+1+2.25+4+6.25+9 = 22.75$
>
> O objetivo da FDA Ã© iterativamente ajustar $W_1$ e os scores $\theta_l(g)$ para minimizar o ASR. Em cada iteraÃ§Ã£o, a regressÃ£o Ã© usada para ajustar os pesos $W_1$, buscando um espaÃ§o transformado onde os scores das classes se relacionem melhor com a projeÃ§Ã£o das amostras. Os valores dos scores podem ser ajustados tambÃ©m, para melhor se adequar Ã  projeÃ§Ã£o.

**CorolÃ¡rio 1:** A FDA utiliza regressÃ£o para modelar a relaÃ§Ã£o entre as classes e as *features*, e busca por scores que maximizam a associaÃ§Ã£o entre esses dois espaÃ§os, o que se demonstra atravÃ©s da minimizaÃ§Ã£o do resÃ­duo quadrado.

A demonstraÃ§Ã£o desse corolÃ¡rio se baseia na anÃ¡lise da formulaÃ§Ã£o da FDA e como a minimizaÃ§Ã£o da funÃ§Ã£o de custo de regressÃ£o busca um espaÃ§o transformado onde os *scores* das classes e a projeÃ§Ã£o das *features* estÃ£o mais fortemente relacionadas.

### CorrelaÃ§Ãµes CanÃ´nicas Penalizadas e a FDA

```mermaid
graph LR
    subgraph "PCCA and FDA Relationship"
        direction TB
        A["Features: X"] --> B["Projection: XW"]
        C["Class Matrix: Y"] --> D["Projection: YV"]
        B & D --> E["Maximize Correlation: Corr(XW, YV)"]
        E --> F["Penalize Complexity: -Î»(||W||Â² + ||V||Â²)"]
        F --> G["PCCA Objective"]
    end
    subgraph "FDA Perspective"
       direction TB
       H["FDA uses regression"]-->I["FDA minimizes ASR"]
       I --> J["FDA can be seen as a PCCA approximation"]
   end
    G-->J
```

A conexÃ£o entre a FDA e as **correlaÃ§Ãµes canÃ´nicas penalizadas** oferece uma perspectiva teÃ³rica alternativa sobre a formulaÃ§Ã£o da FDA. As correlaÃ§Ãµes canÃ´nicas (CCA) buscam encontrar as relaÃ§Ãµes lineares entre dois conjuntos de variÃ¡veis, por exemplo, entre as *features* $X$ e as classes $Y$. As correlaÃ§Ãµes canÃ´nicas penalizadas (PCCA) adicionam um termo de penalizaÃ§Ã£o para controlar a complexidade das soluÃ§Ãµes, o que as torna mais estÃ¡veis e com melhor capacidade de generalizaÃ§Ã£o.

Em essÃªncia, a PCCA busca encontrar projeÃ§Ãµes para os dados $X$ e para a matriz de respostas $Y$ que maximizam a correlaÃ§Ã£o entre os dois conjuntos de variÃ¡veis, ao mesmo tempo em que penaliza a complexidade dos modelos de projeÃ§Ã£o. A PCCA Ã© formulada como:

$$ \max_{W, V} \text{Corr}(XW, YV) - \lambda (||W||^2 + ||V||^2) $$

onde $W$ e $V$ sÃ£o as matrizes de projeÃ§Ã£o para $X$ e $Y$, respectivamente, e $\lambda$ Ã© um parÃ¢metro de regularizaÃ§Ã£o.

A formulaÃ§Ã£o da FDA pode ser vista como uma aproximaÃ§Ã£o da PCCA, onde os *scores* $\theta(g)$ representam os resultados da projeÃ§Ã£o em $Y$, a funÃ§Ã£o de regressÃ£o $\eta(x)$ representa a projeÃ§Ã£o em $X$, e o termo de penalizaÃ§Ã£o da FDA contribui para controlar a complexidade dos modelos de projeÃ§Ã£o, com mecanismos de regularizaÃ§Ã£o que penalizam projeÃ§Ãµes com alta variabilidade nos coeficientes.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos considerar um exemplo simplificado para ilustrar a PCCA. Suponha que temos um conjunto de *features* $X$ e uma matriz de classes $Y$, ambas com 3 amostras e 2 variÃ¡veis:
>
> $X = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \end{bmatrix}$
>
> $Y = \begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 0 & 1 \end{bmatrix}$
>
> O objetivo da PCCA Ã© encontrar as matrizes de projeÃ§Ã£o $W$ e $V$ que maximizem a correlaÃ§Ã£o entre $XW$ e $YV$, penalizando a complexidade de $W$ e $V$.
>
> Seja $W = \begin{bmatrix} w_{11} & w_{12} \\ w_{21} & w_{22} \end{bmatrix}$ e $V = \begin{bmatrix} v_{11} & v_{12} \\ v_{21} & v_{22} \end{bmatrix}$.
>
> A PCCA busca maximizar:
>
> $$ \text{Corr}(XW, YV) - \lambda (||W||^2 + ||V||^2) $$
>
> Inicialmente, vamos simplificar usando $W = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ e $V = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.
>
> EntÃ£o, $XW = X$ e $YV = Y$.
>
> Vamos calcular a correlaÃ§Ã£o entre as colunas de $X$ e $Y$.
>
> $XW = X = \begin{bmatrix} 1 & 2 \\ 2 & 1 \\ 3 & 3 \end{bmatrix}$
>
> $YV = Y = \begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 0 & 1 \end{bmatrix}$
>
>  Agora, vamos usar uma mÃ©trica simplificada para a correlaÃ§Ã£o, como a soma dos produtos das colunas:
>
>  $Corr = (1*1 + 2*1 + 3*0) + (2*0 + 1*1 + 3*1) = 3 + 4 = 7$
>
> Para calcular a norma de $W$ e $V$, vamos usar a norma de Frobenius:
>
> $||W||^2 = 1^2 + 0^2 + 0^2 + 1^2 = 2$
>
> $||V||^2 = 1^2 + 0^2 + 0^2 + 1^2 = 2$
>
> O custo a ser maximizado Ã© entÃ£o: $7 - \lambda (2 + 2) = 7 - 4\lambda$.
>
> Se $\lambda = 0$, o problema se reduz a maximizar a correlaÃ§Ã£o. Se $\lambda$ for grande, a penalizaÃ§Ã£o domina e a soluÃ§Ã£o tende a ser com valores menores em $W$ e $V$.
>
> A PCCA iterativamente ajustaria $W$ e $V$ para maximizar a correlaÃ§Ã£o entre $XW$ e $YV$, enquanto penaliza a complexidade das matrizes de projeÃ§Ã£o. A escolha de $\lambda$ Ã© crucial para equilibrar a correlaÃ§Ã£o e a complexidade do modelo. No contexto da FDA, o termo de penalizaÃ§Ã£o da FDA (como a regularizaÃ§Ã£o) desempenha um papel semelhante ao termo de penalizaÃ§Ã£o da PCCA, controlando a complexidade dos modelos de projeÃ§Ã£o para evitar overfitting.

**Lemma 3:** A FDA pode ser vista como uma aproximaÃ§Ã£o da PCCA, e a busca por *scores* Ã³timos na FDA corresponde Ã  maximizaÃ§Ã£o das correlaÃ§Ãµes canÃ´nicas penalizadas entre as *features* e as classes.

A demonstraÃ§Ã£o desse lemma envolve analisar a formulaÃ§Ã£o da PCCA e mostrar como ela se relaciona com o problema de otimizaÃ§Ã£o da FDA, e como ambas as tÃ©cnicas buscam encontrar projeÃ§Ãµes que maximizam a relaÃ§Ã£o entre as variÃ¡veis, com o uso de termos de penalizaÃ§Ã£o para controlar a complexidade.

### A InterpretaÃ§Ã£o da FDA como uma GeneralizaÃ§Ã£o da LDA atravÃ©s de Scores

```mermaid
graph LR
    subgraph "LDA vs FDA"
        direction TB
        A["Linear Discriminant Analysis (LDA)"] --> B["Linear Projections"]
        A --> C["Single score per class"]
         B & C --> D["Maximizing separation of class means"]
        E["Flexible Discriminant Analysis (FDA)"] --> F["Flexible Scores via Regression"]
        E --> G["Multiple scores to maximize association"]
        F & G --> H["Handles more complex data"]
    end
```

A conexÃ£o entre a FDA e as **correlaÃ§Ãµes canÃ´nicas penalizadas** ilustra como a FDA generaliza a **AnÃ¡lise Discriminante Linear (LDA)**. Ambas as abordagens buscam encontrar projeÃ§Ãµes que maximizem a separaÃ§Ã£o entre as classes, mas a LDA utiliza projeÃ§Ãµes lineares, enquanto a FDA utiliza *scores* mais flexÃ­veis.

A LDA busca encontrar um subespaÃ§o onde as mÃ©dias das classes estÃ£o mais separadas, utilizando um Ãºnico score por classe e uma projeÃ§Ã£o linear sobre os dados originais. A FDA generaliza essa abordagem, modelando a relaÃ§Ã£o entre as *features* e as classes de forma mais flexÃ­vel atravÃ©s da regressÃ£o nÃ£o paramÃ©trica, e usando projeÃ§Ãµes sobre um espaÃ§o de dimensÃ£o menor ou igual ao nÃºmero de classes, que maximizam a separaÃ§Ã£o entre elas. A FDA tambÃ©m relaxa as premissas da LDA sobre a distribuiÃ§Ã£o dos dados, permitindo que o mÃ©todo seja aplicado a problemas mais complexos.

A utilizaÃ§Ã£o de scores na FDA permite um controle maior sobre o processo de modelagem e a escolha dos parÃ¢metros de otimizaÃ§Ã£o, e, por isso, a FDA permite um comportamento mais robusto e com menor risco de *overfitting* do que a LDA para dados complexos.

**CorolÃ¡rio 3:** A FDA generaliza a LDA ao utilizar um critÃ©rio de otimizaÃ§Ã£o mais geral baseado em scores, e ao usar modelos de regressÃ£o flexÃ­vel para modelar a relaÃ§Ã£o entre esses scores e os dados, o que possibilita lidar com dados mais complexos.

A demonstraÃ§Ã£o desse corolÃ¡rio se baseia na anÃ¡lise da formulaÃ§Ã£o da LDA e da FDA, e como a FDA utiliza conceitos de regressÃ£o flexÃ­vel e um modelo baseado em *scores* para generalizar a LDA e construir modelos mais adaptÃ¡veis aos dados.

### ConclusÃ£o

Neste capÃ­tulo, exploramos a conexÃ£o entre a **AnÃ¡lise Discriminante FlexÃ­vel (FDA)** e as **correlaÃ§Ãµes canÃ´nicas penalizadas**, e como a FDA busca **scores Ã³timos** para maximizar a relaÃ§Ã£o entre as classes e as *features*. Vimos como a FDA pode ser vista como uma generalizaÃ§Ã£o da LDA que utiliza um critÃ©rio de otimizaÃ§Ã£o mais geral baseado em scores, utilizando projeÃ§Ãµes nÃ£o lineares para modelar a relaÃ§Ã£o entre as *features* e as classes.

Discutimos como a utilizaÃ§Ã£o de *scores* permite que o modelo se adapte a diferentes tipos de dados e como a regularizaÃ§Ã£o Ã© utilizada para obter modelos mais estÃ¡veis e com melhor capacidade de generalizaÃ§Ã£o. A conexÃ£o da FDA com a PCCA proporciona uma base teÃ³rica sÃ³lida para a compreensÃ£o da formulaÃ§Ã£o do mÃ©todo, e como ele busca as melhores projeÃ§Ãµes sobre os dados originais.

A compreensÃ£o da relaÃ§Ã£o entre a FDA, scores Ã³timos e correlaÃ§Ãµes canÃ´nicas penalizadas Ã© fundamental para a utilizaÃ§Ã£o avanÃ§ada da FDA e para a construÃ§Ã£o de modelos mais robustos em problemas de classificaÃ§Ã£o com dados complexos.

### Footnotes

[^12.1]: "In this chapter we describe generalizations of linear decision boundaries for classification. Optimal separating hyperplanes are introduced in Chapter 4 for the case when two classes are linearly separable. Here we cover extensions to the nonseparable case, where the classes overlap. These techniques are then generalized to what is known as the support vector machine, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.2]: "In Chapter 4 we discussed a technique for constructing an optimal separating hyperplane between two perfectly separated classes. We review this and generalize to the nonseparable case, where the classes may not be separable by a linear boundary." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*
[^12.4]: "In the remainder of this chapter we describe a class of techniques that attend to all these issues by generalizing the LDA model. This is achieved largely by three different ideas." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*
[^12.5]:  "In this section we describe a method for performing LDA using linear re-gression on derived responses." *(Trecho de "Support Vector Machines and Flexible Discriminants")*
