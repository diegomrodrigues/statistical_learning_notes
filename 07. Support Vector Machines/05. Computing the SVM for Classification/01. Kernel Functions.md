Okay, here's the text with all mathematical expressions formatted using LaTeX notation:

## T√≠tulo: Fun√ß√µes Kernel em SVMs: Propriedades, Tipos e Aplica√ß√µes

```mermaid
graph LR
    subgraph "Kernel Function and SVM"
        direction TB
        A["Input Data (x)"] --> B{"Kernel Function K(x, x')"}
        B --> C["Transformed Feature Space (phi(x))"]
        C --> D["SVM Algorithm"]
        D --> E["Nonlinear Decision Boundary"]
        style B fill:#f9f,stroke:#333,stroke-width:2px
    end
```

### Introdu√ß√£o

No estudo das **Support Vector Machines (SVMs)**, as **fun√ß√µes kernel** s√£o elementos fundamentais que possibilitam a constru√ß√£o de modelos n√£o lineares sem a necessidade de calcular explicitamente a transforma√ß√£o dos dados para um espa√ßo de alta dimens√£o. A escolha da fun√ß√£o *kernel* adequada √© crucial para o desempenho do modelo, e a compreens√£o de suas propriedades e de como elas afetam a modelagem dos dados √© essencial para a aplica√ß√£o pr√°tica das SVMs.

Neste cap√≠tulo, exploraremos em profundidade as fun√ß√µes *kernel*, analisando suas propriedades, os tipos mais comuns de *kernels* utilizados em SVMs e como cada tipo de *kernel* influencia a complexidade da fronteira de decis√£o e a capacidade de generaliza√ß√£o do modelo. Analisaremos tamb√©m como a escolha dos par√¢metros de cada *kernel* afeta a sua capacidade de modelar diferentes padr√µes nos dados. A compreens√£o das fun√ß√µes kernel, juntamente com a escolha apropriada dos par√¢metros e da regulariza√ß√£o, √© crucial para a aplica√ß√£o bem sucedida de SVMs em uma variedade de problemas pr√°ticos.

### Propriedades Fundamentais das Fun√ß√µes Kernel

**Conceito 1: Simetria e Positividade Semidefinida**

Como discutido em cap√≠tulos anteriores, para que uma fun√ß√£o $K(x, x')$ seja considerada um **kernel v√°lido**, ela deve satisfazer duas propriedades fundamentais:

1.  **Simetria:** A fun√ß√£o *kernel* deve ser sim√©trica, ou seja, deve satisfazer a propriedade:
    $$ K(x, x') = K(x', x) $$
    Essa propriedade garante que a ordem dos vetores de *features* n√£o afete o resultado do c√°lculo do *kernel*.

> üí° **Exemplo Num√©rico:**
>
>   Suponha que temos dois vetores de *features*, $x = [1, 2]$ e $x' = [3, 4]$.  Se usarmos um *kernel* linear, $K(x, x') = x^T x'$, ent√£o:
>
>   $K(x, x') = [1, 2] \cdot [3, 4] = (1*3) + (2*4) = 3 + 8 = 11$
>
>   $K(x', x) = [3, 4] \cdot [1, 2] = (3*1) + (4*2) = 3 + 8 = 11$
>
>   Como $K(x, x') = K(x', x) = 11$, a propriedade de simetria √© confirmada.

2.  **Positividade Semidefinida:** A matriz *kernel* $K$, definida por $K_{ij} = K(x_i, x_j)$, deve ser semidefinida positiva. Isso significa que para qualquer conjunto finito de vetores de *features* $\{x_1, x_2, \ldots, x_N\}$, todos os autovalores da matriz $K$ devem ser n√£o negativos.
     A propriedade de positividade semidefinida √© uma condi√ß√£o necess√°ria para garantir que a fun√ß√£o *kernel* corresponda a um produto interno em algum espa√ßo de Hilbert.

> üí° **Exemplo Num√©rico:**
>
>   Vamos considerar tr√™s vetores: $x_1 = [1, 0]$, $x_2 = [0, 1]$, e $x_3 = [1, 1]$. Usando o *kernel* linear, podemos construir a matriz *kernel* K:
>
>   $K_{11} = K(x_1, x_1) = [1, 0] \cdot [1, 0] = 1$
>   $K_{12} = K(x_1, x_2) = [1, 0] \cdot [0, 1] = 0$
>   $K_{13} = K(x_1, x_3) = [1, 0] \cdot [1, 1] = 1$
>   $K_{21} = K(x_2, x_1) = 0$
>   $K_{22} = K(x_2, x_2) = 1$
>   $K_{23} = K(x_2, x_3) = 1$
>   $K_{31} = K(x_3, x_1) = 1$
>   $K_{32} = K(x_3, x_2) = 1$
>   $K_{33} = K(x_3, x_3) = 2$
>
>  A matriz *kernel* K √©:
> $$
> K = \begin{bmatrix}
> 1 & 0 & 1 \\
> 0 & 1 & 1 \\
> 1 & 1 & 2
> \end{bmatrix}
> $$
>
> Usando Python e NumPy para verificar os autovalores:
> ```python
> import numpy as np
>
> K = np.array([[1, 0, 1],
>              [0, 1, 1],
>              [1, 1, 2]])
>
> eigenvalues = np.linalg.eigvals(K)
> print(eigenvalues)
> ```
>
> Os autovalores calculados s√£o aproximadamente [3.  , 1. , 0.  ]. Todos s√£o n√£o-negativos, confirmando que a matriz √© semidefinida positiva.

```mermaid
graph LR
    subgraph "Kernel Properties"
        direction TB
        A["Kernel Function K(x, x')"]
        B["Symmetry: K(x, x') = K(x', x)"]
        C["Positive Semidefinite Matrix K"]
        A --> B
        A --> C
        style B fill:#ccf,stroke:#333,stroke-width:2px
        style C fill:#ccf,stroke:#333,stroke-width:2px
    end
```

**Lemma 1:** As propriedades de simetria e positividade semidefinida s√£o condi√ß√µes necess√°rias e suficientes para que uma fun√ß√£o $K(x, x')$ seja considerada um *kernel* v√°lido.

A demonstra√ß√£o desse lemma se baseia no **Teorema de Mercer**, que estabelece que uma fun√ß√£o √© um *kernel* v√°lido se e somente se ela √© sim√©trica e semidefinida positiva. Essas condi√ß√µes garantem que existe uma transforma√ß√£o $\phi(x)$ para um espa√ßo de alta dimens√£o tal que $K(x, x') = \langle \phi(x), \phi(x') \rangle$, onde $\langle \cdot, \cdot \rangle$ denota um produto interno no espa√ßo de Hilbert, com dimens√£o possivelmente infinita.

**Conceito 2: O "Kernel Trick" e a Transforma√ß√£o Impl√≠cita**

O "kernel trick" √© o mecanismo fundamental que permite que as SVMs operem em espa√ßos de *features* de alta dimens√£o sem calcular explicitamente a transforma√ß√£o dos dados para esses espa√ßos. As fun√ß√µes *kernel* calculam o produto interno entre as transforma√ß√µes dos dados, $\langle \phi(x), \phi(x') \rangle$, sem a necessidade de explicitar a transforma√ß√£o $\phi(x)$. Isso torna o c√°lculo da fun√ß√£o de decis√£o computacionalmente eficiente, mesmo em espa√ßos de *features* com dimens√£o muito alta, e at√© mesmo infinita.

```mermaid
graph LR
    subgraph "Kernel Trick"
       direction TB
        A["Input vectors (x, x')"] --> B{"Kernel Function K(x, x')"}
        B --> C["Implicit Transformation: <phi(x), phi(x')>"]
        C --> D["No Explicit Calculation of phi(x)"]
        style B fill:#fcc,stroke:#333,stroke-width:2px
    end
```

O *kernel trick* √© uma das principais raz√µes pela qual as SVMs s√£o t√£o poderosas, e permite que o modelo se adapte a rela√ß√µes n√£o lineares complexas entre os dados. Ao utilizar um *kernel* adequado, o modelo SVM pode construir fronteiras de decis√£o n√£o lineares no espa√ßo original das *features*, sem a necessidade de explicitar a transforma√ß√£o.

**Corol√°rio 1:** O *kernel trick* possibilita que as SVMs operem em espa√ßos de *features* de alta dimens√£o sem calcular explicitamente a transforma√ß√£o, tornando a modelagem de rela√ß√µes n√£o lineares computacionalmente eficiente.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise da formula√ß√£o do problema dual das SVMs e como a fun√ß√£o *kernel* substitui o produto interno no espa√ßo original por um produto interno em um espa√ßo transformado de forma impl√≠cita.

### Tipos Comuns de Fun√ß√µes Kernel

```mermaid
graph LR
    subgraph "Common Kernel Types"
        direction TB
        A["Linear Kernel: K(x, x') = x^T x'"]
        B["Polynomial Kernel: K(x, x') = (x^T x' + c)^d"]
        C["RBF Kernel: K(x, x') = exp(-gamma ||x - x'||^2)"]
        D["Sigmoid Kernel: K(x, x') = tanh(kappa1 x^T x' + kappa2)"]
    end
    A -->| "Linear Decision Boundary" |E
    B --> | "Polynomial Decision Boundary" |E
    C --> | "Complex Localized Decision Boundary"|E
    D --> | "Sigmoidal Decision Boundary" |E
    E["Decision Boundary"]
    style A fill:#eef,stroke:#333,stroke-width:2px
    style B fill:#eef,stroke:#333,stroke-width:2px
    style C fill:#eef,stroke:#333,stroke-width:2px
    style D fill:#eef,stroke:#333,stroke-width:2px
```

Existem diversas fun√ß√µes *kernel* que s√£o utilizadas em SVMs, cada uma com suas propriedades e capacidade de modelar diferentes tipos de rela√ß√µes n√£o lineares. Alguns dos *kernels* mais comuns incluem:

1.  **Kernel Linear:**
    O *kernel* linear √© dado por:
    $$ K(x, x') = x^T x' $$
    onde $x^T x'$ √© o produto interno entre os vetores de *features* $x$ e $x'$ no espa√ßo original. Esse *kernel* corresponde a uma transforma√ß√£o de identidade e, portanto, as SVMs com *kernel* linear geram fronteiras de decis√£o lineares no espa√ßo original.

> üí° **Exemplo Num√©rico:**
>
>   Se temos dois pontos $x = [2, 3]$ e $x' = [1, -1]$, o *kernel* linear calcula:
>   $K(x, x') = [2, 3] \cdot [1, -1] = (2*1) + (3*(-1)) = 2 - 3 = -1$.
>
>   A fronteira de decis√£o criada por um SVM com *kernel* linear ser√° uma linha reta (em 2D) ou um hiperplano (em dimens√µes maiores).

2.  **Kernel Polinomial:**
    O *kernel* polinomial √© dado por:
    $$ K(x, x') = (x^T x' + c)^d $$
    onde $c$ √© uma constante e $d$ √© o grau do polin√¥mio. Esse *kernel* mapeia os dados para um espa√ßo de *features* de maior dimens√£o, incluindo termos como produtos e pot√™ncias das *features* originais, o que possibilita a modelagem de rela√ß√µes n√£o lineares em forma de polin√¥mios. O par√¢metro $d$ controla a complexidade da fronteira de decis√£o gerada.

> üí° **Exemplo Num√©rico:**
>
>   Usando os mesmos pontos $x = [2, 3]$ e $x' = [1, -1]$, e definindo $c = 1$ e $d = 2$, o *kernel* polinomial calcula:
>
>   Primeiro, calculamos o produto interno: $x^T x' = -1$ (do exemplo anterior).
>
>   Ent√£o, $K(x, x') = (-1 + 1)^2 = 0^2 = 0$.
>
>   Se mudarmos para $d=3$, ent√£o $K(x,x') = (-1 + 1)^3 = 0^3 = 0$
>
>   Se usarmos $c=2$ e $d=2$, ent√£o $K(x,x') = (-1 + 2)^2 = 1^2 = 1$.
>
>  O grau $d$ aumenta a complexidade da fronteira de decis√£o.

3.  **Kernel Radial Basis Function (RBF) ou Gaussiano:**
    O *kernel* RBF √© dado por:
    $$ K(x, x') = \exp(-\gamma ||x - x'||^2) $$
     onde $\gamma > 0$ √© um par√¢metro que controla a largura da fun√ß√£o gaussiana. Esse *kernel* mapeia os dados para um espa√ßo de dimens√£o infinita, e √© capaz de modelar fronteiras de decis√£o complexas, com comportamento localizado. O par√¢metro $\gamma$ controla a influ√™ncia de cada ponto no espa√ßo de *features* transformado.

> üí° **Exemplo Num√©rico:**
>
>   Usando $x = [2, 3]$ e $x' = [1, -1]$ e $\gamma = 0.1$:
>
>   $||x - x'||^2 = ||[2, 3] - [1, -1]||^2 = ||[1, 4]||^2 = 1^2 + 4^2 = 17$
>
>   $K(x, x') = \exp(-0.1 * 17) = \exp(-1.7) \approx 0.1827$.
>
>   Se $\gamma$ for maior, a influ√™ncia de $x'$ em rela√ß√£o a $x$ diminui mais rapidamente com a dist√¢ncia.
>   Se usarmos $\gamma = 1$, ent√£o $K(x,x') = \exp(-1 * 17) = \exp(-17) \approx 0.000000000004$
>
>   Valores maiores de $\gamma$ tornam a fronteira de decis√£o mais irregular.

4.  **Kernel Sigm√≥ide:**
     O *kernel* sigm√≥ide √© dado por:
     $$ K(x, x') = \tanh(\kappa_1 x^T x' + \kappa_2) $$
     onde $\kappa_1$ e $\kappa_2$ s√£o par√¢metros que controlam a forma da fun√ß√£o sigmoide. O *kernel* sigm√≥ide pode gerar fronteiras de decis√£o n√£o lineares, mas n√£o sempre satisfaz as condi√ß√µes de ser semidefinida positiva, o que pode limitar sua aplica√ß√£o.

> üí° **Exemplo Num√©rico:**
>
>   Usando $x = [2, 3]$ e $x' = [1, -1]$, e definindo $\kappa_1 = 0.5$ e $\kappa_2 = 1$:
>
>   $x^T x' = -1$ (do exemplo anterior).
>
>   $K(x, x') = \tanh(0.5 * (-1) + 1) = \tanh(0.5) \approx 0.462$.
>
>   Os par√¢metros $\kappa_1$ e $\kappa_2$ ajustam a inclina√ß√£o e o deslocamento da fun√ß√£o sigmoide.

**Lemma 2:** Cada tipo de *kernel* possui propriedades espec√≠ficas que influenciam a forma da fronteira de decis√£o e a capacidade do modelo SVM de capturar rela√ß√µes n√£o lineares nos dados.

A demonstra√ß√£o desse lemma se baseia na an√°lise matem√°tica de cada tipo de *kernel* e como eles mapeiam os dados para um espa√ßo de *features* diferente. A escolha de um *kernel* espec√≠fico envolve considerar as caracter√≠sticas dos dados e as rela√ß√µes que se deseja modelar.

### A Escolha do Kernel e seus Par√¢metros

```mermaid
graph TB
   subgraph "Kernel and Parameter Selection"
        direction TB
        A["Data Analysis"] --> B["Kernel Choice (Linear, Polynomial, RBF, etc.)"]
        B --> C["Parameter Tuning (e.g., d, gamma, etc.)"]
        C --> D["Cross-Validation"]
        D --> E["Model Evaluation"]
        E --> F["Optimal Kernel and Parameters"]
   end
    style B fill:#aaf,stroke:#333,stroke-width:2px
    style C fill:#aaf,stroke:#333,stroke-width:2px
    style D fill:#aaf,stroke:#333,stroke-width:2px
```

A escolha do *kernel* e dos seus par√¢metros √© um passo crucial na aplica√ß√£o das SVMs, pois eles determinam a forma da fronteira de decis√£o e a capacidade de generaliza√ß√£o do modelo. A escolha apropriada do *kernel* e de seus par√¢metros depende da natureza dos dados e das rela√ß√µes que se deseja modelar.

A escolha do *kernel* √© feita com base em algumas considera√ß√µes:

*   **Kernel Linear:** Adequado para dados linearmente separ√°veis ou quando se deseja um modelo mais simples e interpret√°vel.
*   **Kernel Polinomial:** Adequado para problemas onde a rela√ß√£o entre as *features* e as classes pode ser expressa por polin√¥mios, e o par√¢metro $d$ controla a complexidade do modelo.
*   **Kernel RBF:** Adequado para modelar rela√ß√µes n√£o lineares complexas, e o par√¢metro $\gamma$ controla a escala da influ√™ncia dos pontos.
*   **Kernel Sigm√≥ide:** Usado como alternativa aos outros *kernels*, mas com restri√ß√µes quanto √† sua validade em alguns casos.

Al√©m da escolha do tipo de *kernel*, √© fundamental ajustar os seus par√¢metros, como o grau do polin√¥mio no *kernel* polinomial ou a largura da fun√ß√£o gaussiana no *kernel* RBF. A escolha dos par√¢metros do *kernel* √© geralmente feita utilizando t√©cnicas de valida√ß√£o cruzada, onde o desempenho do modelo √© avaliado para diferentes valores dos par√¢metros e o valor que maximiza o desempenho √© selecionado.

> üí° **Exemplo Num√©rico:**
>
>   Suponha que temos um problema de classifica√ß√£o n√£o linear e testamos um SVM com *kernel* RBF com diferentes valores de $\gamma$ usando valida√ß√£o cruzada com 5 folds:
>
>   | $\gamma$ | Acur√°cia M√©dia (Valida√ß√£o Cruzada) |
>   |----------|------------------------------------|
>   | 0.01     | 0.75                               |
>   | 0.1      | 0.88                               |
>   | 1        | 0.92                               |
>   | 10       | 0.85                               |
>
>   Neste caso, $\gamma = 1$ parece ser o valor que leva ao melhor desempenho no conjunto de valida√ß√£o, e seria o valor escolhido para o modelo final.

A combina√ß√£o da escolha apropriada do *kernel* e seus par√¢metros, juntamente com a regulariza√ß√£o (par√¢metro $C$), √© essencial para a constru√ß√£o de modelos SVM robustos e com boa capacidade de generaliza√ß√£o.

**Corol√°rio 2:** A escolha do *kernel* e seus par√¢metros √© um processo emp√≠rico, que envolve a an√°lise da natureza dos dados, a experimenta√ß√£o com diferentes *kernels* e par√¢metros, e a avalia√ß√£o do desempenho do modelo utilizando t√©cnicas de valida√ß√£o cruzada.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise das propriedades dos diferentes *kernels* e seus par√¢metros, e na sua rela√ß√£o com a complexidade do modelo. A valida√ß√£o cruzada √© uma ferramenta para estimar o desempenho do modelo com diferentes configura√ß√µes de par√¢metros e escolher aqueles que levam ao melhor desempenho nos dados n√£o vistos.

### Conex√£o com o Problema Dual e a Interpreta√ß√£o da Solu√ß√£o

```mermaid
graph LR
    subgraph "Dual Problem and Kernel Integration"
        direction TB
        A["Primal Problem"] --> B["Dual Problem Formulation"]
        B --> C{"Kernel Substitution: K(x_i, x_j)"}
        C --> D["Lagrange Multipliers (alpha_i)"]
        D --> E["Decision Function: f(x) = sum(alpha_i * y_i * K(x_i, x)) + b_0"]
        style C fill:#afa,stroke:#333,stroke-width:2px
    end
```

A utiliza√ß√£o das fun√ß√µes *kernel* se manifesta principalmente na formula√ß√£o do **problema dual** das SVMs. Ao substituir o produto interno $x^T x'$ pela fun√ß√£o *kernel* $K(x, x')$, o problema dual se torna:

$$ \max_{\alpha} \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j K(x_i, x_j) $$

sujeito a:

$$ 0 \leq \alpha_i \leq C, \quad \forall i $$
$$ \sum_{i=1}^{N} \alpha_i y_i = 0 $$

A solu√ß√£o desse problema dual nos fornece os multiplicadores de Lagrange $\alpha_i$, que s√£o utilizados para calcular o vetor $\beta$ e o *bias* $\beta_0$. A fun√ß√£o de decis√£o da SVM, com a utiliza√ß√£o do *kernel*, √© dada por:

$$ f(x) = \sum_{i \in SV} \alpha_i y_i K(x_i, x) + \beta_0 $$

onde $SV$ √© o conjunto de vetores de suporte. Essa equa√ß√£o demonstra que a fun√ß√£o de decis√£o depende apenas dos produtos internos entre os vetores de *features* e os vetores de suporte, calculados por meio da fun√ß√£o *kernel*.

> üí° **Exemplo Num√©rico:**
>
>   Suponha que ap√≥s resolver o problema dual, temos dois vetores de suporte, $x_1 = [1, 2]$ com $\alpha_1 = 0.5$, $y_1 = 1$ e $x_2 = [2, 1]$ com $\alpha_2 = 0.3$, $y_2 = -1$. O bias $\beta_0 = 0.1$. Se usarmos um kernel RBF com $\gamma = 0.2$, a fun√ß√£o de decis√£o para um novo ponto $x = [1.5, 1.5]$ seria:
>
>   $K(x_1, x) = \exp(-0.2 * ||[1, 2] - [1.5, 1.5]||^2) = \exp(-0.2 * (0.25 + 0.25)) = \exp(-0.1) \approx 0.904$
>   $K(x_2, x) = \exp(-0.2 * ||[2, 1] - [1.5, 1.5]||^2) = \exp(-0.2 * (0.25 + 0.25)) = \exp(-0.1) \approx 0.904$
>
>   $f(x) = 0.5 * 1 * 0.904 + 0.3 * (-1) * 0.904 + 0.1 = 0.452 - 0.2712 + 0.1 = 0.2808$.
>
>   Como $f(x) > 0$, o ponto $x$ seria classificado como da classe positiva.

Essa formula√ß√£o revela como os *kernels* s√£o integrados na solu√ß√£o das SVMs, permitindo que a n√£o linearidade seja modelada de forma eficiente, pois a transforma√ß√£o n√£o √© explicitamente calculada, apenas o produto interno no espa√ßo transformado √© obtido atrav√©s da fun√ß√£o *kernel*.

**Corol√°rio 3:** As fun√ß√µes *kernel* s√£o integradas na formula√ß√£o do problema dual das SVMs, substituindo o produto interno entre os dados de entrada e permitindo que o modelo construa fronteiras de decis√£o n√£o lineares no espa√ßo original dos dados.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise da formula√ß√£o do problema dual das SVMs e como a fun√ß√£o *kernel* √© utilizada como um substituto para o produto interno, e com isso a transforma√ß√£o dos dados para um espa√ßo de dimens√£o superior n√£o √© calculada diretamente.

### Conclus√£o

Neste cap√≠tulo, exploramos em profundidade as **fun√ß√µes *kernel*** em **Support Vector Machines (SVMs)**, analisando suas propriedades fundamentais, os tipos mais comuns de *kernels* utilizados na pr√°tica e como a escolha do *kernel* e de seus par√¢metros influencia a modelagem e a generaliza√ß√£o do modelo. Vimos como o "kernel trick" permite que as SVMs operem em espa√ßos de alta dimens√£o de forma eficiente, sem explicitamente calcular a transforma√ß√£o dos dados.

A compreens√£o das propriedades das fun√ß√µes *kernel* e da sua rela√ß√£o com o problema dual das SVMs √© fundamental para a utiliza√ß√£o bem-sucedida desse m√©todo em problemas complexos de classifica√ß√£o e regress√£o. A escolha do *kernel* e de seus par√¢metros √© um passo crucial para a constru√ß√£o de modelos robustos, com capacidade de modelar rela√ß√µes n√£o lineares e com boa capacidade de generaliza√ß√£o.

### Footnotes

[^12.1]: "In this chapter we describe generalizations of linear decision boundaries for classification. Optimal separating hyperplanes are introduced in Chapter 4 for the case when two classes are linearly separable. Here we cover extensions to the nonseparable case, where the classes overlap. These techniques are then generalized to what is known as the support vector machine, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.2]: "In Chapter 4 we discussed a technique for constructing an optimal separating hyperplane between two perfectly separated classes. We review this and generalize to the nonseparable case, where the classes may not be separable by a linear boundary." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.3]: "The support vector machine classifier is an extension of this idea, where the dimension of the enlarged space is allowed to get very large, infinite in some cases. It might seem that the computations would become prohibitive. It would also seem that with sufficient basis functions, the data would be separable, and overfitting would occur. We first show how the SVM technology deals with these issues. We then see that in fact the SVM classifier is solving a function-fitting problem using a particular criterion and form of regularization, and is part of a much bigger class of problems that includes the smoothing splines of Chapter 5." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*
