```markdown
## TÃ­tulo: Multiplicadores de Lagrange e a Dualidade de Wolfe em SVMs: Uma AnÃ¡lise Detalhada

```mermaid
graph LR
    subgraph "SVM Optimization Process"
        direction TB
        A["Primal Problem: Minimize Cost Function with Constraints"]
        B["Lagrangian Function: Combine Cost and Constraints with Multipliers"]
        C["Wolfe Duality: Transform Primal to Dual Problem"]
        D["Dual Problem: Maximize Lagrangian Dual with Constraints on Multipliers"]
        E["Solve Dual Problem: Obtain Optimal Lagrange Multipliers"]
        F["Support Vectors: Identify Data Points with Non-zero Multipliers"]
        G["Decision Function: Use Support Vectors to Classify New Data"]
        A --> B
        B --> C
        C --> D
        D --> E
        E --> F
        F --> G
    end
```

### IntroduÃ§Ã£o

No estudo das **Support Vector Machines (SVMs)**, a tÃ©cnica de **dualidade de Wolfe** e o uso de **multiplicadores de Lagrange** sÃ£o ferramentas essenciais para a compreensÃ£o da formulaÃ§Ã£o matemÃ¡tica e do processo de otimizaÃ§Ã£o. A dualidade de Wolfe permite transformar o problema primal das SVMs, que Ã© definido no espaÃ§o dos parÃ¢metros do modelo, em um problema dual, definido no espaÃ§o dos multiplicadores de Lagrange, o que simplifica a obtenÃ§Ã£o da soluÃ§Ã£o Ã³tima e revela propriedades importantes dos modelos SVM [^12.2].

Os multiplicadores de Lagrange sÃ£o variÃ¡veis auxiliares que sÃ£o introduzidas para lidar com as restriÃ§Ãµes de desigualdade do problema primal. Ao transformar o problema primal em um problema dual, os multiplicadores de Lagrange se tornam as variÃ¡veis principais, e a funÃ§Ã£o objetivo do problema dual Ã© formulada em termos desses multiplicadores, juntamente com os produtos internos entre os dados de treinamento.

Neste capÃ­tulo, exploraremos em detalhe a formulaÃ§Ã£o matemÃ¡tica da dualidade de Wolfe e o papel dos multiplicadores de Lagrange nas SVMs. Analisaremos a transformaÃ§Ã£o do problema primal para o problema dual, as condiÃ§Ãµes de otimalidade e como a soluÃ§Ã£o do problema dual nos leva aos parÃ¢metros Ã³timos do modelo. AlÃ©m disso, discutiremos como a dualidade de Wolfe revela a importÃ¢ncia dos vetores de suporte e como essa abordagem permite a utilizaÃ§Ã£o de *kernels* para construir fronteiras de decisÃ£o nÃ£o lineares.

### O Problema Primal e a FunÃ§Ã£o Lagrangiana

**Conceito 1: FormulaÃ§Ã£o do Problema Primal**

O problema primal das SVMs, para o caso nÃ£o separÃ¡vel, busca minimizar a seguinte funÃ§Ã£o de custo:

$$ \min_{\beta, \beta_0, \xi} \frac{1}{2} ||\beta||^2 + C \sum_{i=1}^{N} \xi_i $$

sujeito a:

$$ y_i(\beta^T x_i + \beta_0) \geq 1 - \xi_i, \quad \forall i $$
$$ \xi_i \geq 0, \quad \forall i $$

onde $\beta$ Ã© o vetor normal ao hiperplano, $\beta_0$ Ã© o *bias*, $x_i$ sÃ£o as amostras de treinamento, $y_i \in \{-1, 1\}$ sÃ£o os rÃ³tulos das classes, $\xi_i$ sÃ£o as variÃ¡veis de folga, e $C$ Ã© o parÃ¢metro de regularizaÃ§Ã£o.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Imagine que temos um conjunto de dados com duas classes, onde $x_i$ sÃ£o pontos bidimensionais. Vamos supor que temos trÃªs pontos de treinamento: $x_1 = (1, 1)$ com $y_1 = 1$, $x_2 = (2, 0)$ com $y_2 = -1$ e $x_3 = (0, 2)$ com $y_3 = 1$. O problema primal busca encontrar $\beta$, $\beta_0$ e $\xi_i$ que minimizem a funÃ§Ã£o de custo, respeitando as restriÃ§Ãµes. Se definirmos $C = 1$, a funÃ§Ã£o a ser minimizada Ã© $\frac{1}{2} ||\beta||^2 + \sum_{i=1}^{3} \xi_i$ e teremos as seguintes restriÃ§Ãµes:
>
> $1(\beta^T (1, 1) + \beta_0) \geq 1 - \xi_1$
>
> $-1(\beta^T (2, 0) + \beta_0) \geq 1 - \xi_2$
>
> $1(\beta^T (0, 2) + \beta_0) \geq 1 - \xi_3$
>
> $\xi_1 \geq 0, \xi_2 \geq 0, \xi_3 \geq 0$.
>
> O objetivo Ã© encontrar os valores de $\beta$, $\beta_0$ e $\xi_i$ que satisfaÃ§am essas condiÃ§Ãµes e minimizem a funÃ§Ã£o de custo.

**Lemma 1:** O problema primal das SVMs Ã© um problema de otimizaÃ§Ã£o convexo, caracterizado pela busca do hiperplano que maximiza a margem, tolerando erros de classificaÃ§Ã£o atravÃ©s das variÃ¡veis de folga.

A demonstraÃ§Ã£o desse lemma se baseia na anÃ¡lise da funÃ§Ã£o de custo e das restriÃ§Ãµes, que sÃ£o convexas. A convexidade do problema primal garante que a soluÃ§Ã£o encontrada seja um mÃ­nimo global Ãºnico.

**Conceito 2: ConstruÃ§Ã£o da FunÃ§Ã£o Lagrangiana**

Para lidar com as restriÃ§Ãµes de desigualdade do problema primal, construÃ­mos a **funÃ§Ã£o Lagrangiana**, introduzindo os **multiplicadores de Lagrange** $\alpha_i$ e $\mu_i$:

```mermaid
graph LR
    subgraph "Lagrangian Function Construction"
        direction LR
        A["Cost Function: 1/2 ||Î²||Â² + C Î£Î¾áµ¢"]
        B["Constraint Term: - Î£Î±áµ¢ [yáµ¢(Î²áµ€xáµ¢ + Î²â‚€) - 1 + Î¾áµ¢]"]
        C["Slack Variable Constraint: - Î£Î¼áµ¢Î¾áµ¢"]
        A --> D["Lagrangian Function: L(Î², Î²â‚€, Î¾, Î±, Î¼)"]
        B --> D
        C --> D
    end
```

$$ L_p(\beta, \beta_0, \xi, \alpha, \mu) = \frac{1}{2} ||\beta||^2 + C \sum_{i=1}^{N} \xi_i - \sum_{i=1}^{N} \alpha_i [y_i(\beta^T x_i + \beta_0) - 1 + \xi_i] - \sum_{i=1}^{N} \mu_i \xi_i $$

onde $\alpha_i \geq 0$ e $\mu_i \geq 0$ sÃ£o os multiplicadores de Lagrange associados Ã s restriÃ§Ãµes de desigualdade.

A funÃ§Ã£o Lagrangiana combina a funÃ§Ã£o de custo com as restriÃ§Ãµes, atravÃ©s da introduÃ§Ã£o dos multiplicadores de Lagrange. Os multiplicadores $\alpha_i$ sÃ£o associados Ã  restriÃ§Ã£o de que os pontos devem estar corretamente classificados ou dentro da margem, e os multiplicadores $\mu_i$ sÃ£o associados Ã  restriÃ§Ã£o de nÃ£o negatividade das variÃ¡veis de folga.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Usando o exemplo anterior com $C=1$ e os pontos $x_1 = (1, 1)$, $x_2 = (2, 0)$, e $x_3 = (0, 2)$, a funÃ§Ã£o Lagrangiana seria:
>
> $L_p(\beta, \beta_0, \xi, \alpha, \mu) = \frac{1}{2} ||\beta||^2 + \xi_1 + \xi_2 + \xi_3 - \alpha_1[1(\beta^T (1, 1) + \beta_0) - 1 + \xi_1] - \alpha_2[-1(\beta^T (2, 0) + \beta_0) - 1 + \xi_2] - \alpha_3[1(\beta^T (0, 2) + \beta_0) - 1 + \xi_3] - \mu_1 \xi_1 - \mu_2 \xi_2 - \mu_3 \xi_3$
>
> Onde $\alpha_i \geq 0$ e $\mu_i \geq 0$. Esta funÃ§Ã£o combina a funÃ§Ã£o de custo com as restriÃ§Ãµes atravÃ©s dos multiplicadores de Lagrange.

**CorolÃ¡rio 1:** A funÃ§Ã£o Lagrangiana incorpora as restriÃ§Ãµes do problema primal na funÃ§Ã£o objetivo, o que permite transformar o problema de otimizaÃ§Ã£o restrito em um problema de otimizaÃ§Ã£o irrestrito.

A demonstraÃ§Ã£o desse corolÃ¡rio se baseia na anÃ¡lise da construÃ§Ã£o da funÃ§Ã£o Lagrangiana, onde as restriÃ§Ãµes do problema primal sÃ£o adicionadas Ã  funÃ§Ã£o objetivo atravÃ©s da introduÃ§Ã£o dos multiplicadores de Lagrange. Essa abordagem permite encontrar uma soluÃ§Ã£o para o problema de otimizaÃ§Ã£o restrito atravÃ©s da minimizaÃ§Ã£o (ou maximizaÃ§Ã£o) da funÃ§Ã£o Lagrangeana, o que Ã©, muitas vezes, mais simples do que resolver o problema primal diretamente.

### A Dualidade de Wolfe e a ObtenÃ§Ã£o do Problema Dual

```mermaid
graph TB
 subgraph "Wolfe Duality Transformation"
    direction TB
    A["Minimize Lagrangian Lâ‚š(Î², Î²â‚€, Î¾, Î±, Î¼) with respect to Î², Î²â‚€, Î¾"]
    B["Apply Karush-Kuhn-Tucker (KKT) Conditions"]
    C["Express Î² in terms of Î±: Î² = Î£Î±áµ¢yáµ¢xáµ¢"]
    D["Express Constraint: Î£Î±áµ¢yáµ¢ = 0"]
    E["Eliminate Î¾ using: Î¼áµ¢ = C - Î±áµ¢"]
    F["Substitute into Lagrangian Lâ‚š to Obtain Dual Function L_D(Î±)"]
    A --> B
    B --> C
    B --> D
    B --> E
    C & D & E --> F
  end
```

A **dualidade de Wolfe** Ã© uma tÃ©cnica de otimizaÃ§Ã£o que permite transformar um problema de otimizaÃ§Ã£o primal em um problema dual. Essa transformaÃ§Ã£o Ã© particularmente Ãºtil para as SVMs, pois ela leva a um problema de otimizaÃ§Ã£o que depende apenas dos produtos internos entre os dados de treinamento, e os multiplicadores de Lagrange, que levam aos conceitos de vetores de suporte e possibilitam a utilizaÃ§Ã£o dos kernels.

Para obter o problema dual, minimizamos a funÃ§Ã£o Lagrangiana $L_p(\beta, \beta_0, \xi, \alpha, \mu)$ em relaÃ§Ã£o aos parÃ¢metros primais $\beta$, $\beta_0$ e $\xi_i$, o que resulta nas seguintes condiÃ§Ãµes de otimalidade:

1.  $\frac{\partial L_p}{\partial \beta} = \beta - \sum_{i=1}^{N} \alpha_i y_i x_i = 0 \implies \beta = \sum_{i=1}^{N} \alpha_i y_i x_i$
2.  $\frac{\partial L_p}{\partial \beta_0} = - \sum_{i=1}^{N} \alpha_i y_i = 0 \implies \sum_{i=1}^{N} \alpha_i y_i = 0$
3.  $\frac{\partial L_p}{\partial \xi_i} = C - \alpha_i - \mu_i = 0 \implies \mu_i = C - \alpha_i$

Substituindo essas expressÃµes na funÃ§Ã£o Lagrangiana e utilizando a restriÃ§Ã£o $\sum_{i=1}^{N} \alpha_i y_i = 0$, obtemos a funÃ§Ã£o dual $L_D(\alpha)$:

$$ L_D(\alpha) = \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j x_i^T x_j $$

O problema dual da SVM Ã© entÃ£o:

$$ \max_{\alpha} L_D(\alpha) = \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j x_i^T x_j $$

sujeito a:

$$ 0 \leq \alpha_i \leq C, \quad \forall i $$
$$ \sum_{i=1}^{N} \alpha_i y_i = 0 $$

O problema dual consiste em maximizar a funÃ§Ã£o dual $L_D(\alpha)$ em relaÃ§Ã£o aos multiplicadores de Lagrange $\alpha_i$, sujeitos Ã s restriÃ§Ãµes $0 \leq \alpha_i \leq C$ e $\sum_{i=1}^{N} \alpha_i y_i = 0$.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos calcular o problema dual para os trÃªs pontos do exemplo anterior. Primeiro, calculamos os produtos internos:
>
> $x_1^T x_1 = (1,1)^T (1,1) = 2$
>
> $x_1^T x_2 = (1,1)^T (2,0) = 2$
>
> $x_1^T x_3 = (1,1)^T (0,2) = 2$
>
> $x_2^T x_1 = (2,0)^T (1,1) = 2$
>
> $x_2^T x_2 = (2,0)^T (2,0) = 4$
>
> $x_2^T x_3 = (2,0)^T (0,2) = 0$
>
> $x_3^T x_1 = (0,2)^T (1,1) = 2$
>
> $x_3^T x_2 = (0,2)^T (2,0) = 0$
>
> $x_3^T x_3 = (0,2)^T (0,2) = 4$
>
> A funÃ§Ã£o dual se torna:
>
> $L_D(\alpha) = \alpha_1 + \alpha_2 + \alpha_3 - \frac{1}{2} [\alpha_1^2(1)(1)(2) + \alpha_1\alpha_2(1)(-1)(2) + \alpha_1\alpha_3(1)(1)(2) + \alpha_2\alpha_1(-1)(1)(2) + \alpha_2^2(-1)(-1)(4) + \alpha_2\alpha_3(-1)(1)(0) + \alpha_3\alpha_1(1)(1)(2) + \alpha_3\alpha_2(1)(-1)(0) + \alpha_3^2(1)(1)(4)]$
>
> $L_D(\alpha) = \alpha_1 + \alpha_2 + \alpha_3 - [\alpha_1^2 - 2\alpha_1\alpha_2 + 2\alpha_1\alpha_3 + 2\alpha_2^2 + 2\alpha_3^2]$
>
> O problema dual Ã© maximizar $L_D(\alpha)$ sujeito a $0 \leq \alpha_i \leq 1$ e $\alpha_1 - \alpha_2 + \alpha_3 = 0$.

**Lemma 2:** A dualidade de Wolfe transforma o problema primal das SVMs em um problema dual que depende apenas dos multiplicadores de Lagrange e dos produtos internos entre os dados de treinamento.

A demonstraÃ§Ã£o desse lemma se baseia na anÃ¡lise da derivaÃ§Ã£o do problema dual, onde as variÃ¡veis primais $\beta$, $\beta_0$ e $\xi_i$ sÃ£o eliminadas e a funÃ§Ã£o objetivo do problema dual depende apenas dos multiplicadores de Lagrange e dos produtos internos $x_i^T x_j$.

### InterpretaÃ§Ã£o dos Multiplicadores de Lagrange e Vetores de Suporte

```mermaid
graph LR
 subgraph "Lagrange Multipliers and Support Vectors"
  direction TB
    A["Maximize Dual Function L_D(Î±)"]
    B["Optimal Lagrange Multipliers: Î±áµ¢"]
    C["Support Vectors: xáµ¢ where Î±áµ¢ > 0"]
    D["Margin Vectors: 0 < Î±áµ¢ < C"]
    E["Margin Violation Vectors: Î±áµ¢ = C"]
    F["Non-Support Vectors: Î±áµ¢ = 0"]
    A --> B
    B --> C
    C --> D
    C --> E
    C --> F
 end
```

A soluÃ§Ã£o do problema dual das SVMs, obtida atravÃ©s da maximizaÃ§Ã£o da funÃ§Ã£o dual $L_D(\alpha)$, nos fornece os valores Ã³timos dos multiplicadores de Lagrange $\alpha_i$. Esses multiplicadores tÃªm uma interpretaÃ§Ã£o geomÃ©trica importante e estÃ£o relacionados com o conceito de **vetores de suporte**.

Os **vetores de suporte** sÃ£o as amostras de treinamento que tÃªm multiplicadores de Lagrange $\alpha_i > 0$ na soluÃ§Ã£o Ã³tima. SÃ£o esses pontos que determinam a posiÃ§Ã£o e orientaÃ§Ã£o do hiperplano separador Ã³timo e, portanto, sÃ£o os pontos de treinamento mais importantes para o modelo.

*   Se $0 < \alpha_i < C$, a amostra $x_i$ Ã© um vetor de suporte que estÃ¡ exatamente sobre a margem.
*   Se $\alpha_i = C$, a amostra $x_i$ Ã© um vetor de suporte que viola a margem, ou seja, estÃ¡ dentro ou do lado errado da margem.
*   Se $\alpha_i = 0$, a amostra $x_i$ nÃ£o Ã© um vetor de suporte e estÃ¡ corretamente classificada e fora da margem.

A condiÃ§Ã£o $\sum_{i=1}^{N} \alpha_i y_i = 0$ garante que os vetores de suporte das duas classes se equilibram, o que Ã© fundamental para a construÃ§Ã£o de um hiperplano de decisÃ£o Ã³timo.

A relaÃ§Ã£o entre os multiplicadores de Lagrange, os vetores de suporte e a margem Ã© crucial para o entendimento da formulaÃ§Ã£o da SVM. A funÃ§Ã£o de decisÃ£o da SVM pode ser escrita como:

$$ f(x) = \sum_{i \in SV} \alpha_i y_i x_i^T x + \beta_0 $$

onde SV Ã© o conjunto de vetores de suporte. Essa equaÃ§Ã£o demonstra que a decisÃ£o da SVM depende apenas dos produtos internos entre a amostra a ser classificada e os vetores de suporte, o que possibilita o uso de *kernels* para construir fronteiras de decisÃ£o nÃ£o lineares.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que, ao resolver o problema dual para nossos trÃªs pontos, obtivemos os seguintes multiplicadores de Lagrange: $\alpha_1 = 0.3$, $\alpha_2 = 0.7$, e $\alpha_3 = 0.4$. Como todos sÃ£o maiores que 0, os trÃªs pontos sÃ£o vetores de suporte. O vetor $\beta$ pode ser calculado como:
>
> $\beta = \sum_{i=1}^{3} \alpha_i y_i x_i = 0.3(1)(1,1) + 0.7(-1)(2,0) + 0.4(1)(0,2) = (0.3, 0.3) + (-1.4, 0) + (0, 0.8) = (-1.1, 1.1)$.
>
> Note que a condiÃ§Ã£o $\sum_{i=1}^{3} \alpha_i y_i = 0.3 - 0.7 + 0.4 = 0$ Ã© satisfeita.
>
> Se tivÃ©ssemos um novo ponto, $x = (1, 0)$, a classificaÃ§Ã£o seria dada por:
>
> $f(x) = \sum_{i \in SV} \alpha_i y_i x_i^T x + \beta_0 = 0.3(1)(1,1)^T (1,0) + 0.7(-1)(2,0)^T (1,0) + 0.4(1)(0,2)^T(1,0) + \beta_0$
>
> $f(x) = 0.3(1) - 0.7(2) + 0.4(0) + \beta_0 = 0.3 - 1.4 + \beta_0 = -1.1 + \beta_0$.
>
> Para classificar $x$, precisamos calcular $\beta_0$. Normalmente, isso Ã© feito usando as condiÃ§Ãµes KKT e os vetores de suporte. O sinal de $f(x)$ determina a classe.

**Lemma 3:** Os vetores de suporte sÃ£o as amostras de treinamento que tÃªm multiplicadores de Lagrange $\alpha_i > 0$ na soluÃ§Ã£o do problema dual, e sÃ£o esses pontos que determinam a posiÃ§Ã£o e orientaÃ§Ã£o do hiperplano separador Ã³timo.

A demonstraÃ§Ã£o desse lemma se baseia na anÃ¡lise das condiÃ§Ãµes de Karush-Kuhn-Tucker (KKT) e na funÃ§Ã£o dual. Os multiplicadores de Lagrange $\alpha_i$ sÃ£o diferentes de zero apenas para as amostras que estÃ£o sobre ou dentro da margem, ou seja, os vetores de suporte.

### A Dualidade de Wolfe e a UtilizaÃ§Ã£o de Kernels

```mermaid
graph LR
 subgraph "Kernel Trick in SVM"
    direction TB
    A["Dual Problem: Dependence on Inner Products: xáµ¢áµ€xâ±¼"]
    B["Replace Inner Product: xáµ¢áµ€xâ±¼ with Kernel Function: K(xáµ¢, xâ±¼)"]
    C["Kernel Function Computes Inner Product in Transformed Feature Space"]
    D["Dual Function with Kernel: L_D(Î±) = Î£Î±áµ¢ - 1/2 Î£Î£Î±áµ¢Î±â±¼yáµ¢yâ±¼K(xáµ¢,xâ±¼)"]
    E["Decision Function with Kernel: f(x) = Î£Î±áµ¢yáµ¢K(xáµ¢, x) + Î²â‚€"]
    A --> B
    B --> C
    C --> D
    D --> E
  end
```

A dualidade de Wolfe revela uma propriedade fundamental das SVMs, que Ã© a dependÃªncia dos dados de treinamento apenas atravÃ©s de produtos internos. Essa propriedade permite a utilizaÃ§Ã£o do "**kernel trick**", onde o produto interno $x_i^T x_j$ Ã© substituÃ­do por uma funÃ§Ã£o *kernel* $K(x_i, x_j)$, que calcula o produto interno em um espaÃ§o de *features* de maior dimensÃ£o, sem explicitar a transformaÃ§Ã£o.

A funÃ§Ã£o dual das SVMs, com a substituiÃ§Ã£o do produto interno pelo *kernel*, se torna:

$$ L_D(\alpha) = \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j K(x_i, x_j) $$

A funÃ§Ã£o de decisÃ£o da SVM, tambÃ©m com a utilizaÃ§Ã£o do *kernel*, se torna:

$$ f(x) = \sum_{i \in SV} \alpha_i y_i K(x_i, x) + \beta_0 $$

Essa propriedade Ã© fundamental para a capacidade das SVMs de lidar com problemas de classificaÃ§Ã£o nÃ£o lineares. A escolha do *kernel* apropriado permite que as SVMs construam fronteiras de decisÃ£o complexas em espaÃ§os de *features* de alta dimensÃ£o.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que, em vez de usar o produto interno, optamos por usar um *kernel* polinomial de grau 2, definido como $K(x_i, x_j) = (x_i^T x_j + 1)^2$. Usando os mesmos dados $x_1 = (1,1)$, $x_2 = (2,0)$ e $x_3 = (0,2)$, podemos calcular os valores do kernel:
>
> $K(x_1, x_1) = (2 + 1)^2 = 9$
>
> $K(x_1, x_2) = (2 + 1)^2 = 9$
>
> $K(x_1, x_3) = (2 + 1)^2 = 9$
>
> $K(x_2, x_1) = (2 + 1)^2 = 9$
>
> $K(x_2, x_2) = (4 + 1)^2 = 25$
>
> $K(x_2, x_3) = (0 + 1)^2 = 1$
>
> $K(x_3, x_1) = (2 + 1)^2 = 9$
>
> $K(x_3, x_2) = (0 + 1)^2 = 1$
>
> $K(x_3, x_3) = (4 + 1)^2 = 25$
>
> A funÃ§Ã£o dual com o *kernel* se torna:
>
> $L_D(\alpha) = \alpha_1 + \alpha_2 + \alpha_3 - \frac{1}{2} [\alpha_1^2(1)(1)(9) + \alpha_1\alpha_2(1)(-1)(9) + \alpha_1\alpha_3(1)(1)(9) + \alpha_2\alpha_1(-1)(1)(9) + \alpha_2^2(-1)(-1)(25) + \alpha_2\alpha_3(-1)(1)(1) + \alpha_3\alpha_1(1)(1)(9) + \alpha_3\alpha_2(1)(-1)(1) + \alpha_3^2(1)(1)(25)]$
>
> $L_D(\alpha) = \alpha_1 + \alpha_2 + \alpha_3 - \frac{1}{2} [9\alpha_1^2 - 18\alpha_1\alpha_2 + 18\alpha_1\alpha_3 + 25\alpha_2^2 - 2\alpha_2\alpha_3 + 25\alpha_3^2]$
>
> O problema dual Ã© maximizar $L_D(\alpha)$ sujeito a $0 \leq \alpha_i \leq 1$ e $\alpha_1 - \alpha_2 + \alpha_3 = 0$. Note como o uso do kernel modifica a funÃ§Ã£o dual.

**CorolÃ¡rio 3:** A dualidade de Wolfe permite que as SVMs utilizem o *kernel trick*, que possibilita trabalhar em espaÃ§os de *features* de alta dimensÃ£o sem calcular explicitamente a transformaÃ§Ã£o, e construir fronteiras de decisÃ£o nÃ£o lineares.

A demonstraÃ§Ã£o desse corolÃ¡rio se baseia na anÃ¡lise da funÃ§Ã£o dual e da funÃ§Ã£o de decisÃ£o da SVM, onde a Ãºnica dependÃªncia dos dados $x_i$ Ã© atravÃ©s da funÃ§Ã£o *kernel* $K(x_i, x_j)$. A dualidade de Wolfe transforma a otimizaÃ§Ã£o que seria realizada em um espaÃ§o de features de alta dimensÃ£o em uma otimizaÃ§Ã£o no espaÃ§o dos parÃ¢metros duais, e permite o uso de kernels sem explicitar o mapeamento para esse espaÃ§o.

### ConclusÃ£o

Neste capÃ­tulo, exploramos em detalhes a utilizaÃ§Ã£o de **multiplicadores de Lagrange** e a **dualidade de Wolfe** nas **Support Vector Machines (SVMs)**. Vimos como a dualidade de Wolfe transforma o problema primal, definido no espaÃ§o dos parÃ¢metros do modelo, em um problema dual, definido no espaÃ§o dos multiplicadores de Lagrange, o que simplifica a obtenÃ§Ã£o da soluÃ§Ã£o Ã³tima.

Analisamos o papel fundamental dos multiplicadores de Lagrange e como eles se relacionam com o conceito de **vetores de suporte**. Os vetores de suporte sÃ£o os pontos de treinamento mais importantes para o modelo, e sÃ£o eles que definem a posiÃ§Ã£o e orientaÃ§Ã£o do hiperplano de decisÃ£o. Discutimos tambÃ©m como a dualidade de Wolfe permite o uso do *kernel trick*, que possibilita trabalhar em espaÃ§os de *features* de alta dimensÃ£o e construir fronteiras de decisÃ£o nÃ£o lineares.

A compreensÃ£o da dualidade de Wolfe e dos multiplicadores de Lagrange Ã© essencial para uma compreensÃ£o profunda das SVMs. Esses conceitos sÃ£o fundamentais para a construÃ§Ã£o de modelos robustos e eficientes para problemas complexos de classificaÃ§Ã£o e regressÃ£o. A capacidade de trabalhar com *kernels*, que Ã© resultado da dualidade de Wolfe, e a interpretaÃ§Ã£o dos vetores de suporte como os pontos de treinamento mais relevantes sÃ£o algumas das caracterÃ­sticas que tornam as SVMs uma ferramenta poderosa no aprendizado de mÃ¡quina.

### Footnotes

[^12.1]: "In this chapter we describe generalizations of linear decision boundaries for classification. Optimal separating hyperplanes are introduced in Chapter 4 for the case when two classes are linearly separable. Here we cover extensions to the nonseparable case, where the classes overlap. These techniques are then generalized to what is known as the support vector machine, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space."

[^12.2]: "In Chapter 4 we discussed a technique for constructing an optimal separating hyperplane between two perfectly separated classes. We review this and generalize to the nonseparable case, where the classes may not be separable by a linear boundary."
```