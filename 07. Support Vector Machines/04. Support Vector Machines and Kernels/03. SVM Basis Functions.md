Okay, here's the enhanced text with Mermaid diagrams as requested:

## T√≠tulo: SVMs e Fun√ß√µes de Base: Expandindo o Espa√ßo de Features para Modelagem N√£o Linear

```mermaid
graph LR
    subgraph "Feature Space Transformation"
        direction TB
        A["Original Feature Space 'x'"]
        B["Basis Functions 'h_m(x)'"]
        C["Transformed Feature Space 'h(x)'"]
        A --> B
        B --> C
        style A fill:#f9f,stroke:#333,stroke-width:2px
        style C fill:#ccf,stroke:#333,stroke-width:2px
    end
    
```

### Introdu√ß√£o

Em muitos problemas de classifica√ß√£o e regress√£o, as rela√ß√µes entre as *features* e as vari√°veis de resposta s√£o n√£o lineares. Para lidar com essa n√£o linearidade, os modelos de aprendizado de m√°quina podem utilizar **fun√ß√µes de base** para transformar o espa√ßo de *features* original em um espa√ßo de maior dimens√£o, onde essas rela√ß√µes podem ser modeladas de forma mais eficiente. As **Support Vector Machines (SVMs)** tamb√©m podem se beneficiar do uso de fun√ß√µes de base, e essa abordagem fornece uma perspectiva complementar sobre a utiliza√ß√£o de *kernels*.

Neste cap√≠tulo, exploraremos como as fun√ß√µes de base s√£o utilizadas para estender o espa√ßo de *features* das SVMs, e como essa transforma√ß√£o se relaciona com a utiliza√ß√£o dos *kernels*. Discutiremos a defini√ß√£o das fun√ß√µes de base, como elas s√£o aplicadas aos dados de entrada, e como elas influenciam a forma da fronteira de decis√£o ou da fun√ß√£o de regress√£o. Al√©m disso, analisaremos como a escolha das fun√ß√µes de base e seus par√¢metros impactam a capacidade de generaliza√ß√£o do modelo e o seu desempenho em dados novos.

### Fun√ß√µes de Base: Defini√ß√£o e Transforma√ß√£o do Espa√ßo de Features

**Conceito 1: Defini√ß√£o de Fun√ß√µes de Base**

As **fun√ß√µes de base** s√£o fun√ß√µes matem√°ticas, denotadas por $h_m(x)$, que transformam os vetores de *features* $x$ em um espa√ßo de maior dimens√£o, onde a modelagem pode ser feita de forma mais simples e eficiente. As fun√ß√µes de base s√£o um conjunto de fun√ß√µes predefinidas, e a transforma√ß√£o do espa√ßo de *features* √© realizada atrav√©s da aplica√ß√£o dessas fun√ß√µes a cada amostra.

Formalmente, se temos $M$ fun√ß√µes de base $h_1(x), h_2(x), ..., h_M(x)$, cada amostra $x$ no espa√ßo original de *features* √© transformada em um vetor de *features* $\textbf{h}(x)$ no novo espa√ßo, onde $\textbf{h}(x) = [h_1(x), h_2(x), \ldots, h_M(x)]$. Este vetor $\textbf{h}(x)$ representa a amostra no novo espa√ßo de *features* transformado.

```mermaid
graph LR
    subgraph "Basis Function Transformation"
        direction LR
        A["Input Vector 'x'"] --> B["Basis Functions: 'h_1(x), h_2(x), ..., h_M(x)'"]
         B --> C["Transformed Vector: 'h(x) = [h_1(x), h_2(x), ..., h_M(x)]'"]
    end
    style A fill:#fff,stroke:#333,stroke-width:1px
    style C fill:#fff,stroke:#333,stroke-width:1px

```

A escolha das fun√ß√µes de base √© crucial para o desempenho do modelo, e deve ser feita considerando a natureza dos dados e as rela√ß√µes n√£o lineares que se deseja capturar. As fun√ß√µes de base podem ser polinomiais, radiais, trigonom√©tricas, splines ou outras fun√ß√µes que gerem um espa√ßo onde as classes ou a resposta de regress√£o sejam represent√°veis.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma amostra $x = [2, 3]$ no espa√ßo de *features* original. Vamos aplicar duas fun√ß√µes de base:
>
> 1.  $h_1(x) = x_1^2$
> 2.  $h_2(x) = x_1 * x_2$
>
> A transforma√ß√£o para o novo espa√ßo de *features* seria:
>
> $\textbf{h}(x) = [h_1(x), h_2(x)] = [2^2, 2 * 3] = [4, 6]$
>
> Assim, a amostra $x = [2, 3]$ no espa√ßo original √© mapeada para $\textbf{h}(x) = [4, 6]$ no novo espa√ßo. Este novo espa√ßo, de dimens√£o 2, permite criar modelos lineares que correspondem a modelos n√£o lineares no espa√ßo original.

**Lemma 1:** As fun√ß√µes de base transformam os dados de entrada para um espa√ßo de *features* de maior dimens√£o, onde rela√ß√µes n√£o lineares podem ser modeladas de forma mais eficiente.

A demonstra√ß√£o desse lemma se baseia na an√°lise da defini√ß√£o das fun√ß√µes de base, que s√£o projetadas para transformar as *features* originais em um conjunto de *features* mais adequado para a modelagem. A transforma√ß√£o permite construir modelos mais flex√≠veis e capazes de capturar padr√µes complexos nos dados.

**Conceito 2: Tipos Comuns de Fun√ß√µes de Base**

Existem diversas fun√ß√µes de base que podem ser utilizadas, dependendo do tipo de modelo e da natureza dos dados:

1.  **Fun√ß√µes de Base Polinomial:**
    As fun√ß√µes de base polinomial incluem termos como $x_i$, $x_i^2$, $x_i x_j$, $x_i^3$, etc., onde $x_i$ e $x_j$ s√£o as *features* originais. As fun√ß√µes de base polinomial s√£o utilizadas para modelar rela√ß√µes n√£o lineares em forma de polin√¥mios.

    > üí° **Exemplo Num√©rico:**
    >
    > Para uma amostra $x = [x_1, x_2]$, as fun√ß√µes de base polinomiais de grau 2 poderiam ser:
    >
    > $h_1(x) = 1$ (bias term)
    > $h_2(x) = x_1$
    > $h_3(x) = x_2$
    > $h_4(x) = x_1^2$
    > $h_5(x) = x_2^2$
    > $h_6(x) = x_1 x_2$
    >
    > Se $x = [2, 3]$, ent√£o $\textbf{h}(x) = [1, 2, 3, 4, 9, 6]$. O espa√ßo de *features* foi expandido de 2 dimens√µes para 6.

```mermaid
graph LR
    subgraph "Polynomial Basis Functions"
    direction TB
        A["Input 'x = [x_1, x_2]'"]
        B["Polynomial Basis: 'h(x) = [1, x_1, x_2, x_1^2, x_2^2, x_1x_2]'"]
        A --> B
    end
    style A fill:#fff,stroke:#333,stroke-width:1px
    style B fill:#fff,stroke:#333,stroke-width:1px

```

2.  **Fun√ß√µes de Base Radial (RBF):**
    As fun√ß√µes de base radial s√£o fun√ß√µes gaussianas centradas em pontos espec√≠ficos, e t√™m a forma:
    $$ h_m(x) = \exp(-\gamma ||x - c_m||^2) $$
    onde $c_m$ √© o centro da fun√ß√£o radial e $\gamma$ controla sua largura. As fun√ß√µes de base radial s√£o utilizadas para modelar rela√ß√µes n√£o lineares com comportamento localizado.

    > üí° **Exemplo Num√©rico:**
    >
    > Vamos usar uma fun√ß√£o RBF com centro $c_1 = [1, 1]$ e $\gamma = 0.5$. Para a amostra $x = [2, 2]$:
    >
    > $h_1(x) = \exp(-0.5 * ||[2, 2] - [1, 1]||^2)$
    > $h_1(x) = \exp(-0.5 * ( (2-1)^2 + (2-1)^2 ) )$
    > $h_1(x) = \exp(-0.5 * (1 + 1)) = \exp(-1) \approx 0.368$
    >
    > Se adicionarmos outro RBF com centro $c_2 = [3, 3]$, ent√£o ter√≠amos outra feature $h_2(x)$ e o vetor $\textbf{h}(x)$ teria duas dimens√µes.

```mermaid
graph LR
    subgraph "Radial Basis Function (RBF)"
    direction TB
        A["Input Vector 'x'"]
        B["RBF Center 'c_m'"]
        C["Parameter 'Œ≥'"]
        D["RBF Function: 'h_m(x) = exp(-Œ≥||x - c_m||¬≤)'"]
        A & B & C --> D
    end
     style A fill:#fff,stroke:#333,stroke-width:1px
     style D fill:#fff,stroke:#333,stroke-width:1px
```

3.  **Fun√ß√µes de Base Spline:**
    As fun√ß√µes de base spline s√£o fun√ß√µes polinomiais segmentadas, que s√£o definidas por partes. Elas s√£o utilizadas para modelar rela√ß√µes n√£o lineares suaves e flex√≠veis, e s√£o especialmente √∫teis em problemas de regress√£o.

4.  **Outras Fun√ß√µes de Base:**
    Existem outras fun√ß√µes de base que podem ser utilizadas dependendo do problema em quest√£o, como fun√ß√µes trigonom√©tricas, fun√ß√µes lineares por partes, fun√ß√µes de *wavelet*, etc.

**Corol√°rio 1:** A escolha do tipo de fun√ß√µes de base depende do problema em quest√£o, e a combina√ß√£o de diferentes fun√ß√µes de base pode ser utilizada para modelar rela√ß√µes n√£o lineares complexas.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise das propriedades de diferentes tipos de fun√ß√µes de base, e como essas propriedades se relacionam com a natureza dos dados e com os objetivos do modelo. A combina√ß√£o de fun√ß√µes de base de diferentes tipos permite modelar relacionamentos mais complexos.

### SVMs e Fun√ß√µes de Base: Uma Perspectiva Linear no Espa√ßo Transformado

```mermaid
graph LR
    subgraph "SVM with Basis Functions"
        direction TB
        A["Original Feature Space 'x' (Non-Linear)"]
        B["Basis Function Transformation 'h(x)'"]
        C["Transformed Feature Space 'h(x)' (Linear)"]
        D["Linear Separating Hyperplane in 'h(x)'"]
        E["Non-Linear Decision Boundary in 'x'"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#f9f,stroke:#333,stroke-width:2px
```

Ao utilizar fun√ß√µes de base, o objetivo das SVMs passa a ser encontrar um **hiperplano separador linear** no espa√ßo de *features* transformado, onde o vetor de *features* de entrada √© agora $\textbf{h}(x)$, que corresponde a um modelo n√£o linear no espa√ßo original das *features*. A fun√ß√£o de decis√£o do modelo no espa√ßo de *features* transformado √© dada por:

$$ f(x) = \beta^T \textbf{h}(x) + \beta_0 $$

onde $\beta$ √© o vetor normal ao hiperplano no espa√ßo de *features* transformado, $\beta_0$ √© o *bias*, e $\textbf{h}(x)$ √© o vetor de *features* transformado. Essa fun√ß√£o de decis√£o √© linear no espa√ßo de *features* transformado, mas n√£o linear no espa√ßo de *features* original.

A utiliza√ß√£o de fun√ß√µes de base nas SVMs permite construir modelos n√£o lineares de forma similar ao uso de *kernels*, onde √© encontrado um separador linear em um espa√ßo transformado. No entanto, ao contr√°rio dos kernels, que podem ser vistos como fun√ß√µes impl√≠citas de um produto interno em um espa√ßo transformado (sem explicitar a transforma√ß√£o), as fun√ß√µes de base especificam explicitamente uma transforma√ß√£o para um determinado espa√ßo.

The optimization problem for an SVM with basis functions can be formulated as:

```mermaid
graph LR
    subgraph "Optimization Problem"
        direction TB
        A["Minimize: '1/2 ||Œ≤||¬≤ + C ‚àëŒæ_i'"]
        B["Subject to: 'y_i(Œ≤^T h(x_i) + Œ≤_0) ‚â• 1 - Œæ_i'"]
        C["Constraint: 'Œæ_i ‚â• 0'"]
        A --> B
        B --> C
    end
style A fill:#fff,stroke:#333,stroke-width:1px
style B fill:#fff,stroke:#333,stroke-width:1px

```

$$ \min_{\beta, \beta_0, \xi} \frac{1}{2} ||\beta||^2 + C \sum_{i=1}^{N} \xi_i $$

sujeito a:

$$ y_i(\beta^T \textbf{h}(x_i) + \beta_0) \geq 1 - \xi_i, \quad \forall i $$
$$ \xi_i \geq 0, \quad \forall i $$

O processo de otimiza√ß√£o √© similar ao caso de SVMs sem fun√ß√µes de base, mas agora o hiperplano √© definido no espa√ßo de *features* transformado.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos duas classes de pontos no espa√ßo original 2D que n√£o s√£o linearmente separ√°veis. Ap√≥s aplicar fun√ß√µes de base polinomiais de grau 2 como mostrado no exemplo anterior, temos um novo espa√ßo de 6 dimens√µes. O SVM agora busca um hiperplano linear nesse espa√ßo de 6 dimens√µes.
>
> Seja $\textbf{h}(x) = [1, x_1, x_2, x_1^2, x_2^2, x_1 x_2]$. O problema de otimiza√ß√£o da SVM agora se torna:
>
> $$ \min_{\beta, \beta_0, \xi} \frac{1}{2} ||\beta||^2 + C \sum_{i=1}^{N} \xi_i $$
>
> sujeito a:
>
> $$ y_i(\beta_0 + \beta_1 + \beta_2 x_{i1} + \beta_3 x_{i2} + \beta_4 x_{i1}^2 + \beta_5 x_{i2}^2 + \beta_6 x_{i1}x_{i2}) \geq 1 - \xi_i, \quad \forall i $$
> $$ \xi_i \geq 0, \quad \forall i $$
>
> Onde $\beta = [\beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6]$ e o vetor $\beta$ define o hiperplano nesse espa√ßo transformado. A fun√ß√£o de decis√£o no espa√ßo transformado, que √© linear, corresponde a uma fronteira de decis√£o n√£o linear no espa√ßo original de duas dimens√µes.

**Lemma 3:** A utiliza√ß√£o de fun√ß√µes de base permite que as SVMs construam modelos n√£o lineares no espa√ßo original das *features*, atrav√©s da constru√ß√£o de um hiperplano separador linear no espa√ßo de *features* transformado.

A demonstra√ß√£o desse lemma se baseia na an√°lise da fun√ß√£o de decis√£o da SVM com fun√ß√µes de base, onde a linearidade √© obtida no espa√ßo transformado, mas se traduz em n√£o linearidade no espa√ßo original.

### A Conex√£o com Kernels: Uma Vis√£o Alternativa

```mermaid
graph LR
    subgraph "Basis Functions vs Kernels"
        direction LR
        A["Basis Functions: Explicit Transformation 'h(x)'"]
        B["Kernels: Implicit Transformation 'K(x, x') = <œÜ(x), œÜ(x')>'"]
        A --> C["Linear Model in Transformed Space"]
        B --> C
        
    end
    style A fill:#fff,stroke:#333,stroke-width:1px
     style B fill:#fff,stroke:#333,stroke-width:1px

```

A utiliza√ß√£o de **kernels** em SVMs pode ser vista como uma forma de utilizar fun√ß√µes de base de forma impl√≠cita. O *kernel trick*, discutido em cap√≠tulos anteriores, permite que as SVMs operem em espa√ßos de *features* transformados, sem explicitar a transforma√ß√£o atrav√©s da fun√ß√£o $\phi(x)$.

Em vez de definir explicitamente as fun√ß√µes de base $h_m(x)$ e construir o espa√ßo de *features* transformado $\textbf{h}(x)$, o *kernel* calcula o produto interno entre os dados transformados sem explicitar essa transforma√ß√£o:

$$ K(x, x') = \langle \phi(x), \phi(x') \rangle $$

A conex√£o entre *kernels* e fun√ß√µes de base se torna mais clara quando analisamos a fun√ß√£o de decis√£o da SVM:

$$ f(x) = \sum_{i \in SV} \alpha_i y_i K(x_i, x) + \beta_0 $$

Essa equa√ß√£o pode ser vista como uma combina√ß√£o linear de fun√ß√µes de base, onde cada fun√ß√£o de base √© definida implicitamente pelo *kernel* e um vetor de suporte. No caso de um *kernel* linear, a transforma√ß√£o se reduz a identidade e o espa√ßo de *features* √© igual ao espa√ßo original.

O uso de *kernels* oferece uma forma mais flex√≠vel e poderosa de lidar com a n√£o linearidade, pois n√£o √© necess√°rio explicitar a forma das fun√ß√µes de base, e o espa√ßo de *features* transformado pode ter dimens√£o muito alta ou at√© mesmo infinita.

> üí° **Exemplo Num√©rico:**
>
> Considere o kernel polinomial de grau 2: $K(x, x') = (x^T x' + 1)^2$.  Este kernel implicitamente realiza uma transforma√ß√£o para um espa√ßo de *features* de maior dimens√£o. Por exemplo, em um espa√ßo 2D, ele mapeia os dados para um espa√ßo com termos at√© grau 2, similar √†s fun√ß√µes de base polinomiais.
>
>  Se $x = [2, 3]$ e $x' = [1, 2]$, ent√£o:
>
>  $K(x, x') = ( [2, 3]^T [1, 2] + 1)^2 = (2*1 + 3*2 + 1)^2 = (2 + 6 + 1)^2 = 9^2 = 81$
>
>  O kernel polinomial calcula o produto interno no espa√ßo transformado sem precisar explicitar essa transforma√ß√£o, o que √© mais eficiente computacionalmente.

**Corol√°rio 3:** A utiliza√ß√£o de *kernels* pode ser vista como uma forma de utilizar fun√ß√µes de base implicitamente, o que permite trabalhar em espa√ßos de *features* de alta dimens√£o e construir fronteiras de decis√£o complexas de forma eficiente.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise da fun√ß√£o de decis√£o da SVM com *kernels* e como essa fun√ß√£o pode ser vista como uma combina√ß√£o linear de fun√ß√µes de base impl√≠citas. A dualidade de Wolfe √© fundamental para a obten√ß√£o desse resultado, pois ela demonstra que o modelo SVM pode ser expresso em termos de produtos internos, o que possibilita a utiliza√ß√£o de *kernels*.

### Conclus√£o

Neste cap√≠tulo, exploramos a rela√ß√£o entre as **SVMs** e as **fun√ß√µes de base**, e como ambas as abordagens permitem lidar com a n√£o linearidade em problemas de classifica√ß√£o e regress√£o. Vimos como as fun√ß√µes de base transformam os dados de entrada para um espa√ßo de *features* de maior dimens√£o, onde um modelo linear pode ser utilizado para modelar rela√ß√µes n√£o lineares no espa√ßo original.

Analisamos a defini√ß√£o das fun√ß√µes de base, seus tipos mais comuns e como a escolha das fun√ß√µes de base afeta a capacidade de generaliza√ß√£o do modelo. Vimos que as SVMs com fun√ß√µes de base constroem fronteiras de decis√£o n√£o lineares atrav√©s da aplica√ß√£o de fun√ß√µes de base, e que a escolha dessas fun√ß√µes afeta a complexidade e a capacidade do modelo de modelar rela√ß√µes complexas entre as *features*.

Exploramos tamb√©m como a utiliza√ß√£o de *kernels* pode ser vista como uma forma de utilizar fun√ß√µes de base de forma impl√≠cita, oferecendo uma abordagem mais flex√≠vel e computacionalmente eficiente para trabalhar em espa√ßos de *features* de alta dimens√£o. A compreens√£o da rela√ß√£o entre SVMs, fun√ß√µes de base e *kernels* fornece uma base s√≥lida para o estudo avan√ßado desse poderoso m√©todo de aprendizado de m√°quina.

### Footnotes

[^12.1]: "In this chapter we describe generalizations of linear decision boundaries for classification. Optimal separating hyperplanes are introduced in Chapter 4 for the case when two classes are linearly separable. Here we cover extensions to the nonseparable case, where the classes overlap. These techniques are then generalized to what is known as the support vector machine, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.2]: "In Chapter 4 we discussed a technique for constructing an optimal separating hyperplane between two perfectly separated classes. We review this and generalize to the nonseparable case, where the classes may not be separable by a linear boundary." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.3]: "The support vector machine classifier is an extension of this idea, where the dimension of the enlarged space is allowed to get very large, infinite in some cases. It might seem that the computations would become prohibitive. It would also seem that with sufficient basis functions, the data would be separable, and overfitting would occur. We first show how the SVM technology deals with these issues. We then see that in fact the SVM classifier is solving a function-fitting problem using a particular criterion and form of regularization, and is part of a much bigger class of problems that includes the smoothing splines of Chapter 5." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*
