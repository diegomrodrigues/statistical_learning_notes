Okay, here's the enhanced text with Mermaid diagrams as requested:

## TÃ­tulo: SVMs e FunÃ§Ãµes de Base: Expandindo o EspaÃ§o de Features para Modelagem NÃ£o Linear

```mermaid
graph LR
    subgraph "Feature Space Transformation"
        direction TB
        A["Original Feature Space 'x'"]
        B["Basis Functions 'h_m(x)'"]
        C["Transformed Feature Space 'h(x)'"]
        A --> B
        B --> C
        style A fill:#f9f,stroke:#333,stroke-width:2px
        style C fill:#ccf,stroke:#333,stroke-width:2px
    end
    
```

### IntroduÃ§Ã£o

Em muitos problemas de classificaÃ§Ã£o e regressÃ£o, as relaÃ§Ãµes entre as *features* e as variÃ¡veis de resposta sÃ£o nÃ£o lineares. Para lidar com essa nÃ£o linearidade, os modelos de aprendizado de mÃ¡quina podem utilizar **funÃ§Ãµes de base** para transformar o espaÃ§o de *features* original em um espaÃ§o de maior dimensÃ£o, onde essas relaÃ§Ãµes podem ser modeladas de forma mais eficiente. As **Support Vector Machines (SVMs)** tambÃ©m podem se beneficiar do uso de funÃ§Ãµes de base, e essa abordagem fornece uma perspectiva complementar sobre a utilizaÃ§Ã£o de *kernels*.

Neste capÃ­tulo, exploraremos como as funÃ§Ãµes de base sÃ£o utilizadas para estender o espaÃ§o de *features* das SVMs, e como essa transformaÃ§Ã£o se relaciona com a utilizaÃ§Ã£o dos *kernels*. Discutiremos a definiÃ§Ã£o das funÃ§Ãµes de base, como elas sÃ£o aplicadas aos dados de entrada, e como elas influenciam a forma da fronteira de decisÃ£o ou da funÃ§Ã£o de regressÃ£o. AlÃ©m disso, analisaremos como a escolha das funÃ§Ãµes de base e seus parÃ¢metros impactam a capacidade de generalizaÃ§Ã£o do modelo e o seu desempenho em dados novos.

### FunÃ§Ãµes de Base: DefiniÃ§Ã£o e TransformaÃ§Ã£o do EspaÃ§o de Features

**Conceito 1: DefiniÃ§Ã£o de FunÃ§Ãµes de Base**

As **funÃ§Ãµes de base** sÃ£o funÃ§Ãµes matemÃ¡ticas, denotadas por $h_m(x)$, que transformam os vetores de *features* $x$ em um espaÃ§o de maior dimensÃ£o, onde a modelagem pode ser feita de forma mais simples e eficiente. As funÃ§Ãµes de base sÃ£o um conjunto de funÃ§Ãµes predefinidas, e a transformaÃ§Ã£o do espaÃ§o de *features* Ã© realizada atravÃ©s da aplicaÃ§Ã£o dessas funÃ§Ãµes a cada amostra.

Formalmente, se temos $M$ funÃ§Ãµes de base $h_1(x), h_2(x), ..., h_M(x)$, cada amostra $x$ no espaÃ§o original de *features* Ã© transformada em um vetor de *features* $\textbf{h}(x)$ no novo espaÃ§o, onde $\textbf{h}(x) = [h_1(x), h_2(x), \ldots, h_M(x)]$. Este vetor $\textbf{h}(x)$ representa a amostra no novo espaÃ§o de *features* transformado.

```mermaid
graph LR
    subgraph "Basis Function Transformation"
        direction LR
        A["Input Vector 'x'"] --> B["Basis Functions: 'h_1(x), h_2(x), ..., h_M(x)'"]
         B --> C["Transformed Vector: 'h(x) = [h_1(x), h_2(x), ..., h_M(x)]'"]
    end
    style A fill:#fff,stroke:#333,stroke-width:1px
    style C fill:#fff,stroke:#333,stroke-width:1px

```

A escolha das funÃ§Ãµes de base Ã© crucial para o desempenho do modelo, e deve ser feita considerando a natureza dos dados e as relaÃ§Ãµes nÃ£o lineares que se deseja capturar. As funÃ§Ãµes de base podem ser polinomiais, radiais, trigonomÃ©tricas, splines ou outras funÃ§Ãµes que gerem um espaÃ§o onde as classes ou a resposta de regressÃ£o sejam representÃ¡veis.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos uma amostra $x = [2, 3]$ no espaÃ§o de *features* original. Vamos aplicar duas funÃ§Ãµes de base:
>
> 1.  $h_1(x) = x_1^2$
> 2.  $h_2(x) = x_1 * x_2$
>
> A transformaÃ§Ã£o para o novo espaÃ§o de *features* seria:
>
> $\textbf{h}(x) = [h_1(x), h_2(x)] = [2^2, 2 * 3] = [4, 6]$
>
> Assim, a amostra $x = [2, 3]$ no espaÃ§o original Ã© mapeada para $\textbf{h}(x) = [4, 6]$ no novo espaÃ§o. Este novo espaÃ§o, de dimensÃ£o 2, permite criar modelos lineares que correspondem a modelos nÃ£o lineares no espaÃ§o original.

**Lemma 1:** As funÃ§Ãµes de base transformam os dados de entrada para um espaÃ§o de *features* de maior dimensÃ£o, onde relaÃ§Ãµes nÃ£o lineares podem ser modeladas de forma mais eficiente.

A demonstraÃ§Ã£o desse lemma se baseia na anÃ¡lise da definiÃ§Ã£o das funÃ§Ãµes de base, que sÃ£o projetadas para transformar as *features* originais em um conjunto de *features* mais adequado para a modelagem. A transformaÃ§Ã£o permite construir modelos mais flexÃ­veis e capazes de capturar padrÃµes complexos nos dados.

**Conceito 2: Tipos Comuns de FunÃ§Ãµes de Base**

Existem diversas funÃ§Ãµes de base que podem ser utilizadas, dependendo do tipo de modelo e da natureza dos dados:

1.  **FunÃ§Ãµes de Base Polinomial:**
    As funÃ§Ãµes de base polinomial incluem termos como $x_i$, $x_i^2$, $x_i x_j$, $x_i^3$, etc., onde $x_i$ e $x_j$ sÃ£o as *features* originais. As funÃ§Ãµes de base polinomial sÃ£o utilizadas para modelar relaÃ§Ãµes nÃ£o lineares em forma de polinÃ´mios.

    > ðŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Para uma amostra $x = [x_1, x_2]$, as funÃ§Ãµes de base polinomiais de grau 2 poderiam ser:
    >
    > $h_1(x) = 1$ (bias term)
    > $h_2(x) = x_1$
    > $h_3(x) = x_2$
    > $h_4(x) = x_1^2$
    > $h_5(x) = x_2^2$
    > $h_6(x) = x_1 x_2$
    >
    > Se $x = [2, 3]$, entÃ£o $\textbf{h}(x) = [1, 2, 3, 4, 9, 6]$. O espaÃ§o de *features* foi expandido de 2 dimensÃµes para 6.

```mermaid
graph LR
    subgraph "Polynomial Basis Functions"
    direction TB
        A["Input 'x = [x_1, x_2]'"]
        B["Polynomial Basis: 'h(x) = [1, x_1, x_2, x_1^2, x_2^2, x_1x_2]'"]
        A --> B
    end
    style A fill:#fff,stroke:#333,stroke-width:1px
    style B fill:#fff,stroke:#333,stroke-width:1px

```

2.  **FunÃ§Ãµes de Base Radial (RBF):**
    As funÃ§Ãµes de base radial sÃ£o funÃ§Ãµes gaussianas centradas em pontos especÃ­ficos, e tÃªm a forma:
    $$ h_m(x) = \exp(-\gamma ||x - c_m||^2) $$
    onde $c_m$ Ã© o centro da funÃ§Ã£o radial e $\gamma$ controla sua largura. As funÃ§Ãµes de base radial sÃ£o utilizadas para modelar relaÃ§Ãµes nÃ£o lineares com comportamento localizado.

    > ðŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Vamos usar uma funÃ§Ã£o RBF com centro $c_1 = [1, 1]$ e $\gamma = 0.5$. Para a amostra $x = [2, 2]$:
    >
    > $h_1(x) = \exp(-0.5 * ||[2, 2] - [1, 1]||^2)$
    > $h_1(x) = \exp(-0.5 * ( (2-1)^2 + (2-1)^2 ) )$
    > $h_1(x) = \exp(-0.5 * (1 + 1)) = \exp(-1) \approx 0.368$
    >
    > Se adicionarmos outro RBF com centro $c_2 = [3, 3]$, entÃ£o terÃ­amos outra feature $h_2(x)$ e o vetor $\textbf{h}(x)$ teria duas dimensÃµes.

```mermaid
graph LR
    subgraph "Radial Basis Function (RBF)"
    direction TB
        A["Input Vector 'x'"]
        B["RBF Center 'c_m'"]
        C["Parameter 'Î³'"]
        D["RBF Function: 'h_m(x) = exp(-Î³||x - c_m||Â²)'"]
        A & B & C --> D
    end
     style A fill:#fff,stroke:#333,stroke-width:1px
     style D fill:#fff,stroke:#333,stroke-width:1px
```

3.  **FunÃ§Ãµes de Base Spline:**
    As funÃ§Ãµes de base spline sÃ£o funÃ§Ãµes polinomiais segmentadas, que sÃ£o definidas por partes. Elas sÃ£o utilizadas para modelar relaÃ§Ãµes nÃ£o lineares suaves e flexÃ­veis, e sÃ£o especialmente Ãºteis em problemas de regressÃ£o.

4.  **Outras FunÃ§Ãµes de Base:**
    Existem outras funÃ§Ãµes de base que podem ser utilizadas dependendo do problema em questÃ£o, como funÃ§Ãµes trigonomÃ©tricas, funÃ§Ãµes lineares por partes, funÃ§Ãµes de *wavelet*, etc.

**CorolÃ¡rio 1:** A escolha do tipo de funÃ§Ãµes de base depende do problema em questÃ£o, e a combinaÃ§Ã£o de diferentes funÃ§Ãµes de base pode ser utilizada para modelar relaÃ§Ãµes nÃ£o lineares complexas.

A demonstraÃ§Ã£o desse corolÃ¡rio se baseia na anÃ¡lise das propriedades de diferentes tipos de funÃ§Ãµes de base, e como essas propriedades se relacionam com a natureza dos dados e com os objetivos do modelo. A combinaÃ§Ã£o de funÃ§Ãµes de base de diferentes tipos permite modelar relacionamentos mais complexos.

### SVMs e FunÃ§Ãµes de Base: Uma Perspectiva Linear no EspaÃ§o Transformado

```mermaid
graph LR
    subgraph "SVM with Basis Functions"
        direction TB
        A["Original Feature Space 'x' (Non-Linear)"]
        B["Basis Function Transformation 'h(x)'"]
        C["Transformed Feature Space 'h(x)' (Linear)"]
        D["Linear Separating Hyperplane in 'h(x)'"]
        E["Non-Linear Decision Boundary in 'x'"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#f9f,stroke:#333,stroke-width:2px
```

Ao utilizar funÃ§Ãµes de base, o objetivo das SVMs passa a ser encontrar um **hiperplano separador linear** no espaÃ§o de *features* transformado, onde o vetor de *features* de entrada Ã© agora $\textbf{h}(x)$, que corresponde a um modelo nÃ£o linear no espaÃ§o original das *features*. A funÃ§Ã£o de decisÃ£o do modelo no espaÃ§o de *features* transformado Ã© dada por:

$$ f(x) = \beta^T \textbf{h}(x) + \beta_0 $$

onde $\beta$ Ã© o vetor normal ao hiperplano no espaÃ§o de *features* transformado, $\beta_0$ Ã© o *bias*, e $\textbf{h}(x)$ Ã© o vetor de *features* transformado. Essa funÃ§Ã£o de decisÃ£o Ã© linear no espaÃ§o de *features* transformado, mas nÃ£o linear no espaÃ§o de *features* original.

A utilizaÃ§Ã£o de funÃ§Ãµes de base nas SVMs permite construir modelos nÃ£o lineares de forma similar ao uso de *kernels*, onde Ã© encontrado um separador linear em um espaÃ§o transformado. No entanto, ao contrÃ¡rio dos kernels, que podem ser vistos como funÃ§Ãµes implÃ­citas de um produto interno em um espaÃ§o transformado (sem explicitar a transformaÃ§Ã£o), as funÃ§Ãµes de base especificam explicitamente uma transformaÃ§Ã£o para um determinado espaÃ§o.

The optimization problem for an SVM with basis functions can be formulated as:

```mermaid
graph LR
    subgraph "Optimization Problem"
        direction TB
        A["Minimize: '1/2 ||Î²||Â² + C âˆ‘Î¾_i'"]
        B["Subject to: 'y_i(Î²^T h(x_i) + Î²_0) â‰¥ 1 - Î¾_i'"]
        C["Constraint: 'Î¾_i â‰¥ 0'"]
        A --> B
        B --> C
    end
style A fill:#fff,stroke:#333,stroke-width:1px
style B fill:#fff,stroke:#333,stroke-width:1px

```

$$ \min_{\beta, \beta_0, \xi} \frac{1}{2} ||\beta||^2 + C \sum_{i=1}^{N} \xi_i $$

sujeito a:

$$ y_i(\beta^T \textbf{h}(x_i) + \beta_0) \geq 1 - \xi_i, \quad \forall i $$
$$ \xi_i \geq 0, \quad \forall i $$

O processo de otimizaÃ§Ã£o Ã© similar ao caso de SVMs sem funÃ§Ãµes de base, mas agora o hiperplano Ã© definido no espaÃ§o de *features* transformado.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos duas classes de pontos no espaÃ§o original 2D que nÃ£o sÃ£o linearmente separÃ¡veis. ApÃ³s aplicar funÃ§Ãµes de base polinomiais de grau 2 como mostrado no exemplo anterior, temos um novo espaÃ§o de 6 dimensÃµes. O SVM agora busca um hiperplano linear nesse espaÃ§o de 6 dimensÃµes.
>
> Seja $\textbf{h}(x) = [1, x_1, x_2, x_1^2, x_2^2, x_1 x_2]$. O problema de otimizaÃ§Ã£o da SVM agora se torna:
>
> $$ \min_{\beta, \beta_0, \xi} \frac{1}{2} ||\beta||^2 + C \sum_{i=1}^{N} \xi_i $$
>
> sujeito a:
>
> $$ y_i(\beta_0 + \beta_1 + \beta_2 x_{i1} + \beta_3 x_{i2} + \beta_4 x_{i1}^2 + \beta_5 x_{i2}^2 + \beta_6 x_{i1}x_{i2}) \geq 1 - \xi_i, \quad \forall i $$
> $$ \xi_i \geq 0, \quad \forall i $$
>
> Onde $\beta = [\beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6]$ e o vetor $\beta$ define o hiperplano nesse espaÃ§o transformado. A funÃ§Ã£o de decisÃ£o no espaÃ§o transformado, que Ã© linear, corresponde a uma fronteira de decisÃ£o nÃ£o linear no espaÃ§o original de duas dimensÃµes.

**Lemma 3:** A utilizaÃ§Ã£o de funÃ§Ãµes de base permite que as SVMs construam modelos nÃ£o lineares no espaÃ§o original das *features*, atravÃ©s da construÃ§Ã£o de um hiperplano separador linear no espaÃ§o de *features* transformado.

A demonstraÃ§Ã£o desse lemma se baseia na anÃ¡lise da funÃ§Ã£o de decisÃ£o da SVM com funÃ§Ãµes de base, onde a linearidade Ã© obtida no espaÃ§o transformado, mas se traduz em nÃ£o linearidade no espaÃ§o original.

### A ConexÃ£o com Kernels: Uma VisÃ£o Alternativa

```mermaid
graph LR
    subgraph "Basis Functions vs Kernels"
        direction LR
        A["Basis Functions: Explicit Transformation 'h(x)'"]
        B["Kernels: Implicit Transformation 'K(x, x') = <Ï†(x), Ï†(x')>'"]
        A --> C["Linear Model in Transformed Space"]
        B --> C
        
    end
    style A fill:#fff,stroke:#333,stroke-width:1px
     style B fill:#fff,stroke:#333,stroke-width:1px

```

A utilizaÃ§Ã£o de **kernels** em SVMs pode ser vista como uma forma de utilizar funÃ§Ãµes de base de forma implÃ­cita. O *kernel trick*, discutido em capÃ­tulos anteriores, permite que as SVMs operem em espaÃ§os de *features* transformados, sem explicitar a transformaÃ§Ã£o atravÃ©s da funÃ§Ã£o $\phi(x)$.

Em vez de definir explicitamente as funÃ§Ãµes de base $h_m(x)$ e construir o espaÃ§o de *features* transformado $\textbf{h}(x)$, o *kernel* calcula o produto interno entre os dados transformados sem explicitar essa transformaÃ§Ã£o:

$$ K(x, x') = \langle \phi(x), \phi(x') \rangle $$

A conexÃ£o entre *kernels* e funÃ§Ãµes de base se torna mais clara quando analisamos a funÃ§Ã£o de decisÃ£o da SVM:

$$ f(x) = \sum_{i \in SV} \alpha_i y_i K(x_i, x) + \beta_0 $$

Essa equaÃ§Ã£o pode ser vista como uma combinaÃ§Ã£o linear de funÃ§Ãµes de base, onde cada funÃ§Ã£o de base Ã© definida implicitamente pelo *kernel* e um vetor de suporte. No caso de um *kernel* linear, a transformaÃ§Ã£o se reduz a identidade e o espaÃ§o de *features* Ã© igual ao espaÃ§o original.

O uso de *kernels* oferece uma forma mais flexÃ­vel e poderosa de lidar com a nÃ£o linearidade, pois nÃ£o Ã© necessÃ¡rio explicitar a forma das funÃ§Ãµes de base, e o espaÃ§o de *features* transformado pode ter dimensÃ£o muito alta ou atÃ© mesmo infinita.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere o kernel polinomial de grau 2: $K(x, x') = (x^T x' + 1)^2$.  Este kernel implicitamente realiza uma transformaÃ§Ã£o para um espaÃ§o de *features* de maior dimensÃ£o. Por exemplo, em um espaÃ§o 2D, ele mapeia os dados para um espaÃ§o com termos atÃ© grau 2, similar Ã s funÃ§Ãµes de base polinomiais.
>
>  Se $x = [2, 3]$ e $x' = [1, 2]$, entÃ£o:
>
>  $K(x, x') = ( [2, 3]^T [1, 2] + 1)^2 = (2*1 + 3*2 + 1)^2 = (2 + 6 + 1)^2 = 9^2 = 81$
>
>  O kernel polinomial calcula o produto interno no espaÃ§o transformado sem precisar explicitar essa transformaÃ§Ã£o, o que Ã© mais eficiente computacionalmente.

**CorolÃ¡rio 3:** A utilizaÃ§Ã£o de *kernels* pode ser vista como uma forma de utilizar funÃ§Ãµes de base implicitamente, o que permite trabalhar em espaÃ§os de *features* de alta dimensÃ£o e construir fronteiras de decisÃ£o complexas de forma eficiente.

A demonstraÃ§Ã£o desse corolÃ¡rio se baseia na anÃ¡lise da funÃ§Ã£o de decisÃ£o da SVM com *kernels* e como essa funÃ§Ã£o pode ser vista como uma combinaÃ§Ã£o linear de funÃ§Ãµes de base implÃ­citas. A dualidade de Wolfe Ã© fundamental para a obtenÃ§Ã£o desse resultado, pois ela demonstra que o modelo SVM pode ser expresso em termos de produtos internos, o que possibilita a utilizaÃ§Ã£o de *kernels*.

### ConclusÃ£o

Neste capÃ­tulo, exploramos a relaÃ§Ã£o entre as **SVMs** e as **funÃ§Ãµes de base**, e como ambas as abordagens permitem lidar com a nÃ£o linearidade em problemas de classificaÃ§Ã£o e regressÃ£o. Vimos como as funÃ§Ãµes de base transformam os dados de entrada para um espaÃ§o de *features* de maior dimensÃ£o, onde um modelo linear pode ser utilizado para modelar relaÃ§Ãµes nÃ£o lineares no espaÃ§o original.

Analisamos a definiÃ§Ã£o das funÃ§Ãµes de base, seus tipos mais comuns e como a escolha das funÃ§Ãµes de base afeta a capacidade de generalizaÃ§Ã£o do modelo. Vimos que as SVMs com funÃ§Ãµes de base constroem fronteiras de decisÃ£o nÃ£o lineares atravÃ©s da aplicaÃ§Ã£o de funÃ§Ãµes de base, e que a escolha dessas funÃ§Ãµes afeta a complexidade e a capacidade do modelo de modelar relaÃ§Ãµes complexas entre as *features*.

Exploramos tambÃ©m como a utilizaÃ§Ã£o de *kernels* pode ser vista como uma forma de utilizar funÃ§Ãµes de base de forma implÃ­cita, oferecendo uma abordagem mais flexÃ­vel e computacionalmente eficiente para trabalhar em espaÃ§os de *features* de alta dimensÃ£o. A compreensÃ£o da relaÃ§Ã£o entre SVMs, funÃ§Ãµes de base e *kernels* fornece uma base sÃ³lida para o estudo avanÃ§ado desse poderoso mÃ©todo de aprendizado de mÃ¡quina.

### Footnotes

[^12.1]: "In this chapter we describe generalizations of linear decision boundaries for classification. Optimal separating hyperplanes are introduced in Chapter 4 for the case when two classes are linearly separable. Here we cover extensions to the nonseparable case, where the classes overlap. These techniques are then generalized to what is known as the support vector machine, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.2]: "In Chapter 4 we discussed a technique for constructing an optimal separating hyperplane between two perfectly separated classes. We review this and generalize to the nonseparable case, where the classes may not be separable by a linear boundary." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.3]: "The support vector machine classifier is an extension of this idea, where the dimension of the enlarged space is allowed to get very large, infinite in some cases. It might seem that the computations would become prohibitive. It would also seem that with sufficient basis functions, the data would be separable, and overfitting would occur. We first show how the SVM technology deals with these issues. We then see that in fact the SVM classifier is solving a function-fitting problem using a particular criterion and form of regularization, and is part of a much bigger class of problems that includes the smoothing splines of Chapter 5." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*
