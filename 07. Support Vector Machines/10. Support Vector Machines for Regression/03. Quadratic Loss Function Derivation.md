Okay, let's enhance the text with practical numerical examples to illustrate the concepts discussed.

## T√≠tulo: SVMs com Perda Quadr√°tica: Deriva√ß√£o dos Pesos e Conex√£o com M√©todos de Regress√£o

```mermaid
graph LR
    subgraph "SVM with Quadratic Loss"
        direction TB
        A["Input Data"] --> B["Quadratic Loss Function: $(y_i - f(x_i))^2$"]
        B --> C["Cost Function Minimization"]
        C --> D["Weight Derivation: Œ≤, Œ≤_0"]
        D --> E["Decision Boundary"]
    end
    subgraph "SVM with Hinge Loss"
        direction TB
        F["Input Data"] --> G["Hinge Loss Function"]
        G --> H["Cost Function Minimization"]
         H --> I["Weight Derivation: Œ≤, Œ≤_0"]
        I --> J["Decision Boundary"]
    end
    K["Comparison of Decision Boundaries"] --> E & J

```

### Introdu√ß√£o

Embora as **Support Vector Machines (SVMs)** sejam geralmente associadas √† utiliza√ß√£o da fun√ß√£o de perda *hinge loss* para problemas de classifica√ß√£o e da fun√ß√£o de perda Œµ-insens√≠vel para problemas de regress√£o, a an√°lise do comportamento das SVMs com uma **fun√ß√£o de perda quadr√°tica** fornece uma perspectiva interessante sobre a formula√ß√£o matem√°tica do modelo e sua rela√ß√£o com outros m√©todos de aprendizado de m√°quina, como a regress√£o linear.

Neste cap√≠tulo, exploraremos como a utiliza√ß√£o da fun√ß√£o de perda quadr√°tica afeta a deriva√ß√£o dos pesos nas SVMs e como essa abordagem se conecta com a solu√ß√£o de problemas de ajuste de fun√ß√£o. Analisaremos as diferen√ßas entre a utiliza√ß√£o da fun√ß√£o de perda quadr√°tica e a utiliza√ß√£o da fun√ß√£o de perda *hinge loss* e como essas diferen√ßas impactam a forma da fronteira de decis√£o e a capacidade de generaliza√ß√£o do modelo. Al√©m disso, examinaremos a rela√ß√£o entre a solu√ß√£o obtida com o uso da fun√ß√£o de perda quadr√°tica e m√©todos tradicionais de regress√£o, como a regress√£o linear.

A compreens√£o da utiliza√ß√£o da fun√ß√£o de perda quadr√°tica no contexto das SVMs fornece *insights* valiosos sobre a natureza desse m√©todo e como sua formula√ß√£o pode ser adaptada para diferentes tipos de problemas e dados.

### Deriva√ß√£o dos Pesos com Perda Quadr√°tica

**Conceito 1: Fun√ß√£o de Perda Quadr√°tica e a Formula√ß√£o do Problema**

No contexto das SVMs, vamos analisar o efeito da substitui√ß√£o da fun√ß√£o de perda *hinge loss* pela **fun√ß√£o de perda quadr√°tica**, que penaliza o erro ao quadrado, similar ao que √© feito na regress√£o linear. A fun√ß√£o de perda quadr√°tica √© dada por:

$$ L(y_i, f(x_i)) = (y_i - f(x_i))^2 $$

onde $y_i$ √© o r√≥tulo da classe e $f(x_i)$ √© a predi√ß√£o do modelo. A fun√ß√£o de perda quadr√°tica penaliza erros de predi√ß√£o de forma proporcional ao quadrado da magnitude do erro.

Utilizando essa fun√ß√£o de perda, a fun√ß√£o de custo da SVM pode ser expressa como:

$$ \min_{\beta, \beta_0} \frac{1}{2} ||\beta||^2 + C \sum_{i=1}^N (y_i - (x_i^T\beta + \beta_0))^2 $$

onde o primeiro termo √© o termo de regulariza√ß√£o que controla a complexidade do modelo e o segundo termo penaliza o erro quadr√°tico entre as predi√ß√µes e os r√≥tulos das classes.

> üí° **Exemplo Num√©rico:**
>
> Imagine um conjunto de dados simples com duas amostras, onde $x_1 = [1, 2]^T$, $y_1 = 1$ e $x_2 = [2, 1]^T$, $y_2 = -1$. Vamos supor $C = 1$.  A fun√ß√£o de custo a ser minimizada √©:
>
> $$ J(\beta, \beta_0) = \frac{1}{2} ||\beta||^2 + 1 \cdot \left[ (1 - (x_1^T\beta + \beta_0))^2 + (-1 - (x_2^T\beta + \beta_0))^2 \right] $$
>
> Expandindo:
>
> $$ J(\beta, \beta_0) = \frac{1}{2} (\beta_1^2 + \beta_2^2) + (1 - (\beta_1 + 2\beta_2 + \beta_0))^2 + (-1 - (2\beta_1 + \beta_2 + \beta_0))^2 $$
>
> Este exemplo ilustra como a fun√ß√£o de custo penaliza tanto a magnitude dos pesos ($\beta$) quanto os erros de predi√ß√£o.

**Lemma 1:** A substitui√ß√£o da fun√ß√£o de perda *hinge loss* pela fun√ß√£o de perda quadr√°tica leva a um problema de otimiza√ß√£o que penaliza todos os erros de forma proporcional ao quadrado de sua magnitude, o que √© diferente da *hinge loss*, que penaliza erros de classifica√ß√£o de forma linear, apenas quando a amostra est√° do lado errado da margem.

```mermaid
graph LR
    subgraph "Loss Function Comparison"
    direction LR
        A["Hinge Loss: max(0, 1 - y_i * f(x_i))"] --> B["Linear Penalty for Misclassifications"]
        C["Quadratic Loss: $(y_i - f(x_i))^2$"] --> D["Quadratic Penalty for All Errors"]
    end
    B --> E["Sparse Solution (Support Vectors)"]
    D --> F["Influence of All Data Points"]
    E --> G["Robust Decision Boundary"]
    F --> H["Decision Boundary More Sensitive to Outliers"]
```

A demonstra√ß√£o desse lemma se baseia na an√°lise da forma da fun√ß√£o de perda quadr√°tica e como ela penaliza erros grandes de forma mais acentuada que a *hinge loss*.

**Conceito 2: Deriva√ß√£o dos Pesos utilizando M√≠nimos Quadrados**

Para encontrar a solu√ß√£o √≥tima do problema de otimiza√ß√£o com a fun√ß√£o de perda quadr√°tica, podemos utilizar o m√©todo dos **m√≠nimos quadrados**. A ideia √© encontrar os valores de $\beta$ e $\beta_0$ que minimizam a fun√ß√£o de custo acima. A deriva√ß√£o dos pesos envolve calcular as derivadas parciais da fun√ß√£o de custo em rela√ß√£o a $\beta$ e $\beta_0$ e igualar essas derivadas a zero.

A derivada da fun√ß√£o de custo em rela√ß√£o a $\beta$ √© dada por:

$$ \frac{\partial}{\partial \beta} \left[ \frac{1}{2} ||\beta||^2 + C \sum_{i=1}^N (y_i - (x_i^T\beta + \beta_0))^2 \right] = \beta - 2C \sum_{i=1}^N (y_i - (x_i^T\beta + \beta_0))x_i = 0 $$

e em rela√ß√£o a $\beta_0$:

$$ \frac{\partial}{\partial \beta_0} \left[ \frac{1}{2} ||\beta||^2 + C \sum_{i=1}^N (y_i - (x_i^T\beta + \beta_0))^2 \right] = -2C \sum_{i=1}^N (y_i - (x_i^T\beta + \beta_0)) = 0 $$

Da primeira equa√ß√£o, podemos derivar a express√£o para $\beta$:

$$ \beta = 2C \sum_{i=1}^N (y_i - (x_i^T\beta + \beta_0))x_i $$

e da segunda equa√ß√£o, podemos derivar a express√£o para $\beta_0$:

$$ \sum_{i=1}^N (y_i - (x_i^T\beta + \beta_0)) = 0 $$

que pode ser reescrita como:

$$ \beta_0 = \frac{1}{N} \sum_{i=1}^N y_i - \frac{1}{N} \sum_{i=1}^N x_i^T\beta  $$

Essas equa√ß√µes demonstram como o vetor $\beta$ e o par√¢metro $\beta_0$ s√£o relacionados aos dados de treinamento e ao par√¢metro de regulariza√ß√£o $C$ com uma fun√ß√£o de perda quadr√°tica.

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, podemos escrever as equa√ß√µes para $\beta$ e $\beta_0$. Primeiro, vamos calcular $\beta_0$:
>
> $$ \beta_0 = \frac{1}{2} (1 + (-1)) - \frac{1}{2} \left( [1, 2]\beta + [2, 1]\beta \right) =  - \frac{1}{2} (3\beta_1 + 3\beta_2) = - \frac{3}{2}(\beta_1 + \beta_2) $$
>
> Agora, vamos calcular $\beta$:
>
> $$ \beta = 2 \cdot 1 \cdot \left( (1 - (\beta_1 + 2\beta_2 + \beta_0))[1, 2]^T + (-1 - (2\beta_1 + \beta_2 + \beta_0))[2, 1]^T \right) $$
>
> Substituindo $\beta_0$:
>
> $$ \beta = 2 \cdot \left( (1 - (\beta_1 + 2\beta_2 - \frac{3}{2}(\beta_1 + \beta_2)))[1, 2]^T + (-1 - (2\beta_1 + \beta_2 - \frac{3}{2}(\beta_1 + \beta_2)))[2, 1]^T \right) $$
>
> Simplificando:
>
> $$ \beta = 2 \cdot \left( (1 + \frac{1}{2}\beta_1 - \frac{1}{2}\beta_2)[1, 2]^T + (-1 - \frac{1}{2}\beta_1 + \frac{1}{2}\beta_2)[2, 1]^T \right) $$
>
> $$ \beta = \left[ 2(1 + \frac{1}{2}\beta_1 - \frac{1}{2}\beta_2) + 4(-1 - \frac{1}{2}\beta_1 + \frac{1}{2}\beta_2), 4(1 + \frac{1}{2}\beta_1 - \frac{1}{2}\beta_2) + 2(-1 - \frac{1}{2}\beta_1 + \frac{1}{2}\beta_2) \right]^T $$
>
> $$ \beta = \left[ -2 - \beta_1 + \beta_2, 2 + \beta_1 - \beta_2 \right]^T $$
>
>  Este sistema de equa√ß√µes pode ser resolvido numericamente para encontrar $\beta_1$ e $\beta_2$, que em conjunto com $\beta_0$ definem o modelo.

**Corol√°rio 1:** A utiliza√ß√£o da fun√ß√£o de perda quadr√°tica leva a uma solu√ß√£o em forma fechada para o vetor $\beta$ e o par√¢metro $\beta_0$ por meio do m√©todo dos m√≠nimos quadrados, com uma depend√™ncia do par√¢metro de regulariza√ß√£o $C$ e dos dados de treinamento.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise das derivadas da fun√ß√£o de custo e como elas s√£o usadas para derivar as express√µes de $\beta$ e $\beta_0$, que s√£o definidas em termos de uma equa√ß√£o que pode ser resolvida atrav√©s dos m√©todos de m√≠nimos quadrados.

### Conex√£o com M√©todos de Regress√£o Linear

```mermaid
graph LR
    subgraph "Connection to Linear Regression"
        direction TB
        A["SVM with Quadratic Loss"] --> B["Cost Function: Regularized Quadratic Loss"]
        C["Linear Regression (Ridge)"] --> D["Cost Function: Regularized RSS"]
        B --> E("Similar Cost Function Structure")
        D --> E
        E --> F["Parameter C (SVM) <-> Œª (Regression)"]
        F --> G["Similar Optimization Problem"]
    end

```

A utiliza√ß√£o da fun√ß√£o de perda quadr√°tica nas SVMs aproxima o m√©todo da **regress√£o linear**. A fun√ß√£o de custo da regress√£o linear com regulariza√ß√£o L2 (ridge regression) √© dada por:

$$ \min_{\beta, \beta_0} \frac{1}{2} ||\beta||^2 + \lambda \sum_{i=1}^N (y_i - (x_i^T\beta + \beta_0))^2 $$

Essa fun√ß√£o de custo √© similar √†quela utilizada na SVM com perda quadr√°tica, com a diferen√ßa de que a constante $C$ na SVM corresponde ao inverso do par√¢metro de regulariza√ß√£o $\lambda$ na regress√£o linear.

A similaridade entre os dois m√©todos demonstra que a SVM com fun√ß√£o de perda quadr√°tica pode ser vista como uma vers√£o regularizada da regress√£o linear, onde o termo de regulariza√ß√£o $\frac{1}{2} ||\beta||^2$ controla a complexidade do modelo, e a fun√ß√£o de perda $(y_i - (x_i^T\beta + \beta_0))^2$ penaliza os erros de predi√ß√£o.

A diferen√ßa fundamental entre as SVMs e a regress√£o linear reside na utiliza√ß√£o da fun√ß√£o de perda *hinge loss* nas SVMs, que leva a solu√ß√µes esparsas (com vetores de suporte) e a um comportamento diferente da fun√ß√£o de decis√£o, que √© mais robusta e menos propensa ao *overfitting*. A fun√ß√£o de perda quadr√°tica √© uma alternativa quando se quer um modelo mais pr√≥ximo da regress√£o linear.

> üí° **Exemplo Num√©rico:**
>
>  Considerando um problema de regress√£o com os mesmos dados do exemplo anterior, onde $x_1 = [1, 2]^T$, $y_1 = 1$ e $x_2 = [2, 1]^T$, $y_2 = -1$, e usando a fun√ß√£o de custo da regress√£o linear com regulariza√ß√£o L2 (Ridge) com $\lambda = 1$.  A fun√ß√£o de custo se torna:
>
> $$ J(\beta, \beta_0) = \frac{1}{2} ||\beta||^2 + 1 \cdot \left[ (1 - (x_1^T\beta + \beta_0))^2 + (-1 - (x_2^T\beta + \beta_0))^2 \right] $$
>
>  Note que esta √© a mesma fun√ß√£o de custo do exemplo anterior para SVM com perda quadr√°tica com $C=1$. Isso demonstra a equival√™ncia entre os dois m√©todos sob certas condi√ß√µes. Se usarmos um valor diferente de $\lambda$, por exemplo $\lambda=0.5$, isso seria equivalente a uma SVM com $C=2$.
>
>  Para encontrar os pesos, podemos usar a mesma estrat√©gia de minimizar a fun√ß√£o de custo derivando e igualando a zero, como feito anteriormente.

**Lemma 3:** A SVM com fun√ß√£o de perda quadr√°tica pode ser vista como uma vers√£o regularizada da regress√£o linear, com uma fun√ß√£o de custo similar, mas com um comportamento diferente da fun√ß√£o de decis√£o em compara√ß√£o com a SVM com *hinge loss*.

A demonstra√ß√£o desse lemma se baseia na compara√ß√£o da fun√ß√£o de custo da SVM com perda quadr√°tica e da fun√ß√£o de custo da regress√£o linear com regulariza√ß√£o L2, mostrando a similaridade das duas formula√ß√µes e como o par√¢metro de regulariza√ß√£o controla a complexidade do modelo em ambos os casos.

### Impacto da Perda Quadr√°tica na Generaliza√ß√£o

```mermaid
graph LR
    subgraph "Generalization Impact"
        direction TB
        A["Hinge Loss SVM"] --> B["Sparse Solution (Support Vectors)"]
        B --> C["Maximizes Margin"]
        C --> D["Robust to Outliers"]
        E["Quadratic Loss SVM"] --> F["All Data Points Influence"]
        F --> G["No Explicit Margin"]
        G --> H["Similar Behavior to Linear Regression"]
         D --> J["Better Generalization in Classification"]
        H --> K["Better Generalization in Regression"]
    end
        J--> L["Classification Task"]
        K--> L
```

A substitui√ß√£o da fun√ß√£o de perda *hinge loss* pela fun√ß√£o de perda quadr√°tica tem um impacto significativo na forma da fronteira de decis√£o e na capacidade de generaliza√ß√£o do modelo SVM.

*   **Fun√ß√£o de Perda Hinge Loss:** A fun√ß√£o de perda *hinge loss* leva a solu√ß√µes esparsas, onde apenas os vetores de suporte s√£o relevantes para a defini√ß√£o do hiperplano separador. Essa fun√ß√£o tamb√©m incentiva a maximiza√ß√£o da margem de separa√ß√£o entre as classes, o que resulta em modelos mais robustos e menos propensos ao *overfitting*.

*   **Fun√ß√£o de Perda Quadr√°tica:** A fun√ß√£o de perda quadr√°tica penaliza todos os erros de forma igualit√°ria, e a solu√ß√£o √© mais influenciada por pontos longe da margem do que a *hinge loss*. A fronteira de decis√£o com perda quadr√°tica n√£o √© definida apenas pelos vetores de suporte e pode apresentar uma maior vari√¢ncia quando comparada com a *hinge loss*. Al√©m disso, a fun√ß√£o de decis√£o n√£o possui um conceito de margem como o definido na SVM com perda hinge, o que leva a um comportamento similar √† regress√£o linear.

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar o impacto na generaliza√ß√£o, vamos considerar um conjunto de dados bidimensional com duas classes. Suponha que temos os seguintes pontos:
>
> Classe 1: $x_1 = [1, 1]^T$, $x_2 = [2, 2]^T$, $x_3 = [1, 2]^T$ ($y_i = 1$)
>
> Classe -1: $x_4 = [3, 1]^T$, $x_5 = [4, 2]^T$, $x_6 = [3, 2]^T$ ($y_i = -1$)
>
> 1. **SVM com Hinge Loss:** Ao treinar uma SVM com hinge loss, a fronteira de decis√£o tender√° a maximizar a margem entre as duas classes, possivelmente selecionando $x_1, x_3, x_4$ e $x_6$ como vetores de suporte.
>
> 2. **SVM com Perda Quadr√°tica:** Ao treinar uma SVM com perda quadr√°tica, todos os pontos ir√£o influenciar a decis√£o.  A fronteira ser√° mais sens√≠vel a todos os pontos e tender√° a se comportar de maneira mais similar a uma regress√£o linear.  A fronteira pode ser mais influenciada por pontos mais distantes da fronteira ideal, o que pode levar a uma menor margem de separa√ß√£o.
>
> Podemos visualizar a diferen√ßa entre as duas abordagens no diagrama abaixo:
>
> ```mermaid
> graph LR
>     A[Dados] -->|Hinge Loss| B(Fronteira de Decis√£o Hinge);
>     A -->|Perda Quadr√°tica| C(Fronteira de Decis√£o Quadr√°tica);
>     B --> D{Margem Maximizada};
>     C --> E{Sem Margem Maximizada};
>     D --> F[Robustez];
>     E --> G[Influ√™ncia de Todos os Pontos];
> ```
>
>  A visualiza√ß√£o mostra que a *hinge loss* busca uma margem maximizada e a perda quadr√°tica n√£o.

Em geral, a fun√ß√£o de perda *hinge loss* √© mais adequada para problemas de classifica√ß√£o, onde o objetivo √© separar as classes com uma margem de seguran√ßa e lidar com *outliers*. A fun√ß√£o de perda quadr√°tica pode ser mais apropriada para problemas de regress√£o, onde se busca minimizar o erro m√©dio de predi√ß√£o, sem uma preocupa√ß√£o com a maximiza√ß√£o da margem.

**Corol√°rio 2:** A utiliza√ß√£o da fun√ß√£o de perda quadr√°tica em SVMs leva a modelos que se aproximam da regress√£o linear, enquanto a utiliza√ß√£o da fun√ß√£o de perda *hinge loss* leva a modelos mais robustos, com margem de separa√ß√£o e esparsidade, com a desvantagem de modelos menos suaves, especialmente em problemas de regress√£o.

A demonstra√ß√£o desse corol√°rio se baseia na an√°lise das propriedades das fun√ß√µes de perda *hinge loss* e quadr√°tica, como elas definem como os erros s√£o penalizados, e como isso se reflete no comportamento da fun√ß√£o de decis√£o e na capacidade de generaliza√ß√£o do modelo.

### Conclus√£o

Neste cap√≠tulo, exploramos a utiliza√ß√£o da **fun√ß√£o de perda quadr√°tica** para derivar os pesos nas **Support Vector Machines (SVMs)**. Vimos como a fun√ß√£o de perda quadr√°tica se relaciona com a formula√ß√£o do problema de otimiza√ß√£o, levando a solu√ß√µes similares √†s obtidas na regress√£o linear com regulariza√ß√£o L2.

Analisamos as diferen√ßas entre a utiliza√ß√£o da fun√ß√£o de perda quadr√°tica e da fun√ß√£o de perda *hinge loss*, destacando como a escolha da fun√ß√£o de perda impacta na forma da fronteira de decis√£o, na localiza√ß√£o dos vetores de suporte e na capacidade de generaliza√ß√£o do modelo. A utiliza√ß√£o da fun√ß√£o de perda quadr√°tica fornece uma vis√£o complementar da formula√ß√£o das SVMs, mostrando a sua rela√ß√£o com m√©todos de regress√£o lineares e como essa rela√ß√£o se afasta ao utilizar a fun√ß√£o de perda *hinge loss*.

A compreens√£o do efeito da fun√ß√£o de perda na formula√ß√£o da SVM permite uma utiliza√ß√£o mais apropriada do m√©todo e uma escolha adequada dos par√¢metros, de acordo com o problema em quest√£o e as caracter√≠sticas dos dados.

### Footnotes

[^12.1]: "In this chapter we describe generalizations of linear decision boundaries for classification. Optimal separating hyperplanes are introduced in Chapter 4 for the case when two classes are linearly separable. Here we cover extensions to the nonseparable case, where the classes overlap. These techniques are then generalized to what is known as the support vector machine, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*

[^12.2]: "In Chapter 4 we discussed a technique for constructing an optimal separating hyperplane between two perfectly separated classes. We review this and generalize to the nonseparable case, where the classes may not be separable by a linear boundary." *(Trecho de  "Support Vector Machines and Flexible Discriminants")*
