## Parametric Bootstrap: Inference and Averaging
```mermaid
graph TB
    subgraph "Parametric Bootstrap Process"
    direction TB
        A["Original Data"] --> B["Fit Model (Maximum Likelihood)"]
        B --> C["Estimate Model Parameters (Œ∏_hat)"]
        C --> D["Generate Synthetic Data (Y*) using model distribution"]
        D --> E["Re-fit Model to Synthetic Data (Œ∏*_hat)"]
        E --> F["Estimate Variability of Model Parameters from (Œ∏*_hat) distribution"]
    end
```

### Introdu√ß√£o
O objetivo central deste cap√≠tulo √© apresentar os m√©todos de **infer√™ncia estat√≠stica** e **model averaging**, com foco no m√©todo de **maximum likelihood** e no **Bayesian approach**. Uma das t√©cnicas de *resampling* mais utilizadas no campo do aprendizado estat√≠stico √© o **bootstrap**, que √© introduzido em [^8.1]. O **parametric bootstrap**  √© um m√©todo que utiliza a **distribui√ß√£o do modelo** para gerar novos conjuntos de dados, diferentemente do **nonparametric bootstrap**, que se baseia em amostras com reposi√ß√£o dos dados de treinamento. Em [^8.2], √© feita uma discuss√£o sobre a rela√ß√£o entre o bootstrap, o m√©todo de maximum likelihood e o Bayesian approach, com o **parametric bootstrap** sendo central nesta an√°lise. Este cap√≠tulo abordar√° o **parametric bootstrap** em profundidade, explorando seus fundamentos, aplica√ß√µes e conex√µes com outros m√©todos.  A t√©cnica do **model averaging**, incluindo **committee methods**, **bagging**, **stacking** e **bumping**, s√£o importantes ferramentas que podem ser usadas para melhorar o desempenho do modelo e a robustez das previs√µes, e est√£o tamb√©m contextualizadas neste cap√≠tulo.

### Conceitos Fundamentais
**Conceito 1: Bootstrap e sua Aplica√ß√£o**
O **bootstrap** √© uma t√©cnica de *resampling* que avalia a incerteza das estimativas, por meio da amostragem dos dados de treinamento [^8.1], [^8.2.1]. Em ess√™ncia, o bootstrap cria m√∫ltiplas amostras replicadas a partir do conjunto de dados original, permitindo a estima√ß√£o da variabilidade das estimativas do modelo. Essa abordagem computacional √© crucial em cen√°rios onde as distribui√ß√µes amostrais te√≥ricas s√£o desconhecidas ou dif√≠ceis de derivar. O parametric bootstrap, em particular, gera novas amostras simulando a distribui√ß√£o de erros assumida no modelo e adicionando esses erros a estimativas do modelo original [^8.2.1].
**Lemma 1:** *A rela√ß√£o entre as estimativas de m√≠nimos quadrados e o bootstrap param√©trico em modelos com erros Gaussianos aditivos.*

Seja o modelo linear $$ Y = X\beta + \epsilon, $$ onde $\epsilon \sim N(0, \sigma^2)$. O estimador de m√≠nimos quadrados para $\beta$ √© dado por $$ \hat{\beta} = (X^TX)^{-1}X^TY. $$. No parametric bootstrap, gera-se amostras replicadas $Y^* = X\hat{\beta} + \epsilon^*$, com $\epsilon^* \sim N(0, \hat{\sigma}^2)$. As estimativas bootstrap $\hat{\beta}^*$ s√£o dadas por $$ \hat{\beta}^* = (X^TX)^{-1}X^TY^*. $$
Mostraremos que $E[\hat{\beta}^*] = \hat{\beta}$.
**Prova:** $$ E[\hat{\beta}^*] = E[(X^TX)^{-1}X^TY^*] = E[(X^TX)^{-1}X^T(X\hat{\beta} + \epsilon^*)] = (X^TX)^{-1}X^T X\hat{\beta} + (X^TX)^{-1}X^TE[\epsilon^*] $$
Como $E[\epsilon^*] = 0$, ent√£o $$ E[\hat{\beta}^*] = \hat{\beta} . \blacksquare$$
> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simples com duas vari√°veis preditoras e 5 observa√ß√µes:
>
> ```python
> import numpy as np
>
> # Dados de exemplo
> X = np.array([[1, 2], [1, 3], [1, 4], [1, 5], [1, 6]])
> y = np.array([3, 4, 5, 6, 7])
>
> # C√°lculo do estimador de m√≠nimos quadrados (OLS)
> X_transpose_X_inv = np.linalg.inv(X.T @ X)
> beta_hat = X_transpose_X_inv @ X.T @ y
> print(f"Estimativa OLS beta_hat: {beta_hat}")
>
> # C√°lculo dos res√≠duos e estimativa da vari√¢ncia do erro
> y_hat = X @ beta_hat
> residuals = y - y_hat
> sigma2_hat = np.sum(residuals**2) / (len(y) - X.shape[1])
> print(f"Estimativa da vari√¢ncia do erro sigma2_hat: {sigma2_hat}")
>
> # Bootstrap param√©trico
> num_bootstraps = 100
> beta_hat_bootstrap = np.zeros((num_bootstraps, X.shape[1]))
>
> for i in range(num_bootstraps):
>     epsilon_star = np.random.normal(0, np.sqrt(sigma2_hat), len(y))
>     y_star = X @ beta_hat + epsilon_star
>     beta_hat_star = np.linalg.inv(X.T @ X) @ X.T @ y_star
>     beta_hat_bootstrap[i] = beta_hat_star
>
> print(f"M√©dia das estimativas bootstrap beta_hat: {np.mean(beta_hat_bootstrap,axis=0)}")
>
> ```
>  O c√≥digo acima mostra um exemplo de regress√£o linear, onde $\hat{\beta}$ (OLS) foi calculado, bem como a vari√¢ncia do erro $\hat{\sigma}^2$. No processo de bootstrap param√©trico, 100 amostras sint√©ticas  $y^*_i$ foram geradas e seus respectivos $\hat{\beta}^*$ foram recalculados. A m√©dia das estimativas bootstrap (aproximadamente igual a `[1.97 , 1.  ]`) deve se aproximar do valor de $\hat{\beta}$ (igual a `[2. , 1.]`). Este exemplo num√©rico demonstra a validade da igualdade $E[\hat{\beta}^*] = \hat{\beta}$ demonstrada anteriormente.
```mermaid
graph TB
    subgraph "Lemma 1: Parametric Bootstrap in Linear Models"
        direction TB
        A["Original Linear Model: Y = XŒ≤ + Œµ"]
        B["OLS Estimator: Œ≤_hat = (X^T X)^-1 X^T Y"]
        C["Parametric Bootstrap: Y* = X Œ≤_hat + Œµ*, Œµ* ~ N(0, œÉ_hat^2)"]
        D["Bootstrap Estimator: Œ≤*_hat = (X^T X)^-1 X^T Y*"]
        E["Result: E[Œ≤*_hat] = Œ≤_hat"]
        A --> B
        A --> C
        C --> D
        D --> E
    end
```
**Conceito 2: Maximum Likelihood Inference**
O m√©todo de **maximum likelihood** busca encontrar os par√¢metros que maximizam a verossimilhan√ßa dos dados observados [^8.1], [^8.2.2]. Dada uma amostra $Z = \{z_1, z_2, ..., z_N\}$ e uma fun√ß√£o de densidade (ou massa) de probabilidade $g_\theta(z)$, a fun√ß√£o de verossimilhan√ßa √© definida como $$ L(\theta; Z) = \prod_{i=1}^N g_\theta(z_i). $$
O objetivo √© encontrar o valor de $\theta$ que maximiza essa fun√ß√£o. Na pr√°tica, √© mais comum trabalhar com a log-verossimilhan√ßa: $$ l(\theta; Z) = \sum_{i=1}^N log(g_\theta(z_i)). $$  O estimador de maximum likelihood $\hat{\theta}$ √© o valor de $\theta$ que maximiza $l(\theta; Z)$ [^8.2.2].
**Corol√°rio 1:** *Conex√£o entre estimativas de m√≠nimos quadrados e maximum likelihood em modelos Gaussianos.*
Sob a suposi√ß√£o de que os erros s√£o Gaussianos, o estimador de m√≠nimos quadrados $\hat{\beta}$ √© equivalente ao estimador de maximum likelihood. No modelo $Y = X\beta + \epsilon$, se $\epsilon \sim N(0, \sigma^2)$, a log-verossimilhan√ßa para os dados $Z$ √© dada por $$ l(\beta, \sigma^2; Z) = -\frac{N}{2}log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^N(y_i - x_i^T\beta)^2. $$
A maximiza√ß√£o desta fun√ß√£o em rela√ß√£o a $\beta$ leva ao mesmo resultado obtido com m√≠nimos quadrados: $$ \hat{\beta} = (X^TX)^{-1}X^TY. $$
> üí° **Exemplo Num√©rico:**
>
> Usando os mesmos dados do exemplo anterior, vamos calcular a log-verossimilhan√ßa e mostrar que sua maximiza√ß√£o leva ao mesmo $\hat{\beta}$:
>
> ```python
> import numpy as np
> import scipy.optimize as opt
>
> # Dados de exemplo (j√° definidos anteriormente)
> X = np.array([[1, 2], [1, 3], [1, 4], [1, 5], [1, 6]])
> y = np.array([3, 4, 5, 6, 7])
>
> # Fun√ß√£o de log-verossimilhan√ßa para regress√£o linear com erros gaussianos
> def log_likelihood(params, X, y):
>     beta = params[:-1]
>     sigma2 = params[-1]
>     N = len(y)
>     y_hat = X @ beta
>     residuals = y - y_hat
>     return - (N/2)*np.log(2*np.pi*sigma2) - (1/(2*sigma2)) * np.sum(residuals**2)
>
> # Fun√ß√£o de log-verossimilhan√ßa negativa
> def negative_log_likelihood(params, X, y):
>    return -log_likelihood(params, X, y)
>
> # Chute inicial para os par√¢metros (inclui a vari√¢ncia do erro)
> initial_params = np.array([0.0, 0.0, 1.0])
>
> # Otimiza√ß√£o da log-verossimilhan√ßa
> results = opt.minimize(negative_log_likelihood, initial_params, args=(X, y))
> beta_mle = results.x[:-1]
> sigma2_mle = results.x[-1]
>
> print(f"Estimativa MLE beta_mle: {beta_mle}")
> print(f"Estimativa MLE sigma2_mle: {sigma2_mle}")
>
> # Compara√ß√£o com OLS
> X_transpose_X_inv = np.linalg.inv(X.T @ X)
> beta_hat_ols = X_transpose_X_inv @ X.T @ y
> print(f"Estimativa OLS beta_hat: {beta_hat_ols}")
> ```
> O c√≥digo acima demonstra que a maximiza√ß√£o da log-verossimilhan√ßa (ou minimiza√ß√£o da log-verossimilhan√ßa negativa) leva a uma estimativa $\hat{\beta}_{MLE}$ que √© numericamente igual √† estimativa de m√≠nimos quadrados $\hat{\beta}$. Isso ilustra o Corol√°rio 1, que estabelece a equival√™ncia entre esses dois m√©todos sob a suposi√ß√£o de erros gaussianos.
```mermaid
graph TB
    subgraph "Corolario 1: Equivalence of OLS and MLE"
    direction TB
        A["Linear Model: Y = XŒ≤ + Œµ, Œµ ~ N(0, œÉ¬≤)"]
        B["Log-Likelihood Function: l(Œ≤, œÉ¬≤; Z) = -N/2*log(2œÄœÉ¬≤) - 1/(2œÉ¬≤)*‚àë(yi - xi^TŒ≤)¬≤"]
        C["Maximizing log-likelihood with respect to Œ≤"]
        D["OLS Estimator: Œ≤_hat = (X^T X)^-1 X^T Y"]
        E["Result: Maximizing the log-likelihood leads to the same Œ≤_hat as OLS"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```
**Conceito 3: Bootstrap Param√©trico e sua liga√ß√£o com Maximum Likelihood**
O **parametric bootstrap** utiliza o modelo ajustado via maximum likelihood, para gerar novos conjuntos de dados, simulando a distribui√ß√£o dos erros.  A partir do modelo estimado $\hat{\mu}(x)$ e $\hat{\sigma}^2$, obtidos por maximum likelihood, novas respostas $y^*_i$ s√£o geradas, adicionando ru√≠do Gaussiano: $$ y^*_i = \hat{\mu}(x_i) + \epsilon_i^*, \quad \epsilon_i^* \sim N(0, \hat{\sigma}^2). $$
Esses novos conjuntos de dados $Z^* = \{(x_1, y^*_1), ..., (x_N, y^*_N)\}$ s√£o usados para calcular as estimativas bootstrap do modelo $\hat{\theta}^*$, que seguem a distribui√ß√£o do estimador de maximum likelihood [^8.2.2]. Em [^8.2.1], o texto indica que as bandas de confian√ßa calculadas via bootstrap param√©trico, tendem a concordar com as bandas de confian√ßa baseadas em estimativas de m√≠nimos quadrados (com erros Gaussianos aditivos), para um n√∫mero suficiente de amostras de bootstrap. Em [^8.2.2], fica claro que para outros modelos que n√£o seguem erros gaussianos, o bootstrap param√©trico tende a se comportar de forma mais aderente √† distribui√ß√£o do estimador de maximum likelihood, e n√£o de m√≠nimos quadrados.
> ‚ö†Ô∏è **Nota Importante**: O bootstrap param√©trico, utiliza a distribui√ß√£o dos erros do modelo, gerando amostras sint√©ticas.
> ‚ùó **Ponto de Aten√ß√£o**: Para modelos com erros Gaussianos aditivos, as estimativas bootstrap e de m√≠nimos quadrados tendem a concordar.
> ‚úîÔ∏è **Destaque**: Para modelos com erros n√£o Gaussianos, o bootstrap param√©trico segue a distribui√ß√£o do estimador de maximum likelihood.
```mermaid
graph TB
    subgraph "Parametric Bootstrap and Maximum Likelihood"
        direction TB
         A["MLE Estimated Model: Œº_hat(x), œÉ_hat¬≤"]
         B["Generate Synthetic Data: y*_i = Œº_hat(x_i) + Œµ*_i, Œµ*_i ~ N(0, œÉ_hat¬≤)"]
         C["Compute Bootstrap Estimates: Œ∏*_hat using synthetic data"]
         D["Distribution of Œ∏*_hat approximates the MLE estimator distribution"]
         A --> B
         B --> C
         C --> D
    end
```
### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

A regress√£o linear pode ser usada para classifica√ß√£o atrav√©s da **regress√£o de uma matriz indicadora**, onde cada classe √© representada por uma coluna de uma matriz bin√°ria.  Esta abordagem, embora simples, tem algumas limita√ß√µes, especialmente quando os dados n√£o s√£o linearmente separ√°veis [^4.2]. As estimativas dos coeficientes s√£o encontradas minimizando os erros quadr√°ticos. No entanto, essa abordagem n√£o modela as probabilidades das classes diretamente. O parametric bootstrap pode ser aplicado para estimar a variabilidade dessas estimativas. A ideia √© usar o modelo de regress√£o ajustado para gerar novos conjuntos de dados sint√©ticos, usando erros gaussianos adicionados aos valores ajustados, e posteriormente usar esses novos conjuntos de dados para recalcular os par√¢metros do modelo.
**Lemma 2:** *Converg√™ncia do bootstrap param√©trico para a distribui√ß√£o do estimador de maximum likelihood em modelos lineares*.
Sob a suposi√ß√£o de que o modelo linear $Y = X\beta + \epsilon$ √© correto, onde $\epsilon \sim N(0, \sigma^2)$ e o estimador de maximum likelihood de $\beta$ √© $\hat{\beta}$, ent√£o para um n√∫mero suficientemente grande de amostras bootstrap $B$, a distribui√ß√£o das estimativas $\hat{\beta}^*_b$, obtidas em cada amostra bootstrap, converge para a distribui√ß√£o do estimador de maximum likelihood $\hat{\beta}$.

**Prova:** O estimador de maximum likelihood $\hat{\beta}$ √© assintoticamente normal com m√©dia $\beta$ e matriz de covari√¢ncia igual √† inversa da informa√ß√£o de Fisher. A aplica√ß√£o do parametric bootstrap usa como centro a estimativa $\hat{\beta}$ e simula novos resultados utilizando a mesma distribui√ß√£o de erros. Ao aumentar o n√∫mero de amostras bootstrap, a distribui√ß√£o amostral dos estimadores $\hat{\beta}^*_b$ aproxima-se da distribui√ß√£o assint√≥tica do estimador de maximum likelihood. $\blacksquare$

**Corol√°rio 2:** *Rela√ß√£o entre bootstrap param√©trico e regress√£o com erros n√£o gaussianos*.

Caso os erros do modelo de regress√£o n√£o sejam gaussianos, o bootstrap param√©trico ainda consegue aproximar a distribui√ß√£o assint√≥tica do estimador de maximum likelihood, desde que a distribui√ß√£o dos erros seja corretamente modelada. Isso ocorre porque o bootstrap param√©trico simula dados usando a distribui√ß√£o dos erros assumida, e n√£o os erros de fato presentes nos dados de treinamento.

‚ÄúEm modelos com erros n√£o gaussianos, o bootstrap param√©trico consegue aproximar a distribui√ß√£o amostral do estimador de maximum likelihood, enquanto a regress√£o linear de indicadores pode produzir resultados menos precisos em termos de probabilidades de classe‚Äù, como indicado em [^8.2.2].
‚ÄúEm cen√°rios com dados complexos e n√£o linearmente separ√°veis, a regress√£o de indicadores pode ter limita√ß√µes, e m√©todos mais avan√ßados de classifica√ß√£o e bootstrap devem ser usados‚Äù, conforme descrito em [^4.2] e [^8.1].
> üí° **Exemplo Num√©rico:**
>
>  Vamos adaptar o exemplo anterior para um problema de classifica√ß√£o bin√°ria, usando regress√£o linear com uma matriz indicadora. Suponha que temos duas classes (0 e 1) e duas vari√°veis preditoras. Usamos uma matriz indicadora para representar a classe 1 como 1 e a classe 0 como 0:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Dados de exemplo para classifica√ß√£o
> X = np.array([[1, 2], [1, 3], [2, 1], [2, 2], [3, 1], [3, 2]])
> y = np.array([0, 0, 1, 1, 1, 1]) # Classes 0 e 1
>
> # Ajuste do modelo de regress√£o linear
> model = LinearRegression()
> model.fit(X, y)
> beta_hat = np.concatenate((model.intercept_.reshape(-1), model.coef_.reshape(-1)), axis=0)
>
> # C√°lculo dos res√≠duos e estimativa da vari√¢ncia do erro
> y_hat = model.predict(X)
> residuals = y - y_hat
> sigma2_hat = np.sum(residuals**2) / (len(y) - X.shape[1] - 1) # Usar len(y) - X.shape[1] -1 para o erro n√£o tendencioso
>
> # Bootstrap param√©trico
> num_bootstraps = 100
> beta_hat_bootstrap = np.zeros((num_bootstraps, X.shape[1] + 1)) # +1 para o intercept
>
> for i in range(num_bootstraps):
>     epsilon_star = np.random.normal(0, np.sqrt(sigma2_hat), len(y))
>     y_star = model.predict(X) + epsilon_star
>
>     # Regress√£o linear para cada amostra bootstrap
>     model_star = LinearRegression()
>     model_star.fit(X, y_star)
>     beta_hat_star = np.concatenate((model_star.intercept_.reshape(-1), model_star.coef_.reshape(-1)), axis=0)
>     beta_hat_bootstrap[i] = beta_hat_star
>
> print(f"Estimativa OLS beta_hat: {beta_hat}")
> print(f"M√©dia das estimativas bootstrap beta_hat: {np.mean(beta_hat_bootstrap,axis=0)}")
> ```
>   Neste exemplo, a regress√£o linear foi usada para classificar dados bin√°rios. O bootstrap param√©trico foi usado para gerar novos conjuntos de dados e recalcular os par√¢metros do modelo. A m√©dia das estimativas bootstrap $\hat{\beta}^*$ nos d√° uma ideia da variabilidade das estimativas de $\hat{\beta}$, permitindo a constru√ß√£o de intervalos de confian√ßa. No entanto, √© importante notar que a regress√£o linear de indicadores para classifica√ß√£o n√£o √© ideal e pode produzir valores fora do intervalo [0, 1].
```mermaid
graph TB
    subgraph "Lemma 2: Convergence of Parametric Bootstrap"
    direction TB
        A["Linear Model: Y = XŒ≤ + Œµ, Œµ ~ N(0, œÉ¬≤)"]
        B["MLE Estimator: Œ≤_hat"]
        C["Parametric Bootstrap: Generate B samples with same error distribution"]
        D["Bootstrap Estimates: Œ≤*_hat_b for each sample"]
         E["As B -> infinity, distribution of Œ≤*_hat_b converges to the MLE estimator distribution of Œ≤_hat"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```
### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o
```mermaid
graph TB
    subgraph "Variable Selection with Regularization"
    direction TB
    A["Input Data with Many Features"] --> B["Apply Linear Model (e.g., Logistic Regression)"]
    B --> C["Add L1 and/or L2 Penalties to cost function"]
    C --> D["Evaluate Model Performance"]
    D --> E["Select Relevant Variables based on coefficient magnitude"]
     E --> F["Use parametric bootstrap to assess variable stability"]
    end
```

A sele√ß√£o de vari√°veis e regulariza√ß√£o s√£o t√©cnicas fundamentais em problemas de classifica√ß√£o, especialmente quando se lida com um grande n√∫mero de preditores [^4.5]. Regulariza√ß√£o, como as penalidades L1 (Lasso) e L2 (Ridge) [^4.4.4], s√£o frequentemente aplicadas para evitar *overfitting* e melhorar a generaliza√ß√£o do modelo. No contexto de modelos lineares, como a regress√£o log√≠stica, estas t√©cnicas s√£o implementadas adicionando termos de penaliza√ß√£o √† fun√ß√£o de custo (log-verossimilhan√ßa) do modelo. A penaliza√ß√£o L1 promove a *sparsity* do modelo, eliminando vari√°veis menos importantes, enquanto a penaliza√ß√£o L2 reduz a magnitude dos coeficientes, diminuindo a vari√¢ncia.
O bootstrap param√©trico pode ser usado para estimar a estabilidade das vari√°veis selecionadas e para obter intervalos de confian√ßa para as estimativas dos coeficientes, mesmo ap√≥s a aplica√ß√£o da regulariza√ß√£o.
**Lemma 3:** *Efeito da regulariza√ß√£o L1 na sparsity de modelos de classifica√ß√£o*.
A penaliza√ß√£o L1 na fun√ß√£o de custo da regress√£o log√≠stica incentiva a *sparsity* da solu√ß√£o, ou seja, muitos coeficientes do modelo tendem a ser zero. Isso acontece porque a norma L1 promove solu√ß√µes em cantos, levando coeficientes a se anularem.

**Prova:** A fun√ß√£o de custo da regress√£o log√≠stica com regulariza√ß√£o L1 √© dada por $$ J(\beta) = - \frac{1}{N} \sum_{i=1}^N [y_i \log(\sigma(x_i^T\beta)) + (1-y_i)\log(1-\sigma(x_i^T\beta))] + \lambda ||\beta||_1, $$ onde $\sigma(z)$ √© a fun√ß√£o sigmoide e $||\beta||_1 = \sum_j |\beta_j|$ √© a norma L1. O termo de penaliza√ß√£o $\lambda ||\beta||_1$ adiciona um termo linear na norma dos coeficientes. A minimiza√ß√£o dessa fun√ß√£o leva a solu√ß√µes esparsas. A intui√ß√£o √© que em regi√µes onde a fun√ß√£o de custo √© suave, a inclina√ß√£o da norma L1 leva os coeficientes a zero para minimizar a fun√ß√£o. $\blacksquare$
> üí° **Exemplo Num√©rico:**
> Vamos usar um exemplo com regress√£o log√≠stica e penalidade L1 para ilustrar como a regulariza√ß√£o leva √† *sparsity*:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LogisticRegression
> from sklearn.preprocessing import StandardScaler
>
> # Dados de exemplo com mais features
> X = np.array([[1, 2, 3, 4, 5], [1, 3, 2, 5, 4], [2, 1, 4, 3, 5], [2, 2, 5, 4, 3], [3, 1, 5, 2, 4], [3, 2, 4, 1, 5]])
> y = np.array([0, 0, 1, 1, 1, 1])
>
> # Normaliza√ß√£o dos dados
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)
>
> # Regress√£o log√≠stica com penalidade L1 (Lasso)
> lambda_value = 0.5
> model_l1 = LogisticRegression(penalty='l1', C=1/(2*lambda_value), solver='liblinear',random_state=42)
> model_l1.fit(X_scaled, y)
>
> # Regress√£o log√≠stica sem regulariza√ß√£o
> model_no_reg = LogisticRegression(penalty=None, solver='lbfgs', random_state=42)
> model_no_reg.fit(X_scaled, y)
>
> print(f"Coeficientes com regulariza√ß√£o L1: {np.concatenate((model_l1.intercept_.reshape(-1), model_l1.coef_.reshape(-1)), axis=0)}")
> print(f"Coeficientes sem regulariza√ß√£o: {np.concatenate((model_no_reg.intercept_.reshape(-1), model_no_reg.coef_.reshape(-1)), axis=0)}")
>
> # Bootstrap para estabilidade das vari√°veis
> num_bootstraps = 100
> selected_variables = np.zeros((num_bootstraps, X.shape[1]), dtype=bool)
>
> for i in range(num_bootstraps):
>     sample_indices = np.random.choice(len(y), len(y), replace=True)
>     X_sample = X_scaled[sample_indices]
>     y_sample = y[sample_indices]
>     model_l1_boot = LogisticRegression(penalty='l1', C=1/(2*lambda_value), solver='liblinear', random_state=42)
>     model_l1_boot.fit(X_sample, y_sample)
>     selected_variables[i] = model_l1_boot.coef_ != 0
>
> variable_frequencies = np.mean(selected_variables, axis=0)
> print(f"Frequ√™ncia das vari√°veis selecionadas no bootstrap: {variable_frequencies}")
> ```
>  O exemplo mostra que a regress√£o log√≠stica com penalidade L1 leva a coeficientes esparsos, onde alguns coeficientes s√£o exatamente zero. O bootstrap param√©trico √© usado para amostrar o conjunto de dados original e avaliar a estabilidade das vari√°veis selecionadas por regulariza√ß√£o. A sa√≠da `Frequ√™ncia das vari√°veis selecionadas no bootstrap` mostra a estabilidade de cada vari√°vel, ou seja, quantas vezes cada vari√°vel foi selecionada pelo m√©todo L1 em 100 amostras bootstrap.
```mermaid
graph TB
    subgraph "Lemma 3: Effect of L1 Regularization"
        direction TB
    A["Logistic Regression Cost Function with L1 Penalty: J(Œ≤) = - 1/N * Œ£ [y_i log(œÉ(x_i^TŒ≤)) + (1-y_i)log(1-œÉ(x_i^TŒ≤))] + Œª||Œ≤||‚ÇÅ"]
        B["L1 Penalty Term: Œª||Œ≤||‚ÇÅ"]
        C["L1 Norm Promotes Sparsity by driving small coefficients to zero"]
        A --> B
        B --> C
    end
```
**Corol√°rio 3:** *Bootstrap param√©trico para estabilidade de vari√°veis selecionadas*.
O bootstrap param√©trico pode ser utilizado para estimar a estabilidade das vari√°veis selecionadas ap√≥s a aplica√ß√£o da regulariza√ß√£o. Isso √© feito gerando conjuntos de dados bootstrap, reaplicando o processo de sele√ß√£o de vari√°veis e regulariza√ß√£o, e computando a frequ√™ncia com que cada vari√°vel √© selecionada. Vari√°veis selecionadas em uma alta propor√ß√£o das amostras bootstrap s√£o consideradas mais est√°veis e importantes para o modelo.

> ‚ö†Ô∏è **Ponto Crucial**: A combina√ß√£o de penalidades L1 e L2 (Elastic Net) pode ser √∫til para balancear sparsity e estabilidade.
### Separating Hyperplanes e Perceptrons
```mermaid
graph TB
    subgraph "Hyperplane Learning with Perceptron"
    direction TB
         A["Input data with 2 or more classes"] --> B["Initialize Perceptron Weights and Bias"]
         B --> C["Iteratively Adjust Weights using misclassified data"]
        C --> D["Hyperplane learned when data is classified correctly"]
        D --> E["Use Parametric Bootstrap to estimate hyperplane variability"]
    end
```

Um **hyperplane separador** √© um conceito central em classifica√ß√£o linear, usado para dividir o espa√ßo de entrada em regi√µes correspondentes √†s classes [^4.5.2]. A ideia √© encontrar um hiperplano que maximize a margem de separa√ß√£o entre as classes. Em espa√ßos bidimensionais, um hiperplano √© simplesmente uma linha. Em espa√ßos de dimens√£o superior, um hiperplano √© uma superf√≠cie linear que divide o espa√ßo. O perceptron √© um algoritmo cl√°ssico que visa aprender o hiperplano √≥timo, ajustando os pesos do modelo iterativamente at√© convergir para uma solu√ß√£o que separa as classes corretamente [^4.5.1]. O bootstrap param√©trico pode ser usado para avaliar a incerteza nas estimativas dos hiperplanos e avaliar a sua estabilidade em diferentes amostras.
> üí° **Exemplo Num√©rico:**
>
> Vamos ilustrar o conceito de hiperplano separador com um exemplo simples e usar bootstrap param√©trico para avaliar sua estabilidade. Vamos usar o perceptron para encontrar um hiperplano que separe dois grupos de pontos em um espa√ßo 2D.
> ```python
> import numpy as np
> from sklearn.linear_model import Perceptron
> from sklearn.model_selection import train_test_split
> import matplotlib.pyplot as plt
>
> # Dados de exemplo
> X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])
> y = np.array([0, 0, 1, 1, 0, 1])
>
> # Treino do Perceptron
> perceptron = Perceptron(random_state=42)
> perceptron.fit(X, y)
>
> # Coeficientes do hiperplano
> w = perceptron.coef_[0]
> b = perceptron.intercept_
> print(f"Hiperplano: {w[0]}x + {w[1]}y + {b}")
>
> # Plot dos dados e hiperplano
> plt.figure(figsize=(6,6))
> plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')
> x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
> y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
> xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
> Z = perceptron.decision_function(np.c_[xx.ravel(), yy.ravel()])
> Z = Z.reshape(xx.shape)
> plt.contour(xx, yy, Z, levels=[0], colors='k')
> plt.xlabel("Feature 1")
> plt.ylabel("Feature 2")
> plt.title("Perceptron Hyperplane")
> plt.show()
>
> # Bootstrap para avalia√ß√£o da estabilidade do hiperplano
> num_bootstraps = 100
> bootstrap_hyperplanes = np.zeros((num_bootstraps, 3))  # (w1, w2, b)
>
> for i in range(num_bootstraps):
>     sample_indices = np.random.choice(len(y), len(y), replace=True)
>     X_sample = X[sample_indices]
>     y_sample = y[sample_indices]
>
>     perceptron_boot = Perceptron(random_state=42)
>     perceptron_boot.fit(X_sample, y_sample)
>     w_boot = perceptron_boot.coef_[0]
>     b_boot = perceptron_boot.intercept_
>     bootstrap_hyperplanes[i] = np.concatenate((w_boot.reshape(-1),b_boot.reshape(-1)))
>
> print(f"M√©dia dos hiperplanos bootstrap: {np.mean(bootstrap_hyperplanes, axis=0)}")
> ```
>  Nesse exemplo, o Perceptron foi usado para encontrar um hiperplano que separa as classes e o bootstrap param√©trico foi usado para gerar v√°rias amostras dos dados originais, com o objetivo de reestimar o hiperplano e avaliar sua estabilidade. A sa√≠da `M√©dia dos hiperplanos bootstrap` nos d√° uma ideia da variabilidade das estimativas do hiperplano, permitindo avaliar o qu√£o est√°vel √© o hiperplano encontrado com os dados originais. O gr√°fico mostra o hiperplano separando as classes.
### Pergunta Te√≥rica Avan√ßada: Como o bootstrap param√©trico pode ser usado para estimar a distribui√ß√£o do estimador de maximum likelihood em modelos onde a distribui√ß√£o te√≥rica √© desconhecida?
**Resposta:**
Em modelos mais complexos, a distribui√ß√£o te√≥rica do estimador de maximum likelihood pode ser dif√≠cil ou imposs√≠vel de ser obtida analiticamente. Nesses casos, o bootstrap param√©trico fornece uma forma alternativa para aproximar a distribui√ß√£o amostral do estimador de maximum likelihood, permitindo realizar infer√™ncia estat√≠stica, construir intervalos de confian√ßa e testar hip√≥teses. Ao utilizar o modelo estimado por maximum likelihood para gerar novas amostras simuladas e recalcular o estimador de maximum likelihood em cada uma delas, o bootstrap param√©trico cria uma aproxima√ß√£o da distribui√ß√£o amostral desejada.
**Lemma 4:** *Aproxima√ß√£o da distribui√ß√£o do estimador de maximum likelihood pelo bootstrap param√©trico*.
Seja $\hat{\theta}_{MLE}$ o estimador de maximum likelihood de um par√¢metro $\theta$ e $p(\hat{\theta}_{MLE})$ sua distribui√ß√£o amostral. O bootstrap param√©trico gera $B$ amostras simuladas, e com cada uma estima-se o par√¢metro  $\hat{\theta}^{*}_{MLE}$. No limite quando $B \rightarrow \infty$, a distribui√ß√£o de $\hat{\theta}^{*}_{MLE}$ se aproxima da distribui√ß√£o de $\hat{\theta}_{MLE}$.

**Prova:** O bootstrap param√©trico gera amostras de acordo com a distribui√ß√£o do modelo, centrado nas estimativas de maximum likelihood. Se o modelo estiver bem especificado, essa distribui√ß√£o converge para a distribui√ß√£o verdadeira quando o n√∫mero de amostras cresce, aproximando a distribui√ß√£o amostral do estimador de maximum likelihood. $\blacksquare$
**Corol√°rio 4:** *Bootstrap param√©trico para avaliar a influ√™ncia de outliers*.
Em cen√°rios com outliers, as estimativas de maximum likelihood podem ser fortemente influenciadas. O bootstrap param√©trico pode ser usado para avaliar o efeito desses