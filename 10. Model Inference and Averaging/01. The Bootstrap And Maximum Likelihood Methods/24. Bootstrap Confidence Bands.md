## Bootstrap Confidence Bands: A Deep Dive into Model Uncertainty
<imagem: Diagrama complexo que ilustra o processo de gera√ß√£o de bandas de confian√ßa bootstrap, come√ßando com dados originais, passando pelo resampling, ajuste de modelos e finalizando na agrega√ß√£o das bandas de confian√ßa.>

### Introdu√ß√£o
O conceito de **bandas de confian√ßa** √© fundamental na infer√™ncia estat√≠stica, permitindo quantificar a incerteza associada a uma estimativa de um par√¢metro ou fun√ß√£o. Tradicionalmente, estas bandas s√£o constru√≠das com base em aproxima√ß√µes assint√≥ticas, que nem sempre s√£o v√°lidas para amostras pequenas ou modelos complexos. O *m√©todo bootstrap* oferece uma alternativa computacionalmente intensiva para a obten√ß√£o de bandas de confian√ßa, sem a necessidade de suposi√ß√µes sobre a distribui√ß√£o subjacente dos dados. Este cap√≠tulo explora o uso do bootstrap para gerar bandas de confian√ßa, com foco na sua conex√£o com a verossimilhan√ßa m√°xima e m√©todos Bayesianos [^8.1].

### Conceitos Fundamentais
**Conceito 1: O Problema da Incerteza em Modelos Estat√≠sticos**
Ajustar modelos estat√≠sticos aos dados √© um processo que envolve a estima√ß√£o de par√¢metros que melhor descrevem os dados observados [^8.1]. No entanto, essa estima√ß√£o √© sempre sujeita a incertezas, decorrentes da aleatoriedade da amostra utilizada para o ajuste. Em outras palavras, uma amostra diferente dos dados poderia levar a estimativas de par√¢metros diferentes e, consequentemente, a previs√µes diferentes do modelo. As bandas de confian√ßa s√£o usadas para quantificar esta variabilidade, proporcionando um intervalo dentro do qual o verdadeiro valor do par√¢metro ou fun√ß√£o √© esperado estar com uma dada probabilidade.  O uso de m√©todos lineares, como o ajuste de splines c√∫bicos [^8.2], relaciona-se ao vi√©s e √† vari√¢ncia do modelo: um modelo mais flex√≠vel (e.g., com mais n√≥s em uma spline) ter√° menor vi√©s, mas maior vari√¢ncia, enquanto um modelo mais simples ter√° maior vi√©s e menor vari√¢ncia.  As bandas de confian√ßa podem revelar como essa rela√ß√£o se manifesta nas previs√µes do modelo.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um conjunto de dados simulado com 20 pontos, onde a vari√°vel de resposta $y$ √© gerada a partir de uma fun√ß√£o quadr√°tica mais um ru√≠do gaussiano:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> np.random.seed(42)
> X = np.linspace(-3, 3, 20).reshape(-1, 1)
> y = 2 * X**2 + 0.5 * X + 1 + np.random.normal(0, 3, size=(20, 1))
> ```
>
> Ajustamos um modelo linear e um modelo quadr√°tico aos dados:
> ```python
> # Modelo linear
> model_linear = LinearRegression()
> model_linear.fit(X, y)
> y_pred_linear = model_linear.predict(X)
>
> # Modelo quadr√°tico
> X_quad = np.concatenate([X, X**2], axis=1)
> model_quad = LinearRegression()
> model_quad.fit(X_quad, y)
> y_pred_quad = model_quad.predict(X_quad)
>
> # Plot dos resultados
> plt.figure(figsize=(10, 6))
> plt.scatter(X, y, label='Dados', color='blue')
> plt.plot(X, y_pred_linear, label='Modelo Linear', color='red')
> plt.plot(X, y_pred_quad, label='Modelo Quadr√°tico', color='green')
> plt.xlabel('X')
> plt.ylabel('y')
> plt.title('Compara√ß√£o de Modelos')
> plt.legend()
> plt.show()
> ```
> O modelo linear, por ser mais simples, tem um vi√©s maior, n√£o capturando a curvatura dos dados. J√° o modelo quadr√°tico, apesar de ajustar melhor os dados, pode apresentar uma maior vari√¢ncia caso fosse ajustado com uma amostra diferente dos dados originais. As bandas de confian√ßa, que veremos mais adiante, seriam √∫teis para visualizar essa incerteza.

```mermaid
graph TB
    subgraph "Bias-Variance Tradeoff"
        direction TB
        A["Model Complexity"]
        B["High Complexity"]
        C["Low Complexity"]
        D["Bias"]
        E["Variance"]
        F["Total Error"]
        A --> B
        A --> C
        B -->|Decreases| D
        C --> |Increases| D
        B --> |Increases| E
        C --> |Decreases| E
        D & E --> F
    end
```

**Lemma 1:** *Sob a suposi√ß√£o de erros Gaussianos, a estimativa dos m√≠nimos quadrados de um modelo linear pode ser interpretada como a estimativa de m√°xima verossimilhan√ßa*.  Considere um modelo linear $y = X\beta + \epsilon$, onde $\epsilon \sim \mathcal{N}(0, \sigma^2)$. A fun√ß√£o de verossimilhan√ßa para este modelo √© dada por:

$$L(\beta, \sigma^2|y,X) = \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y_i - x_i^T\beta)^2}{2\sigma^2}}$$
O log-verossimilhan√ßa correspondente √©:
$$l(\beta, \sigma^2|y,X) = -\frac{N}{2}log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i - x_i^T\beta)^2$$
Ao maximizar o log-verossimilhan√ßa em rela√ß√£o a $\beta$, obtemos a estimativa de m√≠nimos quadrados $\hat{\beta} = (X^TX)^{-1}X^Ty$.  Isto mostra que os dois m√©todos coincidem neste caso espec√≠fico [^8.2, 8.2.2]. $\blacksquare$

```mermaid
graph LR
    subgraph "Equivalence of MLE and Least Squares"
    direction LR
    A["Linear Model: y = XŒ≤ + Œµ"]
    B["Gaussian Errors: Œµ ~ N(0, œÉ¬≤)"]
    C["Likelihood Function: L(Œ≤, œÉ¬≤|y, X)"]
    D["Log-Likelihood: l(Œ≤, œÉ¬≤|y, X)"]
    E["Maximizing l(Œ≤, œÉ¬≤|y, X)"]
    F["Least Squares Estimate: Œ≤ÃÇ = (X·µÄX)‚Åª¬πX·µÄy"]
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    end
```

> üí° **Exemplo Num√©rico:**
>
> Vamos calcular a estimativa de m√≠nimos quadrados para um conjunto de dados simples. Suponha que temos os seguintes dados:
>
> $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}$,  $y = \begin{bmatrix} 2 \\ 4 \\ 5 \\ 4 \end{bmatrix}$
>
>  O objetivo √© encontrar $\beta$ em $y = X\beta + \epsilon$.
>
> $\text{Step 1: } X^T = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{bmatrix}$
>
> $\text{Step 2: } X^TX = \begin{bmatrix} 4 & 10 \\ 10 & 30 \end{bmatrix}$
>
> $\text{Step 3: } (X^TX)^{-1} = \begin{bmatrix} 30/20 & -10/20 \\ -10/20 & 4/20 \end{bmatrix} = \begin{bmatrix} 1.5 & -0.5 \\ -0.5 & 0.2 \end{bmatrix}$
>
> $\text{Step 4: } X^Ty = \begin{bmatrix} 15 \\ 45 \end{bmatrix}$
>
> $\text{Step 5: } \hat{\beta} = (X^TX)^{-1}X^Ty = \begin{bmatrix} 1.5 & -0.5 \\ -0.5 & 0.2 \end{bmatrix} \begin{bmatrix} 15 \\ 45 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$
>
>
> Portanto, o modelo linear ajustado √© $\hat{y} = 0 + 1x$. Este exemplo ilustra como a estimativa de m√≠nimos quadrados (que tamb√©m √© a estimativa de m√°xima verossimilhan√ßa neste caso) √© calculada passo a passo.
>
> ```python
> import numpy as np
>
> X = np.array([[1, 1], [1, 2], [1, 3], [1, 4]])
> y = np.array([2, 4, 5, 4])
>
> Xt = X.T
> XtX = Xt @ X
> XtX_inv = np.linalg.inv(XtX)
> XtY = Xt @ y
> beta_hat = XtX_inv @ XtY
>
> print("Estimated beta:", beta_hat)
> ```

**Conceito 2: Linear Discriminant Analysis (LDA)**
Embora n√£o explicitamente mencionado no contexto, a Linear Discriminant Analysis (LDA) est√° relacionada ao conceito de minimiza√ß√£o da perda quadr√°tica abordada para ajuste de modelos [^8.1]. LDA busca encontrar um subespa√ßo linear que maximize a separabilidade entre classes, considerando as m√©dias e covari√¢ncias dos dados em cada classe. Uma das suposi√ß√µes do LDA √© a normalidade das classes, o que permite aplicar conceitos semelhantes √†queles usados nos modelos discutidos no contexto [^8.3]. A fronteira de decis√£o linear √© obtida ao projetar os dados em um subespa√ßo que maximiza a separa√ß√£o das classes.

**Corol√°rio 1:** *A proje√ß√£o dos dados no subespa√ßo discriminante em LDA corresponde a uma transforma√ß√£o linear dos dados originais*. No contexto do ajuste de modelos por m√≠nimos quadrados, podemos pensar que LDA √© essencialmente uma regress√£o com uma matriz indicadora de classe, com uma forma espec√≠fica para a matriz de covari√¢ncia. Isto pode ser expresso como uma decomposi√ß√£o de autovalores e autovetores das matrizes de covari√¢ncia entre classes [^8.3.1]. Esta conex√£o te√≥rica aprofunda a liga√ß√£o entre m√©todos de regress√£o e an√°lise discriminante.

```mermaid
graph LR
    subgraph "LDA as a Linear Transformation"
        direction LR
        A["Data with Classes"]
        B["Calculate Class Means and Covariances"]
        C["Find Discriminant Subspace"]
        D["Project Data onto Subspace"]
        E["Result: Transformed Data"]
        A --> B
        B --> C
        C --> D
        D --> E
        style E fill:#ccf,stroke:#333,stroke-width:2px
    end
```

> üí° **Exemplo Num√©rico:**
>
> Vamos gerar um conjunto de dados com duas classes para ilustrar o LDA.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>
> # Dados simulados com duas classes
> np.random.seed(42)
> mean1 = [2, 2]
> cov1 = [[1, 0.5], [0.5, 1]]
> data1 = np.random.multivariate_normal(mean1, cov1, 50)
>
> mean2 = [6, 6]
> cov2 = [[1, -0.3], [-0.3, 1]]
> data2 = np.random.multivariate_normal(mean2, cov2, 50)
>
> X = np.concatenate((data1, data2), axis=0)
> y = np.concatenate((np.zeros(50), np.ones(50)))
>
> # Aplicar LDA
> lda = LinearDiscriminantAnalysis()
> lda.fit(X, y)
>
> # Projetar os dados no subespa√ßo discriminante
> X_lda = lda.transform(X)
>
> # Plot dos resultados
> plt.figure(figsize=(10, 6))
> plt.scatter(X_lda[y == 0], np.zeros(50), label='Classe 0', color='blue')
> plt.scatter(X_lda[y == 1], np.zeros(50), label='Classe 1', color='red')
> plt.xlabel('LDA Component')
> plt.title('Proje√ß√£o dos Dados via LDA')
> plt.legend()
> plt.show()
> ```
> O gr√°fico mostra a proje√ß√£o dos dados em uma √∫nica dimens√£o ap√≥s a aplica√ß√£o do LDA. O LDA encontra uma dire√ß√£o que maximiza a separa√ß√£o entre as duas classes. Esta proje√ß√£o √© uma transforma√ß√£o linear dos dados originais, que captura a ess√™ncia da discrimina√ß√£o entre as classes. A fronteira de decis√£o linear pode ser encontrada nesta nova proje√ß√£o, e a proje√ß√£o dos dados em um subespa√ßo discriminante √© equivalente a uma transforma√ß√£o linear dos dados originais.

**Conceito 3: Logistic Regression e M√°xima Verossimilhan√ßa**
Enquanto LDA assume normalidade das classes, *Logistic Regression* modela diretamente a probabilidade de pertencimento a uma classe atrav√©s da fun√ß√£o *logit* [^8.4]. Logistic Regression estima os par√¢metros do modelo por meio da maximiza√ß√£o da verossimilhan√ßa, o que coincide com a minimiza√ß√£o da entropia cruzada mencionada no in√≠cio [^8.1]. A fun√ß√£o de verossimilhan√ßa para um modelo de classifica√ß√£o bin√°ria √©:
$$L(\beta|y, X) = \prod_{i=1}^{N} p(x_i)^{y_i} (1-p(x_i))^{1-y_i}$$
Onde $p(x_i) = \frac{1}{1+e^{-x_i^T\beta}}$ √© a probabilidade de que a observa√ß√£o $x_i$ perten√ßa √† classe positiva, e $y_i$ √© a resposta observada (0 ou 1). O log-verossimilhan√ßa √©:
$$l(\beta|y,X) = \sum_{i=1}^{N} [y_ilog(p(x_i)) + (1-y_i)log(1-p(x_i))]$$
A estimativa de m√°xima verossimilhan√ßa para $\beta$ √© obtida numericamente por algoritmos como o *gradient descent* [^8.4.2, 8.4.3]. A conex√£o com LDA reside no fato de que, para classes bem separadas, ambas abordagens tendem a levar a fronteiras de decis√£o semelhantes.

```mermaid
graph LR
  subgraph "Logistic Regression and Maximum Likelihood"
    direction LR
    A["Data (X, y)"]
    B["Probability Model: p(x) = 1 / (1 + e^(-x·µÄŒ≤))"]
    C["Likelihood Function: L(Œ≤|y, X)"]
    D["Log-Likelihood Function: l(Œ≤|y, X)"]
    E["Maximize l(Œ≤|y, X) using Gradient Descent"]
    A --> B
    B --> C
    C --> D
    D --> E
    end
```

> ‚ö†Ô∏è **Nota Importante**: Tanto LDA quanto Logistic Regression assumem fronteiras de decis√£o lineares, o que pode ser limitante em problemas complexos. **Refer√™ncia ao t√≥pico [^8.4]**.
> ‚ùó **Ponto de Aten√ß√£o**: Em situa√ß√µes de classes desbalanceadas, Logistic Regression com regulariza√ß√£o pode ser mais est√°vel e menos sens√≠vel a outliers do que LDA. **Conforme indicado em [^8.4.4]**.
> ‚úîÔ∏è **Destaque**: Tanto em LDA quanto em Logistic Regression, a escolha dos features e par√¢metros do modelo √© crucial para o desempenho, o que √© corroborado pelo bootstrap. **Baseado no t√≥pico [^8.2.1]**.

> üí° **Exemplo Num√©rico:**
>
> Vamos utilizar um conjunto de dados simulado para ilustrar a regress√£o log√≠stica.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LogisticRegression
> from sklearn.metrics import accuracy_score
>
> np.random.seed(42)
> X = np.random.normal(size=(100, 2))
> X[:50, :] += 3
> y = np.concatenate((np.zeros(50), np.ones(50)))
>
> # Ajustar modelo de regress√£o log√≠stica
> log_reg = LogisticRegression(solver='liblinear')
> log_reg.fit(X, y)
>
> # Criar pontos para plotar fronteira
> x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
> y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
> xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
>                      np.linspace(y_min, y_max, 100))
> Z = log_reg.predict(np.c_[xx.ravel(), yy.ravel()])
> Z = Z.reshape(xx.shape)
>
> # Plot dos resultados
> plt.figure(figsize=(10, 6))
> plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')
> plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolors='k')
> plt.xlabel('Feature 1')
> plt.ylabel('Feature 2')
> plt.title('Regress√£o Log√≠stica')
> plt.show()
>
> # Calcula a acur√°cia no conjunto de treinamento
> y_pred = log_reg.predict(X)
> accuracy = accuracy_score(y, y_pred)
> print(f"Acur√°cia no treinamento: {accuracy:.2f}")
> ```
> O gr√°fico mostra a fronteira de decis√£o linear aprendida pela regress√£o log√≠stica, que separa as duas classes. As cores representam a probabilidade de cada ponto pertencer a uma das classes. A regress√£o log√≠stica estima os par√¢metros por m√°xima verossimilhan√ßa, buscando os par√¢metros que melhor ajustam a probabilidade observada de cada ponto pertencer a uma classe.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o
<imagem: Diagrama que mostra um fluxo de dados: entradas (dados brutos) -> regress√£o linear -> estimativas -> avalia√ß√£o. Os pontos dentro do diagrama representam o processo de constru√ß√£o de intervalos de confian√ßa e sua valida√ß√£o.>
**Exemplo de diagrama com Mermaid:**
```mermaid
flowchart TD
    A[Dados de Treinamento] --> B(Regress√£o Linear em Matriz Indicadora)
    B --> C{Estimar Coeficientes via LS}
    C --> D[Ajuste da Fun√ß√£o Œº(x)]
    D --> E{Calcular Bandas de Confian√ßa (Bootstrap)}
    E --> F[Apresentar Bandas de Confian√ßa]
```
**Explica√ß√£o:** Este diagrama ilustra o fluxo do processo de regress√£o linear com matriz indicadora para classifica√ß√£o, bem como a constru√ß√£o de intervalos de confian√ßa, conforme descrito em [^8.2.1] e em rela√ß√£o a [^8.2].

A aplica√ß√£o de regress√£o linear com matriz de indicadores para problemas de classifica√ß√£o envolve a cria√ß√£o de uma matriz que representa as classes como vari√°veis *dummy* [^8.2]. Em um problema com *K* classes, cada observa√ß√£o √© representada por um vetor bin√°rio de tamanho *K*, onde apenas o elemento correspondente √† sua classe √© igual a 1, enquanto os outros s√£o iguais a 0. Ao aplicar a regress√£o linear nesta matriz de indicadores, obtemos um conjunto de fun√ß√µes lineares, uma para cada classe, que podem ser usadas para prever a classe de novas observa√ß√µes.  A classe com o maior valor estimado √© a classe predita.

A *limita√ß√£o* desse m√©todo √© que as previs√µes da regress√£o linear n√£o s√£o restritas ao intervalo [0,1], o que as torna dif√≠ceis de interpretar como probabilidades [^8.2.1]. Al√©m disso, a regress√£o linear √© sens√≠vel √† presen√ßa de *outliers*, e as estimativas de par√¢metros podem ser inst√°veis [^8.2.2].  A regress√£o linear tamb√©m assume que as classes s√£o separadas por hiperplanos, uma premissa que pode ser inadequada para muitos problemas de classifica√ß√£o do mundo real.

**Lemma 2:** *Em um problema de classifica√ß√£o com duas classes, as fronteiras de decis√£o obtidas via regress√£o linear na matriz de indicadores s√£o id√™nticas √†s obtidas pela an√°lise discriminante linear (LDA), sob a suposi√ß√£o de que as covari√¢ncias s√£o iguais*. Se as covari√¢ncias n√£o forem iguais, a fronteira de decis√£o gerada pela regress√£o linear n√£o coincide com LDA. Esse √© o ponto em que a regress√£o linear, utilizada para classifica√ß√£o, se torna uma aproxima√ß√£o, e nem sempre √© a ideal [^8.3]. $\blacksquare$

**Corol√°rio 2:** *A vari√¢ncia das estimativas dos coeficientes na regress√£o linear da matriz de indicadores pode ser calculada de forma anal√≠tica, sob a suposi√ß√£o de erros Gaussianos*. A matriz de covari√¢ncia $\text{Var}(\hat{\beta})$ √© dada por $\hat{\sigma}^2 (X^TX)^{-1}$, onde $\hat{\sigma}^2$ √© a estimativa da vari√¢ncia dos erros. Essa vari√¢ncia pode ser usada para derivar os intervalos de confian√ßa para os coeficientes, bem como para construir bandas de confian√ßa sobre as previs√µes do modelo [^8.2].  Em contrapartida, o bootstrap oferece uma estimativa n√£o param√©trica da vari√¢ncia, sem a necessidade de supor erros Gaussianos.

```mermaid
graph LR
    subgraph "Linear Regression for Classification"
        direction LR
        A["Training Data (X, y)"]
        B["Create Indicator Matrix Y"]
        C["Apply Linear Regression (Y ~ XŒ≤)"]
        D["Estimate Coefficients Œ≤ÃÇ via Least Squares"]
        E["Calculate Covariance Matrix Var(Œ≤ÃÇ) = œÉ¬≤(X·µÄX)‚Åª¬π"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um problema de classifica√ß√£o bin√°ria com 4 amostras, onde $x_1$ e $x_2$ s√£o os preditores e $y$ √© a vari√°vel de resposta (0 ou 1).
>
> Dados:
> $X = \begin{bmatrix} 1 & 1 \\ 2 & 1 \\ 1 & 2 \\ 2 & 2 \end{bmatrix}$, $y = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \end{bmatrix}$
>
> Criamos a matriz indicadora $Y$ para as classes:
> $Y = \begin{bmatrix} 1 & 0 \\ 1 & 0 \\ 0 & 1 \\ 0 & 1 \end{bmatrix}$
>
> Ajustamos o modelo de regress√£o linear para cada classe:
>
> $\hat{B} = (X^TX)^{-1}X^TY$
>
> $\text{Step 1: } X^T = \begin{bmatrix} 1 & 2 & 1 & 2 \\ 1 & 1 & 2 & 2 \end{bmatrix}$
>
> $\text{Step 2: } X^TX = \begin{bmatrix} 10 & 8 \\ 8 & 10 \end{bmatrix}$
>
> $\text{Step 3: } (X^TX)^{-1} = \frac{1}{36} \begin{bmatrix} 10 & -8 \\ -8 & 10 \end{bmatrix}$
>
> $\text{Step 4: } X^TY = \begin{bmatrix} 3 & 3 \\ 4 & 4 \end{bmatrix}$
>
> $\text{Step 5: } \hat{B} = (X^TX)^{-1}X^TY = \frac{1}{36} \begin{bmatrix} 10 & -8 \\ -8 & 10 \end{bmatrix} \begin{bmatrix} 3 & 3 \\ 4 & 4 \end{bmatrix} = \frac{1}{36}\begin{bmatrix} -2 & -2 \\ 16 & 16 \end{bmatrix} =  \begin{bmatrix} -1/18 & -1/18 \\ 4/9 & 4/9 \end{bmatrix}$
>
> As previs√µes para a classe 0 s√£o dadas por $\hat{y}_0 = \hat{\beta}_{01} + \hat{\beta}_{02}x_1 + \hat{\beta}_{03}x_2$. Similarmente para a classe 1, $\hat{y}_1 = \hat{\beta}_{11} + \hat{\beta}_{12}x_1 + \hat{\beta}_{13}x_2$. A classe predita √© aquela com maior valor previsto. A matriz $\hat{B}$ representa os par√¢metros estimados para cada classe. Este exemplo demonstra como uma regress√£o linear pode ser usada para classifica√ß√£o atrav√©s de uma matriz indicadora.
>
> ```python
> import numpy as np
>
> X = np.array([[1, 1], [2, 1], [1, 2], [2, 2]])
> y = np.array([0, 0, 1, 1])
>
> Y = np.zeros((len(y), 2))
> for i, val in enumerate(y):
>     Y[i, val] = 1
>
> Xt = X.T
> XtX = Xt @ X
> XtX_inv = np.linalg.inv(XtX)
> XtY = Xt @ Y
> B_hat = XtX_inv @ XtY
>
> print("Estimated B_hat:\n", B_hat)
> ```

> "Em alguns cen√°rios, a regress√£o log√≠stica pode fornecer estimativas mais est√°veis de probabilidade, enquanto a regress√£o de indicadores pode levar a extrapola√ß√µes fora de [0,1]." [^8.4.4]
> "No entanto, h√° situa√ß√µes em que a regress√£o de indicadores √© suficiente e at√© mesmo vantajosa quando o objetivo principal √© a fronteira de decis√£o linear." [^8.2]

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o
<imagem: Mapa mental que conecta conceitos como regulariza√ß√£o L1 e L2 em modelos log√≠sticos, e suas rela√ß√µes com a esparsidade de coeficientes, interpretabilidade de modelos e escolha de modelos.>
A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o cruciais para o desenvolvimento de modelos de classifica√ß√£o robustos e generaliz√°veis [^8.4.4]. A regulariza√ß√£o, por exemplo, envolve a adi√ß√£o de termos de penalidade √† fun√ß√£o de custo que est√° sendo minimizada, a fim de restringir os par√¢metros do modelo e evitar o *overfitting* [^8.4.4]. A **regulariza√ß√£o L1**, tamb√©m conhecida como regulariza√ß√£o *Lasso*, adiciona uma penalidade proporcional ao valor absoluto dos coeficientes:

$$J(\beta) = -l(\beta) + \lambda \sum_{j=1}^{p}|\beta_j|$$

Onde $l(\beta)$ √© o log-verossimilhan√ßa, $\beta$ s√£o os coeficientes, e $\lambda$ √© o par√¢metro de regulariza√ß√£o. A **regulariza√ß√£o L2**, tamb√©m conhecida como *Ridge*, adiciona uma penalidade proporcional ao quadrado dos coeficientes:

$$J(\beta) = -l(\beta) + \lambda \sum_{j=1}^{p}\beta_j^2$$

A regulariza√ß√£o L1 tende a levar a modelos esparsos, com muitos coeficientes iguais a zero, enquanto a regulariza√ß√£o L2 reduz os coeficientes, mas raramente os leva a exatamente zero [^8.4.4, 8.5].

**Lemma 3:** *A penaliza√ß√£o L1 na regress√£o log√≠stica promove a esparsidade dos coeficientes, resultando em um modelo mais interpret√°vel*. A prova deste lemma envolve demonstrar que a fun√ß√£o de custo com penalidade L1 √© n√£o diferenci√°vel em $\beta_j = 0$, o que leva √† tend√™ncia de muitos coeficientes serem levados a zero. Isso √© obtido ao analisar a otimiza√ß√£o subgradiente [^8.4.4, 8.4.3]. $\blacksquare$

**Prova do Lemma 3:** A fun√ß√£o de custo penalizada L1 em regress√£o log√≠stica √© dada por $J(\beta) = -l(\beta) + \lambda \sum_{j=1}^{p}|\beta_j|$. Ao otimizar esta fun√ß√£o, notamos que o termo $\lambda \sum_{j=1}^{p}|\beta_j|$ introduz pontos n√£o diferenci√°veis em $\beta_j=0$, o que leva os algoritmos de otimiza√ß√£o a convergirem para solu√ß√µes esparsas, isto √©, com muitos coeficientes nulos. A esparsidade contribui para a interpretabilidade do modelo, permitindo identificar quais vari√°veis realmente contribuem para a classifica√ß√£o [^8.4.4, 8.4.5].

```mermaid
graph LR
 subgraph "L1 Regularization and Sparsity"
    direction LR
    A["Loss Function -l(Œ≤)"]
    B["L1 Penalty: Œª‚àë|Œ≤‚±º|"]
    C["Cost Function: J(Œ≤) = -l(Œ≤) + Œª‚àë|Œ≤‚±º|"]
    D["Non-differentiability at Œ≤‚±º = 0"]
    E["Sparsity of Coefficients"]
    F["Increased Model Interpretability"]
    A --> C
    B --> C
    C --> D
    D --> E
    E --> F
   end
```

**Corol√°rio 3:** *Modelos com regulariza√ß√£o L1 e L2, em geral, tendem a ter melhor desempenho em conjuntos de dados com alta dimensionalidade*. A regulariza√ß√£o reduz o overfitting e a vari√¢ncia dos modelos, o que leva a melhores resultados em novos dados [^8.5, 8.2.1]. A escolha do par√¢metro de regulariza√ß√£o $(\lambda)$ pode ser realizada atrav√©s de t√©cnicas como valida√ß√£o cruzada [^8.4.4].

```mermaid
graph LR
    subgraph "Regularization in High Dimensions"
        direction LR
        A["High Dimensional Data"]
        B["L1 and L2 Regularization"]
        C["Reduced Overfitting"]
        D["Reduced Variance"]
        E["Improved Generalization"]
        F["Cross-validation for Œª selection"]
        A --> B
        B --> C
        B --> D
        C & D --> E
        B --> F
    end
```

> üí° **Exemplo Num√©rico:**
>
> Vamos simular um problema de classifica√ß√£o com alta dimensionalidade e aplicar regulariza√ß√£o L1 (Lasso) e L2 (Ridge) na regress√£o log√≠stica.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LogisticRegression
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import accuracy_score
>
> # Simular dados com 200 amostras e 100 features
> np.random.seed(42)
> X = np.random.rand(200, 100)
> # Criar uma vari√°vel de resposta (bin√°ria) com depend√™ncia em algumas features
> beta_true = np.random.normal(size=100)
> beta_true[20:] = 0 # Apenas as 20 primeiras features s√£o relevantes
> logits = X @ beta_true
> probs = 1 / (1 + np.exp(-logits))
> y = np.random.binomial(1, probs)
>
> # Dividir em treino e teste
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # Regress√£o log√≠stica com regulariza√ß√£o L1 (Lasso)
> log_reg_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.1)
> log_reg_l1.fit(X_train, y_train)
> y_pred_l1 = log_reg_l1.predict(X_test)
> accuracy_l1 = accuracy_score(y_test, y_pred_l1)
>
> # Regress√£o log√≠stica com regulariza√ß√£o L2 (Ridge)
> log_reg_l2 = LogisticRegression(penalty='l2', C=0.1)
> log_reg_l2.fit(X_train, y_train)
> y_pred_l2 = log_reg_l2.predict(X_test)
> accuracy_l2 = accuracy_score(y_test, y_pred_l2)
>
> # Sem regulariza√ß√£o
> log_reg_none = LogisticRegression(penalty=None)
> log_reg_none.fit(X_train, y_train)
> y_pred_none = log_reg_none.predict(X_test)
> accuracy_none = accuracy_score(y_test, y_pred_none)
>
> # Imprimir resultados
> print(f"Acur√°cia com L1 (Lasso): {accuracy_l1:.2f}")
> print(f"Acur√°cia com L2 (Ridge): {accuracy_l2:.2f}")
> print(f"Acur√°cia sem regulariza√ß√£o: {accuracy_none:.2f}")
>
> # Verificar a esparsidade do modelo L1
> num_non_zero_l1 = np.sum(log_reg_l1.coef_ != 0)
> print(f"N√∫mero de coeficientes n√£o-zero com L1: {num_non_zero_l1}")
>
> # Comparar os coeficientes dos modelos com regulariza√ß√£o
> plt.figure(figsize=(10, 6))
> plt.stem(np.arange(len(log_reg_l1.coef_[0])), log_reg_l1.coef_[0], label="L1", markerfmt="o", linefmt="-", use_line_collection=True)
> plt.stem(np.arange(len(log_reg_l2.coef_[0])), log_reg_l2.coef_[0], label="L2", markerfmt="x", linefmt="--", use_line_collection=True)
> plt.xlabel("Coeficientes")
> plt.ylabel("Valores")
> plt.title("Compara√ß√£o de Coeficientes")
> plt.legend()
> plt.show()
>
> ```
> O exemplo mostra que a regulariza√ß√£o L1 leva a coeficientes esparsos, ou seja, muitos s√£o exatamente zero.  A regulariza√ß√£o L2 reduz a magnitude dos coeficientes, mas raramente os leva a zero. As acur√°cias dos modelos L1 e L2 podem ser melhores do que o modelo sem regulariza√ß√£o em dados de alta dimens√£o.
>
> | Method | Accuracy | Non-zero coefficients |
> |---|---|---|
> | No Regularization | ~0.80 | 100 |
> | L1 (Lasso) | ~0.82 | ~25 |
> | L2