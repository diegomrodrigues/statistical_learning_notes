## Standard Error do Estimador de M√°xima Verossimilhan√ßa

```mermaid
graph LR
    subgraph "Maximum Likelihood Estimation (MLE)"
    A["Maximum Likelihood Estimation (MLE)"]
    B["Likelihood Function: L(Œ∏; Z)"]
    C["Log-Likelihood: l(Œ∏; Z)"]
    D["Score Function: ‚àÇl(Œ∏; Z)/‚àÇŒ∏"]
    E["Information Matrix: -‚àÇ¬≤l(Œ∏; Z)/‚àÇŒ∏‚àÇŒ∏·µÄ"]
    F["Standard Error: se(Œ∏ÃÇ)"]
    A --> B
    B --> C
    C --> D
    C --> E
    E --> F
    end
```

### Introdu√ß√£o

O conceito de **Standard Error** (Erro Padr√£o) de um estimador √© fundamental na infer√™ncia estat√≠stica, fornecendo uma medida da precis√£o com que um par√¢metro populacional √© estimado a partir de uma amostra. No contexto do **Maximum Likelihood Estimation (MLE)**, o *standard error* nos ajuda a quantificar a incerteza associada √†s estimativas de par√¢metros, e √© crucial para a constru√ß√£o de intervalos de confian√ßa e testes de hip√≥teses. O MLE, como discutido em [^8.1], [^8.2.2], busca os par√¢metros que melhor se ajustam aos dados observados, maximizando a fun√ß√£o de verossimilhan√ßa, um conceito central em infer√™ncia estat√≠stica. Este cap√≠tulo explora em detalhes o *standard error* do estimador de m√°xima verossimilhan√ßa.

### Conceitos Fundamentais

**Conceito 1: Maximum Likelihood Estimation (MLE)**

O **Maximum Likelihood Estimation** √© um m√©todo para estimar os par√¢metros de um modelo estat√≠stico maximizando a **likelihood function** [^8.1], [^8.2.2]. A *likelihood function*, denotada por $L(\theta; Z)$, √© a probabilidade dos dados observados, $Z = \{z_1, z_2, ..., z_N\}$, dado um conjunto de par√¢metros desconhecidos $\theta$. Formalmente, a *likelihood function* √© dada por:

$$L(\theta; Z) = \prod_{i=1}^{N} g_\theta(z_i)$$

onde $g_\theta(z_i)$ √© a fun√ß√£o de densidade de probabilidade (ou massa de probabilidade) para a i-√©sima observa√ß√£o $z_i$, dado o par√¢metro $\theta$ [^8.2.2]. A ideia central do MLE √© encontrar o valor de $\theta$ que torna os dados observados o mais prov√°vel poss√≠vel. Na pr√°tica, ao inv√©s de maximizar $L(\theta; Z)$ diretamente, maximiza-se o seu logaritmo, a **log-likelihood** $l(\theta; Z)$, que √© definido como [^8.2.2]:

$$l(\theta; Z) = \sum_{i=1}^{N} \log g_\theta(z_i)$$

O estimador de m√°xima verossimilhan√ßa $\hat{\theta}$ √© ent√£o o valor de $\theta$ que maximiza $l(\theta; Z)$.

**Lemma 1:** A maximiza√ß√£o da *likelihood function* √© equivalente √† maximiza√ß√£o da *log-likelihood*.

**Prova:** Seja $L(\theta; Z)$ a *likelihood function* e $l(\theta; Z) = \log L(\theta; Z)$ a *log-likelihood*. Como a fun√ß√£o logar√≠tmica √© estritamente crescente, os valores de $\theta$ que maximizam $L(\theta; Z)$ s√£o os mesmos que maximizam $l(\theta; Z)$. Portanto, qualquer $\hat{\theta}$ tal que $\hat{\theta} = \text{argmax}_\theta L(\theta; Z)$ tamb√©m satisfaz $\hat{\theta} = \text{argmax}_\theta l(\theta; Z)$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Suponha que temos um conjunto de dados $Z = \{2.1, 2.8, 3.5, 4.2, 4.9\}$ que acreditamos vir de uma distribui√ß√£o normal com m√©dia $\mu$ e vari√¢ncia $\sigma^2 = 1$ (conhecida). Queremos estimar $\mu$ usando MLE. A fun√ß√£o de densidade de probabilidade normal √© dada por:
> $$g_\mu(z_i) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(z_i-\mu)^2}{2}}$$
> A log-likelihood √©:
> $$l(\mu; Z) = \sum_{i=1}^N \log \left( \frac{1}{\sqrt{2\pi}} e^{-\frac{(z_i-\mu)^2}{2}} \right) = - \frac{N}{2}\log(2\pi) - \frac{1}{2} \sum_{i=1}^N (z_i - \mu)^2$$
> Para maximizar $l(\mu; Z)$, precisamos minimizar $\sum_{i=1}^N (z_i - \mu)^2$, que √© o mesmo que obter a m√©dia amostral. Assim, $\hat{\mu} = \frac{1}{5}(2.1 + 2.8 + 3.5 + 4.2 + 4.9) = 3.5$.
> O estimador de m√°xima verossimilhan√ßa para $\mu$ √© a m√©dia amostral, como esperado.

**Conceito 2: Score Function e Information Matrix**

A **score function**, denotada por $l'(\theta; Z)$ ou $\ell(\theta; Z)$, √© o gradiente da *log-likelihood* em rela√ß√£o aos par√¢metros $\theta$ [^8.2.2]:

```mermaid
graph TB
    subgraph "Score Function and Information Matrix"
        A["Log-Likelihood: l(Œ∏; Z)"]
        B["Score Function: l'(Œ∏; Z) = ‚àÇl(Œ∏; Z)/‚àÇŒ∏"]
        C["Information Matrix: I(Œ∏) = -‚àÇ¬≤l(Œ∏; Z)/‚àÇŒ∏‚àÇŒ∏·µÄ"]
        D["Observed Information: I(Œ∏ÃÇ)"]
        E["Fisher Information: i(Œ∏) = E[I(Œ∏)]"]
        A --> B
        A --> C
        C --> D
        C --> E
    end
```

$$\ell(\theta; Z) = \frac{\partial l(\theta; Z)}{\partial \theta} = \sum_{i=1}^N \frac{\partial}{\partial \theta} \log g_\theta(z_i)$$

O estimador de m√°xima verossimilhan√ßa $\hat{\theta}$ √© obtido resolvendo a equa√ß√£o de *score*:

$$\ell(\hat{\theta}; Z) = 0$$

A **Information Matrix**, $I(\theta)$, √© uma matriz que mede a curvatura da *log-likelihood* ao redor do valor do par√¢metro. √â definida como a matriz Hessiana negativa da *log-likelihood*, ou seja, a matriz das derivadas parciais de segunda ordem [^8.2.2]:

$$I(\theta) = - \frac{\partial^2 l(\theta; Z)}{\partial \theta \partial \theta^T} = - \sum_{i=1}^N \frac{\partial^2}{\partial \theta \partial \theta^T} \log g_\theta(z_i)$$

Quando avaliada no estimador de m√°xima verossimilhan√ßa $\hat{\theta}$, $I(\hat{\theta})$ √© chamada de **observed information** [^8.2.2]. A **Fisher Information** ou *expected information*, denotada por $i(\theta)$, √© o valor esperado da *information matrix*:

$$i(\theta) = E[I(\theta)]$$

**Corol√°rio 1:** A *information matrix* mede a quantidade de informa√ß√£o que uma amostra fornece sobre o par√¢metro $\theta$.

> üí° **Exemplo Num√©rico (continua√ß√£o):**
> Para o exemplo anterior, a score function √© dada por:
>  $$\ell(\mu; Z) = \frac{\partial l(\mu; Z)}{\partial \mu} = \sum_{i=1}^N (z_i - \mu)$$
> Igualando a zero, obtemos $\hat{\mu} = \frac{1}{N}\sum_{i=1}^N z_i = 3.5$.
> A *information matrix* (que, neste caso, √© apenas um escalar) √©:
> $$I(\mu) = - \frac{\partial^2 l(\mu; Z)}{\partial \mu^2} = - \frac{\partial}{\partial \mu} \sum_{i=1}^N (z_i - \mu) = N = 5$$
> A *Fisher information*, neste caso, tamb√©m √© igual a 5, pois a *information matrix* n√£o depende dos dados.

**Conceito 3: Standard Error do Estimador de M√°xima Verossimilhan√ßa**

O **standard error** do estimador de m√°xima verossimilhan√ßa, $\text{se}(\hat{\theta})$, √© uma medida da variabilidade amostral do estimador. Intuitivamente, quanto menor o *standard error*, mais precisa a estimativa de $\theta$. O *standard error* √© obtido a partir da *information matrix*. Um resultado fundamental da teoria assint√≥tica do MLE afirma que, sob condi√ß√µes de regularidade, a distribui√ß√£o amostral do estimador de m√°xima verossimilhan√ßa $\hat{\theta}$ se aproxima de uma distribui√ß√£o normal, centrada no valor verdadeiro do par√¢metro $\theta_0$, com uma matriz de covari√¢ncia igual ao inverso da *Fisher information*:

$$\hat{\theta} \sim N(\theta_0, i(\theta_0)^{-1})$$

```mermaid
graph LR
    subgraph "Standard Error Calculation"
    A["MLE Estimator: Œ∏ÃÇ"]
    B["Fisher Information: i(Œ∏ÃÇ)"]
    C["Observed Information: I(Œ∏ÃÇ)"]
    D["Asymptotic Variance: i(Œ∏ÃÇ)‚Åª¬π or I(Œ∏ÃÇ)‚Åª¬π"]
    E["Standard Error: se(Œ∏ÃÇ) = sqrt(diag(Var(Œ∏ÃÇ)))"]
    A --> B
    A --> C
    B --> D
    C --> D
    D --> E
    end
```

Portanto, a vari√¢ncia assint√≥tica de $\hat{\theta}$ √© dada por $i(\theta_0)^{-1}$ e o *standard error* √© a raiz quadrada da diagonal dessa matriz. Como $\theta_0$ √© desconhecido, o *standard error* √© estimado usando a *observed information* ou a *Fisher information* avaliada em $\hat{\theta}$ [^8.2.2]:

$$\text{se}(\hat{\theta}) = \sqrt{i(\hat{\theta})^{-1}} \quad \text{ou} \quad \text{se}(\hat{\theta}) = \sqrt{I(\hat{\theta})^{-1}}$$

O *standard error* √© essencial para a constru√ß√£o de intervalos de confian√ßa e testes de hip√≥teses.

> ‚ö†Ô∏è **Nota Importante:** A distribui√ß√£o normal assint√≥tica do MLE s√≥ √© v√°lida quando o tamanho da amostra $N$ √© suficientemente grande.
> ‚ùó **Ponto de Aten√ß√£o:** As estimativas do *standard error* usando $I(\hat{\theta})$ e $i(\hat{\theta})$ podem diferir, especialmente em amostras pequenas.
> ‚úîÔ∏è **Destaque:** O conceito de *standard error* se baseia na ideia de que estimadores de m√°xima verossimilhan√ßa s√£o assintoticamente normais [^8.2.2].

> üí° **Exemplo Num√©rico (continua√ß√£o):**
> Para o exemplo da distribui√ß√£o normal com $\mu$ desconhecido, temos $I(\hat{\mu}) = i(\mu) = 5$.
> O *standard error* de $\hat{\mu}$ √© dado por:
> $$\text{se}(\hat{\mu}) = \sqrt{I(\hat{\mu})^{-1}} = \sqrt{\frac{1}{5}} \approx 0.447$$
> Isso significa que a incerteza na nossa estimativa de $\mu$ √© cerca de 0.447. Se tiv√©ssemos uma amostra maior, o *standard error* seria menor, indicando uma estimativa mais precisa.

### Rela√ß√£o com a Matriz de Covari√¢ncia

Em modelos lineares, o estimador de m√°xima verossimilhan√ßa para os coeficientes √© o mesmo obtido pelo m√©todo de m√≠nimos quadrados, e a matriz de covari√¢ncia dos par√¢metros √© dada por [^8.2]:

$$\text{Var}(\hat{\beta}) = (H^TH)^{-1}\hat{\sigma}^2$$

onde $H$ √© a matriz de *design* e $\hat{\sigma}^2$ √© a estimativa da vari√¢ncia dos erros. Esta matriz de covari√¢ncia √© a base para calcular os *standard errors* dos coeficientes na regress√£o linear. No contexto do MLE, a *information matrix* $I(\beta)$ √© o an√°logo ao inverso da matriz de covari√¢ncia, quando multiplicada pela vari√¢ncia do ru√≠do [^8.2.2], [^8.2.3], ou seja:

$$I(\beta) = (H^TH)/\sigma^2$$

E a estimativa do *standard error* de um coeficiente $\hat{\beta}_j$  √© dada pela raiz quadrada do j-√©simo elemento diagonal de $(H^TH)^{-1}\hat{\sigma}^2$, conforme mencionado em [^8.2.3].

**Lemma 2:** A matriz de covari√¢ncia do estimador de m√≠nimos quadrados em modelos lineares √© consistente com o inverso da matriz de *Fisher information* no contexto do MLE para modelos com erros Gaussianos [^8.2.2], [^8.2.3].

**Prova:** Em um modelo linear com erros Gaussianos, a maximiza√ß√£o da *likelihood* leva aos mesmos estimadores de m√≠nimos quadrados. A *Fisher Information* para os par√¢metros de um modelo linear com ru√≠do Gaussiano √© dada por $I(\beta) = (H^TH)/\sigma^2$. O inverso da *Fisher information*, $I(\beta)^{-1}$, corresponde a $(H^TH)^{-1} \sigma^2$. Substituindo a estimativa da vari√¢ncia do erro, $\hat{\sigma}^2$, obt√©m-se uma matriz de covari√¢ncia para o estimador de m√≠nimos quadrados, dada por $(H^TH)^{-1} \hat{\sigma}^2$ . $\blacksquare$

```mermaid
graph LR
    subgraph "Linear Model and MLE"
        A["Linear Model: y = HŒ≤ + Œµ"]
        B["OLS Estimator: Œ≤ÃÇ = (H·µÄH)‚Åª¬πH·µÄy"]
        C["Covariance Matrix: Var(Œ≤ÃÇ) = (H·µÄH)‚Åª¬πœÉÃÇ¬≤"]
        D["MLE Information Matrix: I(Œ≤) = (H·µÄH)/œÉ¬≤"]
        E["MLE Asymptotic Covariance: i(Œ≤)‚Åª¬π ‚âà (H·µÄH)‚Åª¬πœÉ¬≤"]
        A --> B
        A --> C
        A --> D
        D --> E
    end
```

**Corol√°rio 2:** O *standard error* de um coeficiente em um modelo linear, quando derivado da *information matrix*, √© equivalente ao *standard error* obtido pelo m√©todo de m√≠nimos quadrados.

> üí° **Exemplo Num√©rico (Regress√£o Linear):**
> Vamos considerar um modelo de regress√£o linear simples:
> $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$, onde $\epsilon_i \sim N(0, \sigma^2)$.
> Suponha que temos os seguintes dados:
> | $x_i$ | $y_i$ |
> | ----- | ----- |
> | 1     | 3     |
> | 2     | 5     |
> | 3     | 7     |
> | 4     | 9     |
>
> A matriz de design $H$ e o vetor de respostas $y$ s√£o:
> $$ H = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}, \quad y = \begin{bmatrix} 3 \\ 5 \\ 7 \\ 9 \end{bmatrix} $$
>
> Podemos calcular $(H^TH)^{-1}$:
>
> $$ H^TH = \begin{bmatrix} 4 & 10 \\ 10 & 30 \end{bmatrix}, \quad (H^TH)^{-1} = \begin{bmatrix} 1.5 & -0.5 \\ -0.5 & 0.2 \end{bmatrix} $$
>
> Os coeficientes de regress√£o s√£o: $\hat{\beta} = (H^TH)^{-1} H^T y = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$
> A vari√¢ncia do erro √© estimada por $\hat{\sigma}^2 = \frac{\sum_{i=1}^4 (y_i - \hat{y}_i)^2}{4-2} = \frac{0}{2} = 0$, notamos que os dados foram gerados de maneira a n√£o ter res√≠duos.
>  A matriz de covari√¢ncia de $\hat{\beta}$ √©:
> $$\text{Var}(\hat{\beta}) = (H^TH)^{-1}\hat{\sigma}^2 =  \begin{bmatrix} 1.5 & -0.5 \\ -0.5 & 0.2 \end{bmatrix} * 0 = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} $$
>
> Os *standard errors* dos coeficientes s√£o as ra√≠zes quadradas da diagonal de  $\text{Var}(\hat{\beta})$:
> $$ \text{se}(\hat{\beta}_0) = \sqrt{0} = 0, \quad \text{se}(\hat{\beta}_1) = \sqrt{0} = 0 $$
> Note que, nesse exemplo em particular, por n√£o haver res√≠duos, n√£o h√° variabilidade amostral, logo, os erros padr√£o s√£o nulos.

### M√©todos de C√°lculo e Infer√™ncia

O c√°lculo do *standard error* do estimador de m√°xima verossimilhan√ßa geralmente envolve as seguintes etapas:

1.  **Obter o estimador de m√°xima verossimilhan√ßa,** $\hat{\theta}$, maximizando a *log-likelihood* $l(\theta; Z)$.
2.  **Calcular a *observed information matrix*,** $I(\hat{\theta})$, ou a *Fisher information*, $i(\theta)$, avaliada em $\hat{\theta}$.
3.  **Inverter a *information matrix*** e obter a matriz de covari√¢ncia do estimador $\text{Var}(\hat{\theta}) \approx i(\hat{\theta})^{-1}$ ou  $\text{Var}(\hat{\theta}) \approx I(\hat{\theta})^{-1}$.
4.  **Calcular o *standard error*** como a raiz quadrada dos elementos da diagonal de $\text{Var}(\hat{\theta})$.

O *standard error* √© usado para construir intervalos de confian√ßa e testar hip√≥teses. Um intervalo de confian√ßa aproximado de 100(1-$\alpha$)% para o par√¢metro $\theta_j$ √© dado por [^8.2.2]:

$$\hat{\theta}_j \pm z_{1-\alpha/2} \cdot \text{se}(\hat{\theta}_j)$$

onde $z_{1-\alpha/2}$ √© o quantil correspondente da distribui√ß√£o normal padr√£o. Alternativamente, intervalos de confian√ßa mais acurados podem ser obtidos usando o teste da raz√£o de verossimilhan√ßas (likelihood ratio test) [^8.2.2].

> üí° **Exemplo Num√©rico (Intervalo de Confian√ßa):**
> Retornando ao exemplo da estimativa de $\mu$ da distribui√ß√£o normal, onde $\hat{\mu} = 3.5$ e $se(\hat{\mu}) \approx 0.447$. Para construir um intervalo de confian√ßa de 95%, usamos $z_{0.975} \approx 1.96$.
> O intervalo de confian√ßa de 95% para $\mu$ √©:
> $$3.5 \pm 1.96 \times 0.447 = [2.62, 4.38]$$
> Isso significa que temos 95% de confian√ßa de que o valor verdadeiro de $\mu$ est√° entre 2.62 e 4.38.

### Pergunta Te√≥rica Avan√ßada (Exemplo): Qual a rela√ß√£o entre o *standard error* do MLE e a efici√™ncia de um estimador?

**Resposta:** A efici√™ncia de um estimador est√° relacionada com a sua vari√¢ncia, e portanto com o seu *standard error*. Um estimador √© considerado eficiente se ele alcan√ßa o limite inferior da vari√¢ncia para estimadores n√£o-viesados, conhecido como **Cramer-Rao Lower Bound**. Sob certas condi√ß√µes de regularidade, o estimador de m√°xima verossimilhan√ßa √© assintoticamente eficiente, o que significa que a sua vari√¢ncia assint√≥tica atinge o limite de Cramer-Rao. Isso implica que o *standard error* do MLE, quando a amostra √© grande, √© o menor poss√≠vel entre todos os estimadores n√£o-viesados. [^8.2.2].

```mermaid
graph TB
    subgraph "Efficiency and Cramer-Rao Lower Bound"
        A["Estimator Variance"]
        B["Cramer-Rao Lower Bound"]
        C["MLE Efficiency (Asymptotic)"]
        D["Minimun Standard Error"]
        A --> B
        B --> C
        C --> D
    end
```

**Lemma 3:** O limite de Cramer-Rao estabelece um limite inferior para a vari√¢ncia de estimadores n√£o-viesados.

**Corol√°rio 3:** Sob condi√ß√µes de regularidade, o estimador de m√°xima verossimilhan√ßa atinge assintoticamente o limite de Cramer-Rao e, portanto, seu *standard error* √© m√≠nimo entre todos os estimadores n√£o-viesados.

### Conclus√£o

O *standard error* do estimador de m√°xima verossimilhan√ßa √© uma ferramenta essencial na infer√™ncia estat√≠stica, permitindo quantificar a precis√£o e a incerteza associadas √†s estimativas de par√¢metros. O seu c√°lculo envolve a *information matrix* e √© fundamental para a constru√ß√£o de intervalos de confian√ßa e testes de hip√≥teses. Em modelos lineares, o *standard error* do MLE √© consistente com o *standard error* obtido pelo m√©todo de m√≠nimos quadrados. A rela√ß√£o entre o *standard error* e a efici√™ncia de um estimador ressalta a import√¢ncia do MLE como um m√©todo de estimativa estatisticamente robusto. O entendimento e a aplica√ß√£o apropriada do *standard error* s√£o cruciais para uma an√°lise estat√≠stica rigorosa e precisa.

<!-- END DOCUMENT -->

### Footnotes
[^8.1]: "In this chapter we provide a general exposition of the maximum likelihood approach, as well as the Bayesian method for inference." *(Trecho de <Model Inference and Averaging>)*
[^8.2.2]: "Maximum likelihood is based on the likelihood function, given by  $L(\theta; Z) = \prod_{i=1}^{N} g_\theta(z_i)$... The method of maximum likelihood chooses the value $\theta = \hat{\theta}$ to maximize $l(\theta; Z)$... The likelihood function can be used to assess the precision of Œ∏. The score function is defined by $\ell(\theta; Z) = \sum_{i=1}^N \ell(\theta; z_i)$... The information matrix is...$I(\theta) = -\sum_{i=1}^N \frac{\partial^2}{\partial \theta \partial \theta^T} \log g_\theta(z_i)$ When I(Œ∏) is evaluated at $\theta = \hat{\theta}$, it is often called the observed information. The Fisher information (or expected information) is  $i(\theta) = E[I(\theta)]$...A standard result says that the sampling distribution of the maximum likelihood estimator has a limiting normal distribution $\hat{\theta} \sim N(\theta_0, i(\theta_0)^{-1})$..." *(Trecho de <Model Inference and Averaging>)*
[^8.2.3]: "The information matrix for $\theta = (\beta, \sigma^2)$ is block-diagonal, and the block corresponding to $\beta$ is $I(\beta) = (H^TH)/\sigma^2$, so that the estimated variance $(H^TH)^{-1}\hat{\sigma}^2$ agrees with the least squares estimate (8.3)." *(Trecho de <Model Inference and Averaging>)*
[^8.1]: "For most of this book, the fitting (learning) of models has been achieved by minimizing a sum of squares for regression, or by minimizing cross-entropy for classification. In fact, both of these minimizations are instances of the maximum likelihood approach to fitting." *(Trecho de <Model Inference and Averaging>)*
[^8.2]: "The corresponding fit $\mu(x) = \sum_{j=1}^7 \beta_j h_j(x)$ is shown in the top left panel of Figure 8.2. The estimated covariance matrix of $\beta$ is  $Var(\hat{\beta}) = (H^TH)^{-1}\hat{\sigma}^2$" *(Trecho de <Model Inference and Averaging>)*
