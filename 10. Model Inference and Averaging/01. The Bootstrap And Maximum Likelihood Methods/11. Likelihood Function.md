## Model Inference and Averaging: A Deep Dive into Likelihood

```mermaid
graph LR
    subgraph "Inference and Model Averaging"
        A["Maximum Likelihood (ML)"]
        B["Bootstrap"]
        C["Bayesian Methods"]
        D["Model Averaging (Bagging, Stacking, Bumping)"]
        A --> D
        B --> D
        C --> D
    end
```

### Introdu√ß√£o
O presente cap√≠tulo se dedica √† explora√ß√£o aprofundada da infer√™ncia estat√≠stica e da combina√ß√£o de modelos, com um foco especial na **Maximum Likelihood** e em abordagens relacionadas como o **Bootstrap** e m√©todos **Bayesianos** [^8.1]. Exploramos como esses m√©todos podem ser usados para construir modelos mais robustos e confi√°veis, com um foco especial na deriva√ß√£o de intervalos de confian√ßa e avalia√ß√£o de incertezas em modelos estat√≠sticos [^8.1]. Veremos tamb√©m como a ideia de combinar modelos, atrav√©s de t√©cnicas como **bagging**, **stacking** e **bumping**, pode levar a previs√µes mais precisas e est√°veis. Este estudo tem por objetivo fornecer um entendimento profundo e avan√ßado de como os conceitos de verossimilhan√ßa, vari√¢ncia e vi√©s se encaixam nas estrat√©gias de modelagem.

### Conceitos Fundamentais

**Conceito 1: Maximum Likelihood**

O princ√≠pio da **Maximum Likelihood** (ML) √© central na infer√™ncia estat√≠stica, visando encontrar os valores dos par√¢metros de um modelo que maximizam a probabilidade de se observar os dados reais [^8.1]. Formalmente, dado um conjunto de dados $Z = \{z_1, z_2, \ldots, z_N\}$ e um modelo param√©trico definido por uma fun√ß√£o de densidade de probabilidade (pdf) ou fun√ß√£o de massa de probabilidade (pmf) $g_\theta(z)$, a fun√ß√£o de verossimilhan√ßa √© dada por:
$$L(\theta; Z) = \prod_{i=1}^{N} g_\theta(z_i)$$
A ideia √© que, dentre todas as poss√≠veis configura√ß√µes de $\theta$, escolhemos aquela que torna os dados observados mais prov√°veis [^8.1]. O par√¢metro $\theta$ que maximiza $L(\theta;Z)$ √© chamado de estimativa de m√°xima verossimilhan√ßa. Em muitos casos, para facilitar a otimiza√ß√£o, √© usada a fun√ß√£o de log-verossimilhan√ßa, $l(\theta; Z)$, que √© o logaritmo da fun√ß√£o de verossimilhan√ßa:
$$l(\theta; Z) = \sum_{i=1}^{N} \log g_\theta(z_i)$$
A motiva√ß√£o para esta abordagem √© que em muitos casos, a fun√ß√£o de log-verossimilhan√ßa √© mais f√°cil de ser otimizada do que a verossimilhan√ßa original. A escolha do modelo $g_\theta(z)$ √© crucial e define a estrutura do nosso problema de infer√™ncia.
```mermaid
graph LR
    subgraph "Maximum Likelihood Estimation (MLE)"
        direction TB
        A["Data Z = {z1, z2, ..., zN}"]
        B["Parametric Model: g_Œ∏(z)"]
        C["Likelihood Function: L(Œ∏; Z) =  ‚àè g_Œ∏(zi)"]
        D["Log-Likelihood Function: l(Œ∏; Z) = Œ£ log g_Œ∏(zi)"]
        E["Maximize l(Œ∏; Z) to find Œ∏ÃÇ_MLE"]
        A --> C
        B --> C
        C --> D
        D --> E
    end
```

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados $Z = \{2.1, 3.5, 1.8, 4.2, 2.9\}$ que acreditamos ser proveniente de uma distribui√ß√£o normal com m√©dia $\mu$ e vari√¢ncia $\sigma^2$.  Nesse caso, $g_{\mu, \sigma^2}(z_i) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(z_i-\mu)^2}{2\sigma^2}}$.
>
> **Passo 1: Definir a fun√ß√£o de log-verossimilhan√ßa:**
>
> $l(\mu, \sigma^2; Z) = \sum_{i=1}^{5} \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(z_i - \mu)^2}{2\sigma^2}} \right)$
>
> $l(\mu, \sigma^2; Z) = -\frac{5}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^5 (z_i - \mu)^2$
>
> **Passo 2: Estimar $\mu$ e $\sigma^2$ maximizando a log-verossimilhan√ßa.**
>
> Para este exemplo, vamos usar as estimativas de m√°xima verossimilhan√ßa que s√£o conhecidas para uma distribui√ß√£o normal,
>
> $\hat{\mu} = \frac{1}{5} \sum_{i=1}^5 z_i = \frac{2.1 + 3.5 + 1.8 + 4.2 + 2.9}{5} = 2.9$
>
> $\hat{\sigma}^2 = \frac{1}{5} \sum_{i=1}^5 (z_i - \hat{\mu})^2 = \frac{(2.1-2.9)^2 + (3.5-2.9)^2 + (1.8-2.9)^2 + (4.2-2.9)^2 + (2.9-2.9)^2}{5} = \frac{0.64 + 0.36 + 1.21 + 1.69 + 0}{5} = 0.78$
>
> Portanto, a estimativa de m√°xima verossimilhan√ßa para a m√©dia √© $\hat{\mu} = 2.9$ e para a vari√¢ncia √© $\hat{\sigma}^2 = 0.78$.  Esses par√¢metros s√£o os que tornam os dados observados mais prov√°veis, de acordo com o modelo gaussiano escolhido.

**Lemma 1:** A estimativa de m√°xima verossimilhan√ßa (MLE) de um par√¢metro sob um modelo Gaussiano corresponde √† minimiza√ß√£o da soma dos quadrados dos res√≠duos.
*Prova:*
Considere um modelo Gaussiano com m√©dia $\mu$ e vari√¢ncia $\sigma^2$, onde a pdf √© dada por $g_{\mu,\sigma^2}(z) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(z-\mu)^2}{2\sigma^2}}$.  A fun√ß√£o de log-verossimilhan√ßa para um conjunto de $N$ observa√ß√µes independentes √©:

$$ l(\mu, \sigma^2; Z) = \sum_{i=1}^N \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(z_i - \mu)^2}{2\sigma^2}} \right) $$
$$ = -\frac{N}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (z_i - \mu)^2 $$

Maximizar $l(\mu, \sigma^2; Z)$ em rela√ß√£o a $\mu$ √© equivalente a minimizar $\sum_{i=1}^N (z_i - \mu)^2$, que √© a soma dos quadrados dos res√≠duos.  Portanto, a MLE de $\mu$ √© igual ao estimador de m√≠nimos quadrados. $\blacksquare$
```mermaid
graph LR
    subgraph "MLE and Least Squares (Gaussian Model)"
        direction TB
         A["Gaussian PDF: g(z | Œº, œÉ¬≤)"]
        B["Log-Likelihood: l(Œº, œÉ¬≤; Z) = -N/2 log(2œÄœÉ¬≤) - 1/(2œÉ¬≤) Œ£(zi - Œº)¬≤"]
        C["Maximizing l(Œº, œÉ¬≤; Z)"]
        D["Minimizing: Œ£(zi - Œº)¬≤  (Sum of Squared Residuals)"]
        C --> D
        B --> C
         A --> B
    end
```

**Conceito 2: Linear Regression and Maximum Likelihood**
Na regress√£o linear, buscamos o melhor ajuste linear de uma vari√°vel dependente em rela√ß√£o a uma ou mais vari√°veis independentes. Em termos de verossimilhan√ßa, a regress√£o linear com erros Gaussianos assume que cada observa√ß√£o $Y_i$ √© gerada por $Y_i = \mu(x_i) + \epsilon_i$, onde $\mu(x_i)$ √© a resposta m√©dia linear modelada pelas vari√°veis preditoras $x_i$, e $\epsilon_i$ s√£o os erros independentes e identicamente distribu√≠dos com distribui√ß√£o normal $N(0, \sigma^2)$ [^8.5]. O objetivo da regress√£o linear, tanto na abordagem de m√≠nimos quadrados quanto na de m√°xima verossimilhan√ßa, √© encontrar os par√¢metros do modelo que minimizem a soma dos quadrados dos res√≠duos, o que equivale a maximizar a fun√ß√£o de verossimilhan√ßa [^8.5]. Assim, os estimadores obtidos pelos dois m√©todos (minimiza√ß√£o dos quadrados e maximiza√ß√£o da verossimilhan√ßa) coincidem quando assumimos a distribui√ß√£o Gaussiana.
```mermaid
graph LR
    subgraph "Linear Regression and MLE (Gaussian Errors)"
        direction TB
        A["Observed Data: Yi = Œº(xi) + Œµi"]
        B["Gaussian Error: Œµi ~ N(0, œÉ¬≤)"]
        C["Goal: Find Œ≤ to minimize Œ£(Yi - Œº(xi))¬≤"]
        D["MLE: Maximize Likelihood (Equivalent to minimizing sum of squared residuals)"]
         A --> C
         B --> C
         C --> D
    end
```

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um conjunto de dados com uma vari√°vel preditora $x$ e uma vari√°vel resposta $y$, onde queremos ajustar um modelo de regress√£o linear: $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$. Suponha que temos os seguintes dados:
>
> | $x_i$ | $y_i$ |
> |-------|-------|
> | 1     | 3     |
> | 2     | 5     |
> | 3     | 7     |
> | 4     | 9     |
>
>  **Passo 1: Construir a matriz de design H e o vetor y:**
>  
>  $H = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}$  e $y = \begin{bmatrix} 3 \\ 5 \\ 7 \\ 9 \end{bmatrix}$
>  
> **Passo 2: Calcular $(H^T H)^{-1}$ :**
> $H^T H = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix} = \begin{bmatrix} 4 & 10 \\ 10 & 30 \end{bmatrix}$
>  $(H^T H)^{-1} = \frac{1}{4 \cdot 30 - 10 \cdot 10}\begin{bmatrix} 30 & -10 \\ -10 & 4 \end{bmatrix} = \begin{bmatrix} 1.5 & -0.5 \\ -0.5 & 0.2 \end{bmatrix}$
>
>  **Passo 3: Calcular $H^T y$:**
> $H^T y = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} 3 \\ 5 \\ 7 \\ 9 \end{bmatrix} = \begin{bmatrix} 24 \\ 64 \end{bmatrix}$
>
>  **Passo 4: Calcular $\hat{\beta} = (H^T H)^{-1} H^T y$ :**
>  $\hat{\beta} = \begin{bmatrix} 1.5 & -0.5 \\ -0.5 & 0.2 \end{bmatrix} \begin{bmatrix} 24 \\ 64 \end{bmatrix} = \begin{bmatrix} 4 \\ 2 \end{bmatrix}$
>
>  Assim, a estimativa dos par√¢metros √© $\hat{\beta_0} = 1$ e $\hat{\beta_1} = 2$, obtidos tanto pela minimiza√ß√£o dos quadrados quanto pela maximiza√ß√£o da verossimilhan√ßa sob a suposi√ß√£o de erros gaussianos. O modelo de regress√£o ajustado √© $\hat{y} = 1 + 2x$.
>
>   **Passo 5: Calcular os res√≠duos e o erro quadr√°tico m√©dio (MSE):**
>    Os valores preditos s√£o: $\hat{y} = [3, 5, 7, 9]$.
>    Os res√≠duos s√£o: $e = y - \hat{y} = [0, 0, 0, 0]$.
>   O MSE √© calculado como: $MSE = \frac{1}{4} \sum_{i=1}^{4} (y_i - \hat{y}_i)^2 = 0$.  Neste exemplo perfeito, o modelo ajusta os dados exatamente, resultando em um MSE de 0.  Em situa√ß√µes reais, os res√≠duos geralmente ser√£o n√£o nulos.

**Corol√°rio 1:** A vari√¢ncia estimada dos par√¢metros na regress√£o linear, baseada na fun√ß√£o de verossimilhan√ßa, √© dada pela matriz de covari√¢ncia da estimativa $\hat{\beta}$, que √© proporcional √† inversa da matriz $(H^T H)$, onde H √© a matriz de design e $\sigma^2$ a vari√¢ncia do ru√≠do.
*Prova*:
A fun√ß√£o de log-verossimilhan√ßa para a regress√£o linear com erros Gaussianos √© dada por:
$$l(\beta, \sigma^2; Z) = -\frac{N}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - h(x_i)^T \beta)^2 $$
onde $h(x)$ s√£o as vari√°veis preditoras e $\beta$ s√£o os par√¢metros.
A derivada de $l$ em rela√ß√£o a $\beta$ √©:
$$\frac{\partial l}{\partial \beta} = \frac{1}{\sigma^2} \sum_{i=1}^N (y_i - h(x_i)^T \beta)h(x_i)$$
Igualando a zero e resolvendo para $\beta$, obtemos a MLE $\hat{\beta} = (H^T H)^{-1}H^T y$. A matriz de informa√ß√£o de Fisher, $I(\beta)$, √© definida como a esperan√ßa do negativo da segunda derivada da log-verossimilhan√ßa em rela√ß√£o aos par√¢metros [^8.22]:
$$I(\beta) = E\left[ -\frac{\partial^2 l}{\partial \beta \partial \beta^T} \right] = \frac{1}{\sigma^2} H^T H$$
A vari√¢ncia estimada dos par√¢metros √© a inversa da matriz de informa√ß√£o de Fisher:
$$Var(\hat{\beta}) = I(\beta)^{-1} = \sigma^2 (H^T H)^{-1}$$
O corol√°rio segue, mostrando a rela√ß√£o entre a vari√¢ncia estimada dos par√¢metros e a matriz de design e a vari√¢ncia do ru√≠do [^8.22]. $\blacksquare$
```mermaid
graph LR
    subgraph "Variance of Parameter Estimates (Linear Regression)"
    direction TB
        A["Log-Likelihood: l(Œ≤, œÉ¬≤; Z) = -N/2 log(2œÄœÉ¬≤) - 1/(2œÉ¬≤) Œ£(yi - h(xi)TŒ≤)¬≤"]
        B["Fisher Information: I(Œ≤) = E[-‚àÇ¬≤l/‚àÇŒ≤‚àÇŒ≤T] = (1/œÉ¬≤) HTH"]
        C["Variance of Œ≤ÃÇ: Var(Œ≤ÃÇ) = I(Œ≤)-¬π = œÉ¬≤(HTH)-¬π"]
        A --> B
        B --> C
    end
```

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior, vamos calcular a vari√¢ncia estimada dos par√¢metros.
>
> **Passo 1: Calcular a vari√¢ncia do ru√≠do $\sigma^2$:**
>
> Como os res√≠duos foram todos iguais a zero no exemplo anterior, vamos assumir que o MSE de um ajuste anterior (com dados diferentes) foi de 0.2 e assumir que ele √© uma boa estimativa da vari√¢ncia do erro $\sigma^2$.
>
> **Passo 2: Calcular a matriz de covari√¢ncia de $\hat{\beta}$:**
>
> $Var(\hat{\beta}) = \sigma^2 (H^T H)^{-1} = 0.2 \begin{bmatrix} 1.5 & -0.5 \\ -0.5 & 0.2 \end{bmatrix} = \begin{bmatrix} 0.3 & -0.1 \\ -0.1 & 0.04 \end{bmatrix}$
>
> Portanto, a vari√¢ncia estimada do par√¢metro $\beta_0$ (intercepto) √© $0.3$, a vari√¢ncia estimada do par√¢metro $\beta_1$ (coeficiente angular) √© $0.04$, e a covari√¢ncia entre os par√¢metros √© $-0.1$. Esses valores representam a incerteza em torno das estimativas dos par√¢metros e podem ser usados para calcular intervalos de confian√ßa para as estimativas.

**Conceito 3: Bootstrap e Maximum Likelihood**
O m√©todo **Bootstrap** fornece uma abordagem computacional para avaliar a incerteza, por meio de reamostragem do conjunto de dados original [^8.2.1]. Existem duas varia√ß√µes principais: o **Bootstrap n√£o param√©trico**, que amostra os dados originais com reposi√ß√£o, e o **Bootstrap param√©trico**, que simula novos dados a partir de um modelo ajustado. O Bootstrap param√©trico, em particular, se alinha mais de perto com a ideia de m√°xima verossimilhan√ßa, pois usa as estimativas MLE como base para a gera√ß√£o de novos conjuntos de dados [^8.2.2]. No exemplo de suaviza√ß√£o com splines B, as estimativas do bootstrap param√©trico convergem para as bandas de confian√ßa da regress√£o linear, √† medida que o n√∫mero de amostras do bootstrap tende ao infinito [^8.2.2].
```mermaid
graph LR
    subgraph "Bootstrap Methods"
        direction TB
        A["Original Data"]
        B["Non-Parametric Bootstrap: Resample from Original Data"]
        C["Parametric Bootstrap: Simulate New Data Based on Model Fit"]
        D["MLE estimates used in Parametric Bootstrap"]
        A --> B
        A --> C
        C --> D
    end
```

> ‚ö†Ô∏è **Nota Importante:** A correspond√™ncia entre bootstrap e maximum likelihood surge quando o modelo tem erros aditivos Gaussianos. **Refer√™ncia ao t√≥pico [^8.2.2]**.

> ‚ùó **Ponto de Aten√ß√£o:** Em geral, o bootstrap param√©trico converge para a m√°xima verossimilhan√ßa, e n√£o para m√≠nimos quadrados, mas podem coincidir em modelos com erros Gaussianos. **Conforme indicado em [^8.2.2]**.

> ‚úîÔ∏è **Destaque:** A vantagem do bootstrap sobre as abordagens baseadas em f√≥rmulas da m√°xima verossimilhan√ßa √© que ele pode ser aplicado em situa√ß√µes onde n√£o existem f√≥rmulas anal√≠ticas dispon√≠veis. **Baseado no t√≥pico [^8.2.3]**.

> üí° **Exemplo Num√©rico:**
>
> Vamos usar os dados do exemplo de regress√£o linear anterior.
>
>  **Passo 1: Realizar um bootstrap param√©trico:**
>
>   - Ajustamos o modelo linear aos dados originais, obtendo $\hat{\beta_0} = 1$ e $\hat{\beta_1} = 2$ (como calculado anteriormente) e  $\hat{\sigma}^2 = 0.2$
>   - Para cada reamostragem bootstrap (digamos, 1000 reamostragens), geramos novos conjuntos de dados $y^*_i$ para cada $x_i$ usando a f√≥rmula:  $y^*_i = 1 + 2x_i + \epsilon_i$, onde $\epsilon_i$ √© amostrado de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia $\hat{\sigma}^2 = 0.2$.
>   - Para cada conjunto de dados reamostrado, recalculamos as estimativas de regress√£o $\hat{\beta_0}^*$ e $\hat{\beta_1}^*$ .
>
>   **Passo 2: Calcular intervalos de confian√ßa:**
>
>   - Ap√≥s as 1000 reamostragens, temos uma distribui√ß√£o de $\hat{\beta_0}^*$ e $\hat{\beta_1}^*$.  Usamos essa distribui√ß√£o para calcular intervalos de confian√ßa para os par√¢metros originais. Por exemplo, podemos usar o 2.5¬∫ e o 97.5¬∫ percentil da distribui√ß√£o bootstrap para um intervalo de confian√ßa de 95%.
>
>  **Passo 3: Comparar com os resultados te√≥ricos:**
>
>  - Em modelos com erros gaussianos, as estimativas do bootstrap param√©trico devem convergir para as estimativas da m√°xima verossimilhan√ßa, como discutido anteriormente. Se o n√∫mero de reamostragens bootstrap for grande, os intervalos de confian√ßa obtidos atrav√©s do bootstrap devem estar pr√≥ximos daqueles calculados usando a matriz de covari√¢ncia dos par√¢metros.
>
>   Este exemplo ilustra como o bootstrap param√©trico pode ser usado para aproximar a distribui√ß√£o dos par√¢metros e quantificar a incerteza associada a eles.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o
```mermaid
flowchart TD
  subgraph "Linear Regression for Classification"
    A["Encode Classes (One-Hot)"] --> B["Estimate Coefficients via Least Squares"]
    B --> C["Apply Decision Rule (e.g., highest probability)"]
    C --> D["Limitations: Not suitable for non-linear classes"]
  end
```
**Explica√ß√£o:** Este diagrama ilustra o processo de como a regress√£o linear pode ser usada para classifica√ß√£o, usando um esquema de codifica√ß√£o one-hot para as classes e a aplica√ß√£o da regra de decis√£o baseada em maior probabilidade.

A regress√£o linear pode ser utilizada em problemas de classifica√ß√£o ao codificar as classes por meio de vetores indicadores (one-hot encoding). No entanto, essa abordagem apresenta limita√ß√µes, especialmente quando as classes n√£o s√£o linearmente separ√°veis [^8.2]. Al√©m disso, a regress√£o linear pode levar a valores de probabilidade fora do intervalo [0,1], o que √© um problema quando se busca uma interpreta√ß√£o probabil√≠stica das previs√µes [^8.4]. Em contrapartida, m√©todos como a regress√£o log√≠stica s√£o mais adequados para classifica√ß√£o, pois garantem que os resultados estejam dentro desse intervalo, por meio da fun√ß√£o log√≠stica.

**Lemma 2:** Em um problema de classifica√ß√£o bin√°ria com codifica√ß√£o one-hot (0 e 1) para as classes, a minimiza√ß√£o da soma dos quadrados dos res√≠duos na regress√£o linear √© equivalente √† proje√ß√£o das amostras em um subespa√ßo, que, em certas condi√ß√µes, √© similar √† abordagem da an√°lise discriminante linear.
*Prova:*
Seja um problema de classifica√ß√£o bin√°ria com classes codificadas como 0 e 1. A regress√£o linear busca um vetor de par√¢metros $\beta$ que minimize a soma dos quadrados dos res√≠duos. O ajuste de m√≠nimos quadrados pode ser interpretado como a proje√ß√£o dos valores de $y_i$ nos valores preditos $\hat{y_i}$. A proje√ß√£o resulta em um hiperplano. Em casos espec√≠ficos, onde a distribui√ß√£o das classes √© aproximadamente normal com covari√¢ncias semelhantes, essa proje√ß√£o pode ser aproximada √† dire√ß√£o discriminante da LDA. A minimiza√ß√£o dos res√≠duos na regress√£o linear √© equivalente a essa proje√ß√£o, o que justifica o uso da regress√£o linear para classifica√ß√£o em certos cen√°rios. $\blacksquare$
```mermaid
graph LR
    subgraph "Linear Regression for Binary Classification"
    direction TB
        A["Binary Classes (0 and 1)"]
        B["Minimize Sum of Squared Residuals"]
        C["Project Samples onto a Subspace"]
        D["Approximates Linear Discriminant Analysis (LDA) under certain conditions"]
        A --> B
        B --> C
        C --> D
    end
```

**Corol√°rio 2:** Apesar da similaridade com a an√°lise discriminante linear em algumas condi√ß√µes, a regress√£o linear de indicadores n√£o fornece uma interpreta√ß√£o probabil√≠stica direta para cada classe, pois as previs√µes podem extrapolar o intervalo [0,1].
*Prova:*
Na regress√£o linear, o modelo ajusta uma reta ou hiperplano aos dados, permitindo previs√µes em todo o espa√ßo real. Ao usar indicadores para classifica√ß√£o, os valores ajustados podem ser negativos ou maiores que 1. Isso contrasta com m√©todos como a regress√£o log√≠stica, que usa a fun√ß√£o sigmoide para garantir que as probabilidades fiquem dentro do intervalo [0,1], fornecendo uma interpreta√ß√£o probabil√≠stica direta. Portanto, a regress√£o linear de indicadores, embora √∫til para encontrar uma separa√ß√£o linear entre as classes, n√£o fornece diretamente estimativas de probabilidade v√°lidas. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Imagine um problema de classifica√ß√£o com duas classes (A e B) e duas features ($x_1$ e $x_2$).  Temos o seguinte dataset:
>
> |$x_1$ | $x_2$ | Classe |
> |------|------|--------|
> | 1    | 2    | A      |
> | 1.5  | 1.8  | A      |
> | 3    | 4    | B      |
> | 3.2  | 3.8  | B      |
>
> **Passo 1: Codificar as Classes**
>
> Usamos codifica√ß√£o one-hot: A = 0, B = 1.
>
> **Passo 2: Ajustar um modelo de regress√£o linear:**
>
> Ajustamos um modelo da forma $\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2$. Para simplificar, vamos assumir que os coeficientes resultantes s√£o $\beta_0 = -1$, $\beta_1 = 1$, e $\beta_2 = 0.5$.
>
>  **Passo 3: Calcular as previs√µes:**
>
>  - Para o primeiro ponto (1, 2): $\hat{y} = -1 + 1*1 + 0.5*2 = 0$.
>  - Para o segundo ponto (1.5, 1.8): $\hat{y} = -1 + 1*1.5 + 0.5*1.8 = 0.4$.
>  - Para o terceiro ponto (3, 4): $\hat{y} = -1 + 1*3 + 0.5*4 = 4$.
>  - Para o quarto ponto (3.2, 3.8): $\hat{y} = -1 + 1*3.2 + 0.5*3.8 = 4.1$.
>
> **Passo 4: Aplicar a Regra de Decis√£o:**
>
>  - Classificamos como classe A se $\hat{y} < 0.5$ e como classe B se $\hat{y} \ge 0.5$.
>
>  Observamos que, neste caso, os dois primeiros pontos s√£o classificados corretamente como classe A, e os dois √∫ltimos como classe B. No entanto, note que os valores de $\hat{y}$ para a classe B est√£o acima de 1, o que n√£o corresponde a uma probabilidade v√°lida.
>
> Este exemplo demonstra como a regress√£o linear pode ser usada para classifica√ß√£o com codifica√ß√£o one-hot, mas tamb√©m as limita√ß√µes em rela√ß√£o √† interpreta√ß√£o probabil√≠stica, uma vez que os valores preditos podem estar fora do intervalo [0,1].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o
```mermaid
graph LR
    subgraph "Regularization Methods"
        direction TB
        A["L1 Regularization (Lasso): Add penalty ŒªŒ£|Œ≤j|"]
        B["L2 Regularization (Ridge): Add penalty ŒªŒ£Œ≤j¬≤"]
        C["Elastic Net: Combines L1 and L2 penalties"]
        A --> C
        B --> C
    end
```

A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o t√©cnicas cruciais para melhorar a generaliza√ß√£o e a interpretabilidade dos modelos de classifica√ß√£o [^8.2]. M√©todos de regulariza√ß√£o, como L1 (Lasso) e L2 (Ridge), adicionam termos de penaliza√ß√£o √† fun√ß√£o de custo que evitam que os coeficientes do modelo se tornem excessivamente grandes. A regulariza√ß√£o L1 tende a zerar alguns coeficientes, levando a modelos esparsos e, assim, realizando a sele√ß√£o de vari√°veis [^8.1]. A regulariza√ß√£o L2, por outro lado, reduz a magnitude dos coeficientes, melhorando a estabilidade do modelo. A combina√ß√£o dessas duas formas de regulariza√ß√£o √© conhecida como Elastic Net, que equilibra as vantagens de ambas [^8.1]. Em modelos log√≠sticos, esses m√©todos atuam penalizando os par√¢metros na fun√ß√£o de log-verossimilhan√ßa.
$$l(\beta) = \sum_{i=1}^N [y_i \log(p(x_i)) + (1-y_i) \log(1-p(x_i))] - \lambda_1 \sum_{j=1}^p |\beta_j| - \lambda_2 \sum_{j=1}^p \beta_j^2$$

**Lemma 3:** A penaliza√ß√£o L1 na regress√£o log√≠stica leva a estimativas de par√¢metros esparsas, devido √† forma da fun√ß√£o de penaliza√ß√£o.
*Prova*:
Considere a fun√ß√£o de custo regularizada com penaliza√ß√£o L1:
$$J(\beta) = -\frac{1}{N} \sum_{i=1}^N [y_i \log(p(x_i)) + (1-y_i) \log(1-p(x_i))] + \lambda \sum_{j=1}^p |\beta_j|$$
onde $\lambda$ controla a for√ßa da regulariza√ß√£o. A penaliza√ß√£o L1 adiciona uma penalidade proporcional √† soma dos valores absolutos dos coeficientes $\beta_j$. A geometria da fun√ß√£o $|\beta_j|$ (em forma de V) faz com que o m√≠nimo da fun√ß√£o de custo tenda a ocorrer em valores de $\beta_j$ iguais a 0, desde que o par√¢metro de regulariza√ß√£o $\lambda$ seja suficientemente grande. Essa caracter√≠stica contrasta com a penaliza√ß√£o L2 (Ridge), que penaliza os coeficientes atrav√©s de seus quadrados, sem induzir esparsidade. $\blacksquare$
```mermaid
graph LR
    subgraph "L1 Regularization (Lasso)"
         direction TB
        A["Cost Function with L1 Penalty: J(Œ≤) = -1/N Œ£[y·µ¢log(p(x·µ¢)) + (1-y·µ¢)log(1-p(x·µ¢))] + ŒªŒ£|Œ≤‚±º|"]
        B["L1 Penalty: ŒªŒ£|Œ≤‚±º|"]
        C["'V-Shape' of |Œ≤‚±º| tends to make Œ≤‚±º = 0"]
        A --> B
        B --> C

    end
```

**Corol√°rio 3:** A esparsidade induzida pela penaliza√ß√£o L1 na regress√£o log√≠stica leva √† sele√ß√£o de vari√°veis mais relevantes para a classifica√ß√£o, o que melhora a interpretabilidade do modelo e reduz o risco de overfitting.
*Prova*:
A penaliza√ß√£o L1, ao zerar alguns coeficientes, efetivamente remove as vari√°veis associadas a esses coeficientes do modelo, simplificando-o. Esta sele√ß√£o de vari√°veis permite que o modelo se concentre nas caracter√≠sticas mais importantes para a classifica√ß√£o, melhorando sua capacidade de generaliza√ß√£o em novos dados. Essa caracter√≠stica √© particularmente √∫til em conjuntos de dados com um grande n√∫mero de vari√°veis, onde a identifica√ß√£o das mais relevantes √© crucial para a constru√ß√£o de modelos mais eficientes e interpret√°veis. $\blacksquare$
> ‚ö†Ô∏è **Ponto Crucial**: A escolha entre L1, L2 ou Elastic Net depende do problema espec√≠fico e da necessidade de esparsidade ou estabilidade nos par√¢metros. **Conforme discutido em [^8.2]**.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um problema de classifica√ß√£o com 5 vari√°veis preditoras e desejamos aplicar regress√£o log√≠stica com regulariza√ß√£o. Vamos considerar uma simplifica√ß√£o do problema, com um modelo log√≠stico com dois par√¢metros e uma fun√ß√£o de log-verossimilhan√ßa j√° calculada como:
>
> $l(\beta) = -5 + 2\beta_1 + 3\beta_2$
>
> Vamos analisar o efeito das penalidades L1 e L2.
>
> **Caso 1: Sem Regulariza√ß√£o ($\lambda_1 = 0$, $\lambda_2 = 0$):**
>
> Sem regulariza√ß√£o, maximizamos diretamente a log-verossimilhan√ßa. Assumindo que o m√°ximo ocorre em $\beta_1 = 1$ e $\beta_2 = 2$, teremos uma verossimilhan√ßa $l(\beta) = -5 + 2(1) + 3(2) = 3$.
>
> **Caso 2: Regulariza√ß√£o L1 (Lasso) com $\lambda_1 = 0.5$ e $\lambda_2 = 0$:**
>
> A fun√ß√£o a ser maximizada se torna:
>
> $J(\beta) = l(\beta) - \lambda_1 (|\beta_1| + |\beta_2|) = -5 + 2\beta_1 + 3\beta_2 - 0.5(|\beta_1| + |\beta_2|)$
>
> Para simplificar, vamos assumir que o m√°ximo da fun√ß√£o regularizada ocorre em $\beta_1 = 0.8$ e $\beta_2 = 1.5$. Ent√£o,
>
> $J(\beta) = -5 + 2(0.8) + 3(1.5) - 0.5(|0.8| + |1.5|) = -5 + 1.6 + 4.5 - 0.5(2.3) = -5 + 6.1 - 1.15 = -0.05$
>
> Note que a regulariza√ß√£o L1 reduziu os valores dos par√¢metros. Em casos mais complexos, a penalidade L1 pode levar alguns coeficientes a zero, efetivamente realizando a sele√ß√£o de vari√°veis.
>
> **Caso 3: Regulariza√ß√£o L2 (Ridge) com $\lambda_1 = 0$ e $\lambda_2 = 0.5$:**
>
> A fun√ß√£o a ser maximizada se torna:
>
> $J(\beta) = l(\beta) - \lambda_2 (\beta_1^2 + \beta_2^2) = -5 + 2\beta_1 + 3\beta_2 - 0.5(\beta_1^2 + \beta_2^2)$
>
> Novamente, vamos supor que o m√°ximo da fun√ß√£o regularizada ocorre em $\beta_1 = 0.9$ e $\beta_2 = 1.8$, ent√£o:
>
> $J(\beta) = -5 + 2(0.9) + 3(1.8) - 0.5(0.9^2 + 1.8^2) = -5 + 1.8 + 5.4 - 0.5(0.81 + 3.24) = -5 + 7.2 - 2.025 = 0.1