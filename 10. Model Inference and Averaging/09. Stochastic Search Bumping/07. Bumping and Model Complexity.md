## Bumping and Model Complexity

<imagem: Diagrama mostrando um fluxo de trabalho para o bumping, desde a cria√ß√£o de amostras bootstrap at√© a sele√ß√£o do modelo final, com caixas indicando os est√°gios principais e setas mostrando o fluxo de dados. Deve ser usada a linguagem Mermaid.>

```mermaid
graph LR
    A["In√≠cio: Dados de Treinamento"] --> B("Amostragem Bootstrap");
    B --> C{"Ajustar Modelo (v√°rias vezes)"};
    C --> D["Avaliar Modelos"];
    D --> E{"Selecionar Melhor Modelo"};
    E --> F["Modelo Final"];
    F --> G("Fim");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
    style G fill:#cfc,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

A complexidade do modelo √© um fator crucial no desempenho do aprendizado estat√≠stico, influenciando o trade-off entre bias e vari√¢ncia. Modelos complexos podem se ajustar muito bem aos dados de treinamento, mas podem generalizar mal para dados n√£o vistos (overfitting), enquanto modelos simples podem n√£o conseguir capturar padr√µes importantes nos dados (underfitting). T√©cnicas como bumping, discutidas neste cap√≠tulo, oferecem abordagens para navegar neste trade-off, utilizando resampling para encontrar modelos mais robustos e precisos [^8.9]. Este cap√≠tulo explora como o bumping, em conjunto com a sele√ß√£o de modelos e m√©todos de regulariza√ß√£o, ajuda a gerenciar a complexidade do modelo para obter melhores resultados preditivos.

### Conceitos Fundamentais

**Conceito 1: Complexidade do Modelo e Overfitting**
A complexidade de um modelo refere-se ao seu n√∫mero de par√¢metros e √† sua capacidade de se ajustar a dados de treinamento. Modelos com muitos par√¢metros podem se ajustar a ru√≠dos nos dados de treinamento, levando ao overfitting. Este fen√¥meno faz com que o modelo tenha bom desempenho nos dados de treinamento, mas um desempenho ruim em dados n√£o vistos. A complexidade do modelo √© um ponto cr√≠tico no desenvolvimento de modelos preditivos eficazes, sendo um dos principais objetivos do aprendizado de m√°quina encontrar um balan√ßo entre a complexidade e a capacidade de generaliza√ß√£o [^8.1].

**Lemma 1:** *A vari√¢ncia das previs√µes de um modelo geralmente aumenta com o aumento da complexidade do modelo*. Isso ocorre porque modelos complexos tendem a ser mais sens√≠veis a pequenas altera√ß√µes nos dados de treinamento, resultando em maiores varia√ß√µes em suas previs√µes. Formalmente, para um conjunto de modelos $f_1, f_2, \ldots, f_m$ com complexidades crescentes, a vari√¢ncia das previs√µes  $Var(f_i(x))$ tende a aumentar com $i$. [^8.1, ^8.9]

```mermaid
graph LR
    subgraph "Model Complexity and Variance"
        direction TB
        A["Model Complexity (i)"]
        B["Variance of Predictions: Var(f·µ¢(x))"]
        A --> B
        style A fill:#f9f,stroke:#333,stroke-width:2px
        style B fill:#ccf,stroke:#333,stroke-width:2px
    end
```

> üí° **Exemplo Num√©rico:** Suponha que temos tr√™s modelos de regress√£o: $f_1(x) = \beta_0 + \beta_1x$, $f_2(x) = \beta_0 + \beta_1x + \beta_2x^2$, e $f_3(x) = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3$.  Se ajustarmos esses modelos a diferentes amostras bootstrap do mesmo conjunto de dados e calcularmos a vari√¢ncia das previs√µes em um ponto $x=2$, podemos observar que $Var(f_1(2)) < Var(f_2(2)) < Var(f_3(2))$. Isso ilustra que a vari√¢ncia das previs√µes aumenta com a complexidade do modelo, ou seja, com o n√∫mero de par√¢metros.
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> np.random.seed(42)
> X = np.random.rand(100, 1) * 10
> y = 2 * X.squeeze() + 0.5 * X.squeeze()**2 + np.random.randn(100) * 5
>
> def bootstrap_sample(X, y):
>     indices = np.random.choice(len(X), len(X), replace=True)
>     return X[indices], y[indices]
>
> def fit_and_predict(X, y, degree, x_val):
>   X_poly = np.column_stack([X.squeeze()**i for i in range(degree + 1)])
>   model = LinearRegression()
>   model.fit(X_poly, y)
>   x_val_poly = np.array([x_val**i for i in range(degree + 1)]).reshape(1, -1)
>   return model.predict(x_val_poly)
>
> n_bootstraps = 100
> x_val = 2
>
> predictions_f1 = []
> predictions_f2 = []
> predictions_f3 = []
>
> for _ in range(n_bootstraps):
>   X_boot, y_boot = bootstrap_sample(X,y)
>   predictions_f1.append(fit_and_predict(X_boot, y_boot, 1, x_val)[0])
>   predictions_f2.append(fit_and_predict(X_boot, y_boot, 2, x_val)[0])
>   predictions_f3.append(fit_and_predict(X_boot, y_boot, 3, x_val)[0])
>
> print(f"Var(f1(2)): {np.var(predictions_f1):.2f}")
> print(f"Var(f2(2)): {np.var(predictions_f2):.2f}")
> print(f"Var(f3(2)): {np.var(predictions_f3):.2f}")
> ```
> Este c√≥digo gera um conjunto de dados com uma rela√ß√£o quadr√°tica, ajusta modelos de regress√£o linear com diferentes graus de polin√¥mios usando bootstrapping e calcula a vari√¢ncia das previs√µes para x=2 para cada modelo. Os resultados mostram que a vari√¢ncia das previs√µes aumenta com a complexidade do modelo.

**Conceito 2: Bumping como Ferramenta de Busca em Model Space**
O bumping utiliza amostragem bootstrap para explorar diferentes partes do *model space*. Em vez de usar um √∫nico modelo ajustado aos dados originais, o bumping ajusta modelos a m√∫ltiplas amostras bootstrap dos dados. Isso gera uma cole√ß√£o de modelos ligeiramente diferentes que representam diferentes potenciais solu√ß√µes. O melhor desses modelos, avaliado no conjunto de dados de treinamento original, √© ent√£o escolhido como o modelo final. Este processo visa encontrar solu√ß√µes mais robustas do que um ajuste √∫nico baseado em um conjunto espec√≠fico de dados [^8.9].

**Corol√°rio 1:** Ao perturbar os dados de treinamento via bootstrap, o bumping permite que o processo de ajuste explore diferentes regi√µes do *model space*, potencialmente evitando m√≠nimos locais que um √∫nico ajuste poderia encontrar.  Isso √© particularmente √∫til quando o espa√ßo de modelos √© complexo e n√£o convexo, como no caso de √°rvores de decis√£o, conforme discutido em [^8.9].

```mermaid
graph LR
    subgraph "Bumping and Model Space"
        direction TB
        A["Data Perturbation via Bootstrap"]
        B["Exploration of Model Space"]
        C["Avoidance of Local Minima"]
        A --> B
        B --> C
        style A fill:#f9f,stroke:#333,stroke-width:2px
        style C fill:#ccf,stroke:#333,stroke-width:2px
    end
```

> üí° **Exemplo Num√©rico:** Imagine que estamos ajustando uma √°rvore de decis√£o a um conjunto de dados. O espa√ßo de poss√≠veis √°rvores √© vasto e n√£o convexo, e o algoritmo de ajuste pode ficar preso em uma solu√ß√£o sub√≥tima. Ao criar amostras bootstrap do conjunto de dados original e ajustar uma √°rvore de decis√£o a cada uma dessas amostras, obtemos uma variedade de √°rvores, algumas das quais podem estar mais pr√≥ximas de uma solu√ß√£o √≥tima. Bumping seleciona a melhor entre essas √°rvores com base em seu desempenho no conjunto de dados original.

**Conceito 3: Trade-off Bias-Vari√¢ncia e Bumping**
Bumping busca o trade-off bias-vari√¢ncia, reduzindo a vari√¢ncia de previs√µes. Enquanto modelos complexos tendem a ter baixo bias e alta vari√¢ncia, o bumping ajuda a reduzir essa vari√¢ncia atrav√©s do processo de amostragem e sele√ß√£o, conforme observado em [^8.7]. Ao explorar diferentes solu√ß√µes e selecionar aquela que apresenta o melhor desempenho nos dados de treinamento, o bumping pode gerar modelos que generalizam melhor, particularmente quando se lida com dados altamente correlacionados, como ilustrado no exemplo de √°rvores de decis√£o em [^8.7].

```mermaid
graph LR
    subgraph "Bias-Variance Tradeoff with Bumping"
        direction TB
        A["High Variance (Complex Model)"]
        B["Bumping via Bootstrap Sampling"]
        C["Reduction of Variance"]
        A --> B
        B --> C
        style A fill:#f9f,stroke:#333,stroke-width:2px
        style C fill:#ccf,stroke:#333,stroke-width:2px
    end
```

> ‚ö†Ô∏è **Nota Importante**: Bumping n√£o garante sempre uma melhoria, mas √© uma t√©cnica valiosa para modelos inst√°veis, como √°rvores de decis√£o, onde pequenas mudan√ßas nos dados de treinamento levam a grandes mudan√ßas no modelo. **Refer√™ncia ao t√≥pico [^8.7.1]**.

> ‚ùó **Ponto de Aten√ß√£o**: Em compara√ß√£o com o bagging, que gera um conjunto de modelos e os combina para obter um √∫nico resultado, o bumping seleciona apenas um dos modelos gerados via bootstrap, aquele que apresenta melhor desempenho nos dados de treinamento. **Conforme indicado em [^8.9]**.

> ‚úîÔ∏è **Destaque**: A capacidade do bumping de evitar m√≠nimos locais e reduzir a vari√¢ncia o torna um m√©todo particularmente atraente para problemas onde outros m√©todos de regulariza√ß√£o podem n√£o ser t√£o eficazes. **Baseado no t√≥pico [^8.9]**.

### Regress√£o Linear e Bumping: Um Exemplo Te√≥rico

O bumping, embora frequentemente associado a m√©todos n√£o param√©tricos, pode ser explorado em contextos de regress√£o linear para analisar seu impacto na complexidade do modelo. O uso de B-splines, como em [^8.2.1], permite que modelos lineares capturem n√£o linearidades nos dados por meio da expans√£o de uma fun√ß√£o base. Ao usar bumping em um contexto de regress√£o linear, podemos observar como a amostragem bootstrap e a sele√ß√£o de modelos influenciam a complexidade e a estabilidade do modelo final.

**Exemplo de diagrama com Mermaid:**
```mermaid
flowchart TD
    A["Dados"] --> B("Amostragem Bootstrap");
    B --> C{"Regress√£o Linear c/ B-splines"};
    C --> D["Avalia√ß√£o"];
    D --> E{"Sele√ß√£o do Modelo"};
    E --> F["Previs√£o"]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

**Explica√ß√£o:** Este diagrama ilustra o processo do bumping aplicado a modelos de regress√£o linear utilizando B-splines, ressaltando como o bootstrap e a sele√ß√£o do modelo contribuem para um ajuste mais robusto.

O bumping em regress√£o linear pode ser demonstrado por meio de uma an√°lise te√≥rica, onde buscamos modelar uma rela√ß√£o n√£o linear entre vari√°veis usando B-splines como fun√ß√£o base. Seja $Y = f(X) + \epsilon$ um modelo onde $f(X)$ representa a verdadeira rela√ß√£o entre uma vari√°vel preditora $X$ e uma resposta $Y$. Assumimos que  $f(X)$ pode ser aproximado por uma expans√£o linear de fun√ß√µes B-spline, $\mu(x) = \sum_{j=1}^{7}\beta_jh_j(x)$, conforme discutido em [^8.2.1].

**Lemma 2:** Em um cen√°rio de regress√£o linear com B-splines, o bumping ajuda a reduzir a vari√¢ncia do modelo atrav√©s da sele√ß√£o de modelos ajustados a amostras bootstrap, levando a um estimador mais est√°vel e com melhor desempenho de generaliza√ß√£o. Formalmente, se definirmos $\hat{\beta}^b$ como os coeficientes estimados da regress√£o em uma amostra bootstrap $b$, e $\hat{\beta}_{bumping}$ como os coeficientes selecionados via bumping, ent√£o $Var(\hat{\beta}_{bumping}) \leq  \frac{1}{B} \sum_{b=1}^B  Var(\hat{\beta}^b)$.

```mermaid
graph LR
    subgraph "Variance Reduction in Bumping"
        direction TB
        A["Variance of Bootstrap Estimates: Var(Œ≤ÃÇ·µá)"]
        B["Average Variance: (1/B) Œ£ Var(Œ≤ÃÇ·µá)"]
        C["Variance of Bumping Estimate: Var(Œ≤ÃÇ_bumping)"]
        A --> B
        B --> C
        C -.-> D["Var(Œ≤ÃÇ_bumping) ‚â§ (1/B) Œ£ Var(Œ≤ÃÇ·µá)"]
        style A fill:#f9f,stroke:#333,stroke-width:2px
        style D fill:#ccf,stroke:#333,stroke-width:2px

    end
```

**Prova do Lemma 2:** A vari√¢ncia de um estimador linear em regress√£o √© diretamente influenciada pela vari√¢ncia dos dados. Atrav√©s da amostragem bootstrap, criamos v√°rias vers√µes ligeiramente diferentes dos dados, cada um gerando um modelo ligeiramente diferente. O processo de sele√ß√£o do bumping escolhe um desses modelos, cujo desempenho √© avaliado com base na sua performance nos dados originais (n√£o-perturbados). Ao escolher o melhor modelo, reduzimos o risco de selecionar um modelo excessivamente complexo que poderia ter um grande vi√©s devido a uma amostra espec√≠fica. Este efeito de redu√ß√£o de vari√¢ncia ocorre devido ao processo de sele√ß√£o e n√£o devido √† m√©dia, como no bagging. O processo de bootstrap gera uma distribui√ß√£o de modelos, e ao selecionarmos o modelo que melhor se ajusta aos dados originais, estamos implicitamente reduzindo a variabilidade em torno da solu√ß√£o ideal, pois estamos escolhendo um modelo que minimiza o erro dentro desse espa√ßo amostral, comparativamente aos modelos que n√£o apresentam esse comportamento. $\blacksquare$

> üí° **Exemplo Num√©rico:** Vamos considerar um conjunto de dados com 100 pontos que seguem uma rela√ß√£o n√£o linear. Para simplificar, vamos usar apenas duas B-splines, $h_1(x)$ e $h_2(x)$. O modelo completo seria $\mu(x) = \beta_1 h_1(x) + \beta_2 h_2(x)$.  Ajustamos esse modelo a 10 amostras bootstrap. Para cada amostra $b$, obtemos estimativas de $\beta_1$ e $\beta_2$, que chamamos de $\hat{\beta}^b_1$ e $\hat{\beta}^b_2$.  Se a vari√¢ncia de $\hat{\beta}^b_1$ em uma amostra bootstrap $b$ for $Var(\hat{\beta}^b_1) = 0.2$ e a vari√¢ncia de $\hat{\beta}^b_2$ for $Var(\hat{\beta}^b_2)=0.3$, e repetirmos isso para 10 amostras bootstrap, a vari√¢ncia de um modelo selecionado via bumping, $Var(\hat{\beta}_{bumping})$, ser√° menor que a m√©dia das vari√¢ncias individuais das amostras bootstraps.
>
> Suponha que os valores dos coeficientes $\beta$ estimados nas 10 amostras bootstrap e o valor do melhor modelo via bumping $\hat{\beta}_{bumping}$ s√£o mostrados na tabela abaixo:
>
> | Amostra (b) | $\hat{\beta}^b_1$ | $\hat{\beta}^b_2$ |
> |---|---|---|
> | 1 | 1.2 | 2.1 |
> | 2 | 1.3 | 1.9 |
> | 3 | 1.1 | 2.2 |
> | 4 | 1.4 | 1.8 |
> | 5 | 1.2 | 2.0 |
> | 6 | 1.3 | 2.1 |
> | 7 | 1.2 | 2.0 |
> | 8 | 1.1 | 1.9 |
> | 9 | 1.3 | 2.2 |
> | 10 | 1.2 | 2.0 |
> | **$\hat{\beta}_{bumping}$** | **1.25** | **2.05** |
>
> Podemos calcular:
>
> *   $Var(\hat{\beta}_1^b) = \frac{1}{10} \sum_{b=1}^{10} (\hat{\beta}_1^b - \bar{\hat{\beta}}_1)^2 = 0.01$ onde $\bar{\hat{\beta}}_1$ √© a m√©dia das estimativas de $\beta_1$ nas 10 amostras.
> *   $Var(\hat{\beta}_2^b) = \frac{1}{10} \sum_{b=1}^{10} (\hat{\beta}_2^b - \bar{\hat{\beta}}_2)^2 = 0.02$ onde $\bar{\hat{\beta}}_2$ √© a m√©dia das estimativas de $\beta_2$ nas 10 amostras.
>
> E, se o modelo selecionado pelo bumping tiver coeficientes $\hat{\beta}_{bumping} = [1.25, 2.05]$, a sua vari√¢ncia ser√° menor. A vari√¢ncia do melhor modelo √© obtida ao executar o c√≥digo abaixo, onde os valores de $\beta$ para cada modelo bootstrap s√£o guardados, sendo o modelo retornado aquele com menor erro.
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
> from sklearn.preprocessing import SplineTransformer
> from sklearn.metrics import mean_squared_error
>
> np.random.seed(42)
> # Generate synthetic data with non-linear relationship
> X = np.sort(np.random.rand(100) * 10)
> y = 2 * X + 0.5 * X**2 + np.sin(X)  + np.random.randn(100) * 2
> X = X.reshape(-1, 1) # Reshape to (n_samples, n_features)
>
> def create_b_splines(X, n_knots=7):
>   spline = SplineTransformer(n_knots=n_knots, degree=3, extrapolate=False)
>   return spline.fit_transform(X)
>
> def bootstrap_sample(X, y):
>   indices = np.random.choice(len(X), len(X), replace=True)
>   return X[indices], y[indices]
>
> def fit_model(X,y):
>  X_bsplines = create_b_splines(X, n_knots=3)
>  model = LinearRegression()
>  model.fit(X_bsplines, y)
>  return model, X_bsplines
>
> n_bootstraps = 10
> models = []
>
> for _ in range(n_bootstraps):
>     X_boot, y_boot = bootstrap_sample(X,y)
>     model, X_bsplines_boot = fit_model(X_boot, y_boot)
>     models.append((model, X_bsplines_boot))
>
> best_model = None
> best_mse = np.inf
>
> for model, X_bsplines in models:
>     y_pred = model.predict(X_bsplines)
>     mse = mean_squared_error(y, y_pred)
>     if mse < best_mse:
>         best_mse = mse
>         best_model = model
>
> print(f"Best model coefficients: {best_model.coef_}")
>
> print(f"MSE: {best_mse:.2f}")
>
>
> predictions_bs = []
> for model, X_bsplines in models:
>  y_pred_bs = model.predict(X_bsplines)
>  predictions_bs.append(y_pred_bs)
>
> predictions_bm = best_model.predict(create_b_splines(X,n_knots=3))
> print(f"Variance of Bootstrap models: {np.var(predictions_bs):.2f}")
> print(f"Variance of Best Bumping model: {np.var(predictions_bm):.2f}")
>
> ```
> Ao executar o c√≥digo acima, podemos verificar que a vari√¢ncia das previs√µes do melhor modelo (selecionado pelo bumping) √© menor que a vari√¢ncia dos modelos obtidos pelas amostras bootstrap, ilustrando o Lemma 2. O valor exato da vari√¢ncia depende dos dados, mas a rela√ß√£o de ordem √© a esperada.

**Corol√°rio 2:** Enquanto o bagging em um modelo linear com B-splines tende a reproduzir o modelo original (conforme em [^8.7]), o bumping cria a possibilidade de selecionar modelos ligeiramente diferentes que minimizam o erro em um conjunto de dados espec√≠fico. Em modelos complexos,  esta sele√ß√£o pode impactar significativamente o desempenho de generaliza√ß√£o.

> üí° **Exemplo Num√©rico**:  Suponha que temos um modelo linear com B-splines que √© ajustado a um conjunto de dados. Se aplicarmos o bagging, cada modelo ajustado nas amostras bootstrap ser√° muito similar ao modelo original, e a m√©dia desses modelos resultar√° em algo muito pr√≥ximo ao modelo original. No entanto, com o bumping, cada modelo ajustado a uma amostra bootstrap pode ser um pouco diferente do original. O bumping seleciona o modelo que apresenta o melhor desempenho nos dados de treinamento, o que pode resultar em um modelo com melhor capacidade de generaliza√ß√£o.
>
>  Em termos de c√≥digo, o bagging em regress√£o linear com B-splines, tende a reproduzir o modelo original, enquanto o bumping seleciona um modelo ligeiramente diferente, como demonstrado no c√≥digo do Exemplo Num√©rico do Lemma 2. A diferen√ßa crucial est√° na etapa de sele√ß√£o no bumping, em vez da m√©dia no bagging.

> ‚ö†Ô∏è **Ponto Crucial**: Em contraste com o bagging, que faz uma m√©dia sobre v√°rios modelos, o bumping seleciona apenas um modelo ap√≥s a avalia√ß√£o em m√∫ltiplas amostras bootstrap. Este processo de sele√ß√£o √© crucial para a redu√ß√£o de vari√¢ncia.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o no Contexto do Bumping

O bumping pode ser visto como um m√©todo de sele√ß√£o de modelos, ou seja, um m√©todo para escolher o "melhor modelo" dentro de um espa√ßo de modelos gerados por bootstrap. Em contextos de classifica√ß√£o, como discutido em [^8.7.1], o bumping pode melhorar o desempenho de √°rvores de decis√£o, escolhendo aquela que melhor se ajusta aos dados originais. A complexidade do modelo √© controlada implicitamente, pois o bumping avalia cada modelo atrav√©s de sua performance, sendo o modelo com melhor desempenho no conjunto de dados de treinamento o escolhido.

A regulariza√ß√£o √© uma t√©cnica que busca reduzir a complexidade de um modelo adicionando um termo de penaliza√ß√£o √† fun√ß√£o de custo. M√©todos de regulariza√ß√£o, como penalidades L1 e L2, podem ser combinados com bumping, adicionando um n√≠vel adicional de controle sobre a complexidade do modelo. Bumping cria uma cole√ß√£o de modelos ajustados a amostras bootstrap, onde o processo de sele√ß√£o favorece modelos que n√£o apenas se ajustam bem aos dados, mas tamb√©m generalizam melhor.

**Lemma 3:** A combina√ß√£o do bumping com regulariza√ß√£o pode produzir modelos mais robustos e com melhor capacidade de generaliza√ß√£o do que usar cada m√©todo isoladamente. Este efeito √© maximizado em espa√ßos de alta dimens√£o, onde a regulariza√ß√£o ajuda a evitar o overfitting, e o bumping garante que estamos utilizando o modelo com melhor desempenho para os dados dispon√≠veis, entre os diferentes modelos encontrados no processo de bootstrap.

```mermaid
graph LR
    subgraph "Bumping with Regularization"
        direction TB
        A["Bootstrap Sampling"]
        B["Regularized Model Fitting"]
        C["Bumping Model Selection"]
        D["Robust Model with Improved Generalization"]
        A --> B
        B --> C
        C --> D
    end
```

**Prova do Lemma 3:** Considere um modelo de regress√£o com regulariza√ß√£o L2, onde a fun√ß√£o de custo √© dada por $J(\beta) = \sum_{i=1}^N (y_i - x_i^T\beta)^2 + \lambda ||\beta||^2$. Em um cen√°rio de bumping, ajustamos esse modelo a v√°rias amostras bootstrap e escolhemos aquele que apresenta menor erro nos dados originais. A regulariza√ß√£o $\lambda ||\beta||^2$ impede que os coeficientes $\beta$ cres√ßam muito, ajudando a controlar o overfitting. O bumping adiciona uma etapa de sele√ß√£o entre os modelos regularizados, escolhendo o modelo com menor erro nos dados originais, o que leva a modelos com melhor capacidade de generaliza√ß√£o. Assim, a combina√ß√£o de bumping com regulariza√ß√£o L2 leva a modelos com menor vari√¢ncia devido ao bumping e menor bias devido a regulariza√ß√£o. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo de regress√£o linear com 10 vari√°veis preditoras. O modelo inicial seria $y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_{10}x_{10}$.  A regulariza√ß√£o L2 adiciona um termo de penalidade que restringe os coeficientes $\beta$. A fun√ß√£o de custo a ser minimizada com regulariza√ß√£o L2 √©:
>
> $J(\beta) = \sum_{i=1}^N (y_i - \sum_{j=0}^{10} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{10} \beta_j^2$
>
> Onde $\lambda$ √© o par√¢metro de regulariza√ß√£o. Vamos usar $\lambda=0.1$.
>
>  O processo de bumping ajusta este modelo a v√°rias amostras bootstrap e seleciona o que apresenta melhor desempenho (menor erro quadr√°tico m√©dio) nos dados de treinamento originais. A regulariza√ß√£o impede que os coeficientes cres√ßam muito, controlando o overfitting.
>
> ```python
> import numpy as np
> from sklearn.linear_model import Ridge, LinearRegression
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import mean_squared_error
>
> np.random.seed(42)
>
> # Generate synthetic data
> n_samples = 100
> n_features = 10
> X = np.random.rand(n_samples, n_features)
> true_coefs = np.random.randn(n_features)
> y = X @ true_coefs + np.random.randn(n_samples) * 0.5
>
> # Split data into training and validation sets
> X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
>
> def bootstrap_sample(X, y):
>     indices = np.random.choice(len(X), len(X), replace=True)
>     return X[indices], y[indices]
>
> def fit_model(X, y, lambda_val):
>    model = Ridge(alpha=lambda_val)
>    model.fit(X,y)
>    return model
>
> n_bootstraps = 10
> lambda_val = 0.1
> models = []
>
> # Generate and store models in list
> for _ in range(n_bootstraps):
>     X_boot, y_boot = bootstrap_sample(X_train, y_train)
>     model = fit_model(X_boot, y_boot, lambda_val)
>     models.append(model)
>
> # Bumping selection
> best_model = None
> best_mse = np.inf
>
> for model in models:
>     y_pred = model.predict(X_train)
>     mse = mean_squared_error(y_train, y_pred)
>     if mse < best_mse:
>        best_mse = mse
>        best_model = model
>
> # Evaluate best model
> y_pred_best_model = best_model.predict(X_val)
> mse_best_model = mean_squared_error(y_val, y_pred_best_model)
>
> # Without Regularization
> model_no_reg = LinearRegression()
> model_no_reg.fit(X_train, y_train)
> y_pred_no_reg = model_no_reg.predict(X_val)
> mse_no_reg = mean_squared_error(y_val, y_pred_no_reg)
>
> print(f"Bumping with L2 Regularization MSE: {mse_best_model:.2f}")
> print(f"Linear regression MSE: {mse_no_reg:.2f}")
>
> print(f"Best model coefficients: {best_model.coef_}")
>
>
>
> ```
>
> O c√≥digo acima gera um conjunto de dados sint√©ticos, divide em treino e valida√ß√£o, ajusta modelos Ridge (com regulariza√ß√£o L2) para cada amostra bootstrap, seleciona o melhor modelo via bumping, e o avalia. Os resultados mostram que a combina√ß√£o de bumping com regulariza√ß√£o L2 produz melhor resultado (menor MSE) do que o uso de regress√£o linear sem regulariza√ß√£o.

**Corol√°rio 3:** A penaliza√ß√£o L1, que promove a esparsidade de coeficientes, pode ser utilizada em conjunto com o bumping para selecionar automaticamente as vari√°veis mais importantes para a predi√ß√£o. Nesse caso, o bumping escolher√° entre modelos esparsos ajustados a diferentes amostras bootstrap, garantindo que o modelo selecionado seja n√£o apenas esparso, mas tamb√©m eficaz para o problema.

> üí° **Exemplo Num√©rico:** Imagine o mesmo cen√°rio do exemplo num√©rico anterior, mas agora usando regulariza√ß√£o L1 (Lasso). A fun√ß√£o de custo seria:
>
> $J(\beta) = \sum_{i=1}^N (y_i - \sum_{j=0}^{10} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{10} |\beta_j|$
>
> Onde $\lambda$ √© o par√¢metro de regulariza√ß√£o L1.
>
> A regulariza√ß√£o L1 for√ßa alguns coeficientes a serem exatamente zero, realizando assim uma sele√ß√£o de vari√°veis autom√°tica. O bumping ajudaria a selecionar qual o melhor modelo L1 com base em amostras bootstrap, o que leva a um modelo esparso, mas tamb√©m eficiente nos dados originais.
>
> ```python
> import numpy as np
> from sklearn.linear_model import Lasso, LinearRegression
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import mean_squared_error
>
> np.random.seed(42)
>
> # Generate synthetic data
> n_samples = 100
> n_features = 10
> X = np.random.rand(n_samples, n_features)
> true_coefs = np.random.randn(n_features)
> y = X @ true_coefs + np.random.randn(n_samples) * 0.5
>
> # Split data into training and validation sets
> X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
>
> def bootstrap_sample(X, y):
>     indices = np.random.choice(len(X), len(X), replace=True)
>     return X[indices], y[indices]
>
> def fit_model(X, y, lambda_val):
>    model = Lasso(alpha=lambda_val)
>    model.fit(X,y)
>    return model
>
> n_bootstraps = 10
> lambda_val = 0.1
> models = []
>
> # Generate and store models in list
> for _ in range(n_bootstraps):
>     X_boot, y_boot = bootstrap_sample(X_train, y_train)
>     model = fit_model(X_boot, y_boot, lambda_val)
>     models.append(model)
>
> # Bumping selection
> best_model = None
> best_mse = np.inf
>
> for model in models:
>     y_pred = model.predict(X_train)
>     mse = mean_squared_error(y_train, y_pred)
>     if mse < best_mse:
>        best_mse = mse
>        best_model = model
>
> # Evaluate best model
> y_pred_best_model = best_model.predict(X_val)
> mse_best_model = mean_squared_error(y_val, y_pred_best_model)
>
> # Without Regularization
> model_no_reg = LinearRegression()
> model_no_reg.fit(X_train, y_train)
> y_pred_no_reg = model_no_reg.predict(X_val)
> mse_no_reg = mean_squared_error(y_val, y_pred_no_reg)
>
> print(f"Bumping with L1 Regularization MSE: {mse_best_model:.2f}")
> print(f"Linear regression MSE: {mse_no_reg:.2f}")
>
> print(f"Best model coefficients: {best_model.coef_}")
>
> ```
>
> Ao executar este c√≥digo, √© poss√≠vel observar que a combina√ß√£o de bumping com regulariza√ß√£o L1 geralmente leva a um modelo com menor MSE e com um n√∫mero reduzido de coeficientes diferentes de zero (devido √† propriedade de esparsidade do Lasso), o que demonstra uma forma de sele√ß√£o de vari√°veis.

```mermaid
graph LR
    subgraph "L1 Regularization with Bumping"
        direction TB
        A["L1 Regularized Model on Bootstrap Samples"]
        B["Sparse Model Coefficients (Variable Selection)"]
        C["Bumping Model Selection"]
        A --> B
        B --> C
    end
```

> ‚ö†Ô∏è **Ponto Crucial**: A combina√ß√£o de bumping com regulariza√ß√£o oferece uma abordagem mais robusta para o ajuste de modelos complexos, controlando a complexidade do modelo e reduzindo o risco de overfitting. **Conforme discutido em [^8.5] e [^8.9]**.

### Separating Hyperplanes e Bumping

Em cen√°rios de classifica√ß√£o, o conceito de separating hyperplanes pode ser combinado com o bumping para encontrar modelos de classifica√ß√£o mais robustos. O processo de bumping pode gerar diferentes hiperplanos com base em diferentes amostras bootstrap. Uma quest√£o crucial √© como o bumping seleciona um hiperplano entre v√°rios gerados pelas amostras bootstrap. O principal objetivo do bumping √© reduzir a vari√¢ncia