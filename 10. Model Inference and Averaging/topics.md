8.2 **The Bootstrap and Maximum Likelihood Methods**

8.2.1 **A Smoothing Example**
*   **Bootstrap for Uncertainty Assessment:** Using resampling to quantify uncertainty in model predictions through a simple smoothing example.
*  **B-Spline Basis Functions:** The use of linear expansion of B-splines to represent function spaces.
*  **Least Squares Estimates of Parameters:**  The minimization of squared errors in order to obtain the parameter estimates:  Œ≤ = (HTH)-1HTy
*  **Covariance Matrix:** The calculation of the covariance matrix of beta using (HTH)-1œÉ¬≤
* **Standard error:** The use of standard error to estimate confidence bands on the function being fitted using se[Œº(x)] = [h(x)T(HTH)-1h(x)]œÉ.
*  **Nonparametric Bootstrap:** Sampling with replacement from training data to produce new datasets for uncertainty estimation.
*   **Parametric Bootstrap:** Simulating new responses by adding Gaussian noise to predicted values, instead of resampling from the training data, y*i = Œº(xi) + Œµi

8.2.2 **Maximum Likelihood Inference**

*   **Parametric Model:** Defining a parametric model as a probability density or mass function for our data: zi ~ gŒ∏(z)
*   **Likelihood Function:**  The likelihood of the model given parameters, defined as:  L(Œ∏;Z) = ‚àè gŒ∏(zi)
*   **Log-Likelihood:** The log of the likelihood, defined as:  l(Œ∏;Z) = ‚àë log gŒ∏(zi)
* **Maximum Likelihood:**  Defining the maximum likelihood as the method that chooses the parameters that maximize the log-likelihood function: ùúÉ =argmax l(Œ∏;Z)
*   **Score Function:** The derivative of the log-likelihood with respect to parameters, ‚Ñì(0; Z) = Œ£ ‚Ñì(Œ∏; zi).
*   **Observed Information:** The negative second derivative of the log-likelihood evaluated at the maximum likelihood estimates: I(Œ∏) = - ‚àë d¬≤l(Œ∏; zi)/dŒ∏dŒ∏T.
*   **Fisher Information:** The expected value of the observed information: i(Œ∏) = E[I(Œ∏)].
*   **Maximum Likelihood Estimator Distribution:** The limiting distribution of the maximum likelihood estimator that follows a normal distribution as N tends to infinity: Œ∏ ~ N(Œ∏0, i(Œ∏0)-1)
*   **Standard Error of Maximum Likelihood Estimator:** The square root of the inverse of the observed information matrix: ‚àöI(Œ∏)-1
*   **Confidence Intervals:** Constructing confidence intervals for the parameters by using the z score of the normal distribution, or by using a chi-squared approximation,  2[l(Œ∏) ‚Äì l(Œ∏0)] ~ œá¬≤p

8.2.3 **Bootstrap versus Maximum Likelihood**

*   **Maximum Likelihood in Smoothing:** Showing the process for how the maximum likelihood works for the B-spline smoothing, as well as how the maximum likelihood parameters matches the linear estimates.
*  **Bootstrap as a computational implementation**: The use of the bootstrap as a computer implementation of parametric or non parametric maximum likelihood, enabling computation of standard error estimates.
*   **Adaptive Choices of Parameters**: Discussing how bootstrap enables the consideration of uncertainty in adaptive choices of parameters, such as the number of knots.
*   **Bootstrap confidence bands:** The explanation of how bootstrap methods can be used in situations where formulas for standard errors and confidence intervals are difficult or not available.

8.3 **Bayesian Methods**

* **Bayesian Approach:** The specification of a sampling model Pr(Z|Œ∏), and a prior distribution, Pr(Œ∏), in order to calculate a posterior distribution Pr(Œ∏|Z).
*  **Posterior Distribution:** Calculating the posterior distribution as the updated knowledge about the parameters after seeing the data: Pr(Œ∏|Z) = Pr(Z|Œ∏).Pr(Œ∏) / ‚à´ Pr(Z|Œ∏).Pr(Œ∏)dŒ∏.
*   **Predictive Distribution:** The posterior distribution basis for predicting the values of a future observation znew.
    Pr(znew|Z) = ‚à´ Pr(znew|Œ∏).Pr(Œ∏|Z)dŒ∏.
*   **Prior Distributions:** Emphasizing that Bayesian methods use prior distributions that allow for the expression of uncertainty before data is observed.
*   **Gaussian Process Prior:** The use of Gaussian processes to specify a prior in the function space, specifying the covariance between values:   K(x,x‚Äô) = œÑ cov [Œº(x), Œº(x‚Äô)]
*   **Gaussian Prior:** A simpler alternative to the gaussian process, that specifies a gaussian prior on the function coefficients, Œ≤ ~ N(0, œÑ‚àë).
* **Posterior Distribution Computation:** The computation of the posterior distribution for the coefficients when using a gaussian prior, by E(Œ≤|Z) = (HTH + (œÉ¬≤/œÑ)‚àë)-1 H¬πy and cov(Œ≤|Z) = œÉ¬≤ (HTH + (œÉ¬≤/œÑ)‚àë)-1.
* **Bayesian Posterior Function values**: The posterior distribution for the values of the function, based on the coefficients posterior values: E(Œº(x)|Z) = h(x)T (HTH + (œÉ¬≤/œÑ)‚àë)-1 H¬πy, and cov[Œº(x), Œº(x')|Z] = h(x)T (HTH + (œÉ¬≤/œÑ)‚àë)-1 h(x') œÉ¬≤.
*  **Prior Correlation Matrix (Œ£):** The specification of the prior correlation matrix Œ£ and its impact on the posterior results.
*   **Noninformative Prior:** The use of a noninformative prior, where œÑ ‚Üí ‚àû, making the posterior proportional to the likelihood.
*   **Prior Variance:** The discussion of how the prior variance, œÑ, controls the smoothness of the model.
*  **Joint posterior:** Highlighting that, while in this text it is not done, one should put a prior on œÉ, and compute the joint posterior distribution.

8.4 **Relationship Between the Bootstrap and Bayesian Inference**

*   **Simple Example for Bayesian Inference:** A simple example where we observe one point from a normal distribution z ~ N(Œ∏,1), where the prior distribution for Œ∏ is Œ∏ ~ N(0,œÑ).
*  **Posterior with Gaussian Prior:**  The posterior distribution of theta in the simple example, Œ∏|z ~ N(z/(1+1/œÑ), 1/(1+1/œÑ)).
*  **Noninformative Prior:** The limit of the posterior distribution with a noninformative prior, where the variance œÑ goes to infinity, that is, Œ∏|z ~ N(z, 1).
*  **Parametric Bootstrap Connection:** The equivalence of the posterior distribution with a noninformative prior to parametric bootstrap with maximum likelihood.
* **Three Key Ingredients for Correspondence:** Highlighting that three ingredients make this correspondence work: noninformative prior, dependence on maximum likelihood estimate, and symmetry of the log likelihood.
*  **Nonparametric Bootstrap and Bayes Correspondence:** The outline of the correspondence of nonparametric bootstrap and Bayes inference in a multinomial distribution.
* **Dirichlet Distribution:** The use of a Dirichlet distribution as a prior on a multinomial sample space with parameters w: w ~ Dir(Œ±I)
* **Posterior Dirichlet Distribution**: The posterior density for the prior based on samples,  w ~ Dir(Œ±I + N≈µ), and how it tends to a bootstrap-like distribution when Œ± goes to 0, w ~ Dir(N≈µ).
*   **Bootstrap Distribution as Posterior:** Framing the bootstrap as an approximate noninformative posterior, that is obtained by perturbing data.

8.5 **The EM Algorithm**

*   **EM Algorithm Introduction:** The discussion of the EM algorithm as a tool to simplify maximum likelihood problems, specifically on mixture models.
*   **Mixture Model:** An introduction to a simple mixture model, where the response variable Y can come from one of two normal distributions with different parameters,  Y ~ (1 ‚àí Œî)Y1 + ŒîY2.
*   **Log Likelihood Mixture Model:** The log-likelihood of a mixture model for N training cases: l(Œ∏;Z) = ‚àë log[(1 ‚àí œÄ)œÜŒ∏1(yi) + œÄœÜŒ∏2(yi)].
*   **Latent Variables:** Highlighting how the EM algorithm works by introducing a latent unobserved variable Œî that indicates where the data comes from.
*  **Complete Data Log-Likelihood:** The use of the complete data log-likelihood using the latent variable, l(Œ∏;Z,Œî) = ‚àë [(1 ‚Äì Œîi) log œÜŒ∏1(yi) + Œîi log œÜŒ∏2(yi)] + ‚àë [(1 ‚Äì Œîi) log(1 ‚Äì œÄ) + Œîi log œÄ].
*   **Responsibility:** How the EM algorithm computes the expected values of the latent variables, called responsibility,  Œ≥i(Œ∏) = E(Œîi|Œ∏, Z) = Pr(Œîi = 1|Œ∏, Z).
*   **Expectation Step:** The step where we compute the responsibilities using the current estimates of the parameters.
*   **Maximization Step:**  The step where we re-estimate parameters using the responsibilities.
*  **Initial Guesses:** The explanation of how to construct initial parameters for the algorithm.
*   **EM convergence:** The explanation that the EM algorithm only converges to a local maximum, with several local maxima possible.

8.5.2 **The EM Algorithm in General**

*  **Data Augmentation:** Emphasizing that the EM algorithm is useful in problems where maximization is made easier by enlarging the dataset.
*   **Complete Data:** The distinction between observed data Z and the latent or missing data Zm, making T=(Z, Zm) the complete data.
*  **Expectation Step (General):** The computation of the expected value of the complete data log likelihood,  Q(Œ∏', Œ∏(i)) = E(l0(Œ∏'; T)|Z, Œ∏(i)).
*  **Maximization Step (General):** The re-estimation of the parameters by maximizing Q(Œ∏', Œ∏(i)) with respect to Œ∏': ùúÉ(i+1) = argmax Q(Œ∏', Œ∏(i))
*   **EM Convergence:**  Proof that the EM algorithm never decreases the log-likelihood using Jensen's inequality.
*   **Generalized EM (GEM):** The explanation that the maximization step can only increase Q(Œ∏', Œ∏(i)), not necessarily find a global maximum.

8.5.3 **EM as a Maximization-Maximization Procedure**

* **Joint Maximization:** Describing a different approach to the EM procedure as a joint maximization for the objective function F(Œ∏', P) = Ep[lo(Œ∏'; T)] ‚Äì Ep[log P(Zm)].
* **Maximization over Latent Space:** The description of the E step as the maximization of F with respect to the latent variables P.
* **Maximization over Parameter Space:** The description of the M step as the maximization of F with respect to the model parameters, Œ∏.
*  **Alternative Maximization Procedure**: How viewing the EM algorithm as a joint maximization procedure leads to alternative optimization procedures.

8.6 **MCMC for Sampling from the Posterior**
*  **MCMC Introduction:** The introduction of Markov Chain Monte Carlo (MCMC) approaches as a powerful way for sampling from a posterior distribution.
*   **Gibbs Sampling:** The explanation of how a Gibbs sampler generates a sequence of correlated samples by sampling each variable from its conditional distributions,   Pr(Uk|U1, U2, ..., Uk-1, Uk+1, ...,UK)
*  **Gibbs Sampling Convergence**: How a Gibbs sampler produces samples from the joint distribution when the process stabilizes, despite the lack of independence.
*   **Estimation with Gibbs Samples:**  Estimating marginal distributions with a density estimate applied to the Gibbs sample.
*   **Gibbs Sampling for Bayesian Inference:** Application of Gibbs sampling to generate samples from the joint posterior distribution of model parameters, conditional on the data: Pr(Œ∏|Z)
*   **Gibbs Sampling in Gaussian Mixtures:** Application of Gibbs sampling to the gaussian mixture problem by fixing œÉ1, œÉ2 and œÄ and sampling Œº1, Œº2 and  Œîi.
*   **Gibbs Sampling Steps:** The explanation that each Gibbs step, is equivalent to the E and M steps in an EM algorithm, but the parameters are being sampled instead of maximized.
* **Gibbs Sampling Behavior:** A simulation result showing how the parameters seem to stabilize at the correct values.
*  **Proper Priors**:  The need for proper (informative) priors in Gibbs sampling, in order to avoid a degenerate posterior.
*   **Gibbs Sampling is Conditional Sampling:**  Highlighting that Gibbs sampling uses conditional sampling given the rest, making this method suitable for problems where this is easy to carry out.

8.7 **Bagging**
*   **Bootstrap Aggregation (Bagging):** Using the bootstrap to improve prediction by averaging it over a collection of bootstrap samples, thereby reducing the variance of the predictions:  fbag(x) = 1/B  ‚àë f*b(x)
*   **Bagging Estimates:**  Defining the bagged estimate in terms of a real population distribution P as fbag(x) = Epf*(x)
*   **Bagging for Regression:** Describing how Bagging is obtained in regression, and how bagged prediction may only differ from original prediction in adaptive and nonlinear functions.
*   **Bagging for Classification:**  Defining the use of bagging in classification with an underlying indicator vector, defining the bagged classifier as a combination of single classifiers.
*   **Averaging Class Probabilities:**  Highlighting how it might be better to average class probabilities rather than using a ‚Äúvote‚Äù rule.
*  **Bagging and Weak Learners:** How the concept of ‚ÄúWisdom of Crowds‚Äù can be applied to bagging, where a consensus of independent weak learners can produce better prediction.
*   **Loss of Model Structure:** Highlighting how bagging a model might mean losing interpretability in certain cases.
*   **Bagging Limitations:** Emphasizing that bagging might not be the best approach for all problems, especially when using a very simple base learner.
*   **Trees with Simulated Data**: A simulation study of bagging on trees, showing the effects of bagging in variance reduction for tree classifiers.

8.7.1 **Example: Trees with Simulated Data**

*   **Simulation Scenario:** Generating synthetic data with two classes and five correlated features in order to showcase bagging on decision trees.
*  **Variance Reduction in Bagging:** Showing that bagging can reduce the test error of unstable classifiers like decision trees.
*   **Original vs. Bagged Trees:** Visual illustration of differences between the original and bagged trees.
*   **Test Error Curves:** Curves for bagged decision trees, and consensus vote, showing reduction in variance as the number of trees grows.

8.8 **Model Averaging and Stacking**

*   **Bayesian Model Averaging:**  A method where bootstrap values are used as a posterior sample, with the bagged estimate as an approximation of the posterior mean.
*  **Model Averaging:** The process of combining predictions from different candidate models by weighing them by the posterior probabilities of each model: E(Œ∂|Z) = Œ£ E(Œ∂|Mm, Z)Pr(Mm|Z)
*   **Committee Methods:** Taking a simple unweighted average of model predictions.
*  **BIC for Model Averaging**:  How the BIC criterion can be used to estimate the posterior probability of different models.
*  **Bayesian Approach to Model Averaging:** A detailed description on how to do model averaging, using a proper prior and numerically calculating posterior probabilities: Pr(Mm|Z) = Pr(Mm) ‚à´ Pr(Z|Œ∏m, Mm)Pr(Œ∏m|Mm)dŒ∏m
*   **Frequentist Viewpoint of Model Averaging:** A description on how to calculate weights for model averaging by minimizing the squared-error loss, argmin Ep[Y - ‚àëwmfm(x)]¬≤.
* **Linear Regression in Model Averaging**:  How the weights can be found by a linear regression of the outcome with respect to the predictions of the different models:  w = Ep[F(x)F(x)T]¬Ø¬πEp[F(x)Y].
*   **Stacking:**   A technique where the training set linear regression is replaced by leave-one-out cross-validation estimates of the predictions.
*  **Stacking for Model Selection:**  How stacking can be used as an approximation to choosing only the best model with cross-validation, but providing a better performance.
*  **Flexibility of Stacking**: Highlighting how stacking is more flexible, allowing a combination of different learning methods.

8.9 **Stochastic Search: Bumping**
*   **Bumping Introduction:**  The explanation of bumping as an alternative approach to finding a good model by avoiding poor local minima.
*  **Bootstrap Samples for Bumping**: How bumping uses bootstrap samples in order to search the model space.
*   **Bumping Algorithm:** Detailed description of how the bootstrap method is applied to find a better model, by choosing a bootstrap estimate that minimizes the prediction error:  b* = argmin  ‚àë  [yi - f*b(xi)]¬≤
*  **Bumping for Tree Models**: A discussion on why the method is useful for tree-based models, as well as other methods that have unstable procedures due to high variability.
*   **Illustration of Bumping on Tree Models:** The use of a XOR example where standard tree growing procedure does not have an optimal split, and how bumping can help find a better split.
*  **Bumping and Model Complexity**: That when doing bumping, the models must have roughly the same complexity.
*  **Bumping and Optimization**:  The use of bumping when the fitting method is hard to optimize, or lacks smoothness, by optimizing a different criterion.
