## Bagging para Regress√£o: Uma An√°lise Aprofundada
```mermaid
graph LR
    subgraph "Bagging Process"
        direction TB
        A["Original Dataset"] --> B["Bootstrap Sampling"]
        B --> C["Bootstrap Sample 1"]
        B --> D["Bootstrap Sample 2"]
        B --> E["Bootstrap Sample ..."]
        B --> F["Bootstrap Sample B"]
        C --> G["Model 1 (f*1(x))"]
        D --> H["Model 2 (f*2(x))"]
        E --> I["Model ... (f*...(x))"]
        F --> J["Model B (f*B(x))"]
        G & H & I & J --> K["Aggregate Predictions"]
        K --> L["Final Prediction (f_bag(x))"]
    end
```

### Introdu√ß√£o
O conceito de **Bagging (Bootstrap Aggregating)** √© uma t√©cnica poderosa no campo do aprendizado de m√°quina, amplamente utilizada para aprimorar a precis√£o e a estabilidade de modelos preditivos, especialmente em regress√£o [^8.7]. Ao contr√°rio dos m√©todos que se baseiam em um √∫nico modelo, o Bagging opera criando m√∫ltiplas vers√µes de um modelo preditivo a partir de amostras bootstrap do conjunto de dados original, agregando-as para obter uma predi√ß√£o final mais robusta [^8.7]. Este cap√≠tulo explora em profundidade os fundamentos te√≥ricos, as aplica√ß√µes pr√°ticas e os nuances do Bagging para regress√£o, baseando-se em conceitos estat√≠sticos e de aprendizado de m√°quina avan√ßados.

### Conceitos Fundamentais
**Conceito 1: Amostragem Bootstrap**
A ess√™ncia do Bagging reside na **amostragem bootstrap**, um m√©todo de reamostragem que cria m√∫ltiplos conjuntos de dados a partir de um √∫nico conjunto de dados original [^8.2.1]. Cada amostra bootstrap √© obtida por amostragem aleat√≥ria com reposi√ß√£o do conjunto de dados original, o que significa que um mesmo ponto de dado pode aparecer v√°rias vezes numa mesma amostra ou n√£o aparecer. Esse processo gera conjuntos de dados que s√£o ligeiramente diferentes entre si, mas preservam as caracter√≠sticas estat√≠sticas do conjunto de dados original. O bootstrap √©, essencialmente, uma ferramenta computacional para avaliar incertezas atrav√©s da reamostragem dos dados de treinamento [^8.2.1].

> üí° **Exemplo Num√©rico:** Suponha um conjunto de dados original com 5 amostras: $Z = \{(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_4, y_4), (x_5, y_5)\}$. Uma poss√≠vel amostra bootstrap $Z^{*1}$ poderia ser: $\{(x_1, y_1), (x_3, y_3), (x_1, y_1), (x_5, y_5), (x_2, y_2) \}$. Observe que $(x_1, y_1)$ aparece duas vezes e $(x_4, y_4)$ n√£o aparece nesta amostra espec√≠fica. Uma segunda amostra bootstrap $Z^{*2}$ poderia ser $\{(x_2, y_2), (x_2, y_2), (x_4, y_4), (x_5, y_5), (x_3, y_3) \}$, e assim por diante.

**Lemma 1: Propriedades do Bootstrap**
Dada uma amostra de dados $Z = \{z_1, z_2, \ldots, z_N\}$, onde $z_i = (x_i, y_i)$, e $B$ amostras bootstrap $Z^{*b}$ criadas, a distribui√ß√£o emp√≠rica das amostras bootstrap converge para a distribui√ß√£o da amostra original quando $N$ e $B$ tendem ao infinito. Isso garante que as an√°lises feitas com as amostras bootstrap s√£o representativas do conjunto de dados original [^8.2.1]. Formalmente:

$$ \lim_{N \to \infty} \frac{1}{B} \sum_{b=1}^{B} \mathbb{I}(Z^{*b} \in A) = P(Z \in A), $$
onde $\mathbb{I}$ √© a fun√ß√£o indicadora e $P(Z \in A)$ √© a probabilidade de a amostra original estar em um conjunto $A$.
$\blacksquare$
```mermaid
graph LR
    subgraph "Bootstrap Convergence"
        direction TB
        A["Original Data Distribution P(Z)"]
        B["Bootstrap Samples Z*b"]
        C["Empirical Distribution of Z*b"]
        D["Lim N -> ‚àû, B -> ‚àû"]
        B --> C
        C --> D
        D --> A
    end
```
**Conceito 2: Agrega√ß√£o de Modelos**
Ap√≥s a cria√ß√£o das amostras bootstrap, o pr√≥ximo passo no Bagging √© o treinamento de um modelo preditivo (como uma √°rvore de regress√£o ou outro regressor) em cada amostra bootstrap. Cada modelo preditivo, $f^{*b}(x)$, √© treinado independentemente. A fase de agrega√ß√£o consiste em combinar as previs√µes desses modelos individuais para formar uma predi√ß√£o final mais robusta, $f_{bag}(x)$. Para regress√£o, essa agrega√ß√£o √© tipicamente realizada pela m√©dia das previs√µes dos modelos individuais:

$$ f_{bag}(x) = \frac{1}{B} \sum_{b=1}^B f^{*b}(x) $$
Essa m√©dia reduz a vari√¢ncia da predi√ß√£o, tornando-a menos sens√≠vel a pequenas varia√ß√µes nos dados de treinamento [^8.7].

> üí° **Exemplo Num√©rico:** Suponha que temos 3 modelos treinados em amostras bootstrap, e para um certo ponto $x$, suas previs√µes s√£o $f^{*1}(x) = 2.3$, $f^{*2}(x) = 2.7$, e $f^{*3}(x) = 2.5$. A previs√£o agregada do Bagging seria: $f_{bag}(x) = (2.3 + 2.7 + 2.5) / 3 = 2.5$.

**Corol√°rio 1: Redu√ß√£o da Vari√¢ncia**
A agrega√ß√£o de modelos via bagging resulta na redu√ß√£o da vari√¢ncia da previs√£o em compara√ß√£o com um √∫nico modelo, pois ao fazer a m√©dia das predi√ß√µes, erros aleat√≥rios tendem a se cancelar, e a estimativa de $f_{bag}(x)$ se torna mais est√°vel e pr√≥xima do valor esperado do modelo. Isso √© especialmente importante em modelos com alta vari√¢ncia [^8.7]. Formalmente:
$$ Var(f_{bag}(x)) \approx  \frac{1}{B} Var(f^*(x)) $$
onde $f^*(x)$ √© a previs√£o de um modelo qualquer treinado em um conjunto bootstrap, e a aproxima√ß√£o se torna mais precisa quando os modelos s√£o aproximadamente independentes.
$\blacksquare$
```mermaid
graph LR
    subgraph "Variance Reduction"
    direction LR
        A["Variance of Individual Model: Var(f*(x))"]
        B["Number of Bootstrap Samples: B"]
        C["Variance of Bagged Prediction: Var(f_bag(x))"]
        A & B --> D["Var(f_bag(x)) ‚âà 1/B * Var(f*(x))"]
        D --> C
    end
```
**Conceito 3: Bootstrap Param√©trico e N√£o Param√©trico**
No contexto do Bagging, podemos distinguir entre o bootstrap n√£o param√©trico e o bootstrap param√©trico. No bootstrap **n√£o param√©trico**, a reamostragem √© feita diretamente nos dados de treinamento observados. No bootstrap **param√©trico**, simula-se novos dados de treinamento com base em uma distribui√ß√£o probabil√≠stica assumida, usando par√¢metros estimados a partir dos dados originais [^8.2.1]. Em modelos Gaussianos, o bootstrap param√©trico pode ser equivalente aos resultados dos m√≠nimos quadrados, mas em geral, concorda com a m√°xima verossimilhan√ßa [^8.2.2].
> ‚ö†Ô∏è **Nota Importante**: O bootstrap n√£o param√©trico usa os dados brutos e √© mais "model-free", enquanto o param√©trico simula dados usando par√¢metros ajustados, conforme discutido em [^8.2.1].
> ‚ùó **Ponto de Aten√ß√£o**: O bootstrap param√©trico converge para a m√°xima verossimilhan√ßa, e n√£o para os m√≠nimos quadrados em geral [^8.2.2].
> ‚úîÔ∏è **Destaque**: Ambas as vers√µes do bootstrap s√£o √∫teis no contexto de bagging, para reduzir a vari√¢ncia das predi√ß√µes do modelo base [^8.7].

### Regress√£o Linear e M√≠nimos Quadrados com Bagging
```mermaid
graph LR
    subgraph "Bagging in Linear Regression"
        direction TB
        A["Original Dataset"] --> B["Bootstrap Sampling"]
        B --> C["Bootstrap Sample 1"]
        B --> D["Bootstrap Sample 2"]
        B --> E["Bootstrap Sample ..."]
        B --> F["Bootstrap Sample B"]
        C --> G["Linear Model 1"]
        D --> H["Linear Model 2"]
        E --> I["Linear Model ..."]
        F --> J["Linear Model B"]
        G & H & I & J --> K["Aggregate Predictions"]
        K --> L["Bagged Linear Prediction"]
    end
```

A regress√£o linear, por ser um modelo param√©trico com baixa vari√¢ncia, pode n√£o se beneficiar tanto do Bagging quanto outros modelos [^8.7]. No entanto, ao aplicar o Bagging √† regress√£o linear com matrizes de indicadores ou splines lineares, podemos obter melhorias sutis. A utiliza√ß√£o de amostras bootstrap para treinar m√∫ltiplos modelos lineares, combinando as previs√µes atrav√©s da m√©dia, pode levar a uma estimativa mais robusta dos par√¢metros, reduzindo a sensibilidade a outliers.

**Lemma 2: Efeito do Bagging na Regress√£o Linear**
Para um modelo de regress√£o linear com par√¢metros $\beta$ e fun√ß√£o de predi√ß√£o $f(x) = h(x)^T \beta$, a previs√£o m√©dia dos modelos Bagging converge para a previs√£o do modelo linear original quando o bootstrap √© param√©trico, e se os dados s√£o amostrados sem reposi√ß√£o, ou quando a quantidade de amostras tende a infinito no caso n√£o param√©trico. Formalmente:

$$ \lim_{B \to \infty} \frac{1}{B} \sum_{b=1}^{B} f^{*b}(x) \approx f(x) $$
Essa propriedade garante que, no caso da regress√£o linear, o vi√©s n√£o √© significativamente alterado pelo Bagging [^8.7].
$\blacksquare$
```mermaid
graph LR
    subgraph "Bagging Convergence in Linear Regression"
        direction TB
        A["Individual Linear Model Predictions: f*b(x)"]
        B["Number of Bootstrap Samples: B"]
        C["Mean of Bagged Predictions"]
        D["Lim B -> ‚àû"]
        A & B --> C
        C --> D
        D --> E["Original Linear Model Prediction: f(x)"]
    end
```

> üí° **Exemplo Num√©rico:** Considere um modelo de regress√£o linear simples $y = \beta_0 + \beta_1 x$. Onde temos um dataset com 10 pontos $(x_i, y_i)$. Ap√≥s aplicar o bagging, podemos obter v√°rios modelos lineares, cada um treinado em uma amostra bootstrap, por exemplo:
>
>  - Modelo 1: $f^{*1}(x) = 1.1 + 2.1x$
>  - Modelo 2: $f^{*2}(x) = 0.9 + 1.9x$
>  - Modelo 3: $f^{*3}(x) = 1.0 + 2.0x$
>
>  A previs√£o agregada para um novo ponto $x=2$ seria:
>
>  - $f^{*1}(2) = 1.1 + 2.1 * 2 = 5.3$
>  - $f^{*2}(2) = 0.9 + 1.9 * 2 = 4.7$
>  - $f^{*3}(2) = 1.0 + 2.0 * 2 = 5.0$
>
>  A predi√ß√£o agregada seria $f_{bag}(2) = (5.3 + 4.7 + 5.0)/3 = 5.0$.

**Corol√°rio 2: Redu√ß√£o da Vari√¢ncia em Regress√£o Linear Generalizada**
Mesmo que a previs√£o m√©dia n√£o se modifique com o Bagging em um modelo linear, a vari√¢ncia da previs√£o √© reduzida. Em um modelo linear generalizado (GLM) utilizando fun√ß√µes base, como splines, as estimativas de par√¢metros se tornam mais robustas por conta da estabiliza√ß√£o da vari√¢ncia atrav√©s do processo do Bagging. Isto resulta em uma previs√£o final mais suave e generaliz√°vel [^8.7].

> ‚ö†Ô∏è **Ponto Crucial**: A regress√£o linear com Bagging se beneficia da redu√ß√£o da vari√¢ncia em cen√°rios de alta complexidade ou com a presen√ßa de outliers, como em modelos com splines lineares. [^8.7].
> ‚ùó **Ponto de Aten√ß√£o**: O ganho de performance da regress√£o linear com Bagging pode ser sutil em rela√ß√£o a outros modelos, devido a sua natureza param√©trica com baixa vari√¢ncia [^8.7].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o com Bagging
A combina√ß√£o do Bagging com t√©cnicas de sele√ß√£o de vari√°veis e regulariza√ß√£o pode levar a modelos de regress√£o ainda mais robustos e precisos. Ao treinar modelos individuais com regulariza√ß√£o (como L1 ou L2) em diferentes amostras bootstrap, o efeito da regulariza√ß√£o √© estabilizado, levando a uma melhor capacidade de generaliza√ß√£o [^8.7]. A penaliza√ß√£o L1 (Lasso) pode levar a modelos mais esparsos, enquanto a penaliza√ß√£o L2 (Ridge) pode melhorar a estabilidade das estimativas de par√¢metros.
```mermaid
graph LR
    subgraph "Bagging with Regularization"
        direction TB
        A["Original Data"] --> B["Bootstrap Sampling"]
        B --> C["Sample 1"]
        B --> D["Sample 2"]
        B --> E["Sample ..."]
         C --> F["Regularized Model 1 (Œ≤*1)"]
        D --> G["Regularized Model 2 (Œ≤*2)"]
        E --> H["Regularized Model ... (Œ≤*...)"]
        F & G & H --> I["Average Regularized Parameters"]
        I --> J["Stable Regularized Parameters (Œ≤_bagged)"]
    end
```

**Lemma 3: Estabilidade da Regulariza√ß√£o com Bagging**
Quando aplicamos regulariza√ß√£o L1 (Lasso), as estimativas de par√¢metros podem variar muito entre as amostras bootstrap. Entretanto, ao agregar os modelos com o Bagging, as estimativas dos coeficientes tornam-se mais est√°veis e pr√≥ximas de uma solu√ß√£o regularizada mais robusta. Formalmente, se $\beta^{*b}$ s√£o os par√¢metros resultantes da regulariza√ß√£o L1 em cada amostra bootstrap, ent√£o:

$$ \lim_{B \to \infty} \frac{1}{B} \sum_{b=1}^B \beta^{*b} \approx \hat{\beta}_{\text{bagged}} $$
onde $\hat{\beta}_{\text{bagged}}$ √© um par√¢metro mais est√°vel do que o encontrado em uma amostra √∫nica de dados [^8.7].
$\blacksquare$
```mermaid
graph LR
    subgraph "Regularization Stability with Bagging"
        direction TB
        A["Individual Regularized Parameters Œ≤*b"]
         B["Number of Bootstrap Samples: B"]
         C["Average of Bagged Parameters"]
         D["Lim B -> ‚àû"]
         A & B --> C
        C --> D
        D --> E["Stable Bagged Parameter: Œ≤_bagged"]
    end
```
> üí° **Exemplo Num√©rico:** Considere um modelo de regress√£o linear com regulariza√ß√£o Lasso (L1) treinado em tr√™s amostras bootstrap. As estimativas dos par√¢metros s√£o $\beta^{*1} = [2.1, 0.2, -0.1]$, $\beta^{*2} = [1.9, 0.0, 0.0]$, e $\beta^{*3} = [2.0, 0.1, -0.2]$. A estimativa agregada dos coeficientes ap√≥s o bagging √© $\hat{\beta}_{\text{bagged}} = [(2.1+1.9+2.0)/3, (0.2+0.0+0.1)/3, (-0.1+0.0-0.2)/3] = [2.0, 0.1, -0.1]$. Note que, ap√≥s o Bagging, os par√¢metros se tornam mais est√°veis, com o segundo par√¢metro sendo pr√≥ximo de zero, indicando uma poss√≠vel sele√ß√£o de vari√°veis.

**Corol√°rio 3: Sele√ß√£o de Vari√°veis por Bagging**
O Bagging tamb√©m pode auxiliar na sele√ß√£o de vari√°veis. Ao observar quais vari√°veis s√£o frequentemente selecionadas em m√∫ltiplos modelos bootstrap, podemos obter *insights* sobre quais vari√°veis s√£o mais relevantes para a predi√ß√£o [^8.7]. Isto leva a modelos mais interpret√°veis e que mant√©m as vari√°veis mais importantes na predi√ß√£o.
```mermaid
graph LR
    subgraph "Feature Selection with Bagging"
        direction TB
        A["Bootstrap Samples"] --> B["Train Models with Regularization"]
        B --> C["Feature Selection Frequency"]
        C --> D["Variable 1: 90%"]
        C --> E["Variable 2: 10%"]
        C --> F["Variable 3: 85%"]
        D & E & F --> G["Insights on Feature Importance"]
    end
```

> üí° **Exemplo Num√©rico:** Imagine que treinamos v√°rios modelos com Lasso em diferentes amostras bootstrap. Observamos que a primeira e terceira vari√°veis s√£o selecionadas em 90% e 85% das vezes, respectivamente, enquanto a segunda vari√°vel √© selecionada em apenas 10% das vezes. Isso sugere que a segunda vari√°vel pode ser menos importante para a predi√ß√£o, e podemos simplificar o modelo removendo-a.

> | Vari√°vel   | Frequ√™ncia de Sele√ß√£o |
> |------------|----------------------|
> | Vari√°vel 1 | 90%                  |
> | Vari√°vel 2 | 10%                  |
> | Vari√°vel 3 | 85%                  |

> Este exemplo demonstra como o Bagging pode ser usado para obter *insights* sobre a import√¢ncia das vari√°veis.

> ‚ö†Ô∏è **Ponto Crucial**: O Bagging estabiliza o efeito da regulariza√ß√£o e auxilia na sele√ß√£o de vari√°veis, levando a modelos mais precisos e generaliz√°veis [^8.7].

### Separating Hyperplanes e o Bagging
O conceito de separating hyperplanes (hiperplanos separadores) √© central para m√©todos de classifica√ß√£o, mas pode ser aplicado para regress√£o com transforma√ß√µes adequadas do espa√ßo de features. Quando usamos um m√©todo de regress√£o com Bagging, a predi√ß√£o do modelo √© uma m√©dia de resultados de modelos individuais, o que pode resultar em um limite de decis√£o mais suave.

### Pergunta Te√≥rica Avan√ßada: Qual a rela√ß√£o entre o vi√©s e a vari√¢ncia em modelos de regress√£o ap√≥s a aplica√ß√£o do Bagging?

**Resposta:**
O Bagging √© uma t√©cnica que visa principalmente reduzir a vari√¢ncia de um modelo, mantendo o vi√©s aproximadamente constante. Isso ocorre porque ao treinar m√∫ltiplos modelos em amostras bootstrap e agreg√°-los, os erros aleat√≥rios tendem a se cancelar, mas o padr√£o geral de previs√£o (vi√©s) n√£o √© alterado [^8.7]. Em modelos como √°rvores de decis√£o, que tendem a ter alta vari√¢ncia e baixo vi√©s, o Bagging √© muito eficaz. Em modelos lineares, com baixa vari√¢ncia, o efeito do Bagging na vari√¢ncia √© pequeno.
```mermaid
graph LR
    subgraph "Bias-Variance Tradeoff"
        direction TB
         A["Original Model"]
         B["Bagging"]
         C["Bias (Approximately Constant)"]
         D["Variance Reduction"]
         E["Improved Bias-Variance Tradeoff"]
         A --> B
        B --> C
        B --> D
        C & D --> E
    end
```

**Lemma 4: Decomposi√ß√£o do Erro em Vi√©s e Vari√¢ncia com Bagging**
O erro m√©dio quadr√°tico de um modelo de regress√£o pode ser decomposto em vi√©s e vari√¢ncia. Seja $f(x)$ o valor real da vari√°vel de resposta, e $f_{bag}(x)$ a previs√£o do modelo Bagging. O erro m√©dio quadr√°tico √©:

$$ E[(f(x) - f_{bag}(x))^2] = [E[f_{bag}(x)] - f(x)]^2 + E[(f_{bag}(x) - E[f_{bag}(x)])^2] $$
onde o primeiro termo √© o vi√©s ao quadrado e o segundo √© a vari√¢ncia. O Bagging reduz a vari√¢ncia, mas mant√©m o vi√©s aproximadamente constante [^8.7].
$\blacksquare$
```mermaid
graph LR
    subgraph "MSE Decomposition"
        direction TB
        A["MSE: E[(f(x) - f_bag(x))^2]"]
        B["Bias¬≤: (E[f_bag(x)] - f(x))¬≤"]
        C["Variance: E[(f_bag(x) - E[f_bag(x)])¬≤]"]
        A --> B
        A --> C
    end
```

**Corol√°rio 4: Impacto do Bagging no Trade-off Vi√©s-Vari√¢ncia**
Ao reduzir a vari√¢ncia, o Bagging melhora o trade-off vi√©s-vari√¢ncia, resultando em modelos com melhor capacidade de generaliza√ß√£o. Este efeito √© mais pronunciado em modelos com alta vari√¢ncia, como √°rvores de decis√£o [^8.7].
```mermaid
graph LR
    subgraph "Impact on Bias-Variance"
        direction TB
       A["High Variance Model"]
        B["Bagging Application"]
        C["Reduced Variance"]
        D["Improved Generalization"]
        A --> B
        B --> C
        C --> D
    end
```

> üí° **Exemplo Num√©rico:** Suponha um modelo com alta vari√¢ncia (como uma √°rvore de decis√£o profunda) que, em uma determinada previs√£o, tem um vi√©s de 0.1 e uma vari√¢ncia de 0.9, resultando em um erro m√©dio quadr√°tico de $0.1^2 + 0.9 = 0.91$. Ap√≥s aplicar o Bagging, o vi√©s permanece aproximadamente o mesmo (0.1), mas a vari√¢ncia √© reduzida, por exemplo, para 0.3. O novo erro m√©dio quadr√°tico seria de $0.1^2 + 0.3 = 0.31$. O bagging reduziu o erro ao reduzir a vari√¢ncia, enquanto o vi√©s permaneceu o mesmo.

> ‚ö†Ô∏è **Ponto Crucial**: O Bagging reduz o erro de um modelo de regress√£o, especialmente em modelos de alta vari√¢ncia, atuando na vari√¢ncia, e mantendo o vi√©s aproximadamente o mesmo, melhorando a sua capacidade de generaliza√ß√£o [^8.7].

### Conclus√£o
O Bagging √© uma t√©cnica vers√°til e poderosa para melhorar a performance de modelos de regress√£o, principalmente atrav√©s da redu√ß√£o da vari√¢ncia. Baseado em conceitos de amostragem bootstrap e agrega√ß√£o de modelos, o Bagging oferece uma maneira eficaz de construir modelos mais robustos e generaliz√°veis. Ao entender os fundamentos te√≥ricos e as propriedades do Bagging, √© poss√≠vel aplic√°-lo de forma otimizada em diversos problemas de regress√£o, tirando o m√°ximo proveito de seus benef√≠cios.

### Footnotes
[^8.1]: "For most of this book, the fitting (learning) of models has been achieved by minimizing a sum of squares for regression, or by minimizing cross-entropy for classification." *(Trecho de <P√°gina 261>)*
[^8.2.1]: "The bootstrap method provides a direct computational way of assessing uncertainty, by sampling from the training data." *(Trecho de <P√°gina 261>)*
[^8.2.2]: "It turns out that the parametric bootstrap agrees with least squares in the previous example because the model (8.5) has additive Gaussian errors. In general, the parametric bootstrap agrees not with least squares but with maximum likelihood, which we now review." *(Trecho de <P√°gina 265>)*
[^8.7]: "Earlier we introduced the bootstrap as a way of assessing the accuracy of a parameter estimate or a prediction. Here we show how to use the bootstrap to improve the estimate or prediction itself." *(Trecho de <P√°gina 282>)*
<!-- END DOCUMENT -->
