## Model Inference and Averaging: A Deep Dive into Statistical Learning Techniques

```mermaid
graph LR
    subgraph "Statistical Learning Overview"
    direction TB
        A["Data"] --> B["Model Fitting"]
        B --> C["Maximum Likelihood Estimation"]
        B --> D["Bayesian Methods"]
        C --> E["Model Evaluation"]
        D --> E
        E --> F["Bootstrap"]
        E --> G["Model Averaging"]
        E --> H["Model Improvement"]
     end
```

### IntroduÃ§Ã£o
Este capÃ­tulo explora em profundidade os mÃ©todos de inferÃªncia e modelagem estatÃ­stica, focando em abordagens tanto frequentistas quanto bayesianas. O fitting, ou aprendizado de modelos, usualmente envolve a minimizaÃ§Ã£o de alguma funÃ§Ã£o de erro, como a soma de quadrados para regressÃ£o ou a cross-entropy para classificaÃ§Ã£o. Esses procedimentos, na verdade, sÃ£o casos especÃ­ficos da abordagem de **Maximum Likelihood** [^8.1]. Adicionalmente, exploraremos tÃ©cnicas para avaliar a incerteza dos modelos usando o **Bootstrap** [^8.1] e outros mÃ©todos relacionados como model averaging e model improvement.

### Conceitos Fundamentais

**Conceito 1: Maximum Likelihood Estimation (MLE)**

A **Maximum Likelihood Estimation (MLE)** Ã© uma abordagem fundamental na estatÃ­stica para estimar os parÃ¢metros de um modelo a partir de dados observados. A ideia central Ã© encontrar os valores dos parÃ¢metros que maximizam a probabilidade dos dados terem sido gerados pelo modelo. Em essÃªncia, busca-se o modelo que melhor explica os dados observados, conforme mencionado em [^8.1]. Formalmente, dado um conjunto de dados $Z = \{z_1, z_2, \ldots, z_N\}$, onde cada $z_i$ Ã© uma observaÃ§Ã£o, e um modelo paramÃ©trico que descreve a distribuiÃ§Ã£o desses dados como $g_{\theta}(z)$, onde $\theta$ sÃ£o os parÃ¢metros do modelo, a funÃ§Ã£o de **likelihood** Ã© dada por:

$$L(\theta; Z) = \prod_{i=1}^{N} g_{\theta}(z_i)$$

O objetivo da MLE Ã© encontrar o valor de $\theta$ que maximiza essa funÃ§Ã£o de likelihood. Em muitos casos, Ã© mais fÃ¡cil trabalhar com o logaritmo da funÃ§Ã£o de likelihood, denominado log-likelihood:

$$l(\theta; Z) = \sum_{i=1}^{N} \log g_{\theta}(z_i)$$

A MLE Ã© um mÃ©todo bastante utilizado devido a suas boas propriedades assintÃ³ticas, como consistÃªncia e eficiÃªncia, sob certas condiÃ§Ãµes de regularidade [^8.5.2].

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Suponha que temos um conjunto de dados $Z$ de 5 amostras, $Z = \{2.1, 2.8, 3.5, 4.2, 4.9\}$, e queremos modelar esses dados usando uma distribuiÃ§Ã£o normal com mÃ©dia $\mu$ e desvio padrÃ£o $\sigma$. Assumimos que $\sigma = 1$ para simplificar, e queremos estimar $\mu$ usando MLE.
>
> A funÃ§Ã£o de densidade de probabilidade (PDF) para uma distribuiÃ§Ã£o normal Ã©:
>
> $$g_{\mu}(z_i) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(z_i - \mu)^2}{2\sigma^2}}$$
>
> Como $\sigma = 1$, a log-likelihood Ã©:
>
> $$l(\mu; Z) = \sum_{i=1}^{5} \log\left(\frac{1}{\sqrt{2\pi}} e^{-\frac{(z_i - \mu)^2}{2}}\right) = - \frac{5}{2}\log(2\pi) - \frac{1}{2}\sum_{i=1}^{5} (z_i - \mu)^2$$
>
> Para maximizar $l(\mu; Z)$, podemos minimizar a soma dos quadrados $\sum_{i=1}^{5} (z_i - \mu)^2$. A soluÃ§Ã£o para isso Ã© a mÃ©dia amostral:
>
> $$\hat{\mu} = \frac{1}{5} \sum_{i=1}^{5} z_i = \frac{2.1 + 2.8 + 3.5 + 4.2 + 4.9}{5} = 3.5$$
>
> Portanto, a estimativa de mÃ¡xima verossimilhanÃ§a para $\mu$ Ã© 3.5.
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> data = np.array([2.1, 2.8, 3.5, 4.2, 4.9])
> mu_mle = np.mean(data)
> print(f"MLE estimate of mu: {mu_mle}")
> ```

```mermaid
graph LR
    subgraph "Maximum Likelihood Estimation (MLE)"
        direction TB
        A["Observed Data: Z = {z1, z2, ..., zN}"]
        B["Parametric Model: g_Î¸(z)"]
        C["Likelihood Function: L(Î¸; Z) = Î  g_Î¸(zi)"]
        D["Log-Likelihood: l(Î¸; Z) = Î£ log g_Î¸(zi)"]
        E["Maximize l(Î¸; Z) to find Î¸_MLE"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```

**Lemma 1:** Sob condiÃ§Ãµes de regularidade, o estimador de mÃ¡xima verossimilhanÃ§a (MLE) Ã© consistente, ou seja, converge para o verdadeiro valor do parÃ¢metro Ã  medida que o tamanho da amostra aumenta.

*Prova:* Seja $\theta_0$ o verdadeiro valor do parÃ¢metro e $\hat{\theta}_N$ o estimador de mÃ¡xima verossimilhanÃ§a baseado em $N$ observaÃ§Ãµes. Sob condiÃ§Ãµes de regularidade, temos que:

$$\lim_{N\to\infty} P(|\hat{\theta}_N - \theta_0| > \epsilon) = 0$$

para qualquer $\epsilon > 0$. Isso significa que o estimador MLE se torna arbitrariamente prÃ³ximo do verdadeiro valor do parÃ¢metro Ã  medida que o tamanho da amostra $N$ tende ao infinito. $\blacksquare$

**Conceito 2: Bootstrap**

O **Bootstrap** Ã© um mÃ©todo de reamostragem computacional para avaliar a incerteza em estimativas estatÃ­sticas. Em vez de fazer suposiÃ§Ãµes teÃ³ricas sobre a distribuiÃ§Ã£o dos dados, o bootstrap simula o processo de amostragem utilizando os dados observados [^8.2.1]. Existem duas abordagens principais: **nonparametric bootstrap** e **parametric bootstrap**.

No nonparametric bootstrap, amostras sÃ£o geradas com reposiÃ§Ã£o a partir dos dados originais, criando conjuntos de dados sintÃ©ticos que imitam a variabilidade da amostra original [^8.2.1]. Para cada amostra bootstrap, a estimativa estatÃ­stica de interesse Ã© calculada, e a distribuiÃ§Ã£o dessas estimativas bootstrapped Ã© utilizada para avaliar a incerteza da estimativa original. O parametric bootstrap, por outro lado, envolve a geraÃ§Ã£o de dados a partir de um modelo paramÃ©trico, utilizando estimativas dos parÃ¢metros obtidas a partir dos dados originais [^8.2.2].

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considere os mesmos dados $Z = \{2.1, 2.8, 3.5, 4.2, 4.9\}$ do exemplo anterior. Vamos usar o bootstrap nÃ£o paramÃ©trico para estimar a incerteza da mÃ©dia amostral.
>
> 1. **Reamostragem:** Criamos, digamos, 1000 amostras bootstrap, cada uma com 5 pontos amostrados com reposiÃ§Ã£o de $Z$.
> 2. **Estimativa da mÃ©dia:** Calculamos a mÃ©dia amostral para cada amostra bootstrap.
> 3. **Incerteza:** Avaliamos a distribuiÃ§Ã£o das mÃ©dias bootstrapped para obter intervalos de confianÃ§a.
>
> ```python
> import numpy as np
>
> data = np.array([2.1, 2.8, 3.5, 4.2, 4.9])
> n_bootstrap = 1000
> bootstrap_means = []
>
> for _ in range(n_bootstrap):
>     bootstrap_sample = np.random.choice(data, size=len(data), replace=True)
>     bootstrap_means.append(np.mean(bootstrap_sample))
>
> bootstrap_means = np.array(bootstrap_means)
> confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])
> print(f"Bootstrap 95% confidence interval: {confidence_interval}")
> ```
>
> A saÃ­da mostrarÃ¡ um intervalo de confianÃ§a para a mÃ©dia, que quantifica a incerteza da estimativa.

```mermaid
graph LR
    subgraph "Nonparametric Bootstrap"
        direction TB
        A["Original Data: Z"]
        B["Resample with Replacement: Z*i"]
        C["Calculate Statistic on Each Sample: Î¸*_i = f(Z*i)"]
        D["Distribution of Î¸*i for uncertainty"]
        A --> B
        B --> C
        C --> D
    end
    subgraph "Parametric Bootstrap"
         direction TB
         A2["Original Data: Z"]
         B2["Estimate Parameters: Î¸_hat = MLE(Z)"]
         C2["Generate Samples from Model: Z*i ~ g(Î¸_hat)"]
         D2["Calculate Statistic on Each Sample: Î¸*_i = f(Z*i)"]
         E2["Distribution of Î¸*i for uncertainty"]
         A2 --> B2
         B2 --> C2
         C2 --> D2
         D2 --> E2
    end
```

**CorolÃ¡rio 1:** O bootstrap converge para a distribuiÃ§Ã£o assintÃ³tica da estatÃ­stica sob condiÃ§Ãµes de regularidade e conforme o nÃºmero de reamostragens aumenta.

*Prova:* Pelo Teorema de Glivenko-Cantelli, a distribuiÃ§Ã£o empÃ­rica converge para a distribuiÃ§Ã£o real Ã  medida que o tamanho da amostra aumenta. Assim, as amostras bootstrap tornam-se mais representativas da distribuiÃ§Ã£o real. AlÃ©m disso, pela continuidade da estatÃ­stica, a distribuiÃ§Ã£o das estimativas bootstrapped converge para a distribuiÃ§Ã£o assintÃ³tica da estatÃ­stica de interesse. $\blacksquare$

**Conceito 3: Bayesian Methods**

Os **MÃ©todos Bayesianos** fornecem uma abordagem diferente para a inferÃªncia estatÃ­stica, integrando conhecimento prÃ©vio (prior) sobre os parÃ¢metros com a informaÃ§Ã£o dos dados observados (likelihood), resultando em uma distribuiÃ§Ã£o posterior dos parÃ¢metros [^8.3]. Em contraste com os mÃ©todos frequentistas, que tratam os parÃ¢metros como fixos e desconhecidos, a abordagem bayesiana trata os parÃ¢metros como variÃ¡veis aleatÃ³rias que seguem uma distribuiÃ§Ã£o de probabilidade.

O teorema de Bayes fornece a base para a inferÃªncia bayesiana:

$$P(\theta|Z) = \frac{P(Z|\theta)P(\theta)}{\int P(Z|\theta)P(\theta)d\theta}$$

Onde:
-   $P(\theta|Z)$ Ã© a distribuiÃ§Ã£o posterior dos parÃ¢metros, dados os dados observados.
-   $P(Z|\theta)$ Ã© a likelihood, ou verossimilhanÃ§a, da observaÃ§Ã£o dos dados dado os parÃ¢metros.
-   $P(\theta)$ Ã© a distribuiÃ§Ã£o prior, expressando nosso conhecimento prÃ©vio sobre os parÃ¢metros antes da observaÃ§Ã£o dos dados.
-   A integral no denominador Ã© uma constante de normalizaÃ§Ã£o.

Os mÃ©todos bayesianos sÃ£o particularmente Ãºteis quando hÃ¡ informaÃ§Ã£o prÃ©via disponÃ­vel, e a incerteza dos parÃ¢metros pode ser explicitamente quantificada atravÃ©s da distribuiÃ§Ã£o posterior. [^8.3]

> âš ï¸ **Nota Importante**: A escolha da distribuiÃ§Ã£o prior Ã© crucial na inferÃªncia bayesiana e influencia os resultados.
> â— **Ponto de AtenÃ§Ã£o**: A distribuiÃ§Ã£o posterior representa nossa incerteza sobre os parÃ¢metros apÃ³s observar os dados.
> âœ”ï¸ **Destaque**: MÃ©todos bayesianos incorporam tanto dados quanto conhecimento prÃ©vio na inferÃªncia.

```mermaid
graph LR
    subgraph "Bayesian Inference"
        direction TB
        A["Prior Distribution: P(Î¸)"]
        B["Likelihood: P(Z|Î¸)"]
        C["Posterior Distribution: P(Î¸|Z)"]
        D["Bayes Theorem: P(Î¸|Z) = P(Z|Î¸)P(Î¸) / âˆ«P(Z|Î¸)P(Î¸)dÎ¸"]
        A --> D
        B --> D
        D --> C
    end
```

### RegressÃ£o Linear e MÃ­nimos Quadrados para ClassificaÃ§Ã£o

A regressÃ£o linear, embora originalmente concebida para problemas de regressÃ£o, pode ser adaptada para tarefas de classificaÃ§Ã£o, especialmente para problemas com duas classes ou mais. Na abordagem de **regressÃ£o linear para classificaÃ§Ã£o**, as classes sÃ£o codificadas usando uma **matriz indicadora** e, em seguida, um modelo linear Ã© ajustado aos dados usando o mÃ©todo dos mÃ­nimos quadrados.

A ideia central Ã© expressar a classe de um dado ponto como uma combinaÃ§Ã£o linear de suas caracterÃ­sticas (features). Por exemplo, em um problema com K classes, podemos criar uma matriz de indicadores Y de dimensÃ£o N x K, onde N Ã© o nÃºmero de amostras. Cada linha de Y corresponde a uma amostra, e cada coluna representa uma classe. Se a amostra i pertence Ã  classe k, entÃ£o $Y_{ik}$ = 1, e as outras entradas na linha i sÃ£o zero. Em seguida, um modelo linear Ã© ajustado, como:

$$\hat{Y} = X\beta$$

Onde X Ã© a matriz de features (design matrix), $\beta$ Ã© a matriz de coeficientes, e $\hat{Y}$ sÃ£o as classes preditas. As estimativas dos coeficientes $\beta$ sÃ£o calculadas usando o mÃ©todo dos mÃ­nimos quadrados, como em [^8.2]:

$$\hat{\beta} = (H^TH)^{-1}H^Ty$$

Onde $H$ Ã© a matriz de features. ApÃ³s obter as prediÃ§Ãµes $\hat{Y}$, a classe de cada amostra Ã© atribuÃ­da com base na coluna de $\hat{Y}$ com maior valor.

Entretanto, a abordagem de regressÃ£o linear para classificaÃ§Ã£o apresenta limitaÃ§Ãµes, especialmente quando se trata de problemas com mais de duas classes [^8.1]. Embora possa fornecer uma fronteira de decisÃ£o linear, nÃ£o garante que as prediÃ§Ãµes estarÃ£o entre 0 e 1, o que Ã© ideal para interpretar como probabilidades de classe. A regressÃ£o linear tambÃ©m nÃ£o Ã© inerentemente probabilÃ­stica e nÃ£o modela a variabilidade da prediÃ§Ã£o. AlÃ©m disso, a regressÃ£o linear tende a ser sensÃ­vel a outliers e pode levar a extrapolaÃ§Ãµes fora do intervalo [0,1]. O contexto [^8.2] menciona a estimativa do desvio padrÃ£o em modelos de regressÃ£o para anÃ¡lise de incerteza:

$$\text{Var}(\hat{\beta}) = (H^TH)^{-1}\hat{\sigma}^2$$

Onde $\hat{\sigma}^2$ Ã© a variÃ¢ncia dos erros.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considere um problema de classificaÃ§Ã£o binÃ¡ria com duas classes, representadas por 0 e 1. Temos os seguintes dados:
>
> | Feature (x) | Class (y) |
> |-------------|-----------|
> | 1           | 0         |
> | 2           | 0         |
> | 3           | 1         |
> | 4           | 1         |
> | 5           | 1         |
>
>  Nossa matriz de features $X$ e o vetor de classes $y$ sÃ£o:
>
> $$X = \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{bmatrix}, \quad y = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 1 \end{bmatrix}$$
> Para aplicar regressÃ£o linear, adicionamos um bias (intercepto) na matriz X.
>
> $$H = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix}$$
>
> Agora podemos calcular os coeficientes $\beta$ usando mÃ­nimos quadrados:
>
> $\text{Step 1: } H^TH = \begin{bmatrix} 5 & 15 \\ 15 & 55 \end{bmatrix}$
>
> $\text{Step 2: } (H^TH)^{-1} = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix}$
>
> $\text{Step 3: } H^Ty = \begin{bmatrix} 3 \\ 12 \end{bmatrix}$
>
> $\text{Step 4: } \hat{\beta} = (H^TH)^{-1}H^Ty = \begin{bmatrix} 1.1 & -0.3 \\ -0.3 & 0.1 \end{bmatrix} \begin{bmatrix} 3 \\ 12 \end{bmatrix} = \begin{bmatrix} -0.3 \\ 0.3 \end{bmatrix}$
>
> Nosso modelo linear Ã© entÃ£o $\hat{y} = -0.3 + 0.3x$.
>
> Para classificar um novo ponto, digamos $x=2.5$, calculamos $\hat{y} = -0.3 + 0.3*2.5 = 0.45$. Como este valor estÃ¡ mais prÃ³ximo de 0 que de 1, predirÃ­amos a classe 0.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> X = np.array([[1], [2], [3], [4], [5]])
> y = np.array([0, 0, 1, 1, 1])
>
> model = LinearRegression()
> model.fit(X, y)
>
> H = np.c_[np.ones(X.shape[0]), X]
> beta = np.linalg.inv(H.T @ H) @ H.T @ y
>
> x_new = np.array([[2.5]])
> y_pred = model.predict(x_new)
>
> y_pred_manual = beta[0] + beta[1]*x_new
>
> print(f"Beta coefficients (sklearn): {model.intercept_}, {model.coef_}")
> print(f"Beta coefficients (manual): {beta}")
> print(f"Predicted value for x=2.5 (sklearn): {y_pred}")
> print(f"Predicted value for x=2.5 (manual): {y_pred_manual}")
> ```

```mermaid
graph LR
    subgraph "Linear Regression for Classification"
        direction TB
        A["Input Features: X"]
        B["Indicator Matrix: Y"]
        C["Design Matrix: H"]
        D["Parameter Estimation: Î²Ì‚ = (H^TH)^-1 H^Ty"]
        E["Predictions: Å¶ = XÎ²Ì‚"]
        F["Class Assignment based on max(Å¶)"]
        A --> B
        B --> C
        C --> D
        D --> E
        E --> F
    end
```

**Lemma 2:** Se as classes forem separÃ¡veis por um hiperplano, a soluÃ§Ã£o por mÃ­nimos quadrados da regressÃ£o de matriz indicadora irÃ¡ convergir para o hiperplano de separaÃ§Ã£o.

*Prova:* Suponha que as classes possam ser separadas por um hiperplano definido por $w^Tx + b = 0$. Ao aplicar regressÃ£o linear com uma codificaÃ§Ã£o indicadora para cada classe, os coeficientes $\hat{\beta}$ vÃ£o convergir para uma projeÃ§Ã£o dos dados em um espaÃ§o onde as classes sÃ£o separÃ¡veis, ou seja, o hiperplano definido por $w^Tx + b = 0$ [^8.2]. $\blacksquare$

**CorolÃ¡rio 2:** Em um problema de classificaÃ§Ã£o binÃ¡ria, a regressÃ£o linear com matriz indicadora pode levar a decisÃµes similares a uma anÃ¡lise discriminante linear, desde que a separabilidade das classes seja boa.

*Prova:* Em um problema de classificaÃ§Ã£o binÃ¡ria, a projeÃ§Ã£o dos dados no espaÃ§o discriminante obtido pela regressÃ£o linear Ã© tal que os pontos pertencentes a diferentes classes ficam o mais separados possÃ­vel, de forma anÃ¡loga ao que acontece em LDA [^8.2, 8.3]. $\blacksquare$

> "Em alguns cenÃ¡rios, conforme apontado em [^8.3], a regressÃ£o logÃ­stica pode fornecer estimativas mais estÃ¡veis de probabilidade, enquanto a regressÃ£o de indicadores pode levar a extrapolaÃ§Ãµes fora de [0,1]."
>
> â€œNo entanto, hÃ¡ situaÃ§Ãµes em que a regressÃ£o de indicadores, de acordo com [^8.2], Ã© suficiente e atÃ© mesmo vantajosa quando o objetivo principal Ã© a fronteira de decisÃ£o linear.â€

### MÃ©todos de SeleÃ§Ã£o de VariÃ¡veis e RegularizaÃ§Ã£o em ClassificaÃ§Ã£o
```mermaid
graph LR
    subgraph "Regularization Techniques"
        direction TB
        A["Original Cost Function"]
        B["L1 Regularization (Lasso): Î»Î£|Î²j|"]
        C["L2 Regularization (Ridge): Î»Î£Î²jÂ²"]
        D["Elastic Net: Î»1Î£|Î²j| + Î»2Î£Î²jÂ²"]
        E["Regularized Cost Function"]
        A --> B
        A --> C
        B --> D
        C --> D
        B --> E
        C --> E
        D --> E
    end
```

A seleÃ§Ã£o de variÃ¡veis e a regularizaÃ§Ã£o sÃ£o tÃ©cnicas cruciais em modelos de classificaÃ§Ã£o para evitar overfitting e melhorar a generalizaÃ§Ã£o do modelo. Modelos com muitas variÃ¡veis podem se ajustar bem aos dados de treinamento, mas podem ter um desempenho ruim em dados nÃ£o vistos. A regularizaÃ§Ã£o impÃµe restriÃ§Ãµes aos parÃ¢metros do modelo, promovendo modelos mais simples e estÃ¡veis.

A regularizaÃ§Ã£o L1 (Lasso) adiciona um termo de penalidade que Ã© a soma dos valores absolutos dos coeficientes ao custo do modelo. Isso tende a empurrar alguns coeficientes para zero, promovendo esparsidade e seleÃ§Ã£o de variÃ¡veis [^8.2, 8.4]. Formalmente, a funÃ§Ã£o custo Ã© modificada para incluir o termo de penalidade L1:

$$J(\beta) =  \text{custo original} + \lambda \sum_{j=1}^{p} |\beta_j|$$

Onde $\lambda$ Ã© um hiperparÃ¢metro que controla a forÃ§a da regularizaÃ§Ã£o e $p$ Ã© o nÃºmero de variÃ¡veis.

A regularizaÃ§Ã£o L2 (Ridge) adiciona um termo de penalidade que Ã© a soma dos quadrados dos coeficientes ao custo do modelo. Isso tende a reduzir a magnitude dos coeficientes, tornando o modelo menos sensÃ­vel a variaÃ§Ãµes nos dados e melhorando a estabilidade [^8.5]. A funÃ§Ã£o custo Ã© modificada da seguinte forma:

$$J(\beta) = \text{custo original} + \lambda \sum_{j=1}^{p} \beta_j^2$$

A regularizaÃ§Ã£o Elastic Net combina as penalidades L1 e L2, buscando um equilÃ­brio entre esparsidade e estabilidade. Ela Ã© dada por:

$$J(\beta) = \text{custo original} + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2$$

Essas tÃ©cnicas de regularizaÃ§Ã£o sÃ£o comumente utilizadas em modelos de regressÃ£o logÃ­stica para melhorar a performance e a interpretabilidade. Como mencionado no tÃ³pico [^8.4.4], o modelo da regressÃ£o logÃ­stica com regularizaÃ§Ã£o Ã© dado por:

$$ \text{logit}(p(x)) = \beta_0 + \sum_{i=1}^{p} \beta_i x_i + \text{penalidade}(\beta)$$

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Suponha que temos um problema de regressÃ£o logÃ­stica com duas features $x_1$ e $x_2$. O modelo original (sem regularizaÃ§Ã£o) Ã©:
>
> $$\text{logit}(p(x)) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$$
>
> Suponha que os coeficientes estimados via MLE sem regularizaÃ§Ã£o foram $\beta_0 = 0.5$, $\beta_1 = 1.2$ e $\beta_2 = -0.8$.
>
> Agora, vamos aplicar regularizaÃ§Ã£o L1 (Lasso) com $\lambda = 0.5$. A funÃ§Ã£o de custo a ser minimizada torna-se:
>
> $$J(\beta) = \text{custo original} + 0.5(|\beta_1| + |\beta_2|)$$
>
> ApÃ³s a otimizaÃ§Ã£o com L1, supomos que os novos coeficientes sÃ£o $\beta_0 = 0.4$, $\beta_1 = 0.9$ e $\beta_2 = 0$. Note que o $\beta_2$ foi zerado, indicando que essa feature Ã© menos importante para o modelo.
>
> Para regularizaÃ§Ã£o L2 (Ridge) com $\lambda = 0.5$, a funÃ§Ã£o de custo torna-se:
>
> $$J(\beta) = \text{custo original} + 0.5(\beta_1^2 + \beta_2^2)$$
>
>  ApÃ³s a otimizaÃ§Ã£o com L2, supomos que os novos coeficientes sÃ£o $\beta_0 = 0.45$, $\beta_1 = 1.0$ e $\beta_2 = -0.6$. Note que os valores dos coeficientes foram reduzidos em comparaÃ§Ã£o com o modelo sem regularizaÃ§Ã£o, mas nenhum deles foi zerado.
>
> Podemos tambÃ©m usar Elastic Net com $\lambda_1 = 0.25$ e $\lambda_2 = 0.25$:
>
> $$J(\beta) = \text{custo original} + 0.25(|\beta_1| + |\beta_2|) + 0.25(\beta_1^2 + \beta_2^2)$$
>
> ApÃ³s a otimizaÃ§Ã£o com Elastic Net, supomos que os novos coeficientes sÃ£o $\beta_0 = 0.42$, $\beta_1 = 0.85$ e $\beta_2 = -0.1$. O Elastic Net busca um compromisso entre a esparsidade do Lasso e a estabilidade do Ridge.
>
>
> | MÃ©todo       | $\beta_0$ | $\beta_1$ | $\beta_2$ |
> |--------------|-----------|-----------|-----------|
> | Sem Reg.     | 0.5       | 1.2       | -0.8      |
> | Lasso        | 0.4       | 0.9       | 0         |
> | Ridge        | 0.45      | 1.0       | -0.6      |
> | Elastic Net | 0.42      | 0.85      | -0.1     |
>
>
> ```python
> import numpy as np
> from sklearn.linear_model import LogisticRegression
> from sklearn.preprocessing import StandardScaler
> from sklearn.pipeline import Pipeline
>
> # Generate some sample data
> np.random.seed(0)
> X = np.random.randn(100, 2)
> y = (X[:, 0] + X[:, 1] > 0).astype(int)
>
> # Logistic Regression with no regularization
> model_no_reg = Pipeline([('scaler', StandardScaler()), ('logreg', LogisticRegression(penalty=None))])
> model_no_reg.fit(X, y)
>
> # Logistic Regression with L1 (Lasso) regularization
> model_l1 = Pipeline([('scaler', StandardScaler()), ('logreg', LogisticRegression(penalty='l1', solver='liblinear', C=1))])
> model_l1.fit(X, y)
>
> # Logistic Regression with L2 (Ridge) regularization
> model_l2 = Pipeline([('scaler', StandardScaler()), ('logreg', LogisticRegression(penalty='l2', C=1))])
> model_l2.fit(X, y)
>
> # Logistic Regression with Elastic Net regularization
> model_elastic = Pipeline([('scaler', StandardScaler()), ('logreg', LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1))])
> model_elastic.fit(X, y)
>
> print(f"No Reg - Intercept: {model_no_reg.named_steps['logreg'].intercept_}, Coefficients: {model_no_reg.named_steps['logreg'].coef_}")
> print(f"L1 Reg - Intercept: {model_l1.named_steps['logreg'].intercept_}, Coefficients: {model_l1.named_steps['logreg'].coef_}")
> print(f"L2 Reg - Intercept: {model_l2.named_steps['logreg'].intercept_}, Coefficients: {model_l2.named_steps['logreg'].coef_}")
> print(f"Elastic Net - Intercept: {model_elastic.named_steps['logreg'].intercept_}, Coefficients: {model_elastic.named_steps['logreg'].coef_}")
> ```

**Lemma 3:** A penalizaÃ§Ã£o L1 em modelos de classificaÃ§Ã£o logÃ­stica resulta em soluÃ§Ãµes esparsas, onde vÃ¡rios coeficientes sÃ£o exatamente zero.

*Prova:* A penalizaÃ§Ã£o L1 adiciona um termo de valor absoluto aos coeficientes na funÃ§Ã£o de custo, resultando em uma otimizaÃ§Ã£o que favorece soluÃ§Ãµes onde muitos coeficientes sÃ£o iguais a zero, promovendo assim a esparsidade [^8.4.4]. $\blacksquare$

**CorolÃ¡rio 3:** A esparsidade induzida pela regularizaÃ§Ã£o L1 em modelos de classificaÃ§Ã£o melhora a interpretabilidade do modelo, destacando as variÃ¡veis mais relevantes para a classificaÃ§Ã£o.

*Prova:* Com coeficientes iguais a zero, apenas algumas features contribuem para a decisÃ£o, facilitando a interpretaÃ§Ã£o das variÃ¡veis mais importantes [^8.4.5]. $\blacksquare$

> âš ï¸ **Ponto Crucial**: L1 e L2 podem ser combinadas (Elastic Net) para aproveitar vantagens de ambos os tipos de regularizaÃ§Ã£o.

### Separating Hyperplanes e Perceptrons

O conceito de **separating hyperplanes** surge em problemas de classificaÃ§Ã£o quando se busca uma fronteira linear que divide as diferentes classes no espaÃ§o de features. Um **hiperplano** Ã© uma generalizaÃ§Ã£o de uma linha em duas dimensÃµes e um plano em trÃªs dimensÃµes, para espaÃ§os de dimensÃµes arbitrÃ¡rias. A formulaÃ§Ã£o matemÃ¡tica de um hiperplano separador Ã© dada por:

$$w^T x + b = 0$$

Onde $w$ Ã© o vetor normal ao hiperplano, $x$ Ã© um vetor de features, e $b$ Ã© um bias ou termo de deslocamento. O hiperplano divide o espaÃ§o de features em duas regiÃµes: uma onde $w^T x + b > 0$ e outra onde $w^T x + b < 0$.

A **margem** de um hiperplano Ã© a distÃ¢ncia entre o hiperplano e os pontos de dados mais prÃ³ximos de cada classe. O conceito de maximizar a margem de separaÃ§Ã£o leva ao conceito de hiperplanos Ã³timos, que buscam maximizar a distÃ¢ncia entre as classes e o hiperplano decisor. A formulaÃ§Ã£o do problema de otimizaÃ§Ã£o para encontrar o hiperplano Ã³timo geralmente envolve o uso de **duais de Wolfe** [^8.5.2]. Os pontos que determinam a posiÃ§Ã£o e a orientaÃ§Ã£o do hiperplano sÃ£o chamados de **support vectors**.

The **Perceptron**, proposed by Rosenblatt, is a supervised learning algorithm that aims to find a hyperplane that separates different classes in a classification problem. The perceptron adjusts the weights of the linear combination of features based on classification errors. If the dataset is linearly separable, the perceptron algorithm converges to a solution that separates the classes [^8.5.1]. In mathematical terms, the weight update in the perceptron can be expressed as:

$$w_{t+1} = w_t + \alpha (y_i - \hat{y}_i)x_i$$

onde $w_t$ Ã© o peso no instante $t$, $\alpha$ Ã© a taxa de aprendizagem, $y_i$ Ã© o valor verdadeiro da classe, $\hat{y}_i$ Ã© a prediÃ§Ã£o do perceptron, e $x_i$ sÃ£o as features.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Vamos considerar um problema de classificaÃ§Ã£o com duas classes e duas features, onde os dados sÃ£o linearmente separÃ¡veis.
>
> Dados:
> - Classe 1:  $x_1 = (1,1), x_2 = (2,1)$
> - Classe 2:  $x_3 = (1,3), x_4 = (2,3)$
>
> Inicializamos os pesos $w = (0,0)$ e o bias $b = 0$ e taxa de aprendizagem $\alpha = 0.1$.
>
> **IteraÃ§Ã£o 1:**
> - Amostra $x_1$: $\hat{y_1} = \text{sign}(w^T x_1 + b) = \text{sign}(0) = 0$. Erro: $y_1 - \hat{y_1} = 1 - 0 = 1$.
> - AtualizaÃ§Ã£o: $w = (0,0) + 0.1 * 1 * (1,1) = (0.1, 0.1)$ e $b = 0 + 0.1*1 = 0.1$.
>
> **IteraÃ§Ã£o 2:**
> - Amostra $x_2$: $\hat{y_2} = \text{sign}(w^T x_2 + b) = \text{sign}(0.1*2+0.1*1+0.1) = \text{sign}(0.4) = 1$. Erro: $y_2 - \hat{y_2} = 1 - 1 = 0$.
> - AtualizaÃ§Ã£o: $w = (0.1, 0.1)$, $b = 0.1$.
>
> **IteraÃ§Ã£o 3:**
> - Amostra $x_3$: $\hat{y_3} = \text{sign}(w^T x_3 + b) = \text{sign}(0.1*1+0.1*3+0.1) = \text{sign}(0.5) = 1$. Erro: $y_3 - \hat{y_3} = -1 - 1 = -2$.
> - AtualizaÃ§Ã£o: $w = (0.1, 0.1) + 0.1 * -2 * (1,3) = (-0.1, -0.5)$, $b = 0.1 + 0.1*(-2) = -0.1$.
>
>  ... ApÃ³s vÃ¡rias iteraÃ§Ãµes, o perceptron irÃ¡ convergir para um hiperplano separador. Note que a representaÃ§Ã£o das classes Ã© -1 e 1, onde anteriormente era 0 e 1.
>
> ```python
> import numpy as np
>
> def perceptron_update(w, b, x, y, alpha):
>    y_hat = np.sign(np.dot(w,x) + b)
>    w_new = w + alpha * (y - y_hat) * x
>    b_new = b + alpha * (y - y_hat)
>    return w_new, b_new
>
> def perceptron(X, y, alpha, iterations):
>    w = np.zeros(X.shape[1])
>    b = 0
>    for _ in range(iterations):
>        for i in range(X.shape[0]):
>            w, b = perceptron_update(w, b, X[i], y[i], alpha)
>    return w, b
>
> X = np.array([[1, 1], [2, 1], [1, 3], [2, 3]])
>