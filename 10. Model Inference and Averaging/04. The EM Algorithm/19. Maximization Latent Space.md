## Maximization over Latent Space
<imagem: Mapa mental que conecta os m√©todos de infer√™ncia (Maximum Likelihood e Bayesian), abordagens de regulariza√ß√£o e o uso de modelos com vari√°veis latentes. As setas direcionais mostram as rela√ß√µes entre esses t√≥picos, como a utiliza√ß√£o do EM algorithm para problemas com vari√°veis latentes.>

### Introdu√ß√£o
Este cap√≠tulo explora m√©todos de infer√™ncia e modelagem que v√£o al√©m da minimiza√ß√£o de erros quadr√°ticos ou entropia cruzada, focando na **abordagem de Maximum Likelihood (ML)** e no m√©todo Bayesiano de infer√™ncia [^8.1]. Al√©m disso, investigamos t√©cnicas de model averaging e melhoria de modelos, incluindo m√©todos comit√™, bagging, stacking e bumping [^8.1]. Particularmente, a discuss√£o sobre **modelos com vari√°veis latentes** e a **maximiza√ß√£o sobre o espa√ßo latente** (Maximization over Latent Space) atrav√©s do **EM Algorithm** se destacam como m√©todos cruciais para problemas complexos de modelagem.

### Conceitos Fundamentais
**Conceito 1:** O conceito fundamental da **abordagem de Maximum Likelihood (ML)** reside na escolha dos par√¢metros de um modelo que maximizam a probabilidade dos dados observados [^8.1]. Formalmente, dado um conjunto de dados $Z = \{z_1, z_2, \ldots, z_N\}$, onde cada $z_i$ segue uma distribui√ß√£o $g_{\theta}(z)$, a fun√ß√£o de verossimilhan√ßa $L(\theta; Z)$ √© definida como o produto das probabilidades dos dados sob os par√¢metros $\theta$:
```mermaid
graph LR
    subgraph "Maximum Likelihood Formulation"
        direction TB
        A["Likelihood Function: L(Œ∏; Z)"] --> B["Product of Probability Densities: L(Œ∏; Z) = ‚àè·µ¢ g_Œ∏(z·µ¢)"]
        B --> C["Objective: Maximize L(Œ∏; Z) with respect to Œ∏"]
        C --> D["Log-Likelihood Transformation: l(Œ∏; Z) = log(L(Œ∏; Z)) = Œ£·µ¢ log(g_Œ∏(z·µ¢))"]
        D --> E["Objective: Maximize l(Œ∏; Z) with respect to Œ∏"]
    end
```

$$
L(\theta; Z) = \prod_{i=1}^{N} g_{\theta}(z_i)
$$
O objetivo √© encontrar o valor de $\theta = \hat{\theta}$ que maximiza $L(\theta; Z)$ ou, equivalentemente, o log-verossimilhan√ßa $l(\theta; Z) = \log(L(\theta; Z))$ [^8.2.2]:

$$
l(\theta; Z) = \sum_{i=1}^{N} \log g_{\theta}(z_i)
$$
> üí° **Exemplo Num√©rico:** Considere um conjunto de dados $Z$ de 3 observa√ß√µes, $z_1 = 2$, $z_2 = 3$, e $z_3 = 4$, que seguem uma distribui√ß√£o normal com m√©dia $\mu$ (o par√¢metro $\theta$) e desvio padr√£o $\sigma = 1$. A fun√ß√£o de densidade de probabilidade normal √© dada por $g_{\mu}(z_i) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(z_i-\mu)^2}{2}}$. A fun√ß√£o de log-verossimilhan√ßa para este conjunto de dados √©:
>
> $$
> l(\mu; Z) = \sum_{i=1}^{3} \log \left(\frac{1}{\sqrt{2\pi}} e^{-\frac{(z_i-\mu)^2}{2}}\right) = - \frac{3}{2}\log(2\pi) - \frac{1}{2}\sum_{i=1}^{3} (z_i-\mu)^2
> $$
>
> Para encontrar o valor de $\mu$ que maximiza a log-verossimilhan√ßa, podemos calcular a derivada em rela√ß√£o a $\mu$, igualar a zero, e resolver para $\mu$. O estimador de m√°xima verossimilhan√ßa para a m√©dia de uma distribui√ß√£o normal √© a m√©dia amostral. Nesse caso, $\hat{\mu} = \frac{2+3+4}{3} = 3$.
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> data = np.array([2, 3, 4])
> mu_ml = np.mean(data)
> print(f"Estimativa de M√°xima Verossimilhan√ßa para mu: {mu_ml}")
>
> # Verifica√ß√£o da verossimilhan√ßa para alguns valores
> def likelihood(mu, data):
>     return np.prod(norm.pdf(data, loc=mu, scale=1))
>
> mu_values = np.linspace(1, 5, 100)
> likelihood_values = [likelihood(mu, data) for mu in mu_values]
>
> import matplotlib.pyplot as plt
> plt.plot(mu_values, likelihood_values)
> plt.xlabel("mu")
> plt.ylabel("Verossimilhan√ßa")
> plt.title("Verossimilhan√ßa em fun√ß√£o de mu")
> plt.scatter(mu_ml, likelihood(mu_ml, data), color='red', label=f"M√°x em {mu_ml:.2f}")
> plt.legend()
> plt.show()
> ```

**Lemma 1:** Sob condi√ß√µes de regularidade, o estimador de m√°xima verossimilhan√ßa $\hat{\theta}$ converge para a distribui√ß√£o normal, com m√©dia igual ao verdadeiro valor dos par√¢metros $\theta_0$ e vari√¢ncia dada pela inversa da matriz de informa√ß√£o de Fisher $i(\theta_0)^{-1}$ [^8.2.2]:
```mermaid
graph LR
    subgraph "Asymptotic Property of MLE"
    direction TB
        A["MLE Estimator Convergence: ùõ©ÃÇ"] --> B["Converges to Normal Distribution: N(ùõ©‚ÇÄ, i(ùõ©‚ÇÄ)‚Åª¬π)"]
        B --> C["Mean of Normal: True Parameter ùõ©‚ÇÄ"]
        B --> D["Variance of Normal: Inverse of Fisher Information i(ùõ©‚ÇÄ)‚Åª¬π"]
         D --> E["Fisher Information: i(ùõ©‚ÇÄ) = E[I(ùõ©‚ÇÄ)]"]
          E --> F["Observed Information Matrix I(ùõ©) = -Œ£·µ¢ ‚àÇ¬≤l(Œ∏; z·µ¢) / ‚àÇŒ∏‚àÇŒ∏·µÄ"]
    end
```

$$
\hat{\theta} \rightarrow N(\theta_0, i(\theta_0)^{-1})
$$
onde $i(\theta_0) = E[I(\theta_0)]$ √© a matriz de informa√ß√£o de Fisher [^8.2.2].  A matriz de informa√ß√£o $I(\theta)$ √© definida como:

$$
I(\theta) = - \sum_{i=1}^{N} \frac{\partial^2 l(\theta; z_i)}{\partial \theta \partial \theta^T}
$$
Essa converg√™ncia garante que, para amostras grandes, podemos ter estimativas de incerteza para os par√¢metros utilizando as derivadas da log-verossimilhan√ßa [^8.2.2]. $\blacksquare$

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior com dados normais, vamos calcular a matriz de informa√ß√£o de Fisher para estimar a incerteza de $\hat{\mu}$. A log-verossimilhan√ßa de uma √∫nica observa√ß√£o $z_i$ √©:
>
> $$
> l(\mu; z_i) = -\frac{1}{2}\log(2\pi) - \frac{(z_i-\mu)^2}{2}
> $$
>
> A primeira derivada em rela√ß√£o a $\mu$ √©:
>
> $$
> \frac{\partial l(\mu; z_i)}{\partial \mu} = (z_i - \mu)
> $$
>
> E a segunda derivada √©:
>
> $$
> \frac{\partial^2 l(\mu; z_i)}{\partial \mu^2} = -1
> $$
>
> Portanto, a matriz de informa√ß√£o de Fisher (que, neste caso, √© apenas um escalar) √©:
>
> $$
> I(\mu) = - \sum_{i=1}^{N} (-1) = N
> $$
>
> Para os nossos 3 dados, $I(\mu) = 3$. A vari√¢ncia do estimador $\hat{\mu}$ √© a inversa da informa√ß√£o de Fisher: $\text{Var}(\hat{\mu}) = I(\mu)^{-1} = \frac{1}{3}$.  Isso significa que, √† medida que mais dados s√£o observados, a vari√¢ncia da estimativa diminui. Usando o Teorema do Limite Central, sabemos que $\hat{\mu}$ se aproxima de uma distribui√ß√£o normal com m√©dia $\mu_0$ e vari√¢ncia $\frac{1}{3}$, ou seja, $\hat{\mu} \sim N(\mu_0, \frac{1}{3})$.
>
>  ```python
> import numpy as np
>
> data = np.array([2, 3, 4])
> n = len(data)
> fisher_information = n
> variance_mu_hat = 1 / fisher_information
> print(f"Matriz de informa√ß√£o de Fisher: {fisher_information}")
> print(f"Vari√¢ncia do estimador de m√°xima verossimilhan√ßa (mu_hat): {variance_mu_hat}")
> ```

**Conceito 2:** A **Linear Discriminant Analysis (LDA)**, embora n√£o explicitamente abordada no texto, pode ser entendida como um caso espec√≠fico de maximiza√ß√£o de verossimilhan√ßa sob a suposi√ß√£o de normalidade das classes [^8.1]. Ao ajustar um modelo linear para discriminar entre classes, estamos implicitamente maximizando a probabilidade dos dados pertencerem √†s classes corretas [^8.1]. O LDA assume que as classes t√™m m√©dias diferentes, mas compartilham a mesma matriz de covari√¢ncia, o que simplifica a infer√™ncia e a classifica√ß√£o [^8.1].

**Corol√°rio 1:** No contexto da LDA, a fun√ß√£o discriminante linear surge como uma consequ√™ncia da aplica√ß√£o do princ√≠pio de m√°xima verossimilhan√ßa sob as suposi√ß√µes de distribui√ß√£o normal das classes. A fronteira de decis√£o entre classes √© definida pelo hiperplano onde as probabilidades a posteriori se igualam [^8.1].

**Conceito 3:** O **m√©todo Bayesiano de infer√™ncia** [^8.3] contrasta com a abordagem ML ao introduzir uma distribui√ß√£o a priori para os par√¢metros $\theta$, denotada por $Pr(\theta)$. O objetivo √© obter a distribui√ß√£o a posteriori $Pr(\theta|Z)$, que combina a informa√ß√£o dos dados $Z$ com o conhecimento pr√©vio sobre os par√¢metros:
```mermaid
graph LR
    subgraph "Bayesian Inference Process"
        direction TB
        A["Prior Distribution: Pr(Œ∏)"] --> B["Likelihood Function: Pr(Z|Œ∏)"]
        B --> C["Posterior Distribution Calculation: Pr(Œ∏|Z) = Pr(Z|Œ∏)Pr(Œ∏)/‚à´Pr(Z|Œ∏)Pr(Œ∏)dŒ∏"]
          C --> D["Predictive Distribution Calculation: Pr(z_new|Z) = ‚à´ Pr(z_new|Œ∏)Pr(Œ∏|Z)dŒ∏"]
    end
```

$$
Pr(\theta|Z) = \frac{Pr(Z|\theta) \cdot Pr(\theta)}{\int Pr(Z|\theta) \cdot Pr(\theta) d\theta}
$$

A distribui√ß√£o preditiva Bayesiana $Pr(z_{new}|Z)$ [^8.3] para um novo dado $z_{new}$ √© obtida integrando sobre todas as poss√≠veis realiza√ß√µes dos par√¢metros $\theta$:
$$
Pr(z_{new}|Z) = \int Pr(z_{new}|\theta) \cdot Pr(\theta|Z) d\theta
$$
Esta abordagem permite expressar a incerteza tanto antes (prior) quanto depois de observar os dados (posterior) [^8.3].
> ‚ö†Ô∏è **Nota Importante**: Diferente do ML, a infer√™ncia Bayesiana usa uma distribui√ß√£o *a priori* para os par√¢metros, atualizando-a com os dados para obter a distribui√ß√£o *a posteriori*.  [^8.3]
> ‚ùó **Ponto de Aten√ß√£o**: A distribui√ß√£o preditiva Bayesiana incorpora a incerteza na estimativa dos par√¢metros $\theta$, o que n√£o acontece na abordagem ML.  [^8.3]
> ‚úîÔ∏è **Destaque**: O m√©todo Bayesiano oferece uma forma natural de incorporar conhecimento pr√©vio e lidar com incertezas, enquanto o ML busca o melhor ajuste pontual aos dados.  [^8.3]
> üí° **Exemplo Num√©rico:** Suponha que queremos estimar a probabilidade de sucesso de um evento (par√¢metro $\theta$) usando uma abordagem Bayesiana. Nossa *a priori* para $\theta$ √© uma distribui√ß√£o beta com par√¢metros $\alpha = 2$ e $\beta = 5$, o que indica que inicialmente acreditamos que a probabilidade de sucesso √© baixa. Ap√≥s observarmos 3 sucessos em 10 tentativas (dados $Z$), a verossimilhan√ßa √© uma distribui√ß√£o binomial. A *a posteriori* tamb√©m ser√° uma distribui√ß√£o beta, com par√¢metros atualizados.
>
> A *a priori* √© $Pr(\theta) = \text{Beta}(\alpha=2, \beta=5)$. A verossimilhan√ßa √© $Pr(Z|\theta) = \binom{10}{3} \theta^3 (1-\theta)^7$. A *a posteriori* √© proporcional a $Pr(Z|\theta) Pr(\theta)$, resultando em uma $\text{Beta}(\alpha + \text{sucessos}, \beta + \text{fracassos}) = \text{Beta}(2 + 3, 5 + 7) = \text{Beta}(5, 12)$.
>
>  ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from scipy.stats import beta
>
> # Prior
> alpha_prior = 2
> beta_prior = 5
>
> # Data
> successes = 3
> failures = 7
>
> # Posterior
> alpha_posterior = alpha_prior + successes
> beta_posterior = beta_prior + failures
>
> # Plotting
> theta_values = np.linspace(0, 1, 100)
> prior_values = beta.pdf(theta_values, alpha_prior, beta_prior)
> posterior_values = beta.pdf(theta_values, alpha_posterior, beta_posterior)
>
> plt.plot(theta_values, prior_values, label='Prior Beta(2, 5)')
> plt.plot(theta_values, posterior_values, label='Posterior Beta(5, 12)')
> plt.xlabel('Probabilidade de Sucesso (theta)')
> plt.ylabel('Densidade')
> plt.title('Prior e Posterior para Probabilidade de Sucesso')
> plt.legend()
> plt.show()
>
> # Media a posteriori
> mean_posterior = alpha_posterior / (alpha_posterior + beta_posterior)
> print(f"M√©dia a posteriori: {mean_posterior:.2f}")
> ```

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o
<imagem: Diagrama de fluxo detalhado que representa o processo de regress√£o de indicadores para classifica√ß√£o. O diagrama deve mostrar a codifica√ß√£o das classes, o c√°lculo dos coeficientes via m√≠nimos quadrados, a aplica√ß√£o da regra de decis√£o e a compara√ß√£o com m√©todos probabil√≠sticos. Use a linguagem Mermaid se poss√≠vel.>
```mermaid
graph TD
    A["Input Data: X, y"] --> B["Encode Classes: Indicator Matrix Y"];
    B --> C["Linear Regression: Y ~ XŒ≤"];
    C --> D["Estimate Coefficients: Œ≤ = (X'X)^-1 X'Y"];
    D --> E["Predict Classes: ≈∑ = XŒ≤"];
    E --> F["Decision Rule: Assign to class with highest score"];
    F --> G["Compare Results: with Probabilistic Classification Methods"];

    style A fill:#f0f0f0,stroke:#333,stroke-width:2px
    style B fill:#e6f7ff,stroke:#388e3c,stroke-width:2px
    style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style D fill:#e8f5e9,stroke:#1976d2,stroke-width:2px
    style E fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    style F fill:#e1f5fe,stroke:#1e88e5,stroke-width:2px
    style G fill:#e0f2f1,stroke:#00897b,stroke-width:2px
```
A regress√£o linear, quando aplicada a uma matriz indicadora de classes, pode ser usada para classifica√ß√£o [^8.1]. Codificamos cada classe como uma vari√°vel bin√°ria (0 ou 1) e aplicamos regress√£o linear a cada coluna da matriz indicadora, obtendo os coeficientes $\beta$ usando a solu√ß√£o de m√≠nimos quadrados [^8.2]. No entanto, essa abordagem tem limita√ß√µes: ela n√£o fornece diretamente probabilidades, mas sim valores que podem ser interpretados como ‚Äúscores‚Äù para cada classe. Ao contr√°rio da regress√£o log√≠stica, os valores preditos pela regress√£o linear podem cair fora do intervalo [0, 1], dificultando sua interpreta√ß√£o como probabilidades [^8.1]. A matriz $H$, com elementos $h_j(x_i)$, √© usada para construir o modelo linear que minimiza o erro quadr√°tico:

$$
\hat{\beta} = (H^TH)^{-1}H^T y
$$
onde $y$ √© o vetor de respostas. O ajuste do modelo resulta em $\hat{\mu}(x) = \sum_{j=1}^{7} \hat{\beta}_j h_j(x)$, conforme descrito no texto [^8.2].

**Lemma 2:** A solu√ß√£o de m√≠nimos quadrados $\hat{\beta} = (H^TH)^{-1}H^T y$ minimiza o erro quadr√°tico m√©dio [^8.2]:
```mermaid
graph LR
    subgraph "Least Squares Solution"
        direction TB
        A["Least Squares Solution: ùõÉÃÇ = (H·µÄH)‚Åª¬πH·µÄy"] --> B["Minimizes Mean Squared Error (MSE)"]
         B --> C["MSE = (1/N) Œ£·µ¢ (y·µ¢ - Œº(x·µ¢))¬≤"]
          C --> D["Orthogonal Projection: y onto the column space of H"]
           D --> E["Unique Solution: if H·µÄH is invertible"]
    end
```

$$
\text{Erro} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \mu(x_i))^2
$$
Essa solu√ß√£o √© uma proje√ß√£o ortogonal do vetor $y$ no espa√ßo coluna da matriz $H$, e √© √∫nica se a matriz $H^TH$ for invert√≠vel. $\blacksquare$
> üí° **Exemplo Num√©rico:** Considere um problema de classifica√ß√£o com 3 classes e 5 amostras. Vamos criar um exemplo simples com uma caracter√≠stica para cada amostra. A matriz de *features* $X$ e a matriz de classes $Y$ codificada usando *one-hot-encoding* s√£o:
>
> $$
> X = \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{bmatrix}, \quad Y = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix}
> $$
>
> Aqui, as colunas de Y representam cada uma das 3 classes. Para usar a regress√£o linear como classificador, ajustamos um modelo linear a cada coluna de $Y$.  O processo para calcular os coeficientes $\beta$ para cada classe √©:
>
> $\text{Step 1: } X^TX = \begin{bmatrix} 1 & 2 & 3 & 4 & 5 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{bmatrix} = 55 $
>
> $\text{Step 2: } (X^TX)^{-1} = \frac{1}{55}$
>
> $\text{Step 3: } X^TY = \begin{bmatrix} 1 & 2 & 3 & 4 & 5 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix} = \begin{bmatrix} 4 & 3 & 4 \end{bmatrix}$
>
> $\text{Step 4: } \hat{\beta} = (X^TX)^{-1}X^TY = \frac{1}{55} \begin{bmatrix} 4 & 3 & 4 \end{bmatrix} = \begin{bmatrix} 4/55 & 3/55 & 4/55 \end{bmatrix}$
>
> Ap√≥s calcular os $\beta$ para cada classe, para classificar uma nova amostra $x_{new}$, calculamos $x_{new}\beta$ para cada classe e atribu√≠mos √† classe com maior valor.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> # Dados
> X = np.array([[1], [2], [3], [4], [5]])
> Y = np.array([[1, 0, 0],
>               [0, 1, 0],
>               [1, 0, 0],
>               [0, 0, 1],
>               [0, 1, 0]])
>
> # Ajuste do modelo para cada coluna
> model = LinearRegression()
> model.fit(X, Y)
>
> # Predi√ß√µes
> X_new = np.array([[2.5], [3.5]])
> Y_pred = model.predict(X_new)
> print("Coeficientes:", model.coef_)
> print("Interce√ß√£o:", model.intercept_)
> print("Predi√ß√µes para os novos dados:", Y_pred)
>
> # Classifica√ß√£o baseada nos valores preditos
> predicted_classes = np.argmax(Y_pred, axis=1)
> print("Classes Preditas:", predicted_classes)
> ```

**Corol√°rio 2:** A matriz de covari√¢ncia dos estimadores $\hat{\beta}$ √© dada por $\text{Var}(\hat{\beta}) = (H^TH)^{-1}\hat{\sigma}^2$, onde $\hat{\sigma}^2$ √© a estimativa da vari√¢ncia do ru√≠do. Essa vari√¢ncia pode ser usada para calcular intervalos de confian√ßa para as predi√ß√µes e determinar a precis√£o do modelo [^8.2].

Embora a regress√£o de indicadores possa produzir resultados razo√°veis em algumas situa√ß√µes, seus resultados s√£o mais sens√≠veis √† escala dos dados e podem n√£o ser t√£o confi√°veis quanto modelos probabil√≠sticos como a regress√£o log√≠stica, que √© projetada para gerar probabilidades [^8.1]. Em particular, a regress√£o de indicadores n√£o garante que as probabilidades preditas estejam dentro do intervalo [0,1] [^8.1].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o
<imagem: Diagrama que ilustra o efeito da regulariza√ß√£o (L1 e L2) em modelos de classifica√ß√£o. O diagrama deve mostrar como a penalidade L1 for√ßa coeficientes a zero (esparsidade) e a penalidade L2 reduz a magnitude dos coeficientes, melhorando a generaliza√ß√£o. Utilize a linguagem Mermaid se poss√≠vel.>
```mermaid
graph LR
    A["Classification Model"] --> B["Cost Function (Likelihood)"];
    B --> C["L1 Regularization (Lasso)"];
    B --> D["L2 Regularization (Ridge)"];
    C --> E["Sparsity: Coefficients tend to zero"];
    D --> F["Magnitude Reduction: Smaller coefficients"];
    E --> G["Model Interpretability"];
    F --> H["Improved Generalization"];
    G --> I["Simpler Model"];
    H --> J["Reduces Overfitting"];

    style A fill:#f0f0f0,stroke:#333,stroke-width:2px
    style B fill:#e6f7ff,stroke:#388e3c,stroke-width:2px
    style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style D fill:#e8f5e9,stroke:#1976d2,stroke-width:2px
    style E fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    style F fill:#e1f5fe,stroke:#1e88e5,stroke-width:2px
    style G fill:#e0f2f1,stroke:#00897b,stroke-width:2px
    style H fill:#f0f4c3,stroke:#7cb342,stroke-width:2px
    style I fill:#e0f7fa,stroke:#26a69a,stroke-width:2px
    style J fill:#ef9a9a,stroke:#e53935,stroke-width:2px
```
A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o m√©todos importantes para lidar com modelos complexos e evitar overfitting em classifica√ß√£o [^8.1]. T√©cnicas de regulariza√ß√£o, como L1 (Lasso) e L2 (Ridge), adicionam termos de penalidade √† fun√ß√£o de custo original, controlando a magnitude dos coeficientes do modelo [^8.1]. No caso de modelos log√≠sticos, a regulariza√ß√£o √© introduzida na fun√ß√£o de verossimilhan√ßa [^8.2.2] . A **regulariza√ß√£o L1** (Lasso) √© dada por:
```mermaid
graph LR
    subgraph "L1 Regularization (Lasso)"
         direction TB
        A["Cost Function"] --> B["Original Cost: -1/N Œ£·µ¢ [y·µ¢ log(p·µ¢) + (1-y·µ¢) log(1-p·µ¢)]"]
         A --> C["Penalty Term: Œª Œ£‚±º |Œ≤‚±º|"]
         B & C --> D["Combined Cost Function: -1/N Œ£·µ¢ [y·µ¢ log(p·µ¢) + (1-y·µ¢) log(1-p·µ¢)] + Œª Œ£‚±º |Œ≤‚±º|"]
         D --> E["Sparsity Inducement"]
    end
```

$$
\text{Custo} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1-y_i)\log(1-p_i)] + \lambda \sum_{j=1}^{p} |\beta_j|
$$

onde $p_i$ √© a probabilidade predita da classe 1 para a observa√ß√£o $i$, $\beta_j$ s√£o os coeficientes do modelo, e $\lambda$ √© um par√¢metro de regulariza√ß√£o [^8.2.2]. A **regulariza√ß√£o L2** (Ridge) utiliza o quadrado dos coeficientes:
```mermaid
graph LR
    subgraph "L2 Regularization (Ridge)"
         direction TB
        A["Cost Function"] --> B["Original Cost: -1/N Œ£·µ¢ [y·µ¢ log(p·µ¢) + (1-y·µ¢) log(1-p·µ¢)]"]
         A --> C["Penalty Term: Œª Œ£‚±º Œ≤‚±º¬≤"]
         B & C --> D["Combined Cost Function: -1/N Œ£·µ¢ [y·µ¢ log(p·µ¢) + (1-y·µ¢) log(1-p·µ¢)] + Œª Œ£‚±º Œ≤‚±º¬≤"]
          D --> E["Coefficient Magnitude Reduction"]
    end
```

$$
\text{Custo} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1-y_i)\log(1-p_i)] + \lambda \sum_{j=1}^{p} \beta_j^2
$$
A regulariza√ß√£o L1 tende a produzir modelos esparsos, for√ßando muitos coeficientes a serem exatamente zero, o que facilita a interpreta√ß√£o e a sele√ß√£o de vari√°veis [^8.2.2]. A regulariza√ß√£o L2, por outro lado, reduz a magnitude dos coeficientes, mas raramente os torna exatamente zero, melhorando a generaliza√ß√£o do modelo [^8.2.2].
> üí° **Exemplo Num√©rico:** Considere um problema de classifica√ß√£o bin√°ria com 10 caracter√≠sticas e 100 amostras. Vamos simular alguns dados onde a classe √© influenciada pelas primeiras 3 caracter√≠sticas apenas. Vamos comparar a regress√£o log√≠stica com e sem regulariza√ß√£o L1 e L2.
>
> ```python
> import numpy as np
> import pandas as pd
> from sklearn.model_selection import train_test_split
> from sklearn.linear_model import LogisticRegression
> from sklearn.metrics import accuracy_score
> from sklearn.preprocessing import StandardScaler
>
> # Gerar dados
> np.random.seed(42)
> n_samples = 100
> n_features = 10
>
> X = np.random.randn(n_samples, n_features)
> true_coefs = np.array([2, -1.5, 1, 0, 0, 0, 0, 0, 0, 0])
> y_probs = 1 / (1 + np.exp(-np.dot(X, true_coefs)))
> y = (y_probs > np.random.rand(n_samples)).astype(int)
>
> # Divis√£o dos dados
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
> # Padronizar
> scaler = StandardScaler()
> X_train_scaled = scaler.fit_transform(X_train)
> X_test_scaled = scaler.transform(X_test)
>
> # Regress√£o Log√≠stica sem Regulariza√ß√£o
> logistic_model = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)
> logistic_model.fit(X_train_scaled, y_train)
> y_pred_logistic = logistic_model.predict(X_test_scaled)
> acc_logistic = accuracy_score(y_test, y_pred_logistic)
>
> # Regress√£o Log√≠stica com Regulariza√ß√£o L1 (Lasso)
> lasso_model = LogisticRegression(penalty='l1', C=0.5, solver='liblinear', random_state=42)
> lasso_model.fit(X_train_scaled, y_train)
> y_pred_lasso = lasso_model.predict(X_test_scaled)
> acc_lasso = accuracy_score(y_test, y_pred_lasso)
>
> # Regress√£o Log√≠stica com Regulariza√ß√£o L2 (Ridge)
> ridge_model = LogisticRegression(penalty='l2', C=0.5, solver='lbfgs', max_iter=1000, random_state=42)
> ridge_model.fit(X_train_scaled, y_train)
> y_pred_ridge = ridge_model.predict(X_test_scaled)
> acc_ridge = accuracy_score(y_test, y_pred_ridge)
>
> # Tabela de Compara√ß√£o
> results = pd.DataFrame({
>    'Method': ['Logistic (No Reg)', 'Lasso (L1)', 'Ridge (L2)'],
>    'Accuracy': [acc_logistic, acc_lasso, acc_ridge],
>     'Coeficientes': [logistic_model.coef_, lasso_model.coef_, ridge_model.coef_]
> })
> print(results)
>
> ```
>
> Observa-se que o Lasso (L1) zera alguns coeficientes, selecionando as vari√°veis mais relevantes.

**Lemma 3:** A regulariza√ß√£o L1 em classifica√ß√£o log√≠stica leva a coeficientes esparsos. A penalidade L1 induz uma solu√ß√£o em que muitos coeficientes s√£o exatamente zero, devido √† sua forma n√£o diferenci√°vel na origem [^8.2.2].

**Prova do Lemma 3:** A fun√ß√£o objetivo da regress√£o log√≠stica regularizada por L1 √© n√£o diferenci√°vel nos pontos onde $\beta_j = 0$.  Portanto, a solu√ß√£o √≥tima pode ocorrer quando alguns coeficientes s√£o nulos, for√ßando esparsidade no modelo. $\blacksquare$

**Corol√°rio 3:** A esparsidade induzida pela regulariza√ß√£o L1 melhora a interpretabilidade do modelo, pois apenas as vari√°veis mais relevantes s√£o selecionadas. Isso √© √∫til em situa√ß√µes onde o n√∫mero de vari√°veis √© grande e a identifica√ß√£o dos preditores mais importantes √© crucial [^8.2.2].
> ‚ö†Ô∏è **Ponto Crucial**: A combina√ß√£o de regulariza√ß√£o L1 e L2 (Elastic Net) pode equilibrar o trade-off entre esparsidade e generaliza√ß√£o.  [^8.2.2]

### Separating Hyperplanes e Perceptrons
O conceito de **separating hyperplanes** surge ao procurar uma fronteira linear que divide o espa√ßo de entrada em regi√µes correspondentes √†s diferentes classes [^8.1]. A ideia de maximizar a margem, ou seja, a dist√¢ncia entre o hiperplano e os pontos mais pr√≥ximos de cada classe (os *support vectors*), leva √† formula√ß√£o do **Support Vector Machine (SVM)** [^8.1]. O SVM resolve um problema de otimiza√ß√£o que envolve a busca de um hiperplano √≥timo que maximiza a margem, usando conceitos do dual de Wolfe e multiplicadores de Lagrange [^8.1].
The Perceptron de Rosenblatt, por sua vez, √© um algoritmo de aprendizado linear que pode ser visto como um precursor dos m√©todos de hiperplanos separadores. O Perceptron busca iterativamente um hiperplano que separa as classes, ajustando seus pesos a cada erro de classifica√ß√£o. O algoritmo converge para um hiperplano separador se os dados forem linearmente separ√°veis [^8.1].
### Pergunta Te√≥rica Avan√ßada: Como o conceito de boosting se relaciona com o princ√≠pio de model averaging?
**Resposta:**
O **boosting** √© uma t√©cnica que combina m√∫ltiplos classificadores fracos para criar um classificador forte, comumente utilizando o conceito de ponderar as amostras de treinamento, conforme mostrado em [^8.7] e tamb√©m discutido em [^8.8]. Ele n√£o se enquadra estritamente no modelo de "model averaging" (ou m√©dia de modelos) discutido em [^8.8], embora ambos compartilhem o objetivo de melhorar a performance preditiva combinando m√∫ltiplos modelos [^8.7], [^8.8]. Enquanto o model averaging calcula a m√©dia das predi√ß√µes de v√°rios modelos, comumente treinados de forma independente [^8.8], o boosting treina os modelos de maneira sequencial, com cada modelo subsequente focando em exemplos que foram mal classificados pelos modelos anteriores, conforme descrito em [^8.7] e [^8.8]. Embora o resultado final do boosting tamb√©m possa ser interpretado como uma combina√ß√£o de modelos, o m√©todo de treinamento e a combina√ß√£o s√£o diferentes do que √© feito no model averaging [^8.8]. No entanto, h√° uma forte rela√ß√£o com model averaging. Ambos buscam combinar a predi√ß√£o de m√∫ltiplos modelos, mas o fazem por meios distintos: o primeiro pondera o treinamento, enquanto o segundo pondera os modelos em si [^8.8], [^8.7].
**Lemma 4:** A ideia do boosting √© construir uma sequ√™ncia de classificadores $G_1(x), G_2(x), \ldots, G_T(x)$, cada