## Log-Likelihood e Modelos de Mistura: Uma An√°lise Detalhada

```mermaid
graph LR
    subgraph "Model Inference and Averaging"
      direction TB
        A["Maximum Likelihood Inference"]
        B["Bootstrap Methods"]
        C["Bayesian Methods"]
        D["EM Algorithm"]
        E["Model Averaging"]
        A --> B
        A --> C
        A --> D
        C --> E
    end
```

### Introdu√ß√£o

Neste cap√≠tulo, exploraremos a fundo a **infer√™ncia de modelos** e t√©cnicas de **model averaging**, com foco particular em **modelos de mistura** e a maximiza√ß√£o da fun√ß√£o de **log-verossimilhan√ßa**. O conceito central, como apontado em [^8.1], √© que muitos dos m√©todos de ajuste de modelos, como minimizar a soma de quadrados para regress√£o ou a entropia cruzada para classifica√ß√£o, s√£o, na verdade, inst√¢ncias da abordagem de **m√°xima verossimilhan√ßa (maximum likelihood)**. Al√©m disso, examinaremos m√©todos Bayesianos para infer√™ncia e abordaremos t√©cnicas de *bootstrap* para avalia√ß√£o de incertezas, conectando-as a estimativas de m√°xima verossimilhan√ßa e abordagens Bayesianas. T√©cnicas de model averaging, como committee methods, bagging, stacking e bumping ser√£o exploradas [^8.1].

### Conceitos Fundamentais

**Conceito 1: Maximum Likelihood Inference**
A **maximum likelihood inference** busca encontrar os par√¢metros de um modelo probabil√≠stico que maximizam a verossimilhan√ßa dos dados observados [^8.1], [^8.2.2]. Dado um conjunto de dados $Z = \{z_1, z_2, \ldots, z_N\}$, onde cada $z_i$ √© uma observa√ß√£o, e uma fam√≠lia de distribui√ß√µes de probabilidade $g_\theta(z)$, parametrizada por $\theta$, a verossimilhan√ßa √© dada por:

$$L(\theta; Z) = \prod_{i=1}^{N} g_\theta(z_i)$$

O objetivo √© encontrar o valor de $\theta$ que maximiza $L(\theta; Z)$.  Para simplificar os c√°lculos, √© comum trabalhar com o logaritmo da verossimilhan√ßa, a **log-likelihood** [^8.2.2]:

$$l(\theta; Z) = \sum_{i=1}^{N} \log g_\theta(z_i)$$

O m√©todo de **m√°xima verossimilhan√ßa (maximum likelihood)** escolhe o valor $\hat{\theta}$ que maximiza $l(\theta; Z)$. A log-verossimilhan√ßa √© uma fun√ß√£o que expressa a compatibilidade dos dados com os par√¢metros do modelo.

> üí° **Exemplo Num√©rico:** Suponha que temos um conjunto de dados com 3 observa√ß√µes, $Z = \{2, 3, 5\}$, e que nosso modelo assume que os dados s√£o amostrados de uma distribui√ß√£o normal com m√©dia $\mu$ e desvio padr√£o $\sigma = 1$. Queremos encontrar o valor de $\mu$ que maximiza a verossimilhan√ßa. A fun√ß√£o de densidade normal √© dada por $g_\mu(z_i) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(z_i-\mu)^2}{2}}$. A log-verossimilhan√ßa para este problema √©:
>
> $$l(\mu; Z) = \sum_{i=1}^{3} \log\left(\frac{1}{\sqrt{2\pi}} e^{-\frac{(z_i-\mu)^2}{2}}\right) =  -\frac{3}{2}\log(2\pi) - \frac{1}{2}\sum_{i=1}^{3} (z_i - \mu)^2 $$
>
> Maximizar a log-verossimilhan√ßa √© equivalente a minimizar $\sum_{i=1}^{3} (z_i - \mu)^2$. A solu√ß√£o para este problema √© a m√©dia amostral, $\hat{\mu} = \frac{2 + 3 + 5}{3} = \frac{10}{3} \approx 3.33$.  Este valor de $\hat{\mu}$ √© o que maximiza a log-verossimilhan√ßa dos dados sob a suposi√ß√£o de que os dados seguem uma distribui√ß√£o normal com desvio padr√£o 1.
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> data = np.array([2, 3, 5])
> mu_values = np.linspace(0, 7, 100)
> log_likelihoods = []
>
> for mu in mu_values:
>     log_likelihood = np.sum(norm.logpdf(data, loc=mu, scale=1))
>     log_likelihoods.append(log_likelihood)
>
> import matplotlib.pyplot as plt
> plt.plot(mu_values, log_likelihoods)
> plt.xlabel('M√©dia (Œº)')
> plt.ylabel('Log-Verossimilhan√ßa')
> plt.title('Log-Verossimilhan√ßa vs M√©dia')
> plt.grid(True)
> plt.show()
>
> mle_mu = np.mean(data)
> print(f"Estimativa de M√°xima Verossimilhan√ßa para Œº: {mle_mu}")
> ```

**Lemma 1:** *Sob certas condi√ß√µes de regularidade, o estimador de m√°xima verossimilhan√ßa √© consistente e assintoticamente normal.* A prova deste lemma envolve condi√ß√µes t√©cnicas sobre a suavidade da fun√ß√£o de verossimilhan√ßa e a identificabilidade dos par√¢metros, garantindo a converg√™ncia do estimador para o verdadeiro valor do par√¢metro com o aumento do tamanho da amostra e a converg√™ncia para uma distribui√ß√£o normal [^8.2.2].

**Conceito 2: O M√©todo Bootstrap**
O **bootstrap** √© uma t√©cnica de reamostragem computacional que permite estimar a variabilidade de um estimador estat√≠stico [^8.1], [^8.2.1]. A ideia central √© gerar m√∫ltiplas amostras a partir dos dados observados, replicando o processo de coleta de dados. Em cada uma dessas amostras, o estimador desejado √© calculado. A variabilidade dos valores do estimador obtidos nessas r√©plicas bootstrap fornece uma estimativa da sua variabilidade na amostra original. Existem dois tipos principais de bootstrap: o **n√£o param√©trico** e o **param√©trico**. No bootstrap n√£o param√©trico, amostras s√£o obtidas com reposi√ß√£o a partir dos dados originais. No bootstrap param√©trico, as amostras s√£o geradas a partir do modelo ajustado aos dados. A distribui√ß√£o dos estimadores bootstrap √© ent√£o usada para calcular erros padr√£o ou intervalos de confian√ßa. A import√¢ncia do bootstrap reside em sua capacidade de fornecer estimativas de incerteza para situa√ß√µes onde abordagens anal√≠ticas s√£o dif√≠ceis ou imposs√≠veis de aplicar [^8.2.1].

```mermaid
graph LR
    subgraph "Bootstrap Resampling"
        direction TB
        A["Original Data: Z"]
        B["Non-Parametric Bootstrap: Resample Z with replacement"]
        C["Parametric Bootstrap: Sample from fitted model"]
        D["Calculate Estimator in each resample"]
        E["Estimate Variability using distribution of estimators"]
        A --> B
        A --> C
        B --> D
        C --> D
        D --> E
    end
```

> üí° **Exemplo Num√©rico:** Vamos considerar o mesmo conjunto de dados $Z = \{2, 3, 5\}$ e calcular a variabilidade da m√©dia usando bootstrap n√£o param√©trico.
>
> 1.  **Reamostragem:** Vamos gerar 5 amostras bootstrap com reposi√ß√£o:
>     *   Amostra 1: $\{2, 2, 3\}$
>     *   Amostra 2: $\{3, 5, 5\}$
>     *   Amostra 3: $\{2, 3, 5\}$
>     *   Amostra 4: $\{5, 2, 2\}$
>     *   Amostra 5: $\{3, 3, 3\}$
>
> 2.  **C√°lculo do Estimador:** Calculamos a m√©dia de cada amostra:
>     *   $\bar{x}_1 = (2 + 2 + 3) / 3 = 2.33$
>     *   $\bar{x}_2 = (3 + 5 + 5) / 3 = 4.33$
>     *   $\bar{x}_3 = (2 + 3 + 5) / 3 = 3.33$
>     *   $\bar{x}_4 = (5 + 2 + 2) / 3 = 3$
>     *   $\bar{x}_5 = (3 + 3 + 3) / 3 = 3$
>
> 3.  **Variabilidade:** A variabilidade dessas m√©dias bootstrap nos d√° uma estimativa da variabilidade da m√©dia amostral.  Podemos calcular o desvio padr√£o das m√©dias bootstrap para obter um erro padr√£o:
>
> $$ \text{Desvio padr√£o das m√©dias bootstrap} \approx 0.76 $$
>
> Este valor nos d√° uma ideia da precis√£o da nossa estimativa da m√©dia a partir da amostra original. Podemos aumentar o n√∫mero de amostras bootstrap para obter uma estimativa mais precisa.
>
> ```python
> import numpy as np
>
> data = np.array([2, 3, 5])
> n_bootstrap_samples = 1000
> bootstrap_means = []
>
> for _ in range(n_bootstrap_samples):
>     bootstrap_sample = np.random.choice(data, size=len(data), replace=True)
>     bootstrap_mean = np.mean(bootstrap_sample)
>     bootstrap_means.append(bootstrap_mean)
>
> bootstrap_std = np.std(bootstrap_means)
> print(f"Desvio padr√£o das m√©dias bootstrap: {bootstrap_std:.3f}")
>
> import matplotlib.pyplot as plt
> plt.hist(bootstrap_means, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')
> plt.xlabel('M√©dias Bootstrap')
> plt.ylabel('Densidade')
> plt.title('Histograma das M√©dias Bootstrap')
> plt.grid(True)
> plt.show()
> ```

**Corol√°rio 1:** *Sob condi√ß√µes adequadas, o bootstrap param√©trico e o estimador de m√°xima verossimilhan√ßa convergem para a mesma distribui√ß√£o assint√≥tica.* Este resultado enfatiza a conex√£o entre as abordagens bootstrap e m√°xima verossimilhan√ßa, demonstrando como ambos os m√©todos levam a infer√™ncias similares para o comportamento assint√≥tico dos par√¢metros [^8.2.2].

**Conceito 3: M√©todos Bayesianos**
A abordagem **Bayesiana** para infer√™ncia incorpora uma *prior distribution* sobre os par√¢metros do modelo, expressando o conhecimento pr√©vio antes de observar os dados [^8.1], [^8.3]. Dado um modelo estat√≠stico com par√¢metros $\theta$, uma *prior distribution* $Pr(\theta)$ e uma verossimilhan√ßa $Pr(Z|\theta)$, a distribui√ß√£o *posterior* $Pr(\theta|Z)$ √© dada por:

$$Pr(\theta|Z) = \frac{Pr(Z|\theta)Pr(\theta)}{\int Pr(Z|\theta)Pr(\theta)d\theta}$$

A *posterior distribution* representa o conhecimento atualizado sobre os par√¢metros, levando em considera√ß√£o tanto as informa√ß√µes pr√©vias quanto os dados observados. A infer√™ncia Bayesiana envolve analisar a distribui√ß√£o *posterior*, seja atrav√©s de amostragem ou c√°lculo de momentos, ao inv√©s de apenas um ponto estimado. Esta abordagem permite quantificar a incerteza sobre os par√¢metros de maneira mais completa, incluindo tanto a incerteza a priori quanto aquela obtida com os dados [^8.3].

```mermaid
graph LR
    subgraph "Bayesian Inference"
      direction TB
        A["Prior Distribution: Pr(Œ∏)"]
        B["Likelihood Function: Pr(Z|Œ∏)"]
        C["Posterior Distribution: Pr(Œ∏|Z)"]
        D["Posterior Calculation: Pr(Œ∏|Z) = Pr(Z|Œ∏)Pr(Œ∏) / ‚à´Pr(Z|Œ∏)Pr(Œ∏)dŒ∏"]
        A --> D
        B --> D
        D --> C
    end
```

> üí° **Exemplo Num√©rico:** Vamos usar o mesmo exemplo de dados $Z = \{2, 3, 5\}$ e assumir que eles seguem uma distribui√ß√£o normal com desvio padr√£o $\sigma=1$ e m√©dia $\mu$. Vamos usar uma prior normal para $\mu$, com m√©dia 0 e desvio padr√£o 2. Ou seja, $Pr(\mu) = \mathcal{N}(0, 2^2)$. A verossimilhan√ßa √© $Pr(Z|\mu) = \prod_{i=1}^3 \frac{1}{\sqrt{2\pi}} e^{-\frac{(z_i-\mu)^2}{2}}$. A *posterior* √©:
>
> $$Pr(\mu|Z) \propto  Pr(Z|\mu)Pr(\mu) = \prod_{i=1}^3 \frac{1}{\sqrt{2\pi}} e^{-\frac{(z_i-\mu)^2}{2}} \cdot \frac{1}{\sqrt{2\pi \cdot 2^2}} e^{-\frac{\mu^2}{2\cdot 2^2}}$$
>
>  A distribui√ß√£o *posterior* tamb√©m ser√° uma normal.  Ap√≥s alguns c√°lculos alg√©bricos, obtemos que a *posterior* √©:
>
> $$Pr(\mu|Z) = \mathcal{N}\left(\frac{\sum_{i=1}^3 z_i/1^2 + 0/2^2}{\frac{3}{1^2}+\frac{1}{2^2}}, \left(\frac{3}{1^2} + \frac{1}{2^2}\right)^{-1}\right) = \mathcal{N}\left(\frac{10}{\frac{13}{4}}, \left(\frac{13}{4}\right)^{-1}\right) = \mathcal{N}\left(\frac{40}{13}, \frac{4}{13}\right)$$
>
> A m√©dia da *posterior* √© $\frac{40}{13} \approx 3.08$, que √© diferente da m√©dia de m√°xima verossimilhan√ßa de 3.33, devido √† influ√™ncia da prior. O desvio padr√£o da *posterior* √© $\sqrt{\frac{4}{13}} \approx 0.55$, que quantifica a incerteza sobre $\mu$ ap√≥s observar os dados e considerar a prior.
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> data = np.array([2, 3, 5])
> prior_mean = 0
> prior_std = 2
> data_std = 1
>
> posterior_mean = (np.sum(data) / data_std**2 + prior_mean / prior_std**2) / (len(data) / data_std**2 + 1 / prior_std**2)
> posterior_variance = 1 / (len(data) / data_std**2 + 1 / prior_std**2)
> posterior_std = np.sqrt(posterior_variance)
>
> print(f"M√©dia da Posterior: {posterior_mean:.2f}")
> print(f"Desvio Padr√£o da Posterior: {posterior_std:.2f}")
>
> import matplotlib.pyplot as plt
>
> mu_values = np.linspace(-3, 8, 200)
> prior_density = norm.pdf(mu_values, loc=prior_mean, scale=prior_std)
> likelihood_values = np.prod(norm.pdf(data[:, None], loc=mu_values, scale=data_std), axis=0)
> posterior_density = norm.pdf(mu_values, loc=posterior_mean, scale=posterior_std)
>
> plt.plot(mu_values, prior_density, label='Prior', linestyle='--')
> plt.plot(mu_values, likelihood_values / np.max(likelihood_values), label='Verossimilhan√ßa', linestyle=':')
> plt.plot(mu_values, posterior_density, label='Posterior')
> plt.xlabel('M√©dia (Œº)')
> plt.ylabel('Densidade (normalizada)')
> plt.title('Prior, Verossimilhan√ßa e Posterior')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

> ‚ö†Ô∏è **Nota Importante**: Em contraste com a abordagem frequentista, a abordagem Bayesiana usa uma *prior distribution* para expressar incertezas antes de observar os dados, conforme descrito em [^8.3].
> ‚ùó **Ponto de Aten√ß√£o**: A distribui√ß√£o *posterior* expressa a incerteza ap√≥s ver os dados, e √© o foco da an√°lise bayesiana, conforme indicado em [^8.3].
> ‚úîÔ∏è **Destaque**: Amostragem da *posterior distribution* permite a obten√ß√£o de intervalos de credibilidade, que s√£o interpretados de forma diferente dos intervalos de confian√ßa frequentistas [^8.3].

### Regress√£o Linear e M√≠nimos Quadrados para Modelos de Mistura

```mermaid
graph LR
 subgraph "EM Algorithm for Mixture Models"
    direction TB
    A["Initialize Parameters: œÄ, Œº·µ¢, œÉ·µ¢¬≤"]
    B["E-step: Calculate Responsibilities Œ≥·µ¢"]
    C["M-step: Update Parameters using Œ≥·µ¢"]
    D["Iterate E-step and M-step until convergence"]
    A --> B
    B --> C
    C --> D
  end
```

Em modelos de mistura, a abordagem de regress√£o linear usando matriz de indicadores pode ser √∫til para aproximar as probabilidades de pertencimento a cada componente da mistura. A regress√£o linear, no entanto, n√£o imp√µe a restri√ß√£o de que as probabilidades devam somar um e estar entre zero e um. A abordagem do **EM algorithm** √© prefer√≠vel para modelos de mistura pois lida de forma mais adequada com a natureza probabil√≠stica das probabilidades de pertencimento.

A regress√£o linear em matrizes de indicadores √© usada como uma aproxima√ß√£o para a classifica√ß√£o e, quando aplicada diretamente a probabilidades, pode levar a extrapola√ß√µes fora do intervalo [0,1] [^8.2]. O m√©todo de **m√°xima verossimilhan√ßa** para modelos de mistura define a verossimilhan√ßa em termos da densidade da mistura e maximiza esta verossimilhan√ßa para encontrar os par√¢metros do modelo [^8.2.2]. A regress√£o linear usando matrizes de indicadores pode ser entendida como um passo inicial na otimiza√ß√£o da verossimilhan√ßa em modelos de mistura.

**Lemma 2:** *Em modelos de mistura Gaussianas com vari√¢ncia constante, a solu√ß√£o do problema de m√°xima verossimilhan√ßa √© equivalente √† minimiza√ß√£o da soma dos quadrados das dist√¢ncias entre as observa√ß√µes e as m√©dias de seus respectivos componentes.* Esta equival√™ncia permite entender melhor como a regress√£o linear se encaixa como uma aproxima√ß√£o, principalmente quando as observa√ß√µes s√£o inicialmente classificadas para componentes com base na regress√£o linear [^8.2.2].

**Corol√°rio 2:** *A aplica√ß√£o direta de regress√£o linear a problemas de classifica√ß√£o, como no caso da matriz de indicadores, pode gerar probabilidades que n√£o satisfazem a restri√ß√£o de estarem entre 0 e 1.* Este corol√°rio destaca uma das limita√ß√µes da regress√£o linear em cen√°rios de classifica√ß√£o e motiva o uso de abordagens como a regress√£o log√≠stica ou modelos de mistura para obter probabilidades mais bem calibradas [^8.2].

> üí° **Exemplo Num√©rico:** Suponha que temos dados de alturas de pessoas e suspeitamos que haja dois grupos (homens e mulheres). Temos dados de 5 pessoas: $y = [1.60, 1.70, 1.85, 1.65, 1.75]$.  Podemos usar uma regress√£o linear com uma matriz de indicadores para tentar classificar cada pessoa em um dos dois grupos. Inicialmente, atribu√≠mos aleatoriamente as 2 primeiras pessoas ao grupo 1 (mulheres) e as restantes ao grupo 2 (homens). Constru√≠mos a matriz de indicadores $X$, onde a primeira coluna √© o vetor $[1, 1, 0, 0, 0]$ e a segunda coluna √© $[0, 0, 1, 1, 1]$.  Podemos ent√£o realizar uma regress√£o linear $y = X\beta + \epsilon$, onde $\beta$ representa as m√©dias de cada grupo:
>
> $$ \beta = (X^T X)^{-1} X^T y$$
>
> $\text{Passo 1: } X^T X = \begin{bmatrix} 1 & 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 1 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 1 & 0 \\ 0 & 1 \\ 0 & 1 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}$
>
> $\text{Passo 2: } (X^T X)^{-1} = \begin{bmatrix} 1/2 & 0 \\ 0 & 1/3 \end{bmatrix}$
>
> $\text{Passo 3: } X^T y = \begin{bmatrix} 1 & 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 1 & 1 \end{bmatrix} \begin{bmatrix} 1.60 \\ 1.70 \\ 1.85 \\ 1.65 \\ 1.75 \end{bmatrix} = \begin{bmatrix} 3.3 \\ 5.25 \end{bmatrix}$
>
> $\text{Passo 4: } \beta = \begin{bmatrix} 1/2 & 0 \\ 0 & 1/3 \end{bmatrix} \begin{bmatrix} 3.3 \\ 5.25 \end{bmatrix} = \begin{bmatrix} 1.65 \\ 1.75 \end{bmatrix}$
>
> A regress√£o linear estimou as m√©dias dos grupos como 1.65 e 1.75. No entanto, se fizessemos uma classifica√ß√£o baseada em regress√£o linear, a atribui√ß√£o das amostras a um grupo ou outro n√£o seria clara, pois n√£o temos uma probabilidade de pertencimento. Para resolver isso, podemos usar o EM algorithm, que fornece probabilidades de pertencimento entre 0 e 1.

A regress√£o linear pode ser usada como uma forma de obter valores iniciais para as m√©dias e desvios padr√£o nas componentes de um modelo de mistura Gaussiana [^8.5.1]. A limita√ß√£o da regress√£o linear √© que ela n√£o imp√µe a restri√ß√£o de que as probabilidades devam somar 1 e estar entre 0 e 1. A import√¢ncia do EM √© que ele leva em conta essas restri√ß√µes ao maximizar a verossimilhan√ßa.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Modelos de Mistura
```mermaid
graph LR
    subgraph "Regularization Techniques"
      direction LR
        A["Loss Function"]
        B["L1 Regularization: ŒªŒ£|Œ≤·µ¢|"]
        C["L2 Regularization: ŒªŒ£Œ≤·µ¢¬≤"]
        D["Elastic Net: Œª‚ÇÅ(Œ£|Œ≤·µ¢|) + Œª‚ÇÇ(Œ£Œ≤·µ¢¬≤)"]
        A --> B
        A --> C
        B --> D
        C --> D
    end
```

A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o importantes para evitar overfitting e melhorar a interpretabilidade dos modelos, incluindo modelos de mistura [^8.5.1]. Em modelos de mistura, a regulariza√ß√£o pode ser aplicada, por exemplo, sobre os par√¢metros das m√©dias, vari√¢ncias e tamb√©m sobre as probabilidades de mistura. No entanto, a regulariza√ß√£o em modelos de mistura √© menos comum do que em modelos lineares ou de classifica√ß√£o, porque em modelos de mistura √© importante manter todas as vari√°veis para descrever adequadamente a estrutura da mistura.

Uma maneira de selecionar vari√°veis em modelos de mistura √© usar abordagens de sele√ß√£o baseadas em crit√©rios de informa√ß√£o, como AIC ou BIC, que penalizam a complexidade do modelo. Outra op√ß√£o √© selecionar vari√°veis atrav√©s de m√©todos de busca, que buscam os subconjuntos de vari√°veis mais importantes para a descri√ß√£o da mistura [^8.5.1].

**Lemma 3:** *A regulariza√ß√£o L1 promove a esparsidade, levando a uma redu√ß√£o do n√∫mero de par√¢metros relevantes no modelo.* Em modelos de mistura, a esparsidade pode ser usada na sele√ß√£o das componentes da mistura ou nas vari√°veis usadas para descrever cada componente. Esta esparsidade melhora a interpreta√ß√£o do modelo e reduz o overfitting [^8.5.1].

**Prova do Lemma 3:** A penaliza√ß√£o L1, adicionada √† fun√ß√£o de custo, tem como caracter√≠stica impor uma taxa de decaimento constante sobre os par√¢metros. A penaliza√ß√£o L2, por outro lado, imp√µe um decaimento mais suave e n√£o promove tanta esparsidade quanto a L1. O termo da penaliza√ß√£o L1 na fun√ß√£o de custo √© dado por $\lambda \sum_{j=1}^p |\beta_j|$, onde $\beta_j$ s√£o os par√¢metros e $\lambda$ √© o par√¢metro de regulariza√ß√£o. A minimiza√ß√£o desta fun√ß√£o leva a valores de $\beta_j$ iguais a zero para as vari√°veis menos relevantes, produzindo um modelo esparso [^8.5.1]. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo de mistura com tr√™s componentes e 5 vari√°veis. Ap√≥s aplicar o algoritmo EM sem regulariza√ß√£o, obtivemos os seguintes par√¢metros para a m√©dia de cada componente:
>
> $$ \mu_1 = [1, 2, 3, 4, 5]$$
> $$ \mu_2 = [5, 4, 3, 2, 1]$$
> $$ \mu_3 = [2, 3, 1, 5, 4]$$
>
>  Vamos aplicar regulariza√ß√£o L1 com $\lambda = 0.5$. Isso vai adicionar um termo $\lambda \sum_{j=1}^{5} |\mu_{ij}|$ na fun√ß√£o de custo para cada componente. Ap√≥s otimizar com L1, podemos obter algo como:
>
> $$ \mu_1 = [0.8, 1.8, 2.5, 3.2, 0]$$
> $$ \mu_2 = [4.5, 3.6, 0, 0, 0.5]$$
> $$ \mu_3 = [0, 0, 0, 4, 3]$$
>
> A regulariza√ß√£o L1 fez com que algumas vari√°veis tivessem suas m√©dias reduzidas a zero, indicando que elas s√£o menos relevantes na descri√ß√£o de cada componente.
>
> | Componente | Vari√°vel 1 | Vari√°vel 2 | Vari√°vel 3 | Vari√°vel 4 | Vari√°vel 5 |
> |------------|------------|------------|------------|------------|------------|
> | $\mu_1$ sem L1   |  1         | 2          | 3          | 4          | 5          |
> | $\mu_1$ com L1   |  0.8       | 1.8        | 2.5        | 3.2        | 0          |
> | $\mu_2$ sem L1   |  5         | 4          | 3          | 2          | 1          |
> | $\mu_2$ com L1   |  4.5       | 3.6        | 0          | 0          | 0.5        |
> | $\mu_3$ sem L1   |  2         | 3          | 1          | 5          | 4          |
> | $\mu_3$ com L1   |  0         | 0          | 0          | 4          | 3          |

**Corol√°rio 3:** *A aplica√ß√£o da regulariza√ß√£o L1 em modelos de mistura pode levar √† identifica√ß√£o de estruturas de mistura mais simples e interpret√°veis, especialmente quando o n√∫mero de componentes da mistura √© grande.* A esparsidade na sele√ß√£o das componentes da mistura leva a modelos mais simples que podem ter melhor generaliza√ß√£o [^8.5.1].

> ‚ö†Ô∏è **Ponto Crucial**: A escolha da regulariza√ß√£o, seja L1 ou L2, deve ser feita considerando o trade-off entre a complexidade do modelo e sua capacidade de generaliza√ß√£o [^8.5.1].
### EM Algorithm para Modelos de Mistura
O **EM algorithm (Expectation-Maximization)** √© uma abordagem iterativa para encontrar estimativas de m√°xima verossimilhan√ßa para modelos com vari√°veis latentes, como modelos de mistura [^8.1], [^8.5].  O EM alterna entre um passo de Expectation (E-step), onde as probabilidades dos dados pertencerem a cada componente da mistura s√£o estimadas, e um passo de Maximization (M-step), onde os par√¢metros do modelo s√£o atualizados usando as probabilidades do E-step como pesos [^8.5.1]. A verossimilhan√ßa do modelo √© garantida de n√£o diminuir em cada itera√ß√£o [^8.5.2].

O algoritmo EM para um modelo de mistura Gaussiana com dois componentes pode ser descrito da seguinte forma [^8.5.1]:

1. **Inicializa√ß√£o:** Inicialize os par√¢metros do modelo ($\pi$, $\mu_1$, $\sigma_1^2$, $\mu_2$, $\sigma_2^2$). $\pi$ √© a propor√ß√£o de mistura, $\mu_i$ s√£o as m√©dias e $\sigma_i^2$ as vari√¢ncias de cada componente.
2. **Expectation (E-step):** Calcule as responsabilidades $\gamma_{i}$:

$$\gamma_i = \frac{\pi \phi_{\mu_2, \sigma_2^2}(y_i)}{(1-\pi) \phi_{\mu_1, \sigma_1^2}(y_i) + \pi \phi_{\mu_2, \sigma_2^2}(y_i)}$$
onde $\phi_{\mu, \sigma^2}(y_i)$ √© a fun√ß√£o de densidade Gaussiana com m√©dia $\mu$ e vari√¢ncia $\sigma^2$ avaliada em $y_i$.
3. **Maximization (M-step):** Atualize os par√¢metros do modelo usando as responsabilidades:
  
$$ \mu_1 = \frac{\sum_{i=1}^N (1 - \gamma_i)y_i}{\sum_{i=1}^N (1 - \gamma_i)}$$
$$ \mu_2 = \frac{\sum_{i=1}^N \gamma_i y_i}{\sum_{i=1}^N \gamma_i}$$
$$ \sigma_1^2 = \frac{\sum_{i=1}^N (1 - \gamma_i) (y_i - \mu_1)^2}{\sum_{i=1}^N (1 - \gamma_i)}$$
$$ \sigma_2^2 = \frac{\sum_{i=1}^N \gamma_i (y_i - \mu_2)^2}{\sum_{i=1}^N \gamma_i}$$
$$ \pi = \frac{1}{N} \sum_{i=1}^N \gamma_i$$

4. **Itera√ß√£o:** Repita os passos 2 e 3 at√© a converg√™ncia dos par√¢metros ou da verossimilhan√ßa.

> üí° **Exemplo Num√©rico:** Vamos considerar o mesmo conjunto de dados $y = [1.60, 1.70, 1.85, 1.65, 1.75]$ e aplicar o EM para encontrar os par√¢metros de uma mistura Gaussiana com 2 componentes.
>
> 1. **Inicializa√ß√£o:** Vamos inicializar os par√¢metros como: $\pi = 0.5$, $\mu_1 = 1.65$, $\sigma_1^2 = 0.01$, $\mu_2 = 1.75$, $\sigma_2^2 = 0.01$
>
> 2. **E-step (Itera√ß√£o 1):** Calculamos as responsabilidades $\gamma_i$ para cada ponto de dado.  Por exemplo, para $y_1 = 1.60$:
>
>    * $\phi_{\mu_1, \sigma_1^2}(1.60) = \frac{1}{\sqrt{2\pi \cdot 0.01}} \exp\left(-\frac{(1.60 - 1.65)^2}{2 \cdot 0.01}\right) \approx 2.42$
>   * $\phi_{\mu_2, \sigma_2^2}(1.60) = \frac{1}{\sqrt{2\pi \cdot 0.01}} \exp\left(-\frac{(1.60 - 1.75)^2}{2 \cdot 0.01}\right) \approx 0.000000013$
>   * $\gamma_1 = \frac{0.5 \cdot 0.000000013}{(1-0.5) \cdot 2.42 + 0.5 \cdot 0.000000013} \approx 0.00000001$
>
>   Repetimos esse c√°lculo para todos os pontos e obtemos as seguintes responsabilidades: $\gamma = [0.00, 0.00, 0.99, 0.02, 0.99]$
>
> 3. **M-step (Itera√ß√£o 1):** Atualizamos os par√¢metros usando as responsabilidades:
>
>    * $\mu_1 = \frac{(1-0.00) \cdot 1.60 + (1-0.00)\cdot 1.70 + (1-0.99)\cdot 1.85 + (1-0.02)\cdot 1.65 + (1-0.99)\cdot 1.75}{1-0.00 + 1-0.00 + 1-0.99 + 1-0.02 + 1-0.99} \approx 1.68$
>    * $\mu_2 = \frac{0.00 \cdot 1.60 + 0.00\cdot 1.70 + 0.99\cdot 1.85 + 0.02\cdot 1.65 + 0.99\cdot 1.75}{0.00 + 0.00 + 0.99 + 0.02 + 0.99} \approx 1.78$
>    * $\sigma_1^2 = ... \approx 0.005$
>    * $\sigma_2^2 = ... \approx 0.005$
>    * $\pi = \frac{0.00 + 0.00 + 0.99 + 0.02 + 0.99}{5} \approx 0.40$
>
> 4. **Itera√ß√£o:** Repetimos os passos 2 e 3 at√© a converg√™ncia. Ap√≥s algumas itera√ß√µes, os par√¢metros convergir√£o para os valores que maximizam a log-verossimilhan√ßa.

O algoritmo EM √© utilizado na estima√ß√£o de par√¢metros de modelos de mistura [^8.5], e √© um exemplo de um algoritmo de majoriza√ß√£o-minimiza√ß√£o (MM algorithm) [^8.5.3], [^8.7], como pode ser visto em [^8.7].

### Pergunta Te√≥rica Avan√ßada: Qual a Rela√ß√£o entre a Abordagem Bootstrap Param√©trica e o EM Algorithm em Modelos de Mistura Gaussianos?

**Resposta:**
A abordagem *parametric bootstrap* pode ser usada para aproximar a distribui√ß√£o amostral dos par√¢metros em modelos de mistura [^8