## Maximization Step: M√©todos de Otimiza√ß√£o e Infer√™ncia em Modelos Estat√≠sticos

```mermaid
  graph LR
      A["Maximum Likelihood Estimation (MLE)"] --> B["Maximization of Likelihood Function"];
      C["Bayesian Methods"] --> D["Maximization of Posterior Distribution"];
      E["Optimization Algorithms (e.g., EM)"] --> F["Iterative Parameter Updates"];
      B --> F
      D --> F
      F --> G["Optimal Parameters"];
```

### Introdu√ß√£o

A infer√™ncia de modelos estat√≠sticos, como discutido em [^8.1], envolve a estima√ß√£o de par√¢metros que melhor se ajustam aos dados observados. Tradicionalmente, essa estima√ß√£o tem sido realizada por meio da minimiza√ß√£o de fun√ß√µes de erro, como a soma de quadrados ou a entropia cruzada. No entanto, muitos modelos exigem abordagens mais sofisticadas, como a **Maximum Likelihood Estimation (MLE)** e m√©todos Bayesianos, onde a maximiza√ß√£o de fun√ß√µes de verossimilhan√ßa ou a an√°lise de distribui√ß√µes posteriores se tornam cruciais. O processo de **Maximization Step** √© fundamental nesses m√©todos, desempenhando um papel central na busca pelos par√¢metros √≥timos. Este cap√≠tulo explora em detalhes esses conceitos, focando em t√©cnicas de otimiza√ß√£o e suas aplica√ß√µes em diversos contextos de infer√™ncia estat√≠stica.

### Conceitos Fundamentais

A otimiza√ß√£o de par√¢metros √© um processo iterativo que busca os valores dos par√¢metros que maximizam uma fun√ß√£o de objetivo, tipicamente a verossimilhan√ßa ou uma fun√ß√£o relacionada, conforme discutido em [^8.2.2]. Para entender a fundo as t√©cnicas usadas na **Maximization Step**, precisamos definir alguns conceitos chave:

**Conceito 1:** **Maximum Likelihood Estimation (MLE)**: A MLE √© um m√©todo para estimar os par√¢metros de um modelo estat√≠stico, buscando os valores que maximizam a fun√ß√£o de verossimilhan√ßa, que representa a probabilidade dos dados observados dado o modelo, como definido em [^8.2.2]. Em termos matem√°ticos, se temos dados $Z = \{z_1, z_2, \ldots, z_N\}$ e um modelo com par√¢metros $\theta$, a fun√ß√£o de verossimilhan√ßa √© dada por:
$$ L(\theta; Z) = \prod_{i=1}^{N} g_\theta(z_i) $$
O objetivo da MLE √© encontrar $\hat{\theta}$ tal que:
$$\hat{\theta} = \arg \max_{\theta} L(\theta; Z)$$
onde $g_\theta(z_i)$ √© a densidade ou massa de probabilidade dos dados sob o modelo.

> üí° **Exemplo Num√©rico:** Suponha que temos uma amostra de 5 lan√ßamentos de uma moeda, resultando em `Z = [1, 0, 1, 1, 0]`, onde 1 representa cara e 0 coroa. Queremos estimar a probabilidade $\theta$ de sair cara usando MLE. Assumindo que cada lan√ßamento √© uma Bernoulli, a fun√ß√£o de verossimilhan√ßa √©: $L(\theta; Z) = \theta^3(1-\theta)^2$.  Para encontrar o $\hat{\theta}$ que maximiza essa fun√ß√£o,  calculamos a derivada com rela√ß√£o a $\theta$ e igualamos a zero:
>
> $\frac{dL}{d\theta} = 3\theta^2(1-\theta)^2 - 2\theta^3(1-\theta) = 0$.
>
> Simplificando, temos:  $\theta^2(1-\theta)(3(1-\theta) - 2\theta) = 0$
>
> Isso leva a $\theta = 0, \theta = 1,$ ou $3 - 5\theta = 0$, o que d√° $\hat{\theta} = 3/5 = 0.6$. Portanto, a estimativa de m√°xima verossimilhan√ßa para a probabilidade de sair cara √© 0.6. Este valor maximiza a probabilidade dos dados observados.

```mermaid
  graph LR
      subgraph "Maximum Likelihood Estimation"
        direction TB
        A["Likelihood Function: L(Œ∏; Z) = ‚àè g_Œ∏(z_i)"]
        B["Find Optimal Parameters:  argmax_Œ∏  L(Œ∏; Z)"]
        A --> B
      end
```

**Lemma 1:** Sob certas condi√ß√µes de regularidade, o estimador de m√°xima verossimilhan√ßa √© consistente e assintoticamente normal, conforme demonstrado em [^8.2.2]. Isso significa que, com um n√∫mero crescente de observa√ß√µes, o estimador converge para o valor verdadeiro do par√¢metro e a sua distribui√ß√£o se aproxima de uma normal.

**Conceito 2:** **Fun√ß√£o Score e Informa√ß√£o de Fisher**: A **fun√ß√£o score** √© o gradiente do logaritmo da verossimilhan√ßa em rela√ß√£o aos par√¢metros, dada por $\ell(\theta; z_i) = \frac{\partial \log g_\theta(z_i)}{\partial \theta}$, conforme [^8.2.2]. A **informa√ß√£o de Fisher**, por sua vez, √© uma medida da quantidade de informa√ß√£o que os dados fornecem sobre os par√¢metros. Matematicamente:
$$I(\theta) = - \mathbb{E}\left[ \frac{\partial^2}{\partial \theta^2} \log L(\theta; Z) \right]$$
ou
$$I(\theta) =  \sum_{i=1}^{N} \mathbb{E}\left[  \left(\frac{\partial \log g_\theta(z_i)}{\partial \theta} \right)^2 \right]$$
A informa√ß√£o de Fisher desempenha um papel crucial na avalia√ß√£o da precis√£o dos estimadores de MLE.

> üí° **Exemplo Num√©rico:** Usando o exemplo da moeda, a log-verossimilhan√ßa √© $\log L(\theta; Z) = 3\log(\theta) + 2\log(1-\theta)$. A fun√ß√£o score √© a primeira derivada:
>
> $\ell(\theta; Z) = \frac{3}{\theta} - \frac{2}{1-\theta}$
>
> A segunda derivada √©:
>
> $\frac{\partial^2 \log L}{\partial \theta^2} = -\frac{3}{\theta^2} - \frac{2}{(1-\theta)^2}$
>
> A informa√ß√£o de Fisher (esperada) √©:
>
> $I(\theta) = - \mathbb{E}\left[  -\frac{3}{\theta^2} - \frac{2}{(1-\theta)^2} \right] = \frac{5}{\theta(1-\theta)}$
>
>  Substituindo $\hat{\theta} = 0.6$, temos $I(0.6) = \frac{5}{0.6(0.4)} \approx 20.83$. Isso nos diz que a precis√£o da nossa estimativa aumenta com mais dados.
>

```mermaid
  graph LR
    subgraph "Fisher Information"
        direction TB
        A["Score Function: ‚àÇ/‚àÇŒ∏ log L(Œ∏; Z)"]
        B["Fisher Information: I(Œ∏) = -E[‚àÇ¬≤/‚àÇŒ∏¬≤ log L(Œ∏; Z)]"]
        A --> B
    end
```

**Corol√°rio 1:** A vari√¢ncia do estimador de m√°xima verossimilhan√ßa √© inversamente proporcional √† informa√ß√£o de Fisher, como mostrado em [^8.2.2]. Isso implica que quanto maior a informa√ß√£o de Fisher, menor a vari√¢ncia do estimador e, portanto, maior a precis√£o da estimativa.

**Conceito 3:** **M√©todos Bayesianos**: Em contraste com a MLE, os m√©todos Bayesianos incorporam um conhecimento pr√©vio sobre os par√¢metros por meio de uma distribui√ß√£o *prior* $Pr(\theta)$. A infer√™ncia Bayesiana leva em considera√ß√£o tanto a verossimilhan√ßa dos dados quanto a distribui√ß√£o *prior* para obter a distribui√ß√£o *posterior* $Pr(\theta|Z)$, como visto em [^8.3]:
$$ Pr(\theta|Z) = \frac{Pr(Z|\theta) Pr(\theta)}{\int Pr(Z|\theta) Pr(\theta) d\theta} $$
A distribui√ß√£o *posterior* representa nossa cren√ßa atual sobre os par√¢metros, dado os dados e o conhecimento pr√©vio. A **Maximization Step** em um contexto Bayesiano pode envolver a busca por um estimador *MAP* (Maximum a Posteriori), que maximiza a distribui√ß√£o *posterior*.

> üí° **Exemplo Num√©rico:**  Usando o exemplo da moeda, vamos adicionar uma *prior* Beta com par√¢metros $\alpha=2$ e $\beta=2$, que √© uma *prior* n√£o informativa que favorece valores de probabilidade pr√≥ximos a 0.5. A distribui√ß√£o *prior* √© $Pr(\theta) \propto \theta^{\alpha-1}(1-\theta)^{\beta-1} = \theta(1-\theta)$.  A distribui√ß√£o *posterior* √© proporcional ao produto da *prior* com a verossimilhan√ßa:
>
> $Pr(\theta|Z) \propto \theta^3(1-\theta)^2 \cdot \theta(1-\theta) = \theta^4(1-\theta)^3$.
>
> Para encontrar o estimador *MAP*, maximizamos a *posterior*, derivando em rela√ß√£o a $\theta$ e igualando a zero.
>
> $\frac{d}{d\theta}(\theta^4(1-\theta)^3) = 4\theta^3(1-\theta)^3 - 3\theta^4(1-\theta)^2 = 0$.
>
> Resolvendo para $\theta$, obtemos $\hat{\theta}_{MAP} = \frac{4}{7} \approx 0.571$. Note que, por causa da *prior*, o estimador *MAP* √© ligeiramente diferente do estimador MLE.

```mermaid
  graph LR
      subgraph "Bayesian Inference"
        direction TB
        A["Prior Distribution: Pr(Œ∏)"]
        B["Likelihood: Pr(Z|Œ∏)"]
        C["Posterior Distribution: Pr(Œ∏|Z) ‚àù Pr(Z|Œ∏) * Pr(Œ∏)"]
        A --> C
        B --> C
        D["Maximization Step in Bayesian Context: Find  Œ∏_MAP = argmax Pr(Œ∏|Z)"]
        C --> D
      end
```

> ‚ö†Ô∏è **Nota Importante**: Em um contexto Bayesiano, a **Maximization Step** geralmente busca o *MAP*, que √© o modo da distribui√ß√£o posterior, enquanto em MLE o objetivo √© o estimador que maximiza a fun√ß√£o de verossimilhan√ßa.
> ‚ùó **Ponto de Aten√ß√£o**: A escolha da *prior* influencia significativamente o resultado da infer√™ncia Bayesiana e, portanto, a **Maximization Step** neste contexto.
> ‚úîÔ∏è **Destaque**: O processo de **Maximization Step** pode envolver otimiza√ß√£o num√©rica, especialmente quando as solu√ß√µes anal√≠ticas n√£o s√£o vi√°veis.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
  graph LR
      A["Data (X,y)"] --> B["Linear Regression Model: y = H(x)^T * Œ≤"];
      B --> C["Define Loss Function (e.g., MSE)"];
       C --> D["Find Œ≤ that minimizes loss"]
      D --> E["Parameter Œ≤"]
      E --> F["Classification Boundary Decision"]
```

A regress√£o linear, quando aplicada a uma matriz de indicadores para classifica√ß√£o, tamb√©m pode ser interpretada como um caso de MLE sob a suposi√ß√£o de erros Gaussianos, conforme discutido em [^8.2.2]. Ao aplicar m√≠nimos quadrados para estimar os coeficientes de regress√£o, estamos implicitamente maximizando a fun√ß√£o de verossimilhan√ßa, o que faz do processo de ajuste uma **Maximization Step**. As solu√ß√µes para $\beta$ em (8.2) e (8.21), s√£o os valores que minimizam o erro quadr√°tico m√©dio e maximizam a fun√ß√£o de verossimilhan√ßa Gaussianas, o que pode ser visto por meio de suas deriva√ß√µes.

A fun√ß√£o de verossimilhan√ßa nesse contexto √© definida como:
$$L(\beta, \sigma^2; Z) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - h(x_i)^T \beta)^2}{2\sigma^2}\right)$$
onde $h(x_i)$ s√£o as bases, $\beta$ s√£o os par√¢metros e $\sigma^2$ √© a vari√¢ncia. A maximiza√ß√£o dessa fun√ß√£o em rela√ß√£o a $\beta$ leva ao estimador de m√≠nimos quadrados:
$$\hat{\beta} = (H^T H)^{-1} H^T y$$
O que mostra que o m√©todo de m√≠nimos quadrados √© uma forma de executar uma **Maximization Step** sob certas suposi√ß√µes, como a normalidade dos erros, e com a fun√ß√£o de verossimilhan√ßa sendo Gaussianas.

> üí° **Exemplo Num√©rico:** Suponha que temos um conjunto de dados com dois preditores e um alvo:
> ```python
> import numpy as np
>
> X = np.array([[1, 2], [1, 3], [1, 4], [1,5]])
> y = np.array([3, 5, 6, 8])
> ```
>  Nesse caso, $H = X$. Para calcular $\hat{\beta}$,  primeiro calculamos $H^T H$ e $(H^T H)^{-1}$.
>
> ```python
> HTH = X.T @ X
> HTH_inv = np.linalg.inv(HTH)
> ```
> Em seguida, calculamos $H^T y$:
>
> ```python
> HTy = X.T @ y
> ```
> Finalmente, calculamos $\hat{\beta}$:
>
> ```python
> beta_hat = HTH_inv @ HTy
> print(f"Beta estimativo: {beta_hat}")
> ```
>  O resultado ser√° $\hat{\beta} \approx [1, 1.4]$, o que significa que a reta ajustada ser√° aproximadamente $y = 1 + 1.4x$. Os valores encontrados para $\hat{\beta}$ maximizam a verossimilhan√ßa dos dados sob a suposi√ß√£o de erros Gaussianos.

**Lemma 2:** O estimador de m√≠nimos quadrados √© o estimador de m√°xima verossimilhan√ßa sob a suposi√ß√£o de erros Gaussianos, o que demonstra a equival√™ncia entre a minimiza√ß√£o do erro quadr√°tico e a maximiza√ß√£o da verossimilhan√ßa nesse contexto, com base no que foi discutido em [^8.2.2].

```mermaid
    graph LR
    subgraph "Equivalence of Least Squares and MLE"
        direction LR
        A["Assumed Gaussian Errors"] --> B["Minimizing Sum of Squares"]
        A --> C["Maximizing Gaussian Likelihood"]
        B --> D["Equivalent Estimator"]
        C --> D
    end
```

**Corol√°rio 2:** A vari√¢ncia dos estimadores $\beta$ pode ser estimada usando o estimador de vari√¢ncia residual, ou seja, $Var(\hat{\beta}) = \hat{\sigma}^2(H^TH)^{-1}$, confirmando a rela√ß√£o entre MLE e m√≠nimos quadrados.

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, podemos estimar $\hat{\sigma}^2$ como:
>
> ```python
> y_hat = X @ beta_hat
> residuals = y - y_hat
> sigma_squared_hat = np.sum(residuals**2) / (len(y) - X.shape[1])
> print(f"sigma_squared_hat: {sigma_squared_hat}")
> ```
> O resultado ser√° $\hat{\sigma}^2 \approx 0.2$, que √© a estimativa da vari√¢ncia dos erros. A vari√¢ncia dos estimadores de $\beta$ √© ent√£o:
> ```python
> var_beta = sigma_squared_hat * HTH_inv
> print(f"Variance of beta: {var_beta}")
> ```
> Isso fornece uma medida da incerteza nos nossos estimadores de $\beta$.

Em geral, a **Maximization Step** em m√©todos de classifica√ß√£o com regress√£o linear envolve encontrar os par√¢metros $\beta$ que maximizam a verossimilhan√ßa ou minimizam o erro quadr√°tico m√©dio, conforme o caso, garantindo que os par√¢metros estimados estejam alinhados com a melhor representa√ß√£o dos dados.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
  graph LR
      A["Likelihood Function"] --> B["Add Regularization Term"];
      B --> C["L1 Regularization (Sparsity) | L2 Regularization (Magnitude Reduction)"];
      C --> D["Modified Objective Function"];
      D --> E["Optimization to Find Regularized Parameters"];
```

A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o importantes na classifica√ß√£o para lidar com o *overfitting* e para melhorar a interpretabilidade dos modelos, conforme mencionado em [^8.2]. Em ambos os casos, uma **Maximization Step** √© necess√°ria, mas ela √© feita em conjunto com termos de penaliza√ß√£o na fun√ß√£o objetivo, como em [^8.2.2].

Em vez de apenas maximizar a verossimilhan√ßa, m√©todos de regulariza√ß√£o adicionam um termo de penaliza√ß√£o que impede que os par√¢metros assumam valores muito grandes. Por exemplo, na regress√£o log√≠stica, podemos adicionar um termo de regulariza√ß√£o L1 ou L2 na fun√ß√£o de log-verossimilhan√ßa:
$$\ell(\beta) = \sum_{i=1}^N y_i \log(\sigma(\beta^Tx_i)) + (1-y_i)\log(1-\sigma(\beta^Tx_i)) - \lambda ||\beta||_p $$
onde $\sigma(\beta^Tx_i)$ √© a fun√ß√£o sigm√≥ide, $\lambda$ √© o par√¢metro de regulariza√ß√£o e $p = 1$ ou $2$ para L1 ou L2 respectivamente. A **Maximization Step** nesse caso envolve maximizar a fun√ß√£o de log-verossimilhan√ßa penalizada.

> üí° **Exemplo Num√©rico:** Considere a regress√£o log√≠stica com regulariza√ß√£o L2 (Ridge). Vamos usar um conjunto de dados simulado:
> ```python
> import numpy as np
> from sklearn.linear_model import LogisticRegression
> from sklearn.preprocessing import StandardScaler
>
> np.random.seed(42)
> X = np.random.randn(100, 5)
> y = np.random.randint(0, 2, 100)
>
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)
> ```
> Agora, vamos treinar dois modelos, um sem regulariza√ß√£o e um com L2:
> ```python
> model_no_reg = LogisticRegression(penalty=None)
> model_no_reg.fit(X_scaled, y)
>
> model_l2_reg = LogisticRegression(penalty='l2', C=0.1) # C=1/lambda
> model_l2_reg.fit(X_scaled, y)
> ```
> Os coeficientes ser√£o:
> ```python
> print("Coeficientes sem regulariza√ß√£o:", model_no_reg.coef_)
> print("Coeficientes com regulariza√ß√£o L2:", model_l2_reg.coef_)
> ```
>  Observe como a regulariza√ß√£o L2 encolhe os coeficientes em dire√ß√£o a zero.
>
> A **Maximization Step** aqui √© realizada pelo algoritmo de otimiza√ß√£o (como o gradiente descendente) usado pelo `LogisticRegression`. A fun√ß√£o objetivo que o algoritmo minimiza inclui o termo de regulariza√ß√£o, o que leva a um modelo que generaliza melhor em novos dados.
>
>
>
> ```mermaid
>  graph LR
>      A["Fun√ß√£o de Log-Verossimilhan√ßa"] --> B(Otimiza√ß√£o)
>      B --> C{Regulariza√ß√£o L1 ou L2}
>      C --> D[Par√¢metros \(\beta\) Regularizados]
>
> ```

**Lemma 3:** A penaliza√ß√£o L1 induz a esparsidade nos coeficientes, ou seja, alguns coeficientes s√£o exatamente zero, o que ajuda na sele√ß√£o de vari√°veis, conforme [^8.2.2].

**Prova do Lemma 3:** A penaliza√ß√£o L1 tem a forma $\lambda||\beta||_1 = \lambda\sum_j |\beta_j|$. A n√£o diferenciabilidade em $\beta_j=0$ causa os coeficientes serem exatamente zero quando se tenta minimizar a fun√ß√£o de custo. A penaliza√ß√£o L2, por outro lado, tem a forma $\lambda||\beta||_2^2 = \lambda\sum_j \beta_j^2$, que √© diferenci√°vel e n√£o induz esparsidade, como explicado em [^8.2.2]. $\blacksquare$

```mermaid
    graph LR
    subgraph "L1 Regularization and Sparsity"
    direction TB
    A["L1 Penalty:  Œª||Œ≤||‚ÇÅ  =  ŒªŒ£ |Œ≤·µ¢|"]
    B["Non-Differentiability at Œ≤·µ¢=0"]
    C["Induces Sparsity (Some Œ≤·µ¢ = 0)"]
    A --> B
    B --> C
    end
```
**Corol√°rio 3:** A combina√ß√£o de penaliza√ß√µes L1 e L2, conhecida como Elastic Net, permite obter um compromisso entre esparsidade e regulariza√ß√£o, controlando o n√∫mero de vari√°veis relevantes e a magnitude dos coeficientes, como discutido em [^8.2.2].

A **Maximization Step** em m√©todos de classifica√ß√£o regularizados envolve a busca pelos par√¢metros $\beta$ que equilibram o ajuste aos dados e a complexidade do modelo, alcan√ßando um bom desempenho de generaliza√ß√£o.

### Separating Hyperplanes e Perceptrons

```mermaid
    graph LR
        A["Training Data (X,y)"] --> B["Find Separating Hyperplane:  w^Tx + b = 0"]
        B --> C["Optimization to Maximize Margin: max 1/||w|| * min(y_i(w^Tx_i + b))"];
        C --> D["Updated Weights and Bias"]
        D --> E["Separating Hyperplane"];
```

A ideia de **separating hyperplanes** e perceptrons tamb√©m envolvem um processo de otimiza√ß√£o que √© parte do conceito de uma **Maximization Step**, de forma indireta, como em [^8.5.2] e [^8.5.1]. Em um contexto de *Separating Hyperplanes*, o objetivo √© encontrar o hiperplano que maximiza a margem de separa√ß√£o entre as classes. A formula√ß√£o matem√°tica desse problema leva a uma otimiza√ß√£o de par√¢metros que define o hiperplano.
$$ \max_{w,b} \frac{1}{||w||} \min_{i} y_i(w^T x_i + b) $$
onde $w$ √© o vetor normal do hiperplano e $b$ √© o termo de vi√©s.

Os perceptrons, por sua vez, ajustam iterativamente os pesos para minimizar os erros de classifica√ß√£o, como em [^8.5.1]. O algoritmo do perceptron tenta achar um hiperplano que separa as classes utilizando uma fun√ß√£o de custo. A **Maximization Step** aqui pode envolver a atualiza√ß√£o dos par√¢metros $w$ usando um algoritmo de otimiza√ß√£o, como o gradiente descendente:
$$ w_{t+1} = w_t + \alpha y_i x_i$$
onde $\alpha$ √© a taxa de aprendizado.

> üí° **Exemplo Num√©rico:** Vamos criar um dataset simples linearmente separ√°vel e treinar um perceptron:
> ```python
> import numpy as np
>
> # Dados linearmente separ√°veis
> X = np.array([[1, 1], [2, 1], [1, 2], [2, 2], [4, 5], [5, 4], [5, 5], [6, 4]])
> y = np.array([-1, -1, -1, -1, 1, 1, 1, 1]) # -1 para classe 1 e 1 para classe 2
>
> # Inicializa√ß√£o dos pesos
> w = np.array([0.1, -0.1])
> b = 0.1
> alpha = 0.1
> num_epochs = 10
>
> for epoch in range(num_epochs):
>    for i in range(len(X)):
>        if y[i] * (np.dot(w, X[i]) + b) <= 0:
>            w = w + alpha * y[i] * X[i]
>            b = b + alpha * y[i]
>            print(f'Updated w:{w}, b:{b}')
> ```
>
> O loop acima representa a **Maximization Step** do perceptron. A cada itera√ß√£o, os pesos `w` s√£o atualizados na dire√ß√£o que minimiza os erros de classifica√ß√£o. O processo converge para um hiperplano que separa os dados.
>

A otimiza√ß√£o de um perceptron tem o objetivo de maximizar uma fun√ß√£o objetivo que minimiza os erros de classifica√ß√£o, mesmo que n√£o explicitamente uma fun√ß√£o de verossimilhan√ßa. A converg√™ncia do algoritmo depende da separabilidade dos dados.

### Pergunta Te√≥rica Avan√ßada: Como a abordagem EM se relaciona com o conceito de Maximum Likelihood em modelos de mistura Gaussianos?

```mermaid
graph LR
    subgraph "EM Algorithm for Gaussian Mixtures"
        direction TB
        A["Incomplete Data (Latent Variables)"]
        B["E-step: Estimate Latent Variable Distribution"]
        C["M-step: Maximize Likelihood with Estimated Latent Variables"]
        D["Iterate E-step and M-step until Convergence"]
        A --> B
        B --> C
        C --> D
    end
```

**Resposta:** A abordagem EM (Expectation-Maximization) √© um m√©todo iterativo que busca a solu√ß√£o de m√°xima verossimilhan√ßa em modelos onde os dados observados n√£o s√£o completos. No caso de modelos de mistura Gaussianos, a vari√°vel latente √© a qual componente gerou cada ponto de dado, conforme visto em [^8.5]. A **Maximization Step** do EM envolve encontrar os par√¢metros que maximizam a verossimilhan√ßa dos dados completos, mas como a atribui√ß√£o das vari√°veis latentes n√£o √© conhecida, ela √© feita de forma iterativa.

A ideia central do algoritmo EM √© alternar entre duas etapas: a etapa de Expectation (E-step) e a etapa de Maximization (M-step). A etapa E consiste em estimar a distribui√ß√£o das vari√°veis latentes dado os par√¢metros correntes do modelo, enquanto a etapa M consiste em encontrar os par√¢metros que maximizam a verossimilhan√ßa dos dados completados na etapa E.

No contexto de modelos de mistura Gaussianos, a fun√ß√£o de verossimilhan√ßa √© dada por:
$$ L(\theta; Z) = \prod_{i=1}^{N} \sum_{k=1}^K \pi_k \phi(\mathbf{x}_i|\mu_k, \Sigma_k) $$
onde $\pi_k$ √© a probabilidade de cada componente, $\mu_k$ √© a m√©dia e $\Sigma_k$ √© a matriz de covari√¢ncia para cada componente $k$. O algoritmo EM itera entre as seguintes etapas:
1. **E-step**: Calcula as responsabilidades $\gamma_{ik}$ de cada ponto de dados para cada componente, usando os par√¢metros atuais.
$$ \gamma_{ik} = \frac{\pi_k \phi(\mathbf{x}_i|\mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \phi(\mathbf{x}_i|\mu_j, \Sigma_j)} $$
2. **M-step**: Atualiza os par√¢metros $\pi_k$, $\mu_k$ e $\Sigma_k$ usando as responsabilidades calculadas na etapa E.
$$ \pi_k = \frac{1}{N} \sum_{i=1}^N \gamma_{ik} $$
$$ \mu_k = \frac{\sum_{i=1}^N \gamma_{ik}\mathbf{x}_i}{\sum_{i=1}^N \gamma_{ik}} $$
$$ \Sigma_k = \frac{\sum_{i=1}^N \gamma_{ik}(\mathbf{x}_i - \mu_k)(\mathbf{x}_i - \mu_k)^T}{\sum_{i=1}^N \gamma_{ik}} $$

> üí° **Exemplo Num√©rico:** Vamos usar um exemplo simplificado para ilustrar o algoritmo EM com duas Gaussianas. Suponha que temos um conjunto de dados unidimensional e dois componentes Gaussianos com m√©dias $\mu_1$, $\mu_2$ e vari√¢ncias $\sigma^2_1$, $\sigma^2_2$. Inicializamos aleatoriamente $\mu_1=0$, $\mu_2=5$, $\sigma^2_1=1$, $\sigma^2_2=1$ e $\pi_1=\pi_2=0.5$.  Vamos criar alguns dados misturados:
>
> ```python
> import numpy as np
> from scipy.stats import norm
>
> np.random.seed(42)
> X = np.concatenate([np.random.normal(0, 1, 50), np.random.normal(5, 1, 50)])
>
> mu1, mu2 = 0, 5
> sigma1_sq, sigma2_sq = 1, 1
> pi1, pi2 = 0.5, 0.5
>
> for iteration in range(10):
>     # E-step
>     gamma1 = pi1 * norm.pdf(X, mu1, np.sqrt(sigma1_sq))
>     gamma2 = pi2 * norm.pdf(X, mu2, np.sqrt(sigma2_sq))
>     gamma_sum = gamma1 + gamma2
>     gamma1 = gamma1 / gamma_sum
>     gamma2 = gamma2 / gamma_sum
>
>     # M-step
>     pi1 = np.mean(gamma1)
>     pi2 = np.mean(gamma2)
>     mu1 = np.sum(gamma1 * X) / np.sum(gamma1)
>     mu2 = np.sum(gamma2 * X) / np.sum(gamma2)
>     sigma1_sq = np.sum(gamma1 * (X - mu1)**2) / np.sum(gamma1)
>     sigma2_sq = np.sum(gamma2 * (X - mu2)**2) / np.sum(gamma2)
>
>     print(f'Iteration {iteration}: mu1={mu1:.2f}, mu2={mu2:.2f}, sigma1_sq={sigma1_sq:.2f}, sigma2_sq={sigma2_sq:.2f}')
>
> ```
>
> A cada itera√ß√£o do loop, primeiro calculamos as responsabilidades de cada ponto aos componentes (E-step), e em seguida atualizamos as m√©dias e vari√¢ncias dos componentes, al√©m dos pesos (M-step). O algoritmo converge para os par√¢metros que maximizam a verossimilhan√ßa dos dados. A **Maximization Step** dentro de cada itera√ß√£o garante que a verossimilhan√ßa dos dados complete aumente.
>

Essa itera√ß√£o continua at√© a converg√™ncia dos par√¢metros e da fun√ß√£o de verossimilhan√ßa. O **Maximization Step** na itera√ß√£o do EM garante que a fun√ß√£o de verossimilhan√ßa aumente ou permane√ßa a mesma a cada itera√ß√£o, garantindo a converg√™ncia.

**Lemma 4:** O algoritmo EM garante que a verossimilhan√ßa observada aumente em cada itera√ß√£o, o que pode ser demonstrado utilizando a desigualdade de Jensen [^8.5.2] e o fato de que $R(\theta', \theta)$ √© maximizado quando $\theta'=\theta$ [^8.5.2].

```mermaid
  graph LR
      subgraph "EM Algorithm Convergence"
        direction TB
        A["Jensen's Inequality"] --> B["Guarantee of Non-Decreasing Likelihood"];
        B --> C["M-step maximizes R(Œ∏', Œ∏) when Œ∏'=Œ∏"];
        C --> D["Observed Likelihood Increases Each Iteration"];
      end
```

**Corol√°rio 4:** A **Maximization Step** do EM √© realizada na verossimilhan√ßa completa, mas com as vari√°veis latentes sendo estimadas na etapa de Expectation, mostrando que o EM √© um m√©todo para otimiza√ß√£o de modelos com vari√°veis latentes, e tamb√©m um processo para encontrar par√¢metros que maximizam a verossimilhan√ßa [^8.5.2].

> ‚ö†Ô∏è **Ponto Crucial**: A complexidade da fun√ß√£o de verossimilhan√ßa nos modelos de mistura Gaussianos exige uma abordagem iterativa, como o EM, para encontrar par√¢metros que maximizam a verossimilhan√ßa.

### Conclus√£o

A **Maximization Step** √© um componente essencial em muitos m√©todos de infer√™ncia estat√≠stica, incluindo a MLE, m√©todos Bayesianos, modelos lineares de classifica√ß√£o, modelos regularizados, *separating hyperplanes* e modelos de mistura. A otimiza√ß√£o, seja por meio de m√©todos anal√≠ticos ou num√©ricos, garante que os par√¢metros estimados estejam alinhados com os dados observados, maximizando a fun√ß√£o de verossimilhan√ßa ou uma fun√ß√£o relacionada. T√©cnicas como o algoritmo EM, *bumping* [^8.9] e *model averaging* [^8.8] permitem lidar com problemas mais complexos, como modelos com vari√°veis latentes e modelos inst√°veis, permitindo obter solu√ß√µes √≥timas e est√°veis.

[^8.1]: "For most of this book, the fitting (learning) of models has been achieved by minimizing a sum of squares for regression, or by minimizing cross-entropy for classification. In fact, both of these minimizations are instances of the maximum likelihood approach to fitting." *(Trecho de "Model Inference and Averaging")*
[^8.2.2]: "Maximum likelihood is based on the likelihood function, given by $L(Œ∏; Z) = \prod_i g_Œ∏(z_i)$, the probability of the observed data under the model $g_Œ∏$. The likelihood is defined only up to a positive multiplier, which we have taken to be one. We think of $L(Œ∏; Z)$ as a function of Œ∏, with our data Z fixed." *(Trecho de "Model Inference and Averaging")*
[^8.3]: "In the Bayesian approach to inference, we specify a sampling model Pr(Z|Œ∏) (density or probability mass function) for our data given the parameters, and a prior distribution for the parameters Pr(Œ∏) reflecting our knowledge about Œ∏ before we see the data. We then compute the posterior distribution Pr(Œ∏|Z) = Pr(Z|Œ∏).Pr(Œ∏) / ‚à´ Pr(Z|Œ∏).Pr(Œ∏)dŒ∏' which represents our updated knowledge about Œ∏ after we see the data." *(Trecho de "Model Inference and Averaging")*
[^8.5.2]:  "The above procedure is an example of the EM (or Baum-Welch) algorithm for maximizing likelihoods in certain classes of problems. These problems are ones for which maximization of the likelihood is difficult, but made easier by enlarging the sample with latent (unobserved) data." *(Trecho de "Model Inference and Averaging")*
[^8.5.1]:  "In this section we describe a simple mixture model for density estimation, and the associated EM algorithm for carrying out maximum likelihood estimation." *(Trecho de "Model Inference and Averaging")*
[^8.5]: "The EM algorithm is a popular tool for simplifying difficult maximum likelihood problems. We first describe it in the context of a simple mixture model." *(Trecho de "Model Inference and Averaging")*
[^8.9]: "The final method described in this chapter does not involve averaging or combining models, but rather is a technique for finding a better single model. Bumping uses bootstrap sampling to move randomly through model space. For problems where fitting method finds many local minima, bumping can help the method to avoid getting stuck in poor solutions." *(Trecho de "Model Inference and Averaging")*
[^8.8]: "Here we discuss Bayesian model averaging more generally. We have a set of candidate models Mm, m = 1,..., M for our training set Z. These models may be of the same type with different parameter values (e.g., subsets in linear regression), or different models for the same task (