## Stacking para Sele√ß√£o de Modelos

<imagem: Um diagrama que ilustra o processo de stacking. A imagem pode conter v√°rias camadas, representando diferentes modelos de aprendizado de m√°quina, com um modelo de n√≠vel superior (meta-learner) combinando suas predi√ß√µes.>

### Introdu√ß√£o
A sele√ß√£o de modelos √© um passo cr√≠tico no desenvolvimento de qualquer solu√ß√£o de aprendizado de m√°quina. O objetivo √© identificar o modelo que melhor se ajusta aos dados e generaliza bem para dados n√£o vistos. Em vez de escolher um √∫nico modelo, o *stacking* combina as predi√ß√µes de m√∫ltiplos modelos base, com o objetivo de obter uma performance superior. Esta abordagem √© uma forma de *model averaging* mais sofisticada, que, ao inv√©s de simplesmente calcular a m√©dia das predi√ß√µes, treina um modelo (o *meta-learner*) para aprender como melhor combinar as predi√ß√µes dos modelos base. O *stacking* busca, portanto, explorar as for√ßas de cada modelo, mitigando suas fraquezas [^8.8].

```mermaid
graph TB
    subgraph "Stacking Process"
        direction TB
        A["Multiple Base Models"] --> B["Predictions from Base Models"]
        B --> C["Meta-Learner Training"]
        C --> D["Final Prediction"]
    end
```

### Conceitos Fundamentais

**Conceito 1: Ensemble Learning e Model Averaging**
*Ensemble learning* refere-se √† pr√°tica de treinar m√∫ltiplos modelos e combinar suas predi√ß√µes para obter uma performance superior √† que um √∫nico modelo poderia alcan√ßar. Uma forma simples de ensemble learning √© o *model averaging*, onde as predi√ß√µes de m√∫ltiplos modelos s√£o combinadas por meio de uma m√©dia simples ou ponderada [^8.8]. O *stacking* √© uma forma mais complexa de ensemble learning, que tenta aprender a melhor forma de combinar as predi√ß√µes dos modelos base.

**Lemma 1:** A m√©dia de predi√ß√µes de modelos com bias e vari√¢ncia distintos pode resultar em um modelo com menor vari√¢ncia e bias mais controlado, conforme discutido em [^8.7].
**Prova:** Dado que a vari√¢ncia de um estimador √© reduzida ao se calcular a m√©dia de v√°rias realiza√ß√µes independentes do mesmo, e o bias √© preservado, ao usar modelos variados, com diferentes vieses, a m√©dia tende a balancear esses vieses.
$$\text{Var}\left(\frac{1}{n}\sum_{i=1}^n \hat{f}_i(x)\right) = \frac{1}{n^2}\sum_{i=1}^n \text{Var}(\hat{f}_i(x))$$
Se as predi√ß√µes dos modelos forem independentes, a vari√¢ncia da m√©dia √© menor do que a vari√¢ncia de qualquer um dos modelos individuais $\blacksquare$.

```mermaid
graph LR
 subgraph "Variance Reduction through Averaging"
    direction LR
    A["Var(f_i(x))"] --> B["Averaging: (1/n) * Œ£ f_i(x)"]
    B --> C["Var((1/n) * Œ£ f_i(x)) = (1/n^2) * Œ£ Var(f_i(x))"]
    C --> D["Reduced Variance"]
 end
```

> üí° **Exemplo Num√©rico:** Suponha que temos tr√™s modelos, cujas predi√ß√µes para um dado ponto $x$ t√™m as seguintes vari√¢ncias: $\text{Var}(\hat{f}_1(x)) = 0.8$, $\text{Var}(\hat{f}_2(x)) = 0.6$, e $\text{Var}(\hat{f}_3(x)) = 0.9$. Ao calcular a m√©dia dessas predi√ß√µes, a vari√¢ncia da m√©dia seria:
> $$\text{Var}\left(\frac{1}{3}\sum_{i=1}^3 \hat{f}_i(x)\right) = \frac{1}{3^2} (0.8 + 0.6 + 0.9) = \frac{2.3}{9} \approx 0.256$$
> A vari√¢ncia da m√©dia (0.256) √© menor do que a vari√¢ncia de cada modelo individual, demonstrando a redu√ß√£o de vari√¢ncia proporcionada pelo *model averaging*.

**Conceito 2: A Arquitetura do Stacking**
O *stacking* envolve duas camadas de modelos:
1.  **Modelos base (level-0 models):** Um conjunto de modelos de aprendizado de m√°quina √© treinado usando os dados de treinamento. Esses modelos podem variar em tipo (e.g., √°rvores de decis√£o, regress√£o log√≠stica, redes neurais) e complexidade, e geralmente operam de forma independente.
2. **Meta-learner (level-1 model):** Um novo modelo √© treinado utilizando as predi√ß√µes dos modelos base como entradas. O meta-learner aprende a combinar essas predi√ß√µes de forma √≥tima, para realizar a predi√ß√£o final [^8.8].

```mermaid
graph TB
    subgraph "Stacking Architecture"
    direction TB
        A["Level-0 Models"] --> B["Predictions of Level-0 Models"]
        B --> C["Level-1 Meta-Learner"]
        C --> D["Final Prediction"]
    end
```

**Corol√°rio 1:** A escolha do meta-learner √© crucial para o sucesso do *stacking*. Modelos lineares (e.g. regress√£o linear ou log√≠stica) s√£o frequentemente usados, pois s√£o capazes de aprender pesos que refletem a import√¢ncia de cada modelo base, como discutido em [^8.8]. Em casos mais complexos, outros modelos podem ser mais apropriados.

**Conceito 3: Cross-Validation no Stacking**
Um cuidado importante ao aplicar o *stacking* √© evitar *data leakage*. Para isso, a cross-validation √© usada durante o treinamento dos modelos base, e a predi√ß√£o para cada fold √© usada como input para treinar o meta-learner. Isso garante que o meta-learner n√£o veja as mesmas amostras usadas no treinamento dos modelos base, prevenindo overfitting [^8.8]. A ideia √© que, se os modelos base foram treinados usando apenas parte dos dados, o meta-learner pode ser treinado usando o restante do conjunto de dados, de forma a evitar o *data leakage*.

```mermaid
graph TB
    subgraph "Cross-Validation in Stacking"
    direction TB
        A["Training Data"] --> B["Cross-Validation Folds"]
        B --> C["Level-0 Model Training on Folds"]
        C --> D["Predictions on Hold-out Folds"]
        D --> E["Level-1 Meta-Learner Training"]
    end
```

> ‚ö†Ô∏è **Nota Importante**: O uso de cross-validation no *stacking* √© essencial para evitar overfitting e garantir que a generaliza√ß√£o do modelo seja adequada, conforme descrito em [^8.8].

### Regress√£o Linear e M√≠nimos Quadrados para Stacking

<imagem: Diagrama de fluxo que representa o processo de stacking: 1) treinamento dos modelos base (level-0), 2) gera√ß√£o das predi√ß√µes dos modelos base, 3) treinamento do meta-learner (level-1) usando as predi√ß√µes dos modelos base como inputs, e 4) predi√ß√£o final combinando as predi√ß√µes dos modelos base com os pesos aprendidos pelo meta-learner.>

A regress√£o linear pode ser utilizada no *stacking* como um m√©todo para combinar as predi√ß√µes dos modelos base, conforme mencionado em [^8.8]. As predi√ß√µes dos modelos base s√£o tratadas como vari√°veis de entrada, e o meta-learner aprende os coeficientes de regress√£o que melhor combinam essas predi√ß√µes. O m√©todo de m√≠nimos quadrados √© usado para encontrar os coeficientes que minimizam o erro quadr√°tico m√©dio entre as predi√ß√µes e os valores reais.

**Lemma 2:** Em um problema de regress√£o, a solu√ß√£o de m√≠nimos quadrados para combinar predi√ß√µes de modelos base equivale √† proje√ß√£o ortogonal das sa√≠das observadas no espa√ßo gerado pelas predi√ß√µes dos modelos base.
**Prova:**  Seja $Y$ o vetor de sa√≠das, e $F$ a matriz onde cada coluna representa as predi√ß√µes de um dos modelos base. A solu√ß√£o de m√≠nimos quadrados para os coeficientes $w$ √© dada por:
$$w = (F^T F)^{-1} F^T Y$$
Esta f√≥rmula representa a proje√ß√£o ortogonal de $Y$ no espa√ßo gerado pelas colunas de $F$, conforme descrito em [^8.8].  $\blacksquare$

```mermaid
graph TB
    subgraph "Least Squares Projection"
        direction TB
        A["Observed Outputs: Y"]
        B["Predictions Matrix: F"]
        C["Weights: w = (F^T F)^(-1) F^T Y"]
        D["Projection of Y onto Space of F"]
        A --> D
        B --> C
        C --> D
     end
```

> üí° **Exemplo Num√©rico:** Suponha que temos tr√™s modelos base e um conjunto de treinamento com 5 amostras. As predi√ß√µes dos modelos base e as sa√≠das reais s√£o as seguintes:
>
> | Amostra | Modelo 1 ($f_1$) | Modelo 2 ($f_2$) | Modelo 3 ($f_3$) | Sa√≠da Real ($y$) |
> |---|---|---|---|---|
> | 1 | 2.1 | 1.8 | 2.5 | 2.3 |
> | 2 | 3.0 | 2.7 | 3.2 | 3.1 |
> | 3 | 1.5 | 1.6 | 1.9 | 1.8 |
> | 4 | 2.8 | 2.5 | 3.0 | 2.9 |
> | 5 | 2.2 | 2.0 | 2.4 | 2.3 |
>
> A matriz $F$ das predi√ß√µes e o vetor $Y$ das sa√≠das s√£o:
>
> $$F = \begin{bmatrix} 2.1 & 1.8 & 2.5 \\ 3.0 & 2.7 & 3.2 \\ 1.5 & 1.6 & 1.9 \\ 2.8 & 2.5 & 3.0 \\ 2.2 & 2.0 & 2.4 \end{bmatrix} \qquad Y = \begin{bmatrix} 2.3 \\ 3.1 \\ 1.8 \\ 2.9 \\ 2.3 \end{bmatrix}$$
>
> Usando a f√≥rmula de m√≠nimos quadrados, podemos calcular os pesos $w$. Primeiro, calculamos $F^T F$:
>
> $$F^T F = \begin{bmatrix} 25.64 & 23.46 & 27.66 \\ 23.46 & 21.4 & 25.2 \\ 27.66 & 25.2 & 29.82 \end{bmatrix}$$
>
> Em seguida, calculamos $(F^T F)^{-1}$:
>
> $$(F^T F)^{-1} \approx \begin{bmatrix} 1.60 & -1.82 & 0.93 \\ -1.82 & 2.57 & -1.31 \\ 0.93 & -1.31 & 0.72 \end{bmatrix}$$
>
> Agora, calculamos $F^T Y$:
>
> $$F^T Y = \begin{bmatrix} 25.9 \\ 23.6 \\ 27.9 \end{bmatrix}$$
>
> Finalmente, calculamos os pesos $w$:
>
> $$w = (F^T F)^{-1} F^T Y = \begin{bmatrix} 1.60 & -1.82 & 0.93 \\ -1.82 & 2.57 & -1.31 \\ 0.93 & -1.31 & 0.72 \end{bmatrix} \begin{bmatrix} 25.9 \\ 23.6 \\ 27.9 \end{bmatrix} \approx \begin{bmatrix} 0.5 \\ 0.3 \\ 0.2 \end{bmatrix}$$
>
> Os pesos obtidos (aproximadamente 0.5, 0.3 e 0.2) indicam que o modelo 1 tem um peso maior na predi√ß√£o final do meta-learner.
>
> ```python
> import numpy as np
> from numpy.linalg import inv
>
> F = np.array([[2.1, 1.8, 2.5],
>               [3.0, 2.7, 3.2],
>               [1.5, 1.6, 1.9],
>               [2.8, 2.5, 3.0],
>               [2.2, 2.0, 2.4]])
> Y = np.array([2.3, 3.1, 1.8, 2.9, 2.3])
>
> w = inv(F.T @ F) @ F.T @ Y
> print(w)
> ```
>
> Este exemplo ilustra como a regress√£o linear pode ser usada para combinar as predi√ß√µes de diferentes modelos base, onde os pesos s√£o otimizados para minimizar o erro quadr√°tico m√©dio.

**Corol√°rio 2:** A solu√ß√£o de m√≠nimos quadrados para o *stacking* (ou seja, o meta-learner) minimiza o erro quadr√°tico m√©dio, mas n√£o imp√µe nenhuma restri√ß√£o sobre os coeficientes, que podem ser negativos ou maiores que 1. Isto pode levar a resultados contra-intuitivos [^8.8].

A utiliza√ß√£o da regress√£o linear como meta-learner tem a vantagem de ser computacionalmente eficiente e facilmente interpret√°vel. Os coeficientes de regress√£o indicam a import√¢ncia de cada modelo base na predi√ß√£o final. No entanto, a regress√£o linear pode n√£o capturar rela√ß√µes complexas entre os modelos base, limitando o desempenho do *stacking*.

> ‚ùó **Ponto de Aten√ß√£o**: Embora a regress√£o linear seja simples, ela pode n√£o ser a melhor escolha como meta-learner para todos os problemas, especialmente aqueles onde as predi√ß√µes dos modelos base t√™m rela√ß√µes n√£o lineares, como sugerido em [^8.8].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Stacking

A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o importantes no *stacking* para evitar overfitting e melhorar a generaliza√ß√£o do modelo. Quando o n√∫mero de modelos base √© alto, o meta-learner pode ter dificuldade em aprender a melhor combina√ß√£o, sendo importante aplicar t√©cnicas de sele√ß√£o de modelos ou regulariza√ß√£o, como discutido em [^8.8].
O meta-learner √© constru√≠do como uma combina√ß√£o linear de todos os modelos base, e portanto, se torna necess√°rio aplicar t√©cnicas para selecionar quais modelos devem ter um peso maior e quais devem ser negligenciados.

**Lemma 3:** A regulariza√ß√£o L1 (Lasso) em um modelo linear (como o meta-learner em *stacking*) promove a esparsidade de par√¢metros, ou seja, faz com que alguns coeficientes de regress√£o sejam exatamente zero, efetivamente selecionando os modelos base mais relevantes.
**Prova:** A regulariza√ß√£o L1 adiciona a penalidade da norma L1 aos coeficientes da regress√£o. A penalidade $ \lambda \sum |w_i|$, onde  $w_i$ √© o $i$-√©simo coeficiente, leva a que alguns coeficientes sejam exatamente zero durante a minimiza√ß√£o, selecionando os modelos base mais relevantes, conforme descrito em [^8.8] $\blacksquare$.

```mermaid
graph TB
    subgraph "Lasso Regularization"
        direction TB
         A["Original Loss Function"] --> B["L1 Penalty: Œª * Œ£|w_i|"]
        B --> C["Regularized Loss Function"]
       C --> D["Sparse Weights (w_i = 0 for some i)"]
    end
```

> üí° **Exemplo Num√©rico:** Suponha que, ao aplicar regress√£o linear como meta-learner, obtivemos os seguintes pesos para 5 modelos base:  $w = [0.4, -0.2, 0.7, 0.1, -0.3]$.  Ao aplicar a regulariza√ß√£o L1 (Lasso) com um par√¢metro $\lambda$ adequado,  alguns desses pesos podem ser zerados. Por exemplo, se $\lambda$ for suficientemente grande, poder√≠amos obter $w_{lasso} = [0.5, 0, 0.8, 0, -0.1]$, indicando que os modelos base 2 e 4 foram considerados menos importantes e seus pesos foram definidos como zero.
>
> ```python
> import numpy as np
> from sklearn.linear_model import Lasso
>
> # Predi√ß√µes dos modelos base (simuladas)
> F = np.random.rand(100, 5)
> # Sa√≠das reais (simuladas)
> Y = np.random.rand(100)
>
> # Regress√£o linear simples
> w_ols = np.linalg.inv(F.T @ F) @ F.T @ Y
>
> # Regress√£o Lasso com alpha = 0.1
> lasso = Lasso(alpha=0.1)
> lasso.fit(F, Y)
> w_lasso = lasso.coef_
>
> print(f"Pesos OLS: {w_ols}")
> print(f"Pesos Lasso: {w_lasso}")
>
> ```
>
> Este exemplo ilustra como a regulariza√ß√£o L1 (Lasso) pode realizar a sele√ß√£o de modelos base, atribuindo pesos nulos aos menos relevantes.

**Corol√°rio 3:** A regulariza√ß√£o L2 (Ridge) em um modelo linear reduz a magnitude dos coeficientes de regress√£o, mas n√£o os leva a zero. Isso pode melhorar a estabilidade do meta-learner e reduzir o overfitting, como indicado em [^8.8].

```mermaid
graph TB
    subgraph "Ridge Regularization"
    direction TB
        A["Original Loss Function"] --> B["L2 Penalty: Œª * Œ£(w_i)^2"]
        B --> C["Regularized Loss Function"]
       C --> D["Reduced magnitude of Weights"]
    end
```

> üí° **Exemplo Num√©rico:** Usando o mesmo exemplo anterior, ao aplicar a regulariza√ß√£o L2 (Ridge) com um par√¢metro $\lambda$ adequado, os pesos dos modelos base seriam reduzidos, mas n√£o zerados. Por exemplo, se $\lambda$ for apropriado, poder√≠amos obter $w_{ridge} = [0.3, -0.1, 0.6, 0.05, -0.2]$. Isso indica que todos os modelos base ainda contribuem para a predi√ß√£o final, mas seus pesos s√£o menores em magnitude, o que ajuda a prevenir overfitting.
>
> ```python
> import numpy as np
> from sklearn.linear_model import Ridge
>
> # Predi√ß√µes dos modelos base (simuladas)
> F = np.random.rand(100, 5)
> # Sa√≠das reais (simuladas)
> Y = np.random.rand(100)
>
> # Regress√£o Ridge com alpha = 0.1
> ridge = Ridge(alpha=0.1)
> ridge.fit(F, Y)
> w_ridge = ridge.coef_
>
> print(f"Pesos Ridge: {w_ridge}")
> ```
>
>  Este exemplo demonstra como a regulariza√ß√£o L2 (Ridge) pode reduzir a magnitude dos pesos, melhorando a estabilidade do meta-learner.

> ‚ö†Ô∏è **Ponto Crucial**: As penaliza√ß√µes L1 (Lasso) e L2 (Ridge) podem ser combinadas em uma regulariza√ß√£o Elastic Net, que combina as vantagens de ambas as abordagens, promovendo a esparsidade dos modelos e controlando o overfitting, conforme mencionado em [^8.8].

### Separating Hyperplanes e Perceptrons no Stacking

O conceito de *separating hyperplanes* pode ser usado na constru√ß√£o do meta-learner para problemas de classifica√ß√£o. Em vez de fazer uma combina√ß√£o linear das predi√ß√µes dos modelos base, o meta-learner pode ser um classificador que aprende a fronteira de decis√£o linear ou n√£o linear no espa√ßo das predi√ß√µes dos modelos base.

O Perceptron, um algoritmo de classifica√ß√£o linear, pode ser utilizado para construir o meta-learner, buscando aprender o hiperplano que melhor separa as classes no espa√ßo das predi√ß√µes dos modelos base [^8.8]. A ideia √© aprender os pesos que melhor combinam os resultados das predi√ß√µes dos modelos base, de forma a classificar as amostras corretamente, com base em [^8.8].

```mermaid
graph TB
    subgraph "Perceptron Meta-Learner"
        direction TB
        A["Predictions from Base Models"] --> B["Weighted Sum of Predictions"]
        B --> C["Activation Function"]
        C --> D["Class Prediction"]
        D --> E["Update weights using the error"]
         E --> B
     end
```

**Pergunta Te√≥rica Avan√ßada:**
Como a arquitetura e os par√¢metros do meta-learner afetam a complexidade do modelo de *stacking* e o risco de overfitting? Como podemos ajustar esses aspectos para obter um bom desempenho de generaliza√ß√£o?
**Resposta:**
A complexidade do modelo de *stacking* √© afetada pela complexidade dos modelos base e pela complexidade do meta-learner. Se os modelos base forem muito complexos e/ou o meta-learner for um modelo n√£o linear complexo (e.g., redes neurais profundas), o modelo resultante de *stacking* pode ter um alto risco de overfitting.

**Lemma 4:** A capacidade de generaliza√ß√£o de um modelo de *stacking* depende da diversidade entre os modelos base, da capacidade de generaliza√ß√£o de cada modelo base, e da complexidade do meta-learner.  Modelos base muito correlacionados podem levar a um meta-learner inst√°vel e com baixa capacidade de generaliza√ß√£o.
**Prova:** A diversidade entre os modelos base reduz a probabilidade do modelo de *stacking* se adequar a ru√≠dos ou peculiaridades espec√≠ficas do conjunto de treinamento. Um meta-learner mais simples, como uma regress√£o log√≠stica ou linear, pode mitigar o overfitting, penalizando pesos excessivos atribu√≠dos a alguns modelos base.

**Corol√°rio 4:** A regulariza√ß√£o e a sele√ß√£o de vari√°veis no meta-learner s√£o formas de controlar sua complexidade e mitigar o risco de overfitting. Escolher um meta-learner com complexidade adequada ao n√∫mero de modelos base e ao tamanho do conjunto de treinamento tamb√©m √© essencial para garantir a generaliza√ß√£o do modelo [^8.8].

> ‚úîÔ∏è **Destaque**: A escolha do meta-learner e a aplica√ß√£o de t√©cnicas de regulariza√ß√£o s√£o fundamentais para equilibrar a complexidade do modelo de *stacking* e seu desempenho de generaliza√ß√£o.

### Conclus√£o

O *stacking* √© uma t√©cnica poderosa para combinar as predi√ß√µes de m√∫ltiplos modelos de aprendizado de m√°quina, buscando obter um desempenho superior ao de um √∫nico modelo. Esta abordagem requer um planejamento cuidadoso na escolha dos modelos base, do meta-learner e da estrat√©gia de cross-validation para evitar o overfitting. Embora modelos lineares tenham um papel fundamental como meta-learners, √© preciso reconhecer que eles podem n√£o ser suficientes para problemas complexos, sendo necess√°rio recorrer a outros modelos, regulariza√ß√£o, e sele√ß√£o de modelos. A flexibilidade do *stacking* o torna uma ferramenta valiosa na constru√ß√£o de solu√ß√µes de aprendizado de m√°quina, mas √© importante compreender suas nuances para aplic√°-lo corretamente.
<!-- END DOCUMENT -->
### Footnotes
[^8.8]: "In Section 8.4 we viewed bootstrap values of an estimator as approximate posterior values of a corresponding parameter, from a kind of nonparamet-ric Bayesian analysis. Viewed in this way, the bagged estimate (8.51) is an approximate posterior Bayesian mean. In contrast, the training sample estimate f(x) corresponds to the mode of the posterior. Since the posterior mean (not mode) minimizes squared-error loss, it is not surprising that bagging can often reduce mean squared-error. Here we discuss Bayesian model averaging more generally. We have a set of candidate models Mm, m = 1,..., M for our training set Z. These models may be of the same type with different parameter values (e.g., subsets in linear regression), or different models for the same task (e.g., neural networks and regression trees). Suppose Œ∂ is some quantity of interest, for example, a prediction f(x) at some fixed feature value x. The posterior distribution of Œ∂ is" *(Trecho de <Model Inference and Averaging>)*
[^8.7]: "Earlier we introduced the bootstrap as a way of assessing the accuracy of a parameter estimate or a prediction. Here we show how to use the bootstrap to improve the estimate or prediction itself. In Section 8.4 we investigated the relationship between the bootstrap and Bayes approaches, and found that the bootstrap mean is approximately a posterior average. Bagging further exploits this connection. Consider first the regression problem. Suppose we fit a model to our training data Z = {(x1, y1), (x2, y2), ..., (xn, yn)}, obtaining the predic-tion f(x) at input x. Bootstrap aggregation or bagging averages this predic-tion over a collection of bootstrap samples, thereby reducing its variance. For each bootstrap sample Z*b, b = 1, 2, ..., B, we fit our model, giving prediction f*b(x). The bagging estimate is defined by" *(Trecho de <Model Inference and Averaging>)*
