## Gibbs Sampling Convergence

```mermaid
graph LR
    subgraph "Gibbs Sampling in Bayesian Inference"
        A["Bayesian Inference"] --> B["MCMC Methods"]
        B --> C["Gibbs Sampling"]
        C --> D["Iterative Sampling"]
        D --> E["Convergence to Posterior"]
        E --> F["Applications"]
    end
```

### Introdu√ß√£o

O m√©todo de **Gibbs Sampling** √© uma t√©cnica crucial no campo da **infer√™ncia Bayesiana** e, de maneira mais geral, em **m√©todos de Monte Carlo via Cadeias de Markov (MCMC)**. Este m√©todo se destaca por sua capacidade de gerar amostras de distribui√ß√µes de probabilidade complexas, particularmente aquelas para as quais a amostragem direta √© invi√°vel. Como um caso especial de MCMC, o Gibbs Sampling iterativamente amostra cada vari√°vel aleat√≥ria condicionada √†s outras, formando uma cadeia de Markov cuja distribui√ß√£o estacion√°ria coincide com a distribui√ß√£o alvo conjunta [^8.6]. O objetivo deste cap√≠tulo √© explorar os detalhes da converg√™ncia do Gibbs Sampling, um t√≥pico de grande import√¢ncia para a aplica√ß√£o correta e eficaz deste m√©todo.

### Conceitos Fundamentais

**Conceito 1: Infer√™ncia Bayesiana e Amostragem Posterior**

Em infer√™ncia Bayesiana, nosso objetivo √© inferir informa√ß√µes sobre par√¢metros desconhecidos (Œ∏) dado um conjunto de dados observados (Z). A base para esta infer√™ncia √© o **teorema de Bayes**, que define a **distribui√ß√£o posterior**, $Pr(\theta|Z)$, como proporcional ao produto da **verossimilhan√ßa** (a probabilidade dos dados dado os par√¢metros, $Pr(Z|\theta)$) e a **distribui√ß√£o a priori** dos par√¢metros ($Pr(\theta)$):

$$
Pr(\theta|Z) = \frac{Pr(Z|\theta)Pr(\theta)}{Pr(Z)}
$$
```mermaid
graph LR
    subgraph "Bayes' Theorem"
        A["Posterior: Pr(Œ∏|Z)"]
        B["Likelihood: Pr(Z|Œ∏)"]
        C["Prior: Pr(Œ∏)"]
        D["Evidence: Pr(Z)"]
        A -- "‚àù" --> B
        A -- "‚àù" --> C
        B & C --> A
        D --> A
    end
```

A amostragem da distribui√ß√£o posterior, muitas vezes, √© desafiadora devido √† complexidade da mesma. O Gibbs Sampling oferece uma maneira de obter amostras da distribui√ß√£o posterior sem precisar computar a distribui√ß√£o explicitamente, o que a torna aplic√°vel em uma vasta gama de problemas [^8.6].

> üí° **Exemplo Num√©rico:**
>
> Suponha que queremos inferir a m√©dia ($\mu$) e a vari√¢ncia ($\sigma^2$) de uma distribui√ß√£o normal com base em dados observados. Usando a infer√™ncia Bayesiana, precisamos definir uma distribui√ß√£o a priori para $\mu$ e $\sigma^2$. Vamos supor que $\mu \sim \mathcal{N}(\mu_0, \tau_0^2)$ e $\sigma^2 \sim \text{Inv-Gamma}(\alpha_0, \beta_0)$. A distribui√ß√£o posterior conjunta, $Pr(\mu, \sigma^2 | Z)$, √© complexa. No entanto, com Gibbs Sampling, podemos amostrar iterativamente de distribui√ß√µes condicionais:
>
>   1. $Pr(\mu | \sigma^2, Z)$ (a distribui√ß√£o posterior de $\mu$ dada $\sigma^2$ e os dados)
>   2. $Pr(\sigma^2 | \mu, Z)$ (a distribui√ß√£o posterior de $\sigma^2$ dada $\mu$ e os dados)
>
> Estas distribui√ß√µes condicionais costumam ser mais f√°ceis de amostrar do que a distribui√ß√£o posterior conjunta. Este processo iterativo gera amostras que, ap√≥s o *burn-in*, representam a distribui√ß√£o posterior conjunta de $\mu$ e $\sigma^2$.
>
>   *Dados de exemplo*:
>    ```python
>    import numpy as np
>    import matplotlib.pyplot as plt
>    from scipy.stats import norm, invgamma
>
>    # Simula√ß√£o dos dados
>    np.random.seed(42)
>    mu_true = 5
>    sigma_true = 2
>    Z = np.random.normal(mu_true, sigma_true, 100)
>
>    # Priors
>    mu0 = 0
>    tau0 = 10
>    alpha0 = 2
>    beta0 = 2
>
>    # Gibbs Sampling
>    num_iterations = 5000
>    mu_samples = np.zeros(num_iterations)
>    sigma2_samples = np.zeros(num_iterations)
>
>    mu_samples[0] = 0  # Initial values
>    sigma2_samples[0] = 1
>
>    for i in range(1, num_iterations):
>        # Sample mu
>        tau_n = 1 / (1/tau0**2 + len(Z) / sigma2_samples[i-1])
>        mu_n = tau_n * (mu0/tau0**2 + np.sum(Z) / sigma2_samples[i-1])
>        mu_samples[i] = np.random.normal(mu_n, np.sqrt(tau_n))
>
>        # Sample sigma2
>        alpha_n = alpha0 + len(Z)/2
>        beta_n = beta0 + 0.5 * np.sum((Z - mu_samples[i])**2)
>        sigma2_samples[i] = invgamma.rvs(alpha_n, scale=beta_n)
>    
>    # Plotting
>    fig, axs = plt.subplots(2, 1, figsize=(10, 8))
>    axs[0].plot(mu_samples[500:], label=r'$\mu$ samples')
>    axs[0].axhline(y=mu_true, color='r', linestyle='--', label=r'True $\mu$')
>    axs[0].set_ylabel('mu')
>    axs[0].legend()
>
>    axs[1].plot(sigma2_samples[500:], label=r'$\sigma^2$ samples')
>    axs[1].axhline(y=sigma_true**2, color='r', linestyle='--', label=r'True $\sigma^2$')
>    axs[1].set_ylabel('sigma2')
>    axs[1].set_xlabel('Iteration')
>    axs[1].legend()
>
>    plt.tight_layout()
>    plt.show()
>    ```
>  Este c√≥digo ilustra como o Gibbs sampling itera entre amostras de mu e sigma2, convergindo para os verdadeiros valores dos par√¢metros, ap√≥s um per√≠odo de burn-in (neste exemplo descartamos as primeiras 500 amostras).

**Lemma 1:** O Gibbs Sampling gera uma cadeia de Markov cuja distribui√ß√£o estacion√°ria √© a distribui√ß√£o alvo conjunta [^8.6].
*Prova:* A prova deste lemma envolve demonstrar que o n√∫cleo de transi√ß√£o da cadeia de Markov satisfaz a condi√ß√£o de reversibilidade (ou balanceamento detalhado) em rela√ß√£o √† distribui√ß√£o alvo. Em termos pr√°ticos, isso significa que se as vari√°veis aleat√≥rias s√£o sorteadas de acordo com a distribui√ß√£o condicional, a distribui√ß√£o conjunta da cadeia de Markov converge para a distribui√ß√£o conjunta desejada. $\blacksquare$

**Conceito 2: Cadeias de Markov e Distribui√ß√£o Estacion√°ria**

Uma **Cadeia de Markov** √© um processo estoc√°stico que se move de um estado para outro, onde a probabilidade de transi√ß√£o depende apenas do estado atual. O Gibbs Sampling constr√≥i uma cadeia de Markov onde os estados s√£o amostras dos par√¢metros. A **distribui√ß√£o estacion√°ria** de uma cadeia de Markov √© aquela para a qual a distribui√ß√£o do estado atual converge ap√≥s um grande n√∫mero de itera√ß√µes. Para o Gibbs Sampling, essa distribui√ß√£o estacion√°ria √© a distribui√ß√£o posterior de interesse [^8.6].
```mermaid
graph LR
    subgraph "Markov Chain"
        A["Current State: X_t"] --> B["Transition Probability: P(X_{t+1} | X_t)"]
        B --> C["Next State: X_{t+1}"]
        C --> D["Iterative Process"]
        D --> E["Stationary Distribution"]
    end
```
**Corol√°rio 1:** Sob condi√ß√µes de regularidade, a distribui√ß√£o gerada pelo Gibbs Sampling converge para a distribui√ß√£o alvo ap√≥s um per√≠odo de *burn-in*. O *burn-in* √© o per√≠odo inicial de itera√ß√µes descartadas, que serve para que a cadeia se aproxime da distribui√ß√£o estacion√°ria [^8.6].

> üí° **Exemplo Num√©rico:**
>
>  Vamos imaginar uma cadeia de Markov simples onde temos dois estados poss√≠veis: 0 e 1. A matriz de transi√ß√£o √© definida como:
>
>  $$
>    P = \begin{bmatrix}
>        0.7 & 0.3 \\
>        0.4 & 0.6
>    \end{bmatrix}
>  $$
>
>  Onde $P_{ij}$ √© a probabilidade de transi√ß√£o do estado *i* para o estado *j*. Se iniciarmos no estado 0, a sequ√™ncia de estados da cadeia de Markov poderia ser algo como 0, 0, 1, 0, 1, 1, 1, 0, .... Ao longo de muitas itera√ß√µes, a propor√ß√£o de vezes que a cadeia est√° em cada estado converge para a distribui√ß√£o estacion√°ria.
>
>  ```python
>  import numpy as np
>  import matplotlib.pyplot as plt
>
>  P = np.array([[0.7, 0.3], [0.4, 0.6]])
>  initial_state = 0
>  num_iterations = 1000
>
>  states = [initial_state]
>  for _ in range(num_iterations):
>        current_state = states[-1]
>        next_state = np.random.choice([0, 1], p=P[current_state])
>        states.append(next_state)
>  
>  state_0_count = np.cumsum(np.array(states) == 0)
>  state_1_count = np.cumsum(np.array(states) == 1)
>  
>  state_0_prop = state_0_count / np.arange(1, num_iterations + 2)
>  state_1_prop = state_1_count / np.arange(1, num_iterations + 2)
>  
>  plt.figure(figsize=(8, 5))
>  plt.plot(state_0_prop, label='Propor√ß√£o de estado 0')
>  plt.plot(state_1_prop, label='Propor√ß√£o de estado 1')
>  plt.xlabel('Itera√ß√£o')
>  plt.ylabel('Propor√ß√£o de Ocorr√™ncias')
>  plt.title('Converg√™ncia da Cadeia de Markov')
>  plt.legend()
>  plt.grid(True)
>  plt.show()
>  ```
> Este c√≥digo mostra a converg√™ncia das propor√ß√µes de cada estado para a distribui√ß√£o estacion√°ria. No in√≠cio, as propor√ß√µes variam bastante, mas √† medida que o n√∫mero de itera√ß√µes aumenta, elas se estabilizam, ilustrando a ideia de distribui√ß√£o estacion√°ria. Em Gibbs Sampling, o mesmo princ√≠pio se aplica √† converg√™ncia das amostras para a distribui√ß√£o posterior. O burn-in seria um corte inicial para que as amostras representem a distribui√ß√£o estacion√°ria.

**Conceito 3: Algoritmo de Gibbs Sampling**

O algoritmo de Gibbs Sampling para amostragem da distribui√ß√£o conjunta de vari√°veis aleat√≥rias $U_1, U_2,...,U_K$ consiste nos seguintes passos [^8.6]:

1.  Inicializa√ß√£o: Escolher valores iniciais para cada vari√°vel $U_k^{(0)}$, $k = 1, 2, \ldots, K$.
2.  Itera√ß√£o: Para $t = 1, 2, \ldots$:
    -   Para $k = 1, 2, \ldots, K$, gerar um novo valor $U_k^{(t)}$ da distribui√ß√£o condicional $Pr(U_k | U_1^{(t)}, \ldots, U_{k-1}^{(t)}, U_{k+1}^{(t-1)}, \ldots, U_K^{(t-1)})$.
3.  Repeti√ß√£o: Repetir o passo 2 at√© que a distribui√ß√£o conjunta dos $U_k^{(t)}$ n√£o mude significativamente.

```mermaid
graph TB
    subgraph "Gibbs Sampling Algorithm"
        A["Initialization: U_k^(0)"]
        B["Iteration: t = 1, 2, ..."]
        C["Sample: U_k^(t) ~ Pr(U_k | U_{-k})"]
        D["Repeat until Convergence"]
        A --> B
        B --> C
        C --> B
        B --> D
    end
```
> ‚ö†Ô∏è **Nota Importante:** O Gibbs Sampling √© particularmente √∫til quando as distribui√ß√µes condicionais s√£o mais simples de amostrar do que a distribui√ß√£o conjunta.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

Em problemas de classifica√ß√£o, a **regress√£o linear** sobre uma matriz de indicadores pode ser utilizada para modelar a probabilidade de pertencimento a cada classe. A ideia √© codificar as classes com vetores de indicadores e aplicar m√≠nimos quadrados para obter um modelo que prev√™ um valor para cada classe. No entanto, essa abordagem pode sofrer de algumas limita√ß√µes.

```mermaid
graph TD
    A["Indicator Regression"] --> B["Class Encoding"]
    B --> C["Least Squares Fit"]
    C --> D["Class Predictions"]
    D --> E["Decision Rule"]
    E --> F["Classification Results"]
```
**Explica√ß√£o:** Este diagrama ilustra o fluxo do processo de regress√£o de indicadores para a classifica√ß√£o, mostrando desde a codifica√ß√£o das classes at√© a decis√£o final [^4.2].

A regress√£o linear em matriz de indicadores minimiza a soma dos quadrados dos erros entre as previs√µes e os valores dos indicadores de classe. A solu√ß√£o para o vetor de coeficientes (Œ≤) √© dada por:

$$
\hat{\beta} = (H^T H)^{-1} H^T y
$$

Onde *H* √© a matriz de indicadores, e *y* √© o vetor de respostas. A partir das previs√µes, √© poss√≠vel aplicar uma regra de decis√£o (por exemplo, escolher a classe com maior probabilidade) para realizar a classifica√ß√£o.
```mermaid
graph LR
    subgraph "Least Squares Solution"
        A["Œ≤ÃÇ = (H^T H)^-1 H^T y"]
        B["H: Indicator Matrix"]
        C["y: Response Vector"]
        D["H^T: Transpose of H"]
        A -- "depends on" --> B
        A -- "depends on" --> C
        B --> D
        D & B --> A
    end
```

> üí° **Exemplo Num√©rico:**
>
>   Suponha que temos um problema de classifica√ß√£o com 3 classes e 2 features. A matriz de indicadores *H* (com as colunas de indicadores para cada classe) e o vetor de respostas *y* (codificado com 1 para a classe correta e 0 para as outras) podem ser:
>
>   $$
>   H = \begin{bmatrix}
>       1 & 0 & 0 & 1 & 2 \\
>       0 & 1 & 0 & 2 & 1 \\
>       0 & 0 & 1 & 3 & 3 \\
>       1 & 0 & 0 & 4 & 4 \\
>       0 & 1 & 0 & 5 & 5
>   \end{bmatrix}
>  \text{, }
>   y = \begin{bmatrix}
>       1 & 0 & 0 \\
>       0 & 1 & 0 \\
>       0 & 0 & 1 \\
>       1 & 0 & 0 \\
>       0 & 1 & 0
>   \end{bmatrix}
>   $$
>
> Aqui, as 3 primeiras colunas de H representam o encoding one-hot das classes e as duas √∫ltimas as features. O y representa 5 observa√ß√µes, com o n√∫mero 1 indicando a classe correta em cada observa√ß√£o. Para encontrar os coeficientes $\beta$ por m√≠nimos quadrados, primeiro calculamos $H^T H$:
>
> $$
> H^T H = \begin{bmatrix}
> 3 & 0 & 0 & 8 & 10\\
> 0 & 2 & 0 & 7 & 6 \\
> 0 & 0 & 1 & 3 & 3\\
> 8 & 7 & 3 & 55 & 54 \\
> 10 & 6 & 3 & 54 & 55
>  \end{bmatrix}
> $$
>  Ent√£o, calculamos a inversa $(H^T H)^{-1}$ e $H^T y$:
>
> $$
> (H^T H)^{-1} \approx \begin{bmatrix}
>   0.775  &  -0.196 &  -0.020  &  -0.125 &   0.062 \\
>  -0.196  &   0.856 &  -0.012  &  -0.049 &   0.050\\
> -0.020  &  -0.012   & 1.073  &  -0.071 &   -0.071\\
> -0.125 &   -0.049  &  -0.071  &   0.085  &  -0.030\\
>  0.062   &  0.050 &  -0.071  &  -0.030 &   0.073
>  \end{bmatrix}
>  $$
>
> $$
>   H^T y = \begin{bmatrix}
>      2 & 0 & 0 \\
>      0 & 2 & 0 \\
>      0 & 0 & 1 \\
>      5 & 7 & 3 \\
>      6 & 6 & 3
>   \end{bmatrix}
>  $$
>
>  Finalmente, $\hat{\beta} = (H^T H)^{-1} H^T y$:
>
> $$
> \hat{\beta} \approx \begin{bmatrix}
>  0.892 &  -0.492 &  -0.083 \\
> -0.371  &   1.636 &  -0.051 \\
> -0.068 &   -0.034  &   0.856 \\
>  -0.222 &   -0.137 &  -0.148 \\
>  0.070  &  0.114 &   -0.108
>  \end{bmatrix}
> $$
>
> Os coeficientes $\hat{\beta}$ obtidos por m√≠nimos quadrados permitem calcular previs√µes para novas amostras. No entanto, essas previs√µes n√£o s√£o restritas a um intervalo entre 0 e 1, o que pode levar a resultados dif√≠ceis de interpretar como probabilidades.
>
>   ```python
>   import numpy as np
>   from sklearn.linear_model import LinearRegression
>
>   # Dados de exemplo
>   H = np.array([[1, 0, 0, 1, 2],
>                  [0, 1, 0, 2, 1],
>                  [0, 0, 1, 3, 3],
>                  [1, 0, 0, 4, 4],
>                  [0, 1, 0, 5, 5]])
>   y = np.array([[1, 0, 0],
>                  [0, 1, 0],
>                  [0, 0, 1],
>                  [1, 0, 0],
>                  [0, 1, 0]])
>   
>   # Regress√£o linear
>   model = LinearRegression()
>   model.fit(H, y)
>   beta_hat = model.coef_
>   print("Coefficients Beta:\n", beta_hat)
>  
>   # Previs√£o com novos dados
>   new_data = np.array([[1, 0, 0, 2, 2]])
>   predictions = model.predict(new_data)
>   print("Predictions:\n", predictions)
>   ```

**Lemma 2:** A regress√£o linear em matriz de indicadores pode levar a estimativas de probabilidade que n√£o est√£o no intervalo [0,1], dificultando a interpreta√ß√£o das previs√µes como probabilidades verdadeiras [^4.2].
*Prova:* Isso ocorre porque a regress√£o linear n√£o imp√µe restri√ß√µes sobre o intervalo de valores das previs√µes. Em casos onde a covari√¢ncia entre as classes √© alta, as estimativas podem extrapolar para fora desse intervalo. $\blacksquare$

**Corol√°rio 2:** Em alguns cen√°rios, a regress√£o log√≠stica pode ser uma alternativa mais adequada devido √† sua natureza probabil√≠stica, garantindo que as previs√µes estejam sempre entre 0 e 1 [^4.4]. A regress√£o log√≠stica modela a probabilidade de pertencimento a uma classe por meio de uma fun√ß√£o sigmoide.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha entre regress√£o linear em matriz de indicadores e regress√£o log√≠stica depende das caracter√≠sticas dos dados e da necessidade de obter estimativas de probabilidade bem calibradas.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o t√©cnicas cruciais para evitar *overfitting* e melhorar a generaliza√ß√£o de modelos de classifica√ß√£o [^4.5]. Modelos mais complexos podem apresentar alto desempenho nos dados de treinamento, mas podem ter baixo desempenho em novos dados. Regulariza√ß√£o adiciona uma penaliza√ß√£o √† fun√ß√£o de perda, restringindo o crescimento dos coeficientes.

A regulariza√ß√£o L1 (LASSO) e L2 (Ridge) s√£o abordagens comuns [^4.5]:

-   **Regulariza√ß√£o L1:** Adiciona uma penaliza√ß√£o proporcional √† soma dos valores absolutos dos coeficientes. Favorece modelos esparsos (com muitos coeficientes iguais a zero), realizando sele√ß√£o de vari√°veis.
$$
\text{Custo} =  \text{Perda} + \lambda \sum_{j=1}^{p} |\beta_j|
$$
-   **Regulariza√ß√£o L2:** Adiciona uma penaliza√ß√£o proporcional √† soma dos quadrados dos coeficientes. Reduz a magnitude dos coeficientes, evitando que eles cres√ßam muito.
$$
\text{Custo} =  \text{Perda} + \lambda \sum_{j=1}^{p} \beta_j^2
$$
```mermaid
graph LR
    subgraph "Regularization Methods"
        A["L1 Regularization (LASSO)"] --> B["Cost Function:  Loss + Œª * Œ£|Œ≤_j|"]
        C["L2 Regularization (Ridge)"] --> D["Cost Function:  Loss + Œª * Œ£Œ≤_j¬≤"]
        B --> E["Sparsity & Feature Selection"]
        D --> F["Coefficient Shrinkage"]
    end
```
**Explica√ß√£o:** Este diagrama mostra como as regulariza√ß√µes L1 e L2 influenciam as propriedades dos modelos de classifica√ß√£o, como sparsity, interpretabilidade e estabilidade.

> üí° **Exemplo Num√©rico:**
>
>   Vamos usar um problema de regress√£o linear com 10 vari√°veis preditoras, algumas mais importantes que outras. Usaremos um conjunto de dados simulado. O objetivo √© comparar como a regulariza√ß√£o L1 (LASSO) e L2 (Ridge) afetam os coeficientes do modelo.
>
>   ```python
>   import numpy as np
>   import matplotlib.pyplot as plt
>   from sklearn.linear_model import Lasso, Ridge
>   from sklearn.model_selection import train_test_split
>   from sklearn.metrics import mean_squared_error
>
>   # Gerar dados de exemplo
>   np.random.seed(42)
>   n_samples = 100
>   n_features = 10
>   X = np.random.rand(n_samples, n_features)
>   true_coef = np.array([5, -3, 2, 0, 0, 0, 0, 0.5, -1.5, 0]) # apenas 5 features s√£o relevantes
>   y = X @ true_coef + np.random.normal(0, 1, n_samples)
>   
>   # Divis√£o em treino e teste
>   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
>
>   # Regulariza√ß√£o L1 (LASSO)
>   lasso = Lasso(alpha=0.1)
>   lasso.fit(X_train, y_train)
>   y_pred_lasso = lasso.predict(X_test)
>   mse_lasso = mean_squared_error(y_test, y_pred_lasso)
>
>   # Regulariza√ß√£o L2 (Ridge)
>   ridge = Ridge(alpha=1)
>   ridge.fit(X_train, y_train)
>   y_pred_ridge = ridge.predict(X_test)
>   mse_ridge = mean_squared_error(y_test, y_pred_ridge)
>
>   # OLS sem regulariza√ß√£o
>   ols = LinearRegression()
>   ols.fit(X_train,y_train)
>   y_pred_ols = ols.predict(X_test)
>   mse_ols = mean_squared_error(y_test, y_pred_ols)
>
>   # Plot
>   plt.figure(figsize=(10, 6))
>   plt.plot(true_coef, 'bo-', label='True Coefficients', markersize=4)
>   plt.plot(lasso.coef_, 'ro-', label='LASSO Coefficients', markersize=4)
>   plt.plot(ridge.coef_, 'go-', label='Ridge Coefficients', markersize=4)
>   plt.plot(ols.coef_, 'co-', label='OLS Coefficients', markersize=4)
>   plt.xlabel('Feature Index')
>   plt.ylabel('Coefficient Value')
>   plt.title('Coefficients Comparison')
>   plt.legend()
>   plt.grid(True)
>   plt.show()
>
>    # Resultados
>   print(f"MSE OLS: {mse_ols:.4f}")
>   print(f"MSE LASSO: {mse_lasso:.4f}")
>   print(f"MSE Ridge: {mse_ridge:.4f}")
>    
>   # Comparison table
>   print("\nComparison Table:")
>   print("| Method | MSE      | # Non-Zero Coefs |")
>   print("|--------|----------|-----------------|")
>   print(f"| OLS    | {mse_ols:.4f} | {np.sum(ols.coef_ != 0)}             |")
>   print(f"| LASSO  | {mse_lasso:.4f} | {np.sum(lasso.coef_ != 0)}             |")
>   print(f"| Ridge  | {mse_ridge:.4f} | {np.sum(ridge.coef_ != 0)}            |")
>   ```
>
> Este exemplo mostra que o LASSO (L1) zera os coeficientes de vari√°veis menos importantes, realizando sele√ß√£o de vari√°veis. O Ridge (L2), por outro lado, reduz a magnitude dos coeficientes, mas n√£o os zera, ajudando a estabilizar o modelo. A tabela mostra como cada m√©todo afeta o MSE e a esparsidade dos coeficientes, evidenciando os benef√≠cios da regulariza√ß√£o.

**Lemma 3:** A penaliza√ß√£o L1 em modelos de classifica√ß√£o log√≠stica promove a esparsidade dos coeficientes, o que √© √∫til para a sele√ß√£o de vari√°veis e para a interpreta√ß√£o do modelo [^4.4.4].
*Prova:* A penaliza√ß√£o L1 √© n√£o diferenci√°vel no zero, o que for√ßa alguns coeficientes a serem exatamente zero quando otimizados, ao contr√°rio da L2, que apenas os reduz a pequenos valores. $\blacksquare$

**Corol√°rio 3:**  A combina√ß√£o de L1 e L2 (Elastic Net) oferece um meio de equilibrar a esparsidade com a estabilidade, permitindo a sele√ß√£o de vari√°veis e o controle da magnitude dos coeficientes [^4.5].

> ‚úîÔ∏è **Destaque**: A regulariza√ß√£o √© essencial para melhorar a generaliza√ß√£o e a interpretabilidade dos modelos de classifica√ß√£o, e a escolha da penaliza√ß√£o (L1, L2 ou Elastic Net) deve ser feita com base nos objetivos do modelo.

### Separating Hyperplanes e Perceptrons

Um **hiperplano separador** √© uma estrutura que divide um espa√ßo de dados em regi√µes distintas, com o objetivo de separar as classes em um problema de classifica√ß√£o [^4.5.2]. Em sua forma mais simples, um hiperplano separador √© definido por uma equa√ß√£o linear da forma:

$$
w^T x + b = 0
$$
```mermaid
graph LR
    subgraph "Separating Hyperplane"
        A["Equation: w^T x + b = 0"]
        B["w: Weight Vector"]
        C["x: Feature Vector"]
        D["b: Bias"]
         A -- "depends on" --> B
         A -- "depends on" --> C
        A -- "depends on" --> D
    end
```

Onde *w* √© o vetor de pesos, *x* √© o vetor de caracter√≠sticas e *b* √© o vi√©s. O problema de encontrar o melhor hiperplano separador √© um problema de otimiza√ß√£o, frequentemente resolvido utilizando t√©cnicas como a programa√ß√£o linear ou o m√©todo de *Support Vector Machines (SVM)*.

**Perceptrons** s√£o uma forma inicial de redes neurais artificiais que aprendem a separar dados linearmente separ√°veis. Eles atualizam os pesos atrav√©s de um processo iterativo, com o objetivo de encontrar um hiperplano que separa as classes corretamente [^4.5.1].

> üí° **Exemplo Num√©rico:**
>
>   Vamos considerar um exemplo simples em duas dimens√µes onde temos dois conjuntos de pontos, que representam duas classes, e que podem ser separados por uma linha reta (um hiperplano em 2D). O objetivo √© encontrar os par√¢metros do perceptron (pesos w e vi√©s b) para separar as classes:
>
>  ```python
>   import numpy as np
>   import matplotlib.pyplot as plt
>
>   # Dados de exemplo
>   X = np.array([[1, 1], [2, 2], [2, 0], [3, 1], [0, 0], [1, -1], [0,-2], [-1, -1]])
>   y = np.array([1, 1, 1, 1, -1, -1, -1, -1])
>
>   # Inicializa√ß√£o do Perceptron
>   w = np.array([0.1, -0.2])
>   b = 0
>   learning_rate = 0.1
>   epochs = 50
>
>   # Fun√ß√£o para previs√£o
>   def predict(x, w, b):
>     return 1 if np.dot(w, x) + b > 0 else -1
>
>   # Perceptron algorithm
>   for epoch in range(epochs):
>     for i, x in enumerate(X):
>       y_hat = predict(x, w, b)
>       if y_hat != y[i]:
>         w = w + learning_rate * y[i] * x
>         b = b + learning_rate * y[i]
>
>   # Plot
>   plt.figure(figsize=(8, 6))
>   plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', label='Class 1')
>   plt.scatter(X[y == -1, 0], X[y == -1, 1], color='red', label='Class -1')
>   
>   x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
>   y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
>   xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
>                        np.arange(y_min, y_max, 0.02))
>   Z = np.array([predict(np.array([x,y]), w, b) for x, y in np.c_[xx.ravel(), yy.ravel()]])
>   Z = Z.reshape(xx.shape)
>   plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.6)
>
>   plt.xlabel('Feature 1')
>   plt.ylabel('Feature 2')
>   plt.title('Perceptron Separating Hyperplane')
>   plt.legend()
>   plt.grid(True)
>   plt.show()
>
>   print("Final weights:", w)
>   print("Final bias:", b)
> ```
> Este c√≥digo implementa o algoritmo do perceptron e ilustra a converg√™ncia dos pesos e o vi√©s para a obten√ß√£o de um hiperplano separador. Os dados s√£o inicialmente separados por uma linha reta (o hiperplano em 2D), e o perceptron ajusta iterativamente os pesos para garantir que os pontos de classes diferentes estejam em lados opostos da linha.

**Lemma 4:** O perceptron converge para um hiperplano separador (se este existir) em um n√∫mero finito de passos para dados linearmente separ√°veis [^4.5.1].
*Prova:* O algoritmo do perceptron garante converg√™ncia quando os dados s√£o linearmente separ√°veis, pois as atualiza√ß√µes dos pesos garantem um aumento progressivo da separa√ß√£o entre as classes. $\blacksquare$

**Corol√°rio 4:** Em dados n√£o linearmente separ√°veis, o perceptron n√£o garante converg√™ncia e pode entrar em um ciclo de atualiza√ß√µes de pesos sem convergir para uma solu√ß√£o [^4.5.1].
```mermaid
graph LR
    subgraph "Perceptron Convergence"
        A["Linearly Separable Data"] --> B["Perceptron Converges"]
        C["Non-Linearly Separable Data"] --> D["Perceptron May Not Converge"]
    end
```

### Pergunta Te√≥rica Avan√ßada: Como o Gibbs Sampling se relaciona com a Distribui√ß√£o Posterior e o *Burn-in*?

**Resposta:**
O Gibbs Sampling √© uma t√©cnica MCMC que visa obter amostras da distribui√ß√£o posterior (alvo) de um modelo Bayesiano. O algoritmo iterativamente amostra cada par√¢metro condicionado aos valores atuais dos outros par√¢metros, formando uma cadeia de Markov. A converg√™ncia para a distribui√ß√£o posterior √© um conceito chave.

A distribui√ß√£o estacion√°ria da cadeia de Markov gerada pelo Gibbs Sampling √© igual √† distribui√ß√£o posterior alvo, e o per√≠odo de *burn-in* √© essencial para que a cadeia