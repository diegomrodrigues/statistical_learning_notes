## Gibbs Sampling Behavior

```mermaid
graph LR
    A["In√≠cio"] --> B{"Inicializar vari√°veis"}
    B --> C{"Amostragem condicional de U1"}
    C --> D{"Amostragem condicional de U2"}
    D --> E{...}
    E --> F{"Amostragem condicional de UK"}
    F --> G{"Verificar converg√™ncia"}
    G -- "N√£o" --> C
    G -- "Sim" --> H["Fim (Distribui√ß√£o Posterior)"]
    style C fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#f9f,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Este cap√≠tulo explora o **Gibbs sampling**, um m√©todo fundamental de **Markov Chain Monte Carlo (MCMC)** para amostragem da distribui√ß√£o posterior em infer√™ncia Bayesiana [^8.6]. O Gibbs sampling √© uma t√©cnica iterativa onde amostras s√£o geradas sequencialmente a partir das distribui√ß√µes condicionais completas, levando a uma amostra da distribui√ß√£o conjunta [^8.6]. Este m√©todo √© particularmente √∫til quando a distribui√ß√£o conjunta √© complexa, mas as distribui√ß√µes condicionais s√£o mais trat√°veis. Exploraremos a base te√≥rica, os mecanismos e a rela√ß√£o com outros m√©todos, como o EM.

### Conceitos Fundamentais

**Conceito 1: O Problema da Amostragem da Distribui√ß√£o Posterior**

A infer√™ncia Bayesiana requer a amostragem da distribui√ß√£o posterior, $$P(\theta | Z)$$, onde $$\theta$$ s√£o os par√¢metros e $$Z$$ s√£o os dados. Em geral, calcular esta distribui√ß√£o analiticamente √© dif√≠cil. M√©todos MCMC, como o Gibbs sampling, fornecem uma abordagem computacional para gerar amostras desta distribui√ß√£o [^8.6]. A ideia central √© construir uma cadeia de Markov cuja distribui√ß√£o estacion√°ria seja a distribui√ß√£o posterior desejada. O Gibbs sampling √© um caso especial de MCMC onde amostras s√£o obtidas iterativamente a partir das distribui√ß√µes condicionais completas.

**Lemma 1:** Se a cadeia de Markov constru√≠da pelo Gibbs sampling √© irredut√≠vel e aperi√≥dica, ent√£o a sua distribui√ß√£o estacion√°ria √© a distribui√ß√£o alvo.
*Prova:* A prova formal envolve o uso do teorema da converg√™ncia de cadeias de Markov, mostrando que sob as condi√ß√µes de irredutibilidade e aperiodicidade, a distribui√ß√£o das amostras geradas converge para a distribui√ß√£o alvo [^8.6]. $\blacksquare$
```mermaid
graph LR
    subgraph "Markov Chain Convergence"
        A["Cadeia de Markov"] --> B["Irredutibilidade"]
        A --> C["Aperiodicidade"]
        B & C --> D["Distribui√ß√£o Estacion√°ria = Distribui√ß√£o Alvo"]
    end
```

**Conceito 2: O Mecanismo do Gibbs Sampling**

O Gibbs sampling gera amostras iterativamente a partir da distribui√ß√£o conjunta de vari√°veis aleat√≥rias $$U_1, U_2, \ldots, U_K$$. Em cada itera√ß√£o, uma vari√°vel $$U_k$$ √© amostrada da sua distribui√ß√£o condicional, dados os valores mais recentes de todas as outras vari√°veis $$U_{-k}$$, i.e., $$P(U_k | U_{-k})$$. O processo continua sequencialmente para cada vari√°vel at√© que se atinja converg√™ncia, que √© quando a distribui√ß√£o conjunta converge para uma distribui√ß√£o estacion√°ria que √© uma amostra da distribui√ß√£o posterior de interesse [^8.6].
Formalmente, no instante $$t$$, para $$k=1, 2, \ldots, K$$, gera-se $$U_k^{(t)}$$ de $$P(U_k | U_1^{(t)}, \ldots, U_{k-1}^{(t)}, U_{k+1}^{(t-1)}, \ldots, U_K^{(t-1)})$$.

> üí° **Exemplo Num√©rico:** Vamos considerar um caso simplificado com duas vari√°veis,  $$U_1$$ e $$U_2$$. Suponha que as distribui√ß√µes condicionais sejam:
>
> $$P(U_1 | U_2) = \mathcal{N}(U_2, 1)$$
>
> $$P(U_2 | U_1) = \mathcal{N}(0.5U_1, 0.5)$$
>
> Inicializamos com $$U_1^{(0)} = 2$$ e $$U_2^{(0)} = 1$$.
>
> *   **Itera√ß√£o 1:**
>
>     *   Amostramos $$U_1^{(1)}$$ de  $$P(U_1 | U_2^{(0)} = 1)$$.  Suponha que  $$U_1^{(1)} = 1.5$$.
>     *   Amostramos $$U_2^{(1)}$$ de  $$P(U_2 | U_1^{(1)} = 1.5)$$. Suponha que $$U_2^{(1)} = 0.9$$.
> *   **Itera√ß√£o 2:**
>     *   Amostramos $$U_1^{(2)}$$ de $$P(U_1 | U_2^{(1)} = 0.9)$$. Suponha que $$U_1^{(2)} = 1.1$$.
>     *  Amostramos $$U_2^{(2)}$$ de $$P(U_2 | U_1^{(2)} = 1.1)$$. Suponha que $$U_2^{(2)} = 0.6$$.
>
> Este processo continua iterativamente. Ap√≥s um n√∫mero suficiente de itera√ß√µes (burn-in), as amostras ($$U_1^{(t)}$$, $$U_2^{(t)}$$)  aproximam a distribui√ß√£o conjunta alvo. Este exemplo ilustra como o Gibbs sampling atualiza cada vari√°vel usando os valores mais recentes das outras.
>
> ```mermaid
> graph LR
>     A[Inicializar U1, U2] --> B(Amostrar U1 | U2)
>     B --> C(Amostrar U2 | U1)
>     C --> D{Pr√≥xima itera√ß√£o?}
>     D -- Sim --> B
>     D -- N√£o --> E[Fim]
>     style B fill:#f9f,stroke:#333,stroke-width:2px
>     style C fill:#ccf,stroke:#333,stroke-width:2px
> ```

**Corol√°rio 1:** Em modelos onde as distribui√ß√µes condicionais completas s√£o amostr√°veis, o Gibbs sampling produz amostras da distribui√ß√£o conjunta. A facilidade de implementa√ß√£o depende da forma da condicional, que √© crucial para a aplicabilidade do m√©todo.
*Prova:* A prova deste corol√°rio se baseia na propriedade de que se amostras s√£o geradas repetidamente das condicionais completas, a distribui√ß√£o conjunta das amostras converge para a distribui√ß√£o alvo [^8.6]. $\blacksquare$
```mermaid
graph LR
    subgraph "Gibbs Sampling Process"
        A["Distribui√ß√µes Condicionais Completas"] --> B["Amostragem Iterativa"]
        B --> C["Converg√™ncia para Distribui√ß√£o Conjunta"]
    end
```

**Conceito 3: Converg√™ncia e Burn-in**

As amostras iniciais geradas pelo Gibbs sampling, antes de atingir a converg√™ncia da cadeia de Markov, n√£o devem ser usadas para infer√™ncia, pois s√£o influenciadas pelas condi√ß√µes iniciais [^8.6]. Esse per√≠odo inicial √© chamado de *burn-in*. Ap√≥s o *burn-in*, as amostras geradas podem ser usadas para aproximar a distribui√ß√£o posterior, estimar a m√©dia posterior e outras quantidades de interesse [^8.6].
> ‚ö†Ô∏è **Nota Importante**: A determina√ß√£o do tamanho do *burn-in* √© crucial e depende da natureza do problema e da complexidade da distribui√ß√£o alvo. √â geralmente feita por inspe√ß√£o visual das amostras ou utilizando m√©todos de diagn√≥stico de converg√™ncia.
> ‚ùó **Ponto de Aten√ß√£o**: A escolha de valores iniciais pode afetar a velocidade de converg√™ncia do algoritmo. √â aconselh√°vel usar valores iniciais razo√°veis ou executar o algoritmo com m√∫ltiplos valores iniciais para garantir uma amostra representativa da distribui√ß√£o posterior.
> ‚úîÔ∏è **Destaque**: Ap√≥s o burn-in, as amostras geradas podem ser utilizadas para estimar quantidades de interesse, como a m√©dia posterior, vari√¢ncia, e intervalos de credibilidade.

### Gibbs Sampling para Modelos Gaussianos

```mermaid
graph LR
    A["Dados Observados (Y)"] --> B["Amostragem de Vari√°veis Indicadoras (Œî)"]
    B --> C["Amostragem da M√©dia (Œº1)"]
    C --> D["Amostragem da M√©dia (Œº2)"]
    D --> E{"Converg√™ncia?"}
    E -- "N√£o" --> B
    E -- "Sim" --> F["Estimativa Posterior"]
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ddf,stroke:#333,stroke-width:2px
```

**Explica√ß√£o:** Este diagrama ilustra o fluxo do Gibbs sampling para um modelo gaussiano com dois componentes.

Considere um modelo gaussiano com dois componentes onde $$Z$$ representa os dados observados e $$\theta$$ os par√¢metros do modelo. O Gibbs sampling pode ser usado para obter amostras da distribui√ß√£o posterior dos par√¢metros, dados os dados. Para este exemplo, assuma que temos as seguintes vari√°veis aleat√≥rias:
- $$Y_i$$: Dados observados.
- $$\Delta_i \in \{0,1\}$$: Vari√°vel indicadora para o componente gaussiano.
- $$\mu_1$$: M√©dia do primeiro componente.
- $$\mu_2$$: M√©dia do segundo componente.
As condicionais para o Gibbs Sampling ser√£o as seguintes:
1. Amostrar $$\Delta_i$$ da distribui√ß√£o condicional completa $$P(\Delta_i | \mu_1, \mu_2, Y_i)$$, que se relaciona com a probabilidade de cada ponto de dado pertencer a cada uma das distribui√ß√µes Gaussianas.
2. Amostrar $$\mu_1$$ da distribui√ß√£o condicional completa $$P(\mu_1 | \Delta_i, Y_i)$$, que √© uma gaussiana cuja m√©dia e vari√¢ncia dependem dos pontos associados ao primeiro componente.
3. Amostrar $$\mu_2$$ da distribui√ß√£o condicional completa $$P(\mu_2 | \Delta_i, Y_i)$$, similar √† etapa anterior, mas para o segundo componente.
O Gibbs sampling alterna essas etapas iterativamente, usando os valores amostrados mais recentemente para gerar as pr√≥ximas amostras [^8.6].

> üí° **Exemplo Num√©rico:** Suponha que temos 5 dados observados: $$Y = [1, 2, 8, 9, 10]$$. Inicializamos  $$\mu_1 = 2$$ e $$\mu_2 = 9$$. As condicionais completas s√£o assumidas gaussianas para simplicidade:
>
> $$P(\Delta_i = 1 | \mu_1, \mu_2, Y_i) = \frac{\mathcal{N}(Y_i | \mu_1, \sigma^2)}{\mathcal{N}(Y_i | \mu_1, \sigma^2) + \mathcal{N}(Y_i | \mu_2, \sigma^2)}$$
>
> $$P(\Delta_i = 0 | \mu_1, \mu_2, Y_i) = 1 - P(\Delta_i = 1 | \mu_1, \mu_2, Y_i)$$
>
> $$P(\mu_1 | \Delta, Y) = \mathcal{N}\left( \frac{\sum_{i: \Delta_i=1} Y_i}{n_1}, \frac{\sigma^2}{n_1} \right)$$
>
> $$P(\mu_2 | \Delta, Y) = \mathcal{N}\left( \frac{\sum_{i: \Delta_i=0} Y_i}{n_2}, \frac{\sigma^2}{n_2} \right)$$
>
> onde $$n_1$$ √© o n√∫mero de pontos associados ao componente 1 e $$n_2$$ ao componente 2, e $$\sigma^2 = 1$$.
>
> *   **Itera√ß√£o 1:**
>     *   **Amostrar** $$\Delta_i$$. Para $$Y_1 = 1$$, $$P(\Delta_1 = 1) = \frac{e^{-0.5(1-2)^2}}{e^{-0.5(1-2)^2} + e^{-0.5(1-9)^2}} \approx 1$$. Ent√£o,  $$\Delta_1 = 1$$. Analogamente, para $$Y_2 = 2$$, $$\Delta_2 = 1$$, para $$Y_3 = 8$$, $$\Delta_3 = 0$$, para $$Y_4 = 9$$, $$\Delta_4 = 0$$, e para $$Y_5 = 10$$, $$\Delta_5 = 0$$.  Assim, $$\Delta = [1, 1, 0, 0, 0]$$.
>     *   **Amostrar**  $$\mu_1$$: $$P(\mu_1 | \Delta, Y) = \mathcal{N}\left(\frac{1+2}{2}, \frac{1}{2} \right) = \mathcal{N}(1.5, 0.5)$$. Suponha que  $$\mu_1 = 1.7$$.
>     *   **Amostrar**  $$\mu_2$$: $$P(\mu_2 | \Delta, Y) = \mathcal{N}\left(\frac{8+9+10}{3}, \frac{1}{3} \right) = \mathcal{N}(9, 0.33)$$. Suponha que $$\mu_2 = 8.8$$.
>
> *   **Itera√ß√£o 2:**
>     *   **Amostrar** $$\Delta_i$$ usando os valores atualizados de $$\mu_1$$ e $$\mu_2$$, por exemplo, para  $$Y_1$$,  $$P(\Delta_1 = 1) \approx 0.98$$, logo $$\Delta_1 = 1$$.  O processo √© repetido para cada ponto de dado.
>     *   **Amostrar** $$\mu_1$$ usando os novos valores de $$\Delta$$ e $$Y$$.
>     *    **Amostrar** $$\mu_2$$ usando os novos valores de $$\Delta$$ e $$Y$$.
>
> O algoritmo continua iterativamente, atualizando os valores das vari√°veis e convergindo para a distribui√ß√£o posterior.

**Lemma 2:** A converg√™ncia do Gibbs sampling em modelos gaussianos pode ser assegurada se as condicionais completas s√£o bem definidas e as cadeias de Markov associadas s√£o irredut√≠veis e aperi√≥dicas.
*Prova:* Para modelos Gaussianos, as condicionais completas geralmente s√£o gaussianas ou outras distribui√ß√µes bem conhecidas, o que simplifica a verifica√ß√£o das condi√ß√µes de converg√™ncia do Gibbs sampling [^8.6]. $\blacksquare$
```mermaid
graph LR
 subgraph "Gaussian Model Convergence"
    A["Distribui√ß√µes Condicionais Gaussianas"] --> B["Cadeias de Markov Irredut√≠veis e Aperi√≥dicas"]
    B --> C["Converg√™ncia do Gibbs Sampling"]
 end
```

**Corol√°rio 2:** Em um modelo com dois componentes gaussianos, as amostras geradas por Gibbs Sampling convergem para a distribui√ß√£o posterior conjunta de $$\mu_1$$ e $$\mu_2$$ dado os dados. O m√©todo requer que a amostragem de cada um dos par√¢metros nas condicionais seja f√°cil.
*Prova:* A converg√™ncia segue da aplica√ß√£o do teorema de converg√™ncia de cadeias de Markov e das propriedades das condicionais gaussianas. $\blacksquare$

### Gibbs Sampling e o Algoritmo EM

H√° uma estreita rela√ß√£o entre o Gibbs sampling e o algoritmo EM em modelos da fam√≠lia exponencial [^8.6, 8.5.2]. O algoritmo EM busca um ponto m√°ximo na distribui√ß√£o posterior, enquanto o Gibbs sampling busca amostrar a distribui√ß√£o posterior. Em modelos como mixture models, o algoritmo EM alterna passos de Expectation (E) e Maximization (M). No passo E, calculamos a esperan√ßa das vari√°veis latentes, enquanto no passo M maximizamos a fun√ß√£o de verossimilhan√ßa usando essas expectativas [^8.5]. O Gibbs sampling simula as vari√°veis latentes em vez de calcular as esperan√ßas, e simula os par√¢metros do modelo dado as vari√°veis latentes.
> ‚ö†Ô∏è **Ponto Crucial**: Ambos os m√©todos lidam com a complexidade da fun√ß√£o de verossimilhan√ßa atrav√©s da introdu√ß√£o de vari√°veis latentes. Enquanto o EM maximiza a verossimilhan√ßa, o Gibbs sampling amostra a partir da distribui√ß√£o posterior.
> ‚ùó **Ponto de Aten√ß√£o**: No contexto de modelos com vari√°veis latentes como mixture models, o Gibbs sampling amostragem iterativamente as vari√°veis latentes e par√¢metros do modelo, ao passo que o EM calcula o valor esperado das vari√°veis latentes e maximiza o likelihood dado estas esperan√ßas.
```mermaid
graph LR
    subgraph "EM Algorithm"
        A["Inicializar par√¢metros"] --> B["Passo E: Calcular Esperan√ßa das Vari√°veis Latentes"]
        B --> C["Passo M: Maximizar a Verossimilhan√ßa"]
        C --> D{"Converg√™ncia?"}
        D -- "N√£o" --> B
        D -- "Sim" --> E["Estimativa de M√°xima Verossimilhan√ßa"]
    end
    subgraph "Gibbs Sampling"
        F["Inicializar par√¢metros"] --> G["Amostrar Vari√°veis Latentes e Par√¢metros"]
         G --> H{"Converg√™ncia?"}
         H -- "N√£o" --> G
         H -- "Sim" --> I["Amostras da Distribui√ß√£o Posterior"]
    end
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

### Perguntas Te√≥ricas Avan√ßadas

**Pergunta 1: Quais as condi√ß√µes te√≥ricas que garantem a converg√™ncia do Gibbs sampling para a distribui√ß√£o posterior desejada?**
**Resposta:** A converg√™ncia do Gibbs sampling √© garantida se a cadeia de Markov subjacente √© irredut√≠vel e aperi√≥dica [^8.6]. A irredutibilidade significa que qualquer ponto no espa√ßo de par√¢metros pode ser atingido em um n√∫mero finito de etapas. A aperiodicidade impede que a cadeia se torne c√≠clica. Em cen√°rios pr√°ticos, estas condi√ß√µes podem ser verificadas atrav√©s da an√°lise da natureza das distribui√ß√µes condicionais completas e do comportamento da cadeia ao longo das itera√ß√µes [^8.6].

**Lemma 3**: Se as distribui√ß√µes condicionais completas s√£o tais que as cadeias de Markov associadas s√£o irredut√≠veis e aperi√≥dicas, ent√£o a distribui√ß√£o das amostras geradas pelo Gibbs sampling convergir√£o para a distribui√ß√£o posterior alvo.
*Prova:* Esta afirma√ß√£o √© derivada do teorema da converg√™ncia de cadeias de Markov e requer que todas as condicionais sejam tais que garantam que qualquer parte do espa√ßo de par√¢metros possa ser acessada. $\blacksquare$
```mermaid
graph LR
    subgraph "Convergence of Gibbs Sampling"
        A["Distribui√ß√µes Condicionais Completas"] --> B["Cadeia de Markov Irredut√≠vel"]
        A --> C["Cadeia de Markov Aperi√≥dica"]
        B & C --> D["Distribui√ß√£o das Amostras Converge para a Distribui√ß√£o Posterior Alvo"]
    end
```

**Pergunta 2: Como o *burn-in* impacta a infer√™ncia estat√≠stica e como ele deve ser determinado na pr√°tica?**

**Resposta:** O *burn-in* corresponde a um per√≠odo inicial de itera√ß√µes do Gibbs sampling onde a influ√™ncia das condi√ß√µes iniciais ainda √© significativa [^8.6]. Incluir estas amostras na an√°lise final pode levar a estimativas viesadas e incorretas. A determina√ß√£o do *burn-in* deve ser feita atrav√©s da an√°lise visual dos gr√°ficos das amostras, procurando por um per√≠odo onde a cadeia parece ter atingido uma distribui√ß√£o estacion√°ria, ou atrav√©s de m√©todos de diagn√≥stico de converg√™ncia, como o fator de redu√ß√£o de escala, que indica quando cadeias m√∫ltiplas convergem para uma mesma distribui√ß√£o [^8.6].

> üí° **Exemplo Num√©rico:** Suponha que ao executar o Gibbs Sampling em um problema espec√≠fico, visualizamos o hist√≥rico das amostras de um par√¢metro $$\theta$$:
>
> ```mermaid
>   graph LR
>       A[Inicializa√ß√£o] --> B(Itera√ß√µes 1-100)
>       B --> C(Itera√ß√µes 101-500)
>       C --> D(Itera√ß√µes 501-1000)
>       style B fill:#f9f,stroke:#333,stroke-width:2px
>       style C fill:#ccf,stroke:#333,stroke-width:2px
> ```
>
> As amostras de $$\theta$$ nas itera√ß√µes 1-100 mostram uma alta variabilidade e tend√™ncia clara, indicando que o algoritmo ainda est√° se ajustando. Entre as itera√ß√µes 101-500, a variabilidade diminui e a tend√™ncia desaparece. Ap√≥s a itera√ß√£o 500, a cadeia parece ter estabilizado em torno de um valor espec√≠fico com flutua√ß√µes aleat√≥rias, indicando que a fase de *burn-in* pode ser considerada at√© a itera√ß√£o 500. As amostras ap√≥s a itera√ß√£o 500 ser√£o usadas para estimar as propriedades da distribui√ß√£o posterior.

**Corol√°rio 3**: Amostras geradas durante o *burn-in* n√£o devem ser usadas para infer√™ncia, dado que n√£o representam a distribui√ß√£o alvo, mas sim um estado de transi√ß√£o da cadeia de Markov.
*Prova:* Esta √© uma decorr√™ncia direta da defini√ß√£o de *burn-in* e das propriedades de converg√™ncia da cadeia de Markov [^8.6]. $\blacksquare$

### Conclus√£o

O Gibbs sampling √© uma t√©cnica poderosa para amostrar da distribui√ß√£o posterior em modelos complexos, particularmente em modelos onde as condicionais completas s√£o conhecidas e amostr√°veis. Este m√©todo, fundamentado na teoria de cadeias de Markov, oferece um meio eficaz de infer√™ncia estat√≠stica no contexto Bayesiano. As amostras geradas ap√≥s o *burn-in* representam a distribui√ß√£o posterior, permitindo inferir sobre os par√¢metros do modelo de maneira precisa.

### Footnotes
[^8.1]: "For most of this book, the fitting (learning) of models has been achieved by minimizing a sum of squares for regression, or by minimizing cross-entropy for classification. In fact, both of these minimizations are instances of the maximum likelihood approach to fitting." *(Trecho de <Model Inference and Averaging>)*
[^8.2]: "Denote the training data by Z = {z1, Z2,...,zN}, with zi = (xi, Yi), i = 1, 2, ..., N. Here xi is a one-dimensional input, and y‚ÇÅ the outcome, either continuous or categorical." *(Trecho de <Model Inference and Averaging>)*
[^8.3]: "Suppose we decide to fit a cubic spline to the data, with three knots placed at the quartiles of the X values. This is a seven-dimensional lin-ear space of functions, and can be represented, for example, by a linear expansion of B-spline basis functions (see Section 5.9.2):" *(Trecho de <Model Inference and Averaging>)*
[^8.4]: "Here the hj(x), j = 1, 2, ..., 7 are the seven functions shown in the right panel of Figure 8.1. We can think of Œº(x) as representing the conditional mean E(Y|X = x)." *(Trecho de <Model Inference and Averaging>)*
[^8.5]: "The bootstrap method described above, in which we sample with re-placement from the training data, is called the nonparametric bootstrap." *(Trecho de <Model Inference and Averaging>)*
[^8.6]: "Having defined a Bayesian model, one would like to draw samples from the resulting posterior distribution, in order to make inferences about the parameters. Except for simple models, this is often a difficult computational problem. In this section we discuss the Markov chain Monte Carlo (MCMC) approach to posterior sampling. We will see that Gibbs sampling, an MCMC procedure, is closely related to the EM algorithm: the main dif-ference is that it samples from the conditional distributions rather than maximizing over them." *(Trecho de <Model Inference and Averaging>)*
[^8.7]: "There is a close connection between Gibbs sampling from a posterior and the EM algorithm in exponential family models." *(Trecho de <Model Inference and Averaging>)*
[^8.5.2]: "The above procedure is an example of the EM (or Baum-Welch) algorithm for maximizing likelihoods in certain classes of problems. These problems are ones for which maximization of the likelihood is difficult, but made easier by enlarging the sample with latent (unobserved) data." *(Trecho de <Model Inference and Averaging>)*
<!-- END DOCUMENT -->
