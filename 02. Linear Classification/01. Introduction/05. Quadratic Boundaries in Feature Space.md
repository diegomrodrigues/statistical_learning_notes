## T√≠tulo Conciso: M√©todos Lineares para Classifica√ß√£o: Fronteiras Quadr√°ticas Atrav√©s de Espa√ßos Aumentados

```mermaid
graph LR
    subgraph "Linear Classifiers in Augmented Spaces"
        direction TB
        A["Original Feature Space (X)"] --> B["Non-linear Transformation h(X)"]
        B --> C["Augmented Feature Space h(X)"]
        C --> D["Linear Classifier in h(X)"]
        D --> E["Quadratic Decision Boundary in X"]
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora como m√©todos lineares podem gerar **fronteiras de decis√£o quadr√°ticas** no espa√ßo de atributos original atrav√©s da constru√ß√£o de **fronteiras lineares em espa√ßos aumentados**. A ideia central √© que, ao expandir o espa√ßo de atributos original por meio de transforma√ß√µes n√£o lineares, como a adi√ß√£o de termos polinomiais e de intera√ß√£o, √© poss√≠vel aplicar um classificador linear no espa√ßo expandido e obter um classificador com fronteiras n√£o lineares no espa√ßo original. Essa abordagem oferece uma forma eficaz de lidar com problemas de classifica√ß√£o que n√£o s√£o linearmente separ√°veis no espa√ßo de atributos original [^4.1].

A expans√£o do espa√ßo de atributos, tamb√©m conhecida como *feature engineering*, permite que os m√©todos lineares, intrinsecamente limitados a separa√ß√µes lineares, possam modelar rela√ß√µes mais complexas entre os atributos e as classes. A escolha das transforma√ß√µes apropriadas √© um aspecto crucial desse processo, pois ela determina a forma da fronteira de decis√£o no espa√ßo original. Neste cap√≠tulo, vamos detalhar como diferentes transforma√ß√µes, como termos polinomiais e intera√ß√µes entre atributos, levam a diferentes formas de fronteiras de decis√£o no espa√ßo original.

Al√©m disso, vamos discutir as vantagens e as limita√ß√µes desta abordagem. Embora a expans√£o do espa√ßo de atributos permita obter modelos mais flex√≠veis, ela tamb√©m pode aumentar a complexidade do modelo e o risco de sobreajuste. Vamos analisar como t√©cnicas de regulariza√ß√£o podem ser utilizadas para mitigar esses problemas. Ao longo do cap√≠tulo, vamos explorar como a combina√ß√£o de transforma√ß√µes n√£o lineares e m√©todos lineares oferece um poderoso conjunto de ferramentas para classifica√ß√£o.

### Conceitos Fundamentais

**Conceito 1: Limita√ß√µes das Fronteiras Lineares e Espa√ßos Aumentados**

Os m√©todos lineares de classifica√ß√£o, como vimos nos cap√≠tulos anteriores, imp√µem uma restri√ß√£o sobre a forma das fronteiras de decis√£o, ou seja, a fronteira deve ser um hiperplano no espa√ßo de atributos original. No entanto, em muitas aplica√ß√µes, a verdadeira fronteira de decis√£o entre as classes n√£o √© linear. Para lidar com essa limita√ß√£o, podemos expandir o espa√ßo de atributos atrav√©s da inclus√£o de novas vari√°veis que s√£o fun√ß√µes n√£o lineares dos atributos originais [^4.1].

A ideia central √© transformar o espa√ßo de entrada original $\mathbb{R}^p$ em um espa√ßo de maior dimens√£o $\mathbb{R}^q$, com $q > p$, onde a transforma√ß√£o √© dada por:
$$h(X) : \mathbb{R}^p \rightarrow \mathbb{R}^q.$$
Nesse novo espa√ßo, podemos aplicar um classificador linear para construir uma fronteira de decis√£o linear que, quando vista no espa√ßo original, corresponda a uma fronteira n√£o linear.

**Lemma 1:** *Ao aplicar um classificador linear em um espa√ßo aumentado, utilizando transforma√ß√µes n√£o lineares dos atributos originais, obtemos fronteiras de decis√£o n√£o lineares no espa√ßo original. As transforma√ß√µes utilizadas definem o tipo de n√£o linearidade da fronteira.*

A prova deste Lemma decorre da an√°lise da forma da fronteira de decis√£o no espa√ßo aumentado. Seja $f(h(x)) = \beta_0 + \beta^T h(x)$ a fun√ß√£o discriminante no espa√ßo aumentado. A fronteira de decis√£o √© dada por $f(h(x)) = 0$. Se $h(x)$ for uma transforma√ß√£o n√£o linear de $x$, ent√£o a fronteira de decis√£o no espa√ßo original ser√° n√£o linear. A natureza espec√≠fica da transforma√ß√£o $h(x)$ (polinomial, intera√ß√µes etc.) definir√° a forma exata da fronteira no espa√ßo original.  $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 1: Non-linear Boundaries"
    direction TB
        A["Input Space: X ‚àà ‚Ñù·µñ"] --> B["Non-linear Transformation: h(X)"]
        B --> C["Augmented Space: h(X) ‚àà ‚Ñùq"]
        C --> D["Linear Classifier: f(h(x)) = Œ≤‚ÇÄ + Œ≤·µÄh(x)"]
        D --> E["Decision Boundary: f(h(x)) = 0 in ‚Ñùq"]
        E --> F["Non-linear Boundary in X ‚àà ‚Ñù·µñ"]
    end
```

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um espa√ßo de atributos original bidimensional com $X = (X_1, X_2)$, e queremos classificar dados em duas classes. Os dados n√£o s√£o linearmente separ√°veis no espa√ßo original. Vamos criar um espa√ßo aumentado usando uma transforma√ß√£o polinomial de grau 2:
>
> $h(X) = (1, X_1, X_2, X_1^2, X_2^2, X_1X_2)$.
>
> Agora, temos um espa√ßo de atributos de 6 dimens√µes. Um classificador linear nesse espa√ßo tem a forma:
>
> $f(h(X)) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1^2 + \beta_4 X_2^2 + \beta_5 X_1 X_2$.
>
> A fronteira de decis√£o linear no espa√ßo aumentado √© dada por $f(h(X)) = 0$. No espa√ßo original, essa fronteira corresponder√° a uma curva quadr√°tica. Por exemplo, se os par√¢metros forem:
>
> $\beta_0 = -1, \beta_1 = 0.5, \beta_2 = 0.5, \beta_3 = 1, \beta_4 = 1, \beta_5 = -2$,
>
> a fronteira de decis√£o no espa√ßo original seria:
>
> $-1 + 0.5X_1 + 0.5X_2 + X_1^2 + X_2^2 - 2X_1X_2 = 0$.
>
> Esta equa√ß√£o representa uma curva quadr√°tica no espa√ßo original, demonstrando como uma fronteira linear no espa√ßo aumentado pode gerar uma fronteira n√£o linear no espa√ßo original.

**Conceito 2: Transforma√ß√µes Polinomiais e de Intera√ß√£o**

As transforma√ß√µes mais comuns para criar espa√ßos aumentados s√£o as transforma√ß√µes polinomiais e de intera√ß√£o.

*   **Transforma√ß√µes Polinomiais:** Para um espa√ßo de atributos original com vari√°veis $X_1, X_2, ..., X_p$, podemos adicionar termos polinomiais, como $X_1^2, X_2^2, ..., X_p^2, X_1^3, X_2^3,...$ e assim por diante. Ao aplicar um classificador linear nesse espa√ßo, obteremos fronteiras quadr√°ticas, c√∫bicas ou de ordem superior no espa√ßo original.
*   **Transforma√ß√µes de Intera√ß√£o:** Podemos adicionar termos de intera√ß√£o entre os atributos originais, como $X_1X_2, X_1X_3,..., X_{p-1}X_p$. Esses termos permitem modelar a intera√ß√£o entre os atributos, permitindo que o modelo possa gerar separa√ß√µes mais complexas.

A combina√ß√£o dessas transforma√ß√µes permite construir modelos mais flex√≠veis que podem aproximar qualquer tipo de fronteira de decis√£o n√£o linear no espa√ßo original.

**Corol√°rio 1:** *A inclus√£o de termos polinomiais de segunda ordem ou de intera√ß√µes entre atributos em um espa√ßo aumentado leva a fronteiras de decis√£o quadr√°ticas no espa√ßo original. A escolha dos termos polinomiais e de intera√ß√£o influencia a forma e a flexibilidade da fronteira.*

A prova desse corol√°rio est√° na forma da fun√ß√£o discriminante obtida ap√≥s a transforma√ß√£o e o ajuste dos par√¢metros. Por exemplo, ao adicionar termos do tipo $X_i^2$ e $X_i X_j$, a fun√ß√£o discriminante no espa√ßo aumentado assume uma forma quadr√°tica no espa√ßo original. A escolha espec√≠fica desses termos define quais aspectos da fronteira de decis√£o ser√£o modelados. $\blacksquare$

```mermaid
graph LR
    subgraph "Corolario 1: Quadratic Boundaries"
    direction TB
        A["Original Attributes: X‚ÇÅ, X‚ÇÇ,... X‚Çö"] --> B["Polynomial Terms: X·µ¢¬≤, X·µ¢¬≥..."]
        B --> C["Interaction Terms: X·µ¢X‚±º"]
        C --> D["Augmented Feature Space"]
        D --> E["Linear Classifier in Augmented Space"]
        E --> F["Quadratic Decision Boundary in Original Space"]
    end
```

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo com dois atributos $X_1$ e $X_2$.
>
> *   **Transforma√ß√£o Polinomial:** Se expandirmos o espa√ßo de atributos para incluir $X_1^2$ e $X_2^2$, a fun√ß√£o discriminante no espa√ßo aumentado ser√°:
>
>     $f(h(X)) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1^2 + \beta_4 X_2^2$.
>
>     A fronteira de decis√£o ($f(h(X)) = 0$) no espa√ßo original ser√° uma c√¥nica (elipse, hip√©rbole ou par√°bola), o que nos permite modelar fronteiras quadr√°ticas.
>
> *   **Transforma√ß√£o de Intera√ß√£o:** Se expandirmos o espa√ßo de atributos para incluir o termo de intera√ß√£o $X_1X_2$, a fun√ß√£o discriminante ser√°:
>
>     $f(h(X)) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_5 X_1X_2$.
>
>     A fronteira de decis√£o neste caso tamb√©m poder√° assumir formas quadr√°ticas, permitindo que o modelo capture intera√ß√µes entre os atributos.
>
> *   **Combina√ß√£o:** Se combinarmos as transforma√ß√µes polinomiais e de intera√ß√£o, teremos:
>
>     $f(h(X)) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1^2 + \beta_4 X_2^2 + \beta_5 X_1X_2$.
>
>     Essa combina√ß√£o permite modelar fronteiras mais complexas no espa√ßo original.

**Conceito 3: Limita√ß√µes e Regulariza√ß√£o**

A expans√£o do espa√ßo de atributos aumenta a complexidade do modelo e o risco de sobreajuste. Com um n√∫mero muito grande de atributos, o modelo pode se ajustar excessivamente aos dados de treinamento, perdendo a capacidade de generaliza√ß√£o para novos dados. Nesse cen√°rio, as t√©cnicas de regulariza√ß√£o s√£o cruciais. M√©todos como Lasso (penaliza√ß√£o L1), Ridge (penaliza√ß√£o L2) e Elastic Net podem ser usados para controlar a magnitude dos coeficientes do modelo e evitar o sobreajuste, mesmo em espa√ßos aumentados.

> ‚ö†Ô∏è **Nota Importante**: A expans√£o do espa√ßo de atributos pode gerar modelos mais flex√≠veis, mas tamb√©m mais complexos. A regulariza√ß√£o √© fundamental para lidar com o sobreajuste e obter modelos com boa generaliza√ß√£o. **Refer√™ncia ao t√≥pico [^4.1]**.

> ‚ùó **Ponto de Aten√ß√£o**:  A escolha das transforma√ß√µes de atributos e da intensidade da regulariza√ß√£o devem ser feitas com cuidado, considerando o objetivo do problema, o tamanho do conjunto de dados e a necessidade de interpretabilidade. **Conforme indicado em [^4.5]**.

> ‚úîÔ∏è **Destaque**: O uso de transforma√ß√µes n√£o lineares para expandir o espa√ßo de atributos, combinado com m√©todos lineares de classifica√ß√£o e regulariza√ß√£o, oferece uma abordagem poderosa para modelar fronteiras de decis√£o complexas. **Baseado nos t√≥picos [^4.1] e [^4.5]**.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
    subgraph "Linear Regression for Classification"
    direction TB
        A["Original Space: X"] --> B["Augmented Space: h(X)"]
        B --> C["Linear Regression Model: B = (h(X)·µÄh(X))‚Åª¬πh(X)·µÄY"]
        C --> D["Predicted Values in h(X): f(h(x)) = (1, h(x)·µÄ)B"]
        D --> E["Classification: argmax_k f_k(h(x))"]
        E --> F["Quadratic Decision Boundary in Original Space"]
    end
```

A regress√£o linear, quando aplicada a um espa√ßo de atributos aumentado, pode gerar fronteiras de decis√£o quadr√°ticas ou de ordem superior no espa√ßo original [^4.1]. Para ilustrar, considere um espa√ßo de atributos original bidimensional $X = (X_1, X_2)$. Podemos expandir esse espa√ßo adicionando termos quadr√°ticos e de intera√ß√£o, obtendo um novo espa√ßo de atributos $h(X) = (1, X_1, X_2, X_1^2, X_2^2, X_1X_2)$. Em geral, para um espa√ßo de dimens√£o *p*, podemos construir transforma√ß√µes n√£o lineares para criar novas vari√°veis.

Nesse espa√ßo aumentado, ajustamos um modelo de regress√£o linear com as vari√°veis indicadoras $Y_k$ como resposta. A solu√ß√£o de m√≠nimos quadrados para o modelo de regress√£o √©:
$$\mathbf{B} = (\mathbf{h(X)}^T\mathbf{h(X)})^{-1}\mathbf{h(X)}^T\mathbf{Y},$$
onde $\mathbf{h(X)}$ √© a matriz de design constru√≠da com as vari√°veis transformadas e $\mathbf{Y}$ √© a matriz de indicadores de classes.

Para classificar uma nova observa√ß√£o *x*, primeiro transformamos *x* para o espa√ßo aumentado $h(x)$ e calculamos os valores preditos:
$$f(h(x)) = (1, h(x)^T)\mathbf{B}.$$
A classe predita √© aquela correspondente ao maior valor de $f_k(h(x))$:
$$\hat{G}(x) = \arg\max_k f_k(h(x)).$$
No espa√ßo original, a fronteira de decis√£o n√£o ser√° linear devido √† transforma√ß√£o $h(x)$.

**Lemma 2:** *Ao aplicar a regress√£o linear em um espa√ßo de atributos aumentado com transforma√ß√µes polinomiais ou de intera√ß√£o, a fronteira de decis√£o resultante no espa√ßo original assume uma forma quadr√°tica ou de ordem superior, dependendo da natureza das transforma√ß√µes utilizadas.*

A prova deste Lemma segue da forma da fun√ß√£o discriminante no espa√ßo original. A fun√ß√£o discriminante √© linear no espa√ßo transformado, ou seja, $f(h(x)) = \beta_0 + \beta^T h(x)$. Se $h(x)$ incluir termos quadr√°ticos ou de intera√ß√£o, a fronteira de decis√£o $f(h(x))=0$ corresponde a uma equa√ß√£o quadr√°tica ou de ordem superior em fun√ß√£o dos atributos originais ($x$), logo, a fronteira de decis√£o tamb√©m ser√° quadr√°tica. $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 2: Regression Boundaries"
    direction TB
        A["Input: X"] --> B["Transformation: h(X)"]
        B --> C["Linear Regression: f(h(x)) = (1, h(x)·µÄ)B"]
        C --> D["Decision Boundary in h(X): f(h(x)) = 0"]
         D --> E["Quadratic/Higher Order Boundary in Original Space"]
    end
```

**Corol√°rio 2:** *Ao aplicar transforma√ß√µes lineares no espa√ßo transformado, como √© o caso do modelo de regress√£o linear, as fronteiras obtidas no espa√ßo original sempre ser√£o fun√ß√µes lineares das vari√°veis do espa√ßo transformado, e fun√ß√µes polinomiais das vari√°veis do espa√ßo original.*

Este corol√°rio reafirma a ideia principal. O modelo de regress√£o linear, ajustado com uma transforma√ß√£o n√£o linear dos dados, como uma transforma√ß√£o polinomial ou de intera√ß√µes, ir√° produzir uma fronteira linear nos atributos do espa√ßo transformado, que, ao serem mapeados no espa√ßo original, resultar√£o em fronteiras n√£o lineares. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos 5 pontos de dados para duas classes, onde $X = (X_1, X_2)$ e $Y$ √© a classe (0 ou 1):
>
> Dados:
>
> $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 2 & 1 \\ 3 & 2 \\ 2 & 3 \end{bmatrix}$, $Y = \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \\ 1 \end{bmatrix}$
>
> Vamos criar um espa√ßo aumentado com $h(X) = (1, X_1, X_2, X_1^2, X_2^2, X_1X_2)$.
>
> $h(X) = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\ 1 & 1 & 2 & 1 & 4 & 2 \\ 1 & 2 & 1 & 4 & 1 & 2 \\ 1 & 3 & 2 & 9 & 4 & 6 \\ 1 & 2 & 3 & 4 & 9 & 6 \end{bmatrix}$
>
> Agora, podemos usar a f√≥rmula de m√≠nimos quadrados para calcular os coeficientes $\mathbf{B}$:
>
> $\mathbf{B} = (\mathbf{h(X)}^T\mathbf{h(X)})^{-1}\mathbf{h(X)}^T\mathbf{Y}$
>
> 1.  **Calcular** $\mathbf{h(X)}^T\mathbf{h(X)}$:
>
> $\mathbf{h(X)}^T\mathbf{h(X)} = \begin{bmatrix} 5 & 9 & 9 & 29 & 19 & 17 \\ 9 & 19 & 17 & 65 & 41 & 47 \\ 9 & 17 & 19 & 41 & 65 & 47 \\ 29 & 65 & 41 & 205 & 145 & 147 \\ 19 & 41 & 65 & 145 & 205 & 147 \\ 17 & 47 & 47 & 147 & 147 & 177 \end{bmatrix}$
>
> 2. **Calcular** $(\mathbf{h(X)}^T\mathbf{h(X)})^{-1}$ (usando numpy):
> ```python
> import numpy as np
> hX = np.array([[1, 1, 1, 1, 1, 1],
>               [1, 1, 2, 1, 4, 2],
>               [1, 2, 1, 4, 1, 2],
>               [1, 3, 2, 9, 4, 6],
>               [1, 2, 3, 4, 9, 6]])
> Y = np.array([0, 0, 0, 1, 1])
> hX_transpose_hX = np.dot(hX.T, hX)
> hX_transpose_hX_inv = np.linalg.inv(hX_transpose_hX)
> print(hX_transpose_hX_inv)
> ```
>
> ```
> [[ 1.34791667e+00 -4.18750000e-01 -4.18750000e-01  2.08333333e-02
>   2.08333333e-02  1.04166667e-01]
>  [-4.18750000e-01  1.87500000e-01  1.25000000e-01 -2.50000000e-02
>  -1.25000000e-02 -1.25000000e-02]
>  [-4.18750000e-01  1.25000000e-01  1.87500000e-01 -1.25000000e-02
>  -2.50000000e-02 -1.25000000e-02]
>  [ 2.08333333e-02 -2.50000000e-02 -1.25000000e-02  8.33333333e-03
>   3.12500000e-03 -3.12500000e-03]
>  [ 2.08333333e-02 -1.25000000e-02 -2.50000000e-02  3.12500000e-03
>   8.33333333e-03 -3.12500000e-03]
>  [ 1.04166667e-01 -1.25000000e-02 -1.25000000e-02 -3.12500000e-03
>  -3.12500000e-03  6.25000000e-03]]
> ```
>
> 3. **Calcular** $\mathbf{h(X)}^T\mathbf{Y}$:
>
> $\mathbf{h(X)}^T\mathbf{Y} = \begin{bmatrix} 2 \\ 5 \\ 5 \\ 13 \\ 13 \\ 12 \end{bmatrix}$
>
> 4. **Calcular** $\mathbf{B}$:
>
> $\mathbf{B} = \begin{bmatrix} -1.625 \\ 0.9375 \\ 0.9375 \\ -0.125 \\ -0.125 \\ 0.25 \end{bmatrix}$
>
> A fun√ß√£o discriminante no espa√ßo aumentado √©:
>
> $f(h(x)) = -1.625 + 0.9375 X_1 + 0.9375 X_2 - 0.125 X_1^2 - 0.125 X_2^2 + 0.25 X_1X_2$.
>
> A fronteira de decis√£o no espa√ßo original √© dada por $f(h(x)) = 0$, que √© uma equa√ß√£o quadr√°tica.

Em resumo, a regress√£o linear em um espa√ßo aumentado transforma o problema de encontrar fronteiras n√£o lineares no espa√ßo original em encontrar um hiperplano em um espa√ßo de maior dimens√£o.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Variable Selection and Regularization"
    direction TB
        A["Cost Function: -‚Ñì(Œ≤) + Œª * Penalty(Œ≤)"]
        A --> B["L1 Penalty (Lasso): ŒªŒ£|Œ≤‚±º|"]
        A --> C["L2 Penalty (Ridge): ŒªŒ£Œ≤‚±º¬≤"]
         A --> D["Elastic Net: Œª‚ÇÅŒ£|Œ≤‚±º| + Œª‚ÇÇŒ£Œ≤‚±º¬≤"]
         B --> E["Sparse Solutions in Augmented Space"]
        C --> F["Coefficient Shrinkage"]
        D --> G["Combines Sparsity and Shrinkage"]
    end
```

A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o cruciais ao trabalhar com espa√ßos de atributos aumentados, pois o aumento da dimensionalidade pode levar a modelos muito complexos e com alto risco de sobreajuste [^4.5]. A regulariza√ß√£o, atrav√©s da penaliza√ß√£o de coeficientes, visa simplificar o modelo, ao mesmo tempo em que se selecionam as vari√°veis mais importantes.

A fun√ß√£o de custo geral, quando se aplica regulariza√ß√£o, √© dada por:
$$\text{Custo} = - \ell(\beta) + \lambda \text{Penalidade}(\beta),$$
onde $\ell(\beta)$ √© a log-verossimilhan√ßa ou outra fun√ß√£o de custo, $\beta$ s√£o os coeficientes do modelo no espa√ßo aumentado e  $\text{Penalidade}(\beta)$ √© o termo de penaliza√ß√£o.

*   **Penaliza√ß√£o L1 (Lasso):** Adiciona a penalidade:
    $$\text{Penalidade}_{L1}(\beta) = \sum_{j=1}^q |\beta_j|,$$
    onde $q$ √© o n√∫mero de atributos no espa√ßo aumentado. A penalidade L1 promove esparsidade, selecionando um subconjunto de atributos no espa√ßo aumentado, que pode corresponder a combina√ß√µes n√£o lineares dos atributos originais.
*   **Penaliza√ß√£o L2 (Ridge):** Adiciona a penalidade:
    $$\text{Penalidade}_{L2}(\beta) = \sum_{j=1}^q \beta_j^2.$$
    Essa penalidade reduz a magnitude dos coeficientes, o que suaviza a fronteira de decis√£o.
*   **Elastic Net:** Combina as penalidades L1 e L2:
    $$\text{Penalidade}_{ElasticNet}(\beta) = \lambda_1 \sum_{j=1}^q |\beta_j| + \lambda_2 \sum_{j=1}^q \beta_j^2.$$
     Essa penalidade visa combinar os benef√≠cios de ambas as penalidades, induzindo a esparsidade e reduzindo a vari√¢ncia.

**Lemma 3:** *Ao aplicar a regulariza√ß√£o L1 ou o Elastic Net em modelos de classifica√ß√£o em espa√ßos aumentados, obtemos modelos esparsos, onde muitos dos coeficientes associados aos atributos transformados s√£o iguais a zero. Essa esparsidade reflete na complexidade da fronteira de decis√£o no espa√ßo original.*

A prova deste Lemma segue da an√°lise do efeito da norma L1 na fun√ß√£o de custo. A penalidade L1 for√ßa os coeficientes associados aos atributos transformados para zero, levando a modelos esparsos. Elastic Net combina esta propriedade com a penalidade L2, que evita o problema da penaliza√ß√£o excessiva da norma L1, promovendo a sele√ß√£o de vari√°veis e evitando o sobreajuste. $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 3: Sparsity with L1/Elastic Net"
    direction TB
         A["L1 Regularization"] --> B["Sparse Solutions: Many Œ≤‚±º = 0"]
         A --> C["Elastic Net"]
         C --> B
         B --> D["Simplified Decision Boundary"]
    end
```

**Corol√°rio 3:** *Em modelos de classifica√ß√£o com espa√ßos aumentados, a regulariza√ß√£o L1 atua como uma ferramenta de sele√ß√£o de atributos, selecionando as transforma√ß√µes de atributos mais importantes para a constru√ß√£o da fronteira de decis√£o no espa√ßo original. Essa sele√ß√£o melhora a interpretabilidade do modelo e reduz o risco de sobreajuste.*

Este corol√°rio enfatiza o papel da regulariza√ß√£o para simplificar modelos em espa√ßos aumentados. Ao selecionar apenas algumas transforma√ß√µes de atributos, a regulariza√ß√£o L1 tamb√©m aumenta a interpretabilidade do modelo e a sua capacidade de generaliza√ß√£o, por n√£o se ajustar excessivamente ao ru√≠do nos dados de treino. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o mesmo conjunto de dados anterior no espa√ßo aumentado e aplicar regulariza√ß√£o L1 (Lasso) e L2 (Ridge). Usaremos um valor de $\lambda=0.1$ para ambos.
>
> Usando o mesmo h(X) e Y:
>
> $h(X) = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\ 1 & 1 & 2 & 1 & 4 & 2 \\ 1 & 2 & 1 & 4 & 1 & 2 \\ 1 & 3 & 2 & 9 & 4 & 6 \\ 1 & 2 & 3 & 4 & 9 & 6 \end{bmatrix}$, $Y = \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \\ 1 \end{bmatrix}$
>
> 1.  **Lasso (L1):**
>
>     A fun√ß√£o de custo √©:  $Custo = - \ell(\beta) + \lambda \sum_{j=1}^q |\beta_j|$
>
>     Usando a biblioteca `sklearn`, podemos encontrar os coeficientes:
> ```python
> from sklearn.linear_model import Lasso
> lasso = Lasso(alpha=0.1)
> lasso.fit(hX, Y)
> print("Lasso Coefficients:", lasso.coef_)
> ```
>     Resultados:
>   ```
>   Lasso Coefficients: [-0.         0.23109842  0.23109842  0.         0.          0.        ]
>   ```
>     Observe que alguns coeficientes s√£o exatamente zero, indicando que o Lasso selecionou apenas $X_1$ e $X_2$ como atributos relevantes no espa√ßo aumentado. A fronteira de decis√£o no espa√ßo original ser√° simplificada.
>
> 2.  **Ridge (L2):**
>
>     A fun√ß√£o de custo √©: $Custo = - \ell(\beta) + \lambda \sum_{j=1}^q \beta_j^2$
>
>     Usando a biblioteca `sklearn`, podemos encontrar os coeficientes:
> ```python
> from sklearn.linear_model import Ridge
> ridge = Ridge(alpha=0.1)
> ridge.fit(hX, Y)
> print("Ridge Coefficients:", ridge.coef_)
> ```
>     Resultados:
>   ```
>   Ridge Coefficients: [-0.44286707  0.4173996   0.4173996  -0.0653883  -0.0653883   0.09044638]
>   ```
>
>     Os coeficientes do Ridge s√£o menores em magnitude, mas nenhum √© exatamente zero. A fronteira de decis√£o no espa√ßo original ser√° mais suave do que a obtida sem regulariza√ß√£o.
>
> | Method |  $\beta_0$ | $\beta_1$ | $\beta_2$ | $\beta_3$ | $\beta_4$ | $\beta_5$ |
> |--------|------------|-----------|-----------|-----------|-----------|-----------|
> | OLS    |  -1.625    | 0.9375    | 0.9375    | -0.125    | -0.125    | 0.25      |
> | Lasso  |  -0.0      | 0.231     | 0.231     | 0.0       | 0.0       | 0.0       |
> | Ridge  |  -0.443    | 0.417     | 0.417     | -0.065    | -0.065    | 0.09      |
>
> A tabela acima mostra como a regulariza√ß√£o afeta os coeficientes no espa√ßo aumentado. O Lasso zerou alguns coeficientes, simplificando o modelo, enquanto o Ridge reduziu a magnitude de todos os coeficientes, suavizando a fronteira de decis√£o no espa√ßo original.

> ‚ö†Ô∏è **Ponto Crucial**: A regulariza√ß√£o em espa√ßos aumentados √© crucial para evitar o sobreajuste e obter modelos com boa capacidade de generaliza√ß√£o. A escolha da penalidade (L1, L2 ou Elastic Net) depende do objetivo do problema e das caracter√≠sticas dos dados. **Refer√™ncia ao t√≥pico [^4.5]**.

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
    subgraph "Hyperplanes and Perceptrons"
    direction TB
        A["Data in Original Space"] --> B["Transformation to Augmented Space h(X)"]
        B --> C["Separating Hyperplane in h(X)"]
        C --> D["Optimization: min ||Œ≤||¬≤  s.t. y·µ¢(Œ≤·µÄh(x·µ¢)+Œ≤‚ÇÄ) ‚â• 1"]
        C --> E["Perceptron Algorithm"]
        E --> F["Iteration Update: Œ≤(new) = Œ≤(old) + Œ∑y·µ¢h(x·µ¢)"]
        D & F --> G["Non-linear Decision Boundary in Original Space"]
    end
```

O conceito de **hiperplanos separadores** pode ser estendido para espa√ßos aumentados, permitindo a constru√ß√£o de fronteiras de decis√£o n√£o lineares no espa√ßo original. A ideia √© transformar os dados originais para um espa√ßo de maior dimens√£o e aplicar o conceito de maximiza√ß√£o da margem nesse espa√ßo transformado. O problema de otimiza√ß√£o do hiperplano separador no espa√ßo aumentado √© dado por:
$$
\begin{aligned}
  \min_{\beta,\beta_0} \quad & \frac{1}{2} ||\beta||^2 \\
  \text{s.t.} \quad & y_i (\beta^T h(x_i) + \beta_0) \geq 1, \quad \forall i = 1, ..., N,
\end{aligned}
$$
onde $h(x_i)$ √© a transforma√ß√£o n√£o linear de $x_i$ para o espa√ßo aumentado e $\beta$ e $\beta_0$ s√£o os par√¢metros do hiperplano no espa√ßo aumentado. No espa√ßo original, a fronteira resultante ser√° n√£o linear, devido √† transforma√ß√£o $h(x)$.

O **Perceptron de Rosenblatt** tamb√©m pode ser aplicado em um espa√ßo aumentado, buscando um hiperplano separador iterativamente. O algoritmo come√ßa com um hiperplano aleat√≥rio no espa√ßo transformado e, em cada itera√ß√£o, atualiza os pesos com base nas amostras mal classificadas:
$$ \beta^{new} = \beta^{old} + \eta y_i h(x_i), $$
onde $\eta$ √© a taxa de aprendizado, $y_i$ √© o r√≥tulo da classe e $h(x_i)$ √© o vetor de atributos transformado da amostra.

A combina√ß√£o de espa√ßos aumentados com o Perceptron permite que esse algoritmo obtenha fronteiras de decis√£o n√£o lineares no espa√ßo original. Se os dados transformados forem linearmente separ√°veis, o algoritmo converge para um hiperplano que corresponder√° a uma fronteira de decis√£o n√£o linear no espa√ßo original.

**Lemma 4:** *Tanto os hiperplanos separadores √≥timos quanto o Perceptron, quando aplicados em espa√ßos aumentados com transforma√ß√µes n√£o lineares, levam a fronteiras de decis√£o n√£o lineares no espa√ßo original. As transforma√ß√µes definem o grau da n√£o linearidade das fronteiras.*

A prova deste Lemma reside na an√°lise da fun√ß√£o discriminante no espa√ßo original. Ambos os m√©todos, hiperplanos separadores e Perceptron, procuram uma combina√ß√£o linear dos atributos no espa√ßo transformado, ou seja, $\beta^T h(x)$. Como a transforma√ß√£o $h(x)$ √© n√£o linear, a fronteira de decis√£o no espa√ßo original, que √© dada pela condi√ß√£o $\beta^T h(x) + \beta_0 = 0$, define uma superf√≠cie n√£o linear, cuja forma depende da transforma√ß√£o $h(x)$.  $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 4: Non-linear Boundaries from Hyperplanes and Perceptrons"
    direction TB
        A["Transformation h(X)"] --> B["Hyperplane in h(X): Œ≤·µÄh(x) + Œ≤