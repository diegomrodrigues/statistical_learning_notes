## TÃ­tulo Conciso: MÃ©todos Lineares para ClassificaÃ§Ã£o: Fronteiras QuadrÃ¡ticas AtravÃ©s de EspaÃ§os Aumentados

```mermaid
graph LR
    subgraph "Linear Classifiers in Augmented Spaces"
        direction TB
        A["Original Feature Space (X)"] --> B["Non-linear Transformation h(X)"]
        B --> C["Augmented Feature Space h(X)"]
        C --> D["Linear Classifier in h(X)"]
        D --> E["Quadratic Decision Boundary in X"]
    end
```

### IntroduÃ§Ã£o

Este capÃ­tulo explora como mÃ©todos lineares podem gerar **fronteiras de decisÃ£o quadrÃ¡ticas** no espaÃ§o de atributos original atravÃ©s da construÃ§Ã£o de **fronteiras lineares em espaÃ§os aumentados**. A ideia central Ã© que, ao expandir o espaÃ§o de atributos original por meio de transformaÃ§Ãµes nÃ£o lineares, como a adiÃ§Ã£o de termos polinomiais e de interaÃ§Ã£o, Ã© possÃ­vel aplicar um classificador linear no espaÃ§o expandido e obter um classificador com fronteiras nÃ£o lineares no espaÃ§o original. Essa abordagem oferece uma forma eficaz de lidar com problemas de classificaÃ§Ã£o que nÃ£o sÃ£o linearmente separÃ¡veis no espaÃ§o de atributos original [^4.1].

A expansÃ£o do espaÃ§o de atributos, tambÃ©m conhecida como *feature engineering*, permite que os mÃ©todos lineares, intrinsecamente limitados a separaÃ§Ãµes lineares, possam modelar relaÃ§Ãµes mais complexas entre os atributos e as classes. A escolha das transformaÃ§Ãµes apropriadas Ã© um aspecto crucial desse processo, pois ela determina a forma da fronteira de decisÃ£o no espaÃ§o original. Neste capÃ­tulo, vamos detalhar como diferentes transformaÃ§Ãµes, como termos polinomiais e interaÃ§Ãµes entre atributos, levam a diferentes formas de fronteiras de decisÃ£o no espaÃ§o original.

AlÃ©m disso, vamos discutir as vantagens e as limitaÃ§Ãµes desta abordagem. Embora a expansÃ£o do espaÃ§o de atributos permita obter modelos mais flexÃ­veis, ela tambÃ©m pode aumentar a complexidade do modelo e o risco de sobreajuste. Vamos analisar como tÃ©cnicas de regularizaÃ§Ã£o podem ser utilizadas para mitigar esses problemas. Ao longo do capÃ­tulo, vamos explorar como a combinaÃ§Ã£o de transformaÃ§Ãµes nÃ£o lineares e mÃ©todos lineares oferece um poderoso conjunto de ferramentas para classificaÃ§Ã£o.

### Conceitos Fundamentais

**Conceito 1: LimitaÃ§Ãµes das Fronteiras Lineares e EspaÃ§os Aumentados**

Os mÃ©todos lineares de classificaÃ§Ã£o, como vimos nos capÃ­tulos anteriores, impÃµem uma restriÃ§Ã£o sobre a forma das fronteiras de decisÃ£o, ou seja, a fronteira deve ser um hiperplano no espaÃ§o de atributos original. No entanto, em muitas aplicaÃ§Ãµes, a verdadeira fronteira de decisÃ£o entre as classes nÃ£o Ã© linear. Para lidar com essa limitaÃ§Ã£o, podemos expandir o espaÃ§o de atributos atravÃ©s da inclusÃ£o de novas variÃ¡veis que sÃ£o funÃ§Ãµes nÃ£o lineares dos atributos originais [^4.1].

A ideia central Ã© transformar o espaÃ§o de entrada original $\mathbb{R}^p$ em um espaÃ§o de maior dimensÃ£o $\mathbb{R}^q$, com $q > p$, onde a transformaÃ§Ã£o Ã© dada por:
$$h(X) : \mathbb{R}^p \rightarrow \mathbb{R}^q.$$
Nesse novo espaÃ§o, podemos aplicar um classificador linear para construir uma fronteira de decisÃ£o linear que, quando vista no espaÃ§o original, corresponda a uma fronteira nÃ£o linear.

**Lemma 1:** *Ao aplicar um classificador linear em um espaÃ§o aumentado, utilizando transformaÃ§Ãµes nÃ£o lineares dos atributos originais, obtemos fronteiras de decisÃ£o nÃ£o lineares no espaÃ§o original. As transformaÃ§Ãµes utilizadas definem o tipo de nÃ£o linearidade da fronteira.*

A prova deste Lemma decorre da anÃ¡lise da forma da fronteira de decisÃ£o no espaÃ§o aumentado. Seja $f(h(x)) = \beta_0 + \beta^T h(x)$ a funÃ§Ã£o discriminante no espaÃ§o aumentado. A fronteira de decisÃ£o Ã© dada por $f(h(x)) = 0$. Se $h(x)$ for uma transformaÃ§Ã£o nÃ£o linear de $x$, entÃ£o a fronteira de decisÃ£o no espaÃ§o original serÃ¡ nÃ£o linear. A natureza especÃ­fica da transformaÃ§Ã£o $h(x)$ (polinomial, interaÃ§Ãµes etc.) definirÃ¡ a forma exata da fronteira no espaÃ§o original.  $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 1: Non-linear Boundaries"
    direction TB
        A["Input Space: X âˆˆ â„áµ–"] --> B["Non-linear Transformation: h(X)"]
        B --> C["Augmented Space: h(X) âˆˆ â„q"]
        C --> D["Linear Classifier: f(h(x)) = Î²â‚€ + Î²áµ€h(x)"]
        D --> E["Decision Boundary: f(h(x)) = 0 in â„q"]
        E --> F["Non-linear Boundary in X âˆˆ â„áµ–"]
    end
```

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos um espaÃ§o de atributos original bidimensional com $X = (X_1, X_2)$, e queremos classificar dados em duas classes. Os dados nÃ£o sÃ£o linearmente separÃ¡veis no espaÃ§o original. Vamos criar um espaÃ§o aumentado usando uma transformaÃ§Ã£o polinomial de grau 2:
>
> $h(X) = (1, X_1, X_2, X_1^2, X_2^2, X_1X_2)$.
>
> Agora, temos um espaÃ§o de atributos de 6 dimensÃµes. Um classificador linear nesse espaÃ§o tem a forma:
>
> $f(h(X)) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1^2 + \beta_4 X_2^2 + \beta_5 X_1 X_2$.
>
> A fronteira de decisÃ£o linear no espaÃ§o aumentado Ã© dada por $f(h(X)) = 0$. No espaÃ§o original, essa fronteira corresponderÃ¡ a uma curva quadrÃ¡tica. Por exemplo, se os parÃ¢metros forem:
>
> $\beta_0 = -1, \beta_1 = 0.5, \beta_2 = 0.5, \beta_3 = 1, \beta_4 = 1, \beta_5 = -2$,
>
> a fronteira de decisÃ£o no espaÃ§o original seria:
>
> $-1 + 0.5X_1 + 0.5X_2 + X_1^2 + X_2^2 - 2X_1X_2 = 0$.
>
> Esta equaÃ§Ã£o representa uma curva quadrÃ¡tica no espaÃ§o original, demonstrando como uma fronteira linear no espaÃ§o aumentado pode gerar uma fronteira nÃ£o linear no espaÃ§o original.

**Conceito 2: TransformaÃ§Ãµes Polinomiais e de InteraÃ§Ã£o**

As transformaÃ§Ãµes mais comuns para criar espaÃ§os aumentados sÃ£o as transformaÃ§Ãµes polinomiais e de interaÃ§Ã£o.

*   **TransformaÃ§Ãµes Polinomiais:** Para um espaÃ§o de atributos original com variÃ¡veis $X_1, X_2, ..., X_p$, podemos adicionar termos polinomiais, como $X_1^2, X_2^2, ..., X_p^2, X_1^3, X_2^3,...$ e assim por diante. Ao aplicar um classificador linear nesse espaÃ§o, obteremos fronteiras quadrÃ¡ticas, cÃºbicas ou de ordem superior no espaÃ§o original.
*   **TransformaÃ§Ãµes de InteraÃ§Ã£o:** Podemos adicionar termos de interaÃ§Ã£o entre os atributos originais, como $X_1X_2, X_1X_3,..., X_{p-1}X_p$. Esses termos permitem modelar a interaÃ§Ã£o entre os atributos, permitindo que o modelo possa gerar separaÃ§Ãµes mais complexas.

A combinaÃ§Ã£o dessas transformaÃ§Ãµes permite construir modelos mais flexÃ­veis que podem aproximar qualquer tipo de fronteira de decisÃ£o nÃ£o linear no espaÃ§o original.

**CorolÃ¡rio 1:** *A inclusÃ£o de termos polinomiais de segunda ordem ou de interaÃ§Ãµes entre atributos em um espaÃ§o aumentado leva a fronteiras de decisÃ£o quadrÃ¡ticas no espaÃ§o original. A escolha dos termos polinomiais e de interaÃ§Ã£o influencia a forma e a flexibilidade da fronteira.*

A prova desse corolÃ¡rio estÃ¡ na forma da funÃ§Ã£o discriminante obtida apÃ³s a transformaÃ§Ã£o e o ajuste dos parÃ¢metros. Por exemplo, ao adicionar termos do tipo $X_i^2$ e $X_i X_j$, a funÃ§Ã£o discriminante no espaÃ§o aumentado assume uma forma quadrÃ¡tica no espaÃ§o original. A escolha especÃ­fica desses termos define quais aspectos da fronteira de decisÃ£o serÃ£o modelados. $\blacksquare$

```mermaid
graph LR
    subgraph "Corolario 1: Quadratic Boundaries"
    direction TB
        A["Original Attributes: Xâ‚, Xâ‚‚,... Xâ‚š"] --> B["Polynomial Terms: Xáµ¢Â², Xáµ¢Â³..."]
        B --> C["Interaction Terms: Xáµ¢Xâ±¼"]
        C --> D["Augmented Feature Space"]
        D --> E["Linear Classifier in Augmented Space"]
        E --> F["Quadratic Decision Boundary in Original Space"]
    end
```

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos considerar um exemplo com dois atributos $X_1$ e $X_2$.
>
> *   **TransformaÃ§Ã£o Polinomial:** Se expandirmos o espaÃ§o de atributos para incluir $X_1^2$ e $X_2^2$, a funÃ§Ã£o discriminante no espaÃ§o aumentado serÃ¡:
>
>     $f(h(X)) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1^2 + \beta_4 X_2^2$.
>
>     A fronteira de decisÃ£o ($f(h(X)) = 0$) no espaÃ§o original serÃ¡ uma cÃ´nica (elipse, hipÃ©rbole ou parÃ¡bola), o que nos permite modelar fronteiras quadrÃ¡ticas.
>
> *   **TransformaÃ§Ã£o de InteraÃ§Ã£o:** Se expandirmos o espaÃ§o de atributos para incluir o termo de interaÃ§Ã£o $X_1X_2$, a funÃ§Ã£o discriminante serÃ¡:
>
>     $f(h(X)) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_5 X_1X_2$.
>
>     A fronteira de decisÃ£o neste caso tambÃ©m poderÃ¡ assumir formas quadrÃ¡ticas, permitindo que o modelo capture interaÃ§Ãµes entre os atributos.
>
> *   **CombinaÃ§Ã£o:** Se combinarmos as transformaÃ§Ãµes polinomiais e de interaÃ§Ã£o, teremos:
>
>     $f(h(X)) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1^2 + \beta_4 X_2^2 + \beta_5 X_1X_2$.
>
>     Essa combinaÃ§Ã£o permite modelar fronteiras mais complexas no espaÃ§o original.

**Conceito 3: LimitaÃ§Ãµes e RegularizaÃ§Ã£o**

A expansÃ£o do espaÃ§o de atributos aumenta a complexidade do modelo e o risco de sobreajuste. Com um nÃºmero muito grande de atributos, o modelo pode se ajustar excessivamente aos dados de treinamento, perdendo a capacidade de generalizaÃ§Ã£o para novos dados. Nesse cenÃ¡rio, as tÃ©cnicas de regularizaÃ§Ã£o sÃ£o cruciais. MÃ©todos como Lasso (penalizaÃ§Ã£o L1), Ridge (penalizaÃ§Ã£o L2) e Elastic Net podem ser usados para controlar a magnitude dos coeficientes do modelo e evitar o sobreajuste, mesmo em espaÃ§os aumentados.

> âš ï¸ **Nota Importante**: A expansÃ£o do espaÃ§o de atributos pode gerar modelos mais flexÃ­veis, mas tambÃ©m mais complexos. A regularizaÃ§Ã£o Ã© fundamental para lidar com o sobreajuste e obter modelos com boa generalizaÃ§Ã£o. **ReferÃªncia ao tÃ³pico [^4.1]**.

> â— **Ponto de AtenÃ§Ã£o**:  A escolha das transformaÃ§Ãµes de atributos e da intensidade da regularizaÃ§Ã£o devem ser feitas com cuidado, considerando o objetivo do problema, o tamanho do conjunto de dados e a necessidade de interpretabilidade. **Conforme indicado em [^4.5]**.

> âœ”ï¸ **Destaque**: O uso de transformaÃ§Ãµes nÃ£o lineares para expandir o espaÃ§o de atributos, combinado com mÃ©todos lineares de classificaÃ§Ã£o e regularizaÃ§Ã£o, oferece uma abordagem poderosa para modelar fronteiras de decisÃ£o complexas. **Baseado nos tÃ³picos [^4.1] e [^4.5]**.

### RegressÃ£o Linear e MÃ­nimos Quadrados para ClassificaÃ§Ã£o

```mermaid
graph LR
    subgraph "Linear Regression for Classification"
    direction TB
        A["Original Space: X"] --> B["Augmented Space: h(X)"]
        B --> C["Linear Regression Model: B = (h(X)áµ€h(X))â»Â¹h(X)áµ€Y"]
        C --> D["Predicted Values in h(X): f(h(x)) = (1, h(x)áµ€)B"]
        D --> E["Classification: argmax_k f_k(h(x))"]
        E --> F["Quadratic Decision Boundary in Original Space"]
    end
```

A regressÃ£o linear, quando aplicada a um espaÃ§o de atributos aumentado, pode gerar fronteiras de decisÃ£o quadrÃ¡ticas ou de ordem superior no espaÃ§o original [^4.1]. Para ilustrar, considere um espaÃ§o de atributos original bidimensional $X = (X_1, X_2)$. Podemos expandir esse espaÃ§o adicionando termos quadrÃ¡ticos e de interaÃ§Ã£o, obtendo um novo espaÃ§o de atributos $h(X) = (1, X_1, X_2, X_1^2, X_2^2, X_1X_2)$. Em geral, para um espaÃ§o de dimensÃ£o *p*, podemos construir transformaÃ§Ãµes nÃ£o lineares para criar novas variÃ¡veis.

Nesse espaÃ§o aumentado, ajustamos um modelo de regressÃ£o linear com as variÃ¡veis indicadoras $Y_k$ como resposta. A soluÃ§Ã£o de mÃ­nimos quadrados para o modelo de regressÃ£o Ã©:
$$\mathbf{B} = (\mathbf{h(X)}^T\mathbf{h(X)})^{-1}\mathbf{h(X)}^T\mathbf{Y},$$
onde $\mathbf{h(X)}$ Ã© a matriz de design construÃ­da com as variÃ¡veis transformadas e $\mathbf{Y}$ Ã© a matriz de indicadores de classes.

Para classificar uma nova observaÃ§Ã£o *x*, primeiro transformamos *x* para o espaÃ§o aumentado $h(x)$ e calculamos os valores preditos:
$$f(h(x)) = (1, h(x)^T)\mathbf{B}.$$
A classe predita Ã© aquela correspondente ao maior valor de $f_k(h(x))$:
$$\hat{G}(x) = \arg\max_k f_k(h(x)).$$
No espaÃ§o original, a fronteira de decisÃ£o nÃ£o serÃ¡ linear devido Ã  transformaÃ§Ã£o $h(x)$.

**Lemma 2:** *Ao aplicar a regressÃ£o linear em um espaÃ§o de atributos aumentado com transformaÃ§Ãµes polinomiais ou de interaÃ§Ã£o, a fronteira de decisÃ£o resultante no espaÃ§o original assume uma forma quadrÃ¡tica ou de ordem superior, dependendo da natureza das transformaÃ§Ãµes utilizadas.*

A prova deste Lemma segue da forma da funÃ§Ã£o discriminante no espaÃ§o original. A funÃ§Ã£o discriminante Ã© linear no espaÃ§o transformado, ou seja, $f(h(x)) = \beta_0 + \beta^T h(x)$. Se $h(x)$ incluir termos quadrÃ¡ticos ou de interaÃ§Ã£o, a fronteira de decisÃ£o $f(h(x))=0$ corresponde a uma equaÃ§Ã£o quadrÃ¡tica ou de ordem superior em funÃ§Ã£o dos atributos originais ($x$), logo, a fronteira de decisÃ£o tambÃ©m serÃ¡ quadrÃ¡tica. $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 2: Regression Boundaries"
    direction TB
        A["Input: X"] --> B["Transformation: h(X)"]
        B --> C["Linear Regression: f(h(x)) = (1, h(x)áµ€)B"]
        C --> D["Decision Boundary in h(X): f(h(x)) = 0"]
         D --> E["Quadratic/Higher Order Boundary in Original Space"]
    end
```

**CorolÃ¡rio 2:** *Ao aplicar transformaÃ§Ãµes lineares no espaÃ§o transformado, como Ã© o caso do modelo de regressÃ£o linear, as fronteiras obtidas no espaÃ§o original sempre serÃ£o funÃ§Ãµes lineares das variÃ¡veis do espaÃ§o transformado, e funÃ§Ãµes polinomiais das variÃ¡veis do espaÃ§o original.*

Este corolÃ¡rio reafirma a ideia principal. O modelo de regressÃ£o linear, ajustado com uma transformaÃ§Ã£o nÃ£o linear dos dados, como uma transformaÃ§Ã£o polinomial ou de interaÃ§Ãµes, irÃ¡ produzir uma fronteira linear nos atributos do espaÃ§o transformado, que, ao serem mapeados no espaÃ§o original, resultarÃ£o em fronteiras nÃ£o lineares. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos 5 pontos de dados para duas classes, onde $X = (X_1, X_2)$ e $Y$ Ã© a classe (0 ou 1):
>
> Dados:
>
> $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 2 & 1 \\ 3 & 2 \\ 2 & 3 \end{bmatrix}$, $Y = \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \\ 1 \end{bmatrix}$
>
> Vamos criar um espaÃ§o aumentado com $h(X) = (1, X_1, X_2, X_1^2, X_2^2, X_1X_2)$.
>
> $h(X) = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\ 1 & 1 & 2 & 1 & 4 & 2 \\ 1 & 2 & 1 & 4 & 1 & 2 \\ 1 & 3 & 2 & 9 & 4 & 6 \\ 1 & 2 & 3 & 4 & 9 & 6 \end{bmatrix}$
>
> Agora, podemos usar a fÃ³rmula de mÃ­nimos quadrados para calcular os coeficientes $\mathbf{B}$:
>
> $\mathbf{B} = (\mathbf{h(X)}^T\mathbf{h(X)})^{-1}\mathbf{h(X)}^T\mathbf{Y}$
>
> 1.  **Calcular** $\mathbf{h(X)}^T\mathbf{h(X)}$:
>
> $\mathbf{h(X)}^T\mathbf{h(X)} = \begin{bmatrix} 5 & 9 & 9 & 29 & 19 & 17 \\ 9 & 19 & 17 & 65 & 41 & 47 \\ 9 & 17 & 19 & 41 & 65 & 47 \\ 29 & 65 & 41 & 205 & 145 & 147 \\ 19 & 41 & 65 & 145 & 205 & 147 \\ 17 & 47 & 47 & 147 & 147 & 177 \end{bmatrix}$
>
> 2. **Calcular** $(\mathbf{h(X)}^T\mathbf{h(X)})^{-1}$ (usando numpy):
> ```python
> import numpy as np
> hX = np.array([[1, 1, 1, 1, 1, 1],
>               [1, 1, 2, 1, 4, 2],
>               [1, 2, 1, 4, 1, 2],
>               [1, 3, 2, 9, 4, 6],
>               [1, 2, 3, 4, 9, 6]])
> Y = np.array([0, 0, 0, 1, 1])
> hX_transpose_hX = np.dot(hX.T, hX)
> hX_transpose_hX_inv = np.linalg.inv(hX_transpose_hX)
> print(hX_transpose_hX_inv)
> ```
>
> ```
> [[ 1.34791667e+00 -4.18750000e-01 -4.18750000e-01  2.08333333e-02
>   2.08333333e-02  1.04166667e-01]
>  [-4.18750000e-01  1.87500000e-01  1.25000000e-01 -2.50000000e-02
>  -1.25000000e-02 -1.25000000e-02]
>  [-4.18750000e-01  1.25000000e-01  1.87500000e-01 -1.25000000e-02
>  -2.50000000e-02 -1.25000000e-02]
>  [ 2.08333333e-02 -2.50000000e-02 -1.25000000e-02  8.33333333e-03
>   3.12500000e-03 -3.12500000e-03]
>  [ 2.08333333e-02 -1.25000000e-02 -2.50000000e-02  3.12500000e-03
>   8.33333333e-03 -3.12500000e-03]
>  [ 1.04166667e-01 -1.25000000e-02 -1.25000000e-02 -3.12500000e-03
>  -3.12500000e-03  6.25000000e-03]]
> ```
>
> 3. **Calcular** $\mathbf{h(X)}^T\mathbf{Y}$:
>
> $\mathbf{h(X)}^T\mathbf{Y} = \begin{bmatrix} 2 \\ 5 \\ 5 \\ 13 \\ 13 \\ 12 \end{bmatrix}$
>
> 4. **Calcular** $\mathbf{B}$:
>
> $\mathbf{B} = \begin{bmatrix} -1.625 \\ 0.9375 \\ 0.9375 \\ -0.125 \\ -0.125 \\ 0.25 \end{bmatrix}$
>
> A funÃ§Ã£o discriminante no espaÃ§o aumentado Ã©:
>
> $f(h(x)) = -1.625 + 0.9375 X_1 + 0.9375 X_2 - 0.125 X_1^2 - 0.125 X_2^2 + 0.25 X_1X_2$.
>
> A fronteira de decisÃ£o no espaÃ§o original Ã© dada por $f(h(x)) = 0$, que Ã© uma equaÃ§Ã£o quadrÃ¡tica.

Em resumo, a regressÃ£o linear em um espaÃ§o aumentado transforma o problema de encontrar fronteiras nÃ£o lineares no espaÃ§o original em encontrar um hiperplano em um espaÃ§o de maior dimensÃ£o.

### MÃ©todos de SeleÃ§Ã£o de VariÃ¡veis e RegularizaÃ§Ã£o em ClassificaÃ§Ã£o

```mermaid
graph LR
    subgraph "Variable Selection and Regularization"
    direction TB
        A["Cost Function: -â„“(Î²) + Î» * Penalty(Î²)"]
        A --> B["L1 Penalty (Lasso): Î»Î£|Î²â±¼|"]
        A --> C["L2 Penalty (Ridge): Î»Î£Î²â±¼Â²"]
         A --> D["Elastic Net: Î»â‚Î£|Î²â±¼| + Î»â‚‚Î£Î²â±¼Â²"]
         B --> E["Sparse Solutions in Augmented Space"]
        C --> F["Coefficient Shrinkage"]
        D --> G["Combines Sparsity and Shrinkage"]
    end
```

A seleÃ§Ã£o de variÃ¡veis e a regularizaÃ§Ã£o sÃ£o cruciais ao trabalhar com espaÃ§os de atributos aumentados, pois o aumento da dimensionalidade pode levar a modelos muito complexos e com alto risco de sobreajuste [^4.5]. A regularizaÃ§Ã£o, atravÃ©s da penalizaÃ§Ã£o de coeficientes, visa simplificar o modelo, ao mesmo tempo em que se selecionam as variÃ¡veis mais importantes.

A funÃ§Ã£o de custo geral, quando se aplica regularizaÃ§Ã£o, Ã© dada por:
$$\text{Custo} = - \ell(\beta) + \lambda \text{Penalidade}(\beta),$$
onde $\ell(\beta)$ Ã© a log-verossimilhanÃ§a ou outra funÃ§Ã£o de custo, $\beta$ sÃ£o os coeficientes do modelo no espaÃ§o aumentado e  $\text{Penalidade}(\beta)$ Ã© o termo de penalizaÃ§Ã£o.

*   **PenalizaÃ§Ã£o L1 (Lasso):** Adiciona a penalidade:
    $$\text{Penalidade}_{L1}(\beta) = \sum_{j=1}^q |\beta_j|,$$
    onde $q$ Ã© o nÃºmero de atributos no espaÃ§o aumentado. A penalidade L1 promove esparsidade, selecionando um subconjunto de atributos no espaÃ§o aumentado, que pode corresponder a combinaÃ§Ãµes nÃ£o lineares dos atributos originais.
*   **PenalizaÃ§Ã£o L2 (Ridge):** Adiciona a penalidade:
    $$\text{Penalidade}_{L2}(\beta) = \sum_{j=1}^q \beta_j^2.$$
    Essa penalidade reduz a magnitude dos coeficientes, o que suaviza a fronteira de decisÃ£o.
*   **Elastic Net:** Combina as penalidades L1 e L2:
    $$\text{Penalidade}_{ElasticNet}(\beta) = \lambda_1 \sum_{j=1}^q |\beta_j| + \lambda_2 \sum_{j=1}^q \beta_j^2.$$
     Essa penalidade visa combinar os benefÃ­cios de ambas as penalidades, induzindo a esparsidade e reduzindo a variÃ¢ncia.

**Lemma 3:** *Ao aplicar a regularizaÃ§Ã£o L1 ou o Elastic Net em modelos de classificaÃ§Ã£o em espaÃ§os aumentados, obtemos modelos esparsos, onde muitos dos coeficientes associados aos atributos transformados sÃ£o iguais a zero. Essa esparsidade reflete na complexidade da fronteira de decisÃ£o no espaÃ§o original.*

A prova deste Lemma segue da anÃ¡lise do efeito da norma L1 na funÃ§Ã£o de custo. A penalidade L1 forÃ§a os coeficientes associados aos atributos transformados para zero, levando a modelos esparsos. Elastic Net combina esta propriedade com a penalidade L2, que evita o problema da penalizaÃ§Ã£o excessiva da norma L1, promovendo a seleÃ§Ã£o de variÃ¡veis e evitando o sobreajuste. $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 3: Sparsity with L1/Elastic Net"
    direction TB
         A["L1 Regularization"] --> B["Sparse Solutions: Many Î²â±¼ = 0"]
         A --> C["Elastic Net"]
         C --> B
         B --> D["Simplified Decision Boundary"]
    end
```

**CorolÃ¡rio 3:** *Em modelos de classificaÃ§Ã£o com espaÃ§os aumentados, a regularizaÃ§Ã£o L1 atua como uma ferramenta de seleÃ§Ã£o de atributos, selecionando as transformaÃ§Ãµes de atributos mais importantes para a construÃ§Ã£o da fronteira de decisÃ£o no espaÃ§o original. Essa seleÃ§Ã£o melhora a interpretabilidade do modelo e reduz o risco de sobreajuste.*

Este corolÃ¡rio enfatiza o papel da regularizaÃ§Ã£o para simplificar modelos em espaÃ§os aumentados. Ao selecionar apenas algumas transformaÃ§Ãµes de atributos, a regularizaÃ§Ã£o L1 tambÃ©m aumenta a interpretabilidade do modelo e a sua capacidade de generalizaÃ§Ã£o, por nÃ£o se ajustar excessivamente ao ruÃ­do nos dados de treino. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos usar o mesmo conjunto de dados anterior no espaÃ§o aumentado e aplicar regularizaÃ§Ã£o L1 (Lasso) e L2 (Ridge). Usaremos um valor de $\lambda=0.1$ para ambos.
>
> Usando o mesmo h(X) e Y:
>
> $h(X) = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\ 1 & 1 & 2 & 1 & 4 & 2 \\ 1 & 2 & 1 & 4 & 1 & 2 \\ 1 & 3 & 2 & 9 & 4 & 6 \\ 1 & 2 & 3 & 4 & 9 & 6 \end{bmatrix}$, $Y = \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \\ 1 \end{bmatrix}$
>
> 1.  **Lasso (L1):**
>
>     A funÃ§Ã£o de custo Ã©:  $Custo = - \ell(\beta) + \lambda \sum_{j=1}^q |\beta_j|$
>
>     Usando a biblioteca `sklearn`, podemos encontrar os coeficientes:
> ```python
> from sklearn.linear_model import Lasso
> lasso = Lasso(alpha=0.1)
> lasso.fit(hX, Y)
> print("Lasso Coefficients:", lasso.coef_)
> ```
>     Resultados:
>   ```
>   Lasso Coefficients: [-0.         0.23109842  0.23109842  0.         0.          0.        ]
>   ```
>     Observe que alguns coeficientes sÃ£o exatamente zero, indicando que o Lasso selecionou apenas $X_1$ e $X_2$ como atributos relevantes no espaÃ§o aumentado. A fronteira de decisÃ£o no espaÃ§o original serÃ¡ simplificada.
>
> 2.  **Ridge (L2):**
>
>     A funÃ§Ã£o de custo Ã©: $Custo = - \ell(\beta) + \lambda \sum_{j=1}^q \beta_j^2$
>
>     Usando a biblioteca `sklearn`, podemos encontrar os coeficientes:
> ```python
> from sklearn.linear_model import Ridge
> ridge = Ridge(alpha=0.1)
> ridge.fit(hX, Y)
> print("Ridge Coefficients:", ridge.coef_)
> ```
>     Resultados:
>   ```
>   Ridge Coefficients: [-0.44286707  0.4173996   0.4173996  -0.0653883  -0.0653883   0.09044638]
>   ```
>
>     Os coeficientes do Ridge sÃ£o menores em magnitude, mas nenhum Ã© exatamente zero. A fronteira de decisÃ£o no espaÃ§o original serÃ¡ mais suave do que a obtida sem regularizaÃ§Ã£o.
>
> | Method |  $\beta_0$ | $\beta_1$ | $\beta_2$ | $\beta_3$ | $\beta_4$ | $\beta_5$ |
> |--------|------------|-----------|-----------|-----------|-----------|-----------|
> | OLS    |  -1.625    | 0.9375    | 0.9375    | -0.125    | -0.125    | 0.25      |
> | Lasso  |  -0.0      | 0.231     | 0.231     | 0.0       | 0.0       | 0.0       |
> | Ridge  |  -0.443    | 0.417     | 0.417     | -0.065    | -0.065    | 0.09      |
>
> A tabela acima mostra como a regularizaÃ§Ã£o afeta os coeficientes no espaÃ§o aumentado. O Lasso zerou alguns coeficientes, simplificando o modelo, enquanto o Ridge reduziu a magnitude de todos os coeficientes, suavizando a fronteira de decisÃ£o no espaÃ§o original.

> âš ï¸ **Ponto Crucial**: A regularizaÃ§Ã£o em espaÃ§os aumentados Ã© crucial para evitar o sobreajuste e obter modelos com boa capacidade de generalizaÃ§Ã£o. A escolha da penalidade (L1, L2 ou Elastic Net) depende do objetivo do problema e das caracterÃ­sticas dos dados. **ReferÃªncia ao tÃ³pico [^4.5]**.

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
    subgraph "Hyperplanes and Perceptrons"
    direction TB
        A["Data in Original Space"] --> B["Transformation to Augmented Space h(X)"]
        B --> C["Separating Hyperplane in h(X)"]
        C --> D["Optimization: min ||Î²||Â²  s.t. yáµ¢(Î²áµ€h(xáµ¢)+Î²â‚€) â‰¥ 1"]
        C --> E["Perceptron Algorithm"]
        E --> F["Iteration Update: Î²(new) = Î²(old) + Î·yáµ¢h(xáµ¢)"]
        D & F --> G["Non-linear Decision Boundary in Original Space"]
    end
```

O conceito de **hiperplanos separadores** pode ser estendido para espaÃ§os aumentados, permitindo a construÃ§Ã£o de fronteiras de decisÃ£o nÃ£o lineares no espaÃ§o original. A ideia Ã© transformar os dados originais para um espaÃ§o de maior dimensÃ£o e aplicar o conceito de maximizaÃ§Ã£o da margem nesse espaÃ§o transformado. O problema de otimizaÃ§Ã£o do hiperplano separador no espaÃ§o aumentado Ã© dado por:
$$
\begin{aligned}
  \min_{\beta,\beta_0} \quad & \frac{1}{2} ||\beta||^2 \\
  \text{s.t.} \quad & y_i (\beta^T h(x_i) + \beta_0) \geq 1, \quad \forall i = 1, ..., N,
\end{aligned}
$$
onde $h(x_i)$ Ã© a transformaÃ§Ã£o nÃ£o linear de $x_i$ para o espaÃ§o aumentado e $\beta$ e $\beta_0$ sÃ£o os parÃ¢metros do hiperplano no espaÃ§o aumentado. No espaÃ§o original, a fronteira resultante serÃ¡ nÃ£o linear, devido Ã  transformaÃ§Ã£o $h(x)$.

O **Perceptron de Rosenblatt** tambÃ©m pode ser aplicado em um espaÃ§o aumentado, buscando um hiperplano separador iterativamente. O algoritmo comeÃ§a com um hiperplano aleatÃ³rio no espaÃ§o transformado e, em cada iteraÃ§Ã£o, atualiza os pesos com base nas amostras mal classificadas:
$$ \beta^{new} = \beta^{old} + \eta y_i h(x_i), $$
onde $\eta$ Ã© a taxa de aprendizado, $y_i$ Ã© o rÃ³tulo da classe e $h(x_i)$ Ã© o vetor de atributos transformado da amostra.

A combinaÃ§Ã£o de espaÃ§os aumentados com o Perceptron permite que esse algoritmo obtenha fronteiras de decisÃ£o nÃ£o lineares no espaÃ§o original. Se os dados transformados forem linearmente separÃ¡veis, o algoritmo converge para um hiperplano que corresponderÃ¡ a uma fronteira de decisÃ£o nÃ£o linear no espaÃ§o original.

**Lemma 4:** *Tanto os hiperplanos separadores Ã³timos quanto o Perceptron, quando aplicados em espaÃ§os aumentados com transformaÃ§Ãµes nÃ£o lineares, levam a fronteiras de decisÃ£o nÃ£o lineares no espaÃ§o original. As transformaÃ§Ãµes definem o grau da nÃ£o linearidade das fronteiras.*

A prova deste Lemma reside na anÃ¡lise da funÃ§Ã£o discriminante no espaÃ§o original. Ambos os mÃ©todos, hiperplanos separadores e Perceptron, procuram uma combinaÃ§Ã£o linear dos atributos no espaÃ§o transformado, ou seja, $\beta^T h(x)$. Como a transformaÃ§Ã£o $h(x)$ Ã© nÃ£o linear, a fronteira de decisÃ£o no espaÃ§o original, que Ã© dada pela condiÃ§Ã£o $\beta^T h(x) + \beta_0 = 0$, define uma superfÃ­cie nÃ£o linear, cuja forma depende da transformaÃ§Ã£o $h(x)$.  $\blacksquare$

```mermaid
graph LR
    subgraph "Lemma 4: Non-linear Boundaries from Hyperplanes and Perceptrons"
    direction TB
        A["Transformation h(X)"] --> B["Hyperplane in h(X): Î²áµ€h(x) + Î²