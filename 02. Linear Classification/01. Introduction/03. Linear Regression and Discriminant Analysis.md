## TÃ­tulo Conciso: MÃ©todos Lineares para ClassificaÃ§Ã£o: RegressÃ£o, Discriminantes e Probabilidades

```mermaid
graph LR
    subgraph "Linear Classification Methods"
        direction TB
        A["Linear Methods for Classification"]
        B["Regression on Indicator Variables"]
        C["Discriminant Function Models (LDA, QDA)"]
        D["Probabilistic Models (Logistic Regression)"]
        A --> B
        A --> C
        A --> D
    end
    subgraph "Method Details"
        direction LR
        B --> E["Linear Regression Model Formula"]
        C --> F["Discriminant Function Calculation"]
        D --> G["Logistic Regression Probability Model"]
    end
    subgraph "Model Relationships"
       direction TB
        E --> H["Indicator Variable Encoding"]
        F --> I["Assumptions on Data Distribution"]
        G --> J["Probability Estimation"]
    end
    subgraph "Regularization"
       direction TB
         H --> K["L1/L2/Elastic Net Regularization"]
         I --> K
         J --> K
    end
```

### IntroduÃ§Ã£o

Este capÃ­tulo foca em mÃ©todos lineares para classificaÃ§Ã£o, explorando trÃªs abordagens principais: **RegressÃ£o Linear em VariÃ¡veis Indicadoras**, **Modelos de FunÃ§Ãµes Discriminantes** e **Modelos ProbabilÃ­sticos**. Cada uma dessas abordagens oferece uma maneira distinta de construir fronteiras de decisÃ£o lineares, utilizando combinaÃ§Ãµes lineares dos atributos de entrada para atribuir amostras a diferentes classes [^4.1].

A **RegressÃ£o Linear em VariÃ¡veis Indicadoras** utiliza a regressÃ£o linear para modelar variÃ¡veis binÃ¡rias que indicam a quais classes cada observaÃ§Ã£o pertence, atribuindo a classe com o maior valor predito. Essa abordagem Ã© simples e intuitiva, mas tem limitaÃ§Ãµes em termos de garantir que os valores preditos se comportem como probabilidades. JÃ¡ os **Modelos de FunÃ§Ãµes Discriminantes**, como a AnÃ¡lise Discriminante Linear (LDA) e AnÃ¡lise Discriminante QuadrÃ¡tica (QDA), definem funÃ§Ãµes que determinam a classe de uma observaÃ§Ã£o com base em sua pontuaÃ§Ã£o discriminante. Esses modelos fazem premissas sobre a distribuiÃ§Ã£o dos dados, como a normalidade. Os **Modelos ProbabilÃ­sticos**, como a RegressÃ£o LogÃ­stica, modelam diretamente as probabilidades a posteriori das classes, usando funÃ§Ãµes lineares transformadas, como a funÃ§Ã£o logÃ­stica, e nÃ£o impÃµem premissas sobre a distribuiÃ§Ã£o das variÃ¡veis de entrada.

Ao longo deste capÃ­tulo, vamos detalhar cada um desses mÃ©todos, suas formulaÃ§Ãµes matemÃ¡ticas, algoritmos de ajuste e os cenÃ¡rios em que sÃ£o mais apropriados. Vamos discutir como as premissas de cada abordagem influenciam as decisÃµes de classificaÃ§Ã£o e como a regularizaÃ§Ã£o pode ser utilizada para melhorar a generalizaÃ§Ã£o dos modelos. Vamos tambÃ©m explorar as conexÃµes entre essas trÃªs abordagens, mostrando como diferentes formas de modelagem linear podem levar a resultados similares ou complementares.

### Conceitos Fundamentais

**Conceito 1: O Problema de ClassificaÃ§Ã£o e Abordagens Lineares**

O problema de classificaÃ§Ã£o busca atribuir uma observaÃ§Ã£o $x$ a uma de *K* classes distintas, ou seja, um rÃ³tulo discreto $G$ a partir de um espaÃ§o de atributos $X$. Os mÃ©todos lineares abordam esse problema atravÃ©s da construÃ§Ã£o de funÃ§Ãµes discriminantes lineares, que dividem o espaÃ§o de entrada em regiÃµes associadas a cada classe, ou modelando diretamente as probabilidades das classes utilizando funÃ§Ãµes lineares.

As abordagens lineares se caracterizam pela simplicidade, facilidade de interpretaÃ§Ã£o e eficiÃªncia computacional. No entanto, essa abordagem pode nÃ£o ser adequada para problemas com fronteiras de decisÃ£o complexas. A escolha entre diferentes mÃ©todos lineares depende das premissas sobre a distribuiÃ§Ã£o dos dados, do objetivo do problema (classificaÃ§Ã£o ou estimaÃ§Ã£o de probabilidades) e da necessidade de interpretabilidade.

**Lemma 1:** *Modelos lineares de classificaÃ§Ã£o produzem fronteiras de decisÃ£o lineares, e.g., hiperplanos no espaÃ§o de atributos, independentemente de serem modelos de regressÃ£o com variÃ¡veis indicadoras, modelos de funÃ§Ãµes discriminantes ou modelos probabilÃ­sticos baseados em transformaÃ§Ã£o linear.*

A prova deste lemma pode ser feita atravÃ©s da anÃ¡lise das diferentes formulaÃ§Ãµes. Em todos os casos, a decisÃ£o de classe Ã© baseada em uma funÃ§Ã£o linear, que separa as classes. Na regressÃ£o com variÃ¡veis indicadoras, a classe Ã© definida pelo maior valor de uma combinaÃ§Ã£o linear das variÃ¡veis. Nos modelos discriminantes, a fronteira de decisÃ£o Ã© dada por onde duas funÃ§Ãµes discriminantes lineares sÃ£o iguais. Nos modelos probabilÃ­sticos, a fronteira de decisÃ£o tambÃ©m Ã© obtida por uma funÃ§Ã£o linear, como o log-odds, que Ã© linear em termos dos atributos. Portanto, todos os modelos lineares levam a hiperplanos. $\blacksquare$

**Conceito 2: Modelos de FunÃ§Ãµes Discriminantes: LDA e QDA**

Os modelos de funÃ§Ãµes discriminantes, como a AnÃ¡lise Discriminante Linear (LDA) e a AnÃ¡lise Discriminante QuadrÃ¡tica (QDA), sÃ£o mÃ©todos de classificaÃ§Ã£o que buscam construir funÃ§Ãµes discriminantes $\delta_k(x)$ para cada classe *k*, de modo que a observaÃ§Ã£o $x$ seja atribuÃ­da Ã  classe que maximiza $\delta_k(x)$. A LDA assume que as classes tÃªm distribuiÃ§Ãµes Gaussianas com uma mesma matriz de covariÃ¢ncia ($\Sigma$), enquanto que o QDA nÃ£o faz essa premissa.

```mermaid
graph LR
    subgraph "LDA Discriminant Function"
        direction TB
        A["Î´_k(x) for LDA"]
        B["x^T Î£â»Â¹ Î¼_k"]
        C["- 1/2 Î¼_k^T Î£â»Â¹ Î¼_k"]
        D["log(Ï€_k)"]
        A --> B
        A --> C
        A --> D
    end
    subgraph "QDA Discriminant Function"
        direction TB
        E["Î´_k(x) for QDA"]
        F["-1/2 (x-Î¼_k)^T Î£_kâ»Â¹ (x-Î¼_k)"]
        G["log(Ï€_k)"]
        E --> F
        E --> G
    end
    subgraph "Key Differences"
      direction LR
        H["Common Covariance Matrix (LDA)"]
        I["Individual Covariance Matrices (QDA)"]
        H --> J["Linear Decision Boundaries"]
        I --> K["Quadratic Decision Boundaries"]
    end
```

No LDA, a funÃ§Ã£o discriminante Ã© dada por:
$$\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log(\pi_k),$$
que Ã© uma funÃ§Ã£o linear de $x$, onde $\mu_k$ Ã© a mÃ©dia da classe *k*, $\pi_k$ Ã© a probabilidade a priori da classe *k* e $\Sigma$ Ã© a matriz de covariÃ¢ncia comum. Em contraste, no QDA, as funÃ§Ãµes discriminantes sÃ£o quadrÃ¡ticas em *x* devido Ã  ausÃªncia da premissa de covariÃ¢ncia comum:
$$\delta_k(x) = -\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + \log(\pi_k).$$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos considerar um exemplo simplificado com duas classes e duas caracterÃ­sticas. Suponha que temos as seguintes mÃ©dias e matriz de covariÃ¢ncia para LDA:
>
> - Classe 1: $\mu_1 = [1, 1]^T$
> - Classe 2: $\mu_2 = [3, 3]^T$
> - Matriz de covariÃ¢ncia comum: $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$
> - Probabilidades a priori: $\pi_1 = 0.6$, $\pi_2 = 0.4$
>
> Para calcular a funÃ§Ã£o discriminante para um ponto $x = [2, 2]^T$, primeiro precisamos calcular $\Sigma^{-1}$:
>
> $\Sigma^{-1} = \frac{1}{(1*1 - 0.5*0.5)} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \frac{1}{0.75} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \begin{bmatrix} 1.33 & -0.67 \\ -0.67 & 1.33 \end{bmatrix}$
>
> Agora, calculemos $\delta_1(x)$ e $\delta_2(x)$:
>
> $\delta_1(x) = \begin{bmatrix} 2 & 2 \end{bmatrix} \begin{bmatrix} 1.33 & -0.67 \\ -0.67 & 1.33 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 1.33 & -0.67 \\ -0.67 & 1.33 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} + \log(0.6)$
>
> $\delta_1(x) = \begin{bmatrix} 2 & 2 \end{bmatrix} \begin{bmatrix} 0.66 \\ 0.66 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 0.66 \\ 0.66 \end{bmatrix} + \log(0.6)$
>
> $\delta_1(x) = 2.64 - 0.66 + (-0.51) = 1.47$
>
> $\delta_2(x) = \begin{bmatrix} 2 & 2 \end{bmatrix} \begin{bmatrix} 1.33 & -0.67 \\ -0.67 & 1.33 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 3 & 3 \end{bmatrix} \begin{bmatrix} 1.33 & -0.67 \\ -0.67 & 1.33 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix} + \log(0.4)$
>
> $\delta_2(x) = \begin{bmatrix} 2 & 2 \end{bmatrix} \begin{bmatrix} 1.98 \\ 1.98 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 3 & 3 \end{bmatrix} \begin{bmatrix} 1.98 \\ 1.98 \end{bmatrix} + \log(0.4)$
>
> $\delta_2(x) = 7.92 - 5.94 + (-0.92) = 1.06$
>
> Como $\delta_1(x) > \delta_2(x)$, o ponto $x$ seria classificado como pertencente Ã  classe 1.

**CorolÃ¡rio 1:** *Enquanto a LDA gera funÃ§Ãµes discriminantes lineares devido Ã  premissa de covariÃ¢ncia comum, o QDA gera funÃ§Ãµes discriminantes quadrÃ¡ticas devido Ã  estimaÃ§Ã£o de diferentes matrizes de covariÃ¢ncia para cada classe, resultando em fronteiras de decisÃ£o lineares e nÃ£o lineares, respectivamente.*

A prova deste corolÃ¡rio pode ser feita atravÃ©s da comparaÃ§Ã£o das funÃ§Ãµes discriminantes dos dois mÃ©todos. No LDA, a premissa de igualdade das matrizes de covariÃ¢ncia entre as classes leva ao cancelamento dos termos quadrÃ¡ticos na equaÃ§Ã£o, resultando em um funÃ§Ã£o discriminante linear. No QDA, ao assumir matrizes de covariÃ¢ncia distintas, os termos quadrÃ¡ticos nÃ£o se cancelam, levando a funÃ§Ãµes quadrÃ¡ticas. $\blacksquare$

**Conceito 3: Modelos ProbabilÃ­sticos: RegressÃ£o LogÃ­stica**

A RegressÃ£o LogÃ­stica Ã© um modelo probabilÃ­stico que modela diretamente a probabilidade posterior de uma observaÃ§Ã£o *x* pertencer a uma classe *k* por meio de uma funÃ§Ã£o linear transformada pela funÃ§Ã£o logÃ­stica (sigmoide). Para o caso binÃ¡rio (duas classes), a probabilidade de *x* pertencer Ã  classe 1 Ã© dada por:
$$P(G=1|X=x) = \frac{\exp(\beta_0 + \beta^T x)}{1+\exp(\beta_0 + \beta^T x)},$$
onde $\beta_0$ e $\beta$ sÃ£o os parÃ¢metros do modelo.

```mermaid
graph LR
  subgraph "Logistic Regression"
    direction TB
    A["P(G=1|X=x)"]
    B["exp(Î²_0 + Î²^T x)"]
    C["1 + exp(Î²_0 + Î²^T x)"]
    A --> B
    A --> C
  end
    subgraph "Model Parameters"
        direction LR
        D["Î²_0"] --> E["Intercept Term"]
        F["Î²"] --> G["Coefficient Vector"]
    end
     subgraph "Model Behavior"
       direction TB
        H["Transforms linear function into a probability"]
        I["Does not assume input distribution"]
         B --> H
         C --> H
         H --> I
     end
```

A RegressÃ£o LogÃ­stica nÃ£o faz suposiÃ§Ãµes sobre a distribuiÃ§Ã£o das variÃ¡veis de entrada, diferentemente do LDA. Os parÃ¢metros sÃ£o geralmente estimados por mÃ¡xima verossimilhanÃ§a, ou seja, os parÃ¢metros que maximizam a probabilidade dos dados observados dada a forma funcional do modelo.

> âš ï¸ **Nota Importante**:  A RegressÃ£o LogÃ­stica modela diretamente as probabilidades posteriores, enquanto o LDA deriva as probabilidades a partir de suposiÃ§Ãµes sobre as distribuiÃ§Ãµes das variÃ¡veis de entrada. A escolha entre eles depende do objetivo da modelagem e das caracterÃ­sticas dos dados. **ReferÃªncia ao tÃ³pico [^4.5]**.

> â— **Ponto de AtenÃ§Ã£o**: Em situaÃ§Ãµes com classes desbalanceadas, Ã© crucial usar tÃ©cnicas de balanceamento para evitar vieses nas estimativas da RegressÃ£o LogÃ­stica. **Conforme indicado em [^4.4.2]**.

> âœ”ï¸ **Destaque**: LDA, QDA e RegressÃ£o LogÃ­stica, embora distintos em suas formulaÃ§Ãµes, representam abordagens importantes para classificaÃ§Ã£o linear, cada um com suas particularidades e aplicabilidades. **Baseado nos tÃ³picos [^4.3] e [^4.4]**.

### RegressÃ£o Linear e MÃ­nimos Quadrados para ClassificaÃ§Ã£o

```mermaid
graph LR
    subgraph "Linear Regression for Classification"
    direction TB
    A["Input Data (X) and Classes (G)"]
    B["Encode Classes into Indicator Variables (Yk)"]
    C["Create Indicator Matrix (Y)"]
    D["Fit Linear Model (Y_hat = XB) using Least Squares"]
    E["Discriminant Function (f(x) = x^T B)"]
    F["Assign new observation to class with highest f(x)"]
     A --> B
     B --> C
     C --> D
     D --> E
     E --> F
    end
    subgraph "Mathematical Details"
        direction LR
        G["Y = XB + E"] --> H["B = (X^T X)^-1 X^T Y"]
         G --> I["Least Squares Solution"]
         H --> I
        J["f(x) = (1, x^T)B"]
        I --> J
    end
```

A regressÃ£o linear pode ser usada para classificaÃ§Ã£o atravÃ©s da codificaÃ§Ã£o das classes em variÃ¡veis indicadoras. Se o problema de classificaÃ§Ã£o tem *K* classes, para cada amostra *i* Ã© definido um vetor $y_i$, de tamanho *K*, onde $y_{ik} = 1$ se a amostra *i* pertence Ã  classe *k*, e $y_{ik} = 0$ caso contrÃ¡rio. A matriz de indicadores $\mathbf{Y}$ Ã© uma matriz $N \times K$, onde $N$ Ã© o nÃºmero de amostras. Em seguida, ajustamos um modelo de regressÃ£o linear para cada coluna de $\mathbf{Y}$ atravÃ©s da equaÃ§Ã£o:
$$\mathbf{Y} = \mathbf{X}\mathbf{B} + \mathbf{E},$$
onde $\mathbf{X}$ Ã© a matriz de design com as variÃ¡veis de entrada, $\mathbf{B}$ Ã© a matriz de coeficientes e $\mathbf{E}$ Ã© a matriz de erros. A soluÃ§Ã£o de mÃ­nimos quadrados para $\mathbf{B}$ Ã©:
$$\mathbf{B} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}.$$

Para classificar uma nova observaÃ§Ã£o $x$, computamos os valores preditos para cada classe atravÃ©s da equaÃ§Ã£o:
$$f(x) = (1, x^T)\mathbf{B}.$$
A classe predita Ã© aquela correspondente ao maior valor predito:
$$\hat{G}(x) = \arg\max_k f_k(x).$$

Embora esse mÃ©todo seja simples e intuitivo, ele tem algumas limitaÃ§Ãµes. A regressÃ£o linear nÃ£o garante que os valores preditos estejam no intervalo [0,1] ou que a soma dos valores preditos para uma mesma observaÃ§Ã£o seja igual a 1. AlÃ©m disso, esse mÃ©todo Ã© muito sensÃ­vel a outliers e pode nÃ£o produzir bons resultados em casos em que as classes nÃ£o sÃ£o bem separadas.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um problema de classificaÃ§Ã£o binÃ¡ria com 3 amostras e 2 atributos. Os dados sÃ£o:
>
> - Amostra 1: $x_1 = [1, 2]^T$, Classe 1
> - Amostra 2: $x_2 = [2, 1]^T$, Classe 1
> - Amostra 3: $x_3 = [3, 4]^T$, Classe 2
>
> Primeiro, criamos a matriz de design $\mathbf{X}$ (adicionando uma coluna de 1s para o intercepto) e a matriz de indicadores $\mathbf{Y}$:
>
> $\mathbf{X} = \begin{bmatrix} 1 & 1 & 2 \\ 1 & 2 & 1 \\ 1 & 3 & 4 \end{bmatrix}$
>
> $\mathbf{Y} = \begin{bmatrix} 1 & 0 \\ 1 & 0 \\ 0 & 1 \end{bmatrix}$
>
> Agora, calculamos $\mathbf{B} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$.
>
> $\mathbf{X}^T\mathbf{X} = \begin{bmatrix} 3 & 6 & 7 \\ 6 & 14 & 16 \\ 7 & 16 & 21 \end{bmatrix}$
>
> $(\mathbf{X}^T\mathbf{X})^{-1} = \begin{bmatrix} 5.33 & -2.67 & 0.67 \\ -2.67 & 1.67 & -0.33 \\ 0.67 & -0.33 & 0.07 \end{bmatrix}$
>
> $\mathbf{X}^T\mathbf{Y} = \begin{bmatrix} 2 & 1 \\ 3 & 2 \\ 7 & 4 \end{bmatrix}$
>
> $\mathbf{B} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} = \begin{bmatrix} 5.33 & -2.67 & 0.67 \\ -2.67 & 1.67 & -0.33 \\ 0.67 & -0.33 & 0.07 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 3 & 2 \\ 7 & 4 \end{bmatrix} = \begin{bmatrix} 1.67 & 0.33 \\ -0.67 & 0.33 \\ 0.07 & -0.07 \end{bmatrix}$
>
> Para classificar um novo ponto $x_{new} = [2, 3]^T$, adicionamos o intercepto e calculamos $f(x)$:
>
> $f(x_{new}) = \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 1.67 & 0.33 \\ -0.67 & 0.33 \\ 0.07 & -0.07 \end{bmatrix} = \begin{bmatrix} 0.43 & 0.43 \end{bmatrix}$
>
> Como $f_1(x_{new}) > f_2(x_{new})$, a nova observaÃ§Ã£o seria classificada como Classe 1.

**Lemma 2:** *A fronteira de decisÃ£o obtida pela regressÃ£o linear de variÃ¡veis indicadoras, em problemas de classificaÃ§Ã£o binÃ¡ria com codificaÃ§Ã£o -1 e 1, Ã© proporcional Ã  obtida pela AnÃ¡lise Discriminante Linear (LDA) com a mesma codificaÃ§Ã£o, indicando que ambos os modelos geram separaÃ§Ãµes lineares equivalentes em alguns casos.*

A prova deste Lemma reside na comparaÃ§Ã£o das soluÃ§Ãµes obtidas pelos dois mÃ©todos. Na regressÃ£o linear com codificaÃ§Ã£o -1 e 1, a soluÃ§Ã£o de mÃ­nimos quadrados para o vetor de coeficientes Ã© proporcional ao vetor de diferenÃ§a das mÃ©dias das classes ponderado pela inversa da matriz de covariÃ¢ncia, que Ã© o mesmo vetor definido para a funÃ§Ã£o discriminante na LDA. Essa proporcionalidade indica que os dois mÃ©todos produzem a mesma fronteira de decisÃ£o linear. $\blacksquare$

**CorolÃ¡rio 2:** *A equivalÃªncia entre a regressÃ£o linear em variÃ¡veis indicadoras e o LDA em problemas de classificaÃ§Ã£o binÃ¡ria, sob certas premissas, demonstra que ambos os mÃ©todos exploram conceitos similares para definir fronteiras de decisÃ£o lineares, apesar de serem derivados de formulaÃ§Ãµes distintas.*

O corolÃ¡rio segue diretamente do Lemma 2, e demonstra que os dois modelos, embora baseados em princÃ­pios diferentes, conseguem chegar a decisÃµes similares, em especial para o cenÃ¡rio binÃ¡rio com uma codificaÃ§Ã£o especÃ­fica. $\blacksquare$

Em alguns cenÃ¡rios, como os de dados bem separados, a regressÃ£o linear pode ser suficiente e atÃ© mesmo vantajosa quando o objetivo principal Ã© encontrar uma fronteira de decisÃ£o linear, sem necessidade de uma interpretaÃ§Ã£o probabilÃ­stica.

### MÃ©todos de SeleÃ§Ã£o de VariÃ¡veis e RegularizaÃ§Ã£o em ClassificaÃ§Ã£o

```mermaid
graph LR
    subgraph "Variable Selection & Regularization"
        direction TB
        A["Variable Selection and Regularization"]
        B["L1 Penalty (Lasso)"]
        C["L2 Penalty (Ridge)"]
        D["Elastic Net"]
        E["Subset Selection"]
        A --> B
        A --> C
        A --> D
        A --> E
    end
      subgraph "Mathematical Formulation"
        direction LR
        F["Cost Function = -log-likelihood + Î» * Penalty"]
        G["L1 Penalty: sum(|Î²_j|)"]
        H["L2 Penalty: sum(Î²_jÂ²)"]
        I["Elastic Net: Î»1*sum(|Î²_j|) + Î»2*sum(Î²_jÂ²)"]
        F --> G
        F --> H
        F --> I

     end
      subgraph "Model Effects"
       direction TB
         J["L1 -> Sparsity"]
         K["L2 -> Variance Reduction"]
          G --> J
          H --> K
        I --> L["Combines L1 & L2 effects"]
     end
```

A seleÃ§Ã£o de variÃ¡veis e a regularizaÃ§Ã£o sÃ£o tÃ©cnicas cruciais para lidar com a complexidade e evitar o sobreajuste em modelos de classificaÃ§Ã£o linear, especialmente quando o nÃºmero de atributos Ã© grande [^4.5]. A **seleÃ§Ã£o de variÃ¡veis** visa escolher um subconjunto dos atributos que sÃ£o mais relevantes para a classificaÃ§Ã£o, reduzindo a dimensionalidade do espaÃ§o de entrada e melhorando a interpretabilidade do modelo. A **regularizaÃ§Ã£o** adiciona uma penalidade Ã  funÃ§Ã£o de custo, controlando a magnitude dos parÃ¢metros do modelo e reduzindo o risco de sobreajuste.

A funÃ§Ã£o de custo geral para modelos lineares de classificaÃ§Ã£o com regularizaÃ§Ã£o Ã© da forma:
$$\text{Custo} = -\ell(\beta) + \lambda \cdot \text{Penalidade}(\beta),$$
onde $\ell(\beta)$ Ã© a log-verossimilhanÃ§a, $\beta$ Ã© o vetor de parÃ¢metros e $\text{Penalidade}(\beta)$ Ã© o termo de penalizaÃ§Ã£o, e $\lambda$ Ã© um hiperparÃ¢metro que controla a intensidade da regularizaÃ§Ã£o.

A **penalizaÃ§Ã£o L1 (Lasso)** usa a norma L1 dos coeficientes:
$$\text{Penalidade}_{L1}(\beta) = \sum_{j=1}^p |\beta_j|.$$
Essa penalidade tem o efeito de induzir a esparsidade, levando a modelos onde muitos coeficientes sÃ£o iguais a zero, selecionando automaticamente um subconjunto de variÃ¡veis mais relevantes.

A **penalizaÃ§Ã£o L2 (Ridge)** usa a norma L2 dos coeficientes:
$$\text{Penalidade}_{L2}(\beta) = \sum_{j=1}^p \beta_j^2.$$
Essa penalidade reduz a magnitude dos coeficientes, diminuindo a variÃ¢ncia do modelo, mas geralmente nÃ£o leva a soluÃ§Ãµes esparsas.

O **Elastic Net** combina as penalidades L1 e L2:
$$\text{Penalidade}_{ElasticNet}(\beta) = \lambda_1 \sum_{j=1}^p |\beta_j| + \lambda_2 \sum_{j=1}^p \beta_j^2.$$
Essa abordagem visa combinar os benefÃ­cios de ambas as penalidades, induzindo a esparsidade e reduzindo a variÃ¢ncia.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos um problema de regressÃ£o logÃ­stica com 3 atributos ($x_1, x_2, x_3$) e os seguintes coeficientes obtidos por mÃ¡xima verossimilhanÃ§a sem regularizaÃ§Ã£o: $\beta = [0.8, -1.2, 0.5]$. Vamos calcular as penalidades para diferentes mÃ©todos com $\lambda = 0.1$.
>
> **Lasso (L1):**
>
> $\text{Penalidade}_{L1}(\beta) = |0.8| + |-1.2| + |0.5| = 2.5$
>
> $\text{Custo}_{L1} = -\ell(\beta) + 0.1 * 2.5 = -\ell(\beta) + 0.25$
>
> **Ridge (L2):**
>
> $\text{Penalidade}_{L2}(\beta) = 0.8^2 + (-1.2)^2 + 0.5^2 = 0.64 + 1.44 + 0.25 = 2.33$
>
> $\text{Custo}_{L2} = -\ell(\beta) + 0.1 * 2.33 = -\ell(\beta) + 0.233$
>
> **Elastic Net (com $\lambda_1 = 0.05$ e $\lambda_2 = 0.05$):**
>
> $\text{Penalidade}_{ElasticNet}(\beta) = 0.05 * 2.5 + 0.05 * 2.33 = 0.125 + 0.1165 = 0.2415$
>
> $\text{Custo}_{ElasticNet} = -\ell(\beta) + 0.2415$
>
> A regularizaÃ§Ã£o L1 penaliza mais fortemente os coeficientes, induzindo esparsidade. Ridge penaliza menos, encolhendo os coeficientes. Elastic Net combina os dois efeitos. Em termos de custo, a penalizaÃ§Ã£o Ã© adicionada ao custo original, afetando o processo de otimizaÃ§Ã£o.
>
> Suponha que, apÃ³s aplicar a regularizaÃ§Ã£o, os coeficientes se tornem:
>
> - Lasso: $\beta_{L1} = [0.5, -0.8, 0]$
> - Ridge: $\beta_{L2} = [0.7, -1.1, 0.4]$
> - Elastic Net: $\beta_{EN} = [0.6, -0.9, 0.2]$
>
> O Lasso zerou o coeficiente de $x_3$, indicando que este atributo pode nÃ£o ser relevante para o modelo. Ridge e Elastic Net encolheram os coeficientes, mas nÃ£o os zeraram.

**Lemma 3:** *A penalizaÃ§Ã£o L1, aplicada a modelos lineares de classificaÃ§Ã£o, produz uma soluÃ§Ã£o com um nÃºmero menor de coeficientes diferentes de zero em comparaÃ§Ã£o com a penalizaÃ§Ã£o L2 ou a ausÃªncia de regularizaÃ§Ã£o, facilitando a seleÃ§Ã£o de variÃ¡veis.*

A prova deste Lemma se baseia nas propriedades da norma L1. A norma L1 nÃ£o Ã© diferenciÃ¡vel na origem, o que leva a um â€œempurrÃ£oâ€ dos coeficientes para zero durante o processo de otimizaÃ§Ã£o, ao contrÃ¡rio da norma L2, que Ã© diferenciÃ¡vel em toda a sua extensÃ£o. O efeito Ã© que a penalizaÃ§Ã£o L1 tende a favorecer soluÃ§Ãµes esparsas, enquanto que a L2 tende a â€œencolherâ€ todos os coeficientes uniformemente. $\blacksquare$

**CorolÃ¡rio 3:** *Modelos lineares de classificaÃ§Ã£o, com a aplicaÃ§Ã£o da regularizaÃ§Ã£o L1, oferecem uma forma eficaz de seleÃ§Ã£o de variÃ¡veis, selecionando automaticamente os atributos mais importantes para a classificaÃ§Ã£o e reduzindo a complexidade do modelo.*

Este corolÃ¡rio segue diretamente do Lemma 3, indicando que a penalizaÃ§Ã£o L1 nÃ£o apenas reduz o risco de sobreajuste, mas tambÃ©m aumenta a interpretabilidade do modelo, ao focar apenas nos atributos mais relevantes. $\blacksquare$

> âš ï¸ **Ponto Crucial**: A escolha do tipo de regularizaÃ§Ã£o (L1, L2 ou Elastic Net) depende do objetivo do modelo e das caracterÃ­sticas dos dados. L1 Ã© apropriada para seleÃ§Ã£o de variÃ¡veis, enquanto que L2 Ã© mais apropriada para problemas que necessitam de reduÃ§Ã£o da variÃ¢ncia. **Conforme discutido em [^4.5]**.

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
    subgraph "Optimal Separating Hyperplane"
    direction TB
        A["Data Points from Two Classes"]
        B["Separating Hyperplane"]
        C["Margin between classes"]
        D["Support Vectors"]
        A --> B
        B --> C
        C --> D
    end
     subgraph "Optimization Problem"
        direction LR
            E["Minimize 1/2 ||Î²||Â²"]
            F["Subject to y_i (Î²^T x_i + Î²_0) >= 1"]
            E --> F
        end
    subgraph "Perceptron Algorithm"
     direction TB
         G["Iterative Parameter Update"]
            H["Initial Hyperplane"]
            I["Update Î² based on misclassified samples"]
            J["Î²_new = Î²_old + Î· y_i x_i"]
            K["Converges to a separating hyperplane if data is linearly separable"]
            G --> H
            G --> I
            I --> J
            J --> K
    end

```

A abordagem de **hiperplanos separadores** visa encontrar uma fronteira linear que nÃ£o apenas separa as classes, mas tambÃ©m maximiza a margem entre elas, onde a margem Ã© definida como a menor distÃ¢ncia entre o hiperplano e as amostras mais prÃ³ximas de cada classe, chamadas de **vetores de suporte** [^4.5.2]. Essa abordagem Ã© fundamental em mÃ©todos como as **Support Vector Machines (SVM)**.

O problema de otimizaÃ§Ã£o para encontrar o hiperplano separador Ã³timo pode ser expresso como:
$$
\begin{aligned}
    \max_{\beta,\beta_0} \quad & M \\
    \text{s.t.} \quad & y_i (\beta^T x_i + \beta_0) \geq M, \quad \forall i = 1, \ldots, N,
\end{aligned}
$$
onde $M$ Ã© a largura da margem, $\beta$ Ã© o vetor normal ao hiperplano, $\beta_0$ Ã© o intercepto e $y_i$ Ã© o rÃ³tulo da classe da observaÃ§Ã£o $x_i$. Ao fixar a margem em 1, podemos transformar o problema em:
$$
\begin{aligned}
  \min_{\beta,\beta_0} \quad & \frac{1}{2} ||\beta||^2 \\
  \text{s.t.} \quad & y_i (\beta^T x_i + \beta_0) \geq 1, \quad \forall i = 1, \ldots, N,
\end{aligned}
$$
A soluÃ§Ã£o deste problema Ã© dada por uma combinaÃ§Ã£o linear dos vetores de suporte.

O **Perceptron de Rosenblatt**, Ã© um algoritmo iterativo que busca um hiperplano separador, mas ao contrÃ¡rio da abordagem da margem mÃ¡xima, o Perceptron busca um hiperplano que apenas separa as classes, nÃ£o necessariamente com a maior margem [^4.5.1]. O algoritmo comeÃ§a com um hiperplano aleatÃ³rio, e em cada iteraÃ§Ã£o, ele atualiza os parÃ¢metros ($\beta$) com base nas amostras que sÃ£o mal classificadas. A atualizaÃ§Ã£o Ã© feita da seguinte forma:
$$\beta^{new} = \beta^{old} + \eta y_i x_i,$$
onde $\eta$ Ã© a taxa de aprendizado, $y_i$ Ã© o rÃ³tulo da classe e $x_i$ sÃ£o os atributos da observaÃ§Ã£o mal classificada.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos ilustrar o algoritmo do Perceptron com um exemplo simplificado de duas classes e duas features.
>
> Dados:
> - Classe 1 (y=1): $x_1 = [1, 2]^T$, $x_2 = [2, 1]^T$
> - Classe 2 (y=-1): $x_3 = [3, 4]^T$, $x_4 = [4, 3]^T$
>
> InicializaÃ§Ã£o: $\beta = [0.1, 0.1]^T$, $\beta_0 = 0.1$, taxa de aprendizado $\eta = 0.1$.
>
> **IteraÃ§Ã£o 1:**
>
> -  Amostra $x_1$: $y_1(\beta^T x_1 + \beta_0) = 1 * (0.1 * 1 + 0.1 * 2 + 0.1) = 0.4 > 0$. Classificada corretamente.
> -  Amostra $x_2$: $y_2(\beta^T x_2 + \beta_0) = 1 * (0.1 * 2 + 0.1 * 1 + 0.1) = 0.4 > 0$. Classificada corretamente.
> -  Amostra $x_3$: $y_3(\beta^T x_3 + \beta_0) = -1 * (0.1 * 3 + 0.1 * 4 + 0.1) = -0.8 < 0$. Classificada corretamente.
> -  Amostra $x_4$: $y_4(\beta^T x_4 + \beta_0) = -1 * (0.1 * 4 + 0.1 * 3 + 0.1) = -0.8 < 0$. Classificada corretamente.
>
> Como todas as amostras foram classificadas corretamente, nÃ£o hÃ¡ atualizaÃ§Ã£o nesta iteraÃ§Ã£o.
>
> Vamos supor que na iteraÃ§Ã£o seguinte, os parÃ¢metros sejam: $\beta = [0.2, -0.1]^T$ e $\beta_0 = 0.2$
>
> **IteraÃ§Ã£o 2:**
>
> -  Amostra $x_1$: $y_1(\beta^T x_1 + \beta_0) = 1 * (0.2 * 1 - 0.1 * 2 + 0.2) = 0.2 > 0$. Classificada corretamente