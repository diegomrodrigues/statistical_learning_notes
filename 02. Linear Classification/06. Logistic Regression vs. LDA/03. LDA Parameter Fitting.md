## Estimação de Parâmetros em Análise Discriminante Linear via Máxima Verossimilhança

### Introdução

No âmbito dos métodos lineares para classificação, a **Análise Discriminante Linear (LDA - Linear Discriminant Analysis)** representa uma abordagem fundamental, particularmente quando se assume que as densidades de probabilidade condicionais das classes seguem uma distribuição Gaussiana [^8]. Conforme introduzido anteriormente [^6, ^8], a teoria de decisão Bayesiana para classificação requer o conhecimento das probabilidades a posteriori $Pr(G=k|X=x)$, as quais podem ser obtidas a partir das densidades condicionais de classe $f_k(x)$ e das probabilidades a priori $\pi_k$ através do teorema de Bayes (4.7) [^8]. Enquanto métodos como a regressão logística modelam diretamente as probabilidades a posteriori $Pr(G=k|X)$ através de uma abordagem de máxima verossimilhança *condicional* [^20, ^27], a LDA adota uma perspectiva diferente. Este capítulo foca especificamente no processo de estimação dos parâmetros da LDA através da maximização da *log-verossimilhança completa (full log-likelihood)*, fundamentada na densidade de probabilidade conjunta $Pr(X, G=k)$ [^27]. Esta abordagem contrasta com a verossimilhança condicional utilizada na regressão logística e baseia-se em suposições mais fortes sobre a distribuição dos dados, nomeadamente a Gaussianidade das densidades de classe com uma matriz de covariância comum [^8].

### Conceitos Fundamentais

#### Modelo Probabilístico Gaussiano para LDA

A derivação fundamental da LDA assume que a densidade de probabilidade condicional para cada classe $k$, denotada por $f_k(x) = Pr(X=x|G=k)$, segue uma distribuição **Gaussiana multivariada** [^8]. Uma suposição crucial adicional é que todas as classes compartilham uma **matriz de covariância comum**, $\Sigma_k = \Sigma$ para todo $k=1, ..., K$ [^8]. Sob estas condições, a densidade condicional para a classe $k$ é dada por:

$$\
f_k(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (x-\mu_k)^T \Sigma^{-1} (x-\mu_k)\right)
$$

Esta é a expressão (4.8) apresentada no contexto [^8], onde $\mu_k$ é o vetor de médias da classe $k$, $\Sigma$ é a matriz de covariância comum $p \times p$, e $p$ é a dimensionalidade do espaço de entrada $X$. Adicionalmente, $\pi_k$ representa a probabilidade a priori da classe $k$, com a restrição $\sum_{k=1}^K \pi_k = 1$ [^6].

#### Densidade Conjunta e a Função de Log-Verossimilhança Completa

Diferentemente da regressão logística, que foca na modelagem de $Pr(G=k|X)$ e maximiza a verossimilhança condicional [^20, ^27], a LDA estima os parâmetros $\theta = (\\{\mu_k\\}_{k=1}^K, \Sigma, \\{\pi_k\\}_{k=1}^K)$ maximizando a **log-verossimilhança completa (full log-likelihood)** [^27]. Esta abordagem baseia-se na **densidade de probabilidade conjunta** de observar $X$ e a classe $G=k$, que é dada por:

$$\
Pr(X, G=k) = Pr(X|G=k) Pr(G=k) = f_k(X) \pi_k
$$

Substituindo a densidade Gaussiana $f_k(X)$, obtemos a expressão explícita para a densidade conjunta, conforme indicado em (4.37) [^27], onde $\phi(X; \mu_k, \Sigma)$ é a notação para a função de densidade Gaussiana [^28]:

$$\
Pr(X, G=k) = \phi(X; \mu_k, \Sigma) \pi_k
$$

Dado um conjunto de $N$ observações de treinamento $(x_i, g_i)$, onde $x_i \in \mathbb{R}^p$ é o vetor de preditores e $g_i \in \\{1, ..., K\\}$ é o rótulo da classe para a $i$-ésima observação, a função de verossimilhança completa é o produto das densidades conjuntas avaliadas em cada ponto de dados:

$$\
L(\theta) = \prod_{i=1}^N Pr(x_i, G=g_i) = \prod_{i=1}^N \phi(x_i; \mu_{g_i}, \Sigma) \pi_{g_i}
$$

A log-verossimilhança completa é, portanto:

$$\
l(\theta) = \log L(\theta) = \sum_{i=1}^N \log[\phi(x_i; \mu_{g_i}, \Sigma) \pi_{g_i}] = \sum_{i=1}^N [\log \pi_{g_i} + \log \phi(x_i; \mu_{g_i}, \Sigma)]
$$

Expandindo o termo $\log \phi(x_i; \mu_{g_i}, \Sigma)$:

$$\
l(\theta) = \sum_{i=1}^N \left[ \log \pi_{g_i} - \frac{p}{2} \log(2\pi) - \frac{1}{2} \log |\Sigma| - \frac{1}{2} (x_i-\mu_{g_i})^T \Sigma^{-1} (x_i-\mu_{g_i}) \right]
$$

#### Estimadores de Máxima Verossimilhança para os Parâmetros da LDA

A maximização da log-verossimilhança $l(\theta)$ em relação aos parâmetros $\pi_k$, $\mu_k$ e $\Sigma$ leva aos estimadores de máxima verossimilhança (MLEs) para estes parâmetros. Como mencionado no contexto [^28], a teoria normal padrão conduz facilmente a estes estimadores, que são precisamente aqueles apresentados na Seção 4.3 [^9]:

*   **Estimador para a probabilidade a priori $\pi_k$**: A proporção de observações pertencentes à classe $k$.
    $$\
    \hat{\pi}_k = \frac{N_k}{N}
    $$
    onde $N_k$ é o número de observações na classe $k$ [^9].

*   **Estimador para o vetor de médias $\mu_k$**: A média amostral das observações pertencentes à classe $k$.
    $$\
    \hat{\mu}_k = \frac{1}{N_k} \sum_{i: g_i=k} x_i
    $$
    [^9].

*   **Estimador para a matriz de covariância comum $\Sigma$**: A matriz de covariância *pooled* (agrupada), calculada como uma média ponderada das matrizes de covariância de cada classe, ajustada pelos graus de liberdade.
    $$\
    \hat{\Sigma} = \frac{1}{N-K} \sum_{k=1}^K \sum_{i: g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T
    $$
    [^9].

Uma vez que estes parâmetros são estimados, eles podem ser substituídos nas funções discriminantes lineares $\delta_k(x)$ (Equação 4.10) [^9] para classificar novas observações:

$$\
\hat{\delta}_k(x) = x^T \hat{\Sigma}^{-1} \hat{\mu}_k - \frac{1}{2} \hat{\mu}_k^T \hat{\Sigma}^{-1} \hat{\mu}_k + \log \hat{\pi}_k
$$

A regra de classificação é então $G(x) = \text{argmax}_k \hat{\delta}_k(x)$ [^9].

#### O Papel da Densidade Marginal $Pr(X)$

Um ponto distintivo da abordagem de máxima verossimilhança completa da LDA é que a **densidade marginal** $Pr(X)$ desempenha um papel implícito na estimação [^28]. Esta densidade marginal é, sob o modelo LDA, uma mistura de Gaussianas:

$$\
Pr(X) = \sum_{k=1}^K Pr(X|G=k) Pr(G=k) = \sum_{k=1}^K \pi_k \phi(X; \mu_k, \Sigma)
$$

Esta é a expressão (4.38) no contexto [^28]. Como os parâmetros do modelo ($\pi_k, \mu_k, \Sigma$) estão presentes nesta densidade marginal, a maximização da verossimilhança completa $Pr(X, G=k) = Pr(G=k|X)Pr(X)$ utiliza a informação contida em $Pr(X)$ [^27, ^28]. Em contraste, a maximização da verossimilhança condicional $Pr(G=k|X)$ na regressão logística ignora completamente a estrutura de $Pr(X)$, tratando-a efetivamente como não paramétrica [^27].

> A principal consequência é que, ao confiar nas suposições adicionais do modelo (Gaussianidade, covariância comum), a LDA pode obter informações adicionais sobre os parâmetros a partir da estrutura marginal dos dados $X$, potencialmente levando a estimativas mais eficientes (menor variância) *se as suposições do modelo forem verdadeiras* [^28]. Por exemplo, observações distantes da fronteira de decisão, que recebem baixo peso na regressão logística, contribuem significativamente para a estimativa da matriz de covariância comum $\hat{\Sigma}$ na LDA [^28].

#### Implicações e Comparações com a Regressão Logística

A diferença fundamental na abordagem de estimação - verossimilhança completa (LDA) versus condicional (Regressão Logística) - leva a trade-offs importantes [^27, ^28]:

1.  **Suposições:** A LDA faz suposições mais fortes sobre a distribuição dos dados ($X|G=k$ é Gaussiana com $\Sigma$ comum). A Regressão Logística assume apenas uma forma linear para os log-odds (logits) (4.34) [^27], sendo agnóstica quanto à distribuição marginal $Pr(X)$.
2.  **Eficiência:** Se as suposições da LDA forem válidas, a estimação via máxima verossimilhança completa é estatisticamente mais eficiente. Ignorar a parte marginal da verossimilhança pode levar a uma perda de eficiência assintótica [^28].
3.  **Robustez:** A Regressão Logística é geralmente considerada mais robusta a desvios das suposições do modelo, incluindo a presença de outliers ou quando os preditores não são Gaussianos (e.g., variáveis qualitativas) [^28]. A dependência da LDA em relação a todas as observações para estimar $\Sigma$ a torna mais sensível a outliers [^28].
4.  **Casos Separáveis:** Em situações onde os dados são perfeitamente separáveis por um hiperplano, os estimadores de máxima verossimilhança para a regressão logística tendem ao infinito. Os coeficientes da LDA, no entanto, permanecem bem definidos devido à regularização implícita fornecida pela verossimilhança marginal [^28, ^Exercise 4.5].

Apesar destas diferenças teóricas, a experiência prática sugere que ambos os modelos frequentemente produzem resultados de classificação muito similares, mesmo quando as suposições da LDA são violadas [^28].

### Conclusão

A estimação de parâmetros na Análise Discriminante Linear (LDA) é realizada através da maximização da **log-verossimilhança completa**, derivada da **densidade de probabilidade conjunta** $Pr(X, G=k)$ [^27]. Esta abordagem está intrinsecamente ligada à suposição fundamental de que as densidades condicionais de classe $f_k(x)$ são **Gaussianas multivariadas** com vetores de média $\mu_k$ específicos da classe e uma **matriz de covariância comum** $\Sigma$ [^8]. A maximização desta função de verossimilhança resulta nos estimadores padrão para as probabilidades a priori $\hat{\pi}_k$, as médias das classes $\hat{\mu}_k$, e a matriz de covariância agrupada $\hat{\Sigma}$ [^9], que são consistentes com a teoria normal padrão [^28]. Este processo difere significativamente da abordagem de **verossimilhança condicional** empregada pela regressão logística [^27], explorando também a informação contida na **densidade marginal** $Pr(X)$ [^28]. Embora isso possa levar a estimativas mais eficientes se as suposições Gaussianas forem válidas, também torna a LDA menos robusta a violações dessas suposições e a outliers em comparação com a regressão logística [^28]. A escolha entre LDA e regressão logística envolve, portanto, um trade-off entre a força das suposições do modelo, eficiência estatística e robustez.

### Referências

[^1]: Page 101, Section 4.1 Introduction
[^2]: Page 102, Section 4.1 Introduction
[^3]: Page 103, Section 4.2 Linear Regression of an Indicator Matrix, Figure 4.1
[^4]: Page 104, Section 4.2 Linear Regression of an Indicator Matrix
[^5]: Page 105, Section 4.2 Linear Regression of an Indicator Matrix, Figure 4.2
[^6]: Page 106, Section 4.3 Linear Discriminant Analysis
[^7]: Page 107, Figure 4.4, Table 4.1
[^8]: Page 108, Section 4.3 Linear Discriminant Analysis
[^9]: Page 109, Section 4.3 Linear Discriminant Analysis
[^10]: Page 110, Section 4.3 Linear Discriminant Analysis
[^11]: Page 111, Section 4.3 Linear Discriminant Analysis, Figure 4.6
[^12]: Page 112, Section 4.3.1 Regularized Discriminant Analysis
[^13]: Page 113, Section 4.3.2 Computations for LDA, Section 4.3.3 Reduced-Rank Linear Discriminant Analysis
[^14]: Page 114, Section 4.3.3 Reduced-Rank Linear Discriminant Analysis
[^15]: Page 115, Figure 4.8
[^16]: Page 116, Section 4.3.3 Reduced-Rank Linear Discriminant Analysis, Figure 4.9
[^17]: Page 117, Section 4.3.3 Reduced-Rank Linear Discriminant Analysis, Figure 4.10
[^18]: Page 118, Figure 4.11
[^19]: Page 119, Section 4.4 Logistic Regression
[^20]: Page 120, Section 4.4.1 Fitting Logistic Regression Models
[^21]: Page 121, Section 4.4.1 Fitting Logistic Regression Models
[^22]: Page 122, Section 4.4.2 Example: South African Heart Disease, Table 4.2
[^23]: Page 123, Figure 4.12
[^24]: Page 124, Section 4.4.2 Example: South African Heart Disease, Table 4.3, Section 4.4.3 Quadratic Approximations and Inference
[^25]: Page 125, Section 4.4.3 Quadratic Approximations and Inference, Section 4.4.4 L1 Regularized Logistic Regression
[^26]: Page 126, Section 4.4.4 L1 Regularized Logistic Regression, Figure 4.13
[^27]: Page 127, Section 4.4.5 Logistic Regression or LDA?
[^28]: Page 128, Section 4.4.5 Logistic Regression or LDA?
[^29]: Page 129, Section 4.5 Separating Hyperplanes, Figure 4.14
[^30]: Page 130, Figure 4.15, Section 4.5.1 Rosenblatt's Perceptron Learning Algorithm
[^31]: Page 131, Section 4.5.1 Rosenblatt's Perceptron Learning Algorithm
[^32]: Page 132, Section 4.5.2 Optimal Separating Hyperplanes
[^33]: Page 133, Section 4.5.2 Optimal Separating Hyperplanes
[^34]: Page 134, Figure 4.16, Section 4.5.2 Optimal Separating Hyperplanes
[^35]: Page 135, Bibliographic Notes, Exercises
[^36]: Page 136, Exercises
[^37]: Page 137, Exercises
<!-- END -->