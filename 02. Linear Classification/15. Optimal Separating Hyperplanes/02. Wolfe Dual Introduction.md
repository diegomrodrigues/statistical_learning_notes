### Introduzindo o Dual de Wolfe para Otimiza√ß√£o

```mermaid
graph LR
    subgraph "Primal-Dual Relationship"
        direction TB
        A["Primal Problem: Minimization with Constraints"]
        B["Dual Transformation: Wolfe Dual"]
        C["Dual Problem: Maximization with Constraints"]
        D["Lagrange Multipliers (Œ±)"]
        E["Optimal Solution (Œ≤, Œ≤_0) from Dual"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
```

A teoria da **dualidade** em otimiza√ß√£o oferece uma abordagem poderosa para resolver problemas de otimiza√ß√£o com restri√ß√µes, transformando o problema original (o problema primal) em um problema relacionado (o problema dual) [^4.5.2]. O **dual de Wolfe** √© uma formula√ß√£o espec√≠fica do problema dual que √© particularmente √∫til em problemas de otimiza√ß√£o convexa com restri√ß√µes de desigualdade, como o problema da maximiza√ß√£o da margem em **M√°quinas de Vetores de Suporte (SVM)**.

**Problema Primal e Dual:**

O problema de otimiza√ß√£o que estamos estudando (o problema primal), dado por

$$
    \min_{\beta, \beta_0} \frac{1}{2} ||\beta||^2
$$
sujeito a:
$$
    y_i (\beta_0 + \beta^T x_i) \geq 1, \text{ para } i = 1,\ldots,N
$$
√© um problema de minimiza√ß√£o da norma do vetor $\beta$, com restri√ß√µes de desigualdade sobre os dados de treinamento.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um exemplo simples com 3 pontos de dados bidimensionais (N=3) para ilustrar o problema primal. Suponha que temos os seguintes dados:
>
>  - $x_1 = [1, 2]$, $y_1 = 1$
>  - $x_2 = [2, 1]$, $y_2 = 1$
>  - $x_3 = [1, -1]$, $y_3 = -1$
>
> O problema primal busca o $\beta = [\beta_1, \beta_2]$ e $\beta_0$ que minimizem $\frac{1}{2}(\beta_1^2 + \beta_2^2)$ e satisfa√ßam as restri√ß√µes $y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}) \geq 1$. Este √© um problema de otimiza√ß√£o com restri√ß√µes que pode ser dif√≠cil de resolver diretamente. O dual de Wolfe transforma este problema em uma forma mais trat√°vel.

A abordagem do dual de Wolfe envolve a introdu√ß√£o de **multiplicadores de Lagrange** $\alpha_i \geq 0$ para cada restri√ß√£o de desigualdade, e a constru√ß√£o da fun√ß√£o lagrangiana:

```mermaid
graph LR
    subgraph "Lagrangian Function"
        direction TB
        A["Objective Function: 1/2 * ||Œ≤||¬≤"]
        B["Constraint Term: - Œ£ Œ±_i [y_i(Œ≤_0 + Œ≤^T x_i) - 1]"]
        C["Lagrangian: L(Œ≤, Œ≤_0, Œ±)"]
        A --> C
        B --> C
    end
```

$$
    L(\beta, \beta_0, \alpha) = \frac{1}{2} ||\beta||^2 - \sum_{i=1}^N \alpha_i [y_i (\beta_0 + \beta^T x_i) - 1]
$$

A fun√ß√£o lagrangiana combina a fun√ß√£o objetivo e as restri√ß√µes, incluindo um multiplicador de Lagrange para cada restri√ß√£o.

O problema dual de Wolfe √© obtido maximizando a fun√ß√£o lagrangiana em rela√ß√£o aos par√¢metros primais $\beta$ e $\beta_0$, e minimizando a fun√ß√£o dual resultante em rela√ß√£o aos multiplicadores de Lagrange $\alpha_i$. Especificamente, tomamos as derivadas parciais em rela√ß√£o a $\beta_0$ e $\beta$, e igualamos a zero, para obter as rela√ß√µes:

$$
     \frac{\partial L}{\partial \beta_0} = - \sum_{i=1}^N \alpha_i y_i = 0
$$
$$
     \frac{\partial L}{\partial \beta} = \beta - \sum_{i=1}^N \alpha_i y_i x_i = 0
$$
e com isso, obtemos a rela√ß√µes:
$$
     \sum_{i=1}^N \alpha_i y_i = 0
$$
$$
    \beta = \sum_{i=1}^N \alpha_i y_i x_i
$$

> üí° **Exemplo Num√©rico (continua√ß√£o):**
>
> Usando as derivadas parciais, temos:
>
> $\frac{\partial L}{\partial \beta_0} = -\alpha_1(1) - \alpha_2(1) - \alpha_3(-1) = 0$, ou seja, $\alpha_3 = \alpha_1 + \alpha_2$.
>
> $\frac{\partial L}{\partial \beta} = \beta - \alpha_1(1)[1, 2] - \alpha_2(1)[2, 1] - \alpha_3(-1)[1, -1] = 0$, ou seja, $\beta = [\alpha_1 + 2\alpha_2 - \alpha_3, 2\alpha_1 + \alpha_2 + \alpha_3]$.
>
> Substituindo $\alpha_3$, obtemos $\beta = [2\alpha_2, 2\alpha_1 + 2\alpha_2]$.
>
> Estas rela√ß√µes s√£o cruciais para transformar o problema primal no dual.

Ao substituir $\beta$ na fun√ß√£o lagrangiana e adicionar a restri√ß√£o de $\alpha_i \geq 0$, obtemos o problema dual de Wolfe:

```mermaid
graph LR
    subgraph "Wolfe Dual Problem"
        direction TB
        A["Dual Objective: max Œ£ Œ±_i - 1/2 * Œ£ Œ£ Œ±_i Œ±_j y_i y_j x_i^T x_j"]
        B["Dual Constraint 1: Œ£ Œ±_i y_i = 0"]
        C["Dual Constraint 2: Œ±_i ‚â• 0"]
        A --> B
        A --> C
    end
```

$$
\max_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j
$$
sujeito a:
$$
    \sum_{i=1}^N \alpha_i y_i = 0
$$
$$
    \alpha_i \geq 0, \text{ para } i = 1,\ldots,N
$$
O problema dual √© um problema de maximiza√ß√£o com restri√ß√µes de desigualdade, onde o objetivo √© encontrar os multiplicadores de Lagrange $\alpha_i$ que maximizem a fun√ß√£o dual. As solu√ß√µes para os par√¢metros originais $\beta$ e $\beta_0$ podem ser obtidas a partir dos multiplicadores de Lagrange √≥timos, como mostrado acima.

> üí° **Exemplo Num√©rico (continua√ß√£o):**
>
> Substituindo $\beta$ na lagrangiana, obtemos o problema dual:
>
> $\max_{\alpha} \alpha_1 + \alpha_2 + \alpha_3 - \frac{1}{2} (\alpha_1^2(1^2+2^2) + \alpha_2^2(2^2+1^2) + \alpha_3^2(1^2+(-1)^2) + 2\alpha_1\alpha_2(1*2+2*1) - 2\alpha_1\alpha_3(1*1+2*(-1)) - 2\alpha_2\alpha_3(2*1+1*(-1))) $
>
> $\max_{\alpha} \alpha_1 + \alpha_2 + \alpha_3 - \frac{1}{2} (5\alpha_1^2 + 5\alpha_2^2 + 2\alpha_3^2 + 8\alpha_1\alpha_2 + 2\alpha_1\alpha_3 - 2\alpha_2\alpha_3) $
>
> Sujeito a: $\alpha_1 + \alpha_2 - \alpha_3 = 0$ e $\alpha_1, \alpha_2, \alpha_3 \geq 0$.
>
> Este problema dual √© mais f√°cil de resolver computacionalmente. A solu√ß√£o para os $\alpha_i$ √≥timos nos dar√° os pesos para calcular $\beta$ e, consequentemente, o hiperplano de separa√ß√£o.

**Vantagens do Dual de Wolfe:**

A formula√ß√£o dual apresenta algumas vantagens importantes:

1.  **Problema de Otimiza√ß√£o Mais F√°cil:** O problema dual √© um problema de otimiza√ß√£o convexa, e √© mais f√°cil de resolver do que o problema primal original, que pode ser n√£o convexo.

2.  **Interpretabilidade:** As solu√ß√µes do problema dual est√£o diretamente relacionadas aos vetores de suporte, que s√£o as observa√ß√µes que definem a margem. As solu√ß√µes do dual tamb√©m nos dizem quais observa√ß√µes importam para a constru√ß√£o da fronteira de decis√£o.

3.  **Utiliza√ß√£o do *Kernel Trick*:** O problema dual pode ser generalizado para incorporar o *kernel trick*, que permite mapear os dados para espa√ßos de alta dimens√£o e modelar fronteiras de decis√£o n√£o lineares. Essa generaliza√ß√£o √© obtida por meio da substitui√ß√£o do produto interno  $x_i^T x_j$ por uma fun√ß√£o *kernel* $k(x_i, x_j)$.

4.  **Transforma√ß√£o em um problema convexo:** O problema dual √© um problema de otimiza√ß√£o convexa, e esse resultado permite utilizar os algoritmos bem conhecidos para a solu√ß√£o desse tipo de problemas.

A formula√ß√£o dual de Wolfe √© um passo fundamental para a constru√ß√£o de M√°quinas de Vetores de Suporte (SVMs) e para o desenvolvimento de algoritmos eficientes para a resolu√ß√£o de problemas de classifica√ß√£o linear.

**Lemma 52:** *O dual de Wolfe transforma um problema de minimiza√ß√£o com restri√ß√µes (problema primal) em um problema de maximiza√ß√£o com restri√ß√µes (problema dual), por meio da introdu√ß√£o de multiplicadores de Lagrange*.

*Prova:* Ao construir a fun√ß√£o Lagrangiana e aplicar as condi√ß√µes de otimalidade, o problema dual √© obtido. $\blacksquare$

**Corol√°rio 52:** *A solu√ß√£o do dual de Wolfe √© dada em termos dos multiplicadores de Lagrange, que est√£o relacionados com os vetores de suporte, o que torna a solu√ß√£o mais interpret√°vel.*

*Prova:* As solu√ß√µes do problema dual se baseiam nas condi√ß√µes de KKT e em como os multiplicadores de Lagrange s√£o interpretados. $\blacksquare$

A utiliza√ß√£o do dual de Wolfe transforma o problema original, e permite um tratamento mais f√°cil e eficiente da otimiza√ß√£o, que √© fundamental para a constru√ß√£o das M√°quinas de Vetores de Suporte (SVM).

### Condi√ß√µes de Karush-Kuhn-Tucker (KKT) e a Otimalidade

```mermaid
graph LR
    subgraph "KKT Conditions"
        direction TB
        A["Primal Feasibility: g_i(x) ‚â§ 0"]
        B["Dual Feasibility: Œª_i ‚â• 0"]
        C["Stationarity: ‚àáf(x) + Œ£ Œª_i ‚àág_i(x) = 0"]
        D["Complementary Slackness: Œª_i g_i(x) = 0"]
        A --> E["Conditions for Optimality"]
        B --> E
        C --> E
        D --> E

    end
```

As **Condi√ß√µes de Karush-Kuhn-Tucker (KKT)** s√£o um conjunto de condi√ß√µes necess√°rias para a otimalidade em problemas de otimiza√ß√£o convexa com restri√ß√µes de desigualdade [^4.4.4]. Essas condi√ß√µes fornecem um arcabou√ßo formal para entender as propriedades da solu√ß√£o √≥tima e para derivar algoritmos de otimiza√ß√£o. Em particular, as condi√ß√µes KKT s√£o fundamentais para entender a rela√ß√£o entre o problema primal e dual em otimiza√ß√£o e s√£o usadas para derivar as solu√ß√µes para problemas como o da maximiza√ß√£o da margem em M√°quinas de Vetores de Suporte (SVM) e outros problemas de classifica√ß√£o.

**As Condi√ß√µes KKT:**

Em geral, um problema de otimiza√ß√£o convexa com restri√ß√µes de desigualdade pode ser formulado da seguinte forma:

$$
   \min_x f(x)
$$

sujeito a:

$$
   g_i(x) \leq 0 \text{ para } i=1,\ldots,m
$$
onde $f(x)$ √© a fun√ß√£o objetivo a ser minimizada, e $g_i(x)$ s√£o as fun√ß√µes de restri√ß√£o. As condi√ß√µes KKT s√£o dadas por:

1.  **Viabilidade Primal:** As solu√ß√µes $x$ devem satisfazer as restri√ß√µes:

    $$
        g_i(x) \leq 0 \text{ para } i = 1,\ldots,m
    $$

2.  **Viabilidade Dual:** Os multiplicadores de Lagrange $\lambda_i$ devem ser n√£o negativos:

    $$
        \lambda_i \geq 0 \text{ para } i = 1,\ldots,m
    $$

3.  **Estacionariedade:** O gradiente da fun√ß√£o lagrangiana deve ser igual a zero:

    $$
        \nabla f(x) + \sum_{i=1}^m \lambda_i \nabla g_i(x) = 0
    $$

    onde a fun√ß√£o Lagrangiana √© dada por:

    $$
        L(x,\lambda) = f(x) + \sum_{i=1}^m \lambda_i g_i(x)
    $$

4.  **Folga Complementar:** O produto dos multiplicadores de Lagrange pelas restri√ß√µes deve ser igual a zero:

    $$
        \lambda_i g_i(x) = 0 \text{ para } i = 1,\ldots,m
    $$

Essas condi√ß√µes s√£o necess√°rias para a otimalidade da solu√ß√£o de problemas de otimiza√ß√£o convexa com restri√ß√µes de desigualdade. A condi√ß√£o de folga complementar indica que os multiplicadores de Lagrange $\lambda_i$ s√£o diferentes de zero somente quando a restri√ß√£o correspondente √© satisfeita com igualdade, isto √©, quando $g_i(x) = 0$. As restri√ß√µes com multiplicadores iguais a zero s√£o consideradas restri√ß√µes inativas.

> üí° **Exemplo Num√©rico:**
>
> Considere um problema simples de minimizar $f(x) = x^2$ sujeito a $g(x) = x - 2 \leq 0$.
>
> 1.  **Viabilidade Primal:** $x \leq 2$.
> 2.  **Viabilidade Dual:** $\lambda \geq 0$.
> 3.  **Estacionariedade:** $\nabla f(x) + \lambda \nabla g(x) = 2x + \lambda(1) = 0$, portanto $2x + \lambda = 0$.
> 4.  **Folga Complementar:** $\lambda(x - 2) = 0$.
>
> Se $\lambda = 0$, ent√£o $2x = 0$, o que implica $x = 0$. Isso satisfaz as condi√ß√µes KKT e a solu√ß√£o √≥tima.
> Se $x = 2$, ent√£o $2(2) + \lambda = 0$, o que implica $\lambda = -4$. Isso viola a viabilidade dual ($\lambda \geq 0$), portanto, $x=2$ n√£o √© a solu√ß√£o.
>
> A solu√ß√£o √≥tima √© $x = 0$ com $\lambda = 0$. A condi√ß√£o de folga complementar indica que a restri√ß√£o $x \leq 2$ n√£o est√° ativa (n√£o √© satisfeita com igualdade) no √≥timo.

**As Condi√ß√µes KKT em SVM:**

Aplicando as condi√ß√µes KKT ao problema de otimiza√ß√£o do SVM, obtemos informa√ß√µes importantes sobre a rela√ß√£o entre a solu√ß√£o dual e a solu√ß√£o primal, al√©m de entender como os vetores de suporte s√£o selecionados.

O problema primal do SVM com margem m√°xima, para classes linearmente separ√°veis, √© dado por:

$$
    \min_{\beta, \beta_0} \frac{1}{2} ||\beta||^2
$$
sujeito a:
$$
    y_i (\beta_0 + \beta^T x_i) \geq 1, \text{ para } i = 1,\ldots,N
$$

Aplicando as condi√ß√µes KKT, podemos mostrar que os multiplicadores de Lagrange $\alpha_i$ s√£o iguais a zero para observa√ß√µes que n√£o s√£o vetores de suporte (ou seja, para as observa√ß√µes que n√£o se encontram na margem), e s√£o maiores do que zero para vetores de suporte, e com isso, vemos como o hiperplano separador √© definido por apenas um subconjunto dos dados.

```mermaid
graph LR
    subgraph "KKT Conditions in SVM"
        direction TB
        A["Primal Feasibility: y_i(Œ≤_0 + Œ≤^T x_i) ‚â• 1"]
        B["Dual Feasibility: Œ±_i ‚â• 0"]
        C["Stationarity: Œ≤ = Œ£ Œ±_i y_i x_i; Œ£ Œ±_i y_i = 0"]
        D["Complementary Slackness: Œ±_i[y_i(Œ≤_0 + Œ≤^T x_i) - 1] = 0"]
         A --> E["Conditions for SVM Optimality"]
        B --> E
        C --> E
        D --> E
    end
```

> üí° **Exemplo Num√©rico (SVM):**
>
> Retomando o exemplo anterior, as condi√ß√µes KKT para o SVM s√£o:
>
> 1.  **Viabilidade Primal:** $y_i(\beta_0 + \beta^Tx_i) \geq 1$.
> 2.  **Viabilidade Dual:** $\alpha_i \geq 0$.
> 3.  **Estacionariedade:** $\beta = \sum_{i=1}^N \alpha_i y_i x_i$ e $\sum_{i=1}^N \alpha_i y_i = 0$.
> 4.  **Folga Complementar:** $\alpha_i[y_i(\beta_0 + \beta^T x_i) - 1] = 0$.
>
> Se o ponto $x_i$ est√° no lado correto da margem, ent√£o $y_i(\beta_0 + \beta^T x_i) > 1$, e, pela folga complementar, $\alpha_i = 0$. Se o ponto $x_i$ √© um vetor de suporte, ent√£o $y_i(\beta_0 + \beta^T x_i) = 1$ e $\alpha_i > 0$.
>
> Este exemplo ilustra como as condi√ß√µes KKT ajudam a identificar quais pontos s√£o vetores de suporte e como eles s√£o usados para determinar o hiperplano separador.

O dual de Wolfe e as condi√ß√µes KKT oferecem um conjunto de ferramentas para an√°lise e resolu√ß√£o de problemas de otimiza√ß√£o convexa com restri√ß√µes, incluindo o problema da maximiza√ß√£o da margem em modelos de classifica√ß√£o lineares e nas M√°quinas de Vetores de Suporte (SVM).

**Lemma 53:** *As Condi√ß√µes de Karush-Kuhn-Tucker (KKT) s√£o condi√ß√µes necess√°rias para a otimalidade de problemas de otimiza√ß√£o convexa com restri√ß√µes de desigualdade, e relacionam os par√¢metros, os multiplicadores de Lagrange e as restri√ß√µes do problema*.

*Prova:* As condi√ß√µes de KKT s√£o derivadas da teoria da dualidade, e descrevem as propriedades que devem ser satisfeitas no √≥timo da fun√ß√£o.   $\blacksquare$

**Corol√°rio 53:** *A condi√ß√£o de folga complementar nas condi√ß√µes KKT indica que os multiplicadores de Lagrange s√£o diferentes de zero apenas para as restri√ß√µes ativas, o que √© crucial para entender como os vetores de suporte definem o hiperplano separador em SVM*.

*Prova:* O multiplicador de Lagrange est√° associado a cada restri√ß√£o, e valores n√£o nulos indicam que a restri√ß√£o √© ativa (vetores de suporte) e faz parte da defini√ß√£o da solu√ß√£o. $\blacksquare$

As condi√ß√µes KKT fornecem um entendimento formal e preciso das rela√ß√µes entre as vari√°veis de um problema de otimiza√ß√£o convexa e suas solu√ß√µes.

### Conclus√£o

Este cap√≠tulo apresentou o dual de Wolfe como uma ferramenta para a transforma√ß√£o de problemas de otimiza√ß√£o com restri√ß√µes e enfatizou a import√¢ncia das condi√ß√µes de Karush-Kuhn-Tucker (KKT) para a otimalidade em problemas de otimiza√ß√£o convexa. A rela√ß√£o do dual de Wolfe e das condi√ß√µes KKT com a constru√ß√£o de modelos de classifica√ß√£o lineares baseados na maximiza√ß√£o da margem, como as M√°quinas de Vetores de Suporte (SVM), tamb√©m foi discutida. A compreens√£o desses conceitos √© essencial para o estudo de m√©todos de otimiza√ß√£o e para o desenvolvimento de modelos de aprendizado de m√°quina mais sofisticados.

### Footnotes

[^4.1]: "In this chapter we revisit the classification problem and focus on linear methods for classification. Since our predictor G(x) takes values in a discrete set G, we can always divide the input space into a collection of regions labeled according to the classification. We saw in Chapter 2 that the boundaries of these regions can be rough or smooth, depending on the prediction function. For an important class of procedures, these decision boundaries are linear; this is what we will mean by linear methods for classification." *(Trecho de "The Elements of Statistical Learning")*

[^4.3]: "Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Œ£k = ‚àë. In comparing two classes k and l, it is sufficient to look at the log-ratio, and we see that" *(Trecho de "The Elements of Statistical Learning")*

[^4.4]: "The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0,1]." *(Trecho de "The Elements of Statistical Learning")*

[^4.4.4]:  "The L‚ÇÅ penalty used in the lasso (Section 3.4.2) can be used for variable selection and shrinkage with any linear regression model. For logistic regression, we would maximize a penalized version of (4.20):" *(Trecho de "The Elements of Statistical Learning")*

[^4.5]: "In this situation the features are high-dimensional and correlated, and the LDA coefficients can be regularized to be smooth or sparse in the original domain of the signal. This leads to better generalization and allows for easier interpretation of the coefficients." *(Trecho de "The Elements of Statistical Learning")*

[^4.5.2]: "The second method, due to Vapnik (1996), finds an optimally separating hyperplane if one exists, else finds a hyperplane that minimizes some measure of overlap in the training data. We treat the separable case here, and defer treatment of the nonseparable case to Chapter 12." *(Trecho de "The Elements of Statistical Learning")*
