### Encontrando uma Solu√ß√£o como Combina√ß√£o Linear dos Pontos de Suporte e uma Regra de Classifica√ß√£o para Novas Observa√ß√µes

```mermaid
graph LR
    subgraph "SVM Solution & Classification"
        direction TB
        A["Optimization Problem: Maximize Margin"]
        B["Dual Formulation"]
        C["Lagrange Multipliers Œ±·µ¢"]
        D["Support Vectors (SV)"]
        E["Linear Combination of SV: Œ≤ = Œ£ Œ±·µ¢y·µ¢x·µ¢"]
        F["Classification Rule: f(x*) = Œ≤‚ÇÄ + Œ£ Œ±·µ¢y·µ¢k(x·µ¢, x*)"]
        G["Decision: sign(f(x*))"]
        A --> B
        B --> C
        C --> D
        D --> E
        E --> F
        F --> G
    end
```

A solu√ß√£o para o problema de otimiza√ß√£o na busca do **hiperplano separador** √≥timo, em problemas de classifica√ß√£o linear como em **M√°quinas de Vetores de Suporte (SVM)**, apresenta uma caracter√≠stica fundamental: a solu√ß√£o para os par√¢metros do hiperplano pode ser expressa como uma **combina√ß√£o linear dos vetores de suporte**, e essa combina√ß√£o linear define a regra de classifica√ß√£o para **novas observa√ß√µes** [^4.5.2]. Essa propriedade, que √© consequ√™ncia da formula√ß√£o dual do problema e das condi√ß√µes de Karush-Kuhn-Tucker (KKT), √© crucial para o entendimento do SVM e de sua capacidade de generaliza√ß√£o.

**A Solu√ß√£o como Combina√ß√£o Linear dos Vetores de Suporte:**

Conforme discutido nas se√ß√µes anteriores, a formula√ß√£o dual do problema de otimiza√ß√£o do SVM √© dada por:

$$
\max_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j k(x_i, x_j)
$$
sujeito a:
$$
  0 \leq \alpha_i \leq C, \text{ para } i = 1,\ldots,N
$$
$$
    \sum_{i=1}^N \alpha_i y_i = 0
$$
onde $\alpha_i$ s√£o os multiplicadores de Lagrange, $y_i$ s√£o os r√≥tulos das classes, e $k(x_i, x_j)$ √© a fun√ß√£o *kernel* (ou o produto interno $x_i^T x_j$ no caso linear). A solu√ß√£o para esse problema de otimiza√ß√£o √© dada em termos dos multiplicadores de Lagrange $\alpha_i$, onde os multiplicadores que s√£o diferentes de zero correspondem aos **vetores de suporte**.

```mermaid
graph LR
    subgraph "Dual Problem Formulation"
        direction TB
        A["Maximize:  Œ£ Œ±·µ¢ - 1/2 Œ£Œ£ Œ±·µ¢Œ±‚±ºy·µ¢y‚±ºk(x·µ¢, x‚±º)"]
        B["Subject to: 0 ‚â§ Œ±·µ¢ ‚â§ C"]
        C["Subject to: Œ£ Œ±·µ¢y·µ¢ = 0"]
        A --> B
        A --> C
    end
```

O vetor de par√¢metros $\beta$ do hiperplano separador pode ser expresso como uma combina√ß√£o linear dos vetores de suporte:

$$
    \beta = \sum_{i \in SV} \alpha_i y_i x_i
$$
onde $SV$ √© o conjunto de √≠ndices das observa√ß√µes que s√£o vetores de suporte, ou seja, onde $\alpha_i \neq 0$. Essa express√£o mostra que a solu√ß√£o do SVM depende apenas dos vetores de suporte. As observa√ß√µes que n√£o s√£o vetores de suporte n√£o afetam a solu√ß√£o final.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simples com 4 pontos em duas dimens√µes, onde os pontos $(1, 1)$ e $(2, 2)$ pertencem √† classe +1, e os pontos $(1, 2)$ e $(2, 1)$ pertencem √† classe -1. Usaremos um kernel linear, ou seja, $k(x_i, x_j) = x_i^T x_j$.
>
> Os pontos s√£o:
>  - $x_1 = (1, 1)$, $y_1 = 1$
>  - $x_2 = (2, 2)$, $y_2 = 1$
>  - $x_3 = (1, 2)$, $y_3 = -1$
>  - $x_4 = (2, 1)$, $y_4 = -1$
>
> Suponha que, ap√≥s resolver o problema dual, encontramos os seguintes multiplicadores de Lagrange:
>
> - $\alpha_1 = 0.5$
> - $\alpha_2 = 0$
> - $\alpha_3 = 0.5$
> - $\alpha_4 = 0$
>
> Isso significa que os vetores de suporte s√£o $x_1$ e $x_3$. O vetor $\beta$ √© calculado como:
>
> $\beta = \alpha_1 y_1 x_1 + \alpha_3 y_3 x_3 = 0.5 * 1 * (1, 1) + 0.5 * (-1) * (1, 2) = (0.5, 0.5) + (-0.5, -1) = (0, -0.5)$
>
> Portanto, o vetor $\beta$ √© $(0, -0.5)$. Observe que apenas os vetores de suporte contribuem para o c√°lculo de $\beta$.

**Regra de Classifica√ß√£o para Novas Observa√ß√µes:**

Para classificar uma nova observa√ß√£o $x^*$, calculamos a dist√¢ncia sinalizada dessa observa√ß√£o at√© o hiperplano separador. No contexto do *kernel trick*, essa dist√¢ncia pode ser calculada como:

$$
    f(x^*) = \beta_0 + \beta^T \phi(x^*) =  \beta_0 + \sum_{i \in SV} \alpha_i y_i k(x_i, x^*)
$$
onde $\phi(x^*)$ √© o mapeamento de $x^*$ no espa√ßo de caracter√≠sticas. A classe da observa√ß√£o $x^*$ √© ent√£o determinada pelo sinal de $f(x^*)$:

$$
  \hat{G}(x^*) = \text{sign}(f(x^*)) = \text{sign}\left(\beta_0 + \sum_{i \in SV} \alpha_i y_i k(x_i, x^*)\right)
$$

onde o par√¢metro $\beta_0$ pode ser computado a partir de qualquer vetor de suporte.

```mermaid
graph LR
    subgraph "Classification Rule"
    direction TB
        A["f(x*) = Œ≤‚ÇÄ + Œ£ Œ±·µ¢y·µ¢k(x·µ¢, x*)"]
        B["Classification: sign(f(x*))"]
        A --> B
    end
```

> üí° **Exemplo Num√©rico (continua√ß√£o):**
>
> Continuando com o exemplo anterior, vamos calcular o valor de $\beta_0$. Podemos usar um dos vetores de suporte, por exemplo, $x_1$. Como $y_1 = 1$ e sabemos que $f(x_1) = \beta_0 + \beta^T x_1 = 1$, temos:
>
> $1 = \beta_0 + (0, -0.5)^T (1, 1) = \beta_0 - 0.5$,
>
> portanto, $\beta_0 = 1.5$.
>
> Agora, vamos classificar uma nova observa√ß√£o $x^* = (1.5, 1.5)$:
>
> $f(x^*) = 1.5 + \sum_{i \in SV} \alpha_i y_i k(x_i, x^*) = 1.5 + 0.5 * 1 * (1, 1)^T (1.5, 1.5) + 0.5 * (-1) * (1, 2)^T (1.5, 1.5)$
> $f(x^*) = 1.5 + 0.5 * (1.5 + 1.5) - 0.5 * (1.5 + 3) = 1.5 + 1.5 - 2.25 = 0.75$
>
> Como $f(x^*) = 0.75 > 0$, a nova observa√ß√£o $x^*$ √© classificada como pertencente √† classe +1.

Essa express√£o revela que a classifica√ß√£o de uma nova observa√ß√£o depende da soma ponderada da fun√ß√£o *kernel* entre a nova observa√ß√£o e os vetores de suporte, onde os pesos s√£o dados pelos multiplicadores de Lagrange $\alpha_i$ e pelas classes $y_i$. Os vetores de suporte desempenham um papel central na decis√£o de classifica√ß√£o, enquanto as observa√ß√µes que n√£o s√£o vetores de suporte n√£o contribuem para a decis√£o.

A utiliza√ß√£o de *kernels* na regra de classifica√ß√£o permite que o SVM opere em um espa√ßo de alta dimens√£o, sem que seja necess√°rio calcular explicitamente a transforma√ß√£o do espa√ßo de entrada.

**Lemma 54:** *A solu√ß√£o para o problema de otimiza√ß√£o do SVM, expressa em termos de multiplicadores de Lagrange, leva a uma representa√ß√£o dos par√¢metros do hiperplano separador como uma combina√ß√£o linear dos vetores de suporte.*

*Prova:* A solu√ß√£o do dual de Wolfe do SVM fornece os valores dos multiplicadores de Lagrange, que s√£o usados para calcular os par√¢metros do hiperplano.  $\blacksquare$

**Corol√°rio 54:** *A regra de classifica√ß√£o para novas observa√ß√µes em SVM √© definida com base na dist√¢ncia sinalizada da observa√ß√£o em rela√ß√£o ao hiperplano, expressa como uma combina√ß√£o linear das fun√ß√µes *kernel* entre a nova observa√ß√£o e os vetores de suporte.*

*Prova:* A regra de decis√£o √© definida com base na fun√ß√£o $f(x)$, que depende de uma combina√ß√£o dos vetores de suporte, e a classe final da observa√ß√£o √© definida pelo sinal da fun√ß√£o.   $\blacksquare$

A combina√ß√£o linear dos vetores de suporte e a regra de classifica√ß√£o com base na dist√¢ncia sinalizada s√£o elementos chave do SVM e permite entender como esse modelo funciona na pr√°tica.

### Rela√ß√£o com a Regress√£o de Indicadores

```mermaid
graph LR
    subgraph "Regression and SVM"
    direction LR
        A["Indicator Regression: Find Decision Boundary"]
        B["SVM: Maximize Margin"]
        A --> C["Linear Decision Boundary"]
        B --> C
        C --> D["Classification"]
    end
```

A **rela√ß√£o entre a regress√£o de indicadores** e os modelos de classifica√ß√£o baseados na **maximiza√ß√£o da margem**, como as **M√°quinas de Vetores de Suporte (SVM)**, √© uma conex√£o interessante que revela como diferentes abordagens podem levar a resultados similares em problemas de classifica√ß√£o linear [^4.2], [^4.5.2]. Embora a regress√£o de indicadores seja uma t√©cnica mais simples, ela pode ser utilizada para entender alguns dos conceitos por tr√°s do SVM e de como o hiperplano separador pode ser constru√≠do.

**Regress√£o de Indicadores para Classifica√ß√£o:**

Na regress√£o de indicadores, como discutido em cap√≠tulos anteriores, cada classe √© codificada por meio de uma vari√°vel indicadora, que assume o valor 1 quando a observa√ß√£o pertence √† classe correspondente e 0 caso contr√°rio [^4.2]. Um modelo de regress√£o linear √© ajustado para cada vari√°vel indicadora, e as classifica√ß√µes s√£o realizadas com base nos valores ajustados, utilizando a regra:

*   Atribuir a observa√ß√£o √† classe cujo valor ajustado √© maior.

Essa abordagem busca modelar as vari√°veis indicadoras diretamente como uma fun√ß√£o linear dos preditores e pode ser usada para obter uma fronteira de decis√£o entre as classes.

**Rela√ß√£o com o SVM:**

Embora a regress√£o de indicadores seja uma t√©cnica de classifica√ß√£o mais simples, h√° algumas conex√µes importantes com o problema da otimiza√ß√£o do hiperplano separador em SVM. A regress√£o de indicadores busca encontrar o hiperplano que minimize os erros de classifica√ß√£o, enquanto o SVM busca encontrar o hiperplano que maximize a margem entre as classes.

Em um problema de classifica√ß√£o com duas classes, a regress√£o de indicadores, como mostrado em cap√≠tulos anteriores, leva a uma fronteira de decis√£o linear. No entanto, a regress√£o de indicadores busca encontrar um hiperplano que minimize os erros, mas n√£o maximiza a margem entre as classes.

O SVM, por outro lado, busca encontrar o hiperplano que maximize a dist√¢ncia entre as classes, o que geralmente leva a modelos com melhor capacidade de generaliza√ß√£o e com maior robustez. A otimiza√ß√£o do SVM √© um problema de otimiza√ß√£o convexa, o que leva a uma solu√ß√£o mais est√°vel e consistente do que a regress√£o linear simples.

A conex√£o entre as abordagens pode ser entendida da seguinte forma:

*   A regress√£o de indicadores busca encontrar um hiperplano que separa as classes, utilizando m√©todos de m√≠nimos quadrados. A abordagem se baseia em modelar os indicadores, e a decis√£o √© tomada por meio da classe que apresenta maior probabilidade condicional.

*   O SVM tamb√©m busca um hiperplano, mas com o objetivo de maximizar a margem entre as classes. A solu√ß√£o √© dada em termos de vetores de suporte, e o hiperplano separador √© obtido atrav√©s da solu√ß√£o de um problema de otimiza√ß√£o.

```mermaid
graph LR
    subgraph "Method Comparison"
    direction TB
        A["Indicator Regression: Minimize Errors"]
        B["SVM: Maximize Margin"]
        A --> C["Find Linear Separator"]
        B --> C
        C --> D["Decision Boundary"]
        A --> D
    end
```

> üí° **Exemplo Num√©rico:**
>
> Considere o mesmo conjunto de dados do exemplo anterior:
>  - $x_1 = (1, 1)$, $y_1 = 1$
>  - $x_2 = (2, 2)$, $y_2 = 1$
>  - $x_3 = (1, 2)$, $y_3 = -1$
>  - $x_4 = (2, 1)$, $y_4 = -1$
>
> Para regress√£o de indicadores, criamos uma vari√°vel indicadora $I(y_i = 1)$, que √© 1 para os pontos da classe +1 e 0 para os pontos da classe -1. Aplicamos uma regress√£o linear a esses dados, buscando encontrar um hiperplano que separe as classes.
>
> Embora a regress√£o linear possa encontrar um hiperplano separador, a solu√ß√£o n√£o necessariamente maximiza a margem. Por exemplo, um modelo linear que poderia separar os dados seria $f(x) = -0.5 + 0.5x_1 + 0.5x_2$.  A fronteira de decis√£o seria definida por $0 = -0.5 + 0.5x_1 + 0.5x_2$ ou $x_1 + x_2 = 1$.
>
> No entanto, este hiperplano pode n√£o ter a maior margem poss√≠vel, que √© o objetivo do SVM. O SVM, ao maximizar a margem, busca uma separa√ß√£o mais robusta e generaliz√°vel.

Em ess√™ncia, a regress√£o de indicadores busca um ajuste linear dos indicadores por meio de m√≠nimos quadrados, e o SVM busca uma separa√ß√£o linear com a maior margem poss√≠vel. Em certos contextos, o hiperplano separador definido por regress√£o linear (ou LDA, em modelos de classifica√ß√£o com duas classes) √© proporcional ao hiperplano obtido via SVM. No entanto, a abordagem da margem m√°xima, utilizada pelo SVM, √© mais robusta e geral para problemas de classifica√ß√£o.

A regress√£o de indicadores, embora mais simples, pode ser usada para entender o conceito de hiperplano separador e como ele divide o espa√ßo de entrada em diferentes classes. A abordagem do SVM, ao maximizar a margem, se torna mais robusta e eficiente em problemas de classifica√ß√£o.

**Lemma 55:** *A regress√£o de indicadores pode ser vista como uma forma de encontrar um hiperplano separador, que busca minimizar o erro de classifica√ß√£o entre as classes.*

*Prova:* A regress√£o de indicadores modela as classes por meio de indicadores, e a decis√£o final √© tomada com base na classe que apresenta a maior probabilidade.  $\blacksquare$

**Corol√°rio 55:** *O SVM, ao buscar maximizar a margem entre as classes, busca uma separa√ß√£o mais robusta entre elas, o que geralmente leva a um melhor desempenho do modelo quando comparado a regress√£o de indicadores.*

*Prova:* A formula√ß√£o do SVM se baseia no conceito de margem m√°xima e fornece solu√ß√µes mais est√°veis do que modelos baseados em m√≠nimos quadrados. $\blacksquare$

A rela√ß√£o entre regress√£o de indicadores e modelos de margem m√°xima permite entender como diferentes abordagens podem levar a resultados similares e como cada abordagem se posiciona em rela√ß√£o √† capacidade de generaliza√ß√£o e robustez dos modelos.

### Conclus√£o

Este cap√≠tulo explorou como a solu√ß√£o do problema de otimiza√ß√£o do SVM pode ser expressa como uma combina√ß√£o linear de vetores de suporte, e como essa combina√ß√£o define a regra de classifica√ß√£o para novas observa√ß√µes. Foi discutida a rela√ß√£o entre a regress√£o de indicadores e os modelos baseados na maximiza√ß√£o da margem (SVM), mostrando como diferentes abordagens podem buscar fronteiras lineares de separa√ß√£o. A compreens√£o desses conceitos √© crucial para a constru√ß√£o e interpreta√ß√£o de modelos de classifica√ß√£o linear e para entender as propriedades e as limita√ß√µes de cada m√©todo.

<!-- END DOCUMENT -->

### Footnotes

[^4.1]: "In this chapter we revisit the classification problem and focus on linear methods for classification. Since our predictor G(x) takes values in a discrete set G, we can always divide the input space into a collection of regions labeled according to the classification. We saw in Chapter 2 that the boundaries of these regions can be rough or smooth, depending on the prediction function. For an important class of procedures, these decision boundaries are linear; this is what we will mean by linear methods for classification." *(Trecho de "The Elements of Statistical Learning")*

[^4.2]: "In Chapter 2 we fit linear regression models to the class indicator variables, and classify to the largest fit. Suppose there are K classes, for convenience labeled 1,2,..., K, and the fitted linear model for the kth indicator response variable is $f_k(x) = \beta_{k0} + \beta^T x$. The decision boundary between class k and l is that set of points for which $f_k(x) = f_l(x)$, that is, the set $\{x: (\beta_{k0} ‚Äì \beta_{l0}) + (\beta_{\kappa} ‚Äì \beta_l)^T x = 0\}$, an affine set or hyperplane" *(Trecho de "The Elements of Statistical Learning")*

[^4.3]: "Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix $\Sigma_k = \sum$. In comparing two classes k and l, it is sufficient to look at the log-ratio, and we see that" *(Trecho de "The Elements of Statistical Learning")*

[^4.4]: "The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0,1]." *(Trecho de "The Elements of Statistical Learning")*

[^4.5]: "In this situation the features are high-dimensional and correlated, and the LDA coefficients can be regularized to be smooth or sparse in the original domain of the signal. This leads to better generalization and allows for easier interpretation of the coefficients." *(Trecho de "The Elements of Statistical Learning")*

[^4.5.2]: "The second method, due to Vapnik (1996), finds an optimally separating hyperplane if one exists, else finds a hyperplane that minimizes some measure of overlap in the training data. We treat the separable case here, and defer treatment of the nonseparable case to Chapter 12." *(Trecho de "The Elements of Statistical Learning")*
