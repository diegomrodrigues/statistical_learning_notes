### Vetores de Suporte Definem a Separa√ß√£o √ìtima

```mermaid
graph LR
    subgraph "SVM Architecture"
    direction TB
    A["Training Data"] --> B["Support Vectors"]
    B --> C["Hyperplane"]
    C --> D["Optimal Separation"]
    B --> E["Margin"]
    E --> C
    end
```

Em modelos de classifica√ß√£o linear baseados em **maximiza√ß√£o da margem**, como as **M√°quinas de Vetores de Suporte (SVM)**, os **vetores de suporte** desempenham um papel fundamental na defini√ß√£o do **hiperplano separador √≥timo** [^4.5.2]. Esses vetores, que s√£o um subconjunto das observa√ß√µes de treinamento, s√£o os √∫nicos pontos que realmente influenciam a posi√ß√£o e a orienta√ß√£o do hiperplano, tornando-os essenciais para a compress√£o e a constru√ß√£o de modelos SVM.

**O Papel dos Vetores de Suporte:**

Os vetores de suporte s√£o as observa√ß√µes de treinamento que se encontram mais pr√≥ximas ao hiperplano separador. Formalmente, s√£o as observa√ß√µes que est√£o na fronteira da margem ou no lado incorreto da fronteira, ou seja, aquelas que est√£o mais pr√≥ximas da regi√£o de separa√ß√£o. Em termos da formula√ß√£o matem√°tica do SVM, os vetores de suporte s√£o as observa√ß√µes para as quais os multiplicadores de Lagrange ($\alpha_i$) obtidos na solu√ß√£o do problema dual de otimiza√ß√£o s√£o diferentes de zero.

```mermaid
graph LR
    subgraph "Support Vector Properties"
        direction TB
        A["Support Vectors"] --> B["Defines Hyperplane"]
        A --> C["Defines Margin Width"]
        A --> D["Compact Representation"]
        A --> E["Robustness"]
        A --> F["Kernel Trick Basis"]
    end
```

As propriedades dos vetores de suporte s√£o:

1.  **Defini√ß√£o do Hiperplano:** A solu√ß√£o para o hiperplano separador √≥timo em SVM √© dada em termos de uma combina√ß√£o linear dos vetores de suporte. As observa√ß√µes que n√£o s√£o vetores de suporte n√£o contribuem para a solu√ß√£o final. Isso significa que a posi√ß√£o e orienta√ß√£o do hiperplano s√£o totalmente determinadas pelos vetores de suporte, o que torna os modelos SVM esparsos e eficientes.

2.  **Margem:** Os vetores de suporte s√£o os pontos que definem a largura da margem. A dist√¢ncia entre os vetores de suporte e o hiperplano √© igual √† margem, e o SVM busca encontrar um hiperplano que maximize essa dist√¢ncia.

3.  **Representa√ß√£o Compacta:** Como a solu√ß√£o depende apenas dos vetores de suporte, o modelo resultante √© mais compacto e eficiente em termos de armazenamento e computa√ß√£o. A solu√ß√£o, com um n√∫mero reduzido de vetores de suporte, gera um modelo eficiente em termos de capacidade de generaliza√ß√£o.

4.  **Robustez:** Os vetores de suporte s√£o selecionados de forma a garantir a robustez do modelo em rela√ß√£o a pequenas varia√ß√µes nos dados. O modelo n√£o √© influenciado por pontos muito distantes da fronteira de decis√£o, mas √© guiado pelos pontos mais relevantes para a separa√ß√£o das classes.

5.  **Rela√ß√£o com o Kernel Trick:** Quando os SVM s√£o utilizados com *kernels*, os vetores de suporte s√£o projetados no espa√ßo de maior dimensionalidade, e a solu√ß√£o do problema de otimiza√ß√£o depende apenas dos vetores de suporte nesse espa√ßo transformado. A fun√ß√£o *kernel* √© utilizada para o c√°lculo da combina√ß√£o linear no espa√ßo de alta dimens√£o.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simples em 2D com duas classes (azul e vermelho) e alguns pontos de dados. Suponha que ap√≥s o treinamento de um SVM, tenhamos encontrado tr√™s vetores de suporte:
>
> -   $x_1 = (1, 2)$, com r√≥tulo $y_1 = 1$ (classe azul)
> -   $x_2 = (2, 1)$, com r√≥tulo $y_2 = -1$ (classe vermelha)
> -   $x_3 = (2.5, 2.5)$, com r√≥tulo $y_3 = 1$ (classe azul)
>
> E os multiplicadores de Lagrange associados s√£o:
>
> -   $\alpha_1 = 0.5$
> -   $\alpha_2 = 1$
> -   $\alpha_3 = 0.8$
>
> O vetor normal do hiperplano ($\beta$) √© dado por:
>
> $\beta = \sum_{i \in SV} \alpha_i y_i x_i = 0.5 * 1 * (1, 2) + 1 * -1 * (2, 1) + 0.8 * 1 * (2.5, 2.5) = (0.5, 1) + (-2, -1) + (2, 2) = (0.5, 2)$
>
> O intercepto $\beta_0$ √© obtido a partir das condi√ß√µes de Karush-Kuhn-Tucker (KKT) e pode ser calculado usando um dos vetores de suporte, por exemplo, $x_1$:
>
> $\beta_0 = y_1 - \beta^T x_1 = 1 - (0.5 * 1 + 2 * 2) = 1 - 4.5 = -3.5$
>
> Assim, o hiperplano √© definido por $0.5x_1 + 2x_2 - 3.5 = 0$.  Se tiv√©ssemos um novo ponto $x = (3, 1)$, a classifica√ß√£o seria dada por:
>
>  $\hat{G}(x) = \text{sign}(0.5 * 3 + 2 * 1 - 3.5) = \text{sign}(1.5+2-3.5) = \text{sign}(0) = 0$, indicando que estaria no hiperplano.
>
>  Se tiv√©ssemos um novo ponto $x = (3, 2)$, a classifica√ß√£o seria dada por:
>
>  $\hat{G}(x) = \text{sign}(0.5 * 3 + 2 * 2 - 3.5) = \text{sign}(1.5+4-3.5) = \text{sign}(2) = 1$, indicando que estaria na classe azul.
>
> Note que apenas os vetores de suporte influenciaram o c√°lculo de $\beta$ e $\beta_0$.

```mermaid
graph LR
    subgraph "Hyperplane Calculation"
    direction LR
        A["'Œ≤ = Œ£ Œ±_i y_i x_i'"] --> B["'Œ≤' (Hyperplane Normal Vector)"]
        C["'Œ≤_0 = y_i - Œ≤^T x_i'"] --> D["'Œ≤_0' (Hyperplane Intercept)"]
    end
```

**Formalmente:**

Dado o problema de otimiza√ß√£o do SVM:

$$
    \min_{\beta, \beta_0} \frac{1}{2} ||\beta||^2
$$
sujeito a:
$$
    y_i (\beta_0 + \beta^T x_i) \geq 1, \text{ para } i = 1,\ldots,N
$$
a solu√ß√£o para o hiperplano √≥timo √© dada por:

$$
    \beta = \sum_{i \in SV} \alpha_i y_i \phi(x_i)
$$

onde $\alpha_i$ s√£o os multiplicadores de Lagrange, $y_i$ s√£o os r√≥tulos de classe, $\phi(x_i)$ √© o mapeamento do vetor de entrada $x_i$ para o espa√ßo de caracter√≠sticas, e o conjunto SV s√£o os vetores de suporte (para os quais $\alpha_i > 0$). O resultado acima mostra que a solu√ß√£o √© uma combina√ß√£o linear dos vetores de suporte (no espa√ßo transformado pelo kernel), e que os outros pontos n√£o influenciam o hiperplano resultante.

A decis√£o de classifica√ß√£o para uma nova observa√ß√£o $x$ √© dada por:

$$
    \hat{G}(x) = \text{sign} \left( \sum_{i \in SV} \alpha_i y_i k(x_i, x) + \beta_0 \right)
$$
e depende apenas dos vetores de suporte.

```mermaid
graph LR
    subgraph "SVM Solution"
        direction TB
        A["'Hyperplane (Œ≤)'"] --> B["'Linear Combination of Support Vectors'"]
        C["'Classification Function (GÃÇ(x))'"] --> D["'Depends on Support Vectors and Kernel'"]
        B --> D
    end
```

**Lemma 56:** *Os vetores de suporte s√£o as observa√ß√µes de treinamento que se encontram na margem ou no lado incorreto da fronteira de decis√£o, e que determinam a posi√ß√£o e a orienta√ß√£o do hiperplano separador √≥timo em SVM.*

*Prova:* Os vetores de suporte s√£o as observa√ß√µes para as quais os multiplicadores de Lagrange s√£o diferentes de zero, e sua combina√ß√£o linear define o vetor normal do hiperplano.  $\blacksquare$

**Corol√°rio 56:** *A solu√ß√£o para o hiperplano separador √≥timo em SVM √© dada como uma combina√ß√£o linear dos vetores de suporte, o que leva a modelos esparsos e com capacidade de generaliza√ß√£o.*

*Prova:* A solu√ß√£o para o problema do SVM √© dada em termos dos multiplicadores de Lagrange que s√£o diferentes de zero, e sua combina√ß√£o linear define o hiperplano.  $\blacksquare$

A identifica√ß√£o e a utiliza√ß√£o de vetores de suporte √© essencial para a compreens√£o do funcionamento dos modelos de classifica√ß√£o baseados em maximiza√ß√£o da margem, como o SVM.

### Rela√ß√£o com o Algoritmo Perceptron e a Busca por Pontos de Suporte

```mermaid
graph LR
    subgraph "Perceptron vs SVM"
    direction LR
        A["Perceptron"] --> B["Iterative Updates"]
        B --> C["Hyperplane"]
        A --> D["Limitations (Non-convergence, Sensitivity)"]
        C --> E["SVM"]
        E --> F["Maximizes Margin"]
        E --> G["Support Vectors"]
        F & G --> C
    end
```

A rela√ß√£o entre o algoritmo **perceptron** e as **M√°quinas de Vetores de Suporte (SVM)** √© fundamental para entender como os m√©todos de classifica√ß√£o linear evolu√≠ram ao longo do tempo e como as limita√ß√µes de um modelo mais simples levaram ao desenvolvimento de modelos mais sofisticados [^4.5.1], [^4.5.2]. Apesar da simplicidade do perceptron, sua l√≥gica de aprendizado iterativo e a busca por um hiperplano separador s√£o elementos presentes na formula√ß√£o do SVM, mas com algumas melhorias.

**O Perceptron e sua Busca por um Hiperplano Separador:**

O algoritmo do perceptron, como discutido em se√ß√µes anteriores, busca encontrar um hiperplano separador por meio de um processo iterativo baseado em gradiente descendente. O perceptron atualiza os pesos ($\beta$ e $\beta_0$) do hiperplano com base nas observa√ß√µes mal classificadas, buscando um hiperplano que separe as classes com o menor n√∫mero de erros poss√≠vel.

O algoritmo do perceptron utiliza a seguinte regra de atualiza√ß√£o iterativa para os par√¢metros:

$$
    \beta^{(t+1)} = \beta^{(t)} + \rho y_i x_i \text{ se } y_i(\beta_0 + \beta^T x_i) < 0
$$

$$
    \beta_0^{(t+1)} = \beta_0^{(t)} + \rho y_i \text{ se } y_i(\beta_0 + \beta^T x_i) < 0
$$

onde $\rho$ √© a taxa de aprendizagem, $y_i$ √© a classe da observa√ß√£o e $x_i$ s√£o os preditores.

Apesar de sua simplicidade, o perceptron apresenta algumas limita√ß√µes:

1.  **Converg√™ncia em Dados N√£o Separ√°veis:** O perceptron n√£o converge quando os dados n√£o s√£o linearmente separ√°veis, e pode apresentar ciclos e oscila√ß√µes.

2.  **Sensibilidade a Outliers:** O perceptron √© sens√≠vel a *outliers*, e sua solu√ß√£o pode ser influenciada por valores extremos.

3.  **Solu√ß√£o N√£o √önica:** A solu√ß√£o final do perceptron depende da inicializa√ß√£o dos par√¢metros e da ordem de apresenta√ß√£o dos dados, o que pode levar a diferentes hiperplanos separadores.

> üí° **Exemplo Num√©rico:**
>
> Considere um conjunto de dados com duas classes, onde temos os seguintes pontos com r√≥tulos $y_i \in \{-1, 1\}$:
>
> - $x_1 = (1, 1)$, $y_1 = 1$
> - $x_2 = (2, 0)$, $y_2 = 1$
> - $x_3 = (0, 1)$, $y_3 = -1$
> - $x_4 = (0, 0)$, $y_4 = -1$
>
> Inicializamos $\beta = (0, 0)$ e $\beta_0 = 0$. Vamos usar uma taxa de aprendizagem $\rho = 1$.
>
> **Itera√ß√£o 1:**
>
> - Para $x_1$: $y_1(\beta_0 + \beta^T x_1) = 1(0 + 0*1 + 0*1) = 0 < 1$, ent√£o n√£o h√° erro.
> - Para $x_2$: $y_2(\beta_0 + \beta^T x_2) = 1(0 + 0*2 + 0*0) = 0 < 1$, ent√£o n√£o h√° erro.
> - Para $x_3$: $y_3(\beta_0 + \beta^T x_3) = -1(0 + 0*0 + 0*1) = 0 < 1$, ent√£o n√£o h√° erro.
> - Para $x_4$: $y_4(\beta_0 + \beta^T x_4) = -1(0 + 0*0 + 0*0) = 0 < 1$, ent√£o n√£o h√° erro.
>
> Como n√£o h√° erros, vamos supor um novo ponto $x_1$ mal classificado para exemplificar a atualiza√ß√£o.
>
> **Itera√ß√£o 2:**
>
> - Para $x_1$: Suponha que $y_1(\beta_0 + \beta^T x_1) < 0$.
> - Atualiza√ß√£o:
>    - $\beta^{(1)} = \beta^{(0)} + \rho y_1 x_1 = (0, 0) + 1 * 1 * (1, 1) = (1, 1)$
>    - $\beta_0^{(1)} = \beta_0^{(0)} + \rho y_1 = 0 + 1 * 1 = 1$
>
> O hiperplano agora √© definido por $x_1 + x_2 + 1 = 0$, ou $x_1+x_2 = -1$.
>
> Se continuarmos o processo iterativo, o perceptron ajustar√° o hiperplano at√© encontrar uma solu√ß√£o (se os dados forem linearmente separ√°veis). A solu√ß√£o final depender√° da ordem dos dados e da inicializa√ß√£o.

```mermaid
graph LR
    subgraph "Perceptron Update Rule"
    direction TB
        A["'Update Œ≤: Œ≤(t+1) = Œ≤(t) + œÅ y_i x_i'"]
        B["'Update Œ≤‚ÇÄ: Œ≤‚ÇÄ(t+1) = Œ≤‚ÇÄ(t) + œÅ y_i'"]
    end
```

**O SVM e a Busca por um Hiperplano com Margem M√°xima:**

O SVM surge como uma extens√£o do perceptron, que busca n√£o apenas um hiperplano que separe as classes, mas um hiperplano que maximize a margem entre as classes [^4.5.2]. Essa abordagem leva a modelos mais robustos e com maior capacidade de generaliza√ß√£o.

O SVM utiliza um problema de otimiza√ß√£o convexo para encontrar o hiperplano separador que maximize a margem, que √© definida como a dist√¢ncia m√≠nima entre o hiperplano e as observa√ß√µes mais pr√≥ximas (os vetores de suporte). A formula√ß√£o do SVM, como vimos em cap√≠tulos anteriores, resulta em uma solu√ß√£o que depende somente dos vetores de suporte. A busca do SVM √© por um hiperplano que seja est√°vel e que tenha o maior n√≠vel poss√≠vel de generaliza√ß√£o para novos dados.

**Rela√ß√£o Entre os Dois Modelos:**

Embora o perceptron seja mais simples e n√£o otimize a margem, a sua estrutura b√°sica, com a busca por uma fronteira linear separadora, est√° na base do SVM.  O SVM utiliza o mesmo conceito de hiperplano separador, mas o otimiza sob a perspectiva da maximiza√ß√£o da margem. Ambos os modelos utilizam uma combina√ß√£o linear dos preditores para definir uma fronteira de decis√£o.

O SVM surge como uma evolu√ß√£o do perceptron ao adicionar o conceito de margem e o uso de m√©todos *kernel* para lidar com problemas n√£o lineares, sendo mais robusto e generaliz√°vel que o perceptron.

A busca do SVM por uma margem maior pode ser vista como uma forma de buscar a solu√ß√£o que seja mais robusta para novas observa√ß√µes, o que o torna um m√©todo com maior poder e capacidade de generaliza√ß√£o do que o Perceptron, al√©m de apresentar uma conex√£o com os m√©todos para problemas de otimiza√ß√£o convexa.

**Lemma 57:** *O perceptron e o SVM s√£o modelos lineares que buscam um hiperplano separador, mas o SVM adiciona o conceito de margem m√°xima e se baseia em vetores de suporte.*

*Prova:* O Perceptron busca apenas a separa√ß√£o, e o SVM busca o hiperplano que maximize a margem, o que define a sua diferen√ßa fundamental.  $\blacksquare$

**Corol√°rio 57:** *O SVM, ao buscar maximizar a margem, obt√©m solu√ß√µes mais est√°veis e generaliz√°veis do que o perceptron, que √© sens√≠vel √† inicializa√ß√£o e √† ordem dos dados.*

*Prova:* A maximiza√ß√£o da margem e a solu√ß√£o baseada em vetores de suporte diminui a sensibilidade a ru√≠dos e √† ordem da apresenta√ß√£o dos dados de treino. $\blacksquare$

A compreens√£o da rela√ß√£o entre o Perceptron e o SVM permite uma melhor compreens√£o da evolu√ß√£o dos modelos de classifica√ß√£o linear e das vantagens do SVM em rela√ß√£o ao Perceptron.

### Conclus√£o

Este cap√≠tulo explorou como os vetores de suporte definem a separa√ß√£o √≥tima em SVM, apresentando a solu√ß√£o como uma combina√ß√£o linear dos vetores de suporte e a regra de classifica√ß√£o para novas observa√ß√µes. Foi discutida a rela√ß√£o entre a regress√£o de indicadores e os SVM, e como a busca por um hiperplano separador pode ser feita utilizando diferentes abordagens e com diferen√ßas na solu√ß√£o. Foi tamb√©m explorada a rela√ß√£o do algoritmo do perceptron e do SVM, e as vantagens e limita√ß√µes de cada um. A compreens√£o desses conceitos √© essencial para entender os modelos de classifica√ß√£o linear e para a utiliza√ß√£o das M√°quinas de Vetores de Suporte (SVM) em problemas pr√°ticos.

### Footnotes

[^4.1]: "In this chapter we revisit the classification problem and focus on linear methods for classification. Since our predictor G(x) takes values in a discrete set G, we can always divide the input space into a collection of regions labeled according to the classification. We saw in Chapter 2 that the boundaries of these regions can be rough or smooth, depending on the prediction function. For an important class of procedures, these decision boundaries are linear; this is what we will mean by linear methods for classification." *(Trecho de "The Elements of Statistical Learning")*

[^4.2]: "In Chapter 2 we fit linear regression models to the class indicator variables, and classify to the largest fit. Suppose there are K classes, for convenience labeled 1,2,..., K, and the fitted linear model for the kth indicator response variable is fk(x) = Œ≤ko + Œ≤Tx. The decision boundary between class k and l is that set of points for which fk(x) = fl(x), that is, the set {x: (Œ≤ko ‚Äì Œ≤eo) + (Œ≤Œ∫ ‚Äì Œ≤e)Tx = 0}, an affine set or hyperplane" *(Trecho de "The Elements of Statistical Learning")*

[^4.3]: "Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Œ£k = ‚àë. In comparing two classes k and l, it is sufficient to look at the log-ratio, and we see that" *(Trecho de "The Elements of Statistical Learning")*

[^4.4]: "The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0,1]." *(Trecho de "The Elements of Statistical Learning")*

[^4.5]: "In this situation the features are high-dimensional and correlated, and the LDA coefficients can be regularized to be smooth or sparse in the original domain of the signal. This leads to better generalization and allows for easier interpretation of the coefficients." *(Trecho de "The Elements of Statistical Learning")*

[^4.5.1]: "The first is the well-known perceptron model of Rosenblatt (1958), with an algorithm that finds a separating hyperplane in the training data, if one exists." *(Trecho de "The Elements of Statistical Learning")*

[^4.5.2]: "The second method, due to Vapnik (1996), finds an optimally separating hyperplane if one exists, else finds a hyperplane that minimizes some measure of overlap in the training data. We treat the separable case here, and defer treatment of the nonseparable case to Chapter 12." *(Trecho de "The Elements of Statistical Learning")*
