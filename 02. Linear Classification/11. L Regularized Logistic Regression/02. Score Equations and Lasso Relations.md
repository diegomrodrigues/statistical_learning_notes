### Interpreta√ß√£o das Equa√ß√µes de *Score* e Rela√ß√µes com o Lasso Generalizado

```mermaid
graph LR
    subgraph "Score Equation Derivation"
        direction TB
        A["Log-Likelihood Function: $\ell(\beta)$"]
        B["Partial Derivatives: $\frac{\partial \ell}{\partial \beta_0}, \frac{\partial \ell}{\partial \beta}$"]
        C["Score Equations: $\frac{\partial \ell}{\partial \beta_0} = 0, \frac{\partial \ell}{\partial \beta} = 0$"]
        A --> B
        B --> C
    end
    subgraph "Lasso Regularization"
        direction TB
        D["Penalized Log-Likelihood: $\ell_{Lasso}(\beta)$"]
        E["Modified Score Equations (Lasso): $\frac{\partial \ell_{Lasso}}{\partial \beta_j}$"]
        D --> E
    end
    C --> F["Optimal MLE Estimators"]
    E --> G["Optimal Regularized Estimators"]
    F --> H["Insights on variable selection and shrinkage"]
    G --> H
    H --> I["Connection with Generalized Lasso"]
```

As **equa√ß√µes de *score*** desempenham um papel central na otimiza√ß√£o de modelos estat√≠sticos, incluindo a regress√£o log√≠stica. Elas representam as derivadas da fun√ß√£o de log-verossimilhan√ßa em rela√ß√£o aos par√¢metros do modelo e fornecem as condi√ß√µes necess√°rias para encontrar os estimadores de m√°xima verossimilhan√ßa (MLEs). Na presen√ßa de regulariza√ß√£o, como a penalidade Lasso, as equa√ß√µes de *score* s√£o modificadas e podem nos fornecer *insights* importantes sobre o processo de sele√ß√£o de vari√°veis e *shrinkage* de coeficientes. A rela√ß√£o das equa√ß√µes de *score* com o **Lasso Generalizado** nos permite entender melhor a conex√£o com outros m√©todos de regulariza√ß√£o.

Em regress√£o log√≠stica, a fun√ß√£o de log-verossimilhan√ßa √© dada por:

$$
    \ell(\beta) = \sum_{i=1}^N [y_i \log(p_i) + (1-y_i) \log(1 - p_i)]
$$

onde $p_i = \frac{e^{\beta_0 + \beta^T x_i}}{1 + e^{\beta_0 + \beta^T x_i}}$ √© a probabilidade estimada da classe positiva para a observa√ß√£o $i$. As equa√ß√µes de *score* s√£o obtidas calculando as derivadas parciais da log-verossimilhan√ßa em rela√ß√£o aos par√¢metros $\beta_0$ e $\beta$:

$$
    \frac{\partial \ell}{\partial \beta_0} = \sum_{i=1}^N (y_i - p_i)
$$

$$
    \frac{\partial \ell}{\partial \beta} = \sum_{i=1}^N x_i (y_i - p_i)
$$

Igualando as equa√ß√µes de *score* a zero, obtemos um sistema de equa√ß√µes n√£o lineares que define as condi√ß√µes de otimalidade para os estimadores de m√°xima verossimilhan√ßa (MLEs) em modelos de regress√£o log√≠stica sem regulariza√ß√£o. Na presen√ßa de penalidade L1, ou Lasso, a fun√ß√£o objetivo √© modificada:

```mermaid
graph LR
    subgraph "Lasso Objective Function"
        direction LR
        A["Log-Likelihood: $\sum_{i=1}^N [y_i \log(p_i) + (1-y_i) \log(1 - p_i)]$"]
        B["L1 Penalty: $\lambda \sum_{j=1}^p |\beta_j|$"]
        A --> C["Lasso Objective: $\ell_{Lasso}(\beta)$"]
        B --> C
    end
```

$$
    \ell_{Lasso}(\beta) = \sum_{i=1}^N [y_i \log(p_i) + (1-y_i) \log(1 - p_i)] - \lambda \sum_{j=1}^p |\beta_j|
$$

onde $\lambda$ √© o par√¢metro de regulariza√ß√£o que controla a intensidade da penalidade. As equa√ß√µes de *score* para os par√¢metros $\beta$ s√£o ent√£o modificadas:

$$
     \frac{\partial \ell_{Lasso}}{\partial \beta_j} = \sum_{i=1}^N x_{ij} (y_i - p_i) - \lambda \cdot \text{sign}(\beta_j) = 0 \text{ para } \beta_j \neq 0
$$

$$
     \frac{\partial \ell_{Lasso}}{\partial \beta_j} = \sum_{i=1}^N x_{ij} (y_i - p_i) \text{ e } \left| \sum_{i=1}^N x_{ij} (y_i - p_i) \right| \leq \lambda \text{ para } \beta_j = 0
$$

onde $\text{sign}(\beta_j)$ √© o sinal do coeficiente $\beta_j$ (1 se $\beta_j > 0$, -1 se $\beta_j < 0$, e um valor entre -1 e 1 se $\beta_j = 0$). Essas equa√ß√µes de *score* revelam que, para coeficientes n√£o nulos, o score (derivada da verossimilhan√ßa) iguala a penalidade $\lambda$ multiplicada pelo sinal do coeficiente. Se o score for menor que o valor da penaliza√ß√£o, o coeficiente √© zerado.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um modelo de regress√£o log√≠stica com dois preditores ($x_1$ e $x_2$) e um intercepto. Temos 5 observa√ß√µes e os seguintes valores:
>
> | i | $y_i$ | $x_{i1}$ | $x_{i2}$ |
> |---|---|---|---|
> | 1 | 1 | 2 | 3 |
> | 2 | 0 | 1 | 1 |
> | 3 | 1 | 3 | 2 |
> | 4 | 0 | 1 | 4 |
> | 5 | 1 | 2 | 2 |
>
> Ap√≥s ajustar um modelo de regress√£o log√≠stica, obtemos os seguintes coeficientes (sem regulariza√ß√£o):
>
> $\beta_0 = -0.5$
> $\beta_1 = 0.8$
> $\beta_2 = -0.3$
>
> As probabilidades estimadas $p_i$ s√£o calculadas usando a fun√ß√£o log√≠stica:
>
> $p_i = \frac{e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}}{1 + e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}}$
>
> Vamos calcular os scores para $\beta_1$ e $\beta_2$ sem regulariza√ß√£o:
>
> $\frac{\partial \ell}{\partial \beta_1} = \sum_{i=1}^5 x_{i1} (y_i - p_i)$
> $\frac{\partial \ell}{\partial \beta_2} = \sum_{i=1}^5 x_{i2} (y_i - p_i)$
>
> Suponha que, ap√≥s os c√°lculos, obtivemos:
>
> $\frac{\partial \ell}{\partial \beta_1} = 1.2$
> $\frac{\partial \ell}{\partial \beta_2} = -0.5$
>
> Agora, vamos aplicar a penalidade Lasso com $\lambda = 0.7$. As equa√ß√µes de *score* modificadas s√£o:
>
> $\frac{\partial \ell_{Lasso}}{\partial \beta_1} = 1.2 - 0.7 \cdot \text{sign}(0.8) = 1.2 - 0.7 = 0.5$
> $\frac{\partial \ell_{Lasso}}{\partial \beta_2} = -0.5 - 0.7 \cdot \text{sign}(-0.3) = -0.5 - 0.7 \cdot (-1) = 0.2$
>
> Como os valores absolutos das derivadas com a penalidade s√£o diferentes de zero, os coeficientes permanecem no modelo.
>
> Se $\lambda$ fosse 2.0, ter√≠amos:
>
> $\frac{\partial \ell_{Lasso}}{\partial \beta_1} = 1.2 - 2.0 \cdot \text{sign}(0.8) = 1.2 - 2.0 = -0.8$. Como a derivada √© diferente de zero, o coeficiente n√£o √© zerado.
>
> $\frac{\partial \ell_{Lasso}}{\partial \beta_2} = -0.5 - 2.0 \cdot \text{sign}(-0.3) = -0.5 - 2.0 \cdot (-1) = 1.5$.  Como a derivada √© diferente de zero, o coeficiente n√£o √© zerado.
>
> Se, por exemplo, o score de $\beta_2$ fosse -0.3 sem regulariza√ß√£o, com $\lambda = 0.7$ ter√≠amos:
>
> $\frac{\partial \ell_{Lasso}}{\partial \beta_2} = -0.3 - 0.7 \cdot \text{sign}(-0.3) = -0.3 - 0.7 \cdot (-1) = 0.4$.  O coeficiente n√£o √© zerado.
>
> Agora, suponha que o score de $\beta_2$ fosse -0.2 sem regulariza√ß√£o. Com $\lambda=0.7$ ter√≠amos:
>
> $\frac{\partial \ell_{Lasso}}{\partial \beta_2} = -0.2 - 0.7 \cdot \text{sign}(-0.2) = -0.2 - 0.7 \cdot (-1) = 0.5$.  O coeficiente n√£o √© zerado.
>
> Se o score original de $\beta_2$ fosse -0.6, ter√≠amos:
>
> $\frac{\partial \ell_{Lasso}}{\partial \beta_2} = -0.6 - 0.7 \cdot \text{sign}(-0.6) = -0.6 - 0.7 \cdot (-1) = 0.1$. O coeficiente n√£o √© zerado.
>
> Se o score original de $\beta_2$ fosse -0.8, ter√≠amos:
>
> $\frac{\partial \ell_{Lasso}}{\partial \beta_2} = -0.8 - 0.7 \cdot \text{sign}(-0.8) = -0.8 - 0.7 \cdot (-1) = -0.1$.  O coeficiente n√£o √© zerado.
>
> No caso em que o score de $\beta_2$ sem regulariza√ß√£o fosse -0.3 e aplicamos a penaliza√ß√£o com $\lambda = 0.2$, ter√≠amos:
>
> $\frac{\partial \ell_{Lasso}}{\partial \beta_2} = -0.3 - 0.2 \cdot \text{sign}(-0.3) = -0.3 - 0.2 \cdot (-1) = -0.1$ , o coeficiente n√£o √© zerado.
>
> No caso em que o score de $\beta_2$ sem regulariza√ß√£o fosse -0.1 e aplicamos a penaliza√ß√£o com $\lambda = 0.2$, ter√≠amos:
>
> $\frac{\partial \ell_{Lasso}}{\partial \beta_2} = -0.1 - 0.2 \cdot \text{sign}(-0.1) = -0.1 - 0.2 \cdot (-1) = 0.1$. O coeficiente n√£o √© zerado.
>
> No caso em que o score de $\beta_2$ sem regulariza√ß√£o fosse -0.15 e aplicamos a penaliza√ß√£o com $\lambda = 0.2$, ter√≠amos:
>
> $\frac{\partial \ell_{Lasso}}{\partial \beta_2} = -0.15 - 0.2 \cdot \text{sign}(-0.15) = -0.15 - 0.2 \cdot (-1) = 0.05$. O coeficiente n√£o √© zerado.
>
> No caso em que o score de $\beta_2$ sem regulariza√ß√£o fosse 0.15 e aplicamos a penaliza√ß√£o com $\lambda = 0.2$, ter√≠amos:
>
> $\frac{\partial \ell_{Lasso}}{\partial \beta_2} = 0.15 - 0.2 \cdot \text{sign}(0.15) = 0.15 - 0.2 \cdot (1) = -0.05$. O coeficiente n√£o √© zerado.
>
> No caso em que o score de $\beta_2$ sem regulariza√ß√£o fosse 0 e aplicamos a penaliza√ß√£o com $\lambda = 0.2$, ter√≠amos:
>
> $\left| \sum_{i=1}^N x_{i2} (y_i - p_i) \right| = 0 < 0.2$. Portanto, $\beta_2 = 0$.
>
> Este exemplo ilustra como a penalidade Lasso pode levar √† esparsidade, zerando alguns coeficientes, dependendo do valor de $\lambda$ e do score da vari√°vel.

A interpreta√ß√£o das equa√ß√µes de *score* no contexto da penalidade L1 √© crucial para entender a sele√ß√£o de vari√°veis. Os preditores cujas equa√ß√µes de score n√£o s√£o zeradas pela penalidade permanecem no modelo. O valor de $\lambda$ determina quantos preditores permanecer√£o no modelo e o grau de *shrinkage* dos coeficientes.

```mermaid
graph LR
    subgraph "Score-Lasso Relationship"
    direction TB
        A["Score Equation: $\frac{\partial \ell}{\partial \beta_j}$"]
        B["Lasso Penalty: $\lambda \cdot sign(\beta_j)$"]
        C["Condition: $\frac{\partial \ell_{Lasso}}{\partial \beta_j} = 0$ or $\left| \frac{\partial \ell}{\partial \beta_j} \right| \leq \lambda$"]
        A --> C
        B --> C
    end
```

A rela√ß√£o das equa√ß√µes de *score* com o **Lasso Generalizado** pode ser vista no contexto das condi√ß√µes de Karush-Kuhn-Tucker (KKT). O problema do Lasso pode ser visto como um caso especial de um problema de otimiza√ß√£o convexo com restri√ß√µes, e as condi√ß√µes de KKT fornecem um conjunto de condi√ß√µes necess√°rias para a solu√ß√£o √≥tima. As equa√ß√µes de *score* e as condi√ß√µes de complementariedade nas condi√ß√µes de KKT revelam uma rela√ß√£o direta com a penalidade L1:

$$
    \frac{\partial \ell}{\partial \beta_j} = \lambda \cdot v_j
$$

onde $v_j \in [-1,1]$. Em palavras, o score de cada vari√°vel deve estar dentro do intervalo da penalidade. Essa condi√ß√£o se reduz aos casos j√° apresentados, onde $\beta_j = 0$ se $|score_j| < \lambda$ e o score √© igual a $\lambda \cdot \text{sign}(\beta_j)$ quando $\beta_j \neq 0$.

O **Lasso Generalizado** estende o Lasso a outros tipos de penaliza√ß√µes, como a penaliza√ß√£o el√°stica (combina√ß√£o de L1 e L2), penalidades com pesos diferentes para cada vari√°vel (Weighted Lasso), entre outros. As equa√ß√µes de *score* e as condi√ß√µes de KKT podem ser generalizadas para esses casos, permitindo uma an√°lise mais ampla e flex√≠vel de modelos regularizados.

**Lemma 19:** *As equa√ß√µes de *score* para modelos de regress√£o log√≠stica com penalidade Lasso igualam a derivada da log-verossimilhan√ßa com um termo proporcional ao sinal do coeficiente, induzindo a esparsidade.*

*Prova:* A presen√ßa do termo $\lambda \cdot \text{sign}(\beta_j)$ na equa√ß√£o de *score* implica que o gradiente da fun√ß√£o de verossimilhan√ßa deve se equilibrar com um m√∫ltiplo do sinal do coeficiente, levando a coeficientes nulos e *shrinkage*. [^4.4.4] $\blacksquare$

**Corol√°rio 19:** *As condi√ß√µes de Karush-Kuhn-Tucker (KKT) fornecem uma base formal para entender as condi√ß√µes de otimalidade do Lasso, incluindo a rela√ß√£o entre os *scores*, os coeficientes e o par√¢metro de regulariza√ß√£o.*

*Prova:* As condi√ß√µes KKT estabelecem um conjunto de condi√ß√µes necess√°rias para a otimalidade em problemas de otimiza√ß√£o convexa com restri√ß√µes, incluindo o caso da regress√£o log√≠stica com penalidade L1. $\blacksquare$

A interpreta√ß√£o das equa√ß√µes de *score* e sua rela√ß√£o com o Lasso Generalizado oferece *insights* importantes para a compreens√£o dos modelos de regress√£o log√≠stica com penalidade L1 e suas propriedades de sele√ß√£o de vari√°veis e *shrinkage*.

### Regulariza√ß√£o El√°stica e uma Generaliza√ß√£o da Penalidade Lasso

```mermaid
graph LR
    subgraph "Elastic Net Penalty"
        direction LR
        A["L1 Penalty: $\lambda_1 \sum_{j=1}^p |\beta_j|$"]
        B["L2 Penalty: $\lambda_2 \sum_{j=1}^p \beta_j^2$"]
        A --> C["Elastic Net Penalty: $\lambda_1 \sum_{j=1}^p |\beta_j| + \lambda_2 \sum_{j=1}^p \beta_j^2$"]
        B --> C
    end
```

A **regulariza√ß√£o el√°stica** (*Elastic Net*) √© uma generaliza√ß√£o da penalidade Lasso que combina as penalidades L1 (Lasso) e L2 (Ridge) em um √∫nico termo de regulariza√ß√£o [^4.5]. Essa abordagem visa combinar os benef√≠cios da sele√ß√£o de vari√°veis, fornecida pela penalidade L1, com a estabilidade da *shrinkage*, fornecida pela penalidade L2. A fun√ß√£o objetivo da regress√£o log√≠stica com penalidade el√°stica √© dada por:

$$
\ell_{ElasticNet}(\beta) = \sum_{i=1}^N [y_i \log(p_i) + (1-y_i) \log(1-p_i)] - \lambda_1 \sum_{j=1}^p |\beta_j| - \lambda_2 \sum_{j=1}^p \beta_j^2
$$

onde $\lambda_1$ e $\lambda_2$ s√£o os par√¢metros de regulariza√ß√£o que controlam a intensidade da penalidade L1 e L2, respectivamente. O termo de penalidade el√°stica √© uma combina√ß√£o linear das penalidades L1 e L2, permitindo que a regulariza√ß√£o seja feita de forma mais flex√≠vel e adapt√°vel √†s caracter√≠sticas dos dados. O *Elastic Net* pode ser tamb√©m escrito como:

$$
\ell_{ElasticNet}(\beta) = \sum_{i=1}^N [y_i \log(p_i) + (1-y_i) \log(1-p_i)] - \lambda \left[\alpha \sum_{j=1}^p |\beta_j| + (1 - \alpha) \sum_{j=1}^p \beta_j^2 \right]
$$

onde $\lambda = \lambda_1 + \lambda_2$ e $\alpha = \frac{\lambda_1}{\lambda_1 + \lambda_2}$ representa o balan√ßo entre a penalidade L1 e L2. A escolha de $\alpha = 1$ resulta no Lasso, enquanto $\alpha = 0$ resulta no Ridge.

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o mesmo conjunto de dados do exemplo anterior, com os seguintes coeficientes obtidos sem regulariza√ß√£o:
>
> $\beta_0 = -0.5$
> $\beta_1 = 0.8$
> $\beta_2 = -0.3$
>
> Suponha que queremos aplicar a regulariza√ß√£o el√°stica com $\lambda = 1.0$ e $\alpha = 0.6$. Isso significa que $\lambda_1 = \lambda \cdot \alpha = 1.0 \cdot 0.6 = 0.6$ e $\lambda_2 = \lambda \cdot (1 - \alpha) = 1.0 \cdot (1 - 0.6) = 0.4$. A fun√ß√£o objetivo com regulariza√ß√£o el√°stica √©:
>
> $\ell_{ElasticNet}(\beta) = \sum_{i=1}^N [y_i \log(p_i) + (1-y_i) \log(1-p_i)] - 0.6 \sum_{j=1}^p |\beta_j| - 0.4 \sum_{j=1}^p \beta_j^2$
>
> As equa√ß√µes de score para a regulariza√ß√£o el√°stica s√£o dadas por:
>
> $\frac{\partial \ell_{ElasticNet}}{\partial \beta_j} = \sum_{i=1}^N x_{ij} (y_i - p_i) - \lambda_1 \cdot \text{sign}(\beta_j) - 2\lambda_2 \beta_j$
>
> Vamos calcular as derivadas para $\beta_1$ e $\beta_2$. Usando os scores sem regulariza√ß√£o do exemplo anterior:
>
> $\frac{\partial \ell}{\partial \beta_1} = 1.2$
> $\frac{\partial \ell}{\partial \beta_2} = -0.5$
>
> As derivadas com regulariza√ß√£o el√°stica s√£o:
>
> $\frac{\partial \ell_{ElasticNet}}{\partial \beta_1} = 1.2 - 0.6 \cdot \text{sign}(0.8) - 2 \cdot 0.4 \cdot 0.8 = 1.2 - 0.6 - 0.64 = -0.04$
>
> $\frac{\partial \ell_{ElasticNet}}{\partial \beta_2} = -0.5 - 0.6 \cdot \text{sign}(-0.3) - 2 \cdot 0.4 \cdot (-0.3) = -0.5 + 0.6 + 0.24 = 0.34$
>
> Observe que a penalidade el√°stica combina os efeitos da penalidade L1 (Lasso), que leva √† esparsidade, e da penalidade L2 (Ridge), que leva ao *shrinkage*. O par√¢metro $\alpha$ controla o balan√ßo entre esses dois efeitos.
>
> Se $\alpha = 1$, ter√≠amos o Lasso, e se $\alpha = 0$, ter√≠amos o Ridge.
>
> Se $\alpha=0$, ter√≠amos:
>
> $\frac{\partial \ell_{ElasticNet}}{\partial \beta_1} = 1.2  - 2 \cdot 1 \cdot 0.8 = 1.2 - 1.6 = -0.4$
>
> $\frac{\partial \ell_{ElasticNet}}{\partial \beta_2} = -0.5 - 2 \cdot 1 \cdot (-0.3) = -0.5 + 0.6 = 0.1$
>
> Se $\alpha=1$, ter√≠amos:
>
> $\frac{\partial \ell_{ElasticNet}}{\partial \beta_1} = 1.2 - 1 \cdot \text{sign}(0.8) = 1.2 - 1 = 0.2$
>
> $\frac{\partial \ell_{ElasticNet}}{\partial \beta_2} = -0.5 - 1 \cdot \text{sign}(-0.3) = -0.5 + 1 = 0.5$
>
> Este exemplo mostra como a regulariza√ß√£o el√°stica afeta os coeficientes, combinando os efeitos de esparsidade e *shrinkage*.

A regulariza√ß√£o el√°stica oferece uma forma flex√≠vel de ajustar modelos log√≠sticos e permite que o usu√°rio explore o *trade-off* entre a esparsidade e a estabilidade dos coeficientes. As propriedades da penalidade el√°stica combinam as vantagens de ambas as penalidades:

1.  **Sele√ß√£o de Vari√°veis:** A penalidade L1 encoraja a esparsidade, for√ßando alguns coeficientes a zero e, dessa forma, realizando sele√ß√£o de vari√°veis.

2.  **Shrinkage:** A penalidade L2 reduz a magnitude dos coeficientes, aumentando a estabilidade e reduzindo a vari√¢ncia dos estimadores.

3.  **Agrupamento de Vari√°veis:** Em casos onde h√° multicolinearidade, a penalidade el√°stica tende a selecionar grupos de vari√°veis correlacionadas, mantendo a estabilidade do modelo e resolvendo o problema da instabilidade causado pela multicolinearidade.

4.  **Flexibilidade de Ajuste:** A penalidade el√°stica permite ajustar o balan√ßo entre as penalidades L1 e L2 por meio do par√¢metro $\alpha$, adaptando o modelo √†s caracter√≠sticas dos dados e √†s necessidades do problema.

O problema de otimiza√ß√£o da regress√£o log√≠stica com penalidade el√°stica √© mais complexo do que o problema com apenas penalidade L1 ou L2. Algoritmos de otimiza√ß√£o espec√≠ficos, como o *coordinate descent* modificado e m√©todos de ponto interior, s√£o utilizados para encontrar as solu√ß√µes dos modelos com regulariza√ß√£o el√°stica. A escolha dos par√¢metros $\lambda_1$ e $\lambda_2$ (ou $\lambda$ e $\alpha$) √© realizada atrav√©s de valida√ß√£o cruzada ou outras abordagens de sele√ß√£o de modelos.

A penalidade el√°stica, por meio da combina√ß√£o de L1 e L2, oferece uma abordagem mais vers√°til e robusta para a regulariza√ß√£o de modelos log√≠sticos e generaliza√ß√£o da penalidade Lasso.

```mermaid
graph LR
    subgraph "Elastic Net Properties"
        direction TB
        A["L1 Penalty: Encourages Sparsity"]
        B["L2 Penalty: Encourages Shrinkage"]
        C["Elastic Net: Combines Sparsity & Shrinkage"]
        D["Variable Grouping"]
        E["Flexible Adjustment (Œ±)"]
        A --> C
        B --> C
        C --> D
        C --> E

    end
```

**Lemma 20:** *A penalidade el√°stica combina as propriedades de sele√ß√£o de vari√°veis da penalidade L1 com a propriedade de *shrinkage* da penalidade L2, oferecendo um controle mais flex√≠vel da complexidade do modelo*.

*Prova:* A combina√ß√£o linear das penalidades L1 e L2 permite que o modelo se beneficie tanto da esparsidade quanto do *shrinkage* de coeficientes. $\blacksquare$

**Corol√°rio 20:** *A escolha do par√¢metro $\alpha$ na penalidade el√°stica permite controlar o trade-off entre esparsidade e *shrinkage*, e assim adaptar o modelo √†s necessidades do problema.*

*Prova:* Valores maiores de $\alpha$ d√£o mais peso para a penalidade L1, levando a modelos mais esparsos, e valores menores d√£o mais peso para a penalidade L2, levando a modelos com maior *shrinkage*. $\blacksquare$

A regulariza√ß√£o el√°stica fornece uma abordagem geral para a regulariza√ß√£o de modelos log√≠sticos e √© uma ferramenta poderosa na constru√ß√£o de modelos robustos e interpret√°veis.

### Conex√£o com o Generalized Lasso e Condi√ß√µes de KKT

```mermaid
graph LR
    subgraph "KKT Conditions for Lasso"
        direction TB
        A["Problem Formulation: Lasso with Constraints"]
        B["KKT Viability Condition"]
         C["KKT Stationarity Condition: $\frac{\partial \ell}{\partial \beta_j} - \lambda \nu_j = 0$"]
         D["KKT Complementary Slackness Condition: $\beta_j \cdot (\frac{\partial \ell}{\partial \beta_j}  - \lambda \nu_j) = 0$"]
        A --> B
        A --> C
        A --> D
    end
    subgraph "Generalized Lasso"
        E["KKT Conditions Generalization"]
         F["Generalized Penalties"]
         E --> F
    end
   C & D --> G["Parameter Optimality for Regularized Models"]
   F --> G
```

A **conex√£o com o *Generalized Lasso* e as condi√ß√µes de Karush-Kuhn-Tucker (KKT)** oferece um arcabou√ßo formal para entender o problema de otimiza√ß√£o da regress√£o log√≠stica com penalidades como o Lasso e o *Elastic Net*, e como essas penalidades influenciam os par√¢metros do modelo [^4.4.4]. As condi√ß√µes de KKT s√£o um conjunto de condi√ß√µes necess√°rias para a otimalidade em problemas de otimiza√ß√£o convexa com restri√ß√µes.

No caso da regress√£o log√≠stica com penalidade Lasso, o problema de otimiza√ß√£o pode ser visto como um problema de otimiza√ß√£o com restri√ß√µes de desigualdade, onde as restri√ß√µes s√£o dadas pelas penalidades. As condi√ß√µes de KKT estabelecem um conjunto de condi√ß√µes que devem ser satisfeitas na solu√ß√£o √≥tima do problema.

Para ilustrar, vamos considerar a regress√£o log√≠stica com penalidade Lasso:

$$
    \max_{\beta_0, \beta} \left\{ \sum_{i=1}^N \left[ y_i (\beta_0 + \beta^T x_i) - \log(1 + e^{\beta_0 + \beta^T x_i})\right] - \lambda \sum_{j=1}^p |\beta_j| \right\}
$$

As condi√ß√µes de KKT para esse problema incluem:

1.  **Viabilidade:** As restri√ß√µes devem ser satisfeitas. Nesse caso, como a penalidade L1 √© expressa como uma soma de valores absolutos, a viabilidade implica que os par√¢metros $\beta_j$ s√£o finitos.

2.  **Estacionariedade:** As derivadas parciais da fun√ß√£o lagrangiana em rela√ß√£o aos par√¢metros devem ser iguais a zero. A fun√ß√£o lagrangiana √© constru√≠da incluindo a fun√ß√£o objetivo e as restri√ß√µes com multiplicadores de Lagrange.

3.  **Folga Complementar:** O produto dos multiplicadores de Lagrange com as restri√ß√µes deve ser igual a zero. Essa condi√ß√£o indica que as restri√ß√µes ativas (onde a desigualdade se torna igualdade) t√™m multiplicadores de Lagrange diferentes de zero, e as restri√ß√µes n√£o ativas t√™m multiplicadores iguais a zero.

As condi√ß√µes de KKT, aplicadas ao problema da regress√£o log√≠stica com penalidade Lasso, podem ser expressas de forma mais formal:

*   Para cada coeficiente $\beta_j$, a condi√ß√£o de estacionariedade leva a:

    $$
      \frac{\partial \ell}{\partial \beta_j}  - \lambda \nu_j = 0
    $$

    onde $\frac{\partial \ell}{\partial \beta_j}$ √© a derivada da log-verossimilhan√ßa sem regulariza√ß√£o, e $\nu_j \in [-1,1]$. O termo $\lambda \nu_j$ representa o efeito da penalidade L1.

*   A condi√ß√£o de folga complementar implica:

    $$
      \beta_j \cdot (\frac{\partial \ell}{\partial \beta_j}  - \lambda \nu_j) = 0
    $$

    onde $\beta_j \neq 0$ implica que $\frac{\partial \ell}{\partial \beta_j} = \lambda \nu_j$ (a derivada da log-verossimilhan√ßa se iguala a um m√∫ltiplo da penalidade) e que $\beta_j = 0$ implica que $|\frac{\partial \ell}{\partial \beta_j}| \leq \lambda$.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar novamente o exemplo anterior. Para simplificar, vamos analisar apenas o coeficiente $\beta_1$.
>
> Sem regulariza√ß√£o, t√≠nhamos $\frac{\partial \ell}{\partial \beta_1} = 1.2$.
>
> Com a penalidade Lasso e $\lambda = 0.7$, a condi√ß√£o de estacionariedade √©:
>
> $1.2 - 0.7 \cdot \nu_1 = 0$
>
>  onde $\nu_1 \in [-1, 1]$.
>
>  Isso implica que $\nu_1 = \frac{1.2}{0.7} = 1.71$. Como $\nu_1$ deve estar entre -1 e 1, isso significa que o coeficiente n√£o ser√° zerado.
>
> A condi√ß√£o de folga complementar implica que:
>
> $\beta_1 \cdot (1.2 - 0.7 \cdot \nu_1) = 0$
>
> Como $\beta_1$ √© diferente de 0, ent√£o $1.2 - 0.7 \cdot \nu_1 = 0$ e $\nu_1$ estar√° no limite do intervalo, ou seja $\nu_1 = sign(\beta_1)$.
>
> Se $\lambda = 2.0$, a condi√ß√£o de estacionariedade seria:
>
> $1.2 - 2.0 \cdot \nu_1 = 0$
>
>  Isso implica que $\nu_1 = \frac{1.2}{2.0} = 0.6$, que est√° dentro do intervalo [-1,1]. O coeficiente n√£o √© zerado.
>
>
> Se, por outro lado, $\frac{\partial \ell}{\partial \beta_1} = 0.5$ e $\lambda = 0.7$:
>
> $0.5 - 0.7 \cdot \nu_1 = 0$
>
>  Isso implica que $\nu_1 = \frac{0.5}{0.7} = 0.71$, que est√° dentro do intervalo [-1,1]. O coeficiente n√£o √© zerado.
>
> Se $\frac{\partial \ell}{\partial \beta_1} = 0.2$ e $\lambda = 0.7$:
>
> $0.2 - 0.7 \cdot \nu_1 = 0$
>
>  Isso implica que $\nu_1 = \frac{0.2}{0.7} = 0.28$, que est√° dentro do intervalo [-1,1]. O coeficiente n√£o √© zerado.
>
> Se $\frac{\partial \ell}{\partial \beta_1} = 0$ e $\lambda = 0.7$:
>
> $0 - 0.7 \cdot \nu_1 = 0$
>
>  Isso implica que $\nu_1 = 0$, que est√° dentro do intervalo [-1,1]. O coeficiente √© zerado.
>
> Este exemplo ilustra como as condi√ß√µes de KKT formalizam a rela√ß√£o entre o score, o coeficiente e o par√¢metro de regulariza√ß√£o $\lambda$.

Essas condi√ß√µes revelam que, no √≥timo, as vari√°veis com coeficientes diferentes de zero satisfazem uma condi√ß√£o de equil√≠brio entre a verossimilhan√ßa e a penalidade, enquanto as vari√°veis com coeficientes iguais a zero n√£o contribuem com a verossimilhan√ßa.

A conex√£o com o **Generalized Lasso** surge ao perceber que as condi√ß√µes de KKT podem ser utilizadas para derivar solu√ß√µes para modelos com penalidades mais gerais. O *Generalized Lasso* permite a utiliza√ß√£o de diferentes penalidades, incluindo a penalidade el√°stica, e as condi√ß√µes de KKT podem ser adaptadas a cada caso.

As condi√ß√µes de KKT s√£o, portanto, um instrumento valioso para entender e derivar algoritmos de otimiza√ß√£o para modelos log√≠sticos regularizados e para analisar as propriedades de seus estimadores.

**Lemma 21:** *As condi√ß√µes de Karush-Kuhn-Tucker (KKT) fornecem um conjunto de condi√ß√µes necess√°rias que devem ser satisfeitas no √≥timo de problemas de otimiza√ß√£o convexa com restri√ß√µes, incluindo o caso da regress√£o log√≠stica com penalidade L1 e L2*.

*Prova:* As condi√ß√µes de KKT derivam das condi√ß√µes de otimalidade para problemas de otimiza√ß√£o com restri√ß√µes de desigualdade. [^4.4.4] $\blacksquare$

**Corol√°rio 21:** *As condi√ß√µes de KKT podem ser usadas para generalizar a penalidade Lasso a outras penalidades, como o Elastic Net, derivando algoritmos de otimiz
