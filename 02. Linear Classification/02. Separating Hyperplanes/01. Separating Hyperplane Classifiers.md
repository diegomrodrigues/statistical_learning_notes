## Separating Hyperplane Classifiers

### Introdução
Este capítulo explora os classificadores de **hiperplano separador**, que constroem fronteiras de decisão lineares que explicitamente tentam separar os dados em diferentes classes da melhor forma possível, fornecendo a base para os classificadores de vetores de suporte [^1]. Em continuidade ao que foi discutido no Capítulo 4, onde métodos lineares para classificação foram introduzidos, a presente seção aprofunda-se em abordagens que modelam explicitamente as fronteiras entre as classes como lineares [^2].

### Conceitos Fundamentais
Dado um conjunto de pontos de dados, o objetivo é encontrar um **hiperplano** $$f(x) = \beta_0 + \beta^T x = 0$$ que separe as classes [^1]. A distância com sinal de qualquer ponto *x* ao hiperplano é proporcional a $$f(x)$$. Matematicamente, para quaisquer dois pontos *x1* e *x2* pertencentes a *L* (o hiperplano), temos:

$$\beta^T (x_1 - x_2) = 0$$

Portanto, $$\beta^* = \frac{\beta}{||\beta||}$$ é o vetor normal à superfície de *L* [^1].

**Propriedades Importantes:**

1.  Para quaisquer dois pontos *x1* e *x2* no hiperplano *L*, o produto escalar de $$\beta$$ com a diferença entre esses pontos é zero, indicando que $$\beta$$ é ortogonal a qualquer vetor no hiperplano [^1].

2.  O vetor normalizado $$\beta^*$$ fornece a direção normalizada para o hiperplano, crucial para calcular distâncias e projeções [^1].

3.  A distância com sinal de um ponto *x* ao hiperplano é dada por:

$$distância(x, L) = \frac{1}{||\beta||} (\beta^T x + \beta_0)$$

Essa distância indica não apenas a proximidade do ponto ao hiperplano, mas também de que lado do hiperplano o ponto está localizado [^1].

**Algoritmo de Aprendizagem do Perceptron de Rosenblatt**

O algoritmo de aprendizagem do Perceptron tenta encontrar um hiperplano separador minimizando a distância dos pontos mal classificados à fronteira de decisão [^3]. Se uma resposta $$y_i = 1$$ é classificada incorretamente, então $$\beta^T x + \beta_0 < 0$$, e o oposto ocorre para uma resposta mal classificada com $$y_i = -1$$. O objetivo é minimizar:

$$D(\beta, \beta_0) = -\sum_{i \in M} y_i (\beta^T x_i + \beta_0)$$

onde *M* indexa o conjunto de pontos mal classificados. A quantidade é não negativa e proporcional à distância dos pontos mal classificados à fronteira de decisão definida por $$\beta^T x + \beta_0 = 0$$. O gradiente (assumindo que *M* seja fixo) é dado por:

$$\frac{\partial D(\beta, \beta_0)}{\partial \beta} = -\sum_{i \in M} y_i x_i$$

$$\frac{\partial D(\beta, \beta_0)}{\partial \beta_0} = -\sum_{i \in M} y_i$$

O algoritmo usa o método do gradiente descendente estocástico para minimizar este critério linear por partes. Isso significa que, em vez de calcular a soma das contribuições do gradiente de cada observação seguida por um passo na direção do gradiente negativo, um passo é dado após cada observação ser visitada. Portanto, as observações mal classificadas são visitadas em alguma sequência, e os parâmetros $$\beta$$ são atualizados via:

$$\begin{pmatrix} \beta \\\\ \beta_0 \end{pmatrix} \leftarrow \begin{pmatrix} \beta \\\\ \beta_0 \end{pmatrix} + \rho \begin{pmatrix} y_i x_i \\\\ y_i \end{pmatrix}$$

Aqui, $$\rho$$ é a taxa de aprendizagem, que neste caso pode ser tomada como 1 sem perda de generalidade [^3].

**Problemas com o Algoritmo do Perceptron:**

1.  **Multiplicidade de Soluções:** Quando os dados são separáveis, existem muitas soluções, e qual é encontrada depende dos valores iniciais [^3].

2.  **Número de Passos:** O número "finito" de passos pode ser muito grande. Quanto menor a folga, maior o tempo para encontrá-la [^3].

3.  **Não Convergência:** Quando os dados não são separáveis, o algoritmo não converge e os ciclos se desenvolvem. Os ciclos podem ser longos e, portanto, difíceis de detectar [^3].

### Conclusão
Os classificadores de hiperplano separador, embora simples em conceito, são poderosos na prática. A escolha do hiperplano ideal, como discutido, pode ser abordada através de várias técnicas, incluindo a maximização da margem, levando a classificadores de vetores de suporte. A compreensão das propriedades dos hiperplanos e os algoritmos para encontrá-los fornece uma base sólida para métodos de classificação mais avançados [^1].

### Referências
[^1]: Page 101, Section 4
[^2]: Page 102, Section 4
[^3]: Page 131, Section 4
<!-- END -->