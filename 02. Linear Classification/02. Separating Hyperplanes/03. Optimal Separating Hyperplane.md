## Hiperplanos de Separação Ótimos

### Introdução
Em continuidade ao estudo de métodos lineares para classificação [^1], este capítulo explora os hiperplanos de separação, com um foco especial nos hiperplanos de separação ótimos. Como vimos anteriormente, tanto a análise discriminante linear (LDA) quanto a regressão logística estimam fronteiras de decisão lineares de maneiras similares, embora ligeiramente diferentes [^29]. Este capítulo aprofunda-se em classificadores que explicitamente constroem fronteiras de decisão lineares para separar os dados em diferentes classes da melhor maneira possível [^29]. Essas técnicas fornecem a base para classificadores de vetores de suporte, que serão discutidos no Capítulo 12 [^29].

### Conceitos Fundamentais
O **hiperplano de separação ótimo** separa as duas classes e maximiza a distância até o ponto mais próximo de qualquer uma das classes [^32]. Essa propriedade resulta em uma solução única para o problema do hiperplano de separação [^32]. A obtenção desse hiperplano é formulada como um problema de otimização convexa, onde o objetivo é maximizar a margem entre as duas classes nos dados de treinamento, o que leva a um melhor desempenho de classificação nos dados de teste [^32].

Formalmente, o hiperplano de separação ótimo é encontrado maximizando $M$ sujeito a $y_i (x_i^T \beta + \beta_0) \geq M$ e $||\beta|| = 1$ [^32]. Essa condição garante que todos os pontos estejam a uma distância assinada de pelo menos $M$ da fronteira de decisão [^32]. Equivalentemente, o problema pode ser reformulado como a minimização de $\frac{1}{2} ||\beta||^2$ sujeito a $y_i (x_i^T \beta + \beta_0) \geq 1$ [^32], um problema de otimização convexa que maximiza a espessura da margem.

**Demonstração da Equivalência:**

A equivalência entre maximizar $M$ sujeito a $y_i (x_i^T \beta + \beta_0) \geq M$ e $||\beta|| = 1$ e minimizar $\frac{1}{2} ||\beta||^2$ sujeito a $y_i (x_i^T \beta + \beta_0) \geq 1$ pode ser demonstrada da seguinte forma:

1.  **Maximizar** $M$ sujeito a $y_i (x_i^T \beta + \beta_0) \geq M$ e $||\beta|| = 1$ significa encontrar o hiperplano (definido por $\beta$ e $\beta_0$) que garante que todos os pontos estejam a uma distância de pelo menos $M$ da fronteira de decisão, e que $||\beta|| = 1$.

2.  Seja $\beta' = \frac{\beta}{M}$ e $\beta'_0 = \frac{\beta_0}{M}$. Substituindo esses valores nas restrições originais, temos:\
    $y_i(x_i^T \frac{\beta'}{M} + \frac{\beta'_0}{M}) \geq M$\
    $y_i(x_i^T \beta' + \beta'_0) \geq M^2$

3.  Dividindo ambos os lados por M, obtemos:\
    $y_i(x_i^T \beta' + \beta'_0) \geq 1$

4.  Agora, consideremos a norma de $\beta'$:\
    $||\beta'|| = ||\frac{\beta}{M}|| = \frac{||\beta||}{M} = \frac{1}{M}$ (já que $||\beta|| = 1$)

5.  Minimizar $\frac{1}{2} ||\beta||^2$ é equivalente a maximizar $\frac{1}{M^2}$, que é equivalente a maximizar $M$.

6.  Portanto, minimizar $\frac{1}{2} ||\beta'||^2$ sujeito a $y_i (x_i^T \beta' + \beta'_0) \geq 1$ é o mesmo que maximizar $M$ sujeito às restrições originais.

Assim, os dois problemas de otimização são equivalentes. $\blacksquare$

Para resolver este problema de otimização, pode-se utilizar a função Lagrange (primal) para ser minimizada com relação a $\beta$ e $\beta_0$ [^33]:

$$L_p = \frac{1}{2} ||\beta||^2 - \sum_{i=1}^N \alpha_i [y_i (x_i^T \beta + \beta_0) - 1]$$

onde $\alpha_i$ são os multiplicadores de Lagrange [^33].

Ao definir as derivadas para zero, obtemos [^33]:

$$\beta = \sum_{i=1}^N \alpha_i y_i x_i$$

$$0 = \sum_{i=1}^N \alpha_i y_i$$

Substituindo estes na equação Lagrangiana, obtemos o chamado dual de Wolfe [^33]:

$$L_D = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i^T x_k$$

sujeito a $\alpha_i \geq 0$ [^33].

A solução é obtida ao maximizar $L_D$ no ortante positivo, um problema de otimização convexo mais simples, para o qual pode-se usar software padrão [^33]. Além disso, a solução deve satisfazer as condições de Karush-Kuhn-Tucker, que incluem [^33]:

$$\alpha_i [y_i (x_i^T \beta + \beta_0) - 1] = 0 \quad \forall i$$

A partir disso, podemos observar que [^33]:

*   Se $\alpha_i > 0$, então $y_i (x_i^T \beta + \beta_0) = 1$, ou seja, $x_i$ está na fronteira da margem.
*   Se $y_i (x_i^T \beta + \beta_0) > 1$, então $x_i$ não está na fronteira da margem e $\alpha_i = 0$.

A partir da equação $\beta = \sum_{i=1}^N \alpha_i y_i x_i$, vemos que o vetor solução $\beta$ é definido em termos de uma combinação linear dos pontos de suporte $x_i$ — aqueles pontos definidos como estando na fronteira da margem via $\alpha_i > 0$ [^33].

### Conclusão
O hiperplano de separação ótimo oferece uma abordagem robusta e bem fundamentada para problemas de classificação linear [^32]. Ao maximizar a margem entre as classes, o hiperplano de separação ótimo não apenas separa os dados de treinamento, mas também promove uma melhor generalização para dados não vistos [^32]. A formulação do problema como uma otimização convexa garante uma solução única e eficiente [^32]. Embora nenhum dos dados de treinamento caia na margem (por construção), este não será necessariamente o caso para observações de teste [^33]. A intuição é que uma margem grande nos dados de treinamento levará a uma boa separação nos dados de teste [^34].

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer Science & Business Media.
[^29]: Seção 4.5 Separating Hyperplanes, Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer Science & Business Media.
[^32]: Seção 4.5.2 Optimal Separating Hyperplanes, Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer Science & Business Media.
[^33]: Seção 4.5.2 Optimal Separating Hyperplanes, Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer Science & Business Media.
[^34]: Seção 4.5.2 Optimal Separating Hyperplanes, Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer Science & Business Media.
<!-- END -->