## Rosenblatt's Perceptron Learning Algorithm: Uma Busca por Hiperplanos Separadores

### Introdução
Este capítulo revisita o problema de classificação, focando em métodos lineares para classificação. Como vimos anteriormente, a busca por hiperplanos separadores é uma abordagem fundamental em problemas de classificação [^1, ^2]. O algoritmo do perceptron de Rosenblatt, que será detalhado neste capítulo, tenta encontrar um hiperplano separador minimizando a distância dos pontos mal classificados até a fronteira de decisão [^30]. Ele utiliza o método do gradiente descendente estocástico para minimizar um critério linear por partes, atualizando os parâmetros $\beta$ após cada observação mal classificada ser visitada [^30]. Este capítulo se aprofundará no algoritmo, suas propriedades, e suas limitações.

### Conceitos Fundamentais

O algoritmo do perceptron de Rosenblatt é uma abordagem iterativa para encontrar um hiperplano separador em um conjunto de dados linearmente separável. A ideia central é ajustar iterativamente os pesos do hiperplano com base nos erros de classificação encontrados durante o treinamento [^30].

O algoritmo funciona da seguinte maneira [^30]:

1.  **Inicialização:** Inicializa-se um vetor de pesos $\beta$ aleatoriamente.
2.  **Iteração:** Para cada ponto de dados $(x_i, y_i)$ no conjunto de treinamento:
    *   Se o ponto é mal classificado (isto é, $y_i(\beta^T x_i) < 0$, onde $y_i$ é a classe verdadeira, codificada como +1 ou -1), então os pesos são atualizados:
        $$\beta \leftarrow \beta + \rho y_i x_i$$
        onde $\rho$ é a taxa de aprendizado (learning rate).
3.  **Convergência:** O algoritmo continua iterando sobre o conjunto de treinamento até que todos os pontos sejam corretamente classificados, ou até que um número máximo de iterações seja atingido.

O critério que o algoritmo tenta minimizar é a distância total dos pontos mal classificados à fronteira de decisão [^30]:

$$D(\beta) = - \sum_{i \in M} y_i (\beta^T x_i)$$

onde $M$ é o conjunto de pontos mal classificados. O gradiente deste critério é [^31]:

$$\frac{\partial D(\beta)}{\partial \beta} = - \sum_{i \in M} y_i x_i$$

O algoritmo do perceptron utiliza o gradiente descendente *estocástico* [^31], o que significa que ele atualiza os pesos após cada observação mal classificada, em vez de calcular o gradiente sobre todo o conjunto de dados. A regra de atualização é [^31]:

$$\beta \leftarrow \beta + \rho y_i x_i$$

onde $\rho$ é a taxa de aprendizado. No caso do perceptron, $\rho$ pode ser tomado como 1 sem perda de generalidade [^31].

**Observações Importantes:**

*   **Convergência:** Se os dados são linearmente separáveis, o algoritmo do perceptron converge para um hiperplano separador em um número finito de passos [^31].
*   **Múltiplas Soluções:** Se os dados são linearmente separáveis, existem múltiplas soluções (hiperplanos separadores), e a solução encontrada depende da inicialização aleatória dos pesos e da ordem em que os pontos de dados são apresentados [^31].
*   **Dados Não Separáveis:** Se os dados não são linearmente separáveis, o algoritmo não converge e pode ciclar indefinidamente [^31].

**Limitações:**

O algoritmo do perceptron tem algumas limitações [^31]:

*   **Sensibilidade à Inicialização:** A solução final depende da inicialização dos pesos.
*   **Dados Não Separáveis:** Não converge se os dados não são linearmente separáveis.
*   **Número de Passos:** O número de passos para convergência pode ser muito grande, especialmente se a "folga" (gap) entre as classes é pequena.

### Conclusão
O algoritmo do perceptron de Rosenblatt é um método simples e intuitivo para encontrar hiperplanos separadores. Embora tenha limitações, como a necessidade de dados linearmente separáveis e a sensibilidade à inicialização, ele serve como um bloco de construção fundamental para modelos mais avançados, como as Support Vector Machines (SVMs) [^31]. O conceito de minimizar a distância dos pontos mal classificados à fronteira de decisão é central em muitos algoritmos de classificação.

<!-- END -->