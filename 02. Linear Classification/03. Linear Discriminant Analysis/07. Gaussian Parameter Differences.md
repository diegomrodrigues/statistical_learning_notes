## T√≠tulo Conciso: Classifica√ß√£o Linear, Interceptos e a Influ√™ncia da Suposi√ß√£o Gaussiana

```mermaid
graph LR
    subgraph "Linear Classification Models"
        direction TB
        A["Linear Discriminant Analysis (LDA)"]
        B["Logistic Regression"]
        C["Linear Regression with Indicator Matrices"]
    end
    D["Intercept Estimation"]
    E["Gaussian Assumption Influence"]
    F["Variable Selection & Regularization"]
    G["Separating Hyperplanes"]

    A --> D
    B --> D
    C --> D
    D --> E
    E --> F
    F --> G
```

### Introdu√ß√£o

Este cap√≠tulo aprofunda a an√°lise da **classifica√ß√£o linear**, focando nas diferen√ßas na determina√ß√£o dos **interceptos** ou "cut-points" nos modelos, e como a suposi√ß√£o **gaussiana** influencia essa estimativa, particularmente no **Linear Discriminant Analysis (LDA)** [^4.3]. Analisaremos como o LDA, a **regress√£o log√≠stica** e a **regress√£o linear com matrizes de indicadores** abordam a quest√£o do intercepto (ou cut-point) e como cada abordagem se conecta com a tomada de decis√£o em modelos lineares. Examinaremos as suposi√ß√µes por tr√°s de cada m√©todo e suas implica√ß√µes para a estimativa dos interceptos. Discutiremos tamb√©m a relev√¢ncia da **sele√ß√£o de vari√°veis e regulariza√ß√£o** para a constru√ß√£o de modelos mais robustos e com melhor capacidade de generaliza√ß√£o [^4.4.4], [^4.5]. Abordaremos ainda o papel dos **hiperplanos separadores** e como o intercepto afeta a posi√ß√£o desses hiperplanos [^4.5.2]. O objetivo deste cap√≠tulo √© fornecer uma compreens√£o detalhada das diferentes abordagens para a estimativa dos interceptos e como essas escolhas afetam a capacidade de classifica√ß√£o dos modelos lineares.

### Conceitos Fundamentais

**Conceito 1: O Papel do Intercepto (Cut-Point) na Classifica√ß√£o Linear**

Em modelos de classifica√ß√£o linear, o **intercepto** ($\beta_0$ na forma geral da fun√ß√£o discriminante $f(x) = \beta_0 + \beta^T x$) ou *cut-point*, define a posi√ß√£o da fronteira de decis√£o linear no espa√ßo de caracter√≠sticas [^4.1]. O intercepto, ou *cut-point*, representa o ponto de interse√ß√£o do hiperplano com o eixo vertical (quando a fun√ß√£o de decis√£o √© representada graficamente). O intercepto √© crucial para definir o "lado" da fronteira de decis√£o onde uma observa√ß√£o √© classificada, e sua estimativa √© um passo fundamental na constru√ß√£o de modelos de classifica√ß√£o. Apesar de a dire√ß√£o da fronteira de decis√£o (definida por $\beta$) ser, em alguns casos, proporcional entre diferentes m√©todos, o intercepto (ou cut-point) pode variar, dependendo das suposi√ß√µes e dos m√©todos de estima√ß√£o.

```mermaid
graph LR
    subgraph "Linear Discriminant Function"
        direction LR
        A["f(x) = Œ≤‚ÇÄ + Œ≤·µÄx"] --> B["Decision Boundary: f(x) = 0"]
        B --> C["Intercept Œ≤‚ÇÄ"]
        C --> D["Position of Boundary"]
    end
```

**Lemma 1:** *Em modelos lineares de classifica√ß√£o, o intercepto define a posi√ß√£o do hiperplano de decis√£o no espa√ßo de caracter√≠sticas, afetando a separa√ß√£o entre as classes.*

> üí° **Exemplo Num√©rico:**
>
> Imagine um cen√°rio de classifica√ß√£o bin√°ria com uma √∫nica caracter√≠stica $x$. A fun√ß√£o discriminante linear √© dada por $f(x) = \beta_0 + \beta_1 x$. Se $\beta_0 = -2$ e $\beta_1 = 1$, a fronteira de decis√£o √© o ponto onde $f(x) = 0$, ou seja, $-2 + x = 0$, o que implica $x = 2$.  O intercepto $\beta_0 = -2$ define que todos os pontos com $x < 2$ ser√£o classificados como uma classe e os pontos com $x > 2$ como a outra.  Se $\beta_0$ fosse alterado para 0, a fronteira de decis√£o seria $x = 0$, alterando a classifica√ß√£o de todos os pontos no intervalo $0 < x < 2$.
>
> Se tivermos dois pontos, $x_1=1$ e $x_2=3$, com $\beta_1 = 1$, temos:
>
> Para $\beta_0 = -2$, $f(x_1) = -2 + 1 = -1$ (classe 0), $f(x_2) = -2 + 3 = 1$ (classe 1).
> Para $\beta_0 = 0$, $f(x_1) = 0 + 1 = 1$ (classe 1), $f(x_2) = 0 + 3 = 3$ (classe 1).
>
> Este exemplo ilustra como o intercepto desloca a fronteira de decis√£o e afeta a classifica√ß√£o.

**Conceito 2: LDA e a Estima√ß√£o do Intercepto**

No **LDA**, o intercepto √© determinado pelas probabilidades a priori das classes e pelas m√©dias de cada classe, e tamb√©m utiliza informa√ß√µes da matriz de covari√¢ncia compartilhada [^4.3]. Ao analisar a express√£o do log-ratio das probabilidades posteriores no LDA, observamos que o intercepto na fun√ß√£o discriminante linear para uma classe $k$ √© dado por:

$$
-\frac{1}{2}\mu_k^T \Sigma^{-1} \mu_k + \log \pi_k
$$

A presen√ßa de $\mu_k$, $\Sigma$ e $\pi_k$ mostra que o intercepto √© influenciado pela distribui√ß√£o dos dados e pelas probabilidades a priori das classes, que s√£o estimadas a partir dos dados de treinamento. A suposi√ß√£o de gaussianidade com covari√¢ncias iguais √© utilizada para obter uma express√£o que define o intercepto da fun√ß√£o discriminante do LDA [^4.3].

```mermaid
graph LR
    subgraph "LDA Intercept Derivation"
        direction TB
        A["Intercept: -1/2 * Œº‚Çñ·µÄŒ£‚Åª¬πŒº‚Çñ + log(œÄ‚Çñ)"]
        B["Mean Vector Œº‚Çñ"]
        C["Covariance Matrix Œ£"]
        D["Prior Probability œÄ‚Çñ"]
        A --> B
        A --> C
        A --> D
    end
```

**Corol√°rio 1:** *O intercepto da fun√ß√£o discriminante do LDA √© influenciado tanto pela estrutura dos dados (m√©dias e covari√¢ncia) quanto pelas probabilidades a priori das classes, e a suposi√ß√£o gaussiana √© fundamental para a sua deriva√ß√£o.*

> üí° **Exemplo Num√©rico:**
>
> Suponha um problema de classifica√ß√£o com duas classes, onde a classe 1 tem m√©dia $\mu_1 = [1, 1]^T$ e a classe 2 tem m√©dia $\mu_2 = [3, 3]^T$. A matriz de covari√¢ncia compartilhada √© $\Sigma = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$. As probabilidades a priori s√£o $\pi_1 = 0.6$ e $\pi_2 = 0.4$.
>
> Para a classe 1, o intercepto √©:
>
> $ \text{Intercepto}_1 = -\frac{1}{2}\mu_1^T \Sigma^{-1} \mu_1 + \log \pi_1 $
>
> $ \text{Intercepto}_1 = -\frac{1}{2} [1, 1] \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}^{-1} [1, 1]^T + \log(0.6) $
>
> $ \text{Intercepto}_1 = -\frac{1}{2} [1, 1] [1, 1]^T + \log(0.6) $
>
> $ \text{Intercepto}_1 = -\frac{1}{2} (1 + 1) + \log(0.6) $
>
> $ \text{Intercepto}_1 = -1 + \log(0.6) \approx -1 - 0.51 = -1.51 $
>
> Para a classe 2, o intercepto √©:
>
> $ \text{Intercepto}_2 = -\frac{1}{2}\mu_2^T \Sigma^{-1} \mu_2 + \log \pi_2 $
>
> $ \text{Intercepto}_2 = -\frac{1}{2} [3, 3] \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}^{-1} [3, 3]^T + \log(0.4) $
>
> $ \text{Intercepto}_2 = -\frac{1}{2} [3, 3] [3, 3]^T + \log(0.4) $
>
> $ \text{Intercepto}_2 = -\frac{1}{2} (9 + 9) + \log(0.4) $
>
> $ \text{Intercepto}_2 = -9 + \log(0.4) \approx -9 - 0.92 = -9.92 $
>
> A diferen√ßa entre os interceptos √© o que define a posi√ß√£o da fronteira de decis√£o, considerando as m√©dias, covari√¢ncias e probabilidades a priori das classes.

**Conceito 3: Regress√£o Log√≠stica e a Estima√ß√£o do Intercepto**

Na **regress√£o log√≠stica**, o intercepto $\beta_0$ √© determinado atrav√©s da maximiza√ß√£o da verossimilhan√ßa do modelo, e √© um par√¢metro livre que n√£o √© diretamente influenciado pela suposi√ß√£o gaussiana [^4.4.1]. O intercepto da regress√£o log√≠stica √© um par√¢metro a ser estimado diretamente a partir dos dados de treinamento, e ajusta a posi√ß√£o da curva log√≠stica, e a fronteira de decis√£o resultante, para a classifica√ß√£o. A regress√£o log√≠stica n√£o imp√µe nenhuma forma espec√≠fica para a distribui√ß√£o dos dados, como o LDA, o que significa que o intercepto e o vetor de coeficientes podem ser estimados de forma mais flex√≠vel [^4.4.2].

```mermaid
graph LR
    subgraph "Logistic Regression Intercept"
        direction TB
        A["Intercept Œ≤‚ÇÄ: Parameter to be Estimated"]
        B["Maximization of Likelihood"]
        C["Adjusts Logistic Curve Position"]
        A --> B
        B --> C
    end
```

> ‚ö†Ô∏è **Nota Importante**: O intercepto na regress√£o log√≠stica √© um par√¢metro livre que √© estimado diretamente a partir dos dados, ao contr√°rio do LDA, onde o intercepto √© derivado da suposi√ß√£o gaussiana.

> ‚ùó **Ponto de Aten√ß√£o**: Em problemas de classifica√ß√£o com classes desbalanceadas, a estimativa do intercepto na regress√£o log√≠stica pode ser afetada pela distribui√ß√£o das classes, necessitando da aplica√ß√£o de t√©cnicas de repondera√ß√£o ou sobreamostragem para obter uma estima√ß√£o mais adequada.

> ‚úîÔ∏è **Destaque**: A abordagem para a estimativa do intercepto √© um ponto crucial de distin√ß√£o entre os m√©todos de classifica√ß√£o linear, como o LDA e a regress√£o log√≠stica.

> üí° **Exemplo Num√©rico:**
>
> Considere um modelo de regress√£o log√≠stica com uma √∫nica caracter√≠stica $x$. A probabilidade de pertencer √† classe 1 √© modelada como:
>
> $P(Y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}$
>
> Suponha que, ap√≥s o treinamento, os par√¢metros estimados sejam $\beta_0 = -1$ e $\beta_1 = 2$. O intercepto $\beta_0 = -1$ afeta diretamente a probabilidade base de pertencer √† classe 1 quando $x=0$.
>
> Para $x = 0$, $P(Y=1|x=0) = \frac{1}{1 + e^{-(-1 + 2 \cdot 0)}} = \frac{1}{1 + e^{1}} \approx 0.27$.
>
> A fronteira de decis√£o √© encontrada quando $P(Y=1|x) = 0.5$. Isso ocorre quando $\beta_0 + \beta_1 x = 0$, logo, $x = -\frac{\beta_0}{\beta_1} = -\frac{-1}{2} = 0.5$.
>
> Se o intercepto fosse $\beta_0 = 1$, a fronteira de decis√£o mudaria para $x = -\frac{1}{2} = -0.5$. Note que o intercepto $\beta_0$ ajusta a posi√ß√£o da curva log√≠stica e, consequentemente, da fronteira de decis√£o.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
    subgraph "Linear Regression vs LDA Intercept"
        direction TB
         A["Linear Regression Intercept"] --> B["Estimated by Least Squares"]
         C["LDA Intercept"] --> D["Derived from Gaussian Assumption"]
    end
```

Na **regress√£o linear com matrizes de indicadores**, o intercepto $\beta_{k0}$ √© determinado atrav√©s da minimiza√ß√£o da soma dos quadrados dos erros, juntamente com os coeficientes $\beta_k$, e s√£o todos os par√¢metros ajustados diretamente aos dados [^4.2]. Ao contr√°rio do LDA, a regress√£o linear n√£o imp√µe a suposi√ß√£o gaussiana sobre as distribui√ß√µes das classes, e o intercepto √© um par√¢metro livre que n√£o depende de forma expl√≠cita da probabilidade a priori das classes. No entanto, em algumas situa√ß√µes espec√≠ficas, como classes equiprov√°veis e covari√¢ncias esf√©ricas, o intercepto da regress√£o linear pode ser proporcional ao intercepto do LDA, a menos de uma constante de proporcionalidade.

Na regress√£o linear com matrizes de indicadores, a busca por um modelo linear que minimiza os erros tamb√©m define o valor do intercepto da fun√ß√£o discriminante, sem se utilizar de nenhuma informa√ß√£o sobre a distribui√ß√£o dos dados, como faz o LDA. Como consequ√™ncia, o intercepto na regress√£o linear √© um par√¢metro livre, que ser√° estimado para otimizar a separa√ß√£o das classes, sob o crit√©rio dos m√≠nimos quadrados.

Essa diferen√ßa na estimativa do intercepto pode ter implica√ß√µes no desempenho da regress√£o linear como classificador, particularmente em cen√°rios onde a suposi√ß√£o gaussiana e de covari√¢ncias iguais do LDA s√£o v√°lidas.

**Lemma 2:** *O intercepto na regress√£o linear com matrizes de indicadores √© um par√¢metro livre ajustado pelo m√©todo dos m√≠nimos quadrados, e n√£o depende diretamente das probabilidades a priori das classes ou da suposi√ß√£o gaussiana, como no LDA.*

**Corol√°rio 2:** *A estimativa do intercepto na regress√£o linear com matrizes de indicadores, ao contr√°rio do LDA, n√£o depende da suposi√ß√£o gaussiana e das probabilidades a priori das classes, o que torna o m√©todo mais flex√≠vel, mas tamb√©m mais vulner√°vel a problemas como o "masking", que surgem devido √† natureza da minimiza√ß√£o da soma de quadrados.*

A regress√£o linear com matrizes de indicadores, ao determinar o intercepto de forma independente da suposi√ß√£o gaussiana, difere dos modelos que utilizam essa premissa, como o LDA, e tamb√©m da regress√£o log√≠stica, que utiliza o conceito de verossimilhan√ßa para estimar todos os seus par√¢metros [^4.3], [^4.4].

> üí° **Exemplo Num√©rico:**
>
> Considere um problema de classifica√ß√£o bin√°ria com duas classes (0 e 1) e uma √∫nica caracter√≠stica $x$. A regress√£o linear com matrizes de indicadores modela a classe $y$ como $y = \beta_0 + \beta_1 x + \epsilon$. Suponha que ap√≥s ajustar o modelo aos dados, os par√¢metros estimados sejam $\beta_0 = 0.2$ e $\beta_1 = 0.8$.
>
> A fronteira de decis√£o √© encontrada quando a predi√ß√£o linear √© igual a 0.5 (o ponto m√©dio entre as classes 0 e 1). Assim, $0.5 = 0.2 + 0.8x$, o que resulta em $x = \frac{0.5 - 0.2}{0.8} = \frac{0.3}{0.8} = 0.375$.
>
> Se o intercepto fosse alterado para $\beta_0 = -0.1$, a fronteira de decis√£o mudaria para $0.5 = -0.1 + 0.8x$, resultando em $x = \frac{0.5 + 0.1}{0.8} = \frac{0.6}{0.8} = 0.75$.
>
> Este exemplo mostra como o intercepto $\beta_0$ afeta diretamente a posi√ß√£o da fronteira de decis√£o na regress√£o linear.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Regularization Impact on Intercept"
       direction TB
        A["Regularization (L1, L2)"] --> B["Penalty Term in Cost Function"]
        B --> C["Controls Magnitude of Coefficients"]
        C --> D["Affects Intercept Estimation"]
    end
```

A **sele√ß√£o de vari√°veis** e a **regulariza√ß√£o** desempenham um papel fundamental na melhoria da robustez dos modelos de classifica√ß√£o, impactando n√£o apenas os coeficientes, mas tamb√©m a estimativa do intercepto. A regulariza√ß√£o adiciona um termo de penalidade √† fun√ß√£o de custo, o que restringe a magnitude dos coeficientes e evita o *overfitting*, e tamb√©m influencia a estimativa do intercepto, embora em algumas formula√ß√µes o intercepto n√£o seja penalizado diretamente [^4.5].

Na **regress√£o log√≠stica**, a fun√ß√£o de custo regularizada pode ser expressa como:

$$
\max_{\beta_0, \beta} \left[ \sum_{i=1}^N \left( y_i (\beta_0 + \beta^T x_i) - \log(1 + e^{\beta_0 + \beta^T x_i}) \right) - \lambda P(\beta) \right]
$$

onde $P(\beta)$ √© a penalidade e $\lambda$ √© o par√¢metro de regulariza√ß√£o. A penalidade **L1** (Lasso), dada por $P(\beta) = \sum_{j=1}^p |\beta_j|$, promove a esparsidade dos coeficientes, selecionando as vari√°veis mais relevantes para a estima√ß√£o do intercepto e da probabilidade posterior. A penalidade **L2** (Ridge), dada por $P(\beta) = \sum_{j=1}^p \beta_j^2$, reduz a magnitude dos coeficientes, estabilizando o modelo e, indiretamente, a estimativa do intercepto [^4.4.4], [^4.5].

A aplica√ß√£o da regulariza√ß√£o ajuda a controlar a complexidade do modelo e a obter estimativas mais precisas dos par√¢metros, incluindo o intercepto, resultando em modelos mais robustos e generaliz√°veis.

**Lemma 3:** *A regulariza√ß√£o L1, ao promover a esparsidade, leva a estimativas do intercepto mais robustas, selecionando as vari√°veis mais relevantes para o ajuste do modelo.*

**Prova do Lemma 3:** A penalidade L1 adiciona um termo √† fun√ß√£o de custo que √© proporcional ao valor absoluto dos coeficientes, e a minimiza√ß√£o deste termo for√ßa alguns dos coeficientes a se tornarem exatamente zero durante o processo de otimiza√ß√£o. Esta sele√ß√£o de vari√°veis reduz o impacto do ru√≠do na estimativa do intercepto, e o torna mais robusto [^4.4.3], [^4.4.4]. $\blacksquare$

**Corol√°rio 3:** *A regulariza√ß√£o, tanto L1 quanto L2, contribui para melhorar a estimativa do intercepto e aumentar a estabilidade do modelo, melhorando a capacidade de generaliza√ß√£o da fronteira de decis√£o, mesmo que os m√©todos de estima√ß√£o de par√¢metros e interceptos sejam diferentes.*

> ‚ö†Ô∏è **Ponto Crucial**: A regulariza√ß√£o, ao controlar a complexidade dos modelos, impacta na estimativa do intercepto e melhora a qualidade das decis√µes, mesmo quando a deriva√ß√£o dos interceptos s√£o diferentes, e s√£o derivadas da suposi√ß√£o gaussiana ou da maximiza√ß√£o da verossimilhan√ßa, ou da minimiza√ß√£o da soma de quadrados [^4.5].

> üí° **Exemplo Num√©rico:**
>
> Vamos aplicar regulariza√ß√£o L1 (Lasso) em um modelo de regress√£o log√≠stica com duas caracter√≠sticas, $x_1$ e $x_2$. A fun√ß√£o de custo a ser maximizada √©:
>
> $ L(\beta_0, \beta_1, \beta_2) = \sum_{i=1}^N \left( y_i (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}) - \log(1 + e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}) \right) - \lambda (|\beta_1| + |\beta_2|) $
>
> Suponha que, sem regulariza√ß√£o ($\lambda = 0$), os par√¢metros estimados sejam $\beta_0 = -0.5$, $\beta_1 = 1.2$ e $\beta_2 = -0.8$.
>
> Ao aplicar regulariza√ß√£o L1 com $\lambda = 0.5$, o modelo pode convergir para novos valores, por exemplo, $\beta_0 = -0.4$, $\beta_1 = 0.9$ e $\beta_2 = 0$. A penalidade L1 for√ßou $\beta_2$ a ser zero, efetivamente removendo a vari√°vel $x_2$ do modelo. Isso altera o intercepto para -0.4 e simplifica a fronteira de decis√£o, tornando-a dependente apenas de $x_1$.
>
> Se usarmos regulariza√ß√£o L2, a fun√ß√£o de custo seria:
>
> $ L(\beta_0, \beta_1, \beta_2) = \sum_{i=1}^N \left( y_i (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}) - \log(1 + e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}) \right) - \lambda (\beta_1^2 + \beta_2^2) $
>
> Com $\lambda = 0.5$, o modelo poderia convergir para $\beta_0 = -0.45$, $\beta_1 = 0.95$ e $\beta_2 = -0.6$. A regulariza√ß√£o L2 reduz a magnitude de ambos os coeficientes, mas n√£o os for√ßa a zero. O intercepto tamb√©m √© afetado.
>
> A regulariza√ß√£o L1 promove esparsidade, enquanto a L2 promove a redu√ß√£o da magnitude dos coeficientes, ambas afetando o intercepto e a fronteira de decis√£o.

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
    subgraph "Separating Hyperplanes"
       direction TB
        A["Hyperplane Position: Determined by Œ≤‚ÇÄ"]
        B["Hyperplane Orientation: Determined by Œ≤"]
        C["Maximize Separation Margin"]
        A --> C
        B --> C
    end
```

A ideia de **hiperplanos separadores** busca encontrar uma fronteira linear que maximize a separa√ß√£o entre as classes, onde o intercepto ($\beta_0$) define a posi√ß√£o do hiperplano no espa√ßo de caracter√≠sticas e o vetor de coeficientes ($\beta$) define a orienta√ß√£o da fronteira [^4.5.2]. O objetivo √© encontrar o hiperplano √≥timo que maximize a margem de separa√ß√£o, ou seja, a dist√¢ncia entre o hiperplano e as amostras mais pr√≥ximas de cada classe.

O algoritmo do **Perceptron** √© um m√©todo iterativo que busca ajustar o hiperplano separador atrav√©s do ajuste iterativo do intercepto e dos coeficientes, com base nas classifica√ß√µes incorretas [^4.5.1]. Embora o Perceptron n√£o maximize a margem diretamente, ele ilustra como modelos lineares podem ser utilizados para encontrar uma solu√ß√£o que separa as classes, e, se o problema for linearmente separ√°vel, o algoritmo converge para um hiperplano separador, definindo, assim, o intercepto e a fronteira de decis√£o.

**Teorema:** *O algoritmo do Perceptron converge para um hiperplano separador em um n√∫mero finito de itera√ß√µes, se o conjunto de dados for linearmente separ√°vel.*

> üí° **Exemplo Num√©rico:**
>
> Considere um conjunto de dados linearmente separ√°vel com duas classes e duas caracter√≠sticas. Inicializamos os par√¢metros do Perceptron com $\beta_0 = 0$, $\beta_1 = 0$, e $\beta_2 = 0$.
>
> Suponha que a primeira amostra seja $x_i = [1, 2]^T$ e perten√ßa √† classe 1 ($y_i = 1$). A predi√ß√£o √© $f(x_i) = 0 + 0 \cdot 1 + 0 \cdot 2 = 0$. Como $f(x_i) < 0$, o ponto √© classificado incorretamente.
>
> Atualizamos os par√¢metros:
>
> $\beta_0 = \beta_0 + \eta y_i = 0 + 1 = 1$
>
> $\beta_1 = \beta_1 + \eta y_i x_{i1} = 0 + 1 \cdot 1 = 1$
>
> $\beta_2 = \beta_2 + \eta y_i x_{i2} = 0 + 1 \cdot 2 = 2$
>
> (onde $\eta$ √© a taxa de aprendizado, que assumimos como 1 para simplificar).
>
> O novo hiperplano √© definido por $1 + x_1 + 2x_2 = 0$. Se a pr√≥xima amostra for $x_j = [3, 1]^T$ e perten√ßa √† classe 0 ($y_j = 0$), a predi√ß√£o √© $f(x_j) = 1 + 1 \cdot 3 + 2 \cdot 1 = 6$. Como $f(x_j) > 0$, a predi√ß√£o est√° incorreta, e a atualiza√ß√£o agora ser√°:
>
> $\beta_0 = \beta_0 - \eta \cdot 1 = 1 - 1 = 0$
>
> $\beta_1 = \beta_1 - \eta x_{j1} = 1 - 3 = -2$
>
> $\beta_2 = \beta_2 - \eta x_{j2} = 2 - 1 = 1$
>
> O novo hiperplano √© definido por $0 - 2x_1 + x_2 = 0$.
>
> Este processo iterativo ajusta o intercepto e os coeficientes do hiperplano, e, se o problema for linearmente separ√°vel, o algoritmo converge para um hiperplano separador. O intercepto $\beta_0$ desloca o hiperplano no espa√ßo de caracter√≠sticas.

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

```mermaid
graph LR
    subgraph "Bayesian Decision Rule vs LDA"
        direction TB
        A["Bayesian Decision Rule"] --> B["Maximizes Posterior Probability P(G=k|X=x)"]
        C["LDA"] --> D["Simplifies Bayes Rule under Gaussian Assumption"]
        B --> E["Identical Decision Boundaries with Shared Covariance"]
        D --> E
    end
```

**Resposta:**

A **Regra de Decis√£o Bayesiana** busca classificar uma observa√ß√£o $x$ na classe $k$ que maximize a probabilidade posterior $P(G=k|X=x)$ [^4.3]. Sob a suposi√ß√£o de distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia $\Sigma$, a probabilidade posterior √© dada por:

$$
P(G=k|X=x) = \frac{ \phi(x;\mu_k,\Sigma)\pi_k}{\sum_{l=1}^K \phi(x;\mu_l,\Sigma)\pi_l}
$$

onde $\phi(x;\mu_k,\Sigma)$ √© a densidade gaussiana para a classe $k$, $\mu_k$ √© a m√©dia da classe $k$ e $\pi_k$ √© a probabilidade a priori da classe $k$. O **LDA** deriva suas fun√ß√µes discriminantes lineares diretamente dessas suposi√ß√µes, buscando otimizar a separa√ß√£o entre as classes no espa√ßo de caracter√≠sticas [^4.3].

**Lemma 4:** *Sob a suposi√ß√£o de distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia, a regra de decis√£o Bayesiana e as fun√ß√µes discriminantes do LDA levam √† mesma fronteira de decis√£o, incluindo interceptos que levam a posi√ß√µes similares da fronteira.*

**Corol√°rio 4:** *A remo√ß√£o da restri√ß√£o de igualdade de covari√¢ncias na regra de decis√£o Bayesiana leva ao Quadratic Discriminant Analysis (QDA), onde as fronteiras de decis√£o s√£o quadr√°ticas e os par√¢metros, incluindo os interceptos, s√£o estimados de forma diferente, levando √† decis√µes n√£o lineares.*

> ‚ö†Ô∏è **Ponto Crucial**: A principal diferen√ßa entre o LDA e a regra de decis√£o Bayesiana est√° na deriva√ß√£o do modelo. A regra Bayesiana busca maximizar a probabilidade posterior, e o LDA simplifica esta maximiza√ß√£o atrav√©s das suposi√ß√µes gaussianas e da igualdade de covari√¢ncias, onde, sob essas restri√ß√µes, as fronteiras de decis√£o, incluindo os interceptos, s√£o id√™nticas [^4.3].

> üí° **Exemplo Num√©rico:**
>
> Considere um cen√°rio com duas classes e uma √∫nica caracter√≠stica $x$. Suponha que a classe 1 tenha m√©dia $\mu_1 = 2$ e a classe 2 tenha m√©dia $\mu_2 = 4$. A matriz de covari√¢ncia compartilhada √© $\Sigma = 1$. As probabilidades a priori s√£o $\pi_1 = 0.4$ e $\pi_2 = 0.6$.
>
> A regra de decis√£o Bayesiana classifica um ponto $x$ na classe que maximiza a probabilidade posterior:
>
> $P(G=k|X=x) \propto \phi(x; \mu_k, \Sigma)\pi_k = \frac{1}{\sqrt{2\pi\Sigma}}e^{-\frac{(x-\mu_k)^2}{2\Sigma}} \pi_k$
>
> Tomando o log da probabilidade posterior e ignorando os termos constantes, temos:
>
> $\log P(G=k|X=x) \propto -\frac{(x-\mu_k)^2}{2} + \log(\pi_k)$
>
> Para a classe 1, o termo √©: $-\frac{(x-2)^2}{2} + \log(0.4)$
>
> Para a classe 2, o termo √©: $-\frac{(x-4)^2}{2} + \log(0.6)$
>
> A fronteira de decis√£o ocorre quando esses dois termos s√£o iguais:
>
> $-\frac{(x-2)^2}{2} + \log(0.4) = -\frac{(x-4)^2}{2} + \log(0.6)$
>
> Resolvendo para $x$, temos:
>
> $(x-4)^2 - (x-2)^2 = 2(\log(0.6) - \log(0.4))$
>
> $x^2 - 8x + 16 - (x^2 - 4x + 4) = 2 \log(1.5)$
>
> $-4x + 12 = 2 \log(1.5)$
>
> $x = 3 - \frac{\log(1.5)}{2} \approx 3 - 0.20 = 2.8$
>
> O intercepto em LDA √© implicitamente encontrado ao igualar as fun√ß√µes discriminantes.  Sob as suposi√ß√µes de gaussianidade e covari√¢ncias iguais, LDA e a regra Bayesiana levam √† mesma fronteira de decis√£o, incluindo o intercepto, que posiciona o hiperplano separador.

### Conclus√£o

Neste cap√≠tulo, exploramos as diferen√ßas na estima√ß√£o dos interceptos (ou cut-points) em modelos de classifica√ß√£o linear, destacando a influ√™ncia da suposi√ß√£o gaussiana no LDA e o papel da minimiza√ß√£o da soma de quadrados na regress√£o linear com matrizes de indicadores. Discutimos como a regress√£o log√≠stica estima os interceptos diretamente da maximiza√ß√£o da verossimilhan√ßa, e como a escolha do m√©todo impacta a forma da fronteira de decis√£o e a qualidade das estimativas. Analisamos tamb√©m como a sele√ß√£o de vari√°veis e a regulariza√ß√£o podem ser usadas para controlar a complexidade dos modelos e melhorar a robustez da estimativa do intercepto. A compara√ß√£o entre o LDA e a regra de decis√£o Bayesiana destacou como a suposi√ß√£o gaussiana impacta a estimativa dos par√¢metros, incluindo os interceptos, e a forma das fronteiras de decis√£o. Este cap√≠tulo ofereceu uma vis√£o aprofundada de como diferentes abordagens lidam com a estimativa do intercepto e como essa escolha se relaciona com a tomada de decis√£o em modelos de classifica√ß√£o linear.

### Footnotes

[^4.1]: *In this chapter we revisit the classification problem and focus on linear methods for classification...There are several different ways in which linear decision boundaries can be found.*

[^4.2]: *In Chapter 2 we fit linear regression models to the class indicator variables, and classify to the largest fit...Linear inequalities in this space are quadratic inequalities in the original space.*

[^4.3]: *Decision theory for classification (Section 2.4) tells us that we need to know the class posteriors Pr(G|X) for optimal classification. Suppose fk(x) is the class-conditional density of X in class G = k, and let œÄŒ∫ be the prior probability of class k... Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Œ£k = Œ£.*

[^4.3.1]: *The decision boundary between each pair of classes k and l is described by a quadratic equation {x: Œ¥Œ∫(x) = Œ¥(x)}.*

[^4.3.3]: *In the special case when we assume that the classes have a common covariance matrix...When the classes are really Gaussian, then LDA is optimal*

[^4.4]: *The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0,1].*

[^4.4.1]: *Logistic regression models are usually fit by maximum likelihood... The logistic regression model is more general, in that it makes less assumptions.*

[^4.4.2]: *It is convenient to code the two-class gi via a 0/1 response Yi, where yi = 1 when gi = 1, and yi = 0 when gi = 2... Typically many models are fit in a search for a parsimonious model involving a subset of the variables.*

[^4.4.3]: *To maximize the log-likelihood, we set its derivatives to zero. These score equations are...To solve the score equations (4.21), we use the Newton-Raphson algorithm...*

[^4.4.4]: *The L1 penalty used in the lasso (Section 3.4.2) can be used for variable selection and shrinkage with any linear regression model...As with the lasso, we typically do not penalize the intercept term.*

[^4.5]: *Here we present an analysis of binary data to illustrate the traditional statistical use of the logistic regression model... With two classes there is a simple correspondence between linear discriminant analysis and classification by linear least squares, as in (4.5).*

[^4.5.1]: *The perceptron learning algorithm tries to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary.*

[^4.5.2]: *The optimal separating hyperplane separates the two classes and maximizes the distance to the closest point from either class... In light of (4.40), the constraints define an empty slab or margin around the linear decision boundary...*
