## T√≠tulo Conciso: Classifica√ß√£o Linear, Modelagem Gaussiana e Covari√¢ncias Comuns

<imagem: Um diagrama que ilustra a modelagem de densidades condicionais como gaussianas multivariadas com matrizes de covari√¢ncia comuns, destacando os par√¢metros (m√©dias e covari√¢ncias) e como essa abordagem leva a decis√µes de classifica√ß√£o linear. O diagrama deve mostrar distribui√ß√µes gaussianas com a mesma forma e orienta√ß√£o para cada classe, com centroides diferentes.>
```mermaid
graph LR
    subgraph "LDA Model"
    direction TB
        A["Input Data 'X'"]
        B["Class Conditional Densities: P(X|G=k)"]
        C["Gaussian Distribution: 'phi(x; mu_k, Sigma)'"]
        D["Common Covariance Matrix: 'Sigma'"]
        E["Class Means: 'mu_k'"]
        F["Linear Discriminant Functions: 'delta_k(x)'"]
        G["Linear Decision Boundaries"]
        A --> B
        B --> C
        C --> D
        C --> E
        D --> F
        E --> F
        F --> G
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora em profundidade a modelagem de **densidades condicionais de classe** utilizando **distribui√ß√µes gaussianas multivariadas com matrizes de covari√¢ncia comuns**, que √© a base para o **Linear Discriminant Analysis (LDA)** [^4.3].  Analisaremos como essa suposi√ß√£o simplifica o problema de classifica√ß√£o, levando a fronteiras de decis√£o lineares e regras de decis√£o baseadas em fun√ß√µes discriminantes lineares. Discutiremos como as m√©dias e a matriz de covari√¢ncia comum afetam a forma dessas fronteiras de decis√£o e exploraremos as implica√ß√µes dessa escolha para problemas de classifica√ß√£o. Compararemos esta abordagem com a **regress√£o linear com matrizes de indicadores**, que n√£o modela as densidades condicionais diretamente [^4.2], e com a **regress√£o log√≠stica**, que modela diretamente as probabilidades posteriores [^4.4]. Abordaremos tamb√©m a import√¢ncia da **sele√ß√£o de vari√°veis e regulariza√ß√£o** para melhorar a robustez e a interpretabilidade dos modelos [^4.4.4], [^4.5]. O objetivo deste cap√≠tulo √© fornecer uma vis√£o abrangente e detalhada de como a suposi√ß√£o de gaussianidade e covari√¢ncias comuns afeta a constru√ß√£o e o desempenho dos classificadores lineares.

### Conceitos Fundamentais

**Conceito 1: Modelagem Gaussiana e a Suposi√ß√£o de Covari√¢ncias Comuns**

O **LDA** assume que as densidades condicionais de cada classe, $P(X|G=k)$, seguem uma distribui√ß√£o gaussiana multivariada, caracterizada por um vetor de m√©dias $\mu_k$ e uma matriz de covari√¢ncia $\Sigma$ [^4.3]. A suposi√ß√£o fundamental do LDA √© que todas as classes compartilham a mesma matriz de covari√¢ncia $\Sigma$. Esta suposi√ß√£o, embora simplificadora, leva a fun√ß√µes discriminantes lineares, o que facilita a implementa√ß√£o e a interpreta√ß√£o do modelo. A fun√ß√£o densidade gaussiana multivariada √© dada por:

$$
\phi(x; \mu_k, \Sigma) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)}
$$

onde $p$ √© a dimens√£o do espa√ßo de entrada.

> üí° **Exemplo Num√©rico:**
>
> Considere um problema de classifica√ß√£o com duas classes ($k=1, 2$) e duas vari√°veis ($p=2$). Suponha que as m√©dias das classes s√£o $\mu_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$ e $\mu_2 = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$, e a matriz de covari√¢ncia comum √© $\Sigma = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$. Para uma observa√ß√£o $x = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$, podemos calcular as densidades Gaussianas para cada classe. Primeiro, precisamos calcular a inversa da matriz de covari√¢ncia $\Sigma^{-1}$:
>
> $\Sigma^{-1} = \frac{1}{(1*1 - 0.5*0.5)} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \begin{bmatrix} 1.33 & -0.67 \\ -0.67 & 1.33 \end{bmatrix}$
>
> Agora, podemos calcular os termos $(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)$ para ambas as classes:
>
> Para a classe 1:
> $(x-\mu_1) = \begin{bmatrix} 2-1 \\ 3-2 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$
> $(x-\mu_1)^T\Sigma^{-1}(x-\mu_1) = \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 1.33 & -0.67 \\ -0.67 & 1.33 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 0.66 & 0.66 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = 1.32$
>
> Para a classe 2:
> $(x-\mu_2) = \begin{bmatrix} 2-3 \\ 3-4 \end{bmatrix} = \begin{bmatrix} -1 \\ -1 \end{bmatrix}$
> $(x-\mu_2)^T\Sigma^{-1}(x-\mu_2) = \begin{bmatrix} -1 & -1 \end{bmatrix} \begin{bmatrix} 1.33 & -0.67 \\ -0.67 & 1.33 \end{bmatrix} \begin{bmatrix} -1 \\ -1 \end{bmatrix} = \begin{bmatrix} -0.66 & -0.66 \end{bmatrix} \begin{bmatrix} -1 \\ -1 \end{bmatrix} = 1.32$
>
> O determinante de $\Sigma$ √© $1*1 - 0.5*0.5 = 0.75$.
>
> Substituindo na fun√ß√£o densidade Gaussiana (sem o fator constante), temos:
>
> $\phi(x; \mu_1, \Sigma) \propto e^{-\frac{1}{2}1.32} \approx 0.52$
> $\phi(x; \mu_2, \Sigma) \propto e^{-\frac{1}{2}1.32} \approx 0.52$
>
> Neste caso, as densidades s√£o iguais (sem considerar as probabilidades a priori), o que significa que a decis√£o final depender√° das probabilidades a priori das classes. Se $\pi_1 > \pi_2$, a observa√ß√£o ser√° classificada como classe 1 e vice-versa.
>
> Este exemplo demonstra como calcular a densidade Gaussiana para uma observa√ß√£o $x$ em rela√ß√£o a cada classe.

**Lemma 1:** *A suposi√ß√£o de que as densidades condicionais s√£o Gaussianas com a mesma matriz de covari√¢ncia leva a fun√ß√µes discriminantes lineares na teoria de decis√£o, o que simplifica o problema de classifica√ß√£o.*  A prova desse lema √© obtida atrav√©s da an√°lise das propriedades das distribui√ß√µes gaussianas e como elas levam a fun√ß√µes lineares.

**Conceito 2: Linear Discriminant Analysis (LDA) e a Deriva√ß√£o da Fun√ß√£o Discriminante**

No **LDA**, a fun√ß√£o discriminante para cada classe $k$ √© dada por:

$$
\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log \pi_k
$$

onde $\mu_k$ √© o vetor de m√©dias da classe $k$, $\Sigma$ √© a matriz de covari√¢ncia comum e $\pi_k$ √© a probabilidade a priori da classe $k$ [^4.3].  Essa fun√ß√£o discriminante √© derivada da aplica√ß√£o do Teorema de Bayes e da suposi√ß√£o gaussiana, onde as parcelas quadr√°ticas se cancelam devido √† igualdade de covari√¢ncia. A decis√£o √© tomada atribuindo $x$ √† classe $k$ que maximize a fun√ß√£o discriminante $\delta_k(x)$.
```mermaid
graph LR
    subgraph "LDA Discriminant Function"
        direction TB
        A["Discriminant Function: 'delta_k(x)'"]
        B["Linear Term: 'x^T Sigma^-1 mu_k'"]
        C["Quadratic Term Cancellation: '-1/2 mu_k^T Sigma^-1 mu_k'"]
        D["Prior Probability Term: 'log pi_k'"]
        A --> B
        A --> C
        A --> D
    end
```

> üí° **Exemplo Num√©rico:**
>
> Continuando com o exemplo anterior, suponha que as probabilidades a priori s√£o $\pi_1 = 0.6$ e $\pi_2 = 0.4$. Usando os mesmos valores de $\mu_1, \mu_2, \Sigma$ e sua inversa $\Sigma^{-1}$ e a mesma observa√ß√£o $x$, podemos calcular as fun√ß√µes discriminantes:
>
> Para a classe 1:
> $\delta_1(x) = x^T \Sigma^{-1} \mu_1 - \frac{1}{2} \mu_1^T \Sigma^{-1} \mu_1 + \log \pi_1$
> $\delta_1(x) = \begin{bmatrix} 2 & 3 \end{bmatrix} \begin{bmatrix} 1.33 & -0.67 \\ -0.67 & 1.33 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 1 & 2 \end{bmatrix} \begin{bmatrix} 1.33 & -0.67 \\ -0.67 & 1.33 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} + \log(0.6)$
>
> Calculando os termos:
> $x^T \Sigma^{-1} \mu_1 = \begin{bmatrix} 2 & 3 \end{bmatrix} \begin{bmatrix} 1.99 \\ 1.99 \end{bmatrix} = 9.97$
> $\mu_1^T \Sigma^{-1} \mu_1 = \begin{bmatrix} 1 & 2 \end{bmatrix} \begin{bmatrix} 1.99 \\ 1.99 \end{bmatrix} = 5.97$
> $\log(0.6) \approx -0.51$
> $\delta_1(x) = 9.97 - \frac{1}{2} 5.97 - 0.51 \approx 9.97 - 2.985 - 0.51 \approx 6.475$
>
> Para a classe 2:
> $\delta_2(x) = x^T \Sigma^{-1} \mu_2 - \frac{1}{2} \mu_2^T \Sigma^{-1} \mu_2 + \log \pi_2$
> $\delta_2(x) = \begin{bmatrix} 2 & 3 \end{bmatrix} \begin{bmatrix} 1.33 & -0.67 \\ -0.67 & 1.33 \end{bmatrix} \begin{bmatrix} 3 \\ 4 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 3 & 4 \end{bmatrix} \begin{bmatrix} 1.33 & -0.67 \\ -0.67 & 1.33 \end{bmatrix} \begin{bmatrix} 3 \\ 4 \end{bmatrix} + \log(0.4)$
>
> Calculando os termos:
> $x^T \Sigma^{-1} \mu_2 = \begin{bmatrix} 2 & 3 \end{bmatrix} \begin{bmatrix} 1.33 \\ 3.33 \end{bmatrix} = 12.65$
> $\mu_2^T \Sigma^{-1} \mu_2 = \begin{bmatrix} 3 & 4 \end{bmatrix} \begin{bmatrix} 1.33 \\ 3.33 \end{bmatrix} = 17.31$
> $\log(0.4) \approx -0.92$
> $\delta_2(x) = 12.65 - \frac{1}{2} 17.31 - 0.92 \approx 12.65 - 8.655 - 0.92 \approx 3.075$
>
> Como $\delta_1(x) > \delta_2(x)$, a observa√ß√£o $x$ seria classificada como pertencente √† classe 1. Este exemplo demonstra o c√°lculo das fun√ß√µes discriminantes e como elas s√£o usadas na classifica√ß√£o pelo LDA.

**Corol√°rio 1:** *As fun√ß√µes discriminantes do LDA s√£o lineares devido √† suposi√ß√£o de covari√¢ncias iguais.* A linearidade da fun√ß√£o discriminante √© uma consequ√™ncia direta do cancelamento dos termos quadr√°ticos na densidade gaussiana devido √† igualdade da matriz de covari√¢ncias.

**Conceito 3: A Suposi√ß√£o de Covari√¢ncias Comuns e suas Implica√ß√µes**

A suposi√ß√£o de que todas as classes compartilham a mesma matriz de covari√¢ncia $\Sigma$ √© uma simplifica√ß√£o que pode n√£o ser v√°lida em todos os casos. No entanto, essa suposi√ß√£o leva a um modelo mais simples e est√°vel, com menos par√¢metros a serem estimados. A suposi√ß√£o de covari√¢ncias comuns tamb√©m garante que as fronteiras de decis√£o entre as classes sejam lineares, o que √© uma caracter√≠stica fundamental do LDA. Quando as classes apresentam estruturas de covari√¢ncia muito diferentes, pode ser mais adequado utilizar o **Quadratic Discriminant Analysis (QDA)**, que estima uma matriz de covari√¢ncia diferente para cada classe, mas que resulta em fronteiras de decis√£o n√£o lineares. [^4.3.1].
```mermaid
graph LR
    subgraph "Covariance Assumption"
        direction TB
        A["LDA Assumption: 'Sigma_k = Sigma' (Common Covariance)"]
        B["Linear Decision Boundaries"]
        C["QDA Approach: 'Sigma_k != Sigma_l' (Separate Covariances)"]
        D["Quadratic Decision Boundaries"]
        A --> B
        C --> D
    end
```

> ‚ö†Ô∏è **Nota Importante**:  A suposi√ß√£o de covari√¢ncias comuns no LDA simplifica o modelo, mas tamb√©m imp√µe uma restri√ß√£o sobre a forma das fronteiras de decis√£o.

> ‚ùó **Ponto de Aten√ß√£o**: Em casos onde as classes apresentam variabilidades muito diferentes, a suposi√ß√£o de covari√¢ncias comuns pode comprometer o desempenho do modelo.

> ‚úîÔ∏è **Destaque**: A suposi√ß√£o de gaussianidade e de covari√¢ncias comuns √© central para a defini√ß√£o do LDA e para a obten√ß√£o de fronteiras de decis√£o lineares.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

<imagem: Diagrama que compara o LDA com a regress√£o linear, destacando como o LDA modela as densidades condicionais com gaussianas multivariadas, enquanto a regress√£o linear n√£o imp√µe restri√ß√µes sobre a forma da distribui√ß√£o das classes. O diagrama deve mostrar as distribui√ß√µes gaussianas para as classes no LDA e as fun√ß√µes lineares ajustadas pela regress√£o, enfatizando a diferen√ßa na abordagem de modelagem.>
```mermaid
graph LR
    subgraph "Comparison: LDA vs. Linear Regression"
        direction LR
        A["LDA: Models Class Conditional Densities"]
        B["Gaussian Distributions with Common Covariance"]
        C["Linear Decision Boundaries"]
        D["Linear Regression: No Density Assumption"]
         E["Linear Functions Fitted to Indicator Matrices"]
        F["No Direct Density Modeling"]
        A --> B
        B --> C
        D --> E
        E --> F
    end
```

A regress√£o linear, quando aplicada a matrizes de indicadores, busca ajustar uma fun√ß√£o linear $f_k(x) = \beta_{k0} + \beta_k^T x$ para cada classe, com o objetivo de minimizar a soma dos quadrados dos erros [^4.2].  Ao contr√°rio do LDA, a regress√£o linear n√£o imp√µe nenhuma restri√ß√£o sobre a forma da distribui√ß√£o das classes. O ajuste por m√≠nimos quadrados busca minimizar a diferen√ßa entre os *targets* das classes (codificados por indicadores bin√°rios) e as sa√≠das do modelo, mas n√£o h√° modelagem direta das densidades condicionais $P(X|G=k)$.

Essa falta de modelagem direta das densidades condicionais √© uma das limita√ß√µes da regress√£o linear como classificador, pois ela n√£o leva em considera√ß√£o a distribui√ß√£o dos dados e, por isso, pode apresentar estimativas que n√£o se comportam como probabilidades e sofrer do problema do "masking" [^4.2]. Al√©m disso, a regress√£o linear n√£o utiliza a informa√ß√£o de uma matriz de covari√¢ncia comum e n√£o estima as probabilidades a priori das classes.

Em contraste, o LDA modela explicitamente as densidades condicionais como Gaussianas com a mesma matriz de covari√¢ncia, o que permite a utiliza√ß√£o do Teorema de Bayes para derivar uma regra de decis√£o que se baseia em probabilidades posteriores e na maximiza√ß√£o da separa√ß√£o entre as classes sob essa suposi√ß√£o [^4.3].

**Lemma 2:** *A regress√£o linear com matrizes de indicadores n√£o modela explicitamente as densidades condicionais de classe, ao contr√°rio do LDA, que assume uma forma gaussiana para essas densidades.* Essa limita√ß√£o √© uma consequ√™ncia da abordagem da regress√£o linear em sua aplica√ß√£o a matrizes de indicadores.

**Corol√°rio 2:** *A falta de suposi√ß√µes sobre a distribui√ß√£o das classes na regress√£o linear com matrizes de indicadores torna o m√©todo mais flex√≠vel, mas tamb√©m mais sujeito a problemas como o "masking" e estimativas fora do intervalo [0, 1].* Em situa√ß√µes onde as classes seguem uma distribui√ß√£o aproximadamente gaussiana e com covari√¢ncias similares, o LDA pode levar a melhores resultados, por modelar explicitamente as distribui√ß√µes e as probabilidades posteriores [^4.3].

> üí° **Exemplo Num√©rico:**
>
> Considere o mesmo problema de classifica√ß√£o com duas classes e duas vari√°veis. Para usar regress√£o linear com matrizes indicadoras, codificamos a classe 1 como $y=1$ e a classe 2 como $y=0$. Suponha que temos os seguintes dados de treinamento:
>
> | $x_1$ | $x_2$ | Classe (y) |
> |-------|-------|------------|
> | 1     | 2     | 1          |
> | 2     | 3     | 1          |
> | 3     | 4     | 2          |
> | 4     | 5     | 2          |
>
> A matriz de design $X$ e o vetor de respostas $y$ s√£o:
> $X = \begin{bmatrix} 1 & 1 & 2 \\ 1 & 2 & 3 \\ 1 & 3 & 4 \\ 1 & 4 & 5 \end{bmatrix}$, $y = \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}$
>
> Usando a equa√ß√£o normal para obter os coeficientes $\beta$:
> $\beta = (X^TX)^{-1}X^Ty$
>
> Calculando $X^TX$:
> $X^TX = \begin{bmatrix} 4 & 10 & 14 \\ 10 & 30 & 42 \\ 14 & 42 & 62 \end{bmatrix}$
>
> Calculando $(X^TX)^{-1}$ (usando numpy):
> ```python
> import numpy as np
> X = np.array([[1, 1, 2], [1, 2, 3], [1, 3, 4], [1, 4, 5]])
> y = np.array([1, 1, 0, 0])
> XtX = X.T @ X
> XtX_inv = np.linalg.inv(XtX)
> print(XtX_inv)
> ```
>
> ```
> [[ 6.5  -4.   1.5]
> [-4.    2.5 -1. ]
> [ 1.5  -1.   0.5]]
> ```
>
> Calculando $X^Ty$:
> $X^Ty = \begin{bmatrix} 2 \\ 3 \\ 5 \end{bmatrix}$
>
> Calculando $\beta$:
> $\beta = \begin{bmatrix} 6.5 & -4 & 1.5 \\ -4 & 2.5 & -1 \\ 1.5 & -1 & 0.5 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \\ 5 \end{bmatrix} = \begin{bmatrix} -0.5 \\ 0.5 \\ 0 \end{bmatrix}$
>
> A fun√ß√£o linear ajustada √© $f(x) = -0.5 + 0.5x_1$. Por exemplo, se considerarmos o ponto $x = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$, a sa√≠da do modelo de regress√£o √© $f(x) = -0.5 + 0.5*2 = 0.5$. O classificador iria classificar este ponto como classe 1 se $f(x) > 0.5$ e classe 2 caso contr√°rio. Note que a regress√£o linear n√£o restringe as sa√≠das para estarem entre [0, 1].
>
> Este exemplo ilustra como a regress√£o linear com matrizes indicadoras ajusta um modelo linear que n√£o modela diretamente as densidades condicionais e n√£o garante que as estimativas se comportem como probabilidades.

A regress√£o linear com matrizes de indicadores, embora possa ser √∫til como uma aproxima√ß√£o inicial, n√£o utiliza informa√ß√µes sobre as densidades condicionais de classe e n√£o garante que as estimativas se comportem como probabilidades. Modelos como o LDA, que modelam explicitamente as densidades condicionais, podem ser mais apropriados para problemas de classifica√ß√£o onde as classes seguem, pelo menos aproximadamente, distribui√ß√µes gaussianas [^4.3].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

<imagem: Um diagrama que mostra como a regulariza√ß√£o afeta a forma das distribui√ß√µes gaussianas, restringindo a magnitude dos coeficientes e melhorando a capacidade de generaliza√ß√£o dos modelos de classifica√ß√£o. O diagrama deve destacar as diferen√ßas entre as penalidades L1 e L2 na regress√£o log√≠stica.>
```mermaid
graph LR
    subgraph "Regularization in Logistic Regression"
        direction TB
        A["Logistic Regression Cost Function"]
        B["Data Likelihood Term"]
        C["Regularization Term: 'lambda * P(beta)'"]
        D["L1 Penalty (Lasso): '||beta||_1'"]
        E["L2 Penalty (Ridge): '||beta||_2^2'"]
        F["Sparse Feature Selection"]
        G["Coefficient Shrinkage"]
        A --> B
        A --> C
        C --> D
        C --> E
        D --> F
        E --> G
    end
```

A **sele√ß√£o de vari√°veis** e a **regulariza√ß√£o** s√£o t√©cnicas cruciais para melhorar a robustez e o desempenho dos modelos de classifica√ß√£o, especialmente em situa√ß√µes onde se utilizam modelos que assumem distribui√ß√µes gaussianas para as classes. A regulariza√ß√£o, em particular, adiciona um termo de penalidade √† fun√ß√£o de custo, o que restringe a magnitude dos coeficientes e evita o *overfitting*.

Na **regress√£o log√≠stica**, que busca modelar diretamente a probabilidade posterior, a fun√ß√£o de custo regularizada √© dada por:

$$
\max_{\beta_0, \beta} \left[ \sum_{i=1}^N \left( y_i (\beta_0 + \beta^T x_i) - \log(1 + e^{\beta_0 + \beta^T x_i}) \right) - \lambda P(\beta) \right]
$$

onde $P(\beta)$ √© a penalidade e $\lambda$ √© o par√¢metro de regulariza√ß√£o. A penalidade **L1** (Lasso) imp√µe esparsidade nos coeficientes, levando √† sele√ß√£o das vari√°veis mais relevantes para a modelagem das densidades condicionais [^4.4.4]. A penalidade **L2** (Ridge) reduz a magnitude dos coeficientes, estabilizando o modelo e diminuindo o risco de *overfitting* [^4.5].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo de regress√£o log√≠stica com regulariza√ß√£o L1 (Lasso). Suponha que temos 3 vari√°veis preditoras e queremos classificar um problema bin√°rio. Usaremos um conjunto de dados simplificado para ilustrar o efeito da regulariza√ß√£o L1.
>
> Dados de treinamento (simulados):
>
> | $x_1$ | $x_2$ | $x_3$ | y |
> |-------|-------|-------|---|
> | 1     | 2     | 3     | 1 |
> | 2     | 3     | 4     | 1 |
> | 3     | 4     | 5     | 0 |
> | 4     | 5     | 6     | 0 |
> | 5     | 6     | 1     | 1 |
> | 6     | 1     | 2     | 0 |
>
> Usando a biblioteca scikit-learn para ajustar o modelo de regress√£o log√≠stica com regulariza√ß√£o L1:
> ```python
> import numpy as np
> from sklearn.linear_model import LogisticRegression
>
> X = np.array([[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 1], [6, 1, 2]])
> y = np.array([1, 1, 0, 0, 1, 0])
>
> # Ajustando o modelo com diferentes valores de lambda
> for C in [1, 0.1, 0.01]:
>    model = LogisticRegression(penalty='l1', solver='liblinear', C=C, random_state=42)
>    model.fit(X, y)
>    print(f"C = {C}, Coeficientes = {model.coef_}")
> ```
>
> Executando este c√≥digo, obtemos:
>
> ```
> C = 1, Coeficientes = [[-0.17192889  0.05068481 -0.08397992]]
> C = 0.1, Coeficientes = [[-0.         0.         -0.36115846]]
> C = 0.01, Coeficientes = [[ 0.  0. -0.]]
> ```
>
> O par√¢metro `C` √© o inverso de $\lambda$. Quanto menor o valor de `C` (maior $\lambda$), maior √© a penalidade. Podemos ver que com `C = 1`, todos os coeficientes s√£o diferentes de zero. Com `C = 0.1`, os coeficientes de $x_1$ e $x_2$ s√£o zerados e apenas $x_3$ √© mantido. Com `C = 0.01`, todos os coeficientes s√£o zerados. Isso demonstra o efeito da regulariza√ß√£o L1 na sele√ß√£o de vari√°veis, onde vari√°veis menos importantes s√£o eliminadas do modelo.
>
> Este exemplo ilustra como a regulariza√ß√£o L1 pode simplificar o modelo e selecionar as vari√°veis mais relevantes para a classifica√ß√£o.

A regulariza√ß√£o desempenha um papel importante, independentemente da abordagem para modelar as densidades condicionais, porque mesmo modelos gaussianos podem se beneficiar da restri√ß√£o sobre os coeficientes para se tornarem mais robustos e com maior capacidade de generaliza√ß√£o.

**Lemma 3:** *A regulariza√ß√£o L1, ao promover a esparsidade, leva a uma representa√ß√£o mais simples das densidades condicionais e melhora a capacidade de generaliza√ß√£o do modelo.* Essa afirma√ß√£o √© uma consequ√™ncia do efeito da penalidade L1 sobre a fun√ß√£o de custo.

**Prova do Lemma 3:** A penalidade L1 adiciona um termo √† fun√ß√£o de custo que √© proporcional ao valor absoluto dos coeficientes. Este termo for√ßa alguns dos coeficientes a se tornarem exatamente zero durante o processo de otimiza√ß√£o, resultando em modelos mais simples e mais f√°ceis de interpretar [^4.4.3], [^4.4.4]. $\blacksquare$

**Corol√°rio 3:** *A regulariza√ß√£o, seja L1 ou L2, auxilia na obten√ß√£o de estimativas mais precisas e est√°veis das densidades condicionais e das probabilidades posteriores, melhorando a qualidade da tomada de decis√£o na classifica√ß√£o.*  A regulariza√ß√£o pode ser aplicada a modelos Gaussianos e a outros modelos mais complexos para o controle da complexidade e do *overfitting*.

> ‚ö†Ô∏è **Ponto Crucial**: A regulariza√ß√£o, ao controlar a complexidade dos modelos, melhora a estimativa das densidades condicionais e das probabilidades posteriores, tornando os modelos de classifica√ß√£o mais robustos e confi√°veis [^4.5].

### Separating Hyperplanes e Perceptrons

<imagem: Um diagrama que ilustra como hiperplanos separadores podem ser vistos como uma forma de dividir o espa√ßo de caracter√≠sticas, baseando-se nas informa√ß√µes das distribui√ß√µes gaussianas. O diagrama deve mostrar como a margem de separa√ß√£o e os vetores de suporte est√£o relacionados com a modelagem da densidade.>
```mermaid
graph LR
    subgraph "Separating Hyperplanes"
        direction TB
        A["Feature Space"]
        B["Separating Hyperplane"]
        C["Class 1 Data Points"]
        D["Class 2 Data Points"]
         E["Margin of Separation"]
         F["Support Vectors"]
        A --> B
        B --> C
        B --> D
        B --> E
        B --> F
    end
```

A ideia de **hiperplanos separadores** busca encontrar uma fronteira linear que maximize a separa√ß√£o entre as classes, ou seja, que n√£o apenas separe as classes, mas tamb√©m maximize a margem de separa√ß√£o [^4.5.2]. Essa abordagem √© central em modelos como as m√°quinas de vetores de suporte (SVM), onde o objetivo √© encontrar o hiperplano √≥timo que maximize essa margem. A ideia √© que ao maximizar a margem, o modelo se torna mais robusto e com maior capacidade de generaliza√ß√£o.

O algoritmo do **Perceptron** busca um hiperplano separador de forma iterativa ajustando os par√¢metros do modelo com base nas classifica√ß√µes incorretas. Embora o Perceptron n√£o modele diretamente as densidades condicionais, ele busca uma solu√ß√£o que separe as classes de forma eficaz.  A converg√™ncia do Perceptron para um hiperplano separador √© garantida somente se os dados forem linearmente separ√°veis, e ele n√£o garante a maximiza√ß√£o da margem [^4.5.1].

**Teorema:** *O algoritmo do Perceptron converge para um hiperplano separador em um n√∫mero finito de itera√ß√µes, se os dados de treinamento forem linearmente separ√°veis.* Este teorema demonstra a garantia de converg√™ncia para um classificador linear, embora em contextos n√£o linearmente separ√°veis essa garantia n√£o seja v√°lida [^4.5.1].

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

**Resposta:**

A **Regra de Decis√£o Bayesiana** busca classificar uma observa√ß√£o $x$ na classe $k$ que maximize a probabilidade posterior $P(G=k|X=x)$ [^4.3].  Sob a suposi√ß√£o de que as distribui√ß√µes condicionais s√£o Gaussianas com a mesma matriz de covari√¢ncia $\Sigma$, a regra de decis√£o Bayesiana √© dada por:

$$
\hat{G}(x) = \arg\max_k P(G=k|X=x) = \arg\max_k \frac{ \phi(x;\mu_k,\Sigma)\pi_k}{\sum_{l=1}^K \phi(x;\mu_l,\Sigma)\pi_l}
$$

onde $\phi(x;\mu_k,\Sigma)$ √© a fun√ß√£o densidade gaussiana para a classe $k$, e $\pi_k$ √© a probabilidade a priori da classe $k$. O **LDA**, por sua vez, deriva suas fun√ß√µes discriminantes lineares diretamente das densidades Gaussianas e da suposi√ß√£o de igualdade de matrizes de covari√¢ncia.
```mermaid
graph LR
    subgraph "Bayes Decision Rule vs. LDA"
        direction TB
        A["Bayes Decision Rule: Maximize 'P(G=k|X=x)'"]
        B["Posterior Probability: 'P(G=k|X=x) = P(X|G=k)P(G=k) / P(X)'"]
        C["Gaussian Conditional Densities 'phi(x;mu_k,Sigma)'"]
        D["LDA: Linear Discriminant Functions: 'delta_k(x)'"]
        E["Equivalent Decision Boundaries Under Common Covariance"]
        A --> B
        B --> C
        C --> D
         C --> E
        D --> E

    end
```

**Lemma 4:** *Sob a suposi√ß√£o de distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia, a regra de decis√£o Bayesiana e o LDA s√£o equivalentes, o que significa que ambas as abordagens levam √† mesma fronteira de decis√£o linear.* A equival√™ncia √© estabelecida atrav√©s da manipula√ß√£o alg√©brica, e mostra que a fun√ß√£o discriminante do LDA pode ser vista como uma aproxima√ß√£o da regra de decis√£o Bayesiana sob a suposi√ß√£o gaussiana e covari√¢ncia igual [^4.3].

**Corol√°rio 4:** *A remo√ß√£o da suposi√ß√£o de igualdade de covari√¢ncias na regra de decis√£o Bayesiana leva ao Quadratic Discriminant Analysis (QDA), onde as fronteiras de decis√£o s√£o quadr√°ticas, n√£o mais lineares*. QDA, portanto, relaxa uma premissa do LDA, e permite uma maior flexibilidade do modelo em termos da forma das fronteiras de decis√£o [^4.3.1], [^4.3.3].
```mermaid
graph LR
    subgraph "LDA vs QDA decision boundaries"
    direction TB
        A["LDA: Common Covariance Assumption ('Sigma_k = Sigma')"]
        B["Linear Decision Boundaries"]
        C["QDA: Separate Covariance Matrices ('Sigma_k != Sigma_l')"]
        D["Quadratic Decision Boundaries"]
        A --> B
        C --> D
    end
```

> ‚ö†Ô∏è **Ponto Crucial**:  A principal diferen√ßa entre LDA e a regra de decis√£o Bayesiana reside na forma como os m√©todos s√£o derivados. O LDA imp√µe a restri√ß√£o da igualdade de covari√¢ncias para obter fun√ß√µes discriminantes lineares, e, sob a mesma restri√ß√£o, a regra de decis√£o Bayesiana resulta nas mesmas fun√ß√µes discriminantes [^4.3].

### Conclus√£o

Neste cap√≠tulo, exploramos a modelagem de densidades condicionais como gaussianas multivariadas com matrizes de covari√¢ncia comuns, que √© a base do LDA. Analisamos como a suposi√ß√£o de gaussianidade e de covari√¢ncias iguais leva a fun√ß√µes discriminantes lineares e como essa modelagem afeta a tomada de decis√µes de classifica√ß√£o. Discutimos como a escolha do modelo para as densidades condicionais afeta a complexidade e a capacidade de generaliza√ß√£o dos modelos, e vimos que m√©todos como a regress√£o linear com matrizes de indicadores n√£o modelam as densidades condicionais diretamente, resultando em modelos com limita√ß√µes espec√≠ficas. Discutimos como a sele√ß√£o de vari√°veis e a regulariza√ß√£o melhor
