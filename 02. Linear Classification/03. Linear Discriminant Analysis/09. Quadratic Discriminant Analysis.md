## T√≠tulo Conciso: Classifica√ß√£o Quadr√°tica: Fronteiras de Decis√£o N√£o Lineares e QDA

```mermaid
graph LR
    A["Dados de Entrada"] --> B["QDA: Gaussianas com Covari√¢ncias Vari√°veis"];
    B --> C{"Fronteiras de Decis√£o Quadr√°ticas"};
    A --> D["LDA: Gaussianas com Covari√¢ncia Comum"];
    D --> E{"Fronteiras de Decis√£o Lineares"};
    C --> F["Classifica√ß√£o"];
    E --> F;
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#cfc,stroke:#333,stroke-width:2px
    style F fill:#f9f,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o

Este cap√≠tulo explora em detalhe o **Quadratic Discriminant Analysis (QDA)**, um m√©todo de classifica√ß√£o que, ao contr√°rio do **Linear Discriminant Analysis (LDA)**, permite que cada classe tenha sua pr√≥pria **matriz de covari√¢ncia**, levando √† forma√ß√£o de **fronteiras de decis√£o quadr√°ticas** [^4.3]. Analisaremos como a remo√ß√£o da suposi√ß√£o de covari√¢ncias iguais no LDA afeta a forma da fun√ß√£o discriminante e como isso resulta em fronteiras de decis√£o n√£o lineares. Compararemos o QDA com a **regress√£o linear com matrizes de indicadores**, que n√£o modela as densidades condicionais de forma direta, e com a **regress√£o log√≠stica**, que modela as probabilidades posteriores por meio de uma fun√ß√£o log√≠stica [^4.2], [^4.4]. Discutiremos a import√¢ncia da **sele√ß√£o de vari√°veis e regulariza√ß√£o** para controlar a complexidade dos modelos quadr√°ticos [^4.4.4], [^4.5]. O conceito de **hiperplanos separadores** ser√° abordado no contexto de modelos lineares, e contrastaremos com o QDA [^4.5.2]. O objetivo deste cap√≠tulo √© fornecer uma compreens√£o aprofundada das fronteiras de decis√£o quadr√°ticas no QDA e de como elas se relacionam com as suposi√ß√µes sobre a distribui√ß√£o dos dados.

### Conceitos Fundamentais

**Conceito 1: Quadratic Discriminant Analysis (QDA) e a Flexibilidade das Covari√¢ncias**

O **Quadratic Discriminant Analysis (QDA)** √© uma generaliza√ß√£o do LDA que relaxa a suposi√ß√£o de igualdade das matrizes de covari√¢ncia entre as classes. No QDA, cada classe $k$ √© modelada com uma distribui√ß√£o gaussiana multivariada, caracterizada por um vetor de m√©dias $\mu_k$ e uma matriz de covari√¢ncia $\Sigma_k$ espec√≠fica para cada classe [^4.3]. Ao permitir que as covari√¢ncias variem entre as classes, o QDA torna-se mais flex√≠vel do que o LDA e √© capaz de modelar dados com distribui√ß√µes mais complexas, resultando em fronteiras de decis√£o n√£o lineares [^4.3.1]. A fun√ß√£o densidade gaussiana para a classe $k$ √© dada por:

$$
P(X=x|G=k) = \phi(x; \mu_k, \Sigma_k) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}} e^{-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)}
$$

onde $p$ √© a dimens√£o do espa√ßo de entrada.

> üí° **Exemplo Num√©rico:**
>
> Considere um problema de classifica√ß√£o com duas classes ($k=1, 2$) e duas vari√°veis ($p=2$). Suponha que os par√¢metros estimados para cada classe s√£o:
>
> - Classe 1: $\mu_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$, $\Sigma_1 = \begin{bmatrix} 2 & 0.5 \\ 0.5 & 1 \end{bmatrix}$
> - Classe 2: $\mu_2 = \begin{bmatrix} 3 \\ 3 \end{bmatrix}$, $\Sigma_2 = \begin{bmatrix} 1 & -0.2 \\ -0.2 & 1.5 \end{bmatrix}$
>
> Note que as matrizes de covari√¢ncia $\Sigma_1$ e $\Sigma_2$ s√£o diferentes. A fun√ß√£o densidade para cada classe pode ser calculada para qualquer ponto $x = [x_1, x_2]^T$. Por exemplo, para o ponto $x = \begin{bmatrix} 2 \\ 2 \end{bmatrix}$:
>
> Para a Classe 1:
>
>  $\Sigma_1^{-1} = \frac{1}{(2*1 - 0.5*0.5)} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 2 \end{bmatrix} =  \frac{1}{1.75} \begin{bmatrix} 1 & -0.5 \\ -0.5 & 2 \end{bmatrix} \approx \begin{bmatrix} 0.57 & -0.29 \\ -0.29 & 1.14 \end{bmatrix}$
>
>  $(x - \mu_1)^T \Sigma_1^{-1} (x - \mu_1) =  \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 0.57 & -0.29 \\ -0.29 & 1.14 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = 1.13$
>
>  $|\Sigma_1| = 2*1 - 0.5*0.5 = 1.75$
>
>  $\phi(x; \mu_1, \Sigma_1) = \frac{1}{(2\pi)^{2/2}(1.75)^{1/2}} e^{-0.5 * 1.13} \approx 0.081$
>
> Para a Classe 2:
>
>  $\Sigma_2^{-1} = \frac{1}{(1*1.5 - (-0.2)*(-0.2))} \begin{bmatrix} 1.5 & 0.2 \\ 0.2 & 1 \end{bmatrix} = \frac{1}{1.46} \begin{bmatrix} 1.5 & 0.2 \\ 0.2 & 1 \end{bmatrix} \approx \begin{bmatrix} 1.03 & 0.14 \\ 0.14 & 0.68 \end{bmatrix}$
>
> $(x - \mu_2)^T \Sigma_2^{-1} (x - \mu_2) = \begin{bmatrix} -1 & -1 \end{bmatrix} \begin{bmatrix} 1.03 & 0.14 \\ 0.14 & 0.68 \end{bmatrix} \begin{bmatrix} -1 \\ -1 \end{bmatrix} = 1.85$
>
>  $|\Sigma_2| = 1*1.5 - (-0.2)*(-0.2) = 1.46$
>
>  $\phi(x; \mu_2, \Sigma_2) = \frac{1}{(2\pi)^{2/2}(1.46)^{1/2}} e^{-0.5 * 1.85} \approx 0.074$
>
> A densidade da classe 1 √© maior nesse ponto, o que sugere que este ponto estaria mais pr√≥ximo da classe 1, mas a decis√£o final depende tamb√©m das probabilidades a priori e da fun√ß√£o discriminante completa.

```mermaid
graph LR
    subgraph "QDA: Gaussian Density"
        direction TB
        A["P(X=x|G=k)"] --> B["(1 / (2œÄ)^(p/2) |Œ£k|^(1/2))"]
        A --> C["exp(-0.5 * (x - Œºk)^T * Œ£k^-1 * (x - Œºk))"]
        B --> D["(2œÄ)^(p/2): Normaliza√ß√£o"]
        B --> E["|Œ£k|^(1/2): Determinante da Covari√¢ncia"]
        C --> F["(x - Œºk)^T: Desvio do Centroide"]
        C --> G["Œ£k^-1: Inversa da Covari√¢ncia"]
    end
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

**Lemma 1:** *No QDA, a remo√ß√£o da suposi√ß√£o de igualdade de covari√¢ncias entre as classes leva √† forma√ß√£o de fun√ß√µes discriminantes quadr√°ticas e, consequentemente, a fronteiras de decis√£o n√£o lineares.* A prova deste lema √© obtida analisando a forma das fun√ß√µes discriminantes no QDA e como as parcelas quadr√°ticas n√£o se cancelam quando a covari√¢ncia √© distinta entre as classes.

**Conceito 2: Fun√ß√µes Discriminantes Quadr√°ticas e Fronteiras de Decis√£o**

No QDA, a fun√ß√£o discriminante para a classe $k$ √© dada por:

$$
\delta_k(x) = -\frac{1}{2} \log |\Sigma_k| - \frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) + \log \pi_k
$$

onde $\mu_k$ √© o vetor de m√©dias da classe $k$, $\Sigma_k$ √© a matriz de covari√¢ncia da classe $k$ e $\pi_k$ √© a probabilidade a priori da classe $k$. A decis√£o de classifica√ß√£o √© tomada atribuindo a observa√ß√£o $x$ √† classe que maximize $\delta_k(x)$ [^4.3]. Ao contr√°rio do LDA, essa fun√ß√£o discriminante √© quadr√°tica em $x$, o que significa que as fronteiras de decis√£o entre as classes n√£o s√£o mais hiperplanos, mas sim superf√≠cies quadr√°ticas, como par√°bolas, elipses e hip√©rboles [^4.3.1].

> üí° **Exemplo Num√©rico:**
>
> Usando os mesmos par√¢metros do exemplo anterior, e assumindo probabilidades a priori iguais para ambas as classes ($\pi_1 = \pi_2 = 0.5$), podemos calcular as fun√ß√µes discriminantes para o ponto $x = \begin{bmatrix} 2 \\ 2 \end{bmatrix}$:
>
> Para a Classe 1:
>
> $\delta_1(x) = -\frac{1}{2} \log(1.75) - \frac{1}{2} (1.13) + \log(0.5) \approx -0.28 - 0.565 - 0.693 \approx -1.538$
>
> Para a Classe 2:
>
> $\delta_2(x) = -\frac{1}{2} \log(1.46) - \frac{1}{2} (1.85) + \log(0.5) \approx -0.19 - 0.925 - 0.693 \approx -1.808$
>
> Como $\delta_1(x) > \delta_2(x)$, o ponto $x$ seria classificado na classe 1. A fronteira de decis√£o seria dada por $\delta_1(x) = \delta_2(x)$, que resulta em uma equa√ß√£o quadr√°tica.
>
> Podemos visualizar essa fronteira de decis√£o no espa√ßo de features usando um gr√°fico de contorno. Para isso, precisar√≠amos calcular os valores de $\delta_1(x)$ e $\delta_2(x)$ para uma grade de valores de $x_1$ e $x_2$, e ent√£o plotar a curva onde $\delta_1(x) = \delta_2(x)$.

```mermaid
graph LR
    subgraph "Quadratic Discriminant Function"
        direction TB
         A["Œ¥k(x)"] --> B["-0.5 * log(|Œ£k|)"]
         A --> C["-0.5 * (x - Œºk)^T * Œ£k^-1 * (x - Œºk)"]
         A --> D["log(œÄk)"]
         C --> E["(x - Œºk)^T: Desvio do Centroide"]
         C --> F["Œ£k^-1: Inversa da Covari√¢ncia"]
     end
     style A fill:#ccf,stroke:#333,stroke-width:2px
     style B fill:#ccf,stroke:#333,stroke-width:2px
     style C fill:#ccf,stroke:#333,stroke-width:2px
     style D fill:#ccf,stroke:#333,stroke-width:2px
     style E fill:#ccf,stroke:#333,stroke-width:2px
     style F fill:#ccf,stroke:#333,stroke-width:2px
```

**Corol√°rio 1:** *A utiliza√ß√£o de matrizes de covari√¢ncia distintas para cada classe no QDA resulta em fun√ß√µes discriminantes quadr√°ticas e, consequentemente, em fronteiras de decis√£o n√£o lineares.* Essa diferen√ßa √© fundamental na distin√ß√£o entre LDA e QDA.

**Conceito 3: Vantagens e Desvantagens do QDA**

O QDA oferece maior flexibilidade em rela√ß√£o ao LDA, permitindo que as fronteiras de decis√£o se adaptem √† estrutura dos dados de forma mais precisa. No entanto, essa flexibilidade tem um custo. O QDA possui um n√∫mero maior de par√¢metros a serem estimados, o que o torna mais suscet√≠vel ao *overfitting*, especialmente quando o n√∫mero de observa√ß√µes por classe √© pequeno. Al√©m disso, a estima√ß√£o das matrizes de covari√¢ncia para cada classe pode ser computacionalmente mais custosa do que a estima√ß√£o de uma √∫nica matriz de covari√¢ncia compartilhada, como no LDA. Portanto, o QDA √© mais adequado para situa√ß√µes onde a suposi√ß√£o de covari√¢ncias iguais √© claramente violada, e √© necess√°rio modelar fronteiras de decis√£o mais complexas [^4.3.2].

> ‚ö†Ô∏è **Nota Importante**: O QDA, ao remover a restri√ß√£o de igualdade das covari√¢ncias do LDA, se torna mais flex√≠vel e capaz de modelar dados com distribui√ß√µes mais complexas, mas tamb√©m mais suscet√≠vel ao *overfitting*.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha entre LDA e QDA deve levar em considera√ß√£o o trade-off entre vi√©s (LDA) e vari√¢ncia (QDA), bem como o tamanho da amostra e a complexidade da estrutura dos dados.

> ‚úîÔ∏è **Destaque**: O QDA, ao permitir que cada classe tenha sua pr√≥pria matriz de covari√¢ncia, produz fronteiras de decis√£o que s√£o quadr√°ticas, e n√£o mais lineares como no LDA.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
    subgraph "Comparison: LDA, QDA, Linear Regression"
        direction TB
         A["LDA"] --> B["Gaussian, Shared Covariance"];
         A --> C{"Linear Decision Boundary"};
         D["QDA"] --> E["Gaussian, Different Covariance"];
         D --> F{"Quadratic Decision Boundary"};
         G["Linear Regression"] --> H["No Distribution Assumptions"];
         G --> I{"Linear Decision Boundary"};
     end
     style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#cfc,stroke:#333,stroke-width:2px
        style G fill:#ccf,stroke:#333,stroke-width:2px
    style H fill:#ccf,stroke:#333,stroke-width:2px
    style I fill:#cfc,stroke:#333,stroke-width:2px
```

A **regress√£o linear com matrizes de indicadores** busca ajustar fun√ß√µes lineares separadamente para cada classe, sem modelar diretamente as densidades condicionais das classes ou impor suposi√ß√µes sobre as matrizes de covari√¢ncia [^4.2]. Ao contr√°rio do LDA, que assume covari√¢ncias iguais, e do QDA, que assume distribui√ß√µes gaussianas mas permite covari√¢ncias diferentes, a regress√£o linear n√£o imp√µe restri√ß√µes sobre as distribui√ß√µes dos dados. A regress√£o linear ajusta modelos independentes para cada classe usando m√≠nimos quadrados, e a decis√£o de classifica√ß√£o √© tomada atribuindo a observa√ß√£o √† classe com maior sa√≠da ajustada.

A abordagem da regress√£o linear n√£o leva √† forma√ß√£o de fronteiras de decis√£o quadr√°ticas, como o QDA, uma vez que o ajuste dos coeficientes √© feito atrav√©s da minimiza√ß√£o da soma dos quadrados dos erros, o que resulta em fun√ß√µes lineares e fronteiras de decis√£o lineares. A regress√£o linear n√£o utiliza informa√ß√µes sobre a estrutura da matriz de covari√¢ncia e n√£o busca modelar as densidades condicionais como o LDA ou o QDA [^4.2].

Enquanto o LDA e QDA buscam modelos que se baseiam nas propriedades distribucionais dos dados, a regress√£o linear com matrizes de indicadores busca apenas minimizar a dist√¢ncia entre a resposta predita e o target da classe, atrav√©s de fun√ß√µes lineares separadas para cada classe, sem considerar explicitamente a forma das densidades condicionais ou a probabilidade posterior [^4.2].

> üí° **Exemplo Num√©rico:**
>
> Considere um problema de classifica√ß√£o bin√°ria com duas classes, onde a classe 1 √© representada por $y=1$ e a classe 2 por $y=0$. Suponha que temos um √∫nico preditor $x$ e o seguinte conjunto de dados:
>
> | x    | y |
> |------|---|
> | 1    | 0 |
> | 2    | 0 |
> | 3    | 1 |
> | 4    | 1 |
>
> Podemos ajustar dois modelos de regress√£o linear separadamente, um para cada classe. Para a classe 1 (y=1), o modelo seria $f_1(x) = \beta_{01} + \beta_{11}x$, e para a classe 2 (y=0), o modelo seria $f_2(x) = \beta_{02} + \beta_{12}x$.
>
> Utilizando o m√©todo dos m√≠nimos quadrados para ajustar esses modelos, podemos obter os seguintes resultados (este √© um exemplo simplificado, em geral, precisar√≠amos usar um pacote estat√≠stico para calcular os coeficientes):
>
> $f_1(x) = -1 + 0.7x$
> $f_2(x) = 0 + 0.1x$
>
> A decis√£o de classifica√ß√£o seria feita comparando $f_1(x)$ e $f_2(x)$. Se $f_1(x) > f_2(x)$, classificamos como classe 1, caso contr√°rio, como classe 2. A fronteira de decis√£o seria onde $f_1(x) = f_2(x)$, que neste caso √© uma linha reta.
>
> Visualmente, podemos notar que a regress√£o linear ajusta retas para cada classe, que n√£o necessariamente capturam a estrutura dos dados como o QDA faria se os dados tivessem uma distribui√ß√£o gaussiana com covari√¢ncias diferentes.

**Lemma 2:** *Ao contr√°rio do QDA, a regress√£o linear com matrizes de indicadores n√£o leva √† forma√ß√£o de fronteiras de decis√£o quadr√°ticas, uma vez que n√£o modela as densidades condicionais e estima os par√¢metros de forma independente.* A prova desse lema reside na forma da fun√ß√£o de custo na regress√£o linear e na forma das fun√ß√µes discriminantes no QDA.

**Corol√°rio 2:** *A utiliza√ß√£o da regress√£o linear com matrizes de indicadores para problemas de classifica√ß√£o, ao n√£o considerar as matrizes de covari√¢ncia das classes, n√£o leva √† forma√ß√£o de fronteiras de decis√£o quadr√°ticas, como o QDA, e portanto, n√£o consegue capturar rela√ß√µes mais complexas entre as classes.*  Este corol√°rio estabelece uma distin√ß√£o clara entre as duas abordagens e a forma das fronteiras de decis√£o resultantes.

A regress√£o linear com matrizes de indicadores, portanto, n√£o se conecta diretamente com o conceito de fronteiras de decis√£o quadr√°ticas, uma vez que n√£o modela as densidades condicionais com distribui√ß√µes gaussianas com diferentes covari√¢ncias. O QDA oferece uma forma de relaxar a suposi√ß√£o do LDA, gerando fronteiras de decis√£o n√£o lineares com maior capacidade de modelagem, e melhor ader√™ncia a problemas onde a suposi√ß√£o de igualdade de covari√¢ncia √© claramente violada [^4.3].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Regularization in QDA"
        direction TB
        A["QDA Model"] --> B["Parameter Estimation"]
        B --> C["Regularization Term (L1 or L2)"]
        C --> D["Penalized Objective Function"]
        D --> E["Improved Generalization"]
		E --> F["Feature Selection and Shrinkage"]

    end
	style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#cfc,stroke:#333,stroke-width:2px
	style F fill:#cfc,stroke:#333,stroke-width:2px
```

A **sele√ß√£o de vari√°veis** e a **regulariza√ß√£o** s√£o t√©cnicas importantes para lidar com a complexidade dos modelos e evitar o *overfitting* em problemas de classifica√ß√£o, incluindo aqueles onde as fronteiras de decis√£o s√£o quadr√°ticas, como no QDA. A regulariza√ß√£o, ao adicionar um termo de penalidade √† fun√ß√£o de custo, busca controlar a magnitude dos coeficientes do modelo e melhorar a capacidade de generaliza√ß√£o para novos dados [^4.5].

Na **regress√£o log√≠stica**, a regulariza√ß√£o √© aplicada atrav√©s da modifica√ß√£o da fun√ß√£o de verossimilhan√ßa, de acordo com a formula√ß√£o:

$$
\max_{\beta_0, \beta} \left[ \sum_{i=1}^N \left( y_i (\beta_0 + \beta^T x_i) - \log(1 + e^{\beta_0 + \beta^T x_i}) \right) - \lambda P(\beta) \right]
$$

onde $P(\beta)$ √© a penalidade e $\lambda$ √© o par√¢metro de regulariza√ß√£o. A penalidade **L1** (Lasso), dada por $P(\beta) = \sum_{j=1}^p |\beta_j|$, promove a esparsidade dos coeficientes, selecionando as vari√°veis mais relevantes e simplificando o modelo [^4.4.4]. A penalidade **L2** (Ridge), dada por $P(\beta) = \sum_{j=1}^p \beta_j^2$, reduz a magnitude dos coeficientes, estabilizando o modelo e diminuindo o risco de *overfitting* [^4.5].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo de regress√£o log√≠stica com duas vari√°veis ($x_1$ e $x_2$) e um termo de regulariza√ß√£o L1. A fun√ß√£o de verossimilhan√ßa regularizada √©:
>
> $$
> L(\beta) = \sum_{i=1}^N \left[ y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}) - \log(1 + e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}) \right] - \lambda (|\beta_1| + |\beta_2|)
> $$
>
> Suponha que, sem regulariza√ß√£o ($\lambda=0$), os coeficientes estimados sejam $\beta_0 = -1$, $\beta_1 = 2$ e $\beta_2 = -3$. Agora, vamos aplicar regulariza√ß√£o L1 com $\lambda = 1$. A fun√ß√£o de verossimilhan√ßa regularizada ser√° maximizada por um algoritmo de otimiza√ß√£o, resultando em novos coeficientes. Um poss√≠vel resultado seria, por exemplo, $\beta_0 = -0.8$, $\beta_1 = 1.2$ e $\beta_2 = -2$.
>
> Observe que a regulariza√ß√£o L1 tende a reduzir a magnitude dos coeficientes, e, em casos mais extremos, pode zerar alguns deles (esparsidade). Se, em vez disso, tiv√©ssemos usado a regulariza√ß√£o L2, os coeficientes seriam reduzidos de forma diferente, mas ainda haveria uma redu√ß√£o em suas magnitudes, embora n√£o necessariamente para zero. O efeito da regulariza√ß√£o √© evitar o overfitting, e a escolha de $\lambda$ √© normalmente feita atrav√©s de valida√ß√£o cruzada.
>
> Podemos comparar os resultados com e sem regulariza√ß√£o:
>
> | M√©todo        | $\beta_0$ | $\beta_1$ | $\beta_2$ |
> |---------------|-----------|-----------|-----------|
> | Sem Reg.      | -1        | 2         | -3        |
> | L1 Reg. ($\lambda$=1) | -0.8      | 1.2       | -2        |
>
> A regulariza√ß√£o L1, neste exemplo, reduziu a magnitude dos coeficientes e pode, em outros casos, zerar alguns deles, o que √© √∫til para sele√ß√£o de vari√°veis.

```mermaid
graph LR
    subgraph "L1 and L2 Regularization"
        direction TB
        A["Loss Function"] --> B["L1 Penalty: Œª * Œ£|Œ≤j|"]
        A --> C["L2 Penalty: Œª * Œ£Œ≤j¬≤"]
        B --> D["Sparse Coefficients"]
        C --> E["Coefficient Shrinkage"]
    end
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

A aplica√ß√£o da regulariza√ß√£o √© particularmente importante em modelos mais complexos como o QDA, pois modelos com muitos par√¢metros (como o QDA) podem ser mais suscet√≠veis ao *overfitting* e a problemas de instabilidade. Ao controlar a complexidade e selecionar as vari√°veis mais importantes, a regulariza√ß√£o auxilia na constru√ß√£o de modelos com melhor capacidade de generaliza√ß√£o.

**Lemma 3:** *A penalidade L1, ao induzir esparsidade nos coeficientes, leva a um modelo mais simples e com melhor capacidade de generaliza√ß√£o, mesmo em modelos com fronteiras de decis√£o quadr√°ticas, como o QDA.* Isso ocorre devido ao impacto da penalidade L1 na fun√ß√£o de custo.

**Prova do Lemma 3:** A penalidade L1 adiciona um termo que √© proporcional ao valor absoluto dos coeficientes na fun√ß√£o de custo. A minimiza√ß√£o da fun√ß√£o de custo, com este termo, for√ßa a maioria dos coeficientes menos importantes a se tornarem exatamente zero, o que leva a modelos mais simples e com melhor capacidade de generaliza√ß√£o [^4.4.3], [^4.4.4].  $\blacksquare$

**Corol√°rio 3:** *A regulariza√ß√£o, tanto L1 quanto L2, auxilia no controle do overfitting em modelos de classifica√ß√£o complexos, e a sele√ß√£o de vari√°veis atrav√©s da penalidade L1 melhora a interpretabilidade e a generaliza√ß√£o dos modelos, mesmo quando as fronteiras de decis√£o s√£o quadr√°ticas.* O controle do *overfitting* aumenta a qualidade da estimativa dos par√¢metros e da fronteira de decis√£o.

> ‚ö†Ô∏è **Ponto Crucial**: A regulariza√ß√£o, seja L1 ou L2, desempenha um papel fundamental na constru√ß√£o de modelos de classifica√ß√£o mais robustos, mesmo quando as fronteiras de decis√£o s√£o quadr√°ticas, como no QDA, ou quando o modelo n√£o considera suposi√ß√µes Gaussianas como a regress√£o log√≠stica, e auxilia na sele√ß√£o das vari√°veis mais importantes [^4.5].

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
    subgraph "Separating Hyperplanes and Perceptron"
        direction TB
        A["Linear Decision Boundary: w^T * x + b = 0"] --> B["Maximum Margin"]
        A --> C["Perceptron Iterative Updates"]
        B --> D["Support Vectors"]
        C --> E["Convergence for Linearly Separable Data"]
	end
	style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

A ideia de **hiperplanos separadores** busca encontrar uma fronteira linear que maximize a dist√¢ncia entre as classes, ou seja, a margem de separa√ß√£o. Essa abordagem √© central em modelos como as m√°quinas de vetores de suporte (SVM), onde o objetivo √© encontrar o hiperplano √≥timo que maximize a separa√ß√£o entre as classes, buscando uma fronteira linear que seja a melhor poss√≠vel dentro da fam√≠lia de fun√ß√µes lineares [^4.5.2].

O algoritmo do **Perceptron** busca um hiperplano separador ajustando os par√¢metros do modelo de forma iterativa com base nas classifica√ß√µes incorretas [^4.5.1].  O Perceptron, embora seja uma abordagem mais simples, ilustra a capacidade de um modelo linear para separar classes em problemas que s√£o linearmente separ√°veis.  Em situa√ß√µes onde a separabilidade linear n√£o √© poss√≠vel, o Perceptron n√£o garante converg√™ncia, e o m√©todo se torna inadequado.

> üí° **Exemplo Num√©rico:**
>
> Considere um problema de classifica√ß√£o bin√°ria com duas vari√°veis $x_1$ e $x_2$ e os seguintes dados (linearmente separ√°veis):
>
> - Classe 1: $(1,1), (2,1), (2,2)$
> - Classe 2: $(0,0), (1,0), (0,1)$
>
> O Perceptron busca um hiperplano da forma $w_0 + w_1x_1 + w_2x_2 = 0$. O algoritmo come√ßa com pesos iniciais (por exemplo, $w_0=0, w_1=0, w_2=0$) e itera sobre os dados, atualizando os pesos quando h√° uma classifica√ß√£o incorreta.
>
> Suponha que o Perceptron itera sobre os dados e classifica inicialmente o ponto (1,1) da classe 1 como classe 2 (ou seja, $w_0 + w_1(1) + w_2(1) < 0$). Ent√£o, os pesos s√£o atualizados utilizando a regra de atualiza√ß√£o do Perceptron $w_{new} = w_{old} + \alpha y x$, onde $\alpha$ √© a taxa de aprendizagem e $y$ √© o r√≥tulo da classe (1 ou -1).
>
> Ap√≥s algumas itera√ß√µes, o Perceptron pode encontrar um hiperplano separador, como por exemplo, $-0.5 + 1x_1 + 1x_2 = 0$. Este hiperplano classifica corretamente todos os pontos dos dados.
>
> O Perceptron ilustra como um modelo linear pode separar classes linearmente separ√°veis, mas este modelo n√£o √© adequado para dados n√£o linearmente separ√°veis, onde o QDA poderia ser mais adequado.
>
>  ```mermaid
>  graph LR
>      A[Classe 1: (1,1), (2,1), (2,2)] -->|Positive Side of Hyperplane| C(Hyperplane: -0.5 + x1 + x2 = 0);
>      B[Classe 2: (0,0), (1,0), (0,1)] -->|Negative Side of Hyperplane| C;
>  ```

O conceito de hiperplanos separadores est√° diretamente ligado √† classifica√ß√£o linear, e a sua capacidade de modelar problemas com fronteiras de decis√£o lineares contrasta com o QDA, que gera fronteiras quadr√°ticas, e com a complexidade dos modelos n√£o lineares [^4.3.1].

**Teorema:** *Em um cen√°rio de dados linearmente separ√°veis, o algoritmo do Perceptron converge para um hiperplano separador em um n√∫mero finito de itera√ß√µes.* Este teorema garante a converg√™ncia sob as condi√ß√µes de linearidade, mas n√£o representa uma solu√ß√£o para a n√£o linearidade dos problemas [^4.5.1].

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

**Resposta:**

A **Regra de Decis√£o Bayesiana** busca classificar uma observa√ß√£o $x$ na classe $k$ que maximize a probabilidade posterior $P(G=k|X=x)$ [^4.3]. Sob a suposi√ß√£o de que as classes seguem distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia $\Sigma$, a probabilidade posterior √© dada por:

$$
P(G=k|X=x) = \frac{ \phi(x;\mu_k,\Sigma)\pi_k}{\sum_{l=1}^K \phi(x;\mu_l,\Sigma)\pi_l}
$$

onde $\phi(x;\mu_k,\Sigma)$ √© a densidade gaussiana para a classe $k$, $\mu_k$ √© a m√©dia da classe $k$ e $\pi_k$ √© a probabilidade a priori da classe. O **LDA**, por sua vez, deriva suas fun√ß√µes discriminantes lineares a partir dessas suposi√ß√µes, buscando maximizar a separa√ß√£o entre as classes com base nas m√©dias e na covari√¢ncia compartilhada [^4.3].

```mermaid
graph LR
    subgraph "Bayes Decision Rule vs. LDA"
        direction TB
        A["Bayes Decision Rule: P(G=k|X=x)"] --> B["Maximize Posterior Probability"]
        B --> C["Gaussian Conditional Densities with Equal Covariance"]
        C --> D["Leads to LDA Decision Boundary"]
        E["LDA"] --> F["Derives Linear Discriminant Functions"]
        F --> C
    end
	style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
	style F fill:#ccf,stroke:#333,stroke-width:2px
```

**Lemma 4:** *Sob a suposi√ß√£o de distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia, a regra de decis√£o Bayesiana e o LDA levam √† mesma fronteira de decis√£o linear, e os coeficientes e interceptos utilizados na defini√ß√£o da fronteira s√£o equivalentes.*