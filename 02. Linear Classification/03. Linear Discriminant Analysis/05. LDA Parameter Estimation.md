## T√≠tulo Conciso: Estima√ß√£o de Par√¢metros no LDA, Propor√ß√µes de Classe e Covari√¢ncias Conjuntas

```mermaid
graph LR
    subgraph "LDA Parameter Estimation"
        direction TB
        A["Training Data"]
        B["Estimate Class Proportions: œÄÃÇ_k = N_k / N"]
        C["Estimate Class Means: ŒºÃÇ_k = (1/N_k) Œ£ x_i"]
        D["Estimate Pooled Covariance: Œ£ÃÇ"]
        E["LDA Model Parameters: (œÄÃÇ_k, ŒºÃÇ_k, Œ£ÃÇ)"]
        A --> B
        A --> C
        A --> D
        B & C & D --> E
        style E fill:#ccf,stroke:#333,stroke-width:2px
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora em profundidade o processo de **estima√ß√£o dos par√¢metros** no **Linear Discriminant Analysis (LDA)**, focando em como as **propor√ß√µes de classe** e as **covari√¢ncias conjuntas** s√£o estimadas a partir dos dados de treinamento. Analisaremos como as m√©dias de cada classe e a matriz de covari√¢ncia comum s√£o estimadas, e como essas estimativas se relacionam com as probabilidades a priori das classes. Compararemos a abordagem do LDA com a **regress√£o linear com matrizes de indicadores**, que n√£o imp√µe as mesmas suposi√ß√µes sobre a distribui√ß√£o dos dados, e com a **regress√£o log√≠stica**, que modela diretamente as probabilidades posteriores [^4.2], [^4.4]. Al√©m disso, discutiremos a import√¢ncia da **sele√ß√£o de vari√°veis e regulariza√ß√£o** para melhorar a estabilidade e a capacidade de generaliza√ß√£o dos modelos [^4.4.4], [^4.5]. Abordaremos tamb√©m o conceito de **hiperplanos separadores** e sua rela√ß√£o com a estima√ß√£o de par√¢metros no LDA [^4.5.2]. O objetivo deste cap√≠tulo √© fornecer uma compreens√£o clara de como os par√¢metros do LDA s√£o estimados e como esses par√¢metros afetam as decis√µes de classifica√ß√£o.

### Conceitos Fundamentais

**Conceito 1: Estima√ß√£o dos Par√¢metros no LDA e a Suposi√ß√£o Gaussiana**

O **LDA** assume que as densidades condicionais de classe, $P(X|G=k)$, seguem uma distribui√ß√£o gaussiana multivariada com a mesma matriz de covari√¢ncia $\Sigma$ para todas as classes [^4.3]. Os par√¢metros do LDA a serem estimados a partir dos dados de treinamento s√£o: o vetor de m√©dias $\mu_k$ para cada classe $k$, a matriz de covari√¢ncia comum $\Sigma$, e as probabilidades a priori das classes $\pi_k$. A estimativa desses par√¢metros √© fundamental para a constru√ß√£o do modelo de classifica√ß√£o. A suposi√ß√£o de gaussianidade com covari√¢ncias iguais permite que as estimativas sejam obtidas de forma simples atrav√©s das estat√≠sticas amostrais.

**Lemma 1:** *A suposi√ß√£o de distribui√ß√µes gaussianas com a mesma matriz de covari√¢ncia permite a deriva√ß√£o de estimadores simples para os par√¢metros do LDA usando estat√≠sticas amostrais.* A prova reside na forma da densidade gaussiana multivariada e na aplica√ß√£o do princ√≠pio da m√°xima verossimilhan√ßa.

**Conceito 2: Estimativa das Propor√ß√µes de Classe e das M√©dias Amostrais**

As probabilidades a priori das classes $\pi_k$ s√£o estimadas pelas **propor√ß√µes de classe** nos dados de treinamento, ou seja, $\hat{\pi}_k = N_k/N$, onde $N_k$ √© o n√∫mero de observa√ß√µes da classe $k$ e $N$ √© o n√∫mero total de observa√ß√µes [^4.3]. Os vetores de m√©dias $\mu_k$ s√£o estimados pelas **m√©dias amostrais** de cada classe:

$$
\hat{\mu}_k = \frac{1}{N_k} \sum_{i: g_i = k} x_i
$$

onde a soma √© feita sobre todas as observa√ß√µes da classe $k$. Estas estimativas das m√©dias e das probabilidades a priori s√£o utilizadas na constru√ß√£o das fun√ß√µes discriminantes do LDA. As propor√ß√µes de classe s√£o estimativas n√£o-viesadas das probabilidades a priori, e as m√©dias amostrais s√£o estimativas n√£o-viesadas das m√©dias populacionais (sob condi√ß√µes de amostragem aleat√≥ria).

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados com duas classes (A e B) e as seguintes observa√ß√µes:
>
> Classe A: $x_1 = [2, 3]$, $x_2 = [3, 4]$, $x_3 = [4, 5]$
> Classe B: $x_4 = [6, 7]$, $x_5 = [7, 8]$
>
> O n√∫mero de observa√ß√µes na classe A √© $N_A = 3$, e na classe B √© $N_B = 2$. O n√∫mero total de observa√ß√µes √© $N = 5$.
>
> As propor√ß√µes de classe s√£o:
> $\hat{\pi}_A = N_A / N = 3 / 5 = 0.6$
> $\hat{\pi}_B = N_B / N = 2 / 5 = 0.4$
>
> As m√©dias amostrais s√£o:
> $\hat{\mu}_A = \frac{1}{3} ([2, 3] + [3, 4] + [4, 5]) = [3, 4]$
> $\hat{\mu}_B = \frac{1}{2} ([6, 7] + [7, 8]) = [6.5, 7.5]$
>
> Estas estimativas de $\hat{\pi}_A$, $\hat{\pi}_B$, $\hat{\mu}_A$ e $\hat{\mu}_B$ ser√£o usadas no c√°lculo das fun√ß√µes discriminantes do LDA.

**Corol√°rio 1:** *As estimativas das probabilidades a priori das classes e dos vetores de m√©dias s√£o obtidas diretamente atrav√©s das estat√≠sticas amostrais, representando estimativas n√£o viesadas dos par√¢metros populacionais sob a suposi√ß√£o de amostragem aleat√≥ria.* Isso simplifica o processo de estima√ß√£o de par√¢metros e torna o m√©todo computacionalmente eficiente.

**Conceito 3: Estima√ß√£o da Matriz de Covari√¢ncia Comum (Pooled Covariance)**

A matriz de covari√¢ncia comum $\Sigma$ √© estimada a partir das vari√¢ncias e covari√¢ncias das amostras de todas as classes, calculando-se uma **matriz de covari√¢ncia conjunta**, ou "pooled covariance". A estimativa da matriz de covari√¢ncia comum √© dada por:

$$
\hat{\Sigma} = \frac{1}{N - K} \sum_{k=1}^K \sum_{i: g_i = k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T
$$

onde $N$ √© o n√∫mero total de observa√ß√µes, $K$ √© o n√∫mero de classes e $\hat{\mu}_k$ √© a m√©dia amostral da classe $k$. Esta estimativa representa uma m√©dia ponderada das matrizes de covari√¢ncia de cada classe, onde o peso de cada classe √© proporcional ao n√∫mero de amostras. A suposi√ß√£o de covari√¢ncias iguais √© fundamental para que esta estimativa seja usada para todas as classes [^4.3].

```mermaid
graph LR
    subgraph "Pooled Covariance Estimation"
        direction TB
        A["Data from each class (x_i)"]
        B["Class Means: ŒºÃÇ_k"]
        C["Calculate centered data: (x_i - ŒºÃÇ_k)"]
        D["Calculate Outer Products: (x_i - ŒºÃÇ_k)(x_i - ŒºÃÇ_k)·µÄ"]
        E["Sum of Outer Products across all classes"]
        F["Normalize by (N - K)"]
        G["Pooled Covariance: Œ£ÃÇ"]
        A --> B
        A --> C
        B --> C
        C --> D
        D --> E
        E --> F
        F --> G
    end
```

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, vamos calcular a matriz de covari√¢ncia conjunta.
>
> Primeiro, calculamos as matrizes de covari√¢ncia para cada classe:
>
> Para a classe A:
> $(x_1 - \hat{\mu}_A) = [2, 3] - [3, 4] = [-1, -1]$
> $(x_2 - \hat{\mu}_A) = [3, 4] - [3, 4] = [0, 0]$
> $(x_3 - \hat{\mu}_A) = [4, 5] - [3, 4] = [1, 1]$
> $\Sigma_A = \frac{1}{3-1} \left( [-1, -1]^T[-1, -1] + [0,0]^T[0,0] + [1, 1]^T[1, 1] \right) = \frac{1}{2} \begin{bmatrix} 2 & 2 \\ 2 & 2 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$
>
> Para a classe B:
> $(x_4 - \hat{\mu}_B) = [6, 7] - [6.5, 7.5] = [-0.5, -0.5]$
> $(x_5 - \hat{\mu}_B) = [7, 8] - [6.5, 7.5] = [0.5, 0.5]$
> $\Sigma_B = \frac{1}{2-1} \left( [-0.5, -0.5]^T[-0.5, -0.5] + [0.5, 0.5]^T[0.5, 0.5] \right) = \begin{bmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{bmatrix}$
>
> Agora calculamos a matriz de covari√¢ncia conjunta:
> $\hat{\Sigma} = \frac{1}{5 - 2} \left( \sum_{i \in A} (x_i - \hat{\mu}_A)(x_i - \hat{\mu}_A)^T + \sum_{i \in B} (x_i - \hat{\mu}_B)(x_i - \hat{\mu}_B)^T \right) $
>
> $\hat{\Sigma} = \frac{1}{3} \left( (3-1)\Sigma_A + (2-1)\Sigma_B \right) = \frac{1}{3} \left( 2 \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} + 1 \begin{bmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{bmatrix} \right) = \frac{1}{3} \begin{bmatrix} 2.5 & 2.5 \\ 2.5 & 2.5 \end{bmatrix} = \begin{bmatrix} 5/6 & 5/6 \\ 5/6 & 5/6 \end{bmatrix} $

> Esta √© a matriz de covari√¢ncia conjunta que ser√° utilizada no LDA.

> ‚ö†Ô∏è **Nota Importante**: A estimativa dos par√¢metros no LDA utiliza as estat√≠sticas amostrais (propor√ß√µes, m√©dias e covari√¢ncia conjunta) para obter estimativas n√£o viesadas dos par√¢metros populacionais.

> ‚ùó **Ponto de Aten√ß√£o**: A estimativa da matriz de covari√¢ncia conjunta depende da suposi√ß√£o de que as classes compartilham a mesma matriz de covari√¢ncia, o que pode n√£o ser v√°lido em todos os casos.

> ‚úîÔ∏è **Destaque**: A combina√ß√£o das propor√ß√µes de classe, m√©dias amostrais e matriz de covari√¢ncia comum permite a estimativa dos par√¢metros do LDA, o que leva a fun√ß√µes discriminantes lineares e a fronteiras de decis√£o lineares.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
    subgraph "Comparison of LDA and Linear Regression"
        direction LR
        A["LDA"] --> B["Estimates Œº_k, Œ£"]
        B --> C["Models Conditional Densities P(X|G=k)"]
        C --> E["Uses Pooled Covariance Œ£ÃÇ"]
        F["Linear Regression with Indicator Matrices"] --> G["Estimates Œ≤_k directly"]
        G --> H["Fits models f_k(x) independently"]
        H --> I["No common covariance matrix"]
         E --"Gaussian Assumption"--> J
         I --"No Assumption"--> K
        J -->L ["Linear Decision Boundaries"]
        K --> M ["Potentially complex boundaries"]
    end
```

Na **regress√£o linear com matrizes de indicadores**, o ajuste dos modelos lineares $f_k(x) = \beta_{k0} + \beta_k^T x$ para cada classe √© realizado de forma independente, utilizando o m√©todo dos m√≠nimos quadrados, sem a necessidade de modelar as densidades condicionais de classe de forma expl√≠cita e sem a suposi√ß√£o de igualdade das matrizes de covari√¢ncia [^4.2]. As estimativas dos coeficientes $\beta_{k0}$ e $\beta_k$ s√£o obtidas minimizando a soma dos quadrados das diferen√ßas entre os valores preditos e os valores observados, e n√£o atrav√©s de estimativas das m√©dias e covari√¢ncias como no LDA.

O m√©todo dos m√≠nimos quadrados na regress√£o linear busca minimizar a fun√ß√£o de custo, dada por:

$$
\min_{\beta_{k0}, \beta_k} \sum_{i=1}^N (y_{ik} - (\beta_{k0} + \beta_k^T x_i))^2
$$

onde $y_{ik}$ √© o indicador da classe $k$ para a observa√ß√£o $i$, o que leva a uma solu√ß√£o com um conjunto de par√¢metros separados para cada classe [^4.2]. A regress√£o linear, portanto, n√£o utiliza informa√ß√µes sobre as probabilidades a priori das classes e n√£o modela as densidades condicionais, o que √© um contraste importante com o LDA.

> üí° **Exemplo Num√©rico:**
>
> Vamos usar os mesmos dados do exemplo anterior para ilustrar a regress√£o linear com matrizes de indicadores. Primeiro, criamos as matrizes indicadoras para cada classe.
>
> Observa√ß√µes:
> $x_1 = [2, 3]$, $y_{A1} = 1$, $y_{B1} = 0$
> $x_2 = [3, 4]$, $y_{A2} = 1$, $y_{B2} = 0$
> $x_3 = [4, 5]$, $y_{A3} = 1$, $y_{B3} = 0$
> $x_4 = [6, 7]$, $y_{A4} = 0$, $y_{B4} = 1$
> $x_5 = [7, 8]$, $y_{A5} = 0$, $y_{B5} = 1$
>
> Para a classe A, o modelo de regress√£o linear √©:
> $f_A(x) = \beta_{A0} + \beta_{A1} x_1 + \beta_{A2} x_2$
>
> Para a classe B, o modelo de regress√£o linear √©:
> $f_B(x) = \beta_{B0} + \beta_{B1} x_1 + \beta_{B2} x_2$
>
> Usando o m√©todo dos m√≠nimos quadrados, podemos encontrar os coeficientes $\beta_{A0}$, $\beta_{A1}$, $\beta_{A2}$, $\beta_{B0}$, $\beta_{B1}$ e $\beta_{B2}$.
>
> O resultado do ajuste por m√≠nimos quadrados para este conjunto de dados (usando um software de estat√≠stica ou bibliotecas como scikit-learn) pode ser algo como:
>
> $\hat{\beta}_{A0} \approx -0.667, \hat{\beta}_{A1} \approx 0.333, \hat{\beta}_{A2} \approx 0.333$
> $\hat{\beta}_{B0} \approx -0.333, \hat{\beta}_{B1} \approx 0.167, \hat{\beta}_{B2} \approx 0.167$
>
> Observe que estes par√¢metros s√£o estimados de forma independente para cada classe, e n√£o h√° compartilhamento da matriz de covari√¢ncia como no LDA.
>
> A previs√£o para uma nova observa√ß√£o $x_{new}$ ser√° feita atribuindo √† classe $k$ cujo modelo linear $f_k(x_{new})$ tiver o maior valor.

A compara√ß√£o com a estima√ß√£o de par√¢metros no LDA destaca as diferen√ßas nas abordagens. Enquanto o LDA modela as densidades condicionais como Gaussianas e estima a matriz de covari√¢ncia comum, a regress√£o linear n√£o utiliza essas suposi√ß√µes. A consequ√™ncia dessa diferen√ßa √© que a regress√£o linear pode n√£o ter o mesmo desempenho em situa√ß√µes onde as suposi√ß√µes do LDA s√£o v√°lidas.

**Lemma 2:** *A regress√£o linear com matrizes de indicadores n√£o estima os par√¢metros a partir das m√©dias e da covari√¢ncia conjunta, como faz o LDA, e o ajuste √© feito independentemente para cada classe usando o m√©todo dos m√≠nimos quadrados.* A prova reside na formula√ß√£o da fun√ß√£o objetivo e da deriva√ß√£o dos par√¢metros para cada m√©todo.

**Corol√°rio 2:** *A falta da suposi√ß√£o de igualdade de covari√¢ncias na regress√£o linear com matrizes de indicadores leva a modelos mais flex√≠veis, mas tamb√©m mais suscet√≠veis ao overfitting e com resultados que n√£o necessariamente se comportam como probabilidades.* Essa caracter√≠stica da regress√£o linear, a impede de ser totalmente consistente com a base te√≥rica da teoria da decis√£o.

A regress√£o linear, ao n√£o utilizar estimativas das m√©dias e covari√¢ncias, pode ser menos eficiente em situa√ß√µes onde a suposi√ß√£o de gaussianidade e igualdade das covari√¢ncias se verifica, do que m√©todos como o LDA, que levam essas informa√ß√µes em considera√ß√£o. Em contrapartida, ela pode ser mais robusta em dados onde essas suposi√ß√µes n√£o se aplicam [^4.2], [^4.3].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Regularization Effects on Logistic Regression"
        direction TB
        A["Logistic Regression Cost Function (Without Regularization)"]
        B["Add L1 Penalty: Œª‚àë|Œ≤_j| (Lasso)"]
        C["Add L2 Penalty: Œª‚àëŒ≤_j¬≤ (Ridge)"]
        D["L1 Promotes Sparsity: Forces Some Œ≤_j to 0"]
        E["L2 Reduces Magnitude of Œ≤_j, Stabilizes Model"]
        A --> B
        A --> C
        B --> D
        C --> E
    end
```

A **sele√ß√£o de vari√°veis** e a **regulariza√ß√£o** desempenham um papel fundamental na melhoria da capacidade de generaliza√ß√£o e da estabilidade dos modelos de classifica√ß√£o, incluindo aqueles que estimam par√¢metros como no LDA [^4.5]. A regulariza√ß√£o, em particular, adiciona um termo de penalidade √† fun√ß√£o de custo, restringindo os valores dos coeficientes e evitando o overfitting.

Na **regress√£o log√≠stica**, que modela diretamente a probabilidade posterior, a fun√ß√£o de custo regularizada pode ser expressa como:

$$
\max_{\beta_0, \beta} \left[ \sum_{i=1}^N \left( y_i (\beta_0 + \beta^T x_i) - \log(1 + e^{\beta_0 + \beta^T x_i}) \right) - \lambda P(\beta) \right]
$$

onde $P(\beta)$ √© a penalidade e $\lambda$ √© o par√¢metro de regulariza√ß√£o. A penalidade **L1** (Lasso), dada por $P(\beta) = \sum_{j=1}^p |\beta_j|$, promove a esparsidade nos coeficientes, selecionando as vari√°veis mais relevantes para a modelagem da probabilidade posterior. A penalidade **L2** (Ridge), dada por $P(\beta) = \sum_{j=1}^p \beta_j^2$, reduz a magnitude dos coeficientes, estabilizando o modelo e evitando o *overfitting* [^4.4.4], [^4.5].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo com regress√£o log√≠stica e regulariza√ß√£o L1 (Lasso). Suponha que temos um modelo de regress√£o log√≠stica com duas vari√°veis preditoras ($x_1$ e $x_2$) e um termo de intercepto, e que a fun√ß√£o de custo ap√≥s regulariza√ß√£o L1 √© dada por:
>
> $J(\beta) = -\sum_{i=1}^N \left( y_i \log(\hat{p}_i) + (1-y_i) \log(1-\hat{p}_i) \right) + \lambda (|\beta_1| + |\beta_2|)$
>
> onde $\hat{p}_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}}$ √© a probabilidade prevista para a observa√ß√£o $i$.
>
> Suponha que, sem regulariza√ß√£o ($\lambda = 0$), os coeficientes estimados sejam:
>
> $\beta_0 = 0.5$, $\beta_1 = 2.0$, $\beta_2 = -1.5$
>
> Agora, vamos aplicar a regulariza√ß√£o L1 com $\lambda = 0.8$. O processo de otimiza√ß√£o (que √© realizado numericamente) ir√° encontrar novos valores para os coeficientes, que podem ser, por exemplo:
>
> $\beta_0 = 0.4$, $\beta_1 = 1.2$, $\beta_2 = 0$
>
> Note que o coeficiente $\beta_2$ foi for√ßado a zero, indicando que a vari√°vel $x_2$ foi selecionada para ser removida do modelo.
>
> Se tiv√©ssemos usado regulariza√ß√£o L2 (Ridge) com o mesmo valor de $\lambda$, os coeficientes poderiam ser algo como:
>
> $\beta_0 = 0.45$, $\beta_1 = 1.6$, $\beta_2 = -1.2$
>
> Neste caso, todos os coeficientes s√£o reduzidos em magnitude, mas nenhum foi for√ßado exatamente a zero.

A aplica√ß√£o da regulariza√ß√£o, mesmo em m√©todos como o LDA que utilizam m√©dias e covari√¢ncias para modelar as densidades condicionais, ajuda a controlar a complexidade do modelo, melhorando a sua capacidade de generaliza√ß√£o e reduzindo o risco de *overfitting*.

**Lemma 3:** *A penalidade L1 na regress√£o log√≠stica, ao promover a esparsidade, leva a uma estimativa dos par√¢metros mais simples e com melhor capacidade de generaliza√ß√£o.* Isso ocorre devido ao efeito da penalidade L1 sobre o valor dos coeficientes, for√ßando-os a serem exatamente zero [^4.4.4].

**Prova do Lemma 3:** A penalidade L1 imp√µe uma taxa constante de decr√©scimo nos coeficientes durante o processo de otimiza√ß√£o da fun√ß√£o de custo. O efeito deste termo √© fazer com que alguns dos coeficientes se tornem exatamente zero, o que leva a uma representa√ß√£o mais esparsa do modelo. A esparsidade resulta em modelos mais simples e f√°ceis de interpretar [^4.4.3]. $\blacksquare$

**Corol√°rio 3:** *A regulariza√ß√£o, tanto L1 quanto L2, ajuda a controlar o overfitting, melhorando a estimativa dos par√¢metros nos modelos de classifica√ß√£o linear, e resulta em modelos mais robustos e com maior capacidade de generaliza√ß√£o.* As penalidades levam a estimativas mais est√°veis dos par√¢metros, e por consequ√™ncia a modelos mais robustos.

> ‚ö†Ô∏è **Ponto Crucial**: A regulariza√ß√£o √© uma ferramenta fundamental para melhorar a estabilidade e a capacidade de generaliza√ß√£o dos modelos de classifica√ß√£o linear, incluindo aqueles que estimam par√¢metros com base em estat√≠sticas amostrais como no LDA [^4.5].

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
    subgraph "Hyperplane and Perceptron"
        direction TB
        A["LDA: Hyperplane from means and pooled covariance"]
        B["Perceptron: Iterative search for separating hyperplane"]
        C["Perceptron: Initialize weights"]
        D["Perceptron: Classify data points"]
         E["Perceptron: Update weights based on misclassifications"]
         F["Perceptron: Repeat until convergence"]
        B --> C
        C --> D
        D --> E
        E --> F
        A --"Explicit Formula"--> G
        F --"Iterative Update"--> H
        G--> I["Optimal Linear Boundary"]
        H--> J ["Approximation of Linear Boundary"]
    end
```

A ideia de **hiperplanos separadores** busca encontrar uma fronteira linear que maximize a dist√¢ncia entre as classes, separando as amostras de forma clara, com o objetivo de construir modelos mais robustos e com boa capacidade de generaliza√ß√£o [^4.5.2]. A busca pelo hiperplano separador se relaciona com a estima√ß√£o dos par√¢metros no LDA, pois o hiperplano √≥timo √© derivado a partir das m√©dias das classes e da matriz de covari√¢ncia comum.

O algoritmo do **Perceptron**, por sua vez, busca um hiperplano separador ajustando os par√¢metros do modelo de forma iterativa com base nas classifica√ß√µes incorretas [^4.5.1]. Embora o Perceptron n√£o modele as densidades condicionais explicitamente, ele busca uma solu√ß√£o que separe as classes, e que pode ser vista como uma aproxima√ß√£o para o hiperplano ideal, dada a forma linear do modelo. Se os dados forem linearmente separ√°veis, o Perceptron converge para um hiperplano separador em um n√∫mero finito de itera√ß√µes, mas sem garantir a maximiza√ß√£o da margem.

> üí° **Exemplo Num√©rico:**
>
> Vamos ilustrar o conceito de hiperplano separador e o Perceptron com um exemplo simplificado. Suponha que temos duas classes com os seguintes pontos:
>
> Classe A: $x_1 = [1, 1]$, $x_2 = [2, 1]$
> Classe B: $x_3 = [1, 3]$, $x_4 = [2, 3]$
>
> O objetivo do Perceptron √© encontrar um hiperplano (neste caso, uma linha) que separe as classes. O Perceptron come√ßa com um hiperplano aleat√≥rio e o atualiza iterativamente.
>
> Inicializa√ß√£o: $\beta = [0.1, 0.2]$, $\beta_0 = -0.5$ (valores aleat√≥rios).
>
> Itera√ß√£o 1:
> - Para $x_1$: $0.1*1 + 0.2*1 - 0.5 = -0.2$. Classificado incorretamente como B (deveria ser A). Atualizamos os pesos: $\beta = \beta + \eta x_1$, $\beta_0 = \beta_0 + \eta$, onde $\eta$ √© a taxa de aprendizagem (e.g. $\eta = 0.1$).
> - $\beta = [0.1, 0.2] + 0.1 * [1, 1] = [0.2, 0.3]$, $\beta_0 = -0.5 + 0.1 = -0.4$.
>
> Itera√ß√£o 2:
> - Para $x_2$: $0.2*2 + 0.3*1 - 0.4 = 0.3$. Classificado corretamente como A.
> - Para $x_3$: $0.2*1 + 0.3*3 - 0.4 = 0.7$. Classificado incorretamente como A (deveria ser B). Atualizamos os pesos: $\beta = [0.2, 0.3] - 0.1 * [1, 3] = [0.1, 0]$, $\beta_0 = -0.4 - 0.1 = -0.5$.
>
> Itera√ß√µes seguintes: O algoritmo continua ajustando o hiperplano at√© convergir para uma solu√ß√£o que separe as classes corretamente.
>
> Ao final da converg√™ncia, o Perceptron ter√° encontrado um hiperplano separador.
>
> Este hiperplano separador √© determinado pelos par√¢metros $\beta$ e $\beta_0$. No LDA, o hiperplano separador tamb√©m √© derivado usando as m√©dias das classes e a matriz de covari√¢ncia conjunta, mas de forma n√£o iterativa.
>
> O Perceptron, por ser um m√©todo iterativo, n√£o garante a maximiza√ß√£o da margem, ao contr√°rio do SVM que busca o hiperplano com maior margem.

**Teorema:** *Se o conjunto de dados de treinamento √© linearmente separ√°vel, o algoritmo do Perceptron garante a converg√™ncia para um hiperplano separador em um n√∫mero finito de passos.* Essa propriedade de converg√™ncia sob condi√ß√µes de separabilidade linear √© uma das caracter√≠sticas do algoritmo do Perceptron [^4.5.1].

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

```mermaid
graph LR
    subgraph "LDA vs. Bayesian Decision Rule"
        direction TB
        A["Bayesian Decision Rule: Maximize P(G=k|X=x)"]
        B["P(G=k|X=x) = (œï(x;Œº_k,Œ£)œÄ_k) / Œ£(œï(x;Œº_l,Œ£)œÄ_l)"]
        C["LDA: Linear Discriminant functions derived from Gaussian assumption"]
        D["LDA and Bayesian Rule: Same decision boundary under Gaussian with shared covariance"]
         E["QDA: Different covariance per class results in non-linear boundary"]
        A --> B
        B --> C
        C --> D
        D --> E
    end
```

**Resposta:**

A **Regra de Decis√£o Bayesiana** busca classificar uma observa√ß√£o $x$ na classe $k$ que maximize a probabilidade posterior $P(G=k|X=x)$ [^4.3]. Sob a suposi√ß√£o de que as distribui√ß√µes condicionais s√£o Gaussianas com a mesma matriz de covari√¢ncia $\Sigma$, a probabilidade posterior √© dada por:

$$
P(G=k|X=x) = \frac{ \phi(x;\mu_k,\Sigma)\pi_k}{\sum_{l=1}^K \phi(x;\mu_l,\Sigma)\pi_l}
$$

onde $\phi(x;\mu_k,\Sigma)$ √© a densidade gaussiana da classe $k$, $\mu_k$ √© a m√©dia da classe $k$ e $\pi_k$ √© a probabilidade a priori da classe $k$. O **LDA** deriva suas fun√ß√µes discriminantes lineares diretamente dessas suposi√ß√µes, atrav√©s da an√°lise da log-raz√£o das probabilidades posteriores [^4.3].

**Lemma 4:** *Sob a suposi√ß√£o de distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia, a regra de decis√£o Bayesiana e o LDA levam √† mesma fronteira de decis√£o linear.* A prova √© feita mostrando que o log-ratio das probabilidades posteriores na regra de decis√£o Bayesiana resulta na mesma forma da fun√ß√£o discriminante do LDA. [^4.3]

**Corol√°rio 4:** *A remo√ß√£o da restri√ß√£o de igualdade de covari√¢ncias na regra de decis√£o Bayesiana leva ao Quadratic Discriminant Analysis (QDA), onde a fronteira de decis√£o n√£o √© mais linear e as matrizes de covari√¢ncia s√£o estimadas separadamente para cada classe.* Essa diferen√ßa destaca o impacto das suposi√ß√µes sobre a distribui√ß√£o dos dados no resultado da classifica√ß√£o [^4.3.1], [^4.3.3].

> ‚ö†Ô∏è **Ponto Crucial**: A principal diferen√ßa entre LDA e a regra de decis√£o Bayesiana reside na forma como os m√©todos s√£o derivados e na imposi√ß√£o ou n√£o da restri√ß√£o de covari√¢ncias iguais. O LDA imp√µe esta restri√ß√£o enquanto que, sob a mesma premissa, a regra Bayesiana resulta no mesmo m√©todo. [^4.3]

### Conclus√£o

Neste cap√≠tulo, exploramos em detalhes o processo de estima√ß√£o dos par√¢metros no LDA, enfatizando a import√¢ncia das propor√ß√µes de classe e da covari√¢ncia conjunta na constru√ß√£o do modelo. Analisamos como o LDA utiliza a suposi√ß√£o de gaussianidade e de igualdade de covari√¢ncias para derivar fun√ß√µes discriminantes lineares, e como essas suposi√ß√µes se relacionam com a teoria de decis√£o. Compararmos o LDA com a regress√£o linear com matrizes de indicadores, que n√£o modela as densidades condicionais diretamente, e com a regress√£o log√≠stica, que modela as probabilidades posteriores de forma mais direta. Abordamos tamb√©m como a sele√ß√£o de vari√°veis e a regulariza√ß√£o melhoram a estabilidade dos modelos, e como a busca por hiperplanos separadores se conecta com a estima√ß√£o de par√¢metros no LDA. Ao longo do cap√≠tulo, procuramos fornecer uma compreens√£o clara e aprofundada de como os par√¢metros do LDA s√£o estimados e como esses par√¢metros afetam a qualidade das decis√µes de classifica√ß√£o.

### Footnotes

[^4.1]: *In this chapter we revisit the classification problem and focus on linear methods for classification...There are several different ways in which linear decision boundaries can be found.* *(Trecho de Linear Methods for Classification)*

[^4.2]: *In Chapter 2 we fit linear regression models to the class indicator variables, and classify to the largest fit...Linear inequalities in this space are quadratic inequalities in the original space.* *(Trecho de Linear Methods for Classification)*

[^4.3]: *Decision theory for classification (Section 2.4) tells us that we need to know the class posteriors Pr(G|X) for optimal classification. Suppose fk(x) is the class-conditional density of X in class G = k, and let œÄŒ∫ be the prior probability of class k... Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Œ£k = Œ£.* *(Trecho de Linear Methods for Classification)*

[^4.3.1]: *The decision boundary between each pair of classes k and l is described by a quadratic equation {x: Œ¥Œ∫(x) = Œ¥(x)}.* *(Trecho de Linear Methods for Classification)*

[^4.3.3]: *In the special case when we assume that the classes have a common covariance matrix...When the classes are really Gaussian, then LDA is optimal* *(Trecho de Linear Methods for Classification)*

[^4.4]: *The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0,1].* *(Trecho de Linear Methods for Classification)*

[^4.4.1]: *Logistic regression models are usually fit by maximum likelihood... The logistic regression model is more general, in that it makes less assumptions.* *(Trecho de Linear Methods for Classification)*

[^4.4.2]: *It is convenient to code the two-class gi via a 0/1 response Yi, where yi = 1 when gi = 1, and yi = 0 when gi = 2... Typically many models are fit in a search for a parsimonious model involving a subset of the variables.* *(Trecho de Linear Methods for Classification)*

[^4.4.3]: *To maximize the log-likelihood, we set its derivatives to zero. These score equations are...To solve the score equations (4.21), we use the Newton-Raphson algorithm...* *(Trecho de Linear Methods for Classification)*

[^4.4.4]: *The L1 penalty used in the lasso (Section 3.4.2) can be used for variable selection and shrinkage with any linear regression model...As with the lasso, we typically do not penalize the intercept term.* *(Trecho de Linear Methods for Classification)*

[^4.5]: *Here we present an analysis of binary data to illustrate the traditional statistical use of the logistic regression model... With two classes there is a simple correspondence between linear discriminant analysis and classification by linear least squares, as in (4.5).* *(Trecho de Linear Methods for Classification)*

[^4.5.1]: *The perceptron learning algorithm tries to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary.* *(Trecho de Linear Methods for Classification)*

[^4.5.2]: *The optimal separating hyperplane separates the two classes and maximizes the distance to the closest point