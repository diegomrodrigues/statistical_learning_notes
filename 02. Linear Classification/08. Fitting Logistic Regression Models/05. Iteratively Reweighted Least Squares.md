## Classifica√ß√£o Linear e Otimiza√ß√£o: Algoritmo Iterativo de M√≠nimos Quadrados Re-ponderados (IRLS) e Regress√£o Log√≠stica

```mermaid
graph LR
    subgraph "IRLS Algorithm Overview"
        direction TB
        A["Start with initial parameters Œ≤"]
        B["Calculate 'p' (probabilities) using logistic function"]
        C["Calculate diagonal weight matrix 'W'"]
        D["Calculate adjusted response 'z'"]
        E["Solve weighted least squares for updated Œ≤"]
        F["Check for Convergence"]
        G["Update 'Œ≤'"]
        A --> B
        B --> C
        C --> D
        D --> E
        E --> F
        F -- "Not Converged" --> G
        G --> B
        F -- "Converged" --> H["End with optimized parameters Œ≤"]
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora em profundidade o **algoritmo iterativo de m√≠nimos quadrados re-ponderados (IRLS)** e sua aplica√ß√£o na otimiza√ß√£o dos par√¢metros em modelos de classifica√ß√£o linear, particularmente na **regress√£o log√≠stica**. Analisaremos como o IRLS, atrav√©s da utiliza√ß√£o de um processo iterativo, aproxima a solu√ß√£o do problema de maximiza√ß√£o da verossimilhan√ßa na regress√£o log√≠stica, e como o m√©todo de m√≠nimos quadrados √© utilizado a cada itera√ß√£o para obter uma resposta ajustada e a atualiza√ß√£o dos par√¢metros.  Compararemos o IRLS com a **regress√£o linear com matrizes de indicadores**, que n√£o utiliza a mesma abordagem de otimiza√ß√£o iterativa [^4.2], e com o **Linear Discriminant Analysis (LDA)** e o **Quadratic Discriminant Analysis (QDA)**, que n√£o se baseiam explicitamente na maximiza√ß√£o da verossimilhan√ßa [^4.3]. Discutiremos como a **sele√ß√£o de vari√°veis e regulariza√ß√£o** podem ser incorporadas ao algoritmo IRLS para melhorar a qualidade do modelo [^4.4.4], [^4.5]. Abordaremos tamb√©m como a busca por **hiperplanos separadores** se relaciona com a itera√ß√£o do IRLS [^4.5.2]. O objetivo deste cap√≠tulo √© fornecer uma compreens√£o detalhada de como o algoritmo IRLS funciona e como ele √© utilizado para estimar os par√¢metros na regress√£o log√≠stica, utilizando a linguagem de m√≠nimos quadrados.

### Conceitos Fundamentais

**Conceito 1: O Algoritmo Iterativo de M√≠nimos Quadrados Re-ponderados (IRLS)**

O **algoritmo iterativo de m√≠nimos quadrados re-ponderados (IRLS)** √© um m√©todo iterativo para encontrar os par√¢metros $\beta$ que maximizam a fun√ß√£o de log-verossimilhan√ßa na regress√£o log√≠stica.  O IRLS transforma o problema de otimiza√ß√£o n√£o linear em uma sequ√™ncia de problemas de m√≠nimos quadrados ponderados, que s√£o resolvidos de forma iterativa [^4.4.1]. Em cada itera√ß√£o, o IRLS calcula uma resposta ajustada $z$ e utiliza essa resposta para atualizar os par√¢metros do modelo, utilizando a solu√ß√£o dos m√≠nimos quadrados ponderados.  Essa abordagem permite resolver o problema de otimiza√ß√£o da regress√£o log√≠stica de forma eficiente, e faz uso de ferramentas de √°lgebra linear.

**Lemma 1:** *O algoritmo IRLS transforma o problema de otimiza√ß√£o n√£o linear da regress√£o log√≠stica em uma sequ√™ncia de problemas de m√≠nimos quadrados ponderados, tornando a resolu√ß√£o computacionalmente eficiente.* A demonstra√ß√£o da rela√ß√£o entre IRLS e a regress√£o log√≠stica se d√° atrav√©s da an√°lise da derivada da fun√ß√£o de log-verossimilhan√ßa.

**Conceito 2:  A Resposta Ajustada e os Pesos na Regress√£o Log√≠stica com IRLS**

Na regress√£o log√≠stica, o algoritmo IRLS calcula a **resposta ajustada** $z$ em cada itera√ß√£o como:

$$
z = X\beta^{old} + W^{-1}(y-p)
$$

onde $X$ √© a matriz de preditores (incluindo a coluna de interceptos), $\beta^{old}$ s√£o os par√¢metros da itera√ß√£o anterior, $y$ √© o vetor de respostas bin√°rias (0 ou 1), $p$ √© o vetor de probabilidades estimadas para cada observa√ß√£o na itera√ß√£o anterior e $W$ √© uma matriz diagonal de pesos que √© obtida atrav√©s da fun√ß√£o log√≠stica, com elementos dados por $p_i(1-p_i)$, onde $p_i$ √© a probabilidade estimada da i-√©sima observa√ß√£o [^4.4.1].  Os pesos $W$ s√£o utilizados para ponderar as observa√ß√µes no ajuste do modelo, e as observa√ß√µes com probabilidade mais pr√≥xima de 0 ou de 1 t√™m pesos menores, enquanto observa√ß√µes com probabilidades mais pr√≥ximas de 0.5 t√™m pesos maiores.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um cen√°rio simplificado com 3 observa√ß√µes e uma √∫nica vari√°vel preditora. Suponha que ap√≥s a primeira itera√ß√£o do IRLS, tenhamos:
>
> - Matriz de preditores $X$:
> ```
> X = np.array([[1, 2],
>               [1, 3],
>               [1, 5]])
> ```
> (A primeira coluna √© o intercepto, e a segunda √© a vari√°vel preditora)
>
> - Par√¢metros da itera√ß√£o anterior $\beta^{old}$:
> ```
> beta_old = np.array([0.5, 0.2])
> ```
>
> - Respostas bin√°rias $y$:
> ```
> y = np.array([0, 1, 0])
> ```
>
> - Probabilidades estimadas $p$:
> ```
> p = np.array([0.3, 0.6, 0.8])
> ```
>
> - Matriz de pesos $W$:
> ```
> W = np.diag(p * (1 - p))
> W = np.array([[0.21, 0,    0   ],
>              [0,    0.24, 0    ],
>              [0,    0,    0.16 ]])
> ```
>
> Agora, podemos calcular a resposta ajustada $z$:
>
> $z = X\beta^{old} + W^{-1}(y-p)$
>
> Primeiro, calculamos $X\beta^{old}$:
>
> ```python
> import numpy as np
>
> X = np.array([[1, 2], [1, 3], [1, 5]])
> beta_old = np.array([0.5, 0.2])
> y = np.array([0, 1, 0])
> p = np.array([0.3, 0.6, 0.8])
> W = np.diag(p * (1 - p))
>
> X_beta_old = np.dot(X, beta_old)
> print(f"X * beta_old = {X_beta_old}")
> ```
>
> Resultado:
> ```
> X * beta_old = [0.9 1.1 1.5]
> ```
>
> Em seguida, calculamos $W^{-1}(y-p)$:
>
> ```python
> W_inv = np.linalg.inv(W)
> y_minus_p = y - p
> W_inv_y_minus_p = np.dot(W_inv, y_minus_p)
> print(f"W^-1 * (y - p) = {W_inv_y_minus_p}")
> ```
>
> Resultado:
> ```
> W^-1 * (y - p) = [-1.42857143  1.66666667 -5.        ]
> ```
>
> Finalmente, somamos os dois termos para obter $z$:
>
> ```python
> z = X_beta_old + W_inv_y_minus_p
> print(f"z = {z}")
> ```
>
> Resultado:
> ```
> z = [-0.52857143  2.76666667 -3.5       ]
> ```
>
> A resposta ajustada $z$ √© ent√£o usada para atualizar os par√¢metros $\beta$ na pr√≥xima itera√ß√£o do IRLS. Note que os pesos em $W$ (0.21, 0.24 e 0.16) variam de acordo com a probabilidade estimada, e isso influencia o valor da resposta ajustada $z$.

```mermaid
graph LR
    subgraph "Adjusted Response 'z' Calculation"
        direction TB
        A["'X' (Predictor Matrix)"]
        B["'Œ≤_old' (Previous Parameters)"]
        C["'XŒ≤_old'"]
        D["'y' (Binary Responses)"]
        E["'p' (Estimated Probabilities)"]
        F["'y-p'"]
        G["'W' (Weight Matrix)"]
        H["'W^-1'"]
        I["'W^-1(y-p)'"]
        J["'z' = 'XŒ≤_old' + 'W^-1(y-p)'"]
        A & B --> C
        D & E --> F
        F & G --> H
        H --> I
        C & I --> J
    end
```

**Corol√°rio 1:** *O algoritmo IRLS utiliza os pesos $W$ e a resposta ajustada $z$ para calcular uma solu√ß√£o de m√≠nimos quadrados ponderados que aproxima a solu√ß√£o do problema de maximiza√ß√£o da log-verossimilhan√ßa na regress√£o log√≠stica.* Este corol√°rio destaca o papel dos pesos e da resposta ajustada na itera√ß√£o do IRLS.

**Conceito 3:  Itera√ß√µes e a Converg√™ncia do IRLS**

O algoritmo IRLS √© um m√©todo iterativo, e a cada itera√ß√£o, os par√¢metros $\beta$ s√£o atualizados atrav√©s da resolu√ß√£o do problema de m√≠nimos quadrados ponderados:

$$
\beta^{new} = \arg \min_{\beta} (z-X\beta)^T W (z - X\beta)
$$

O algoritmo itera at√© que a solu√ß√£o convirja para um ponto de m√°ximo da fun√ß√£o de log-verossimilhan√ßa, ou seja, que as diferen√ßas nos valores dos par√¢metros de uma itera√ß√£o para a seguinte seja menor que um determinado valor. Embora a converg√™ncia nem sempre seja garantida para todos os problemas, o IRLS √© um m√©todo eficiente e amplamente utilizado para estimar os par√¢metros da regress√£o log√≠stica [^4.4.1].

> ‚ö†Ô∏è **Nota Importante**: O algoritmo IRLS transforma o problema de otimiza√ß√£o n√£o linear da regress√£o log√≠stica em uma sequ√™ncia de problemas de m√≠nimos quadrados ponderados, o que possibilita encontrar a solu√ß√£o iterativamente.

> ‚ùó **Ponto de Aten√ß√£o**: A converg√™ncia do algoritmo IRLS nem sempre √© garantida, e pode depender da inicializa√ß√£o dos par√¢metros e da estrutura dos dados.

> ‚úîÔ∏è **Destaque**: O algoritmo IRLS √© uma ferramenta fundamental para estimar os par√¢metros da regress√£o log√≠stica, e utiliza um procedimento iterativo que se baseia no m√©todo dos m√≠nimos quadrados ponderados.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
    subgraph "Comparison of Logistic Regression and Linear Regression"
        direction LR
        subgraph "Logistic Regression (IRLS)"
            direction TB
            A["Maximize Log-Likelihood"]
            B["Uses Logistic Function & Log-Odds"]
            C["Iterative IRLS Algorithm"]
            A --> B
            B --> C
        end
        subgraph "Linear Regression with Indicator Matrices"
           direction TB
           D["Minimize Sum of Squared Errors"]
           E["Direct Linear Fit to Each Class"]
           F["Analytical Solution"]
           D --> E
           E --> F
        end
    end
```

A **regress√£o linear com matrizes de indicadores**, ao contr√°rio da regress√£o log√≠stica que utiliza o algoritmo IRLS, n√£o se baseia na maximiza√ß√£o da verossimilhan√ßa para a estima√ß√£o de par√¢metros [^4.2]. Na regress√£o linear, o objetivo √© ajustar modelos lineares para cada classe, atrav√©s da minimiza√ß√£o da soma dos quadrados dos erros, o que leva a uma solu√ß√£o anal√≠tica para os par√¢metros do modelo:

$$
\min_{\beta_{k0}, \beta_k} \sum_{i=1}^N (y_{ik} - (\beta_{k0} + \beta_k^T x_i))^2
$$

onde $y_{ik}$ √© o valor do indicador para a observa√ß√£o $i$ na classe $k$, e $\beta_{k0}$ e $\beta_k$ s√£o os par√¢metros a serem estimados. A regress√£o linear, portanto, n√£o utiliza nenhum processo iterativo como o IRLS para estimar os seus par√¢metros. Essa diferen√ßa na forma da estima√ß√£o dos par√¢metros √© uma das caracter√≠sticas que distingue a regress√£o linear da regress√£o log√≠stica.

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o mesmo conjunto de dados do exemplo anterior, mas agora para regress√£o linear com matrizes de indicadores.  Como temos apenas duas classes (0 e 1), vamos criar uma matriz de indicadores $Y$ onde a coluna representa a classe 1.
>
> - Matriz de preditores $X$:
> ```
> X = np.array([[1, 2],
>               [1, 3],
>               [1, 5]])
> ```
>
> - Respostas bin√°rias $y$:
> ```
> y = np.array([0, 1, 0])
> ```
>
> A matriz de indicadores $Y$ ser√° igual ao vetor $y$.
>
> Para encontrar os par√¢metros $\beta$ usando m√≠nimos quadrados, resolvemos:
>
> $\beta = (X^TX)^{-1}X^Ty$
>
> Primeiro, calculamos $X^T X$:
>
> ```python
> X_transpose = X.T
> X_transpose_X = np.dot(X_transpose, X)
> print(f"X^T * X = \n{X_transpose_X}")
> ```
>
> Resultado:
> ```
> X^T * X =
> [[ 3 10]
> [10 38]]
> ```
>
> Em seguida, calculamos a inversa de $(X^T X)$:
>
> ```python
> X_transpose_X_inv = np.linalg.inv(X_transpose_X)
> print(f"(X^T * X)^-1 = \n{X_transpose_X_inv}")
> ```
>
> Resultado:
> ```
> (X^T * X)^-1 =
> [[ 1.1875 -0.3125]
> [-0.3125  0.09375]]
> ```
>
> Agora, calculamos $X^Ty$:
>
> ```python
> X_transpose_y = np.dot(X_transpose, y)
> print(f"X^T * y = {X_transpose_y}")
> ```
>
> Resultado:
> ```
> X^T * y = [1 3]
> ```
>
> Finalmente, calculamos $\beta$:
>
> ```python
> beta = np.dot(X_transpose_X_inv, X_transpose_y)
> print(f"beta = {beta}")
> ```
>
> Resultado:
> ```
> beta = [-0.1875  0.3125]
> ```
>
> Os par√¢metros $\beta$ obtidos por m√≠nimos quadrados s√£o diferentes dos par√¢metros obtidos por IRLS na regress√£o log√≠stica. A regress√£o linear busca minimizar a soma dos quadrados dos erros diretamente, enquanto a regress√£o log√≠stica usa um processo iterativo para maximizar a verossimilhan√ßa.

A regress√£o linear com matrizes de indicadores n√£o utiliza o log-odds, a fun√ß√£o log√≠stica ou um procedimento iterativo de estima√ß√£o como o IRLS, e por isso, apresenta limita√ß√µes na modelagem das probabilidades posteriores e na utiliza√ß√£o da teoria de decis√£o.

**Lemma 2:** *A regress√£o linear com matrizes de indicadores n√£o utiliza o algoritmo IRLS para a estima√ß√£o dos par√¢metros, e a minimiza√ß√£o da soma de quadrados dos erros leva a uma solu√ß√£o anal√≠tica, ao contr√°rio da abordagem iterativa da regress√£o log√≠stica.*  A prova desse lema est√° na formula√ß√£o do m√©todo dos m√≠nimos quadrados, que leva a uma solu√ß√£o direta para os par√¢metros e n√£o a um processo iterativo.

**Corol√°rio 2:** *A aus√™ncia do uso do algoritmo IRLS e da maximiza√ß√£o da verossimilhan√ßa na regress√£o linear com matrizes de indicadores torna o modelo mais simples do ponto de vista computacional, mas tamb√©m limita a sua capacidade de modelar as probabilidades posteriores com a mesma precis√£o e calibra√ß√£o que a regress√£o log√≠stica.* Este corol√°rio destaca como a diferen√ßa no m√©todo de estima√ß√£o de par√¢metros impacta as propriedades dos modelos.

A regress√£o linear com matrizes de indicadores, portanto, ao n√£o utilizar um processo iterativo como o IRLS e ao n√£o se basear na maximiza√ß√£o da verossimilhan√ßa, se distingue da regress√£o log√≠stica e de outros modelos que se conectam com a teoria da decis√£o e que utilizam a fun√ß√£o de log-verossimilhan√ßa como guia para a otimiza√ß√£o dos seus par√¢metros [^4.2], [^4.4.1].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Impact of Regularization on IRLS"
        direction TB
        A["Original Log-Likelihood Function"]
        B["Regularization Penalty (L1 or L2)"]
        C["Regularized Log-Likelihood Function"]
        D["Modified Cost Function"]
        E["Adjusted 'W' (Weights)"]
        F["Adjusted 'z' (Response)"]
        G["IRLS Iteration with Regularization"]
        A --> B
        A & B --> C
        C --> D
        D --> E
        D --> F
        E & F --> G
    end
```

A **sele√ß√£o de vari√°veis** e a **regulariza√ß√£o** s√£o t√©cnicas importantes para complementar o algoritmo IRLS, especialmente em modelos de classifica√ß√£o com um n√∫mero elevado de vari√°veis preditoras. A regulariza√ß√£o, ao adicionar um termo de penalidade √† fun√ß√£o de log-verossimilhan√ßa, busca controlar a magnitude dos coeficientes e evitar o *overfitting*, e tamb√©m estabilizar o processo iterativo do IRLS [^4.5].

Na **regress√£o log√≠stica**, a fun√ß√£o de log-verossimilhan√ßa regularizada √© dada por:

$$
\ell(\beta) = \sum_{i=1}^N \log P(G=g_i|X=x_i; \beta) - \lambda P(\beta)
$$

onde $P(\beta)$ √© a penalidade e $\lambda$ √© o par√¢metro de regulariza√ß√£o. A penalidade **L1** (Lasso), dada por $P(\beta) = \sum_{j=1}^p |\beta_j|$, promove a esparsidade dos coeficientes, selecionando as vari√°veis mais relevantes para a modelagem da probabilidade posterior e para a maximiza√ß√£o da log-verossimilhan√ßa [^4.4.4]. A penalidade **L2** (Ridge), dada por $P(\beta) = \sum_{j=1}^p \beta_j^2$, reduz a magnitude dos coeficientes e estabiliza o modelo, facilitando a converg√™ncia do IRLS [^4.5].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo de regulariza√ß√£o L2 (Ridge) aplicada √† regress√£o log√≠stica com o mesmo conjunto de dados. Suponha que, ap√≥s algumas itera√ß√µes do IRLS (sem regulariza√ß√£o), tenhamos os seguintes par√¢metros:
>
> - Par√¢metros $\beta$ (sem regulariza√ß√£o):
> ```
> beta_unreg = np.array([0.5, 0.2])
> ```
>
> Agora, vamos aplicar a regulariza√ß√£o L2 com $\lambda = 0.1$. A fun√ß√£o de custo regularizada se torna:
>
> $ \ell(\beta) = \text{Log-Verossimilhan√ßa} - \lambda \sum_{j=1}^p \beta_j^2 $
>
> Para encontrar os novos par√¢metros $\beta$ com regulariza√ß√£o L2, precisamos modificar a atualiza√ß√£o do IRLS para incorporar a penalidade. O problema de minimiza√ß√£o se torna:
>
>  $\beta^{new} = \arg \min_{\beta} (z-X\beta)^T W (z - X\beta) + \lambda \sum_{j=1}^p \beta_j^2$
>
>  Em termos pr√°ticos, a atualiza√ß√£o dos par√¢metros $\beta$ com regulariza√ß√£o Ridge pode ser feita atrav√©s da seguinte equa√ß√£o:
>
>  $\beta^{new} = (X^T W X + \lambda I)^{-1} X^T W z$
>
>  Onde $I$ √© a matriz identidade e $\lambda$ √© o par√¢metro de regulariza√ß√£o.
>
> Vamos supor que, em uma itera√ß√£o espec√≠fica do IRLS, temos a matriz de preditores $X$, a matriz de pesos $W$ e a resposta ajustada $z$:
>
> ```python
> X = np.array([[1, 2],
>               [1, 3],
>               [1, 5]])
>
> W = np.array([[0.21, 0,    0   ],
>              [0,    0.24, 0    ],
>              [0,    0,    0.16 ]])
>
> z = np.array([-0.52857143,  2.76666667, -3.5       ])
>
> lambda_ridge = 0.1
>
> p = X.shape[1]  # N√∫mero de preditores (incluindo o intercepto)
> I = np.eye(p)   # Matriz identidade
>
> X_transpose = X.T
>
> # Calcula X^T W X
> X_transpose_W = np.dot(X_transpose, W)
> X_transpose_W_X = np.dot(X_transpose_W, X)
>
> # Calcula lambda * I
> lambda_I = lambda_ridge * I
>
> # Calcula (X^T W X + lambda * I)
> X_transpose_W_X_plus_lambda_I = X_transpose_W_X + lambda_I
>
> # Calcula a inversa de (X^T W X + lambda * I)
> inverse_matrix = np.linalg.inv(X_transpose_W_X_plus_lambda_I)
>
> # Calcula X^T W z
> X_transpose_W_z = np.dot(X_transpose_W, z)
>
> # Calcula beta_new
> beta_new_ridge = np.dot(inverse_matrix, X_transpose_W_z)
>
> print(f"beta com regulariza√ß√£o L2 (Ridge) = {beta_new_ridge}")
> ```
>
> Resultado:
> ```
> beta com regulariza√ß√£o L2 (Ridge) = [-0.03674381  0.41134677]
> ```
>
> Observe que os coeficientes de $\beta$ com regulariza√ß√£o L2 s√£o diferentes dos coeficientes sem regulariza√ß√£o. A regulariza√ß√£o L2 encolhe os coeficientes em dire√ß√£o a zero, o que ajuda a reduzir o overfitting e a estabilizar o modelo.

A regulariza√ß√£o, portanto, modifica a fun√ß√£o de log-verossimilhan√ßa e o processo iterativo do IRLS, e leva a modelos mais robustos e com melhor capacidade de generaliza√ß√£o para novos dados.

**Lemma 3:** *A regulariza√ß√£o L1 na regress√£o log√≠stica, ao promover a esparsidade dos coeficientes, altera a forma da fun√ß√£o de log-verossimilhan√ßa e o comportamento do algoritmo IRLS, e com isso, leva a modelos mais simples, com menor n√∫mero de vari√°veis, e melhor capacidade de generaliza√ß√£o.*  A prova deste lema reside na forma da penalidade L1 e em como ela afeta a fun√ß√£o de log-verossimilhan√ßa e o seu ponto de m√°ximo.

**Prova do Lemma 3:**  A penalidade L1, ao adicionar um termo proporcional ao valor absoluto dos coeficientes √† fun√ß√£o de custo, for√ßa os coeficientes menos relevantes a se tornarem exatamente zero, selecionando as vari√°veis mais importantes e tornando a fun√ß√£o mais simples para ser otimizada pelo IRLS [^4.4.3], [^4.4.4]. $\blacksquare$

**Corol√°rio 3:** *A sele√ß√£o de vari√°veis e a regulariza√ß√£o, ao controlarem a complexidade do modelo e a magnitude dos coeficientes, contribuem para uma otimiza√ß√£o mais est√°vel e eficiente da fun√ß√£o de log-verossimilhan√ßa atrav√©s do algoritmo IRLS, e para a obten√ß√£o de modelos mais robustos e com melhor capacidade de generaliza√ß√£o*.  A regulariza√ß√£o afeta a estabilidade da otimiza√ß√£o e o desempenho do modelo.

> ‚ö†Ô∏è **Ponto Crucial**:  A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o ferramentas essenciais para a aplica√ß√£o do algoritmo IRLS, e para controlar a complexidade do modelo, e melhorar a capacidade de generaliza√ß√£o e a estabilidade da otimiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa [^4.5].

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
    subgraph "Hyperplane Search and IRLS Relation"
        direction TB
        A["Goal: Find Separating Hyperplane"]
        B["Perceptron Iterative Adjustment"]
        C["Optimization of Hyperplane Parameters"]
         D["IRLS Optimization in Logistic Regression"]
        E["Maximization of Log-Likelihood"]
        B --> A
        C --> A
        D --> E
        A <--> E
       
    end
```

A busca por **hiperplanos separadores** visa encontrar uma fronteira linear que maximize a separa√ß√£o entre as classes, e essa busca pode ser relacionada com a otimiza√ß√£o da verossimilhan√ßa em modelos como a regress√£o log√≠stica. A ideia √© que a busca por uma fronteira de decis√£o que separe as classes de forma eficiente pode ser vista como uma forma de aproximar a maximiza√ß√£o da probabilidade das classifica√ß√µes observadas.  A utiliza√ß√£o da t√©cnica do Perceptron, embora n√£o maximize explicitamente a verossimilhan√ßa, busca a obten√ß√£o de um hiperplano que separe as classes corretamente, de forma similar ao objetivo da regress√£o log√≠stica [^4.5.2].

O algoritmo do **Perceptron** busca um hiperplano separador atrav√©s do ajuste iterativo de seus par√¢metros com base nas classifica√ß√µes incorretas.  O algoritmo atualiza os par√¢metros de uma fun√ß√£o linear, baseando-se nos dados de treinamento, e o hiperplano resultante tem como objetivo a separa√ß√£o entre as classes. Embora o Perceptron n√£o utilize o log-odds ou a fun√ß√£o log√≠stica, como na regress√£o log√≠stica, a sua abordagem iterativa para a busca de um hiperplano separador pode ser vista como um algoritmo que aproxima a solu√ß√£o de modelos que maximizam a verossimilhan√ßa [^4.5.1].

**Teorema:** *Em problemas de classifica√ß√£o com dados linearmente separ√°veis, o algoritmo do Perceptron converge para um hiperplano separador em um n√∫mero finito de itera√ß√µes, e a busca pelo hiperplano pode ser vista como uma aproxima√ß√£o do problema de otimiza√ß√£o atrav√©s da maximiza√ß√£o da verossimilhan√ßa, que √© utilizado na regress√£o log√≠stica.* A converg√™ncia do Perceptron e a conex√£o com a busca de hiperplanos que separem as classes demonstra a relev√¢ncia desse m√©todo na classifica√ß√£o linear, ainda que n√£o modele as probabilidades de forma direta [^4.5.1].

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

**Resposta:**

A **Regra de Decis√£o Bayesiana** busca classificar uma observa√ß√£o $x$ na classe $k$ que maximize a probabilidade posterior $P(G=k|X=x)$ [^4.3]. Sob a suposi√ß√£o de que as classes seguem distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia $\Sigma$, a probabilidade posterior √© dada por:

$$
P(G=k|X=x) = \frac{ \phi(x;\mu_k,\Sigma)\pi_k}{\sum_{l=1}^K \phi(x;\mu_l,\Sigma)\pi_l}
$$

onde $\phi(x;\mu_k,\Sigma)$ √© a densidade gaussiana da classe $k$, $\mu_k$ √© a m√©dia da classe $k$ e $\pi_k$ √© a probabilidade a priori da classe. O **LDA**, por sua vez, deriva suas fun√ß√µes discriminantes lineares diretamente dessas suposi√ß√µes e busca maximizar a separa√ß√£o entre as classes atrav√©s da constru√ß√£o de fun√ß√µes que se baseiam nas m√©dias e na covari√¢ncia conjunta, que √© uma forma de aproximar a maximiza√ß√£o da probabilidade posterior [^4.3].

**Lemma 4:** *Sob a suposi√ß√£o de distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia, a regra de decis√£o Bayesiana e o LDA s√£o equivalentes, e ambos buscam maximizar a probabilidade posterior da classe, e a forma do hiperplano resultante √© definida pelas m√©dias, pela covari√¢ncia, e pelas probabilidades a priori.*  A equival√™ncia √© demonstrada atrav√©s da manipula√ß√£o alg√©brica, que relaciona o log-ratio das probabilidades posteriores com a fun√ß√£o discriminante do LDA [^4.3].

```mermaid
graph LR
    subgraph "LDA and Bayesian Decision Rule"
        direction TB
        A["Bayesian Decision Rule"]
        B["Maximize Posterior Probability: P(G=k|X=x)"]
        C["Gaussian Distributions with Shared Covariance Œ£"]
        D["Linear Discriminant Analysis (LDA)"]
        E["Maximizes Class Separation based on Means & Covariance"]
        F["Equivalent Results under Gaussian Assumptions"]
        A --> B
        B --> C
        C --> F
        D --> E
        E --> F
    end
```

**Corol√°rio 4:** *Ao relaxar a restri√ß√£o de igualdade de covari√¢ncias na regra de decis√£o Bayesiana, obtemos o QDA (Quadratic Discriminant Analysis), onde a fronteira de decis√£o n√£o √© mais linear e as matrizes de covari√¢ncia s√£o estimadas separadamente para cada classe, e os par√¢metros n√£o s√£o mais obtidos como no LDA, atrav√©s da estima√ß√£o de momentos dos dados.* A forma da fronteira de decis√£o √© mais flex√≠vel no QDA devido √† elimina√ß√£o da premissa de covari√¢ncia igual para todas as classes [^4.3.1], [^4.3.3].

> ‚ö†Ô∏è **Ponto Crucial**: A principal diferen√ßa entre LDA e a regra de decis√£o Bayesiana est√° na forma de obten√ß√£o da fronteira de decis√£o. O LDA imp√µe a restri√ß√£o de covari√¢ncias iguais, e, sob esta suposi√ß√£o, a regra Bayesiana e o LDA levam ao mesmo resultado. O QDA remove a restri√ß√£o de igualdade das covari√¢ncias, resultando em fronteiras n√£o lineares e na aus√™ncia da conex√£o com a deriva√ß√£o da fun√ß√£o discriminante via o Crit√©rio de Fisher. [^4.3]

### Conclus√£o

Neste cap√≠tulo, exploramos a aplica√ß√£o do algoritmo iterativo de m√≠nimos quadrados re-ponderados (IRLS) na regress√£o log√≠stica para a estima√ß√£o de par√¢metros. Analisamos como o IRLS transforma o problema de otimiza√ß√£o da regress√£o log√≠stica em uma sequ√™ncia de problemas de m√≠nimos quadrados ponderados e como a fun√ß√£o de log-verossimilhan√ßa √© utilizada como crit√©rio para o ajuste do modelo. Discutimos como o algoritmo de Newton-Raphson √© utilizado para resolver as score equations, e como a regress√£o linear com matrizes de indicadores n√£o utiliza essa abordagem de estima√ß√£o de par√¢metros. Exploramos a rela√ß√£o entre os hiperplanos separadores e a maximiza√ß√£o da verossimilhan√ßa e analisamos como a sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o importantes para obter modelos mais robustos e com melhor capacidade de generaliza√ß√£o. Atrav√©s deste cap√≠tulo, buscamos fornecer uma vis√£o clara de como o algoritmo IRLS funciona e como ele √© fundamental para a estima√ß√£o dos par√¢metros e para a otimiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa na regress√£o log√≠stica, e como ele se conecta com o processo de tomada de decis√£o.

### Footnotes

[^4.1]: *In this chapter we revisit the classification problem and focus on linear methods for classification...There are several different ways in which linear decision boundaries can be found.*

[^4.2]: *In Chapter 2 we fit linear regression models to the class indicator variables, and classify to the largest fit...Linear inequalities in this space are quadratic inequalities in the original space.*

[^4.3]: *Decision theory for classification (Section 2.4) tells us that we need to know the class posteriors Pr(G|X) for optimal classification. Suppose fk(x) is the class-conditional density of X in class G = k, and let œÄŒ∫ be the prior probability of class k... Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Œ£k = Œ£.*

[^4.3.1]: *The decision boundary between each pair of classes k and l is described by a quadratic equation {x: Œ¥Œ∫(x) = Œ¥(x)}.*

[^4.3.3]: *In the special case when we assume that the classes have a common covariance matrix...When the classes are really Gaussian, then LDA is optimal*

[^4.4]: *The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0,1].*

[^4.4.1]: *Logistic regression models are usually fit by maximum likelihood... The logistic regression model is more general, in that it makes less assumptions.*

[^4.4.2]: *It is convenient to code the two-class gi via a 0/1 response Yi, where yi = 1 when gi = 1, and yi = 0 when gi = 2... Typically many models are fit in a search for a parsimonious model involving a subset of the variables.*

[^4.4.3]: *To maximize the log-likelihood, we set its derivatives to zero. These score equations are...To solve the score equations (4.21), we use the Newton-Raphson algorithm...*

[^4.4.4]: *The L1 penalty used in the lasso (Section 3.4.2) can be used for variable selection and shrinkage with any linear regression model...As with the lasso, we typically do not penalize the intercept term.*

[^4.5]: *Here we present an analysis of binary data to illustrate the traditional statistical use of the logistic regression model... With two classes there is a simple correspondence between linear discriminant analysis and classification by linear least squares, as in (4.5).*

[^4.5.1]: *The perceptron learning algorithm tries to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary.*

[^4.5.2]: *The optimal separating hyperplane separates the two classes and maximizes the distance to the closest point from either class... In light of (4.40), the constraints define an empty slab or margin around the linear decision boundary...*
