## T√≠tulo Conciso: Classifica√ß√£o Linear e Otimiza√ß√£o: Deriva√ß√£o da Hessiana e Atualiza√ß√£o de Par√¢metros com Nota√ß√£o Matricial

```mermaid
graph LR
    subgraph "Newton-Raphson Optimization"
        direction TB
        A["Log-Likelihood Function: '‚Ñì(Œ≤)'"]
        B["Gradient: '‚àÇ‚Ñì(Œ≤)/‚àÇŒ≤'"]
        C["Hessian Matrix: '‚àÇ¬≤‚Ñì(Œ≤)/‚àÇŒ≤‚àÇŒ≤·µÄ'"]
        D["Parameter Update: 'Œ≤_new = Œ≤_old - (H^-1)‚àá‚Ñì'"]
        A --> B
        A --> C
        B & C --> D
        style A fill:#f9f,stroke:#333,stroke-width:2px
        style D fill:#ccf,stroke:#333,stroke-width:2px
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora a fundo a deriva√ß√£o da **matriz Hessiana** e a **atualiza√ß√£o dos par√¢metros** no algoritmo de Newton-Raphson, com √™nfase na utiliza√ß√£o da **nota√ß√£o matricial** para simplificar a representa√ß√£o e o c√°lculo das derivadas e dos passos da otimiza√ß√£o em modelos de classifica√ß√£o linear, particularmente na **regress√£o log√≠stica**. Analisaremos como a fun√ß√£o de log-verossimilhan√ßa, o gradiente e a matriz Hessiana podem ser expressos em nota√ß√£o matricial, facilitando o desenvolvimento e a implementa√ß√£o do algoritmo de Newton-Raphson [^4.4.1]. Compararemos essa abordagem com a **regress√£o linear com matrizes de indicadores**, que n√£o utiliza a mesma formula√ß√£o de otimiza√ß√£o [^4.2], e com o **Linear Discriminant Analysis (LDA)**, onde a otimiza√ß√£o √© feita atrav√©s de estimativas de momentos dos dados e proje√ß√£o em um espa√ßo adequado [^4.3]. Discutiremos tamb√©m a import√¢ncia da **sele√ß√£o de vari√°veis e regulariza√ß√£o** para controlar a complexidade dos modelos e melhorar a estabilidade da otimiza√ß√£o [^4.4.4], [^4.5]. Abordaremos tamb√©m como a busca por **hiperplanos separadores** se conecta com o processo de otimiza√ß√£o por meio do algoritmo de Newton-Raphson [^4.5.2]. O objetivo deste cap√≠tulo √© fornecer uma compreens√£o detalhada de como a matriz Hessiana e a nota√ß√£o matricial s√£o utilizadas para a deriva√ß√£o e a implementa√ß√£o do algoritmo de Newton-Raphson na regress√£o log√≠stica e em outros modelos lineares de classifica√ß√£o.

### Conceitos Fundamentais

**Conceito 1: A Fun√ß√£o de Log-Verossimilhan√ßa e sua Deriva√ß√£o**

A fun√ß√£o de **log-verossimilhan√ßa** (log-likelihood) √© uma medida que quantifica o ajuste do modelo aos dados, e na regress√£o log√≠stica para um problema de classifica√ß√£o bin√°ria, ela pode ser escrita como:

$$
\ell(\beta) = \sum_{i=1}^N \left[ y_i \log p(x_i; \beta) + (1-y_i) \log(1 - p(x_i; \beta)) \right]
$$

onde $y_i$ √© a resposta bin√°ria (0 ou 1), $p(x_i; \beta)$ √© a probabilidade modelada pela regress√£o log√≠stica, e $\beta$ √© o vetor de par√¢metros. A maximiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa busca encontrar os par√¢metros $\beta$ que melhor se ajustam aos dados, e essa otimiza√ß√£o √© realizada atrav√©s do algoritmo de Newton-Raphson [^4.4.1]. Para essa otimiza√ß√£o, √© necess√°rio calcular a primeira e a segunda derivada da fun√ß√£o de verossimilhan√ßa em rela√ß√£o aos par√¢metros $\beta$.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados com 3 observa√ß√µes e uma vari√°vel preditora $x$. Os dados s√£o:
>
> | Observa√ß√£o (i) | $x_i$ | $y_i$ |
> |-----------------|-------|-------|
> | 1               | 1     | 1     |
> | 2               | 2     | 0     |
> | 3               | 3     | 1     |
>
> Inicialmente, vamos supor um vetor de par√¢metros $\beta = [\beta_0, \beta_1]^T = [0.1, 0.2]^T$. A probabilidade $p(x_i; \beta)$ √© dada pela fun√ß√£o log√≠stica:
>
> $$
> p(x_i; \beta) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}}
> $$
>
> Calculando as probabilidades para cada observa√ß√£o:
>
> $p_1 = \frac{1}{1 + e^{-(0.1 + 0.2 * 1)}} = \frac{1}{1 + e^{-0.3}} \approx 0.574$
>
> $p_2 = \frac{1}{1 + e^{-(0.1 + 0.2 * 2)}} = \frac{1}{1 + e^{-0.5}} \approx 0.622$
>
> $p_3 = \frac{1}{1 + e^{-(0.1 + 0.2 * 3)}} = \frac{1}{1 + e^{-0.7}} \approx 0.669$
>
> Agora podemos calcular a log-verossimilhan√ßa:
>
> $\ell(\beta) = [1 * \log(0.574) + (1-1) * \log(1-0.574)] + [0 * \log(0.622) + (1-0) * \log(1-0.622)] + [1 * \log(0.669) + (1-1) * \log(1-0.669)]$
>
> $\ell(\beta) = \log(0.574) + \log(1-0.622) + \log(0.669) \approx -0.555 + -0.970 + -0.402  \approx -1.927$
>
> O objetivo √© encontrar o vetor $\beta$ que maximize essa fun√ß√£o de log-verossimilhan√ßa.

**Lemma 1:** *A fun√ß√£o de log-verossimilhan√ßa √© uma medida da qualidade do ajuste do modelo aos dados, e a sua maximiza√ß√£o busca os par√¢metros que melhor representam as rela√ß√µes entre os dados e os r√≥tulos das classes.* A prova deste lema se encontra na teoria de estima√ß√£o de par√¢metros via maximiza√ß√£o da verossimilhan√ßa.

**Conceito 2: Deriva√ß√£o da Matriz Hessiana e o Gradiente em Nota√ß√£o Matricial**

Para utilizar o algoritmo de Newton-Raphson, √© necess√°rio calcular o gradiente (derivadas de primeira ordem) e a matriz Hessiana (derivadas de segunda ordem) da fun√ß√£o de log-verossimilhan√ßa. Em nota√ß√£o matricial, o gradiente da fun√ß√£o de log-verossimilhan√ßa, pode ser expresso como:

$$
\frac{\partial \ell(\beta)}{\partial \beta} = X^T(y - p)
$$

onde $X$ √© a matriz de dados com cada linha correspondendo a uma observa√ß√£o com o intercepto (uma coluna de 1‚Äôs), $y$ √© o vetor de respostas (0 ou 1) e $p$ √© o vetor de probabilidades preditas pelo modelo. A matriz Hessiana, que √© a matriz das derivadas de segunda ordem, √© dada por:

$$
\frac{\partial^2 \ell(\beta)}{\partial \beta \partial \beta^T} = - X^T W X
$$

onde $W$ √© uma matriz diagonal de pesos, com cada elemento na diagonal sendo dado por $p_i(1-p_i)$ e $p_i$ √© o valor da probabilidade para a observa√ß√£o $i$. A nota√ß√£o matricial facilita a representa√ß√£o e o c√°lculo dessas derivadas, especialmente em modelos com muitas vari√°veis [^4.4.1].

```mermaid
graph LR
    subgraph "Gradient & Hessian in Matrix Notation"
        direction TB
        A["Gradient: '‚àÇ‚Ñì/‚àÇŒ≤ = X·µÄ(y - p)'"]
        B["Hessian: '‚àÇ¬≤‚Ñì/‚àÇŒ≤‚àÇŒ≤·µÄ = -X·µÄWX'"]
        C["Data Matrix: 'X'"]
        D["Response Vector: 'y'"]
        E["Probability Vector: 'p'"]
        F["Weight Matrix: 'W'"]
        C --> A
        D --> A
        E --> A
        C --> B
        F --> B
    end
```

> üí° **Exemplo Num√©rico:**
>
> Usando os mesmos dados do exemplo anterior, vamos construir a matriz $X$, o vetor $y$ e o vetor $p$:
>
> $$
> X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix}, \quad y = \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}, \quad p = \begin{bmatrix} 0.574 \\ 0.622 \\ 0.669 \end{bmatrix}
> $$
>
> Calculando o gradiente:
>
> $$
> \frac{\partial \ell(\beta)}{\partial \beta} = X^T(y - p) = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 1 - 0.574 \\ 0 - 0.622 \\ 1 - 0.669 \end{bmatrix} = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 0.426 \\ -0.622 \\ 0.331 \end{bmatrix} = \begin{bmatrix} 0.135 \\ -0.419 \end{bmatrix}
> $$
>
> Calculando a matriz $W$:
>
> $W = \begin{bmatrix} 0.574 * (1-0.574) & 0 & 0 \\ 0 & 0.622 * (1-0.622) & 0 \\ 0 & 0 & 0.669 * (1-0.669) \end{bmatrix} = \begin{bmatrix} 0.244 & 0 & 0 \\ 0 & 0.235 & 0 \\ 0 & 0 & 0.221 \end{bmatrix}$
>
> Calculando a matriz Hessiana:
>
> $$
> \frac{\partial^2 \ell(\beta)}{\partial \beta \partial \beta^T} = - X^T W X = - \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 0.244 & 0 & 0 \\ 0 & 0.235 & 0 \\ 0 & 0 & 0.221 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix} = - \begin{bmatrix} 0.700 & 1.195 \\ 1.195 & 2.476 \end{bmatrix}
> $$
>
> A matriz Hessiana √© sim√©trica e negativa definida, o que indica que a fun√ß√£o de log-verossimilhan√ßa √© c√¥ncava, e podemos usar o algoritmo de Newton-Raphson para encontrar o m√°ximo.

**Corol√°rio 1:** *A representa√ß√£o do gradiente e da matriz Hessiana em nota√ß√£o matricial simplifica o c√°lculo das derivadas da fun√ß√£o de log-verossimilhan√ßa, e esta representa√ß√£o √© fundamental para a implementa√ß√£o do algoritmo de Newton-Raphson em modelos com muitas vari√°veis.* Este corol√°rio destaca a import√¢ncia da nota√ß√£o matricial na simplifica√ß√£o dos c√°lculos envolvidos na otimiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa.

**Conceito 3: Atualiza√ß√£o dos Par√¢metros e o Algoritmo de Newton-Raphson**

O algoritmo de Newton-Raphson utiliza o gradiente e a matriz Hessiana para atualizar os par√¢metros $\beta$ de forma iterativa, buscando maximizar a fun√ß√£o de log-verossimilhan√ßa. A atualiza√ß√£o dos par√¢metros na regress√£o log√≠stica, utilizando nota√ß√£o matricial, √© dada por:

$$
\beta^{new} = \beta^{old} + (X^T W X)^{-1} X^T (y-p)
$$

Essa atualiza√ß√£o, que √© expressa de forma concisa com nota√ß√£o matricial, permite que os par√¢metros sejam ajustados iterativamente at√© que a solu√ß√£o convirja para um ponto de m√°ximo da fun√ß√£o de log-verossimilhan√ßa [^4.4.1]. A nota√ß√£o matricial, portanto, torna o algoritmo mais eficiente para ser implementado computacionalmente, e tamb√©m facilita a sua compreens√£o te√≥rica.

```mermaid
graph LR
    subgraph "Newton-Raphson Parameter Update"
        direction TB
        A["Old Parameters: 'Œ≤_old'"]
        B["Hessian Inverse: '(X·µÄWX)‚Åª¬π'"]
        C["Gradient: 'X·µÄ(y-p)'"]
        D["New Parameters: 'Œ≤_new'"]
        A & B & C --> D
        style D fill:#ccf,stroke:#333,stroke-width:2px
    end
```

> üí° **Exemplo Num√©rico:**
>
> Usando os resultados do exemplo anterior, vamos calcular a atualiza√ß√£o dos par√¢metros:
>
> Temos:
>
> $\beta^{old} = \begin{bmatrix} 0.1 \\ 0.2 \end{bmatrix}$
>
> $(X^T W X)^{-1} = \begin{bmatrix} 0.700 & 1.195 \\ 1.195 & 2.476 \end{bmatrix}^{-1} \approx \begin{bmatrix} 4.812 & -2.321 \\ -2.321 & 1.358 \end{bmatrix}$
>
> $X^T(y-p) = \begin{bmatrix} 0.135 \\ -0.419 \end{bmatrix}$
>
> Calculando a atualiza√ß√£o:
>
> $\beta^{new} = \beta^{old} - (X^T W X)^{-1} X^T (y-p) =  \begin{bmatrix} 0.1 \\ 0.2 \end{bmatrix} - \begin{bmatrix} 4.812 & -2.321 \\ -2.321 & 1.358 \end{bmatrix} \begin{bmatrix} 0.135 \\ -0.419 \end{bmatrix} =  \begin{bmatrix} 0.1 \\ 0.2 \end{bmatrix} - \begin{bmatrix} 1.644 \\ -0.932 \end{bmatrix} = \begin{bmatrix} -1.544 \\ 1.132 \end{bmatrix}$
>
> Portanto, os novos par√¢metros s√£o $\beta^{new} \approx [-1.544, 1.132]^T$. O algoritmo de Newton-Raphson repetiria esse processo at√© a converg√™ncia.

> ‚ö†Ô∏è **Nota Importante**:  A nota√ß√£o matricial simplifica a representa√ß√£o do gradiente e da matriz Hessiana, o que facilita a implementa√ß√£o do algoritmo de Newton-Raphson para a otimiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa.

> ‚ùó **Ponto de Aten√ß√£o**: O algoritmo de Newton-Raphson utiliza derivadas de primeira e segunda ordem e, portanto, pode ser computacionalmente custoso em modelos com um grande n√∫mero de par√¢metros e pode, em alguns casos, n√£o convergir.

> ‚úîÔ∏è **Destaque**: A deriva√ß√£o da matriz Hessiana e a atualiza√ß√£o dos par√¢metros, atrav√©s da utiliza√ß√£o da nota√ß√£o matricial, s√£o etapas cruciais para a implementa√ß√£o do algoritmo de Newton-Raphson, e para a obten√ß√£o de estimativas dos par√¢metros na regress√£o log√≠stica.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
    subgraph "Comparison of Optimization Approaches"
        direction LR
        A["Logistic Regression"] --> B["Log-Likelihood Maximization"]
        B --> C["Newton-Raphson Algorithm"]
        C --> D["Hessian Matrix"]
        A["Linear Regression"] --> E["Sum of Squares Minimization"]
        E --> F["Analytical Solution"]
         style A fill:#f9f,stroke:#333,stroke-width:2px
         style E fill:#ccf,stroke:#333,stroke-width:2px
    end
```

A **regress√£o linear com matrizes de indicadores**, ao contr√°rio da regress√£o log√≠stica, n√£o utiliza a fun√ß√£o de log-verossimilhan√ßa para a estima√ß√£o dos par√¢metros e, portanto, n√£o deriva a matriz Hessiana e n√£o se beneficia do algoritmo de Newton-Raphson para otimizar a fun√ß√£o de custo [^4.2]. Na regress√£o linear, o objetivo √© minimizar a soma dos quadrados dos erros atrav√©s da seguinte fun√ß√£o de custo:

$$
\min_{\beta_{k0}, \beta_k} \sum_{i=1}^N (y_{ik} - (\beta_{k0} + \beta_k^T x_i))^2
$$

onde $y_{ik}$ √© o indicador da classe $k$ para a observa√ß√£o $i$, o que leva a uma solu√ß√£o anal√≠tica para a estimativa dos par√¢metros que n√£o utiliza nenhum conceito de segunda derivada.  A regress√£o linear, portanto, utiliza a minimiza√ß√£o da soma de quadrados para ajustar os coeficientes de forma independente para cada classe, sem utilizar a fun√ß√£o de log-verossimilhan√ßa, o que resulta em uma abordagem distinta em rela√ß√£o √† estima√ß√£o dos par√¢metros [^4.2].

> üí° **Exemplo Num√©rico:**
>
> Usando os mesmos dados anteriores, e considerando que temos apenas uma classe (a classe "1"), podemos ajustar um modelo de regress√£o linear para prever $y_i$ diretamente. A matriz $X$ e o vetor $y$ s√£o os mesmos:
>
> $$
> X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix}, \quad y = \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}
> $$
>
> A solu√ß√£o para os par√¢metros $\beta$ usando m√≠nimos quadrados √© dada por:
>
> $$
> \beta = (X^T X)^{-1} X^T y
> $$
>
> Calculando $X^T X$:
>
> $$
> X^T X = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix} = \begin{bmatrix} 3 & 6 \\ 6 & 14 \end{bmatrix}
> $$
>
> Calculando $(X^T X)^{-1}$:
>
> $$
> (X^T X)^{-1} = \begin{bmatrix} 3 & 6 \\ 6 & 14 \end{bmatrix}^{-1} = \begin{bmatrix} 3.5 & -1.5 \\ -1.5 & 0.75 \end{bmatrix}
> $$
>
> Calculando $X^T y$:
>
> $$
> X^T y = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 2 \\ 4 \end{bmatrix}
> $$
>
> Calculando $\beta$:
>
> $$
> \beta = (X^T X)^{-1} X^T y = \begin{bmatrix} 3.5 & -1.5 \\ -1.5 & 0.75 \end{bmatrix} \begin{bmatrix} 2 \\ 4 \end{bmatrix} = \begin{bmatrix} 1 \\ -0.5 \end{bmatrix}
> $$
>
> Portanto, os par√¢metros s√£o $\beta_0 = 1$ e $\beta_1 = -0.5$. O modelo de regress√£o linear √© dado por $\hat{y} = 1 - 0.5x$. Note que este modelo n√£o √© um classificador probabil√≠stico e os valores previstos podem ser fora do intervalo [0,1].

A aus√™ncia da deriva√ß√£o da matriz Hessiana e do uso do algoritmo de Newton-Raphson na regress√£o linear com matrizes de indicadores reflete uma diferen√ßa fundamental entre os dois m√©todos na forma como eles abordam a estima√ß√£o dos par√¢metros, e como eles se conectam com a modelagem das probabilidades posteriores.

**Lemma 2:** *A regress√£o linear com matrizes de indicadores n√£o deriva a matriz Hessiana da fun√ß√£o de log-verossimilhan√ßa, e, portanto, n√£o utiliza o algoritmo de Newton-Raphson para otimizar os par√¢metros, ao contr√°rio da regress√£o log√≠stica.* A prova desse lema reside na forma de ajuste dos par√¢metros em cada um dos modelos.

**Corol√°rio 2:** *A aus√™ncia do c√°lculo da matriz Hessiana e do uso do algoritmo de Newton-Raphson na regress√£o linear com matrizes de indicadores simplifica o processo de estima√ß√£o dos par√¢metros, mas tamb√©m limita a capacidade do m√©todo de modelar as probabilidades de forma consistente com a teoria de decis√£o, como √© feita na regress√£o log√≠stica.*  A diferen√ßa na abordagem da estima√ß√£o dos par√¢metros impacta na forma como os modelos s√£o derivados.

A regress√£o linear com matrizes de indicadores, portanto, ao minimizar a soma de quadrados dos erros, e ao n√£o utilizar a deriva√ß√£o da matriz Hessiana e o m√©todo de Newton-Raphson para a estima√ß√£o de par√¢metros, difere fundamentalmente da regress√£o log√≠stica, que busca a maximiza√ß√£o da log-verossimilhan√ßa condicional e utiliza a nota√ß√£o matricial para simplificar o processo de otimiza√ß√£o [^4.2], [^4.4.1].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Regularization Impact on Optimization"
        direction TB
        A["Log-Likelihood: '‚Ñì(Œ≤)'"]
        B["Penalty Term: 'ŒªP(Œ≤)'"]
        C["Regularized Log-Likelihood: '‚Ñì_reg(Œ≤) = ‚Ñì(Œ≤) - ŒªP(Œ≤)'"]
        D["Modified Gradient: '‚àá‚Ñì_reg(Œ≤)'"]
        E["Modified Hessian: 'H_reg(Œ≤)'"]
        A --> C
        B --> C
        C --> D
        C --> E
    end
```

A **sele√ß√£o de vari√°veis** e a **regulariza√ß√£o** s√£o t√©cnicas importantes para complementar o processo de otimiza√ß√£o da fun√ß√£o de log-verossimilhan√ßa condicional em modelos como a regress√£o log√≠stica. A regulariza√ß√£o, ao adicionar um termo de penalidade √† fun√ß√£o de log-verossimilhan√ßa, restringe a magnitude dos coeficientes e auxilia na estabilidade do processo de otimiza√ß√£o e evita o *overfitting*, al√©m de impactar tamb√©m na forma da matriz Hessiana [^4.5].

Na **regress√£o log√≠stica**, a fun√ß√£o de log-verossimilhan√ßa regularizada pode ser expressa como:

$$
\ell(\beta) = \sum_{i=1}^N \log P(G=g_i|X=x_i; \beta) - \lambda P(\beta)
$$

onde $P(\beta)$ √© a penalidade e $\lambda$ √© o par√¢metro de regulariza√ß√£o.  A penalidade **L1** (Lasso) √© dada por $P(\beta) = \sum_{j=1}^p |\beta_j|$, que promove a esparsidade dos coeficientes e torna a fun√ß√£o mais simples, o que pode tamb√©m simplificar o c√°lculo do gradiente e da matriz Hessiana [^4.4.4]. A penalidade **L2** (Ridge) √© dada por $P(\beta) = \sum_{j=1}^p \beta_j^2$, que reduz a magnitude dos coeficientes e estabiliza o modelo, o que facilita a converg√™ncia do algoritmo de Newton-Raphson [^4.5].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar a regress√£o log√≠stica com regulariza√ß√£o L2 (Ridge). A fun√ß√£o de log-verossimilhan√ßa regularizada √©:
>
> $$
> \ell(\beta) = \sum_{i=1}^N \left[ y_i \log p(x_i; \beta) + (1-y_i) \log(1 - p(x_i; \beta)) \right] - \lambda \sum_{j=1}^p \beta_j^2
> $$
>
> Usando os dados anteriores, e considerando $\lambda = 0.1$, e o mesmo valor inicial de $\beta = [0.1, 0.2]^T$:
>
> A fun√ß√£o de log-verossimilhan√ßa regularizada √©:
>
> $\ell(\beta) \approx -1.927 - 0.1 * (0.1^2 + 0.2^2) = -1.927 - 0.005 = -1.932$
>
> O gradiente regularizado √©:
>
> $$
> \frac{\partial \ell(\beta)}{\partial \beta} = X^T(y - p) - 2\lambda\beta
> $$
>
> $\frac{\partial \ell(\beta)}{\partial \beta} = \begin{bmatrix} 0.135 \\ -0.419 \end{bmatrix} - 2 * 0.1 * \begin{bmatrix} 0.1 \\ 0.2 \end{bmatrix} = \begin{bmatrix} 0.135 \\ -0.419 \end{bmatrix} - \begin{bmatrix} 0.02 \\ 0.04 \end{bmatrix} = \begin{bmatrix} 0.115 \\ -0.459 \end{bmatrix}$
>
> A matriz Hessiana regularizada √©:
>
> $$
> \frac{\partial^2 \ell(\beta)}{\partial \beta \partial \beta^T} = - X^T W X - 2\lambda I
> $$
>
> $\frac{\partial^2 \ell(\beta)}{\partial \beta \partial \beta^T} = - \begin{bmatrix} 0.700 & 1.195 \\ 1.195 & 2.476 \end{bmatrix} - 2 * 0.1 * \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = - \begin{bmatrix} 0.900 & 1.195 \\ 1.195 & 2.676 \end{bmatrix}$
>
> A atualiza√ß√£o dos par√¢metros com regulariza√ß√£o L2 seria:
>
> $\beta^{new} = \beta^{old} - (X^T W X + 2\lambda I)^{-1} (X^T (y-p) - 2\lambda\beta)$
>
> Os valores de $\beta$ com regulariza√ß√£o L2 tendem a ser menores em magnitude em compara√ß√£o com o modelo sem regulariza√ß√£o, o que ajuda a evitar o overfitting.

A aplica√ß√£o da regulariza√ß√£o, portanto, impacta n√£o apenas nos valores √≥timos dos par√¢metros, mas tamb√©m nos passos e na estabilidade do algoritmo de Newton-Raphson que √© utilizado para maximizar a log-verossimilhan√ßa.

**Lemma 3:** *A regulariza√ß√£o L1, ao promover a esparsidade, afeta a forma das score equations e da matriz Hessiana, e resulta em modelos mais simples e com melhor capacidade de generaliza√ß√£o.* A prova reside na forma da penalidade L1 e como ela influencia o processo de otimiza√ß√£o.

**Prova do Lemma 3:**  A penalidade L1 adiciona um termo √† fun√ß√£o de custo que √© proporcional ao valor absoluto dos coeficientes. A minimiza√ß√£o dessa fun√ß√£o com o termo da penalidade L1 for√ßa alguns coeficientes a se tornarem exatamente zero, simplificando o c√°lculo das derivadas e tamb√©m reduzindo a complexidade do modelo [^4.4.3], [^4.4.4]. $\blacksquare$

**Corol√°rio 3:** *A sele√ß√£o de vari√°veis e a regulariza√ß√£o, ao controlarem a complexidade do modelo e a magnitude dos coeficientes, melhoram a qualidade da estima√ß√£o da fun√ß√£o de log-verossimilhan√ßa, e facilitam o processo de otimiza√ß√£o atrav√©s do algoritmo de Newton-Raphson, al√©m de reduzir o overfitting.*  O impacto da regulariza√ß√£o na otimiza√ß√£o √© fundamental para a converg√™ncia dos modelos e a estima√ß√£o de par√¢metros consistentes.

> ‚ö†Ô∏è **Ponto Crucial**: A sele√ß√£o de vari√°veis e a regulariza√ß√£o complementam o processo de otimiza√ß√£o da verossimilhan√ßa condicional e o uso do algoritmo de Newton-Raphson, e tamb√©m tornam a estimativa dos par√¢metros mais robusta e com maior capacidade de generaliza√ß√£o para novos dados [^4.5].

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
    subgraph "Perceptron Learning"
        direction TB
        A["Initial Hyperplane: 'Œ≤‚ÇÄ + Œ≤‚ÇÅx = 0'"]
        B["Misclassified Point: 'x·µ¢'"]
        C["Parameter Update: 'Œ≤_new = Œ≤_old + Œ∑y·µ¢x·µ¢'"]
        D["Adjusted Hyperplane"]
        B --> C
        C --> D
        A --> B
    end
```

A busca por **hiperplanos separadores** visa encontrar uma fronteira linear que maximize a separa√ß√£o entre as classes.  Essa busca pode ser relacionada com a maximiza√ß√£o da verossimilhan√ßa condicional, especialmente em problemas de classifica√ß√£o bin√°ria, onde um hiperplano que separa as classes de forma adequada tamb√©m pode ser visto como uma solu√ß√£o que maximiza a probabilidade de classificar os dados corretamente [^4.5.2].  O algoritmo do Perceptron, busca encontrar uma solu√ß√£o para o hiperplano separador que minimize os erros de classifica√ß√£o.

O algoritmo do **Perceptron** busca um hiperplano separador ajustando iterativamente os par√¢metros do modelo com base nas classifica√ß√µes incorretas, e o seu objetivo √© minimizar o n√∫mero de classifica√ß√µes incorretas, o que pode ser visto como uma forma de maximizar a verossimilhan√ßa da resposta, dados os preditores [^4.5.1]. O Perceptron, no entanto, n√£o utiliza explicitamente a fun√ß√£o de log-verossimilhan√ßa e nem a matriz Hessiana para guiar o processo de otimiza√ß√£o, e sim o ajuste dos par√¢metros com base nas classifica√ß√µes incorretas.

> üí° **Exemplo Num√©rico:**
>
> Vamos supor os mesmos dados, com as classes representadas por $y_i \in \{-1, 1\}$:
>
> | Observa√ß√£o (i) | $x_i$ | $y_i$ |
> |-----------------|-------|-------|
> | 1               | 1     | 1     |
> | 2               | 2     | -1    |
> | 3               | 3     | 1     |
>
> Inicialmente, vamos supor um vetor de par√¢metros $\beta = [\beta_0, \beta_1]^T = [0.1, 0.2]^T$.  O Perceptron atualiza os par√¢metros apenas quando h√° um erro de classifica√ß√£o.
>
> A previs√£o do Perceptron √© dada por $\hat{y_i} = sign(\beta_0 + \beta_1 x_i)$.
>
> Para a primeira observa√ß√£o: $\hat{y_1} = sign(0.1 + 0.2 * 1) = sign(0.3) = 1$. A classifica√ß√£o est√° correta.
>
> Para a segunda observa√ß√£o: $\hat{y_2} = sign(0.1 + 0.2 * 2) = sign(0.5) = 1$. A classifica√ß√£o est√° incorreta.
>
> Para a terceira observa√ß√£o: $\hat{y_3} = sign(0.1 + 0.2 * 3) = sign(0.7) = 1$. A classifica√ß√£o est√° correta.
>
> A atualiza√ß√£o dos par√¢metros quando h√° um erro √© dada por:
>
> $\beta^{new} = \beta^{old} + \eta y_i x_i$
>
> Onde $\eta$ √© a taxa de aprendizado, por exemplo $\eta = 0.1$.  A √∫nica atualiza√ß√£o acontece na segunda observa√ß√£o:
>
> $\beta^{new} = \begin{bmatrix} 0.1 \\ 0.2 \end{bmatrix} + 0.1 * (-1) * \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 0.1 \\ 0.2 \end{bmatrix} + \begin{bmatrix} -0.1 \\ -0.2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$
>
> Os novos par√¢metros s√£o $\beta^{new} = [0, 0]^T$. O Perceptron continuaria iterando at√© que todos os pontos fossem classificados corretamente.  O Perceptron busca um hiperplano separador, que neste caso √© uma linha, mas n√£o utiliza a fun√ß√£o de log-verossimilhan√ßa.

**Teorema:** *O algoritmo do Perceptron converge para um hiperplano separador em um n√∫mero finito de itera√ß√µes, se os dados forem linearmente separ√°veis, e essa solu√ß√£o pode ser vista como uma aproxima√ß√£o para o problema de maximiza√ß√£o da verossimilhan√ßa, sob a restri√ß√£o de que a fronteira de decis√£o seja um hiperplano.* A garantia da converg√™ncia sob a condi√ß√£o de separabilidade linear destaca a busca por uma solu√ß√£o que minimize os erros, que pode ser interpretado como uma aproxima√ß√£o para a maximiza√ß√£o da probabilidade da classifica√ß√£o observada [^4.5.1].

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

**Resposta:**

A **Regra de Decis√£o Bayesiana** busca classificar uma observa√ß√£o $x$ na classe $k$ que maximize a probabilidade posterior $P(G=k|X=x)$ [^4.3]. Sob a suposi√ß√£o de que as classes seguem distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia $\Sigma$, a probabilidade posterior √© dada por:

$$
P(G=k|X=x) = \frac{ \phi(x;\mu_k,\Sigma)\pi_k}{\sum_{l=1}^K \phi(x;\mu_l,\Sigma)\pi_l}
$$

onde $\phi(x;\mu_k,\Sigma)$ √© a densidade gaussiana para a classe $k$, $\mu_k$ √© a m√©dia da classe $k$ e $\pi_k$ √© a probabilidade a priori da classe. O **LDA**, por sua vez, deriva suas fun√ß√µes discriminantes lineares diretamente dessas suposi√ß√µes, e busca maximizar a separa√ß√£o entre as classes atrav√©s de proje√ß√µes lineares, o que sob a mesma premissa, equivale a maximizar a probabilidade posterior.  A estima√ß√£o dos par√¢metros do LDA n√£o se baseia na maximiza√ß√£o da verossimilhan√ßa condicional, mas sim em estimativas amostrais das m√©dias e da covari√¢ncia [^4.3].

```mermaid
graph LR
    subgraph "Bayesian Decision vs LDA"
        direction TB
        A["Bayes Decision Rule: 'argmax_k P(G=k|X