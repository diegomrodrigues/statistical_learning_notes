## O Problema do Mascaramento na Regressão Linear para Classificação Multiclasse

### Introdução

Como explorado anteriormente no contexto da **Linear Regression of an Indicator Matrix** [^1], uma abordagem para o problema de classificação com *K* classes consiste em codificar as categorias de resposta através de uma matriz indicadora **Y** de dimensão *N* x *K*. Cada coluna *k* de **Y** corresponde a uma classe, e um modelo de regressão linear é ajustado simultaneamente para cada coluna, resultando numa matriz de coeficientes $\hat{B}$ [^1]. Uma nova observação *x* é então classificada na classe *k* para a qual a função de escore ajustada, $f_k(x)$, é máxima, ou seja, $\hat{G}(x) = \text{argmax}_{k \in \mathcal{G}} f_k(x)$ [^1]. Embora esta abordagem possa ser justificada como uma aproximação da esperança condicional $E(Y_k|X=x) = Pr(G=k|X=x)$ [^2], ou vista como a minimização da distância Euclidiana quadrada entre os vetores ajustados $f(x)$ e os vetores alvo $t_k$ [^2], ela apresenta uma limitação significativa e potencialmente severa, conhecida como **masking** (mascaramento), especialmente quando o número de classes *K* é maior que 3 [^3]. Este capítulo aprofunda a natureza deste problema, suas consequências e como ele se compara a outras abordagens lineares de classificação.

### Conceitos Fundamentais

#### Definição e Causa do Mascaramento

O problema do **masking** na abordagem de regressão linear para classificação surge devido à natureza rígida do modelo de regressão quando aplicado a múltiplas respostas indicadoras simultaneamente [^3]. Quando $K > 3$, e particularmente prevalente quando *K* é grande [^3], certas classes podem ser "mascaradas" por outras. Isso significa que a região de decisão correspondente a uma classe específica pode ser inteiramente subsumida pelas regiões de outras classes, de modo que a função de escore $f_k(x)$ para essa classe nunca seja a maior para qualquer valor de *x* no espaço de entrada. Consequentemente, o classificador $\hat{G}(x) = \text{argmax}_k f_k(x)$ nunca atribuirá observações a essa classe mascarada [^5].

A rigidez provém do fato de que um único modelo linear (definido pela matriz $\hat{B}$) tenta simultaneamente ajustar alvos binários (0 ou 1) para todas as *K* classes. As fronteiras de decisão entre quaisquer duas classes *k* e *l* são hiperplanos definidos por $f_k(x) = f_l(x)$ [^10]. A interação dessas múltiplas fronteiras, todas derivadas do mesmo processo de ajuste por mínimos quadrados sobre a matriz indicadora, pode levar a configurações geométricas onde certas regiões de decisão "desaparecem".

#### Ilustração do Problema (K=3)

Uma ilustração extrema do **masking** ocorre mesmo para $K=3$, como demonstrado na Figura 4.2 (painel esquerdo) do material de referência [^3], [^4]. Neste exemplo, existem três classes em $\mathbb{R}^2$ que são perfeitamente separáveis por fronteiras de decisão lineares (como as encontradas pela **Linear Discriminant Analysis (LDA)**, mostradas no painel direito da Figura 4.2) [^4]. No entanto, a regressão linear das variáveis de resposta indicadoras falha completamente em identificar a classe intermediária [^3], [^4].

A Figura 4.3 aprofunda essa análise projetando os dados em uma única dimensão (a linha que une os centróides das três classes) [^5]. As três linhas de regressão ajustadas ($f_1(x), f_2(x), f_3(x)$) são mostradas no painel esquerdo. Observa-se que a linha de regressão correspondente à classe do meio (classe 2) é essencialmente horizontal e seus valores ajustados $f_2(x)$ nunca são dominantes; ou seja, para qualquer *x*, ou $f_1(x)$ ou $f_3(x)$ (ou ambos) são maiores que $f_2(x)$ [^5]. Como resultado direto, as observações pertencentes à classe 2 são invariavelmente classificadas como pertencentes à classe 1 ou à classe 3 pelo procedimento de regressão linear [^5].

> **O Efeito do Mascaramento:** Devido à natureza rígida do modelo de regressão, classes podem ser mascaradas por outras, resultando em fronteiras de decisão que falham em separar todas as classes, mesmo quando a separação linear é possível [^3]. A classe do meio na Figura 4.2 é completamente mascarada [^4].

#### Generalização para K > 3 e Potenciais Soluções

O problema tende a piorar à medida que o número de classes *K* aumenta [^3]. Se as classes estiverem dispostas de forma colinear no espaço de características (ou numa projeção relevante), a complexidade necessária para separar as classes intermediárias aumenta. Para o exemplo simples com $K=3$ na Figura 4.3, uma regressão quadrática (em vez de linear) para a classe do meio poderia resolver o problema [^6]. No entanto, se houvesse quatro classes alinhadas dessa maneira, um ajuste quadrático não seria suficiente para "descer" rápido o bastante para capturar a quarta classe, e um ajuste cúbico seria necessário [^6].

Como regra geral, embora informal, para resolver cenários de pior caso onde *K* classes estão alinhadas, pode ser necessário incluir termos polinomiais e produtos cruzados das variáveis de entrada até grau $K-1$ [^6]. No espaço de entrada *p*-dimensional, isso implicaria a necessidade de $O(p^{K-1})$ termos no total, o que rapidamente se torna computacionalmente inviável e propenso a overfitting [^7].

#### Impacto no Desempenho e Comparação com LDA

O **masking** não é apenas uma curiosidade teórica; ele pode ter um impacto substancial no desempenho da classificação em problemas reais. O exemplo do reconhecimento de vogais (Vowel recognition problem), com $K=11$ classes e $p=10$ dimensões, ilustra isso [^7]. A Tabela 4.1 mostra as taxas de erro para vários métodos lineares neste conjunto de dados [^8]. A regressão linear da matriz indicadora apresenta uma taxa de erro de teste de 67%, significativamente maior do que a taxa de erro de 56% obtida pela **Linear Discriminant Analysis (LDA)** [^8]. O texto sugere que o efeito de **masking** prejudicou o desempenho da regressão linear neste caso [^7].

É crucial notar que, embora a LDA (sob a suposição de covariâncias iguais) também produza fronteiras de decisão lineares [^8], ela não sofre do mesmo problema de **masking** que a regressão linear da matriz indicadora quando $K > 2$ [^9]. A LDA modela as densidades de classe condicionais $f_k(x)$ (assumindo-as Gaussianas com covariância comum $\Sigma$) e usa o teorema de Bayes para obter as probabilidades a posteriori $Pr(G=k|X=x)$, ou equivalentemente, as funções discriminantes lineares $\delta_k(x)$ [^8], [^9]. A estimação dos parâmetros na LDA é feita de forma diferente, maximizando a verossimilhança conjunta $Pr(X, G=k)$, o que evita a rigidez inerente ao ajuste direto da matriz indicadora por mínimos quadrados [^9], [^27]. Outros métodos abordados no capítulo 4 do material de referência também são baseados em funções lineares de *x*, mas as utilizam de maneira a evitar o problema de **masking** [^7].

### Conclusão

Em suma, o fenômeno do **masking** representa uma desvantagem crítica da aplicação direta da regressão linear a uma matriz indicadora para problemas de classificação multicasse com $K>3$ classes [^3]. A estrutura inerentemente rígida deste método pode impedir que certas classes sejam corretamente identificadas, levando a regiões de decisão onde algumas classes nunca são preditas, mesmo que sejam linearmente separáveis [^4], [^5]. Este problema pode resultar em taxas de erro significativamente mais altas em comparação com métodos alternativos como a **Linear Discriminant Analysis (LDA)**, que, embora também linear, não sofre desta limitação específica [^7], [^8], [^9]. Embora a introdução de termos polinomiais de alto grau possa, em teoria, mitigar o **masking** [^6], isso acarreta um custo computacional elevado e aumenta o risco de overfitting, tornando-se impraticável na maioria dos cenários [^7]. Portanto, para classificação multicasse, abordagens como LDA ou regressão logística (que também evita este problema específico) são geralmente preferíveis à regressão linear da matriz indicadora.

### Referências

[^1]: Page 103, Section 4.2 Linear Regression of an Indicator Matrix: "Here each of the response categories are coded via an indicator variable... We fit a linear regression model to each of the columns of Y simultaneously, and the fit is given by Y = X(XTX)⁻¹XTY... A new observation with input x is classified as follows: compute the fitted output f(x)T = (1, xT)B, a K vector; identify the largest component and classify accordingly: Ĝ(x) = argmax_{k \in \mathcal{G}} f_k(x)."
[^2]: Page 104: "What is the rationale for this approach? One rather formal justification is to view the regression as an estimate of conditional expectation... Alternatively, are the f_k(x) reasonable estimates of the posterior probabilities Pr(G = k|X = x)... A more simplistic viewpoint is to construct targets t_k for each class... We might then fit the linear model by least squares: min_B Σ ||y_i - [(1, x_i^T)B]^T||^2... The criterion is a sum-of-squared Euclidean distances... A new observation is classified by computing its fitted vector f(x) and classifying to the closest target: Ĝ(x) = argmin_k ||f(x) - t_k||^2. This is exactly the same as the previous approach."
[^3]: Page 105: "There is a serious problem with the regression approach when the number of classes K > 3, especially prevalent when K is large. Because of the rigid nature of the regression model, classes can be masked by others. Figure 4.2 illustrates an extreme situation when K = 3."
[^4]: Page 105, Figure 4.2 caption: "The data come from three classes in IR^2 and are easily separated by linear decision boundaries. The right plot shows the boundaries found by linear discriminant analysis. The left plot shows the boundaries found by linear regression of the indicator response variables. The middle class is completely masked (never dominates)."
[^5]: Page 105: "In Figure 4.3 we have projected the data onto the line joining the three centroids... The three regression lines (left panel) are included, and we see that the line corresponding to the middle class is horizontal and its fitted values are never dominant! Thus, observations from class 2 are classified either as class 1 or class 3."
[^6]: Page 105: "For this simple example a quadratic rather than linear fit (for the middle class at least) would solve the problem. However, it can be seen that if there were four rather than three classes lined up like this, a quadratic would not come down fast enough, and a cubic would be needed as well. A loose but general rule is that if K > 3 classes are lined up, polynomial terms up to degree K − 1 might be needed to resolve them."
[^7]: Page 106: "The example is extreme, but for large K and small p such maskings naturally occur... As a more realistic illustration, Figure 4.4 is a projection of the training data for a vowel recognition problem... There are K = 11 classes in p = 10 dimensions... The main point here is summarized in Table 4.1; linear regression has an error rate of 67%, while a close relative, linear discriminant analysis, has an error rate of 56%. It seems that masking has hurt in this case. While all the other methods in this chapter are based on linear functions of x as well, they use them in such a way that avoids this masking problem... So in p-dimensional input space, one would need general polynomial terms and cross-products of total degree K − 1, O(p^{K−1}) terms in all, to resolve such worst-case scenarios."
[^8]: Page 106, Table 4.1 & Page 107, Table 4.1 caption: "Training and test error rates using a variety of linear techniques on the vowel data... Linear regression 0.48 (Train) 0.67 (Test), Linear discriminant analysis 0.32 (Train) 0.56 (Test)... We see that linear regression is hurt by masking, increasing the test and training error by over 10%."
[^9]: Page 110: "With more than two classes, LDA is not the same as linear regression of the class indicator matrix, and it avoids the masking problems associated with that approach (Hastie et al., 1994)."
[^10]: Page 101, Section 4.1 Introduction: "...the fitted linear model for the kth indicator response variable is f_k(x) = β_{k0} + β_k^T x. The decision boundary between class k and l is that set of points for which f_k(x) = f_l(x), that is, the set {x : (β_{k0} - β_{l0}) + (β_k - β_l)^T x = 0}, an affine set or hyperplane."
[^27]: Page 127, Section 4.4.5 Logistic Regression or LDA?: Compares LDA (maximizing joint likelihood $Pr(X, G=k) = \phi(X; \mu_k, \Sigma)\pi_k$) with Logistic Regression (maximizing conditional likelihood $Pr(G=k|X)$). LDA uses assumptions about the marginal density $Pr(X)$ derived from the Gaussian model, while logistic regression does not.

<!-- END -->