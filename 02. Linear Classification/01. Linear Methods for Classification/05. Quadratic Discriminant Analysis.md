## Generalizações das Fronteiras de Decisão Lineares: Análise Discriminante Quadrática e Regularizada

### Introdução

Como explorado anteriormente neste capítulo sobre **Métodos Lineares para Classificação** [^page1], uma classe importante de procedimentos de classificação resulta em fronteiras de decisão que são lineares no espaço de entrada [^page1]. Vimos que métodos como a regressão linear de variáveis indicadoras [^page3] e a Análise Discriminante Linear (LDA) [^page8], sob certas condições, produzem tais fronteiras hiperplanares. No entanto, a restrição a fronteiras lineares pode ser limitante em muitos cenários práticos onde a separação ótima entre classes requer superfícies mais complexas.

Este capítulo foca em generalizações que transcendem a linearidade estrita, permitindo fronteiras de decisão quadráticas. Exploraremos duas abordagens principais. A primeira envolve a **expansão do conjunto de variáveis** para incluir termos quadráticos e produtos cruzados das variáveis originais. Como mencionado na introdução [^page2], funções lineares neste espaço aumentado correspondem a funções quadráticas no espaço original, transformando efetivamente fronteiras de decisão lineares em quadráticas [^page2]. A segunda abordagem, mais direta dentro do paradigma da análise discriminante, é a **Análise Discriminante Quadrática (QDA)**, que surge naturalmente ao relaxar a suposição de matrizes de covariância comuns entre as classes, inerente à LDA [^page10]. Finalmente, discutiremos a **Análise Discriminante Regularizada (RDA)**, que oferece um meio-termo flexível entre a LDA e a QDA [^page12].

### Conceitos Fundamentais

#### Fronteiras de Decisão Quadráticas via Expansão de Base

Uma maneira conceitualmente direta de obter fronteiras de decisão não lineares, especificamente quadráticas, é aumentar o espaço de variáveis de entrada $X = (X_1, ..., X_p)$. Conforme introduzido anteriormente [^page2], podemos criar um novo conjunto de variáveis $h(X)$ incluindo não apenas as variáveis originais, mas também seus quadrados ($X_i^2$) e produtos cruzados ($X_i X_j$ para $i \neq j$). Este espaço aumentado terá $p\' = p + p(p+1)/2$ variáveis (excluindo um intercepto).

Uma função linear neste espaço aumentado $h(X)$ pode ser escrita como $f(h(x)) = \gamma_0 + \sum_{j=1}^{p\'} \gamma_j h_j(x)$. Substituindo os termos $h_j(x)$ pelos correspondentes termos lineares, quadráticos e de produtos cruzados de $x$, a função $f(h(x))$ torna-se uma função quadrática nas variáveis originais $x$:
$$ q(x) = \beta_0 + \sum_{i=1}^p \beta_i x_i + \sum_{i=1}^p \sum_{j=i}^p \beta_{ij} x_i x_j $$
onde os coeficientes $\beta$ são derivados dos coeficientes $\gamma$.

Consequentemente, se aplicarmos um método de classificação que produz fronteiras de decisão lineares, como LDA ou regressão linear sobre indicadores, ao *espaço aumentado* $h(X)$, as fronteiras de decisão resultantes, quando mapeadas de volta para o espaço original de $X$, serão quadráticas [^page2]. A fronteira de decisão entre as classes $k$ e $l$ no espaço aumentado é definida por $f_k(h(x)) = f_l(h(x))$, que corresponde a $q_k(x) = q_l(x)$ no espaço original, uma equação quadrática em $x$.

> A Figura 4.1 [^page3] ilustra visualmente essa ideia, mostrando como fronteiras lineares no espaço aumentado de cinco dimensões ($X_1, X_2, X_1X_2, X_1^2, X_2^2$) resultam em fronteiras quadráticas no espaço original bidimensional $(X_1, X_2)$. A Figura 4.6 (painel esquerdo) [^page11] também demonstra este método, aplicando LDA ao espaço aumentado de cinco dimensões.

Esta abordagem de expansão de base é bastante geral e pode ser utilizada com qualquer método de classificação linear [^page2]. No entanto, o aumento na dimensionalidade pode ser substancial, especialmente para valores elevados de $p$, o que pode levar a problemas computacionais e de variância na estimação.

#### Análise Discriminante Quadrática (QDA)

Uma alternativa mais direta para obter fronteiras quadráticas, derivada da teoria de decisão Bayesiana sob suposições Gaussianas, é a Análise Discriminante Quadrática (QDA). Relembrando a Seção 4.3 [^page8], a LDA surge ao modelar a densidade condicional de classe $f_k(x)$ como uma Gaussiana multivariada $N(\mu_k, \Sigma_k)$ e assumindo que as matrizes de covariância são idênticas para todas as classes, $\Sigma_k = \Sigma$ [^page8].

A QDA, por outro lado, mantém a suposição Gaussiana para $f_k(x) = N(\mu_k, \Sigma_k)$, mas *não* assume que as matrizes de covariância $\Sigma_k$ são iguais entre as classes [^page10]. Ao derivar o log-ratio das probabilidades a posteriori (similar à Equação 4.9 para LDA [^page8]), os termos envolvendo as matrizes de covariância e os termos quadráticos em $x$ não se cancelam mais. Isso leva a **funções discriminantes quadráticas** [^page10]:

> $$ \delta_k(x) = -\frac{1}{2}\log|\Sigma_k| - \frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + \log \pi_k \quad (4.12) $$ [^page10]

A função $\delta_k(x)$ é claramente uma função quadrática de $x$ devido ao termo $(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)$. A regra de classificação ainda é atribuir $x$ à classe com a maior função discriminante: $G(x) = \text{argmax}_k \delta_k(x)$ [^page9]. A fronteira de decisão entre quaisquer duas classes $k$ e $l$ é o conjunto de pontos $x$ onde $\delta_k(x) = \delta_l(x)$. Como $\delta_k(x)$ e $\delta_l(x)$ são quadráticas em $x$, a equação $\delta_k(x) = \delta_l(x)$ define uma superfície quadrática no espaço de entrada $p$-dimensional [^page10].

Na prática, os parâmetros $\pi_k, \mu_k, \Sigma_k$ são desconhecidos e devem ser estimados a partir dos dados de treinamento. As estimativas para as priors $\hat{\pi}_k = N_k/N$ e as médias $\hat{\mu}_k = \sum_{g_i=k} x_i / N_k$ são as mesmas que na LDA [^page9]. No entanto, para QDA, estimamos uma matriz de covariância separada para cada classe:
$$ \hat{\Sigma}_k = \frac{1}{N_k-1} \sum_{g_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T $$
(Note que o texto usa um denominador comum $N-K$ para a estimativa *pooled* $\hat{\Sigma}$ na LDA [^page9], mas estimativas separadas geralmente usam $N_k-1$). A necessidade de estimar $K$ matrizes de covariância distintas aumenta drasticamente o número de parâmetros em comparação com a LDA, especialmente quando a dimensão $p$ é grande [^page10]. Enquanto LDA requer a estimação de $K \times p$ parâmetros para as médias e $p(p+1)/2$ para a covariância comum, QDA requer $K \times p$ para as médias e $K \times p(p+1)/2$ para as covariâncias separadas. A contagem de parâmetros efetivos para as fronteiras de decisão é discutida em [^page11], resultando em $(K-1) \times \\{p(p+3)/2 + 1\\}$ parâmetros para QDA.

A Figura 4.6 (painel direito) [^page11] mostra as fronteiras de decisão quadráticas obtidas pela QDA para os mesmos dados da Figura 4.1. O texto observa que as diferenças entre QDA e LDA no espaço aumentado (painel esquerdo da Figura 4.6) são geralmente pequenas, mas QDA é considerada a abordagem preferida quando as suposições subjacentes (Gaussianas com covariâncias diferentes) são apropriadas [^page10].

A escolha entre LDA e QDA envolve um **tradeoff viés-variância** [^page11]. Se as verdadeiras fronteiras de decisão são lineares, LDA terá baixo viés e, por ter menos parâmetros, provavelmente menor variância que QDA. Se as fronteiras são quadráticas e as classes são aproximadamente Gaussianas com covariâncias diferentes, QDA terá menor viés. No entanto, a maior flexibilidade da QDA vem ao custo de uma variância potencialmente muito maior devido ao grande número de parâmetros a serem estimados, especialmente se $p$ for grande em relação a $N_k$. O texto sugere que o bom desempenho histórico da LDA e QDA pode dever-se ao fato de que os dados muitas vezes só suportam fronteiras de decisão simples (lineares ou quadráticas) e as estimativas fornecidas pelos modelos Gaussianos são estáveis, mesmo que a suposição Gaussiana não seja estritamente verdadeira [^page11].

#### Análise Discriminante Regularizada (RDA)

Reconhecendo o tradeoff entre a rigidez da LDA (baixa variância, potencialmente alto viés) e a flexibilidade da QDA (baixo viés se apropriado, potencialmente alta variância), Friedman (1989) propôs a Análise Discriminante Regularizada (RDA) como um compromisso [^page12]. A ideia central é "encolher" (shrink) as estimativas das matrizes de covariância separadas $\hat{\Sigma}_k$ (usadas na QDA) em direção à estimativa da matriz de covariância comum $\hat{\Sigma}$ (usada na LDA) [^page12]. Este processo é análogo em espírito à regressão ridge [^page12].

As matrizes de covariância regularizadas são formadas como uma combinação convexa:
> $$ \hat{\Sigma}_k(\alpha) = \alpha \hat{\Sigma}_k + (1 - \alpha)\hat{\Sigma} \quad (4.13) $$ [^page12]
onde $\hat{\Sigma}$ é a matriz de covariância pooled estimada como na LDA [^page9], e $\alpha$ é um parâmetro de regularização tal que $\alpha \in [0, 1]$.

Este parâmetro $\alpha$ controla o grau de regularização e permite um *continuum* de modelos entre LDA e QDA [^page12]:
*   Se $\alpha = 0$, então $\hat{\Sigma}_k(0) = \hat{\Sigma}$ para todas as classes $k$, e recuperamos a LDA.
*   Se $\alpha = 1$, então $\hat{\Sigma}_k(1) = \hat{\Sigma}_k$, e recuperamos a QDA.
*   Para $0 < \alpha < 1$, obtemos um modelo intermediário, onde as matrizes de covariância estimadas são uma mistura das estimativas individuais e da estimativa pooled. Isso permite que as formas das covariâncias variem entre as classes, mas as encolhe em direção a uma forma comum, estabilizando as estimativas.

O valor ótimo de $\alpha$ não é conhecido a priori e é tipicamente tratado como um hiperparâmetro a ser determinado a partir dos dados, por exemplo, usando validação cruzada ou um conjunto de validação separado para minimizar o erro de classificação [^page12]. A Figura 4.7 [^page12] ilustra o desempenho da RDA nos dados de vogais (vowel data) para diferentes valores de $\alpha$, mostrando como o erro de teste pode ser otimizado em um valor intermediário de $\alpha$, neste caso próximo de $\alpha=0.9$, indicando que um modelo próximo da QDA foi o ideal.

O texto também menciona uma modificação adicional onde a própria matriz de covariância pooled $\hat{\Sigma}$ (ou as $\hat{\Sigma}_k$) pode ser encolhida em direção a uma matriz de covariância escalar, $\hat{\sigma}^2 I$:
$$ \hat{\Sigma}(\gamma) = \gamma\hat{\Sigma} + (1 - \gamma)\hat{\sigma}^2 I \quad (4.14) $$ [^page12]
onde $\gamma \in [0, 1]$ e $\hat{\sigma}^2$ é uma estimativa da variância média. Substituir $\hat{\Sigma}$ na Equação (4.13) por $\hat{\Sigma}(\gamma)$ leva a uma família mais geral de matrizes de covariância regularizadas $\hat{\Sigma}_k(\alpha, \gamma)$, indexada por dois parâmetros de regularização [^page12]. Versões adicionais de LDA regularizada são mencionadas como sendo mais adequadas para dados específicos como sinais digitais ou imagens [^page12].

### Conclusão

Este capítulo detalhou generalizações importantes dos métodos lineares de classificação, focando em abordagens que produzem fronteiras de decisão quadráticas. Vimos que a expansão do espaço de características com termos quadráticos e produtos cruzados permite que métodos lineares padrão gerem fronteiras quadráticas no espaço original [^page2, ^page3]. Alternativamente, a Análise Discriminante Quadrática (QDA) surge diretamente da modelagem Gaussiana quando a restrição de covariâncias iguais da LDA é relaxada, resultando em funções discriminantes e fronteiras de decisão inerentemente quadráticas [^page10]. A QDA oferece maior flexibilidade que a LDA, mas ao custo de estimar significativamente mais parâmetros, o que pode aumentar a variância [^page11]. A Análise Discriminante Regularizada (RDA) fornece uma ponte entre LDA e QDA, utilizando um parâmetro de regularização $\alpha$ para controlar o encolhimento das covariâncias individuais em direção a uma covariância comum, permitindo um ajuste fino do tradeoff viés-variância [^page12]. A escolha entre essas abordagens depende das características específicas do problema de classificação e da quantidade de dados disponíveis para estimar os parâmetros do modelo de forma robusta. Essas generalizações aumentam significativamente o escopo de aplicação da análise discriminante.

### Referências

[^page1]: Contexto OCR, Página 101
[^page2]: Contexto OCR, Página 102
[^page3]: Contexto OCR, Página 103
[^page4]: Contexto OCR, Página 104
[^page5]: Contexto OCR, Página 105
[^page6]: Contexto OCR, Página 106
[^page7]: Contexto OCR, Página 107
[^page8]: Contexto OCR, Página 108
[^page9]: Contexto OCR, Página 109
[^page10]: Contexto OCR, Página 110
[^page11]: Contexto OCR, Página 111
[^page12]: Contexto OCR, Página 112
[^page13]: Contexto OCR, Página 113
[^page14]: Contexto OCR, Página 114
[^page16]: Contexto OCR, Página 116
[^page17]: Contexto OCR, Página 117
[^page18]: Contexto OCR, Página 118
[^page19]: Contexto OCR, Página 119
[^page20]: Contexto OCR, Página 120
[^page21]: Contexto OCR, Página 121
[^page22]: Contexto OCR, Página 122
[^page23]: Contexto OCR, Página 123
[^page24]: Contexto OCR, Página 124
[^page25]: Contexto OCR, Página 125
[^page26]: Contexto OCR, Página 126
[^page27]: Contexto OCR, Página 127
[^page28]: Contexto OCR, Página 128
[^page29]: Contexto OCR, Página 129
[^page30]: Contexto OCR, Página 130
[^page31]: Contexto OCR, Página 131
[^page32]: Contexto OCR, Página 132
[^page33]: Contexto OCR, Página 133
[^page34]: Contexto OCR, Página 134
[^page35]: Contexto OCR, Página 135
[^page36]: Contexto OCR, Página 136
[^page37]: Contexto OCR, Página 137

<!-- END -->