## L1 Regularization in Logistic Regression for Feature Selection and Shrinkage

### Introdução
Este capítulo explora o uso da regularização L1 na regressão logística para seleção de variáveis e *shrinkage*. A regressão logística é um método linear para classificação [^1], onde modelamos as probabilidades posteriores das classes usando funções lineares [^19]. A regularização L1 adiciona uma penalidade à função de log-verossimilhança, promovendo soluções esparsas e auxiliando na identificação de variáveis relevantes [^25]. Este capítulo detalha a formulação matemática, os algoritmos de solução e as propriedades desta técnica.

### Conceitos Fundamentais

**Regressão Logística:**
A regressão logística modela as probabilidades posteriores das classes usando funções lineares [^19]. Para um problema de classificação com K classes, o modelo tem a forma:
$$\
\log \frac{Pr(G = k|X = x)}{Pr(G = K|X = x)} = \beta_{k0} + \beta_k^T x,
$$
onde $Pr(G = k|X = x)$ é a probabilidade da classe k dado o vetor de entrada x, $\beta_{k0}$ é o intercepto, e $\beta_k$ é o vetor de coeficientes para a classe k [^19].

**Log-Verossimilhança:**
Os modelos de regressão logística são geralmente ajustados por máxima verossimilhança [^20]. A log-verossimilhança condicional de G dado X é dada por:
$$\
l(\theta) = \sum_{i=1}^N \log p_{g_i}(x_i; \theta),
$$
onde $p_k(x; \theta) = Pr(G = k|X = x; \theta)$ e $\theta$ representa o conjunto de parâmetros do modelo [^20]. No caso de duas classes, codificamos a resposta gi via uma resposta 0/1 $y_i$, onde $y_i = 1$ quando $g_i = 1$ e $y_i = 0$ quando $g_i = 2$ [^20]. A log-verossimilhança pode ser escrita como:
$$\
l(\beta) = \sum_{i=1}^N \{y_i \log p(x_i; \beta) + (1 - y_i) \log (1 - p(x_i; \beta))\},
$$
onde $p(x; \beta)$ é a probabilidade da classe 1 [^20].

**Regularização L1 (Lasso):**
A regularização L1 adiciona uma penalidade à norma L1 dos coeficientes [^25]. O objetivo é maximizar uma versão penalizada da log-verossimilhança:
$$\
\max_{\beta_0, \beta} \sum_{i=1}^N [y_i(\beta_0 + \beta^T x_i) - \log(1 + e^{\beta_0 + \beta^T x_i})] - \lambda \sum_{j=1}^p |\beta_j|,
$$
onde $\lambda$ é o parâmetro de regularização, controlando a força da penalidade [^25]. O termo de penalidade $\lambda \sum_{j=1}^p |\beta_j|$ força alguns coeficientes a serem exatamente zero, realizando a seleção de variáveis [^25].

**Algoritmos de Solução:**

1.  **Newton-Raphson com Aproximações Quadráticas:**
    *   O algoritmo de Newton-Raphson pode ser usado para maximizar a log-verossimilhança penalizada [^25].
    *   A cada iteração, o problema é aproximado por uma função quadrática, e um algoritmo *lasso* ponderado é aplicado [^25].
    *   O passo de Newton pode ser expresso como um passo de mínimos quadrados ponderados iterativamente (IRLS):
        $$\
        \beta^{new} = \arg \min_{\beta} (z - X\beta)^T W (z - X\beta),
        $$
        onde $z$ é a resposta ajustada e $W$ é uma matriz de pesos [^21].

2.  **Coordinate Descent:**
    *   Métodos de *coordinate descent* podem ser usados para maximizar a log-verossimilhança eficientemente [^21].
    *   O algoritmo otimiza cada coeficiente individualmente, mantendo os outros fixos [^21].
    *   O pacote `glmnet` em R é uma implementação eficiente de regressão logística com regularização, tanto em N quanto em p [^21].

**Propriedades da Regularização L1:**
*   **Seleção de Variáveis:** A regularização L1 força alguns coeficientes a serem exatamente zero, realizando a seleção de variáveis e simplificando o modelo [^25].
*   ***Shrinkage***: Reduz a magnitude dos coeficientes, diminuindo a variância e melhorando a capacidade de generalização [^25].
*   **Esparsidade:** Promove modelos esparsos, que são mais interpretáveis e requerem menos recursos computacionais [^25].

**Relação com a Correlação:**
Para as variáveis com coeficientes não nulos, as equações de *score* têm a forma:
$$\
X^T(y - p) = \lambda \cdot \text{sign}(\beta_j),
$$
onde $\lambda$ representa a força da regularização e *sign*($\beta_j$) o sinal do coeficiente [^26].

### Conclusão
A regularização L1 é uma técnica eficaz para seleção de variáveis e *shrinkage* na regressão logística. Ao penalizar a norma L1 dos coeficientes, promove soluções esparsas e melhora a capacidade de generalização do modelo. Os algoritmos de solução, como Newton-Raphson com aproximações quadráticas e *coordinate descent*, permitem a otimização eficiente da log-verossimilhança penalizada. A regularização L1 é amplamente utilizada em aplicações de análise de dados e inferência, onde o objetivo é entender o papel das variáveis de entrada e construir modelos mais simples e interpretáveis [^21].

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: Data mining, inference, and prediction*. Springer Science & Business Media.
[^19]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: Data mining, inference, and prediction*. Springer Science & Business Media.
[^20]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: Data mining, inference, and prediction*. Springer Science & Business Media.
[^21]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: Data mining, inference, and prediction*. Springer Science & Business Media.
[^25]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: Data mining, inference, and prediction*. Springer Science & Business Media.
[^26]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: Data mining, inference, and prediction*. Springer Science & Business Media.
<!-- END -->