## Análise Discriminante Linear (LDA)

### Introdução
Este capítulo explora a Análise Discriminante Linear (LDA), um método clássico e fundamental para classificação, especialmente útil quando as fronteiras de decisão entre as classes podem ser aproximadas por hiperplanos. Conforme mencionado no capítulo introdutório sobre métodos lineares para classificação [^1], a LDA se encaixa na categoria de métodos que modelam funções discriminantes $\delta_k(x)$ para cada classe, classificando um ponto $x$ para a classe com o maior valor da função discriminante [^1]. Veremos como a LDA se baseia em suposições sobre as densidades de classe para derivar essas funções discriminantes lineares.

### Conceitos Fundamentais
A LDA modela as densidades de classe usando distribuições Gaussianas e assume que todas as classes compartilham a mesma matriz de covariância [^1]. Essa suposição de covariância comum é crucial, pois leva a funções discriminantes lineares e, consequentemente, a fronteiras de decisão lineares.

#### Derivação das Funções Discriminantes
Sob as suposições da LDA, a função discriminante para a classe $k$ é dada por [^1]:

$$\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + log \pi_k$$

onde:
- $x$ é o vetor de características de entrada.
- $\Sigma$ é a matriz de covariância comum a todas as classes.
- $\mu_k$ é o vetor de média da classe $k$.
- $\pi_k$ é a probabilidade *a priori* da classe $k$.

A regra de decisão da LDA classifica $x$ para a classe que maximiza $\delta_k(x)$ [^1]:

$$G(x) = argmax_k \delta_k(x)$$

A linearidade das funções discriminantes $\delta_k(x)$ é evidente, pois cada termo é linear em $x$ ou constante.

#### Estimativa dos Parâmetros
Na prática, os parâmetros das distribuições Gaussianas (médias $\mu_k$, covariância $\Sigma$ e probabilidades *a priori* $\pi_k$) são estimados a partir dos dados de treinamento [^1]. As estimativas são dadas por:

- $\pi_k = N_k/N$, onde $N_k$ é o número de observações na classe $k$ e $N$ é o número total de observações [^1].
- $\mu_k = \sum_{g_i=k} x_i / N_k$, onde a soma é sobre todas as observações $x_i$ pertencentes à classe $k$ [^1].
- $\Sigma = \sum_{k=1}^K \sum_{g_i=k} (x_i - \mu_k)(x_i - \mu_k)^T / (N - K)$, onde a soma é sobre todas as classes e observações, e $K$ é o número total de classes [^1].

#### Simplificações Computacionais
Os cálculos para LDA podem ser simplificados diagonalizando $\Sigma$ [^1]. Isso pode ser feito através da decomposição espectral da matriz de covariância comum. A diagonalização facilita a inversão de $\Sigma$, que é uma operação crucial na função discriminante.

#### Implementação da LDA
A LDA pode ser implementada transformando os dados através de uma operação de "sphering" em relação à estimativa da covariância comum $\Sigma$, e então classificando para o centróide da classe mais próximo no espaço transformado, levando em consideração as probabilidades *a priori* $\pi_k$ [^1].

**Passos:**

1. **Sphering:** Transformar os dados usando a decomposição de Cholesky de $\Sigma$ (ou decomposição espectral) para obter uma matriz de covariância identidade.
2. **Classificação:** Calcular a distância Euclidiana de cada ponto transformado para os centróides de classe transformados. Classificar para a classe com o centróide mais próximo, ajustando pelas probabilidades *a priori*.

### Conclusão
A Análise Discriminante Linear é uma técnica de classificação poderosa e interpretável, especialmente útil quando as classes podem ser separadas por fronteiras lineares. A suposição fundamental da LDA é que as classes compartilham uma matriz de covariância comum, o que leva a funções discriminantes lineares. Embora essa suposição possa ser restritiva em alguns casos, a LDA muitas vezes funciona bem na prática, especialmente quando o número de características é relativamente pequeno em comparação com o número de observações. A LDA também serve como base para métodos mais avançados, como a Análise Discriminante Quadrática (QDA) e a Análise Discriminante Regularizada, que relaxam algumas das suposições da LDA para obter maior flexibilidade [^1].

### Referências
[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer Science & Business Media.

<!-- END -->