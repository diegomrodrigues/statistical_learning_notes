## Regressão Logística: Modelagem de Probabilidades Posteriores para Classificação

### Introdução
Este capítulo explora a aplicação de **regressão logística** no contexto de métodos lineares para classificação. A regressão logística se destaca por modelar as probabilidades *posteriores* das classes, garantindo que estas somem um e permaneçam no intervalo [0, 1] [^19]. Este modelo é amplamente utilizado, especialmente em problemas de classificação binária, devido à sua simplicidade e interpretabilidade [^19].

### Conceitos Fundamentais

A regressão logística modela as probabilidades posteriores de *K* classes através de funções lineares em *x*, assegurando que as probabilidades somem um e permaneçam no intervalo [0,1] [^19]. A modelagem utiliza *K-1* transformações *log-odds* ou *logit*, refletindo a restrição de que as probabilidades somam um [^19]. A escolha do denominador é arbitrária devido à *equivariância* das estimativas [^19].

**Log-Odds (Logit):** A transformação log-odds é definida como o logaritmo da razão entre a probabilidade de um evento ocorrer e a probabilidade de ele não ocorrer. Para duas classes, a transformação log-odds é expressa como:
$$
log \frac{Pr(G = 1|X = x)}{Pr(G = 2|X = x)} = \beta_0 + \beta^Tx \quad [^2]
$$
onde $\beta_0$ é o intercepto e $\beta$ é o vetor de coeficientes [^2]. A decisão de classificar um ponto *x* é baseada no sinal de $\beta_0 + \beta^Tx$.

**Ajuste do Modelo:** Os modelos de regressão logística são geralmente ajustados por **máxima verossimilhança**, usando a verossimilhança condicional de *G* dado *X* [^19]. A *log-verossimilhança* para *N* observações é dada por:
$$
l(\theta) = \sum_{i=1}^N log \, p_{g_i}(x_i; \theta) \quad [^19]
$$
onde $p_k(x_i; \theta) = Pr(G = k|X = x_i; \theta)$ [^19]. Para maximizar a *log-verossimilhança*, suas derivadas são igualadas a zero, resultando em equações de *score* que são resolvidas usando o algoritmo de **Newton-Raphson** [^19].

**Algoritmo de Newton-Raphson:** Este algoritmo envolve *iteratively reweighted least squares* (IRLS), onde cada iteração resolve um problema de mínimos quadrados ponderados [^19]. Para o caso de duas classes, a log-verossimilhança pode ser escrita como:
$$
l(\beta) = \sum_{i=1}^N \{y_i \log p(x_i; \beta) + (1 - y_i) \log(1 - p(x_i; \beta))\} \quad [^20]
$$
onde $y_i$ é a resposta binária (0 ou 1) e $p(x_i; \beta)$ é a probabilidade prevista de que $y_i = 1$ [^20]. As equações de *score* são obtidas derivando a log-verossimilhança em relação a $\beta$:
$$
\frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^N x_i(y_i - p(x_i; \beta)) = 0 \quad [^20]
$$
O algoritmo de Newton-Raphson requer o cálculo da matriz Hessiana:
$$
\frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T} = - \sum_{i=1}^N x_i x_i^T p(x_i; \beta) (1 - p(x_i; \beta)) \quad [^20]
$$
A atualização de Newton é dada por:
$$
\beta^{new} = \beta^{old} - \left( \frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T} \right)^{-1} \frac{\partial l(\beta)}{\partial \beta} \quad [^20]
$$
Em notação matricial, o passo de Newton pode ser reescrito como um passo de mínimos quadrados ponderados:
$$
\beta^{new} = (X^T W X)^{-1} X^T W z \quad [^21]
$$
onde *W* é uma matriz diagonal de pesos com elementos $p(x_i; \beta) (1 - p(x_i; \beta))$ e *z* é a resposta ajustada [^21].

### Conclusão
A regressão logística oferece uma abordagem flexível e poderosa para problemas de classificação, especialmente em cenários binários. Sua capacidade de modelar probabilidades posteriores diretamente, combinada com métodos eficientes de otimização como o algoritmo de Newton-Raphson, a torna uma ferramenta valiosa na análise de dados e inferência estatística [^19, 20, 21].

### Referências
[^19]: Seção 4.4
[^2]: Seção 4.1 e 4.4
[^20]: Seção 4.4.1
[^21]: Seção 4.4.1
<!-- END -->