## Linear Regression of an Indicator Matrix in Linear Methods for Classification

### Introdução
Este capítulo explora os métodos lineares para classificação, revisitando o problema de classificação e focando em como as fronteiras de decisão podem ser encontradas utilizando regressão linear e suas variações [^1]. Uma abordagem específica é a regressão linear de uma matriz indicadora, que envolve codificar cada categoria de resposta através de uma variável indicadora e ajustar um modelo de regressão linear para cada coluna da matriz de resposta indicadora [^3].

### Conceitos Fundamentais

A regressão linear de uma **matriz indicadora** é uma técnica onde cada categoria de resposta é codificada por uma variável *dummy*. Suponha que tenhamos *K* classes, convenientemente rotuladas como 1, 2, ..., *K*. O modelo linear ajustado para a *k*-ésima variável de resposta indicadora é dado por [^3]:

$$f_k(x) = \beta_{k0} + \beta_k^T x$$

A fronteira de decisão entre as classes *k* e *l* é definida pelo conjunto de pontos para os quais [^1]:

$$f_k(x) = f_l(x)$$

isto é,

$$\{x: (\beta_{k0} - \beta_{l0}) + (\beta_k - \beta_l)^T x = 0\}$$

Este conjunto representa um hiperplano [^1].
A classificação de uma nova observação com entrada *x* é feita da seguinte forma [^3]:
1. Calcula-se a saída ajustada $f(x) = (1, x^T)B$, que é um vetor *K*.
2. Identifica-se o maior componente e classifica-se a observação de acordo:

$$\hat{G}(x) = \text{argmax}_{k \in \mathcal{G}} f_k(x)$$

Este método pode ser interpretado como uma estimativa da esperança condicional [^4]:

$$E(Y_k|X = x) = Pr(G = k|X = x)$$

onde $Y_k$ é a variável indicadora para a classe *k*.

No entanto, devido à natureza rígida da regressão linear, os valores ajustados $f_k(x)$ podem ser negativos ou maiores que 1 [^3]. Isso é especialmente problemático ao fazer previsões fora do *hull* dos dados de treinamento [^3]. Além disso, com *K* > 3 classes, pode ocorrer *masking*, onde algumas classes são mascaradas por outras, especialmente quando as classes são perfeitamente separadas por fronteiras de decisão lineares [^3].

**O fenômeno de *masking*** ocorre quando a rigidez do modelo de regressão linear impede que ele capture a complexidade das relações entre as classes, resultando na supressão de algumas classes em favor de outras [^3]. Um exemplo extremo é ilustrado na Figura 4.2, onde a classe do meio é completamente mascarada [^5].

Para mitigar o problema de *masking*, uma abordagem é usar termos polinomiais até o grau *K*-1 [^3]. No entanto, isso resulta em $O(p^{K-1})$ termos em um espaço de entrada *p*-dimensional, tornando-se computacionalmente caro [^3].

**Box de destaque:** A utilização de termos polinomiais de grau *K-1* pode resolver os piores cenários de *masking*, mas ao custo de uma complexidade computacional elevada [^3].

### Conclusão

A regressão linear de uma matriz indicadora é uma abordagem direta para a classificação, mas sofre de limitações devido à sua natureza rígida, especialmente em cenários com múltiplas classes e dados complexos. O *masking* é um problema sério que pode ser abordado com a inclusão de termos polinomiais, embora isso aumente significativamente a complexidade computacional. Métodos alternativos, como *Linear Discriminant Analysis (LDA)*, podem evitar esses problemas ao modelar as densidades de classe diretamente [^8].

### Referências
[^1]: Page 101, "Linear Methods for Classification"
[^3]: Page 103, "Linear Regression of an Indicator Matrix"
[^4]: Page 104, "Linear Methods for Classification"
[^5]: Page 105, "Linear Regression of an Indicator Matrix"
[^8]: Page 108, "Linear Methods for Classification"
<!-- END -->