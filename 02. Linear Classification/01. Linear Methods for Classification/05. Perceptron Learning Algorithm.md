## 4.5.1 Rosenblatt's Perceptron Learning Algorithm: Finding Separating Hyperplanes

### Introdução
Em continuidade à exploração de métodos lineares para classificação, focaremos agora no algoritmo de aprendizado do Perceptron, um método fundamental para encontrar hiperplanos separadores [^4]. Este algoritmo busca minimizar a distância de pontos mal classificados à fronteira de decisão, utilizando um método de descida do gradiente estocástico [^4]. O Perceptron, proposto por Rosenblatt em 1958 [^29, ^30], estabeleceu as bases para os modelos de redes neurais das décadas de 1980 e 1990 [^30]. Embora existam muitas soluções quando os dados são separáveis, a solução encontrada depende dos valores iniciais [^31].

### Conceitos Fundamentais
O algoritmo do Perceptron tem como objetivo encontrar um hiperplano separador, ajustando iterativamente os parâmetros do hiperplano com base em observações mal classificadas até que um hiperplano separador seja encontrado, assumindo que as classes são linearmente separáveis [^4].

**Definição do Problema:**
Dado um conjunto de dados com classes linearmente separáveis, o objetivo é encontrar um hiperplano definido por $\beta$ e $\beta_0$ tal que:
*   $\beta^T x + \beta_0 > 0$ para todas as observações da classe 1.
*   $\beta^T x + \beta_0 < 0$ para todas as observações da classe -1.

**Algoritmo:**
1.  **Inicialização:** Comece com um vetor de pesos inicial $\beta$ e um bias $\beta_0$ (geralmente inicializados com valores pequenos ou zeros).
2.  **Iteração:** Para cada observação $(x_i, y_i)$ no conjunto de treinamento:
    *   Se $y_i (\beta^T x_i + \beta_0) \le 0$, a observação está mal classificada.
    *   Atualize os pesos:
        *   $\beta \leftarrow \beta + \rho y_i x_i$
        *   $\beta_0 \leftarrow \beta_0 + \rho y_i$
        onde $\rho$ é a taxa de aprendizado (learning rate), que pode ser definida como 1 sem perda de generalidade neste caso [^31].
3.  **Convergência:** Repita o passo 2 até que todas as observações sejam corretamente classificadas ou até que um número máximo de iterações seja atingido.

O algoritmo do Perceptron busca minimizar a distância de pontos mal classificados à fronteira de decisão. Se uma resposta $y_i = 1$ é mal classificada, então $x_i^T \beta + \beta_0 < 0$, e o oposto se aplica para uma resposta mal classificada com $y_i = -1$ [^31]. O objetivo é minimizar a função [^31]:

$$ D(\beta, \beta_0) = - \sum_{i \in M} y_i (x_i^T \beta + \beta_0), $$

onde $M$ indexa o conjunto de pontos mal classificados. Essa quantidade é não negativa e proporcional à distância dos pontos mal classificados à fronteira de decisão definida por $\beta^T x + \beta_0 = 0$ [^31]. O gradiente é dado por [^31]:

$$ \frac{\partial D(\beta, \beta_0)}{\partial \beta} = - \sum_{i \in M} y_i x_i, $$
$$ \frac{\partial D(\beta, \beta_0)}{\partial \beta_0} = - \sum_{i \in M} y_i. $$

O algoritmo usa a descida do gradiente estocástico para minimizar esse critério linear por partes [^31]. Em vez de computar a soma das contribuições do gradiente de cada observação, um passo é tomado após cada observação ser visitada [^31]. As observações mal classificadas são visitadas em alguma sequência, e os parâmetros são atualizados via [^31]:

$$\
\begin{pmatrix}\
\beta \\\\\
\beta_0\
\end{pmatrix}\
\leftarrow\
\begin{pmatrix}\
\beta \\\\\
\beta_0\
\end{pmatrix}\
+ \rho\
\begin{pmatrix}\
y_i x_i \\\\\
y_i\
\end{pmatrix}.\
$$

**Propriedades:**

*   **Convergência:** Se as classes são linearmente separáveis, o algoritmo converge para um hiperplano separador em um número finito de passos [^31].
*   **Múltiplas soluções:** Quando os dados são separáveis, existem muitas soluções, e qual é encontrada depende dos valores iniciais [^31].
*   **Sensibilidade à escala:** O número de passos pode ser muito grande, especialmente se a folga (gap) entre as classes for pequena [^31].
*   **Não separabilidade:** Se os dados não são separáveis, o algoritmo não converge e desenvolve ciclos, que podem ser longos e difíceis de detectar [^31].

A Figura 4.14 [^29] ilustra um exemplo de duas classes separáveis por um hiperplano. A linha laranja representa a solução dos mínimos quadrados, que classifica incorretamente um dos pontos de treinamento. As linhas azuis mostram dois hiperplanos separadores encontrados pelo algoritmo do Perceptron, cada um iniciado com diferentes valores aleatórios [^29].

### Conclusão
O algoritmo do Perceptron é um método simples e intuitivo para encontrar hiperplanos separadores. No entanto, ele apresenta algumas limitações, como a dependência da inicialização e a não convergência em casos não linearmente separáveis [^31]. Para contornar essas limitações, é possível utilizar transformações não lineares do espaço original ou buscar hiperplanos ótimos que maximizem a margem entre as classes, como no caso das máquinas de vetores de suporte (SVMs), que serão discutidas no Capítulo 12 [^4, ^29].

### Referências
[^4]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.
[^29]: Página 129, *The Elements of Statistical Learning*.
[^30]: Página 130, *The Elements of Statistical Learning*.
[^31]: Página 131, *The Elements of Statistical Learning*.
<!-- END -->