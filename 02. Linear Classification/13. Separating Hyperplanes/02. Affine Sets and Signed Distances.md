### Conjuntos Afins e o Uso de Dist√¢ncias Sinalizadas a um Hiperplano

```mermaid
graph LR
    subgraph "Affine Sets and Signed Distances"
    direction TB
    A["Affine Set: 'A = {x_0 + v | v ‚àà V}'"] --> B["Hyperplane: 'Œ≤‚ÇÄ + Œ≤·µÄx = 0'"]
    B --> C["Signed Distance: 'd(x) = (Œ≤‚ÇÄ + Œ≤·µÄx) / ||Œ≤||'"]
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#fcf,stroke:#333,stroke-width:2px
```

A compreens√£o dos **conjuntos afins** e o uso de **dist√¢ncias sinalizadas** a um **hiperplano** s√£o fundamentais para entender a geometria dos modelos de classifica√ß√£o linear, incluindo os *perceptrons* e os hiperplanos separadores. Esses conceitos fornecem uma base matem√°tica para a constru√ß√£o e interpreta√ß√£o de modelos de classifica√ß√£o, e s√£o essenciais para modelar as fronteiras de decis√£o.

**Conjuntos Afins:**

Um **conjunto afim** √© um subconjunto de um espa√ßo vetorial que √© obtido por meio da transla√ß√£o de um subespa√ßo vetorial. Formalmente, um conjunto afim $A$ pode ser definido como o conjunto de todos os pontos $x$ que podem ser expressos como:

$$
    x = x_0 + v
$$

onde $x_0$ √© um ponto fixo no espa√ßo, e $v$ √© um vetor que pertence a um subespa√ßo vetorial $V$. Uma forma de descrever um conjunto afim √© por meio de sua equa√ß√£o, que define um hiperplano:

$$
    \beta_0 + \beta^T x = 0
$$

onde $\beta$ √© um vetor normal ao hiperplano, e $\beta_0$ √© um termo que define a transla√ß√£o. Quando $\beta_0 = 0$, o hiperplano passa pela origem, e nesse caso, ele tamb√©m √© um subespa√ßo vetorial. Quando $\beta_0 \neq 0$, o hiperplano n√£o passa pela origem, mas ainda representa um conjunto afim.

> üí° **Exemplo Num√©rico:**
>
> Considere um espa√ßo bidimensional onde $x = [x_1, x_2]^T$. Um hiperplano (neste caso, uma linha) pode ser definido por $\beta_0 + \beta_1 x_1 + \beta_2 x_2 = 0$. Se $\beta = [2, -1]^T$ e $\beta_0 = 3$, a equa√ß√£o do hiperplano √© $3 + 2x_1 - x_2 = 0$.
>
> Se escolhermos um ponto $x_0 = [0, 3]^T$ que satisfaz a equa√ß√£o (j√° que $3 + 2(0) - 3 = 0$), podemos encontrar outros pontos no mesmo hiperplano. Por exemplo, se $v = [1, 2]^T$ √© um vetor paralelo ao hiperplano, ent√£o o ponto $x = x_0 + v = [1, 5]^T$ tamb√©m deve pertencer ao hiperplano. De fato, $3 + 2(1) - 5 = 0$.
>
> Se $\beta_0 = 0$, ent√£o o hiperplano passa pela origem. Por exemplo, se a equa√ß√£o fosse $2x_1 - x_2 = 0$, o ponto $[0, 0]^T$ pertenceria ao hiperplano.

Em duas dimens√µes, um hiperplano √© uma linha, em tr√™s dimens√µes, um hiperplano √© um plano, e em geral, em um espa√ßo de dimens√£o $p$, o hiperplano tem dimens√£o $p-1$. Um hiperplano divide o espa√ßo de entrada em duas regi√µes, e a classifica√ß√£o de um ponto √© feita com base em qual regi√£o o ponto pertence.

**Dist√¢ncias Sinalizadas a um Hiperplano:**

A **dist√¢ncia sinalizada** de um ponto $x$ a um hiperplano definido por $\beta_0 + \beta^T x = 0$ √© uma medida da dist√¢ncia perpendicular do ponto ao hiperplano, com um sinal que indica o lado do hiperplano em que o ponto est√° localizado.

```mermaid
graph LR
    subgraph "Signed Distance Calculation"
    direction TB
    A["Hyperplane Equation: 'Œ≤‚ÇÄ + Œ≤·µÄx = 0'"] --> B["Vector Norm: '||Œ≤||'"]
    B --> C["Signed Distance: 'd(x) = (Œ≤‚ÇÄ + Œ≤·µÄx) / ||Œ≤||'"]
    end
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#fcf,stroke:#333,stroke-width:2px
    style C fill:#f9f,stroke:#333,stroke-width:2px
```

A dist√¢ncia sinalizada √© dada por:

$$
    d(x) = \frac{\beta_0 + \beta^T x}{||\beta||}
$$

onde $||\beta||$ √© a norma euclidiana do vetor $\beta$. Essa medida √© uma dist√¢ncia em que:

*   Se $d(x) > 0$, o ponto $x$ est√° localizado no lado do hiperplano para onde aponta o vetor normal $\beta$.
*   Se $d(x) < 0$, o ponto $x$ est√° localizado no lado oposto.
*   Se $d(x) = 0$, o ponto $x$ est√° no pr√≥prio hiperplano.

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior, onde $\beta = [2, -1]^T$ e $\beta_0 = 3$, vamos calcular a dist√¢ncia sinalizada de um ponto $x = [2, 2]^T$ ao hiperplano $3 + 2x_1 - x_2 = 0$.
>
> Primeiro, calculamos a norma de $\beta$:
>
> $$||\beta|| = \sqrt{2^2 + (-1)^2} = \sqrt{4 + 1} = \sqrt{5}$$
>
> Agora, calculamos a dist√¢ncia sinalizada:
>
> $$d(x) = \frac{3 + 2(2) - 2}{\sqrt{5}} = \frac{3 + 4 - 2}{\sqrt{5}} = \frac{5}{\sqrt{5}} = \sqrt{5} \approx 2.236$$
>
> Como $d(x) > 0$, o ponto $[2, 2]^T$ est√° no lado do hiperplano para onde aponta o vetor normal $\beta = [2, -1]^T$.
>
> Agora, vamos calcular a dist√¢ncia sinalizada de um ponto $x = [0, 4]^T$:
>
> $$d(x) = \frac{3 + 2(0) - 4}{\sqrt{5}} = \frac{3 - 4}{\sqrt{5}} = \frac{-1}{\sqrt{5}} \approx -0.447$$
>
> Como $d(x) < 0$, o ponto $[0, 4]^T$ est√° no lado oposto ao vetor normal $\beta$.
>
> Se o ponto estivesse no hiperplano, como o ponto $x = [1, 5]^T$, a dist√¢ncia seria:
>
> $$d(x) = \frac{3 + 2(1) - 5}{\sqrt{5}} = \frac{0}{\sqrt{5}} = 0$$
>
> A dist√¢ncia sinalizada √© zero, indicando que o ponto est√° exatamente no hiperplano.

O uso de dist√¢ncias sinalizadas permite que as t√©cnicas de classifica√ß√£o linear considerem a dire√ß√£o da dist√¢ncia em rela√ß√£o ao hiperplano, e essa informa√ß√£o √© crucial para separar as classes e para a constru√ß√£o de modelos como o perceptron e o hiperplano separador √≥timo.

Em termos de classifica√ß√£o, cada classe est√° associada a uma regi√£o do espa√ßo definida pela dist√¢ncia sinalizada em rela√ß√£o ao hiperplano. Os pontos s√£o atribu√≠dos √† classe cuja regi√£o corresponde ao sinal da dist√¢ncia sinalizada. Em classificadores como o *perceptron*, os par√¢metros $\beta$ e $\beta_0$ s√£o ajustados durante o processo de aprendizado para definir a orienta√ß√£o e a posi√ß√£o do hiperplano que melhor separa as classes.

**Lemma 43:** *Um conjunto afim √© um subconjunto de um espa√ßo vetorial obtido por meio de uma transla√ß√£o de um subespa√ßo vetorial, e pode ser descrito por meio de uma equa√ß√£o linear, que define um hiperplano no espa√ßo.*

*Prova:* A equa√ß√£o de um conjunto afim (hiperplano) √© dada por $\beta_0 + \beta^T x = 0$, e essa equa√ß√£o define uma regi√£o do espa√ßo.  $\blacksquare$

**Corol√°rio 43:** *A dist√¢ncia sinalizada de um ponto a um hiperplano √© uma medida da dist√¢ncia perpendicular do ponto ao hiperplano, com um sinal que indica o lado em que o ponto est√° localizado, permitindo que modelos de classifica√ß√£o linear se baseiem na dist√¢ncia e na dire√ß√£o dos dados em rela√ß√£o √† fronteira de decis√£o.*

*Prova:* A dist√¢ncia sinalizada √© dada pela equa√ß√£o $\frac{\beta_0 + \beta^T x}{||\beta||}$, e permite entender a posi√ß√£o relativa de cada ponto em rela√ß√£o ao hiperplano. $\blacksquare$

A compreens√£o de conjuntos afins e dist√¢ncias sinalizadas s√£o fundamentais para a constru√ß√£o de modelos lineares para classifica√ß√£o.

### Rela√ß√£o com os Modelos de Classifica√ß√£o Linear

```mermaid
graph LR
    subgraph "Linear Models & Hyperplanes"
        direction TB
    A["Hyperplane: 'Œ≤‚ÇÄ + Œ≤·µÄx = 0'"] --> B["Perceptron Decision: 'sign(Œ≤‚ÇÄ + Œ≤·µÄx)'"]
    A --> C["LDA Discriminant: 'x·µÄŒ£‚Åª¬πŒº‚Çñ - 1/2Œº‚Çñ·µÄŒ£‚Åª¬πŒº‚Çñ + log œÄ‚Çñ'"]
    A --> D["Logistic Regression Probability: 'exp(Œ≤‚ÇÄ + Œ≤·µÄx) / (1 + exp(Œ≤‚ÇÄ + Œ≤·µÄx))'"]
    end
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#fcf,stroke:#333,stroke-width:2px
    style D fill:#f9f,stroke:#333,stroke-width:2px
```

Os conceitos de **conjuntos afins** e **dist√¢ncias sinalizadas a um hiperplano** s√£o a base matem√°tica para muitos **modelos de classifica√ß√£o linear**, como o *perceptron*, a **An√°lise Discriminante Linear (LDA)** e a **regress√£o log√≠stica** [^4.1], [^4.3], [^4.4]. Esses conceitos fornecem a estrutura geom√©trica que permite definir as fronteiras de decis√£o e classificar as observa√ß√µes em diferentes classes.

1.  ***Perceptron***: O algoritmo do *perceptron* utiliza um hiperplano para separar as classes e ajusta seus par√¢metros por meio de um processo iterativo que busca minimizar a dist√¢ncia de observa√ß√µes mal classificadas at√© a fronteira de decis√£o. A fun√ß√£o de decis√£o do *perceptron* √© baseada no sinal da dist√¢ncia sinalizada ao hiperplano. Formalmente, a decis√£o de classe do perceptron √© dada por:

    $$
        \hat{G}(x) = \text{sign}(\beta_0 + \beta^T x)
    $$

    onde $\text{sign}$ √© a fun√ß√£o sinal (1 para valores positivos e -1 para valores negativos). Nesse caso, a dist√¢ncia sinalizada ao hiperplano divide o espa√ßo em duas regi√µes, e as observa√ß√µes s√£o classificadas com base em qual regi√£o elas pertencem.

    > üí° **Exemplo Num√©rico:**
    >
    > Suponha que ap√≥s o treinamento, o perceptron tenha encontrado os par√¢metros $\beta = [2, -1]^T$ e $\beta_0 = 3$. A fun√ß√£o de decis√£o √© $\hat{G}(x) = \text{sign}(3 + 2x_1 - x_2)$.
    >
    > Para o ponto $x = [2, 2]^T$, calculamos $3 + 2(2) - 2 = 5$. Como $5 > 0$, $\text{sign}(5) = 1$, e o ponto √© classificado como pertencente √† classe 1.
    >
    > Para o ponto $x = [0, 4]^T$, calculamos $3 + 2(0) - 4 = -1$. Como $-1 < 0$, $\text{sign}(-1) = -1$, e o ponto √© classificado como pertencente √† classe -1.
    >
    > O perceptron usa o sinal da dist√¢ncia sinalizada para classificar os pontos.

2.  ***LDA (An√°lise Discriminante Linear)***: A LDA utiliza um hiperplano para separar as classes, e a decis√£o de classe √© baseada na dist√¢ncia sinalizada do ponto ao hiperplano e nas probabilidades *a priori* das classes. A fun√ß√£o discriminante da LDA pode ser interpretada como um conjunto de dist√¢ncias sinalizadas com um ponto de corte que considera a vari√¢ncia comum entre as classes e as probabilidades *a priori* de cada classe.

    A regra de decis√£o da LDA √© baseada na compara√ß√£o da fun√ß√£o discriminante para cada classe.  Quando assumimos covari√¢ncias iguais, cada discriminante √© uma fun√ß√£o linear de $x$:

$$
 \delta_k(x) =  x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log \pi_k
$$

que representa uma dist√¢ncia sinalizada ponderada pelos par√¢metros do modelo.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos duas classes com m√©dias $\mu_1 = [1, 1]^T$ e $\mu_2 = [3, 3]^T$, uma matriz de covari√¢ncia comum $\Sigma = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ e probabilidades a priori $\pi_1 = 0.6$ e $\pi_2 = 0.4$.
>
> A fun√ß√£o discriminante para a classe 1 √©:
>
> $$
> \delta_1(x) = x^T \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}^{-1} \begin{bmatrix} 1 \\ 1 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}^{-1} \begin{bmatrix} 1 \\ 1 \end{bmatrix} + \log(0.6)
> $$
>
> $$
> \delta_1(x) = x^T \begin{bmatrix} 1 \\ 1 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} + \log(0.6) = x_1 + x_2 - 1 + \log(0.6)
> $$
>
> Similarmente, para a classe 2:
>
> $$
> \delta_2(x) = x^T \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}^{-1} \begin{bmatrix} 3 \\ 3 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 3 & 3 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}^{-1} \begin{bmatrix} 3 \\ 3 \end{bmatrix} + \log(0.4)
> $$
>
> $$
> \delta_2(x) = x^T \begin{bmatrix} 3 \\ 3 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 3 & 3 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix} + \log(0.4) = 3x_1 + 3x_2 - 9 + \log(0.4)
> $$
>
> Para classificar um ponto $x = [2, 2]^T$, calculamos:
>
> $$
> \delta_1([2, 2]^T) = 2 + 2 - 1 + \log(0.6) \approx 2.51
> $$
>
> $$
> \delta_2([2, 2]^T) = 3(2) + 3(2) - 9 + \log(0.4) \approx 2.08
> $$
>
> Como $\delta_1(x) > \delta_2(x)$, o ponto √© classificado como pertencente √† classe 1.

3.  ***Regress√£o Log√≠stica***: A regress√£o log√≠stica tamb√©m utiliza um hiperplano como fronteira de decis√£o, e a probabilidade de uma observa√ß√£o pertencer a uma classe √© modelada atrav√©s de uma fun√ß√£o log√≠stica aplicada √† dist√¢ncia sinalizada ao hiperplano. A decis√£o de classe √© feita com base na compara√ß√£o entre a probabilidade de cada classe:

$$
    Pr(G=1|X=x) = \frac{exp(\beta_0 + \beta^T x)}{1+exp(\beta_0 + \beta^T x)}
$$

A probabilidade √© uma transforma√ß√£o da dist√¢ncia sinalizada.

> üí° **Exemplo Num√©rico:**
>
> Suponha que a regress√£o log√≠stica tenha encontrado os par√¢metros $\beta = [1, -1]^T$ e $\beta_0 = -1$. A probabilidade de um ponto $x$ pertencer √† classe 1 √©:
>
> $$
> Pr(G=1|X=x) = \frac{exp(-1 + x_1 - x_2)}{1+exp(-1 + x_1 - x_2)}
> $$
>
> Para o ponto $x = [2, 1]^T$:
>
> $$
> Pr(G=1|X=[2, 1]^T) = \frac{exp(-1 + 2 - 1)}{1+exp(-1 + 2 - 1)} = \frac{exp(0)}{1+exp(0)} = \frac{1}{1+1} = 0.5
> $$
>
> Para o ponto $x = [1, 2]^T$:
>
> $$
> Pr(G=1|X=[1, 2]^T) = \frac{exp(-1 + 1 - 2)}{1+exp(-1 + 1 - 2)} = \frac{exp(-2)}{1+exp(-2)} \approx \frac{0.135}{1+0.135} \approx 0.119
> $$
>
> Se o limiar de decis√£o for 0.5, o ponto [2, 1] seria classificado na classe 1, enquanto o ponto [1, 2] seria classificado na classe 0.

Em todos esses modelos, os par√¢metros $\beta$ e $\beta_0$ definem a orienta√ß√£o e a posi√ß√£o do hiperplano, e o algoritmo de aprendizado de cada modelo ajusta os par√¢metros de forma a obter uma fronteira de decis√£o que separe as classes com a maior precis√£o poss√≠vel. As dist√¢ncias sinalizadas s√£o, ent√£o, um elemento fundamental para expressar as regras de decis√£o desses modelos.

A organiza√ß√£o das classes no espa√ßo √© baseada nas dist√¢ncias sinalizadas com rela√ß√£o a hiperplanos que separam o espa√ßo de entrada em regi√µes correspondentes a cada classe.

**Lemma 44:** *Os modelos de classifica√ß√£o linear, como o perceptron, a LDA e a regress√£o log√≠stica, utilizam hiperplanos como fronteiras de decis√£o, e a classifica√ß√£o das observa√ß√µes √© baseada na dist√¢ncia sinalizada dessas observa√ß√µes em rela√ß√£o a esses hiperplanos.*

*Prova:* As fun√ß√µes de decis√£o do perceptron, da LDA e da regress√£o log√≠stica s√£o baseadas no sinal da dist√¢ncia sinalizada ou em transforma√ß√µes da dist√¢ncia sinalizada.  $\blacksquare$

**Corol√°rio 44:** *O ajuste dos par√¢metros dos modelos de classifica√ß√£o linear envolve a otimiza√ß√£o da posi√ß√£o e orienta√ß√£o do hiperplano para separar as classes com a maior precis√£o poss√≠vel*.

*Prova:* O ajuste dos modelos de classifica√ß√£o linear tem como objetivo obter os par√¢metros que definem a melhor posi√ß√£o e orienta√ß√£o do hiperplano separador, minimizando os erros de classifica√ß√£o. $\blacksquare$

A compreens√£o da rela√ß√£o entre conjuntos afins, dist√¢ncias sinalizadas e modelos de classifica√ß√£o linear permite entender a geometria desses modelos e a forma como eles operam para classificar dados.

### Otimiza√ß√£o de Hiperplanos Separadores e Rela√ß√£o com SVM

```mermaid
graph LR
    subgraph "Hyperplane Optimization"
        direction TB
    A["Optimization Objective: 'min 1/2||Œ≤||¬≤'"] --> B["Constraints: 'y·µ¢(Œ≤‚ÇÄ + Œ≤·µÄx·µ¢) ‚â• 1'"]
    B --> C["Support Vectors"]
    C --> D["SVM Formulation"]
    end
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#fcf,stroke:#333,stroke-width:2px
    style C fill:#f9f,stroke:#333,stroke-width:2px
     style D fill:#ccf,stroke:#333,stroke-width:2px
```

A busca por **hiperplanos separadores** √≥timos √© uma extens√£o natural do conceito de conjuntos afins e dist√¢ncias sinalizadas, e √© central na formula√ß√£o das **M√°quinas de Vetores de Suporte (SVM)** [^4.5.2]. O objetivo √© encontrar um hiperplano que n√£o apenas separe as classes, mas que tamb√©m maximize a **margem** entre elas. A margem √© definida como a dist√¢ncia m√≠nima entre o hiperplano e as observa√ß√µes mais pr√≥ximas de cada classe.

A otimiza√ß√£o do hiperplano separador envolve a solu√ß√£o de um problema de otimiza√ß√£o convexa com restri√ß√µes de desigualdade. A fun√ß√£o objetivo busca maximizar a margem, e as restri√ß√µes garantem que todas as observa√ß√µes sejam classificadas corretamente e que estejam a uma dist√¢ncia m√≠nima do hiperplano [^4.5.2].

O problema de otimiza√ß√£o do hiperplano separador pode ser formulado da seguinte forma:

$$
    \min_{\beta, \beta_0} \frac{1}{2} ||\beta||^2
$$

sujeito a:
$$
    y_i (\beta_0 + \beta^T x_i) \geq 1, \text{ para } i = 1,...,N
$$
onde $y_i$ √© o r√≥tulo da classe da observa√ß√£o $i$, e $\beta$ e $\beta_0$ s√£o os par√¢metros do hiperplano.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simples com 3 pontos na classe +1: $x_1 = [1, 1]^T$, $x_2 = [2, 2]^T$, $x_3 = [3, 1]^T$ e 2 pontos na classe -1: $x_4 = [1, 3]^T$, $x_5 = [2, 3]^T$. O objetivo √© encontrar um hiperplano que maximize a margem entre as duas classes.
>
> O problema de otimiza√ß√£o a ser resolvido √©:
>
> $$
> \min_{\beta, \beta_0} \frac{1}{2} ||\beta||^2
> $$
>
> sujeito a:
>
> $$
>  \begin{cases}
>  \beta_0 + \beta^T x_1 \geq 1 \\
>  \beta_0 + \beta^T x_2 \geq 1 \\
>  \beta_0 + \beta^T x_3 \geq 1 \\
>  -(\beta_0 + \beta^T x_4) \geq 1 \\
>  -(\beta_0 + \beta^T x_5) \geq 1
>  \end{cases}
> $$
>
> Este problema pode ser resolvido usando um solver de otimiza√ß√£o. A solu√ß√£o, ap√≥s a otimiza√ß√£o, poderia ser algo como $\beta = [-1, 1]^T$ e $\beta_0 = 2$, que define um hiperplano $-x_1 + x_2 + 2 = 0$. A margem seria a dist√¢ncia entre o hiperplano e os pontos mais pr√≥ximos (os vetores de suporte).

O problema de otimiza√ß√£o acima representa a busca por um hiperplano que separa as classes com a maior margem poss√≠vel. Esse problema pode ser resolvido utilizando a teoria da dualidade de Wolfe e o m√©todo de multiplicadores de Lagrange, o que leva a uma formula√ß√£o dual do problema que √© mais f√°cil de resolver.

Na formula√ß√£o dual do problema, a solu√ß√£o para o hiperplano separador √≥timo √© expressa como uma combina√ß√£o linear dos **vetores de suporte**, que s√£o as observa√ß√µes que se encontram mais pr√≥ximas da fronteira de decis√£o (e que definem a margem). A maioria dos outros pontos de treino n√£o contribuem para a defini√ß√£o da solu√ß√£o final. A solu√ß√£o do problema dual √© dada por:

$$
   \max_{\alpha}  \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j
$$

sujeito a:
$$
  \sum_{i=1}^N \alpha_i y_i = 0
$$
$$
    \alpha_i \geq 0, \text{ para } i = 1,...,N
$$

onde $\alpha_i$ s√£o os multiplicadores de Lagrange. Os multiplicadores $\alpha_i$ que s√£o diferentes de zero correspondem aos vetores de suporte. Os par√¢metros do hiperplano s√£o calculados por meio desses multiplicadores, e as observa√ß√µes n√£o correspondentes a vetores de suporte n√£o influenciam a solu√ß√£o.

> üí° **Exemplo Num√©rico:**
>
> Usando os mesmos dados do exemplo anterior, o problema dual seria:
>
> $$
> \max_{\alpha} \sum_{i=1}^5 \alpha_i - \frac{1}{2} \sum_{i=1}^5 \sum_{j=1}^5 \alpha_i \alpha_j y_i y_j x_i^T x_j
> $$
>
> sujeito a:
>
> $$
> \alpha_1 + \alpha_2 + \alpha_3 - \alpha_4 - \alpha_5 = 0
> $$
>
> $$
> \alpha_i \geq 0, \text{ para } i = 1, ..., 5
> $$
>
> A solu√ß√£o deste problema, obtida atrav√©s de um solver de otimiza√ß√£o, nos daria valores para $\alpha_i$. Suponha que os valores resultantes fossem $\alpha_1 = 0.5$, $\alpha_2 = 0$, $\alpha_3 = 0.5$, $\alpha_4 = 0.5$, $\alpha_5 = 0$. Os vetores de suporte seriam $x_1$, $x_3$ e $x_4$ (onde $\alpha_i > 0$). Os par√¢metros $\beta$ e $\beta_0$ podem ser obtidos a partir destes multiplicadores e vetores de suporte.

A rela√ß√£o com as **M√°quinas de Vetores de Suporte (SVM)** √© direta, j√° que o SVM √© uma generaliza√ß√£o dessa abordagem ao permitir que o modelo lide com dados que n√£o s√£o linearmente separ√°veis, e utiliza uma formula√ß√£o similar para determinar o hiperplano separador. O SVM utiliza uma abordagem similar de otimiza√ß√£o para obter os par√¢metros e utiliza uma fun√ß√£o kernel para transformar os dados para um espa√ßo de maior dimensionalidade, onde a separa√ß√£o linear √© poss√≠vel.

A busca por hiperplanos separadores √≥timos, por meio da maximiza√ß√£o da margem, √© uma abordagem fundamental em muitos m√©todos de classifica√ß√£o linear, e desempenha um papel importante para garantir a capacidade de generaliza√ß√£o desses modelos.

**Lemma 45:** *A otimiza√ß√£o do hiperplano separador envolve a maximiza√ß√£o da margem entre as classes, o que leva a uma solu√ß√£o que √© expressa como uma combina√ß√£o linear dos vetores de suporte*.

*Prova:* A otimiza√ß√£o da margem leva a um problema de otimiza√ß√£o convexa cuja solu√ß√£o √© definida em termos dos vetores de suporte, que s√£o as observa√ß√µes mais pr√≥ximas √† fronteira de decis√£o.  $\blacksquare$

**Corol√°rio 45:** *A formula√ß√£o dual do problema de otimiza√ß√£o do hiperplano separador permite a obten√ß√£o de solu√ß√µes mais eficientes e a aplica√ß√£o de m√©todos de otimiza√ß√£o convexa padr√£o, sendo a base da formula√ß√£o das M√°quinas de Vetores de Suporte (SVM).*

*Prova:* A formula√ß√£o dual transforma o problema em um problema mais f√°cil de resolver, e leva a um modelo que se baseia nos vetores de suporte.  $\blacksquare$

A otimiza√ß√£o de hiperplanos separadores √© um conceito central no desenvolvimento de modelos de classifica√ß√£o lineares robustos e com boa capacidade de generaliza√ß√£o.

### Conclus√£o

Este cap√≠tulo explorou os conjuntos afins e o uso de dist√¢ncias sinalizadas a um hiperplano como base geom√©trica para modelos de classifica√ß√£o linear. Foi demonstrado como os perceptrons, a LDA e a regress√£o log√≠stica utilizam esses conceitos para definir fronteiras de decis√£o e classificar observa√ß√µes. A otimiza√ß√£o de hiperplanos separadores e sua rela√ß√£o com as M√°quinas de Vetores de Suporte (SVM) foi discutida. A compreens√£o desses conceitos geom√©tricos √© fundamental para entender os modelos de classifica√ß√£o linear e para o desenvolvimento de abordagens mais sofisticadas para modelagem e infer√™ncia estat√≠stica.

### Footnotes

[^4.1]: "In this chapter we revisit the classification problem and focus on linear methods for classification. Since our predictor G(x) takes values in a discrete set G, we can always divide the input space into a collection of regions labeled according to the classification. We saw in Chapter 2 that the boundaries of these regions can be rough or smooth, depending on the prediction function. For an important class of procedures, these decision boundaries are linear; this is what we will mean by linear methods for classification." *(Trecho de "The Elements of Statistical Learning")*

[^4.3]: "Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Œ£k = ‚àë. In comparing two classes k and l, it is sufficient to look at the log-ratio, and we see that" *(Trecho de "The Elements of Statistical Learning")*

[^4.4]: "The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0,1]." *(Trecho de "The Elements of Statistical Learning")*

[^4.5]: "In this situation the features are high-dimensional and correlated, and the LDA coefficients can be regularized to be smooth or sparse in the original domain of the signal. This leads to better generalization and allows for easier interpretation of the coefficients." *(Trecho de "The Elements of Statistical Learning")*

[^4.5.1]: "The first is the well-known perceptron model of Rosenblatt (1958), with an algorithm that finds a separating hyperplane in the training data, if one exists." *(Trecho de "The Elements of Statistical Learning")*

[^4.5.2]: "The second method, due to Vapnik (1996), finds an optimally separating hyperplane if one exists, else finds a hyperplane that minimizes some measure of overlap in the training data. We treat the separable case here, and defer treatment of the nonseparable case to Chapter 12." *(Trecho de "The Elements of Statistical Learning")*
