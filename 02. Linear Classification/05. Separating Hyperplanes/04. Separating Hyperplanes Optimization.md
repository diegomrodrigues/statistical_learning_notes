## Formulação e Solução do Problema de Otimização do Hiperplano Ótimo Separador

### Introdução

Como explorado na Seção 4.5, os classificadores baseados em **hiperplanos separadores** buscam construir fronteiras de decisão lineares que separam explicitamente os dados em classes distintas [^33]. Essa abordagem contrasta com métodos como *Linear Discriminant Analysis* (LDA) e *Logistic Regression*, que estimam fronteiras lineares de maneiras ligeiramente diferentes [^33]. A Seção 4.5.1 introduziu o algoritmo de aprendizagem do *perceptron* de Rosenblatt, que encontra um hiperplano separador, se existente. No entanto, uma limitação notável desse algoritmo é a não unicidade da solução; quando os dados são separáveis, existem infinitos hiperplanos separadores, e o algoritmo converge para um deles dependendo das condições iniciais [^31]. Para superar essa ambiguidade e potencialmente melhorar o desempenho da classificação, busca-se uma solução mais elegante através da adição de restrições adicionais ao hiperplano separador [^35].

Este capítulo foca no conceito de **hiperplano ótimo separador** (*optimal separating hyperplane*). Esta abordagem não apenas fornece uma solução única para o problema do hiperplano separador, mas o faz maximizando a **margem** (*margin*) entre as duas classes nos dados de treinamento [^1]. A intuição subjacente é que uma margem maior nos dados de treinamento levará a um melhor desempenho de classificação em dados de teste [^2], [^25]. O desenvolvimento deste conceito envolve um nível matemático um pouco mais elevado que as seções anteriores [^34], culminando em um problema de otimização convexa que pode ser resolvido eficientemente. Este método forma a base para os *support vector classifiers*, discutidos no Capítulo 12 [^33].

### Conceitos Fundamentais

#### Formulação Primal: Maximizando a Margem

O objetivo do hiperplano ótimo separador é encontrar a fronteira linear $f(x) = x^T\beta + \beta_o = 0$ que não apenas separa as duas classes (rotuladas como $y_i \in \{-1, 1\}$), mas também maximiza a distância até o ponto mais próximo de qualquer uma das classes [^1]. Formalmente, consideramos o seguinte problema de otimização [^3]:

> $$ \max_{\beta, \beta_o, ||\beta||=1} M $$
> $$ \text{sujeito a } y_i(x_i^T\beta + \beta_o) \geq M, \quad i=1, \dots, N $$

Aqui, $M$ representa a margem. As restrições garantem que todos os pontos de dados $x_i$ estejam corretamente classificados e localizados a uma distância assinada de pelo menos $M$ da fronteira de decisão $f(x)=0$ [^4]. Recordando a propriedade 3 da Seção 4.5.1 [^32], a distância assinada de um ponto $x$ ao hiperplano $L$ definido por $f(x)=0$ é $\frac{1}{||\beta||} f(x)$. Portanto, a restrição $y_i(x_i^T\beta + \beta_o) \geq M$ com $||\beta||=1$ implica que $y_i \frac{f(x_i)}{||\beta||} \geq M$, ou seja, a distância geométrica de cada ponto à fronteira é pelo menos $M$. Buscamos o maior valor de $M$ e os parâmetros $\beta, \beta_o$ associados [^5].

A restrição explícita $||\beta||=1$ pode ser inconveniente. Podemos eliminá-la reformulando as condições [^6]. Se $(\beta, \beta_o)$ satisfaz $y_i(x_i^T\beta + \beta_o) \geq M$, então a distância geométrica é $y_i \frac{f(x_i)}{||\beta||} \geq \frac{M}{||\beta||}$. Podemos reescrever a restrição como [^7]:
$$ y_i(x_i^T\beta + \beta_o) \geq M' $$
onde $M' = M||\beta||$. Como a escala de $\beta$ e $\beta_o$ pode ser ajustada arbitrariamente sem mudar o hiperplano (qualquer múltiplo positivo $(c\beta, c\beta_o)$ define a mesma fronteira e satisfaz as desigualdades se $(\beta, \beta_o)$ o fizer), podemos fixar a margem funcional $M'$ em 1 [^8]. Ou seja, estabelecemos $M||\beta|| = 1$, o que implica $||\beta|| = 1/M$. Maximizar a margem geométrica $M$ é, portanto, equivalente a minimizar $||\beta||$, ou, por conveniência, minimizar $\frac{1}{2}||\beta||^2$.

Assim, o problema de otimização (4.45) é equivalente a [^9]:

> $$ \min_{\beta, \beta_o} \frac{1}{2} ||\beta||^2 $$
> $$ \text{sujeito a } y_i(x_i^T\beta + \beta_o) \geq 1, \quad i=1, \dots, N $$

As restrições $y_i(x_i^T\beta + \beta_o) \geq 1$ definem uma "laje" vazia ou **margem** (*slab* or *margin*) em torno da fronteira de decisão linear [^10]. A espessura total desta margem é $2/||\beta||$ [^10]. O problema consiste em encontrar $\beta$ e $\beta_o$ que maximizem essa espessura, minimizando $||\beta||^2$ [^11]. Este é um **problema de otimização convexa**, especificamente, um problema de programação quadrática (critério quadrático com restrições de desigualdade lineares) [^12].

#### Dualidade de Lagrange e Pontos de Suporte

Para resolver o problema de otimização (4.48), utilizamos a **dualidade de Lagrange**. A função Lagrangiana (primal), a ser minimizada em relação a $\beta$ e $\beta_o$ e maximizada em relação aos multiplicadores de Lagrange $\alpha_i \geq 0$, é dada por [^13]:
$$ L_P = \frac{1}{2}||\beta||^2 - \sum_{i=1}^N \alpha_i [y_i(x_i^T\beta + \beta_o) - 1] $$
Igualando as derivadas de $L_P$ em relação a $\beta$ e $\beta_o$ a zero, obtemos as seguintes condições de otimalidade [^14]:
$$ \frac{\partial L_P}{\partial \beta} = \beta - \sum_{i=1}^N \alpha_i y_i x_i = 0 \implies \beta = \sum_{i=1}^N \alpha_i y_i x_i $$
$$ \frac{\partial L_P}{\partial \beta_o} = - \sum_{i=1}^N \alpha_i y_i = 0 \implies \sum_{i=1}^N \alpha_i y_i = 0 $$
Substituindo estas condições de volta em $L_P$, obtemos a função dual de Wolfe, $L_D$, que deve ser maximizada em relação a $\alpha_i$ [^15]:
$$ L_D = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i^T x_k $$
sujeito às restrições:\
$$ \alpha_i \geq 0, \quad i=1, \dots, N $$
$$ \sum_{i=1}^N \alpha_i y_i = 0 $$
A solução para o problema do hiperplano ótimo separador é então obtida maximizando $L_D$ no ortante positivo, sujeito à restrição de soma [^15], [^16]. Este problema dual é também um problema de otimização convexa (programação quadrática) e frequentemente mais simples de resolver, existindo *software* padrão para tal [^16].

Além das condições derivadas acima, a solução deve satisfazer as condições de **Karush-Kuhn-Tucker (KKT)**, que incluem as restrições primais ($y_i(x_i^T\beta + \beta_o) \geq 1$), as restrições duais ($\alpha_i \geq 0, \sum \alpha_i y_i = 0$), e a condição de *complementary slackness* [^17]:
$$ \alpha_i [y_i(x_i^T\beta + \beta_o) - 1] = 0 \quad \forall i $$
A condição de *complementary slackness* (4.53) tem implicações importantes [^17]:
1.  Se $\alpha_i > 0$, então necessariamente $y_i(x_i^T\beta + \beta_o) - 1 = 0$, o que significa que $y_i(x_i^T\beta + \beta_o) = 1$. Isso implica que o ponto $x_i$ está exatamente na fronteira da margem (ou *slab*) [^18]. Estes pontos são chamados de **pontos de suporte** (*support points* ou *support vectors*).
2.  Se $y_i(x_i^T\beta + \beta_o) > 1$, o ponto $x_i$ está estritamente dentro da região correta, fora da margem. Neste caso, a condição KKT força $\alpha_i = 0$ [^19].

Da condição $\beta = \sum \alpha_i y_i x_i$ [^14], vemos que o vetor de solução $\beta$ é definido como uma combinação linear *apenas* daqueles pontos $x_i$ para os quais $\alpha_i > 0$, ou seja, os **pontos de suporte** [^20]. Todos os outros pontos, para os quais $\alpha_i = 0$, não têm influência direta na determinação de $\beta$. A Figura 4.16 ilustra isso para um exemplo simples, mostrando três pontos de suporte que definem o hiperplano ótimo [^21]. O parâmetro $\beta_o$ (o *intercept*) pode ser determinado usando a condição KKT $y_i(x_i^T\beta + \beta_o) = 1$ para qualquer ponto de suporte $x_i$ (onde $\alpha_i > 0$) [^22]. Por exemplo, se $x_s$ é um ponto de suporte com $y_s=1$, então $x_s^T\beta + \beta_o = 1$, logo $\beta_o = 1 - x_s^T\beta$. Na prática, é mais robusto calcular $\beta_o$ como uma média sobre todos os pontos de suporte.

#### Classificação e Propriedades

Uma vez determinados $\beta$ e $\beta_o$, o hiperplano ótimo separador produz a função de decisão $f(x) = x^T\beta + \beta_o$. Novas observações $x$ são classificadas com base no sinal desta função [^23]:
$$ G(x) = \text{sign}(f(x)) = \text{sign}(x^T\beta + \beta_o) $$
Por construção, nenhuma das observações de treinamento cai *dentro* da margem definida por $y(x^T\beta + \beta_o)=1$ [^24]. No entanto, observações de teste podem cair dentro desta margem [^24].

A formulação em termos de pontos de suporte sugere que o hiperplano ótimo se concentra nos pontos mais "difíceis" ou informativos (aqueles na fronteira da margem), tornando o método potencialmente mais robusto a erros de especificação do modelo em comparação com o LDA, que depende de todos os pontos de dados, mesmo aqueles longe da fronteira de decisão [^26]. É importante notar, contudo, que a identificação dos pontos de suporte requer o uso de todos os dados durante o processo de otimização [^27].

A Figura 4.16 também mostra a solução obtida por *logistic regression* para o mesmo problema [^28]. Neste caso, as duas soluções são muito semelhantes [^28]. De fato, quando um hiperplano separador existe, o algoritmo de *logistic regression* (ajustado por máxima verossimilhança) sempre o encontrará, pois a log-verossimilhança pode ser levada a zero (ou $-\infty$ na prática, levando a parâmetros infinitos) [^29]. A solução de *logistic regression* compartilha algumas características qualitativas com a solução do hiperplano ótimo separador, como atribuir pesos maiores (implicitamente, através da matriz Hessiana na otimização) aos pontos próximos da fronteira de decisão [^30].

Finalmente, é crucial notar que a formulação apresentada aqui assume que os dados são linearmente separáveis. Quando os dados não são separáveis, não haverá solução viável para o problema (4.48) [^31]. Uma formulação alternativa, conhecida como *support vector machine*, é necessária para lidar com o caso de sobreposição entre as classes, minimizando uma medida da extensão dessa sobreposição. Esta extensão é discutida no Capítulo 12 [^31], [^35].

### Conclusão

O **hiperplano ótimo separador** oferece uma solução única e geometricamente motivada para o problema de encontrar uma fronteira de decisão linear entre duas classes. Ao maximizar a **margem** entre as classes, busca-se melhorar a capacidade de generalização do classificador. A formulação matemática leva a um **problema de otimização convexa** (programação quadrática), que pode ser resolvido eficientemente usando a **dualidade de Lagrange**.

A solução resultante é elegantemente caracterizada pelas **condições KKT**, que revelam a importância crucial dos **pontos de suporte** – os pontos de dados que se situam exatamente nas bordas da margem. O vetor normal ao hiperplano, $\beta$, é uma combinação linear apenas desses pontos de suporte, conferindo ao método uma certa robustez em comparação com métodos que utilizam todos os dados, como o LDA. Embora a identificação dos pontos de suporte dependa de todo o conjunto de dados, a estrutura da solução final é mais localizada.

Este método estabelece a base conceitual para as *Support Vector Machines* (SVMs), que estendem essas ideias para lidar com dados não linearmente separáveis e introduzem o uso de *kernels* para criar fronteiras não lineares em espaços de características de maior dimensão, como será visto no Capítulo 12 [^31], [^33].

### Referências

[^1]: The optimal separating hyperplane separates the two classes and maximizes the distance to the closest point from either class (Vapnik, 1996). [Page 132]
[^2]: Maximizing the margin between the two classes on the training data leads to better classification performance on test data. [Page 132]
[^3]: Consider the optimization problem: max M subject to yi(xTiβ + βo) ≥ M, i = 1, ..., N, and ||β||=1. [Page 132, Eq. 4.45]
[^4]: The set of conditions ensure that all the points are at least a signed distance M from the decision boundary defined by β and βo. [Page 132]
[^5]: We seek the largest such M and associated parameters. [Page 132]
[^6]: We can get rid of the ||β|| = 1 constraint by replacing the conditions with (1/||β||)yi(xTiβ + βo) ≥ M. [Page 132, Eq. 4.46]
[^7]: Equivalently, yi(xTiβ + βo) ≥ M||β||. [Page 132, Eq. 4.47]
[^8]: Since for any β and βo satisfying these inequalities, any positively scaled multiple satisfies them too, we can arbitrarily set ||β|| = 1/M. [Page 132]
[^9]: Thus (4.45) is equivalent to min (1/2)||β||² subject to yi(xTiβ + βo) ≥ 1, i = 1, ..., N. [Page 132, Eq. 4.48]
[^10]: In light of (4.40) [f(x) proportional to signed distance], the constraints define an empty slab or margin around the linear decision boundary of thickness 1/||β||. [Page 132, referencing Eq. 4.40 on page 130]
[^11]: We choose β and βo to maximize its thickness (1/||β||). [Page 132]
[^12]: This is a convex optimization problem (quadratic criterion with linear inequality constraints). [Page 132-133]
[^13]: The Lagrange (primal) function, to be minimized w.r.t. β and βo, is Lp = (1/2)||β||² - Σ αi[yi(xTiβ + βo) - 1], with αi ≥ 0. [Page 133, Eq. 4.49]
[^14]: Setting the derivatives to zero, we obtain: β = Σ αi yi xi and Σ αi yi = 0. [Page 133, Eq. 4.50, 4.51]
[^15]: Substituting these in (4.49) we obtain the so-called Wolfe dual LD = Σ αi - (1/2) Σ Σ αi αk yi yk xTi xk, subject to αi ≥ 0 and Σ αi yi = 0. [Page 133, Eq. 4.52]
[^16]: The solution is obtained by maximizing LD in the positive orthant, a simpler convex optimization problem, for which standard software can be used. [Page 133]
[^17]: The solution must satisfy the Karush-Kuhn-Tucker (KKT) conditions, which include (4.50), (4.51), (4.52) and αi[yi(xTiβ + βo) - 1] = 0 ∀i. [Page 133, Eq. 4.53]
[^18]: From KKT: if αi > 0, then yi(xTiβ + βo) = 1, meaning xi is on the boundary of the slab. [Page 133]
[^19]: From KKT: if yi(xTiβ + βo) > 1, xi is not on the boundary of the slab, and αi = 0. [Page 133]
[^20]: From (4.50), the solution vector β is defined in terms of a linear combination of the *support points* xi—those points defined to be on the boundary of the slab via αi > 0. [Page 133]
[^21]: Figure 4.16 shows the optimal separating hyperplane for our toy example; there are three support points. [Page 133]
[^22]: βo is obtained by solving (4.53) [using αi[yi(xTiβ + βo) - 1] = 0] for any of the support points (where αi > 0). [Page 133]
[^23]: The optimal separating hyperplane produces a function f(x) = xTβ + βo for classifying new observations: G(x) = sign(f(x)). [Page 133, Eq. 4.54]
[^24]: Although none of the training observations fall in the margin (by construction), this will not necessarily be the case for test observations. [Page 133]
[^25]: The intuition is that a large margin on the training data will lead to good separation on the test data. [Page 134]
[^26]: The description of the solution in terms of support points suggests that the optimal hyperplane focuses more on the points that count, and is more robust to model misspecification compared to LDA. [Page 134]
[^27]: The identification of these support points required the use of all the data. [Page 134]
[^28]: Included in Figure 4.16 is the logistic regression solution... Both solutions are similar in this case. [Page 134]
[^29]: When a separating hyperplane exists, logistic regression will always find it... [Page 134]
[^30]: The logistic regression solution shares some other qualitative features with the separating hyperplane solution... weights are larger for points near the decision boundary... [Page 134]
[^31]: When the data are not separable, there will be no feasible solution to this problem, and an alternative formulation is needed (Support Vector Machine, Chapter 12). [Page 134, referencing page 135]
[^32]: (Context from previous section 4.5.1, page 130) The signed distance of any point x to L {x | f(x)=0} is (1/||β||)f(x). [Eq. 4.40]
[^33]: (Context from section 4.5, page 129) Separating hyperplane classifiers construct linear decision boundaries that explicitly try to separate the data into different classes. They provide the basis for support vector classifiers (Chapter 12).
[^34]: (Context from section 4.5, page 129) Mathematical level of this section is somewhat higher.
[^35]: (Context from section 4.5.1, page 131) A rather elegant solution to the [perceptron algorithm's non-uniqueness] problem is to add additional constraints to the separating hyperplane.

<!-- END -->