### Uso da Verossimilhan√ßa Marginal como Regularizador e Discuss√£o da Degeneresc√™ncia com Separa√ß√£o Perfeita

```mermaid
graph LR
    subgraph "Marginal Likelihood as Regularizer"
        direction TB
        A["Data (X, Y)"] --> B("Joint Distribution Model Pr(X, Y) or Pr(X, G)")
        B --> C("Marginal Likelihood Maximization")
        C --> D("Regularized Parameter Estimates")
        D --> E("Reduced Overfitting & Increased Stability")
    end
```

A **verossimilhan√ßa marginal**, al√©m de fornecer uma maneira mais completa de modelar os dados, pode ser utilizada como uma ferramenta de **regulariza√ß√£o** em modelos de classifica√ß√£o, atuando como um mecanismo para evitar o *overfitting* e aumentar a estabilidade das estimativas dos par√¢metros. A regulariza√ß√£o, nesse contexto, imp√µe uma restri√ß√£o na complexidade do modelo e evita que ele se ajuste excessivamente aos ru√≠dos ou particularidades dos dados de treinamento. A discuss√£o da degeneresc√™ncia, especialmente em casos de **separa√ß√£o perfeita** das classes, permite entender como a verossimilhan√ßa marginal pode ser utilizada para garantir a estabilidade dos modelos.

**Verossimilhan√ßa Marginal como Regularizador:**

A abordagem tradicional de modelos como a regress√£o log√≠stica envolve a maximiza√ß√£o da verossimilhan√ßa condicional $Pr(Y|X)$, o que muitas vezes leva a modelos complexos e inst√°veis, especialmente quando o n√∫mero de par√¢metros √© compar√°vel ou maior que o tamanho da amostra. A verossimilhan√ßa marginal, por outro lado, modela a distribui√ß√£o conjunta de $X$ e $Y$ ou $G$, e pode ser usada para construir estimadores regularizados que controlam a complexidade do modelo e melhoram a capacidade de generaliza√ß√£o.

```mermaid
graph LR
    subgraph "Likelihood Approaches"
        direction LR
        A["Conditional Likelihood: Pr(Y|X)"] --> B["Maximization of Pr(Y|X)"]
        B --> C["Complex and Potentially Unstable Models"]
        D["Marginal Likelihood: Pr(X,Y)"] --> E["Maximization of Pr(X,Y)"]
        E --> F["Regularized, Stable Models"]
    end
```

Quando a verossimilhan√ßa marginal √© utilizada, a fun√ß√£o objetivo a ser maximizada inclui a distribui√ß√£o marginal de $X$ como parte do modelo, o que leva a uma estimativa dos par√¢metros que leva em conta a distribui√ß√£o conjunta de $X$ e $G$. Essa abordagem, em geral, adiciona mais informa√ß√µes sobre os dados, j√° que incorpora n√£o apenas o modelo de $G|X$, mas tamb√©m modela as vari√°veis preditoras $X$.

> üí° **Exemplo Num√©rico:**
>
> Considere um problema de classifica√ß√£o bin√°ria com duas vari√°veis preditoras ($X_1$ e $X_2$) e uma vari√°vel resposta bin√°ria $Y$ (0 ou 1). Suponha que temos os seguintes dados de treinamento:
>
> | $X_1$ | $X_2$ | $Y$ |
> |-------|-------|-----|
> | 1     | 2     | 0   |
> | 1.5   | 1.8   | 0   |
> | 2     | 3     | 0   |
> | 5     | 7     | 1   |
> | 6     | 6.5   | 1   |
> | 7     | 8     | 1   |
>
> **Verossimilhan√ßa Condicional (Regress√£o Log√≠stica):**
>
> A regress√£o log√≠stica modelaria $P(Y|X)$. A maximiza√ß√£o da verossimilhan√ßa condicional levaria a par√¢metros que se ajustam bem aos dados de treinamento, mas podem sofrer *overfitting*. Por exemplo, os par√¢metros poderiam levar a uma fronteira de decis√£o complexa que se ajusta aos pontos de treinamento, mas n√£o generaliza bem.
>
> **Verossimilhan√ßa Marginal (LDA):**
>
> A LDA modelaria a distribui√ß√£o conjunta de $X$ e $Y$. Isso significa que o modelo consideraria a distribui√ß√£o de $X_1$ e $X_2$ para cada classe de $Y$. A maximiza√ß√£o da verossimilhan√ßa marginal levaria a par√¢metros que consideram tanto a rela√ß√£o entre $X$ e $Y$ quanto a distribui√ß√£o de $X$. Isso tende a produzir fronteiras de decis√£o mais simples e est√°veis, reduzindo o *overfitting*.
>
> Em termos pr√°ticos, a LDA pode, por exemplo, assumir que as vari√°veis preditoras t√™m uma distribui√ß√£o normal multivariada para cada classe, com m√©dias diferentes, mas mesma matriz de covari√¢ncia. Ao modelar a distribui√ß√£o de $X$, a LDA n√£o apenas se ajusta a rela√ß√£o entre $X$ e $Y$, mas tamb√©m considera a estrutura de $X$, o que leva a uma solu√ß√£o mais regularizada.

Ao maximizar a verossimilhan√ßa marginal, √© poss√≠vel obter estimativas mais est√°veis dos par√¢metros e evitar problemas de *overfitting*, onde o modelo se ajusta muito bem aos dados de treinamento, mas apresenta um mau desempenho em dados n√£o vistos. A verossimilhan√ßa marginal, nesse sentido, pode ser vista como um regularizador impl√≠cito que controla a complexidade do modelo, j√° que o modelo deve se ajustar √† distribui√ß√£o dos preditores, al√©m do efeito dos preditores sobre a vari√°vel resposta.

Em termos bayesianos, a verossimilhan√ßa marginal corresponde √† probabilidade marginal dos dados, que √© obtida pela integra√ß√£o sobre os par√¢metros, ou seja, integrando a verossimilhan√ßa conjunta para todos os valores poss√≠veis dos par√¢metros. Essa abordagem tem como efeito penalizar modelos que atribuem altas probabilidades aos dados de treinamento, mas que levam a uma probabilidade marginal baixa.

**Discuss√£o da Degeneresc√™ncia com Separa√ß√£o Perfeita:**

Um problema importante em modelos de classifica√ß√£o com fronteiras lineares √© o fen√¥meno da **separa√ß√£o perfeita**, que ocorre quando um hiperplano pode separar perfeitamente os dados de treinamento em classes distintas. Em modelos de regress√£o log√≠stica com verossimilhan√ßa condicional, quando a separa√ß√£o perfeita ocorre, a verossimilhan√ßa pode aumentar indefinidamente (atingindo valores infinitos), e as estimativas de m√°xima verossimilhan√ßa dos par√¢metros $\beta$ tornam-se infinitas ou indefinidas, o que causa uma degeneresc√™ncia nos resultados [^4.5].

```mermaid
graph LR
    subgraph "Perfect Separation Scenario"
        direction TB
        A["Perfectly Separable Data"] --> B["Conditional Likelihood (Logistic Regression)"]
        B --> C["Likelihood approaches infinity"]
        B --> D["Unstable/Infinite Parameter Estimates"]
        A --> E["Marginal Likelihood (LDA)"]
         E --> F["Stable Parameter Estimates"]
    end
```

> üí° **Exemplo Num√©rico:**
>
> Considere um conjunto de dados com duas vari√°veis preditoras ($X_1$ e $X_2$) e uma vari√°vel resposta bin√°ria ($Y$) em que as classes est√£o perfeitamente separadas:
>
> | $X_1$ | $X_2$ | $Y$ |
> |-------|-------|-----|
> | 1     | 1     | 0   |
> | 1     | 2     | 0   |
> | 2     | 1     | 0   |
> | 4     | 4     | 1   |
> | 5     | 5     | 1   |
> | 6     | 4     | 1   |
>
> **Regress√£o Log√≠stica (Separa√ß√£o Perfeita):**
>
> Ao tentar ajustar um modelo de regress√£o log√≠stica a esses dados, o algoritmo de otimiza√ß√£o tentaria encontrar uma fronteira de decis√£o que separasse perfeitamente as classes. Isso levaria a coeficientes muito grandes (tendendo ao infinito), pois a verossimilhan√ßa aumentaria indefinidamente √† medida que a fronteira de decis√£o se torna mais precisa.
>
> **LDA (Verossimilhan√ßa Marginal):**
>
> A LDA, ao modelar a distribui√ß√£o conjunta de $X$ e $Y$, n√£o sofreria com essa degeneresc√™ncia. O modelo LDA estimaria par√¢metros que equilibram a separa√ß√£o das classes com a estrutura de $X$. Mesmo que haja separa√ß√£o perfeita, a LDA ainda forneceria coeficientes finitos e razo√°veis, pois a distribui√ß√£o marginal de $X$ imp√µe uma penalidade sobre solu√ß√µes extremas.
>
> **Visualiza√ß√£o:**
>
> ```mermaid
> graph LR
>     A["Dados Separados Perfeitamente"] --> B("Regress√£o Log√≠stica: Coeficientes ‚Üí ‚àû");
>     A --> C("LDA: Coeficientes Est√°veis");
> ```
>
> Nesse caso, a regress√£o log√≠stica tende a produzir par√¢metros inst√°veis e muito grandes, enquanto a LDA fornece par√¢metros est√°veis.

A degeneresc√™ncia impede que o modelo generalize bem para dados n√£o vistos, j√° que a estima√ß√£o dos par√¢metros n√£o est√° estabilizada pela fun√ß√£o de verossimilhan√ßa e pode resultar em fronteiras de decis√£o que n√£o s√£o relevantes em outros dados.

O uso da verossimilhan√ßa marginal pode mitigar esse problema, j√° que a verossimilhan√ßa marginal imp√µe uma restri√ß√£o na complexidade do modelo e evita que ele se ajuste excessivamente aos dados de treinamento. A verossimilhan√ßa marginal, neste caso, atua como um regularizador, controlando a magnitude dos par√¢metros e evitando que eles divirjam quando os dados apresentam uma separabilidade perfeita. Isso ocorre porque a modelagem marginal de $X$ adiciona termos que atuam como penalidades na fun√ß√£o de verossimilhan√ßa.

Modelos com verossimilhan√ßa marginal, como a LDA, podem fornecer coeficientes bem definidos, mesmo em casos de separa√ß√£o perfeita, e pode, em cen√°rios espec√≠ficos, ser uma alternativa √† regress√£o log√≠stica. Em muitos casos, para tornar a log√≠stica robusta √† separa√ß√£o perfeita, a introdu√ß√£o de penalidades √© necess√°ria para evitar a degeneresc√™ncia.

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar a estabilidade dos coeficientes da LDA em casos de separa√ß√£o perfeita, vamos considerar um exemplo com dados simulados.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LogisticRegression
> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>
> # Dados com separa√ß√£o perfeita
> X = np.array([[1, 1], [1, 2], [2, 1], [4, 4], [5, 5], [6, 4]])
> y = np.array([0, 0, 0, 1, 1, 1])
>
> # Regress√£o Log√≠stica
> logistic = LogisticRegression(solver='liblinear')
> logistic.fit(X, y)
> print("Regress√£o Log√≠stica - Coeficientes:", logistic.coef_)
>
> # LDA
> lda = LinearDiscriminantAnalysis()
> lda.fit(X, y)
> print("LDA - Coeficientes:", lda.coef_)
> ```
>
> **Resultados:**
>
> A Regress√£o Log√≠stica pode apresentar coeficientes muito grandes, especialmente em casos de separa√ß√£o perfeita, indicando instabilidade. A LDA, por outro lado, geralmente fornece coeficientes mais est√°veis e de magnitude menor.
>
> **Interpreta√ß√£o:**
>
> Enquanto a regress√£o log√≠stica tenta ajustar uma fronteira de decis√£o o mais precisamente poss√≠vel aos dados de treinamento, levando a coeficientes que podem ser muito grandes e inst√°veis, a LDA considera a distribui√ß√£o marginal de X, e isso leva a uma solu√ß√£o mais est√°vel. Essa estabilidade √© crucial para a capacidade de generaliza√ß√£o do modelo.

```mermaid
graph LR
   subgraph "Stability Comparison"
    direction LR
      A["Logistic Regression"] --> B["Tries to Perfectly Fit"]
    B --> C["Large and Unstable Coefficients"]
        D["LDA"] --> E["Considers Marginal Distribution of X"]
        E --> F["More Stable Coefficients"]
    end
```

Em resumo, a verossimilhan√ßa marginal pode servir como uma forma de regulariza√ß√£o, evitando *overfitting*, reduzindo a vari√¢ncia dos estimadores, e tratando problemas de degeneresc√™ncia em casos de separa√ß√£o perfeita.

**Lemma 34:** *A verossimilhan√ßa marginal, ao modelar a distribui√ß√£o de X, pode atuar como um regularizador impl√≠cito, evitando o ajuste excessivo aos dados de treinamento e fornecendo estimativas mais est√°veis dos par√¢metros.*

*Prova:* Ao incorporar termos relativos √† distribui√ß√£o marginal dos dados, modelos que utilizam a verossimilhan√ßa marginal tendem a ter uma maior estabilidade dos coeficientes, evitando *overfitting* e reduzindo sua vari√¢ncia.  $\blacksquare$

**Corol√°rio 34:** *Em problemas com separa√ß√£o perfeita, a verossimilhan√ßa marginal pode mitigar a degeneresc√™ncia que ocorre com a verossimilhan√ßa condicional, levando a estimativas mais robustas e bem definidas dos par√¢metros.*

*Prova:* A modelagem conjunta de $X$ e $G$ por meio da verossimilhan√ßa marginal evita que os par√¢metros tendam ao infinito em casos de separa√ß√£o perfeita, mantendo a estabilidade e a interpretabilidade dos resultados. [^4.5] $\blacksquare$

A verossimilhan√ßa marginal oferece uma abordagem interessante para a regulariza√ß√£o, e pode ser uma forma de tratar problemas de degeneresc√™ncia em problemas de classifica√ß√£o.

### Interpreta√ß√£o dos Coeficientes e Limita√ß√µes da Verossimilhan√ßa Marginal

```mermaid
graph LR
   subgraph "Coefficient Interpretation Challenges"
        direction TB
        A["Marginal Likelihood Models"] --> B["Parameters influenced by Pr(X)"]
        B --> C["Complex parameter interpretation"]
         C --> D["Loss of direct odds ratio interpretation"]
        E["Conditional Likelihood Models"] --> F["Direct odds ratio interpretation"]

    end
```

Apesar dos benef√≠cios em termos de robustez e regulariza√ß√£o, o uso da **verossimilhan√ßa marginal** em modelos de classifica√ß√£o pode trazer alguns desafios em rela√ß√£o √† **interpreta√ß√£o dos coeficientes** e √†s **limita√ß√µes** da abordagem [^4.5].

Em modelos que utilizam a verossimilhan√ßa condicional, como a regress√£o log√≠stica, a interpreta√ß√£o dos coeficientes $\beta_j$ √© relativamente direta. O coeficiente $\beta_j$ representa a mudan√ßa no log das *odds* para um aumento unit√°rio na vari√°vel preditora $x_j$, mantendo as demais vari√°veis constantes. O *odds ratio*, $e^{\beta_j}$, quantifica a mudan√ßa nas *odds* da ocorr√™ncia do evento para um aumento unit√°rio no preditor. Essa interpretabilidade √© uma das vantagens da regress√£o log√≠stica e √© muito utilizada em aplica√ß√µes pr√°ticas.

> üí° **Exemplo Num√©rico:**
>
> Considere um modelo de regress√£o log√≠stica com duas vari√°veis preditoras, $X_1$ (idade) e $X_2$ (n√≠vel de educa√ß√£o), e uma vari√°vel resposta bin√°ria $Y$ (probabilidade de comprar um produto). Suponha que, ap√≥s ajustar o modelo, obtivemos os seguintes coeficientes:
>
> - $\beta_0 = -2.0$ (intercepto)
> - $\beta_1 = 0.05$ (coeficiente de idade)
> - $\beta_2 = 0.8$ (coeficiente de n√≠vel de educa√ß√£o)
>
> A interpreta√ß√£o seria:
>
> - Para cada ano adicional de idade, mantendo o n√≠vel de educa√ß√£o constante, o log das *odds* de comprar o produto aumenta em 0.05. O *odds ratio* √© $e^{0.05} \approx 1.05$, o que significa que as *odds* de comprar o produto aumentam em aproximadamente 5% para cada ano adicional de idade.
> - Para cada n√≠vel de educa√ß√£o adicional, mantendo a idade constante, o log das *odds* de comprar o produto aumenta em 0.8. O *odds ratio* √© $e^{0.8} \approx 2.23$, o que significa que as *odds* de comprar o produto aumentam em aproximadamente 123% para cada n√≠vel de educa√ß√£o adicional.
>
> Essa interpreta√ß√£o √© direta e √∫til para entender o impacto de cada vari√°vel preditora na probabilidade de compra.

Em modelos que utilizam a verossimilhan√ßa marginal, a interpreta√ß√£o dos coeficientes torna-se mais complexa, j√° que a fun√ß√£o objetivo inclui a distribui√ß√£o marginal das vari√°veis preditoras. Os coeficientes $\beta_j$ n√£o representam mais a mudan√ßa no log das *odds*, pois a rela√ß√£o entre a resposta e os preditores √© mediada pela distribui√ß√£o conjunta de $X$ e $G$. Nesse sentido, os coeficientes perdem a interpreta√ß√£o simples como em modelos que utilizam a verossimilhan√ßa condicional.

A interpreta√ß√£o dos par√¢metros, nesse sentido, √© mais complexa, e depende de como a densidade marginal Pr(X) √© modelada. Os coeficientes passam a ter uma interpreta√ß√£o dependente do modelo utilizado para a modelagem conjunta de $X$ e $G$.

```mermaid
graph LR
   subgraph "Interpretation Dependency"
    direction TB
      A["Parameters in Marginal Likelihood"] --> B["Dependent on Pr(X)"]
        B --> C["Interpretation context-dependent"]
            C --> D["Complex understanding"]
    end
```

> üí° **Exemplo Num√©rico:**
>
> Considere os mesmos dados do exemplo anterior, mas agora usando LDA (que utiliza a verossimilhan√ßa marginal). Os coeficientes obtidos n√£o ter√£o mais a mesma interpreta√ß√£o direta em termos de *odds ratio*.
>
> Suponha que os coeficientes obtidos da LDA sejam:
>
> - $\beta_1 = 0.1$ (coeficiente de idade)
> - $\beta_2 = 0.5$ (coeficiente de n√≠vel de educa√ß√£o)
>
> Esses coeficientes n√£o podem ser interpretados como o impacto direto no log das *odds* como na regress√£o log√≠stica. Eles refletem o efeito conjunto de $X$ e $Y$, levando em considera√ß√£o a distribui√ß√£o marginal de $X$. A interpreta√ß√£o exata depender√° da forma da distribui√ß√£o marginal de $X$ que o modelo LDA assume (no caso, uma normal multivariada para cada classe).
>
> A interpreta√ß√£o em modelos com verossimilhan√ßa marginal √© mais complexa e contextual, dependendo da forma como a densidade marginal $P(X)$ √© modelada.

Al√©m disso, a complexidade dos modelos que utilizam a verossimilhan√ßa marginal, como a LDA, pode dificultar a compreens√£o intuitiva da influ√™ncia de cada preditor na resposta. Em modelos log√≠sticos, as rela√ß√µes s√£o diretas, e o efeito de cada preditor pode ser facilmente calculado e interpretado, enquanto modelos que utilizam a verossimilhan√ßa marginal podem ter um comportamento mais complexo e dif√≠cil de intuir.

A complexidade na interpreta√ß√£o dos par√¢metros pode levar aos seguintes desafios:

1.  **Dificuldade em entender os efeitos:** Em modelos com verossimilhan√ßa marginal, a influ√™ncia de cada preditor na resposta √© indireta, e mediada pela distribui√ß√£o conjunta dos dados. A interpreta√ß√£o dos par√¢metros pode ser mais dif√≠cil e menos intuitiva, necessitando de maior cuidado ao interpretar os resultados.

2.  **Impacto das Suposi√ß√µes:** A interpreta√ß√£o dos par√¢metros depende das suposi√ß√µes que s√£o feitas na modelagem da distribui√ß√£o de $X$. Diferentes suposi√ß√µes podem levar a resultados diferentes, o que torna a interpreta√ß√£o mais sens√≠vel √† escolha do modelo.

3.  **Dificuldade na Compara√ß√£o:** Modelos com verossimilhan√ßa condicional e marginal, por utilizarem diferentes m√©tricas para a estimativa dos par√¢metros, dificilmente podem ser comparados diretamente em termos de interpreta√ß√£o de resultados.

4.  **Perda de Informa√ß√£o:** A verossimilhan√ßa marginal pode obscurecer o efeito direto dos preditores sobre a resposta, j√° que os coeficientes passam a depender do efeito de cada preditor na distribui√ß√£o marginal de X, dificultando a identifica√ß√£o de preditores chave.

```mermaid
graph LR
    subgraph "Challenges in Marginal Likelihood Interpretation"
        direction TB
        A["Complex Effects"] --> B["Indirect influence of predictors"]
        A --> C["Sensitive to Assumptions"]
        C --> D["Model-dependent interpretations"]
        A --> E["Difficult Comparisons"]
         E --> F["Different metrics for parameters"]
    end
```

√â importante ressaltar que a perda de interpretabilidade dos coeficientes √© um *trade-off* pelo aumento da robustez e pela utiliza√ß√£o de mais informa√ß√µes nos dados. Em situa√ß√µes onde a interpretabilidade do modelo n√£o √© uma prioridade, o uso da verossimilhan√ßa marginal pode ser vantajoso, mas para casos em que a interpreta√ß√£o do efeito das vari√°veis preditoras sobre a resposta seja central, modelos que utilizam a verossimilhan√ßa condicional, como a regress√£o log√≠stica, podem ser mais apropriados.

**Lemma 35:** *A modelagem da distribui√ß√£o marginal de X em modelos que utilizam a verossimilhan√ßa marginal dificulta a interpreta√ß√£o dos coeficientes, pois estes passam a depender da distribui√ß√£o conjunta de X e G.*

*Prova:* Ao modelar explicitamente a distribui√ß√£o de X, os par√¢metros perdem a interpreta√ß√£o direta em termos do efeito de cada preditor sobre a resposta. [^4.5] $\blacksquare$

**Corol√°rio 35:** *A interpreta√ß√£o dos par√¢metros em modelos que utilizam a verossimilhan√ßa marginal depende das suposi√ß√µes sobre a distribui√ß√£o de X, e da escolha do modelo para a distribui√ß√£o marginal, o que dificulta a compara√ß√£o com modelos que utilizam a verossimilhan√ßa condicional.*

*Prova:* A interpreta√ß√£o dos par√¢metros √© mediada pela forma como a distribui√ß√£o marginal de X √© modelada, o que torna a interpreta√ß√£o mais complexa e dependente das suposi√ß√µes feitas. $\blacksquare$

A escolha entre verossimilhan√ßa condicional e marginal envolve um *trade-off* entre interpretabilidade e robustez, e deve ser feita considerando os objetivos do problema.

### Conclus√£o

Este cap√≠tulo explorou o uso da verossimilhan√ßa marginal como um regularizador, enfatizando como ela pode atuar para evitar *overfitting* e aumentar a estabilidade dos modelos. A discuss√£o da degeneresc√™ncia com separa√ß√£o perfeita mostrou como a verossimilhan√ßa marginal pode ser usada para mitigar esse problema. Al√©m disso, foram discutidos os desafios de interpretabilidade e infer√™ncia em modelos que utilizam a verossimilhan√ßa marginal, que incluem um aumento da complexidade e uma maior dificuldade na interpreta√ß√£o dos par√¢metros. Os conceitos apresentados neste cap√≠tulo s√£o fundamentais para entender as vantagens e limita√ß√µes dos m√©todos de classifica√ß√£o que v√£o al√©m da modelagem da verossimilhan√ßa condicional, fornecendo uma vis√£o mais abrangente das t√©cnicas de modelagem e infer√™ncia estat√≠stica.

### Footnotes

[^4.1]: "In this chapter we revisit the classification problem and focus on linear methods for classification. Since our predictor G(x) takes values in a discrete set G, we can always divide the input space into a collection of regions labeled according to the classification. We saw in Chapter 2 that the boundaries of these regions can be rough or smooth, depending on the prediction function. For an important class of procedures, these decision boundaries are linear; this is what we will mean by linear methods for classification." *(Trecho de "The Elements of Statistical Learning")*

[^4.3]: "Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix $\Sigma_k = \sum$. In comparing two classes k and l, it is sufficient to look at the log-ratio, and we see that" *(Trecho de "The Elements of Statistical Learning")*

[^4.4]: "The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0,1]." *(Trecho de "The Elements of Statistical Learning")*

[^4.4.1]: "Logistic regression models are usually fit by maximum likelihood, using the conditional likelihood of G given X. Since Pr(G|X) completely specifies the conditional distribution, the multinomial distribution is appropriate. The log-likelihood for N observations is" *(Trecho de "The Elements of Statistical Learning")*

[^4.4.4]:  "The L‚ÇÅ penalty used in the lasso (Section 3.4.2) can be used for variable selection and shrinkage with any linear regression model. For logistic regression, we would maximize a penalized version of (4.20):" *(Trecho de "The Elements of Statistical Learning")*

[^4.5]: "In this situation the features are high-dimensional and correlated, and the LDA coefficients can be regularized to be smooth or sparse in the original domain of the signal. This leads to better generalization and allows for easier interpretation of the coefficients." *(Trecho de "The Elements of Statistical Learning")*
