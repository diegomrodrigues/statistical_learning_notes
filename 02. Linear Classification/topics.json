{
  "topics": [
    {
      "topic": "Linear Methods for Classification",
      "sub_topics": [
        "Linear methods for classification involve creating linear decision boundaries to separate classes, dividing the input space into regions labeled according to the classification. These methods model either discriminant functions \\u03b4\\u03ba(x) for each class or posterior probabilities Pr(G = k|X = x), classifying an observation x to the class with the largest value or probability. The decision boundary between classes k and l is defined by the set of points where the discriminant functions are equal, resulting in an affine set or hyperplane in p dimensions.",
        "Linear regression of an indicator matrix involves coding each response category via an indicator variable and fitting a linear regression model to each column of the indicator response matrix. A new observation is classified by computing the fitted output and identifying the largest component, effectively assigning the observation to the class with the largest fitted value. This approach can be viewed as estimating conditional expectation, where \\\\(E(Y_k|X = x) = Pr(G = k|X = x)\\\\). However, the fitted values \\\\(f_k(x)\\\\) can be negative or greater than 1 due to the rigid nature of linear regression, especially when making predictions outside the hull of the training data. With K > 3 classes, masking can occur due to the rigid nature of the regression model; classes can be masked by others, especially when the classes are perfectly separated by linear decision boundaries. Addressing masking involves using polynomial terms up to degree \\\\(K-1\\\\) to resolve the worst-case scenarios, requiring \\\\(O(p^{K-1})\\\\) terms in \\\\(p\\\\)-dimensional input space.",
        "Linear Discriminant Analysis (LDA) uses Gaussian densities to model class densities and assumes that the classes have a common covariance matrix. By comparing two classes, LDA derives a linear log-odds function, implying that the decision boundary between classes is linear and forms a hyperplane in p dimensions. LDA results in linear discriminant functions given by \\\\(\\\\delta_k(x) = x^T \\\\Sigma^{-1} \\\\mu_k - \\\\frac{1}{2} \\\\mu_k^T \\\\Sigma^{-1} \\\\mu_k + log \\\\pi_k\\\\), leading to a decision rule of \\\\(G(x) = argmax_k \\\\delta_k(x)\\\\). The parameters of the Gaussian distributions are estimated from training data, with \\\\(\\\\pi_k = N_k/N\\\\), \\\\(\\\\mu_k = \\\\sum_{g_i=k} x_i / N_k\\\\), and \\\\(\\\\Sigma = \\\\sum_{k=1}^K \\\\sum_{g_i=k} (x_i - \\\\mu_k)(x_i - \\\\mu_k)^T / (N - K)\\\\). Computations for LDA and QDA are simplified by diagonalizing \\\\(\\\\Sigma\\\\) or \\\\(\\\\Sigma_k\\\\). The LDA classifier can be implemented by sphering the data with respect to the common covariance estimate \\\\(\\\\Sigma\\\\) and classifying to the closest class centroid in the transformed space, modulo the effect of the class prior probabilities \\\\(\\\\pi_k\\\\).",
        "Logistic regression models the posterior probabilities of K classes via linear functions in x, ensuring that the probabilities sum to one and remain in the range [0,1]. The model uses K-1 log-odds or logit transformations, reflecting the constraint that the probabilities sum to one, and the choice of denominator is arbitrary due to the equivariance of the estimates. Logistic regression models are usually fit by maximum likelihood, using the conditional likelihood of \\\\(G\\\\) given \\\\(X\\\\). The log-likelihood for \\\\(N\\\\) observations is \\\\(l(\\\\theta) = \\\\sum_{i=1}^N log p_{g_i}(x_i; \\\\theta)\\\\), where \\\\(p_k(x_i; \\\\theta) = Pr(G = k|X = x_i; \\\\theta)\\\\). To maximize the log-likelihood, its derivatives are set to zero, resulting in score equations that are solved using the Newton-Raphson algorithm. The Newton-Raphson algorithm involves iteratively reweighted least squares (IRLS), where each iteration solves a weighted least squares problem.",
        "The perceptron learning algorithm seeks to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary. It uses stochastic gradient descent to iteratively adjust the hyperplane parameters based on misclassified observations until a separating hyperplane is found, assuming the classes are linearly separable. The optimal separating hyperplane separates the two classes and maximizes the distance to the closest point from either class. The set of conditions ensure that all the points are at least a signed distance M from the decision boundary defined by \\u1e9e and 30, and we seek the largest such M and associated parameters.",
        "Generalizations of linear decision boundaries can be achieved by expanding the variable set \\\\(X_1, ..., X_p\\\\) to include squares and cross-products, mapping linear functions in the augmented space to quadratic functions in the original space. This approach allows for quadratic decision boundaries by adding \\\\(p(p+1)/2\\\\) additional variables and using basis transformations to map linear boundaries in the augmented space to quadratic boundaries in the original space.",
        "Regularized Discriminant Analysis (RDA) is a compromise between LDA and QDA that shrinks the separate covariances of QDA toward a common covariance as in LDA, similar to ridge regression. The regularized covariance matrices are of the form \\\\(\\\\Sigma_k(\\\\alpha) = \\\\alpha \\\\hat{\\\\Sigma}_k + (1 - \\\\alpha) \\\\Sigma\\\\), where \\\\(\\\\alpha \\\\in [0, 1]\\\\) and \\\\(\\\\Sigma\\\\) is the pooled covariance matrix as used in LDA.",
        "L1 regularization can be applied to logistic regression for variable selection and shrinkage, maximizing a penalized version of the log-likelihood, solved using algorithms like coordinate descent methods."
      ]
    },
    {
      "topic": "Separating Hyperplanes",
      "sub_topics": [
        "Separating hyperplane classifiers construct linear decision boundaries that explicitly try to separate the data into different classes as well as possible, providing the basis for support vector classifiers. Given a set of data points, the goal is to find a hyperplane \\\\(f(x) = \\\\beta_0 + \\\\beta^T x = 0\\\\) that separates the classes, where the signed distance of any point \\\\(x\\\\) to the hyperplane is proportional to \\\\(f(x)\\\\). For any two points 11 and 12 lying in L, \\u03b2T (x1 - x2) = 0, and hence \\u03b2* = \\u03b2/||3|| is the vector normal to the surface of L.",
        "Rosenblatt's perceptron learning algorithm tries to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary. The algorithm uses stochastic gradient descent to minimize a piecewise linear criterion, updating the parameters \\\\(\\\\beta\\\\) after each misclassified observation is visited. The algorithm uses stochastic gradient descent to minimize this piecewise linear criterion, updating the parameters \\u03b2 via \\u03b2 \\u2190 \\u03b2 + \\u03c1 \\u03a3i\\u2208M yixi, where M indexes the set of misclassified points and \\u03c1 is the learning rate.",
        "The optimal separating hyperplane separates the two classes and maximizes the distance to the closest point from either class, providing a unique solution to the separating hyperplane problem. This is achieved by solving a convex optimization problem, where the goal is to maximize the margin between the two classes on the training data, leading to better classification performance on test data. The optimal separating hyperplane is found by maximizing \\\\(M\\\\) subject to \\\\(y_i (x_i^T \\\\beta + \\\\beta_0) \\\\geq M\\\\) and \\\\(||\\\\beta|| = 1\\\\), which ensures that all points are at least a signed distance \\\\(M\\\\) from the decision boundary. This is equivalent to minimizing \\\\(\\\\frac{1}{2} ||\\\\beta||^2\\\\) subject to \\\\(y_i (x_i^T \\\\beta + \\\\beta_0) \\\\geq 1\\\\), a convex optimization problem that maximizes the thickness of the margin.",
        "The solution for the optimal separating hyperplane can be obtained by maximizing the dual Lagrangian function \\\\(L_D\\\\) in the positive orthant, where the solution must satisfy the Karush-Kuhn-Tucker conditions. The solution vector \\\\(\\\\beta\\\\) is defined in terms of a linear combination of the support points \\\\(x_i\\\\), which are the points on the boundary of the slab, and the optimal separating hyperplane produces a function \\\\(f(x) = x^T \\\\beta + \\\\beta_0\\\\) for classifying new observations. The Karush-Kuhn-Tucker conditions for the optimal separating hyperplane problem include \\u03b1i[yi(xTi\\u03b2 + \\u03b2\\u2080) - 1] = 0 for all i, which implies that if \\u03b1i > 0, then xi is on the boundary of the slab, and if yi(xTi\\u03b2 + \\u03b2\\u2080) > 1, then \\u03b1i = 0."
      ]
    }
  ]
}