{
  "topics": [
    {
      "topic": "Linear Methods for Classification",
      "sub_topics": [
        "Linear methods for classification create linear decision boundaries to divide the input space into regions, each corresponding to a specific class. These methods model discriminant functions \u03b4k(x) for each class, classifying input x to the class with the largest value, or directly model posterior probabilities Pr(G = k|X = x) using models like logistic regression. Linearity in x or a monotone transformation of it leads to linear decision boundaries.",
        "Linear regression models can be fitted to class indicator variables, classifying observations based on the largest fit among K classes. The decision boundary between classes k and l is determined by the set of points where fk(x) = fl(x), resulting in an affine set or hyperplane, leading to piecewise hyperplanar decision boundaries.",
        "Linear Discriminant Analysis (LDA) and linear logistic regression are popular methods resulting in linear log-odds or logits. They differ in their derivation and how the linear function is fit to the training data. LDA arises from modeling each class density as a multivariate Gaussian distribution with a common covariance matrix, leading to linear discriminant functions.",
        "Explicitly modeling boundaries between classes as linear involves defining the decision boundary as a hyperplane, characterized by a normal vector and a cut-point. Methods like the perceptron model find a separating hyperplane in the training data if one exists.",
        "Generalizations of linear decision boundaries can be achieved by expanding the variable set to include squares and cross-products of the original variables. This transforms linear functions in the augmented space into quadratic functions in the original space, effectively transforming linear decision boundaries into quadratic ones. Quadratic Discriminant Analysis (QDA) arises when classes do not have a common covariance matrix, resulting in quadratic discriminant functions. Regularized Discriminant Analysis (RDA) is a compromise between LDA and QDA, using regularized covariance matrices to allow a continuum of models between LDA and QDA.",
        "The logit transformation, log[p/(1-p)], is a monotone transformation used in logistic regression, where the log-odds of class membership are modeled as a linear function of the input features. L1 Regularized Logistic Regression maximizes a penalized version of the log-likelihood to achieve variable selection and shrinkage, using the L1 penalty, and can be solved by repeated application of a weighted lasso algorithm."
      ]
    },
    {
      "topic": "Linear Regression of an Indicator Matrix",
      "sub_topics": [
        "Linear regression of an indicator matrix involves coding response categories via indicator variables, creating K indicators (Yk) for K classes and forming an N \u00d7 K indicator response matrix Y from N training instances. A linear regression model is fitted to each column of Y simultaneously, resulting in the fit Y = X(XTX)-1XTY, where X is the model matrix with p+1 columns. This approach views regression as an estimate of conditional expectation, where E(Yk|X = x) = Pr(G = k|X = x).",
        "A new observation with input x is classified by computing the fitted output f(x) = (1, xT)B, a K vector, and identifying the largest component, classifying accordingly as \u011c(x) = argmaxk\u2208G fk(x). This is equivalent to constructing targets tk for each class, where tk is the kth column of the K \u00d7 K identity matrix, and fitting the linear model by least squares, minimizing the sum-of-squared Euclidean distances of the fitted vectors from their targets.",
        "A significant problem with the regression approach arises when the number of classes K > 3, as classes can be masked by others due to the rigid nature of the regression model. Polynomial terms and cross-products of the input variables can be used to resolve masking issues, requiring polynomial terms up to degree K - 1 to resolve worst-case scenarios. Linear regression can suffer from masking effects, leading to higher error rates compared to other methods like linear discriminant analysis, which avoid this masking problem.",
        "The sum-of-squared-norm criterion is exactly the criterion for multiple response linear regression, which can be rearranged as a separate linear model for each element due to the lack of binding between different responses."
      ]
    },
    {
      "topic": "Linear Discriminant Analysis",
      "sub_topics": [
        "Linear Discriminant Analysis (LDA) is based on decision theory for classification, requiring knowledge of class posteriors Pr(G|X) for optimal classification, which are derived from class-conditional densities fk(x) and prior probabilities \u03c0k using Bayes' theorem. LDA arises when modeling each class density as a multivariate Gaussian distribution with a common covariance matrix \u03a3k = \u03a3, leading to linear discriminant functions \u03b4k(x) that define decision boundaries as hyperplanes.",
        "In practice, the parameters of the Gaussian distributions are estimated from training data, with estimates for prior probabilities (\u03c0k), class means (\u03bck), and the common covariance matrix (\u03a3) computed from the data.",
        "With two classes, LDA has a simple correspondence to classification by linear least squares, where the LDA rule classifies to class 2 if a linear combination of the input features exceeds a threshold, and this direction is proportional to the LDA direction.",
        "Quadratic Discriminant Analysis (QDA) arises when the covariance matrices are not assumed to be equal, resulting in quadratic discriminant functions and decision boundaries described by quadratic equations. Regularized Discriminant Analysis (RDA) is a compromise between LDA and QDA, shrinking separate covariances of QDA toward a common covariance as in LDA, using regularized covariance matrices that allow a continuum of models between LDA and QDA.",
        "Computations for LDA and QDA are simplified by diagonalizing the covariance matrices, and LDA can be implemented by sphering the data with respect to the common covariance estimate and classifying to the closest class centroid in the transformed space. Reduced-Rank LDA involves the centroids lying in an affine subspace of dimension < K \u2212 1, allowing for a fundamental dimension reduction. Fisher's approach to LDA maximizes the Rayleigh quotient to find linear combinations that maximize between-class variance relative to within-class variance, leading to discriminant coordinates."
      ]
    },
    {
      "topic": "Logistic Regression",
      "sub_topics": [
        "Logistic regression models the posterior probabilities of K classes via linear functions in x while ensuring they sum to one and remain in [0,1], using log-odds or logit transformations to specify the model. The model is specified in terms of K - 1 log-odds or logit transformations reflecting the constraint that the probabilities sum to one, and although the model uses the last class as the denominator in the odds-ratios, the choice of denominator is arbitrary. For binary classification (K = 2), the model simplifies to a single linear function, widely used in biostatistical applications for binary responses.",
        "Logistic regression models are typically fit by maximum likelihood, using the conditional likelihood of G given X, and the log-likelihood is maximized using algorithms like Newton-Raphson, which requires the second-derivative or Hessian matrix. The Newton step in logistic regression can be re-expressed as a weighted least squares step, with the response being a modified version of the original response, and this algorithm is referred to as iteratively reweighted least squares (IRLS). Model building can be costly because each model fitted requires iteration, and popular shortcuts are the Rao score test (tests for inclusion of a term) and the Wald test (tests for exclusion of a term).",
        "L1 regularization can be used for variable selection and shrinkage with logistic regression by maximizing a penalized version of the log-likelihood, and path algorithms like LAR can be used to compute the coefficient profiles, though they are more difficult due to piecewise smooth coefficient profiles. This can be solved by repeated application of a weighted lasso algorithm.",
        "Logistic regression is more general than LDA because it makes fewer assumptions; logistic regression models Pr(G|X) directly, while LDA models the joint density Pr(X, G) assuming Gaussian class densities and a common covariance matrix."
      ]
    },
    {
      "topic": "Separating Hyperplanes",
      "sub_topics": [
        "Separating hyperplane classifiers construct linear decision boundaries to explicitly separate data into different classes, providing the basis for support vector classifiers. These classifiers compute a linear combination of the input features and return the sign.",
        "The perceptron learning algorithm tries to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary using stochastic gradient descent, updating parameters after each misclassified observation. The algorithm updates parameters \u03b2 via \u03b2o \u2192 \u03b2o + \u03c1\u03b3i and \u03b2 \u2192 \u03b2 + \u03c1\u03b3ixi, where \u03c1 is the learning rate, and converges to a separating hyperplane in a finite number of steps if classes are linearly separable.",
        "The optimal separating hyperplane separates the two classes and maximizes the distance to the closest point from either class, providing a unique solution and maximizing the margin between the two classes, leading to better generalization. The optimal separating hyperplane is found by solving a convex optimization problem (quadratic criterion with linear inequality constraints) to maximize the margin, resulting in a solution defined in terms of support points.",
        "The optimization problem involves maximizing the margin M subject to constraints yi(xTi\u03b2 + \u03b2o) \u2265 M, leading to a convex optimization problem solved using Lagrange duality. The solution defines a linear combination of support points xi, those points on the boundary of the slab, and the optimal separating hyperplane produces a function f(x) = xT\u03b2 + \u03b2o for classifying new observations."
      ]
    },
    {
      "topic": "Logistic Regression vs. LDA",
      "sub_topics": [
        "Both Logistic Regression and LDA result in linear functions, but differ in how the linear coefficients are estimated. Logistic regression is more general as it makes fewer assumptions.",
        "Logistic regression leaves the marginal density of X as an arbitrary density function Pr(X), fitting the parameters of Pr(G|X) by maximizing the conditional likelihood.",
        "LDA fits parameters by maximizing the full log-likelihood, based on the joint density Pr(X, G = k) = \u03c6(X; \u03bck, \u03a3)\u03c0k, assuming Gaussian densities.",
        "The marginal likelihood in LDA acts as a regularizer, requiring class densities to be visible from this marginal view, preventing degeneracies like infinite parameter estimates in perfectly separated data.",
        "Logistic regression is generally considered safer and more robust than LDA due to fewer assumptions, although both models often yield similar results."
      ]
    }
  ]
}