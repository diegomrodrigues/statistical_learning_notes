## T√≠tulo Conciso: Classifica√ß√£o Linear, Coordenadas Discriminantes e a Conex√£o com a Decomposi√ß√£o em Valores Singulares (SVD)

```mermaid
graph LR
    subgraph "Conceptual Relationships"
        direction TB
        A["Data Original Space"]
        B["Fisher's Criterion"]
        C["Discriminant Coordinates"]
        D["SVD Application"]
        E["Reduced Dimensional Space"]
        F["Classification Model"]
        A --> B
        B --> C
        C --> D
        D --> E
        E --> F
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora a rela√ß√£o entre **coordenadas discriminantes**, **maximiza√ß√£o da vari√¢ncia** e a **decomposi√ß√£o em valores singulares (SVD)**, com foco na constru√ß√£o de modelos de classifica√ß√£o linear. Analisaremos como a proje√ß√£o dos dados em coordenadas discriminantes, tamb√©m conhecidas como **vari√°veis can√¥nicas**, pode ser utilizada para maximizar a separa√ß√£o entre as classes e como a SVD pode ser utilizada para obter essas coordenadas. Discutiremos como o **Crit√©rio de Fisher** se conecta com o conceito de coordenadas discriminantes e como a maximiza√ß√£o da vari√¢ncia entre classes leva a proje√ß√µes lineares que simplificam o problema de classifica√ß√£o [^4.3.3]. Compararemos essa abordagem com a **regress√£o linear com matrizes de indicadores**, que n√£o utiliza a decomposi√ß√£o em valores singulares para obten√ß√£o de coordenadas discriminantes [^4.2], e com a **regress√£o log√≠stica**, que modela a probabilidade posterior sem utilizar coordenadas discriminantes [^4.4]. Abordaremos tamb√©m a import√¢ncia da **sele√ß√£o de vari√°veis e regulariza√ß√£o** para controlar a complexidade dos modelos e melhorar sua capacidade de generaliza√ß√£o [^4.4.4], [^4.5]. O conceito de **hiperplanos separadores** tamb√©m ser√° discutido, enfatizando a forma como as coordenadas discriminantes se relacionam com a busca por fronteiras de decis√£o √≥timas [^4.5.2]. O objetivo deste cap√≠tulo √© fornecer uma compreens√£o detalhada de como a decomposi√ß√£o em valores singulares se conecta com a constru√ß√£o de coordenadas discriminantes e como essas ferramentas s√£o utilizadas para otimizar a classifica√ß√£o linear.

### Conceitos Fundamentais

**Conceito 1: Coordenadas Discriminantes e a Maximiza√ß√£o da Vari√¢ncia**

As **coordenadas discriminantes**, tamb√©m conhecidas como **vari√°veis can√¥nicas**, s√£o componentes de um subespa√ßo obtido por meio da maximiza√ß√£o da raz√£o entre a vari√¢ncia entre classes e a vari√¢ncia dentro das classes [^4.3.3]. A ideia √© projetar os dados em um subespa√ßo de menor dimens√£o onde a separa√ß√£o entre as classes seja maximizada e a variabilidade dentro de cada classe seja minimizada. O Crit√©rio de Fisher √© uma ferramenta para derivar essas coordenadas, buscando uma transforma√ß√£o linear dos dados que maximize essa raz√£o de vari√¢ncias.

As coordenadas discriminantes podem ser obtidas atrav√©s da decomposi√ß√£o espectral da matriz $W^{-1}B$, onde $W$ √© a matriz de covari√¢ncia dentro das classes (pooled covariance) e $B$ √© a matriz de covari√¢ncia entre as classes. Essas coordenadas definem as dire√ß√µes que melhor separam as classes, e a proje√ß√£o dos dados sobre essas coordenadas simplifica o problema de classifica√ß√£o e pode ser visto como uma forma de selecionar os eixos que melhor separam as classes.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos duas classes com os seguintes dados bidimensionais:
>
> Classe 1:  $X_1 = \begin{bmatrix} 1 & 2 \\ 1.5 & 2.5 \\ 2 & 2 \end{bmatrix}$
>
> Classe 2:  $X_2 = \begin{bmatrix} 4 & 5 \\ 4.5 & 5.5 \\ 5 & 5 \end{bmatrix}$
>
> Primeiro, calculamos as m√©dias de cada classe:
>
> $\mu_1 = \begin{bmatrix} 1.5 \\ 2.167 \end{bmatrix}$,  $\mu_2 = \begin{bmatrix} 4.5 \\ 5.167 \end{bmatrix}$
>
> Calculamos a matriz de covari√¢ncia dentro das classes (assumindo igual para ambas):
>
> $W = \frac{1}{N_1 + N_2 - 2} \left(\sum_{x \in X_1} (x-\mu_1)(x-\mu_1)^T + \sum_{x \in X_2} (x-\mu_2)(x-\mu_2)^T\right)$
>
> $W \approx \begin{bmatrix} 0.25 & 0.125 \\ 0.125 & 0.083 \end{bmatrix}$
>
> E a matriz de covari√¢ncia entre classes:
>
> $B = \frac{N_1 N_2}{N_1 + N_2}(\mu_1 - \mu_2)(\mu_1 - \mu_2)^T$
>
> $B \approx \begin{bmatrix} 4.5 & 4.5 \\ 4.5 & 4.5 \end{bmatrix}$
>
> A matriz $W^{-1}B$ √© ent√£o calculada:
>
> $W^{-1}B \approx \begin{bmatrix} 18 & 18 \\ -27 & -27 \end{bmatrix}$
>
> Os autovetores de $W^{-1}B$ nos dar√£o as coordenadas discriminantes. O primeiro autovetor, correspondente ao maior autovalor, aponta na dire√ß√£o que maximiza a separa√ß√£o entre as classes. A proje√ß√£o dos dados sobre este autovetor resulta em uma representa√ß√£o unidimensional onde a separa√ß√£o entre as classes √© maximizada.

**Lemma 1:** *As coordenadas discriminantes, obtidas atrav√©s do Crit√©rio de Fisher, maximizam a separa√ß√£o entre as classes ao otimizar a raz√£o entre a vari√¢ncia entre classes e a vari√¢ncia dentro das classes no espa√ßo de caracter√≠sticas transformado.* A prova desse lema reside na deriva√ß√£o dos autovetores da matriz $W^{-1}B$, que √© a solu√ß√£o do problema de otimiza√ß√£o do Crit√©rio de Fisher.

```mermaid
graph LR
    subgraph "Fisher's Criterion Optimization"
        direction TB
        A["Maximize: J(w) = (w^T B w) / (w^T W w)"]
        B["B: Between-class covariance matrix"]
        C["W: Within-class covariance matrix"]
        D["w: Projection vector (Discriminant Coordinates)"]
        A --> B
        A --> C
        A --> D
    end
```

**Conceito 2: Rela√ß√£o entre SVD e as Coordenadas Discriminantes**

A **decomposi√ß√£o em valores singulares (SVD)** pode ser utilizada para obter as coordenadas discriminantes de forma computacionalmente eficiente. A SVD da matriz $M = W^{-1/2} B W^{-1/2}$ , onde $W$ √© a matriz de covari√¢ncia dentro das classes e $B$ √© a matriz de covari√¢ncia entre as classes, dada por $M = V D V^T$ onde $V$ √© a matriz de autovetores e $D$ √© uma matriz diagonal com autovalores, permite encontrar as dire√ß√µes que maximizam a separabilidade entre as classes [^4.3.3]. As colunas da matriz $V$ definem as coordenadas discriminantes, que podem ser utilizadas para projetar os dados em um subespa√ßo de menor dimens√£o, onde o problema de classifica√ß√£o se torna mais simples.

> üí° **Exemplo Num√©rico:**
>
> Continuando com o exemplo anterior, vamos normalizar as matrizes de covari√¢ncia para aplicar a SVD:
>
> $W^{1/2} \approx \begin{bmatrix} 0.5 & 0.25 \\ 0.25 & 0.29 \end{bmatrix}$
>
> $W^{-1/2} \approx \begin{bmatrix} 2.26 & -1.94 \\ -1.94 & 3.88 \end{bmatrix}$
>
> $M = W^{-1/2} B W^{-1/2} \approx \begin{bmatrix} 10.12 & 10.12 \\ -15.18 & -15.18 \end{bmatrix}$
>
> Aplicando a SVD em $M$ ($M = U \Sigma V^T$):
>
> $V \approx \begin{bmatrix} 0.707 & -0.707 \\ 0.707 & 0.707 \end{bmatrix}$
>
> A primeira coluna de $V$ (0.707, 0.707) representa a coordenada discriminante principal, que √© a dire√ß√£o que maximiza a separa√ß√£o entre as classes. Ao projetar os dados originais sobre esse vetor, reduzimos a dimensionalidade do problema para 1.

**Corol√°rio 1:** *A SVD oferece uma forma eficiente de obter as coordenadas discriminantes, ao mesmo tempo que destaca a estrutura da variabilidade entre as classes.* Este corol√°rio demonstra a import√¢ncia da SVD como ferramenta para gerar coordenadas discriminantes.

```mermaid
graph LR
    subgraph "SVD for Discriminant Coordinates"
        direction LR
        A["M = W^(-1/2) B W^(-1/2)"]
        B["SVD: M = U Œ£ V^T"]
        C["V: Matrix of Eigenvectors (Discriminant Coordinates)"]
        D["Œ£: Diagonal Matrix of Singular Values"]
        A --> B
        B --> C
        B --> D
    end
```

**Conceito 3: A Utiliza√ß√£o de Subespa√ßos Discriminantes em Classifica√ß√£o Linear**

Ao projetar os dados no subespa√ßo definido pelas coordenadas discriminantes, a dimensionalidade do problema √© reduzida, e a classifica√ß√£o pode ser realizada de forma mais eficiente, e sem uma perda significativa de informa√ß√£o relevante. O n√∫mero de coordenadas discriminantes pode ser controlado para balancear a complexidade do modelo com a sua capacidade de generaliza√ß√£o. A classifica√ß√£o pode ser feita utilizando a dist√¢ncia euclidiana dos dados projetados em rela√ß√£o aos centroides das classes tamb√©m projetados no subespa√ßo, onde a dist√¢ncia euclidiana √© uma boa aproxima√ß√£o para a maximiza√ß√£o da probabilidade posterior [^4.3.3].

> ‚ö†Ô∏è **Nota Importante**: A combina√ß√£o do Crit√©rio de Fisher com a SVD permite a constru√ß√£o de modelos de classifica√ß√£o mais eficientes e com melhor capacidade de generaliza√ß√£o.

> ‚ùó **Ponto de Aten√ß√£o**: A escolha do n√∫mero de coordenadas discriminantes deve ser feita cuidadosamente, utilizando t√©cnicas de valida√ß√£o, para evitar a perda de informa√ß√µes relevantes para a classifica√ß√£o.

> ‚úîÔ∏è **Destaque**: A utiliza√ß√£o de coordenadas discriminantes, obtidas atrav√©s da decomposi√ß√£o espectral de matrizes de covari√¢ncia, permite reduzir a dimensionalidade do problema de classifica√ß√£o e melhorar a efici√™ncia dos modelos lineares.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
    subgraph "Comparison of Approaches"
        direction TB
        A["Original Data Space"]
        B["Discriminant Analysis"]
        C["Linear Regression"]
        D["Projection on Discriminant Space"]
        E["Model Fitting on Original Space"]
        A --> B
        A --> C
        B --> D
        C --> E
    end
```

A **regress√£o linear com matrizes de indicadores**, ao contr√°rio do LDA que utiliza coordenadas discriminantes, ajusta um modelo linear para cada classe no espa√ßo de caracter√≠sticas original, sem utilizar informa√ß√µes sobre a vari√¢ncia entre e dentro das classes [^4.2]. A decis√£o de classifica√ß√£o √© feita atrav√©s da escolha da classe com maior valor na fun√ß√£o linear ajustada, sem a proje√ß√£o dos dados em um subespa√ßo de menor dimens√£o que maximiza a separabilidade entre as classes.

A aus√™ncia da proje√ß√£o sobre coordenadas discriminantes na regress√£o linear com matrizes de indicadores implica que o modelo n√£o busca explicitamente a variabilidade entre as classes e a variabilidade dentro das classes, o que pode limitar a capacidade do modelo de generalizar para dados n√£o vistos durante o treinamento e pode ser mais suscet√≠vel ao problema do "masking" em problemas multiclasse. A regress√£o linear com matrizes de indicadores, portanto, n√£o se beneficia da SVD na mesma medida que o LDA e o QDA.

A proje√ß√£o em subespa√ßos discriminantes √© uma forma de reduzir a dimens√£o dos dados e de focar apenas naqueles atributos que s√£o mais importantes para a separa√ß√£o das classes, um processo que n√£o √© utilizado pela regress√£o linear [^4.3.3].

> üí° **Exemplo Num√©rico:**
>
> Usando os mesmos dados de exemplo, podemos aplicar a regress√£o linear com matrizes de indicadores. Criamos uma matriz de indicadores $Y$ onde a primeira coluna representa a classe 1 e a segunda coluna a classe 2:
>
> $Y = \begin{bmatrix} 1 & 0 \\ 1 & 0 \\ 1 & 0 \\ 0 & 1 \\ 0 & 1 \\ 0 & 1 \end{bmatrix}$
>
> A matriz de dados √©:
>
> $X = \begin{bmatrix} 1 & 2 \\ 1.5 & 2.5 \\ 2 & 2 \\ 4 & 5 \\ 4.5 & 5.5 \\ 5 & 5 \end{bmatrix}$
>
> Calculamos os coeficientes $\hat{\beta}$ para cada classe usando a f√≥rmula de m√≠nimos quadrados: $\hat{\beta} = (X^T X)^{-1} X^T Y$.
>
> $X^T X = \begin{bmatrix} 64.5 & 72 \\ 72 & 81.5 \end{bmatrix}$
>
> $(X^T X)^{-1} \approx \begin{bmatrix} 0.25 & -0.22 \\ -0.22 & 0.20 \end{bmatrix}$
>
> $X^T Y = \begin{bmatrix} 8.5 & 13.5 \\ 10.5 & 16 \end{bmatrix}$
>
> $\hat{\beta} = (X^T X)^{-1} X^T Y \approx \begin{bmatrix} 0.15 & 0.12 \\ 0.15 & 0.12 \end{bmatrix}$
>
> Agora, para classificar um novo ponto, calculamos o score para cada classe usando o modelo linear:  $score = \hat{\beta}^T x$.
>
> Por exemplo, para um novo ponto $x = \begin{bmatrix} 3 \\ 3 \end{bmatrix}$:
>
> $score_1 = \begin{bmatrix} 0.15 & 0.15 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix} = 0.9$
>
> $score_2 = \begin{bmatrix} 0.12 & 0.12 \end{bmatrix} \begin{bmatrix} 3 \\ 3 \end{bmatrix} = 0.72$
>
> O ponto seria classificado como pertencente √† classe 1, pois tem o maior score. A regress√£o linear n√£o se beneficiou de nenhuma transforma√ß√£o ou proje√ß√£o baseada na vari√¢ncia entre classes.

**Lemma 2:** *A regress√£o linear com matrizes de indicadores n√£o utiliza o Crit√©rio de Fisher para derivar coordenadas discriminantes e projetar os dados em um subespa√ßo de menor dimens√£o, e o ajuste do modelo √© feito diretamente no espa√ßo original.* A prova desse lema se encontra na formula√ß√£o da regress√£o linear e na aus√™ncia da utiliza√ß√£o de qualquer informa√ß√£o sobre a matriz de covari√¢ncia dos dados.

**Corol√°rio 2:** *A falta de utiliza√ß√£o das coordenadas discriminantes na regress√£o linear com matrizes de indicadores pode tornar o modelo menos eficiente em compara√ß√£o com o LDA, especialmente em situa√ß√µes onde a dimensionalidade dos dados √© alta e onde a separa√ß√£o das classes n√£o √© evidente no espa√ßo original.* A escolha de um modelo linear sem transforma√ß√£o dos dados pode, portanto, apresentar limita√ß√µes.

A regress√£o linear com matrizes de indicadores, ao n√£o utilizar coordenadas discriminantes, n√£o explora a estrutura de vari√¢ncia dos dados da mesma maneira que os m√©todos que utilizam o Crit√©rio de Fisher. Isso limita a capacidade de lidar com problemas complexos onde a redu√ß√£o de dimensionalidade e a proje√ß√£o em subespa√ßos informativos s√£o importantes para a constru√ß√£o de modelos mais robustos e eficientes [^4.2], [^4.3.3].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Regularization and Variable Selection"
       direction TB
       A["Discriminant Coordinates Space"]
       B["Variable Selection Methods"]
       C["Regularization Techniques"]
       D["Model Simplification"]
       E["Improved Generalization"]
       A --> B
       A --> C
       B --> D
       C --> D
       D --> E
    end
```

A **sele√ß√£o de vari√°veis** e a **regulariza√ß√£o** desempenham um papel importante para complementar a redu√ß√£o de dimensionalidade atrav√©s de coordenadas discriminantes, permitindo que os modelos de classifica√ß√£o se tornem mais robustos e interpret√°veis [^4.5]. A regulariza√ß√£o, ao adicionar um termo de penalidade √† fun√ß√£o de custo, busca controlar a magnitude dos coeficientes e evitar o *overfitting*, o que √© especialmente √∫til quando se trabalha em espa√ßos de menor dimens√£o.

Na **regress√£o log√≠stica**, que pode ser utilizada ap√≥s a redu√ß√£o de dimensionalidade atrav√©s do Crit√©rio de Fisher, a fun√ß√£o de custo regularizada √© expressa como:

$$
\max_{\beta_0, \beta} \left[ \sum_{i=1}^N \left( y_i (\beta_0 + \beta^T x_i) - \log(1 + e^{\beta_0 + \beta^T x_i}) \right) - \lambda P(\beta) \right]
$$

onde $P(\beta)$ √© a penalidade e $\lambda$ √© o par√¢metro de regulariza√ß√£o. A penalidade **L1** (Lasso) √© dada por $P(\beta) = \sum_{j=1}^p |\beta_j|$, promovendo a esparsidade dos coeficientes e selecionando as vari√°veis mais relevantes [^4.4.4]. A penalidade **L2** (Ridge) √© dada por $P(\beta) = \sum_{j=1}^p \beta_j^2$, reduzindo a magnitude dos coeficientes e estabilizando o modelo [^4.5].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo de regress√£o log√≠stica com regulariza√ß√£o L1 (Lasso) para ilustrar a sele√ß√£o de vari√°veis. Suponha que, ap√≥s a proje√ß√£o em um subespa√ßo discriminante, temos os seguintes dados e r√≥tulos de classe:
>
> $X_{proj} = \begin{bmatrix} -2 \\ -1 \\ 0 \\ 1 \\ 2 \end{bmatrix}$, $y = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 1 \end{bmatrix}$
>
> E que a regress√£o log√≠stica com regulariza√ß√£o L1 nos d√° os seguintes coeficientes para diferentes valores de $\lambda$:
>
> $\lambda = 0.1$: $\beta_0 = -0.1$, $\beta_1 = 0.7$
>
> $\lambda = 0.5$: $\beta_0 = 0.05$, $\beta_1 = 0.4$
>
> $\lambda = 1.0$: $\beta_0 = 0.1$, $\beta_1 = 0.0$
>
> Com $\lambda=0.1$, temos um modelo que considera a vari√°vel projetada. Com $\lambda = 1$, o coeficiente $\beta_1$ se torna zero, indicando que o modelo selecionou apenas o intercepto. Isso mostra como a regulariza√ß√£o L1 pode levar √† sele√ß√£o de vari√°veis.
>
> A fun√ß√£o de custo para regress√£o log√≠stica com regulariza√ß√£o L1 √©:
>
>  $J(\beta) = -\frac{1}{N}\sum_{i=1}^N [y_i \log(p_i) + (1-y_i) \log(1-p_i)] + \lambda \sum_{j=1}^p |\beta_j|$
>
>  onde $p_i = \frac{1}{1+e^{-(\beta_0 + \beta^Tx_i)}}$
>
>  O termo de regulariza√ß√£o $\lambda \sum_{j=1}^p |\beta_j|$ penaliza os coeficientes, for√ßando alguns a serem zero quando $\lambda$ √© alto, e consequentemente, selecionando vari√°veis.

A combina√ß√£o de redu√ß√£o de dimensionalidade com regulariza√ß√£o permite obter modelos que s√£o mais simples de interpretar e que generalizam melhor para dados n√£o vistos no treinamento. Al√©m disso, o efeito da regulariza√ß√£o se soma √† transforma√ß√£o do espa√ßo pela utiliza√ß√£o de coordenadas discriminantes, fazendo com que o modelo tenha uma maior robustez.

**Lemma 3:** *A regulariza√ß√£o L1, ao promover a esparsidade dos coeficientes em subespa√ßos discriminantes, leva a modelos mais simples e com melhor capacidade de generaliza√ß√£o, e ajuda a reduzir o impacto do ru√≠do no ajuste dos par√¢metros do modelo.* A prova desse lema est√° na natureza da penalidade L1 e no seu efeito sobre a fun√ß√£o de custo.

**Prova do Lemma 3:** A penalidade L1, ao adicionar um termo proporcional ao valor absoluto dos coeficientes, for√ßa alguns deles a se tornarem exatamente zero durante o processo de otimiza√ß√£o, levando √† sele√ß√£o de vari√°veis e a modelos mais simples, al√©m de mais est√°veis [^4.4.3], [^4.4.4]. $\blacksquare$

**Corol√°rio 3:** *A utiliza√ß√£o conjunta de redu√ß√£o de dimensionalidade atrav√©s do Crit√©rio de Fisher com t√©cnicas de regulariza√ß√£o, como L1 e L2, resulta em modelos de classifica√ß√£o mais robustos e eficientes, com melhor capacidade de generaliza√ß√£o e menor complexidade computacional.* A combina√ß√£o das diferentes abordagens leva a modelos mais simples e de melhor qualidade.

> ‚ö†Ô∏è **Ponto Crucial**: A sele√ß√£o de vari√°veis e a regulariza√ß√£o podem ser utilizadas para complementar a redu√ß√£o de dimensionalidade atrav√©s do Crit√©rio de Fisher, levando a modelos de classifica√ß√£o mais eficientes e robustos [^4.5].

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
   subgraph "Hyperplane and Discriminant Space"
      direction TB
      A["Data in Original Space"]
      B["Projection on Discriminant Space"]
      C["Improved Class Separation"]
      D["Simplified Hyperplane Search"]
      A --> B
      B --> C
      C --> D
   end
```

A busca por **hiperplanos separadores** visa encontrar uma fronteira linear que maximize a separa√ß√£o entre as classes, e a proje√ß√£o dos dados atrav√©s do Crit√©rio de Fisher simplifica essa busca, concentrando as informa√ß√µes mais relevantes para a classifica√ß√£o em um subespa√ßo de menor dimens√£o [^4.5.2]. A proje√ß√£o sobre as coordenadas discriminantes tamb√©m leva a uma separa√ß√£o das classes mais evidente, e, consequentemente, facilita a busca por hiperplanos separadores.

O algoritmo do **Perceptron** busca um hiperplano separador ajustando iterativamente os par√¢metros do modelo. A aplica√ß√£o do Perceptron no subespa√ßo projetado atrav√©s do Crit√©rio de Fisher pode acelerar a converg√™ncia do algoritmo, pois o problema √© simplificado pela proje√ß√£o. Em contextos n√£o linearmente separ√°veis, o Perceptron n√£o garante a converg√™ncia para uma solu√ß√£o ideal [^4.5.1].

> üí° **Exemplo Num√©rico:**
>
> Vamos usar os dados projetados no exemplo anterior:
>
> $X_{proj} = \begin{bmatrix} -2 \\ -1 \\ 0 \\ 1 \\ 2 \end{bmatrix}$, $y = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 1 \end{bmatrix}$
>
> O algoritmo do Perceptron busca um hiperplano (neste caso, um ponto na dimens√£o projetada) que separa as classes. Inicializamos um vetor de pesos $\beta = [0.1]$.
>
> 1. Para o primeiro ponto $x_1 = -2$, a predi√ß√£o √© $\hat{y}_1 = \text{sign}(\beta x_1) = \text{sign}(-0.2) = -1$, que est√° incorreta, pois $y_1 = 0$. Atualizamos o peso: $\beta = \beta + \eta(y_1 - \hat{y}_1)x_1 = 0.1 + 0.1(0 - (-1))(-2) = 0.1 - 0.2 = -0.1$ (onde $\eta = 0.1$ √© a taxa de aprendizagem)
> 2. Para o segundo ponto $x_2 = -1$, a predi√ß√£o √© $\hat{y}_2 = \text{sign}(\beta x_2) = \text{sign}(0.1) = 1$, que est√° incorreta, pois $y_2 = 0$. Atualizamos o peso: $\beta = -0.1 + 0.1(0 - 1)(-1) = -0.1 + 0.1 = 0.0$
> 3. Para o terceiro ponto $x_3 = 0$, a predi√ß√£o √© $\hat{y}_3 = \text{sign}(\beta x_3) = \text{sign}(0) = 0$, que est√° incorreta, pois $y_3 = 1$. Atualizamos o peso: $\beta = 0 + 0.1(1 - 0)(0) = 0$
> 4. Para o quarto ponto $x_4 = 1$, a predi√ß√£o √© $\hat{y}_4 = \text{sign}(\beta x_4) = \text{sign}(0) = 0$, que est√° incorreta, pois $y_4 = 1$. Atualizamos o peso: $\beta = 0 + 0.1(1 - 0)(1) = 0.1$
> 5. Para o quinto ponto $x_5 = 2$, a predi√ß√£o √© $\hat{y}_5 = \text{sign}(\beta x_5) = \text{sign}(0.2) = 1$, que est√° correta.
>
> O processo continua iterativamente at√© que todos os pontos sejam classificados corretamente ou um n√∫mero m√°ximo de itera√ß√µes seja atingido. A proje√ß√£o sobre as coordenadas discriminantes simplifica a busca pelo hiperplano separador, pois reduz a dimensionalidade do problema.

**Teorema:** *Em um cen√°rio de dados linearmente separ√°veis, a proje√ß√£o dos dados atrav√©s do Crit√©rio de Fisher para a obten√ß√£o das coordenadas discriminantes, seguida da aplica√ß√£o do algoritmo do Perceptron, garante converg√™ncia para um hiperplano separador em um n√∫mero finito de itera√ß√µes.* A proje√ß√£o simplifica o problema e a converg√™ncia para o hiperplano √© garantida pelo m√©todo. [^4.5.1]

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

**Resposta:**

A **Regra de Decis√£o Bayesiana** busca classificar uma observa√ß√£o $x$ na classe $k$ que maximize a probabilidade posterior $P(G=k|X=x)$ [^4.3]. Sob a suposi√ß√£o de que as distribui√ß√µes condicionais $P(X|G=k)$ s√£o Gaussianas com a mesma matriz de covari√¢ncia $\Sigma$, a probabilidade posterior √© dada por:

$$
P(G=k|X=x) = \frac{ \phi(x;\mu_k,\Sigma)\pi_k}{\sum_{l=1}^K \phi(x;\mu_l,\Sigma)\pi_l}
$$

onde $\phi(x;\mu_k,\Sigma)$ √© a densidade gaussiana para a classe $k$, $\mu_k$ √© a m√©dia da classe $k$ e $\pi_k$ √© a probabilidade a priori da classe. O **LDA** deriva suas fun√ß√µes discriminantes lineares diretamente dessas suposi√ß√µes e tamb√©m aplica o Crit√©rio de Fisher para derivar coordenadas discriminantes que permitam projetar os dados em um subespa√ßo onde as classes s√£o mais bem separadas [^4.3], [^4.3.3].

```mermaid
graph LR
    subgraph "Bayesian Decision vs LDA"
      direction TB
        A["Bayesian Decision Rule"]
        B["Assumes Gaussian Distributions with Equal Covariance Matrices"]
        C["LDA Derives Linear Discriminant Functions"]
        D["LDA Uses Fisher's Criterion for Dimensionality Reduction"]
        E["Equivalent when Assumptions Hold"]
        A --> B
        B --> C
        C --> D
        B & D --> E
    end
```

**Lemma 4:** *Sob a suposi√ß√£o de distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia, a regra de decis√£o Bayesiana e o LDA s√£o equivalentes, e a aplica√ß√£o do Crit√©rio de Fisher para obten√ß√£o das coordenadas discriminantes n√£o altera a capacidade de ambos os m√©todos de separar as classes, mas leva a um espa√ßo de menor dimens√£o, e simplifica o processo.* A equival√™ncia √© demonstrada mostrando que o log-ratio das probabilidades posteriores leva √† fun√ß√£o discriminante do LDA. [^4.3]

**Corol√°rio 4:** *Quando a suposi√ß√£o de igualdade de covari√¢ncias √© relaxada, a regra de decis√£o Bayesiana leva ao QDA, que n√£o utiliza o Crit√©rio de Fisher para derivar coordenadas discriminantes, pois as matrizes de covari√¢ncia s√£o diferentes para cada classe e a fronteira de decis√£o se torna quadr√°tica, n√£o sendo poss√≠vel a aplica√ß√£o da transforma√ß√£o linear gerada pelo Crit√©rio de Fisher.* A flexibilidade do QDA em rela√ß√£o ao LDA o torna um classificador mais adequado para cen√°rios onde as premissas do LDA n√£o s√£o v√°lidas. [^4.3.1], [^4.3.3].

> ‚ö†Ô∏è **Ponto Crucial**: A principal diferen√ßa entre LDA e a regra de decis√£o Bayesiana est√° nas suposi√ß√µes. O LDA imp√µe a restri√ß√£o da igualdade de covari√¢ncias para gerar uma fun√ß√£o discriminante linear e aplica o Crit√©rio de Fisher para obter coordenadas discriminantes e simplificar o problema. A regra de decis√£o Bayesiana, quando combinada com a suposi√ß√£o gaussiana e de covari√¢ncia igual, leva √† mesma solu√ß√£o do LDA [^4.3].

### Conclus√£o

Neste cap√≠tulo, exploramos a utiliza√ß√£o do Crit√©rio de Fisher para a redu√ß√£o de dimensionalidade e a obten√ß√£o de coordenadas discriminantes em modelos de classifica√ß√£o linear. Analisamos como a SVD pode ser utilizada para derivar essas coordenadas de forma eficiente e como a proje√ß√£o dos dados sobre esses subespa√ßos simplifica o problema de classifica√ß√£o. Discutimos como a regress√£o linear com matrizes de indicadores n√£o utiliza essa t√©cnica e n√£o se beneficia da mesma redu√ß√£o de dimensionalidade e como a sele√ß√£o de vari√°veis e regulariza√ß√£o complementam a redu√ß√£o de dimensionalidade, e como a obten√ß√£o de hiperplanos separadores pode ser facilitada. A an√°lise comparativa entre LDA e a regra de decis√£o Bayesiana sob distribui√ß√µes gaussianas com covari√¢ncias iguais tamb√©m elucidou a base te√≥rica desses m√©todos. Atrav√©s deste cap√≠tulo, procuramos fornecer uma vis√£o abrangente de como a redu√ß√£o de dimensionalidade e a utiliza√ß√£o de coordenadas discriminantes contribuem para a constru√ß√£o de modelos de classifica√ß√£o linear mais robustos e eficientes.

### Footnotes

[^4.1]: *In this chapter we revisit the classification problem and focus on linear methods for classification...There are several different ways in which linear decision boundaries can be found.* *(Trecho de Linear Methods for Classification)*

[^4.2]: *In Chapter 2 we fit linear regression models to the class indicator variables, and classify to the largest fit...Linear inequalities in this space are quadratic inequalities in the original space.* *(Trecho de Linear Methods for Classification)*

[^4.3]: *Decision theory for classification (Section 2.4) tells us that we need to know the class posteriors Pr(G|X) for optimal classification. Suppose fk(x) is the class-conditional density of X in class G = k, and let œÄŒ∫ be the prior probability of class k... Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Œ£k = Œ£.* *(Trecho de Linear Methods for Classification)*

[^4.3.1]: *The decision boundary between each pair of classes k and l is described by a quadratic equation {x: Œ¥Œ∫(x) = Œ¥(x)}.* *(Trecho de Linear Methods for Classification)*

[^4.3.2]: *The estimates for QDA are similar to those for LDA, except that separate covariance matrices must be estimated for each class...Their computations are simplified by diagonalizing ‚àë or √âk.* *(Trecho de Linear Methods for Classification)*

[^4.3.3]: *In the special case when we assume that the classes have a common covariance matrix...When the classes are really Gaussian, then LDA is optimal* *(Trecho de Linear Methods for Classification)*

[^4.4]: *The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0,1].* *(Trecho de Linear Methods for Classification)*

[^4.4.1]: *Logistic regression models are usually fit by maximum likelihood... The logistic regression model is more general, in that it makes less assumptions.* *(Trecho de Linear Methods for Classification)*

[^4.4.2]: *It is convenient to code the two-class gi via a 0/1 response Yi, where yi = 1 when gi = 1, and yi = 0 when gi = 2... Typically many models are fit in a search for a parsimonious model involving a subset of the variables.* *(Trecho de Linear Methods for Classification)*

[^4.4.3]: *To maximize the log-likelihood, we set its derivatives to zero. These score equations are...To solve the score equations (4.21), we use the Newton-Raphson algorithm...* *(Trecho de Linear Methods for Classification)*

[^4.4.4]: *The L1 penalty used in the lasso (Section 3.4.2) can be used for variable selection and shrinkage with any linear regression model...As with the lasso, we typically do not penalize the intercept term.* *(Trecho de Linear Methods for Classification)*

[^4.5]: *Here we present an analysis of binary data to illustrate the traditional statistical use of the logistic regression model... With two classes there is a simple correspondence between linear discriminant analysis and classification by linear least squares, as in (4.5).* *(Trecho de Linear Methods for Classification)*

[^4.5.1]: *The perceptron learning algorithm tries to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary.* *(Trecho de Linear Methods for Classification)*

[^4.5.2]: *The optimal separating hyperplane separates the two classes and maximizes the distance to the closest point from either class... In light of (4.40), the constraints define an empty slab or margin around the linear decision boundary...* *(Trecho de Linear Methods for Classification)*
