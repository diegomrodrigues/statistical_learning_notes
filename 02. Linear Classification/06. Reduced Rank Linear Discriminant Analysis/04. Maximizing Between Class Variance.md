## T√≠tulo Conciso: Classifica√ß√£o Linear e Otimiza√ß√£o da Separa√ß√£o de Classes: Maximizando a Vari√¢ncia Inter-Classe e Minimizado a Intra-Classe

```mermaid
graph LR
    subgraph "Class Separation Optimization"
        direction TB
        A["Data Distribution"]
        B["Class Centroids (Œº_k)"]
        C["Inter-Class Variance (B)"]
        D["Intra-Class Variance (W)"]
        E["Maximize B/W"]
        F["Optimal Linear Classifier"]
        A --> B
        B --> C
        B --> D
        C & D --> E
        E --> F
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora a fundo o conceito de **maximiza√ß√£o da vari√¢ncia entre classes (inter-classe)** e a **minimiza√ß√£o da vari√¢ncia dentro das classes (intra-classe)**, e como essa ideia fundamental se conecta com a constru√ß√£o de modelos de classifica√ß√£o linear. Analisaremos como essa forma de otimiza√ß√£o, que busca projetar os dados de forma a separar as classes da melhor forma poss√≠vel, √© utilizada para derivar as **coordenadas discriminantes** ou **vari√°veis can√¥nicas** no **Linear Discriminant Analysis (LDA)** e como o **Crit√©rio de Fisher** quantifica essa otimiza√ß√£o [^4.3.3]. Discutiremos como essa abordagem contrasta com a **regress√£o linear com matrizes de indicadores**, que n√£o utiliza explicitamente a vari√¢ncia entre e dentro das classes no ajuste do modelo [^4.2], e com a **regress√£o log√≠stica**, que modela as probabilidades posteriores, mas n√£o utiliza a decomposi√ß√£o da matriz de covari√¢ncia das classes [^4.4]. Abordaremos a import√¢ncia da **sele√ß√£o de vari√°veis e regulariza√ß√£o** para controlar a complexidade dos modelos e melhorar a estabilidade dos resultados [^4.4.4], [^4.5]. Exploraremos tamb√©m a rela√ß√£o entre **hiperplanos separadores** e a ideia de maximizar a separa√ß√£o entre as classes atrav√©s da maximiza√ß√£o da vari√¢ncia inter-classe e minimiza√ß√£o da vari√¢ncia intra-classe [^4.5.2]. O objetivo deste cap√≠tulo √© fornecer uma compreens√£o detalhada de como a maximiza√ß√£o da vari√¢ncia entre classes e a minimiza√ß√£o da vari√¢ncia dentro das classes se relacionam com a constru√ß√£o de modelos de classifica√ß√£o linear eficientes.

### Conceitos Fundamentais

**Conceito 1: Maximiza√ß√£o da Vari√¢ncia Inter-Classe e Minimiza√ß√£o da Vari√¢ncia Intra-Classe**

O objetivo central da classifica√ß√£o linear √© encontrar uma fronteira de decis√£o que separe as classes da melhor forma poss√≠vel. Uma maneira de quantificar essa separa√ß√£o √© atrav√©s da maximiza√ß√£o da **vari√¢ncia entre classes**, tamb√©m conhecida como vari√¢ncia inter-classe, que mede o grau de separa√ß√£o entre os centroides das classes, e atrav√©s da minimiza√ß√£o da **vari√¢ncia dentro das classes**, tamb√©m conhecida como vari√¢ncia intra-classe, que mede a dispers√£o das observa√ß√µes em torno do seu centroide [^4.3.3]. A ideia √© que, ao projetar os dados em um subespa√ßo, as classes sejam o mais separadas poss√≠vel e que as observa√ß√µes de cada classe estejam o mais agrupadas poss√≠vel [^4.3.3]. A raz√£o entre vari√¢ncia inter-classe e vari√¢ncia intra-classe √© utilizada para encontrar um subespa√ßo que maximize essa raz√£o.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos duas classes com as seguintes amostras bidimensionais:
>
> Classe 1: $X_1 = \begin{bmatrix} 1 & 2 \\ 1.5 & 1.8 \\ 1.2 & 2.1 \end{bmatrix}$
>
> Classe 2: $X_2 = \begin{bmatrix} 5 & 8 \\ 5.5 & 8.2 \\ 4.8 & 7.9 \end{bmatrix}$
>
> 1. **C√°lculo dos centroides:**
>   $\mu_1 = \frac{1}{3} \begin{bmatrix} 1+1.5+1.2 \\ 2+1.8+2.1 \end{bmatrix} = \begin{bmatrix} 1.23 \\ 1.97 \end{bmatrix}$
>   $\mu_2 = \frac{1}{3} \begin{bmatrix} 5+5.5+4.8 \\ 8+8.2+7.9 \end{bmatrix} = \begin{bmatrix} 5.1 \\ 8.03 \end{bmatrix}$
>
> 2. **Centroide geral:** Supondo $\pi_1 = \pi_2 = 0.5$,
>   $\mu = 0.5 * \mu_1 + 0.5 * \mu_2 = \begin{bmatrix} 3.165 \\ 5 \end{bmatrix}$
>
> 3. **Vari√¢ncia inter-classe (B):**
>  $B = 0.5 * (\mu_1 - \mu)(\mu_1 - \mu)^T + 0.5 * (\mu_2 - \mu)(\mu_2 - \mu)^T$
>  $B = 0.5 * \begin{bmatrix} -1.935 \\ -3.03 \end{bmatrix} \begin{bmatrix} -1.935 & -3.03 \end{bmatrix} + 0.5 * \begin{bmatrix} 1.935 \\ 3.03 \end{bmatrix} \begin{bmatrix} 1.935 & 3.03 \end{bmatrix}$
>  $B = \begin{bmatrix} 3.744 & 5.863 \\ 5.863 & 9.181 \end{bmatrix}$
>
> 4. **Vari√¢ncia intra-classe (W):**
>   Para simplificar, vamos supor que a vari√¢ncia intra-classe seja a mesma para ambas as classes e igual √† matriz identidade:
>   $W = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$
>
> O objetivo √© encontrar uma proje√ß√£o que maximize a raz√£o $\frac{a^T B a}{a^T W a}$. Neste exemplo, podemos ver que a vari√¢ncia entre as classes √© muito maior do que a vari√¢ncia dentro das classes (que foi simplificada como a matriz identidade), indicando que as classes est√£o bem separadas. O Crit√©rio de Fisher busca um vetor *a* que maximize essa separa√ß√£o.

**Lemma 1:** *A maximiza√ß√£o da vari√¢ncia inter-classe e a minimiza√ß√£o da vari√¢ncia intra-classe, atrav√©s do Crit√©rio de Fisher, busca uma transforma√ß√£o linear dos dados que maximiza a separa√ß√£o entre as classes e o agrupamento dos dados dentro de cada classe.* A prova desse lema se baseia na formula√ß√£o do Crit√©rio de Fisher e na sua deriva√ß√£o para encontrar a transforma√ß√£o √≥tima.

**Conceito 2: O Crit√©rio de Fisher e as Matrizes de Vari√¢ncia entre e dentro das Classes**

O **Crit√©rio de Fisher** busca formalizar o objetivo de maximizar a separa√ß√£o entre as classes e minimizar a dispers√£o dentro de cada classe. Para isso, utiliza duas matrizes de vari√¢ncia: a matriz de vari√¢ncia entre classes $B$, dada por:

$$
B = \sum_{k=1}^K \pi_k (\mu_k - \mu)(\mu_k - \mu)^T
$$

onde $\mu_k$ √© o centroide da classe $k$, $\mu$ √© o centroide geral, e $\pi_k$ √© a probabilidade a priori da classe $k$, e a matriz de vari√¢ncia dentro das classes (pooled covariance) $W$, dada por:

$$
W = \frac{1}{N - K} \sum_{k=1}^K \sum_{i: g_i = k} (x_i - \mu_k) (x_i - \mu_k)^T
$$

onde a segunda soma se restringe √†s amostras da classe $k$. O Crit√©rio de Fisher busca a transforma√ß√£o linear $Z = a^T X$ que maximize a raz√£o $\frac{a^T B a}{a^T W a}$, e essa raz√£o √© conhecida como Rayleigh quotient [^4.3.3]. Os autovetores da matriz $W^{-1}B$ definem as dire√ß√µes no espa√ßo original onde a separa√ß√£o entre as classes √© maximizada.

```mermaid
graph LR
    subgraph "Fisher's Criterion"
        direction TB
        A["Inter-Class Variance Matrix (B)"]
        B["Intra-Class Variance Matrix (W)"]
        C["Transformation Vector (a)"]
        D["Maximize Rayleigh Quotient: (a^T B a) / (a^T W a)"]
        E["Eigenvectors of W^-1B"]
        A & B --> D
        D --> C
        C --> E
    end
```

> üí° **Exemplo Num√©rico:**
>
>  Usando os dados do exemplo anterior, vamos calcular $W^{-1}B$. Assumimos $W = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$, ent√£o $W^{-1} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.
>
>  $W^{-1}B = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 3.744 & 5.863 \\ 5.863 & 9.181 \end{bmatrix} = \begin{bmatrix} 3.744 & 5.863 \\ 5.863 & 9.181 \end{bmatrix}$
>
>  Para encontrar as dire√ß√µes que maximizam a separa√ß√£o das classes, precisamos calcular os autovetores desta matriz. Os autovetores de $W^{-1}B$ apontam nas dire√ß√µes onde a separa√ß√£o entre as classes √© maximizada.
>
>  Os autovalores e autovetores podem ser calculados usando numpy:
>
> ```python
> import numpy as np
>
> B = np.array([[3.744, 5.863], [5.863, 9.181]])
> W = np.array([[1, 0], [0, 1]])
>
> w, v = np.linalg.eig(np.linalg.inv(W) @ B)
>
> print("Autovalores:", w)
> print("Autovetores:", v)
> ```
>  Isso retornar√° dois autovalores, onde o maior corresponde √† dire√ß√£o de maior separa√ß√£o, e os autovetores associados indicam a dire√ß√£o no espa√ßo original.

**Corol√°rio 1:** *O Crit√©rio de Fisher, ao utilizar a raz√£o entre a vari√¢ncia entre classes e a vari√¢ncia dentro das classes, quantifica o objetivo de maximizar a separa√ß√£o entre classes em rela√ß√£o √† variabilidade dentro de cada classe.*  Este corol√°rio enfatiza o significado das matrizes de vari√¢ncia e de seu uso no crit√©rio de Fisher.

**Conceito 3: A Interpreta√ß√£o Geom√©trica da Maximiza√ß√£o de Vari√¢ncias**

A maximiza√ß√£o da vari√¢ncia entre classes e a minimiza√ß√£o da vari√¢ncia dentro das classes pode ser interpretada geometricamente como a busca por um subespa√ßo onde os centroides das classes estejam o mais distantes poss√≠vel, enquanto as amostras de cada classe estejam o mais agrupadas poss√≠vel [^4.3.3].  Ao projetar os dados nesse subespa√ßo, a separa√ß√£o entre as classes torna-se mais evidente, o que simplifica a tomada de decis√£o.  Essa interpreta√ß√£o geom√©trica auxilia na compreens√£o intuitiva do Crit√©rio de Fisher.

> üí° **Exemplo Num√©rico:**
>
>  Imagine os dados do exemplo anterior plotados em um gr√°fico 2D. A proje√ß√£o √≥tima encontrada pelo Crit√©rio de Fisher seria uma linha (1D) que, quando os dados s√£o projetados nela, maximiza a dist√¢ncia entre os centroides das classes e minimiza a dispers√£o dos pontos de cada classe em torno de seus respectivos centroides. Essa linha seria definida pelo autovetor principal de $W^{-1}B$.

> ‚ö†Ô∏è **Nota Importante**: A maximiza√ß√£o da vari√¢ncia entre classes e a minimiza√ß√£o da vari√¢ncia dentro das classes s√£o objetivos fundamentais para a constru√ß√£o de modelos de classifica√ß√£o linear eficientes, e esses objetivos s√£o formalizados atrav√©s do Crit√©rio de Fisher.

> ‚ùó **Ponto de Aten√ß√£o**: A maximiza√ß√£o da raz√£o de vari√¢ncias no Crit√©rio de Fisher busca um compromisso entre a separa√ß√£o das classes e a variabilidade dentro de cada classe, e os autovalores da matriz $W^{-1}B$ quantificam a import√¢ncia das dire√ß√µes encontradas.

> ‚úîÔ∏è **Destaque**: O Crit√©rio de Fisher, ao otimizar a raz√£o entre vari√¢ncia inter-classes e vari√¢ncia intra-classes, busca o subespa√ßo onde as classes sejam mais bem separadas.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
    subgraph "Comparison of Methods"
        direction LR
        A["Linear Regression with Indicator Matrices"]
        B["Fisher's Linear Discriminant Analysis (LDA)"]
        C["Minimizes Sum of Squared Errors"]
        D["Maximizes Inter-Class Variance / Intra-Class Variance"]
        A --> C
        B --> D
        C -.-> E["No explicit use of covariance matrices"]
        D -.-> F["Uses between-class and within-class covariance matrices"]

    end
```

A **regress√£o linear com matrizes de indicadores**, ao contr√°rio do LDA, n√£o utiliza explicitamente as informa√ß√µes sobre a vari√¢ncia entre classes e a vari√¢ncia dentro das classes [^4.2]. A regress√£o linear busca encontrar os coeficientes que minimizem a soma dos quadrados dos erros, de forma independente para cada classe, sem considerar explicitamente as rela√ß√µes entre as classes em termos de vari√¢ncia e covari√¢ncia.

O ajuste da regress√£o linear √© feita atrav√©s da minimiza√ß√£o da seguinte fun√ß√£o de custo:

$$
\min_{\beta_{k0}, \beta_k} \sum_{i=1}^N (y_{ik} - (\beta_{k0} + \beta_k^T x_i))^2
$$

onde $y_{ik}$ √© o indicador da classe $k$ para a observa√ß√£o $i$.  Essa forma de ajuste n√£o considera como a distribui√ß√£o das classes se relaciona entre si ou a variabilidade dentro e entre as classes, e pode levar a modelos que n√£o separam as classes da melhor forma, especialmente quando o n√∫mero de vari√°veis √© grande [^4.2].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos duas classes e um preditor. Vamos criar dados para exemplificar a regress√£o linear:
>
> Classe 1: $X_1 = \begin{bmatrix} 1 \\ 1.5 \\ 2 \end{bmatrix}$, $Y_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$ (representado por 1)
>
> Classe 2: $X_2 = \begin{bmatrix} 4 \\ 4.5 \\ 5 \end{bmatrix}$, $Y_2 = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$ (representado por 0)
>
>  Podemos usar a regress√£o linear para modelar um indicador para cada classe. Para a classe 1, o modelo seria:
>
>  $y_{i1} = \beta_{10} + \beta_{1} x_i + \epsilon_i$
>
>  Para a classe 2, o modelo seria:
>
>  $y_{i2} = \beta_{20} + \beta_{2} x_i + \epsilon_i$
>
>  O ajuste desses modelos independentemente, usando m√≠nimos quadrados, n√£o leva em considera√ß√£o a separabilidade entre as classes.
>
>  Usando Python, podemos calcular os coeficientes:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> X_1 = np.array([[1], [1.5], [2]])
> Y_1 = np.array([1, 1, 1])
> X_2 = np.array([[4], [4.5], [5]])
> Y_2 = np.array([0, 0, 0])
>
> model_1 = LinearRegression().fit(X_1, Y_1)
> model_2 = LinearRegression().fit(X_2, Y_2)
>
> print("Classe 1 - Intercept:", model_1.intercept_, "Coeficiente:", model_1.coef_)
> print("Classe 2 - Intercept:", model_2.intercept_, "Coeficiente:", model_2.coef_)
> ```
>
>  Este exemplo mostra que a regress√£o linear ajusta um modelo para cada classe independentemente, sem considerar a separa√ß√£o entre as classes. O LDA, por outro lado, buscaria um subespa√ßo que maximize a separa√ß√£o entre as classes.

Em contraste, o LDA utiliza o Crit√©rio de Fisher para projetar os dados em um subespa√ßo que maximize a separa√ß√£o entre as classes e minimize a variabilidade dentro de cada classe, utilizando as informa√ß√µes sobre a matriz de covari√¢ncia entre e dentro das classes [^4.3.3].

**Lemma 2:** *A regress√£o linear com matrizes de indicadores n√£o busca explicitamente a maximiza√ß√£o da vari√¢ncia entre classes e a minimiza√ß√£o da vari√¢ncia dentro das classes, ao contr√°rio do LDA que utiliza o Crit√©rio de Fisher para definir uma proje√ß√£o que maximize essa raz√£o.* A demonstra√ß√£o desse lema se encontra na formula√ß√£o da fun√ß√£o de custo da regress√£o linear, que n√£o utiliza informa√ß√µes sobre a matriz de covari√¢ncia.

**Corol√°rio 2:** *A falta de considera√ß√£o da vari√¢ncia entre classes e dentro das classes na regress√£o linear com matrizes de indicadores pode resultar em modelos menos eficientes em termos de separa√ß√£o das classes, do que modelos que maximizam a separabilidade atrav√©s do Crit√©rio de Fisher, como o LDA.*  Este corol√°rio ressalta a import√¢ncia de maximizar a separabilidade entre as classes para a obten√ß√£o de bons modelos lineares de classifica√ß√£o.

A regress√£o linear com matrizes de indicadores, embora simples de implementar, n√£o utiliza as informa√ß√µes sobre a variabilidade dos dados e n√£o busca explicitamente a separa√ß√£o entre as classes, o que a torna menos adequada em compara√ß√£o com o LDA, especialmente em problemas onde a separabilidade linear n√£o √© evidente no espa√ßo original e onde a escolha de um bom subespa√ßo de proje√ß√£o √© fundamental [^4.2], [^4.3.3].

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Regularization Impact"
        direction TB
        A["Feature Selection"]
        B["Regularization"]
        C["L1 Penalty (Lasso):  P(Œ≤) = ‚àë|Œ≤_j|"]
        D["L2 Penalty (Ridge): P(Œ≤) = ‚àëŒ≤_j¬≤"]
        E["Controls Model Complexity"]
         F["Improves Generalization"]
        A & B --> E
        C --> A
        D --> B
        E --> F
   end
```

A **sele√ß√£o de vari√°veis** e a **regulariza√ß√£o** desempenham um papel fundamental na constru√ß√£o de modelos de classifica√ß√£o linear mais robustos e generaliz√°veis, especialmente quando se busca maximizar a vari√¢ncia entre classes e minimizar a vari√¢ncia dentro das classes [^4.5]. A regulariza√ß√£o, ao adicionar um termo de penalidade √† fun√ß√£o de custo, restringe a magnitude dos coeficientes, o que pode estabilizar a proje√ß√£o dos dados em subespa√ßos discriminantes, e tamb√©m permite o controle do overfitting.

Na **regress√£o log√≠stica**, a fun√ß√£o de custo regularizada pode ser expressa como:

$$
\max_{\beta_0, \beta} \left[ \sum_{i=1}^N \left( y_i (\beta_0 + \beta^T x_i) - \log(1 + e^{\beta_0 + \beta^T x_i}) \right) - \lambda P(\beta) \right]
$$

onde $P(\beta)$ √© a penalidade e $\lambda$ √© o par√¢metro de regulariza√ß√£o. A penalidade **L1** (Lasso), dada por $P(\beta) = \sum_{j=1}^p |\beta_j|$, promove a esparsidade dos coeficientes, selecionando as vari√°veis mais relevantes para a modelagem do log-odds e da separabilidade entre as classes [^4.4.4]. A penalidade **L2** (Ridge), dada por $P(\beta) = \sum_{j=1}^p \beta_j^2$, reduz a magnitude dos coeficientes, estabilizando o modelo e melhorando a sua capacidade de generaliza√ß√£o [^4.5].

> üí° **Exemplo Num√©rico:**
>
> Vamos criar um exemplo com regress√£o log√≠stica com regulariza√ß√£o L1 e L2:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LogisticRegression
> from sklearn.preprocessing import StandardScaler
>
> # Dados de exemplo (2 classes, 5 features)
> np.random.seed(42)
> X = np.random.rand(100, 5)
> y = np.random.randint(0, 2, 100)
>
> # Normaliza√ß√£o dos dados
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)
>
> # Regress√£o log√≠stica com regulariza√ß√£o L1 (Lasso)
> model_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.5)
> model_l1.fit(X_scaled, y)
>
> # Regress√£o log√≠stica com regulariza√ß√£o L2 (Ridge)
> model_l2 = LogisticRegression(penalty='l2', C=0.5)
> model_l2.fit(X_scaled, y)
>
> print("Coeficientes L1:", model_l1.coef_)
> print("Coeficientes L2:", model_l2.coef_)
>
> # Comparando o n√∫mero de coeficientes n√£o nulos
> print("N√∫mero de coeficientes n√£o nulos (L1):", np.sum(model_l1.coef_ != 0))
> print("N√∫mero de coeficientes n√£o nulos (L2):", np.sum(model_l2.coef_ != 0))
> ```
>
> Neste exemplo, podemos observar que a regulariza√ß√£o L1 (Lasso) leva a mais coeficientes iguais a zero, promovendo a sele√ß√£o de vari√°veis, enquanto a regulariza√ß√£o L2 (Ridge) reduz a magnitude dos coeficientes, mas geralmente mant√©m todos diferentes de zero. A escolha de $\lambda$ √© feita via valida√ß√£o cruzada.

A combina√ß√£o de redu√ß√£o de dimensionalidade com t√©cnicas de regulariza√ß√£o e sele√ß√£o de vari√°veis permite obter modelos que s√£o mais robustos, e que focam nas vari√°veis mais informativas, resultando em uma separa√ß√£o entre classes mais eficiente e um melhor ajuste dos par√¢metros.

**Lemma 3:** *A regulariza√ß√£o L1, ao promover a esparsidade nos coeficientes, leva √† sele√ß√£o de vari√°veis mais relevantes para a separa√ß√£o das classes, e, portanto, a modelos mais simples e com melhor capacidade de generaliza√ß√£o.* A prova desse lema se encontra na forma da penalidade L1 e como ela influencia na escolha das vari√°veis.

**Prova do Lemma 3:** A penalidade L1 for√ßa os coeficientes menos importantes a se tornarem exatamente zero, selecionando as vari√°veis que mais contribuem para a separa√ß√£o das classes.  Essa sele√ß√£o leva a modelos mais simples e menos sujeitos ao *overfitting*, e garante que a maximiza√ß√£o da vari√¢ncia entre classes e a minimiza√ß√£o da vari√¢ncia dentro das classes seja mais est√°vel e com mais poder preditivo [^4.4.3], [^4.4.4]. $\blacksquare$

**Corol√°rio 3:** *A utiliza√ß√£o conjunta de t√©cnicas de sele√ß√£o de vari√°veis e regulariza√ß√£o, em conjunto com a otimiza√ß√£o do Rayleigh quociente e a proje√ß√£o em subespa√ßos discriminantes, leva √† constru√ß√£o de modelos de classifica√ß√£o mais robustos e eficientes, maximizando a vari√¢ncia entre classes e minimizando a vari√¢ncia intra-classes em subespa√ßos mais informativos.*  A combina√ß√£o das abordagens resulta em um modelo mais eficiente e com maior capacidade de generaliza√ß√£o.

> ‚ö†Ô∏è **Ponto Crucial**:  A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o ferramentas importantes para complementar a maximiza√ß√£o da vari√¢ncia entre classes e minimiza√ß√£o da vari√¢ncia dentro das classes, levando a modelos de classifica√ß√£o lineares mais robustos e generaliz√°veis [^4.5].

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
    subgraph "Hyperplane Optimization"
        direction TB
        A["Fisher's Criterion (W^-1B)"]
        B["Optimal Subspace Projection"]
        C["Separating Hyperplane"]
        D["Perceptron Iterative Search"]
        A --> B
        B --> C
        C --> D
        A -.-> E["Maximizes Class Separation"]
        D -.-> E
    end
```

A busca por **hiperplanos separadores** visa encontrar uma fronteira linear que maximize a dist√¢ncia entre as classes e maximize a separa√ß√£o das classes [^4.5.2].  A utiliza√ß√£o do Crit√©rio de Fisher e a proje√ß√£o dos dados em subespa√ßos discriminantes podem ser vistas como uma forma de simplificar essa busca, utilizando as matrizes de vari√¢ncia entre e dentro das classes para identificar dire√ß√µes que separam melhor as classes.

O algoritmo do **Perceptron**, por sua vez, busca um hiperplano separador atrav√©s do ajuste iterativo dos seus par√¢metros com base nas amostras mal classificadas [^4.5.1].  Embora o Perceptron n√£o utilize o Crit√©rio de Fisher de forma expl√≠cita, a sua converg√™ncia, em casos linearmente separ√°veis, busca uma solu√ß√£o onde a separa√ß√£o entre as classes seja maximizada, e a proje√ß√£o em um subespa√ßo discriminante pode auxiliar na converg√™ncia do m√©todo. A escolha do hiperplano √≥timo atrav√©s do Crit√©rio de Fisher resulta em um problema de otimiza√ß√£o que pode ser resolvido atrav√©s da aplica√ß√£o da SVD generalizada, o que corresponde √† maximiza√ß√£o da vari√¢ncia entre classes e minimiza√ß√£o da vari√¢ncia dentro das classes, como tamb√©m √© o objetivo do Perceptron [^4.3.3].

> üí° **Exemplo Num√©rico:**
>
> Imagine duas classes de dados que podem ser separadas por um hiperplano (uma linha em 2D). O algoritmo do Perceptron ajustaria iterativamente a inclina√ß√£o e a posi√ß√£o dessa linha at√© que todas ou a maioria das amostras estejam corretamente classificadas. O Crit√©rio de Fisher, por outro lado, encontraria uma proje√ß√£o √≥tima para os dados, que maximizaria a separa√ß√£o entre as classes, o que poderia facilitar a converg√™ncia do Perceptron.

**Teorema:** *Em um cen√°rio de dados linearmente separ√°veis, a proje√ß√£o dos dados sobre subespa√ßos discriminantes, obtidos atrav√©s da otimiza√ß√£o do Crit√©rio de Fisher, facilita a busca por um hiperplano separador, e o algoritmo do Perceptron pode convergir para uma solu√ß√£o que maximize a separa√ß√£o entre classes, e, portanto, com o mesmo objetivo de maximizar a raz√£o do Crit√©rio de Fisher.* Este teorema demonstra a conex√£o entre as abordagens de otimiza√ß√£o do Rayleigh quociente e a busca por hiperplanos separadores [^4.5.1].

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

**Resposta:**

A **Regra de Decis√£o Bayesiana** busca classificar uma observa√ß√£o $x$ na classe $k$ que maximize a probabilidade posterior $P(G=k|X=x)$ [^4.3].  Sob a suposi√ß√£o de que as classes seguem distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia $\Sigma$, a probabilidade posterior √© dada por:

$$
P(G=k|X=x) = \frac{ \phi(x;\mu_k,\Sigma)\pi_k}{\sum_{l=1}^K \phi(x;\mu_l,\Sigma)\pi_l}
$$

onde $\phi(x;\mu_k,\Sigma)$ √© a densidade gaussiana para a classe $k$, $\mu_k$ √© a m√©dia da classe $k$ e $\pi_k$ √© a probabilidade a priori da classe. O **LDA**, por sua vez, deriva suas fun√ß√µes discriminantes lineares a partir dessas suposi√ß√µes, maximizando a separabilidade entre as classes, utilizando a matriz de covari√¢ncia conjunta e o conceito de Rayleigh quociente [^4.3], [^4.3.3].

```mermaid
graph LR
    subgraph "Bayesian vs LDA"
        direction TB
       A["Bayes Decision Rule: Maximize P(G=k|X=x)"]
        B["Gaussian Distribution Assumption with Same Covariance Œ£"]
        C["Linear Discriminant Analysis (LDA)"]
        D["Maximizes Inter-Class Variance / Intra-Class Variance"]
        E["Posterior probability: P(G=k|X=x)"]
        A --> B
        B --> E
        C --> D
        B -.-> F["Equivalent when covariances are the same"]
        E --> F
         D --> F
    end
```

**Lemma 4:** *Sob a suposi√ß√£o de que as classes seguem distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia, a regra de decis√£o Bayesiana e o LDA s√£o equivalentes, e o Crit√©rio de Fisher se conecta ao LDA atrav√©s da maximiza√ß√£o da vari√¢ncia entre classes e da minimiza√ß√£o da vari√¢ncia dentro das classes.* A prova reside na manipula√ß√£o alg√©brica, mostrando que a maximiza√ß√£o da probabilidade posterior √© equivalente a maximizar a fun√ß√£o discriminante do LDA. [^4.3]

**Corol√°rio 4:** *Quando a restri√ß√£o de igualdade de covari√¢ncias √© relaxada, a regra de decis√£o Bayesiana leva ao QDA, onde a busca por uma solu√ß√£o √≥tima envolve a estima√ß√£o de matrizes de covari√¢ncia distintas para cada classe, e as fun√ß√µes discriminantes se tornam quadr√°ticas.  A aplica√ß√£o do Crit√©rio de Fisher para o QDA n√£o tem a mesma simplicidade e garantia de separabilidade que existe no LDA.* A forma da fronteira de decis√£o torna-se mais flex√≠vel no QDA devido √† suposi√ß√£o de covari√¢ncias diferentes entre as classes. [^4.3.1], [^4.3.3]

> üí° **Exemplo Num√©rico:**
>
>  No caso de LDA, as fronteiras de decis√£o s√£o sempre lineares devido √† suposi√ß√£o de covari√¢ncias iguais. No QDA, as fronteiras podem ser quadr√°ticas, o que permite modelar situa√ß√µes onde as classes t√™m diferentes formas e dispers√µes.
>
>  Se as classes tiverem as mesmas matrizes de covari√¢ncia:
>  $\Sigma_1 = \Sigma_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$, a decis√£o do classificador Bayesiano seria equivalente √† do LDA.
>
>  Se as classes tiverem matrizes de covari√¢ncia diferentes:
>  $\Sigma_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ e $\Sigma_2 = \begin{bmatrix} 2 & 0 \\ 0 & 0.5 \end{bmatrix}$, a decis√£o do classificador Bayesiano levaria a uma fronteira quadr√°tica, como no QDA.

> ‚ö†Ô∏è **Ponto Crucial**: A principal diferen√ßa entre LDA e a regra de decis√£o Bayesiana est√° na forma como a decis√£o √© derivada. O LDA se baseia no Crit√©rio de Fisher para maximizar a separa√ß√£o entre as classes, e tamb√©m se baseia na suposi√ß√£o da igualdade das matrizes de covari√¢ncia, enquanto a regra Bayesiana fornece uma formula√ß√£o geral que, quando aplicada a uma suposi√ß√£o gaussiana com covari√¢ncias iguais, leva ao mesmo resultado do LDA [^4.3].

### Conclus√£o

Neste cap√≠tulo, exploramos o conceito da maximiza√ß√£o da vari√¢ncia entre classes e da minimiza√ß√£o da vari√¢ncia dentro das classes, e como essa ideia est√° no cora√ß√£o da constru√ß√£o de modelos lineares de classifica√ß√£o como o LDA. Analisamos como o Crit√©rio de Fisher quantifica essa forma de otimiza√ß√£o e como a SVD generalizada pode ser utilizada para encontrar os subespa√ßos discriminantes que levam √† melhor separa√ß√£o entre as classes. Discutimos como a regress√£o linear com matrizes de indicadores n√£o explora essas rela√ß√µes de vari√¢ncia e covari√¢ncia. Vimos como a sele√ß√£o de vari√°veis e a regulariza√ß√£o complementam a otimiza√ß√£o do Rayleigh quociente, levando a modelos mais robustos e eficientes, e como os hiperplanos separadores se relacionam com a busca por modelos lineares. Ao longo do cap√≠tulo, procuramos oferecer uma vis√£o clara e detalhada de como a maximiza√ß√£o da vari√¢ncia entre classes e a minimiza√ß√£o da vari√¢ncia intra-classes se conectam com a teoria e a pr√°tica da classifica√ß√£o linear, e como o Crit√©rio de Fisher e a SVD generalizada podem ser utilizadas para encontrar boas solu√ß√µes para o problema de classifica√ß√£o.

### Footnotes

[^4.1]: *In this chapter we revisit the classification problem and focus on linear methods for classification...There are several different ways in which linear decision boundaries can be found.* *(Trecho de Linear Methods for Classification)*

[^4.2]: *In Chapter 2 we fit linear regression models to the class indicator variables, and classify to the largest fit...Linear inequalities in this space are quadratic inequalities in the original space.* *(Trecho de Linear Methods for Classification)*

[^4.3]: *Decision theory for classification (Section 2.4) tells us that we need to know the class posteriors Pr(G|X) for optimal classification. Suppose fk(x) is the class-conditional density of X in class G = k, and let œÄŒ∫ be the prior probability of class k... Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Œ£k = Œ£.* *(Trecho de Linear Methods for Classification)*

[^4.3.1]: *The decision boundary between each pair of classes k and l is described by a quadratic equation {x: Œ¥Œ∫(x) = Œ¥(x)}.* *(Trecho de Linear Methods for Classification)*

[^4.3.2]: *The estimates for QDA are similar to those for LDA, except that separate covariance matrices must be estimated for each class...Their computations are simplified by diagonalizing ‚àë or √âk.* *(Trecho de Linear Methods for Classification)*

[^4.3.3]: *In the special case when we assume that the classes have a common covariance matrix...When the classes are really Gaussian, then LDA is optimal... Finding the sequences of optimal subspaces for LDA involves the following steps:* *(Trecho de Linear Methods for Classification)*

[^4.4]: *The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0,1].* *(Trecho de Linear Methods for Classification)*

[^4.4.1]: *Logistic regression models are usually fit by maximum likelihood... The logistic regression model is more general, in that it makes less assumptions.* *(Trecho de Linear Methods for Classification)*

[^4.4.2]: *It is convenient to code the two-class gi via a 0/1 response Yi, where yi = 1 when gi = 1, and yi = 0 when gi = 2... Typically many models are fit in a search for a parsimonious model involving a subset of the variables.* *(Trecho de Linear Methods for Classification)*

[^4.4.3]: *To maximize the log-likelihood, we set its derivatives to zero. These score equations are...To solve the score equations (4.21), we use the Newton-Raphson algorithm...* *(Trecho de Linear Methods for Classification)*

[^4.4.4]: *The L1 penalty used in the lasso (Section 3.4.2) can be used for variable selection and shrinkage with any linear regression model...As with the lasso, we typically do not penalize the intercept term.* *(Trecho de Linear Methods for Classification)*

[^4.5]: *Here we present an analysis of binary data to illustrate the traditional statistical use of the logistic regression model... With two classes there is a simple correspondence between linear discriminant analysis and classification by linear least squares, as in (4.5).* *(Trecho de Linear Methods for Classification)*

[^4.5