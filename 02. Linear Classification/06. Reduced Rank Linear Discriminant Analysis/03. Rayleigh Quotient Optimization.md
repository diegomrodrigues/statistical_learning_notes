## T√≠tulo Conciso: Classifica√ß√£o Linear e Otimiza√ß√£o: Rayleigh Quociente e Generaliza√ß√£o da SVD

```mermaid
graph LR
    subgraph "Linear Classification System"
        direction TB
        A["Input Data"] --> B["Rayleigh Quotient Optimization"]
        B --> C["Generalized SVD"]
        C --> D["Discriminant Coordinates"]
        D --> E["Linear Classifier"]
        E --> F["Classification Output"]
    end
```

### Introdu√ß√£o

Este cap√≠tulo explora a fundo o problema de otimiza√ß√£o que envolve o **Rayleigh quociente**, e como a **decomposi√ß√£o em valores singulares (SVD) generalizada** pode ser utilizada para encontrar a solu√ß√£o para esse problema, construindo modelos lineares de classifica√ß√£o. Analisaremos como o **Crit√©rio de Fisher**, que busca maximizar a raz√£o entre vari√¢ncia entre classes e vari√¢ncia dentro das classes, pode ser reformulado como um problema de maximiza√ß√£o de um Rayleigh quociente. Discutiremos como a SVD generalizada, que permite a decomposi√ß√£o de matrizes n√£o quadr√°ticas, pode ser utilizada para encontrar as dire√ß√µes que maximizam o Rayleigh quociente, o que leva √† identifica√ß√£o das **coordenadas discriminantes**, tamb√©m conhecidas como **vari√°veis can√¥nicas** [^4.3.3]. Compararemos essa abordagem com a **regress√£o linear com matrizes de indicadores**, que n√£o utiliza o conceito de Rayleigh quociente para derivar a fronteira de decis√£o, e com a **regress√£o log√≠stica**, que utiliza a maximiza√ß√£o da verossimilhan√ßa [^4.2], [^4.4]. Abordaremos tamb√©m a import√¢ncia da **sele√ß√£o de vari√°veis e regulariza√ß√£o** para controlar a complexidade dos modelos e garantir a sua estabilidade [^4.4.4], [^4.5].  Exploraremos como a busca por **hiperplanos separadores** se conecta com o problema de otimiza√ß√£o do Rayleigh quociente [^4.5.2]. O objetivo deste cap√≠tulo √© fornecer uma compreens√£o detalhada de como a otimiza√ß√£o do Rayleigh quociente e a SVD generalizada s√£o utilizadas na constru√ß√£o de modelos de classifica√ß√£o linear.

### Conceitos Fundamentais

**Conceito 1: O Rayleigh Quociente e a Maximiza√ß√£o da Separabilidade**

O **Rayleigh quociente** √© uma fun√ß√£o que, quando aplicada a um vetor $a$, representa a raz√£o entre duas formas quadr√°ticas envolvendo matrizes sim√©tricas. No contexto de classifica√ß√£o linear, o Rayleigh quociente √© definido como:

$$
\frac{a^T B a}{a^T W a}
$$

onde $B$ √© a matriz de covari√¢ncia entre classes e $W$ √© a matriz de covari√¢ncia dentro das classes (pooled covariance).  A maximiza√ß√£o desse quociente busca encontrar as dire√ß√µes $a$ que maximizem a vari√¢ncia entre as m√©dias das classes, em rela√ß√£o √† vari√¢ncia dentro das classes, que √© a ideia central do Crit√©rio de Fisher.  A solu√ß√£o para esse problema de otimiza√ß√£o √© dada pelos autovetores correspondentes aos maiores autovalores da matriz $W^{-1}B$, e que definem os eixos que melhor separam as classes no espa√ßo de entrada [^4.3.3]. O Rayleigh quociente √© uma forma de quantificar a qualidade da proje√ß√£o.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos duas classes, com m√©dias $\mu_1 = [1, 1]^T$ e $\mu_2 = [3, 3]^T$.  A matriz de covari√¢ncia dentro das classes √© $W = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$. A matriz de covari√¢ncia entre classes, $B$, √© dada por $B = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T = \begin{bmatrix} 4 & 4 \\ 4 & 4 \end{bmatrix}$.
>
> Para um vetor arbitr√°rio $a = [1, 0]^T$, temos:
>
> $a^T B a = \begin{bmatrix} 1 & 0 \end{bmatrix} \begin{bmatrix} 4 & 4 \\ 4 & 4 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = 4$
>
> $a^T W a = \begin{bmatrix} 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = 1$
>
> O Rayleigh quociente √© $\frac{4}{1} = 4$.
>
> Agora, considere o vetor $a = [1, 1]^T$ (que est√° na dire√ß√£o da diferen√ßa entre as m√©dias das classes):
>
> $a^T B a = \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 4 & 4 \\ 4 & 4 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = 16$
>
> $a^T W a = \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = 2$
>
> O Rayleigh quociente √© $\frac{16}{2} = 8$.
>
> Este exemplo ilustra como o Rayleigh quociente varia de acordo com a dire√ß√£o de $a$. A maximiza√ß√£o busca o vetor $a$ que maximiza essa raz√£o, e esse vetor define a dire√ß√£o que melhor separa as classes.

```mermaid
graph LR
    subgraph "Rayleigh Quotient"
        direction TB
        A["Vector 'a'"]
        B["Between-Class Covariance Matrix 'B'"]
        C["Within-Class Covariance Matrix 'W'"]
        D["'a^T B a'"]
        E["'a^T W a'"]
        F["Rayleigh Quotient: (a^T B a) / (a^T W a)"]
        A --> D
        B --> D
        A --> E
        C --> E
        D & E --> F
    end
```

**Lemma 1:** *O Rayleigh quociente quantifica a raz√£o entre vari√¢ncia entre classes e a vari√¢ncia dentro das classes em um determinado subespa√ßo, e a sua maximiza√ß√£o busca a proje√ß√£o que maximize a separabilidade entre classes.* A prova deste lema se encontra nas condi√ß√µes de otimalidade do Rayleigh quociente.

**Conceito 2: SVD Generalizada e a Solu√ß√£o do Problema de Otimiza√ß√£o**

A **decomposi√ß√£o em valores singulares (SVD) generalizada** √© uma extens√£o da SVD que pode ser aplicada a pares de matrizes, mesmo que estas n√£o sejam quadr√°ticas. Essa t√©cnica √© utilizada para resolver o problema de otimiza√ß√£o do Rayleigh quociente, ao encontrar as dire√ß√µes $a$ que maximizam a raz√£o $\frac{a^T B a}{a^T W a}$.  A solu√ß√£o √© obtida atrav√©s da decomposi√ß√£o $W^{-1/2} B W^{-1/2} = V D V^T$, onde $V$ √© a matriz de autovetores e $D$ √© a matriz diagonal com autovalores.  Os autovetores $V$ correspondem √†s dire√ß√µes que maximizam o Rayleigh quociente, e essas dire√ß√µes definem os subespa√ßos discriminantes [^4.3.3].  Os autovalores da matriz $W^{-1}B$ representam as raz√µes de vari√¢ncia entre e intra-classes, e a proje√ß√£o das observa√ß√µes sobre os autovetores cria as coordenadas discriminantes.

> üí° **Exemplo Num√©rico:**
>
> Usando as matrizes $W$ e $B$ do exemplo anterior, calculamos $W^{-1} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$. Ent√£o, $W^{-1}B = \begin{bmatrix} 4 & 4 \\ 4 & 4 \end{bmatrix}$.
>
> A decomposi√ß√£o em autovalores e autovetores de $W^{-1}B$ nos d√°:
>
> Autovalores: $\lambda_1 = 8$, $\lambda_2 = 0$.
>
> Autovetores: $v_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$, $v_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} -1 \\ 1 \end{bmatrix}$.
>
> O autovetor $v_1$ corresponde ao maior autovalor e √© a dire√ß√£o que maximiza o Rayleigh quociente. A proje√ß√£o dos dados nessa dire√ß√£o criar√° as coordenadas discriminantes, que separam as classes o m√°ximo poss√≠vel. O autovalor $\lambda_1 = 8$ representa a raz√£o de vari√¢ncia entre e intra-classes nessa dire√ß√£o, demonstrando a efic√°cia da proje√ß√£o. O autovalor $\lambda_2 = 0$ indica que n√£o h√° separa√ß√£o das classes na dire√ß√£o de $v_2$.

```mermaid
graph LR
    subgraph "Generalized SVD Process"
        direction TB
        A["'B': Between-Class Covariance"]
        B["'W': Within-Class Covariance"]
        C["'W^(-1/2)'"]
        D["'W^(-1/2) B W^(-1/2)'"]
        E["SVD: 'W^(-1/2) B W^(-1/2) = V D V^T'"]
        F["'V': Eigenvectors (Discriminant Directions)"]
        G["'D': Eigenvalues (Variance Ratios)"]
        A --> D
        B --> C
        C --> D
        D --> E
        E --> F
        E --> G
    end
```

**Corol√°rio 1:** *A SVD generalizada oferece uma forma eficiente de encontrar os autovetores que maximizam o Rayleigh quociente, o que corresponde √† solu√ß√£o do problema de otimiza√ß√£o do Crit√©rio de Fisher, e esses autovetores definem os subespa√ßos discriminantes que ser√£o utilizados para a proje√ß√£o dos dados.* Este corol√°rio demonstra como a SVD generalizada fornece uma forma de resolver um problema de otimiza√ß√£o e definir os subespa√ßos discriminantes.

**Conceito 3:  Coordenadas Discriminantes e a Generaliza√ß√£o da SVD**

As **coordenadas discriminantes**, tamb√©m conhecidas como **vari√°veis can√¥nicas**, s√£o obtidas atrav√©s da proje√ß√£o dos dados nos autovetores obtidos pela SVD generalizada da matriz $W^{-1/2} B W^{-1/2}$ [^4.3.3]. Essas coordenadas representam um subespa√ßo de menor dimens√£o onde a separa√ß√£o entre as classes √© maximizada, em rela√ß√£o √† vari√¢ncia dentro das classes. O uso das coordenadas discriminantes simplifica o problema de classifica√ß√£o, pois o n√∫mero de dimens√µes √© reduzido, e as classes podem ser representadas de forma mais eficiente nesse subespa√ßo projetado.

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, suponha que temos um ponto de dados $x = [2, 2]^T$. Projetando esse ponto no autovetor $v_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$, obtemos a coordenada discriminante $z$:
>
> $z = v_1^T x = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \end{bmatrix} = \frac{4}{\sqrt{2}} = 2\sqrt{2} \approx 2.83$
>
> Esta coordenada discriminante $z$ representa a posi√ß√£o do ponto $x$ no subespa√ßo que maximiza a separa√ß√£o das classes. Ao projetar todos os pontos de dados nesse subespa√ßo, obtemos uma representa√ß√£o de menor dimens√£o onde as classes s√£o mais separ√°veis.

```mermaid
graph LR
    subgraph "Discriminant Coordinate Derivation"
        direction TB
        A["Input Data Point 'x'"]
        B["Eigenvector 'v' from SVD"]
        C["Discriminant Coordinate 'z = v^T x'"]
        A --> C
        B --> C
    end
```

> ‚ö†Ô∏è **Nota Importante**: A SVD generalizada oferece uma forma eficiente de obter os subespa√ßos discriminantes atrav√©s da resolu√ß√£o do problema de otimiza√ß√£o do Rayleigh quociente, e assim, se torna uma ferramenta fundamental para o LDA e outras abordagens de redu√ß√£o de dimensionalidade.

> ‚ùó **Ponto de Aten√ß√£o**:  A escolha do n√∫mero de autovetores (e consequentemente a dimensionalidade do subespa√ßo projetado) deve ser feita de forma a equilibrar a complexidade do modelo com a sua capacidade de generaliza√ß√£o.

> ‚úîÔ∏è **Destaque**: A SVD generalizada, aplicada ao problema de otimiza√ß√£o do Rayleigh quociente, permite encontrar subespa√ßos discriminantes que maximizam a separa√ß√£o entre as classes e que constituem uma forma de redu√ß√£o de dimensionalidade em problemas de classifica√ß√£o.

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
    subgraph "Comparison of Approaches"
        direction TB
        A["LDA (Rayleigh Quotient + SVD)"] --> B["Maximize Variance Ratio"]
        B --> C["Project onto Discriminant Subspace"]
        D["Linear Regression"] --> E["Minimize Sum of Squared Errors"]
        E --> F["Independent Fits for Each Class"]
    end
```

A **regress√£o linear com matrizes de indicadores**, ao contr√°rio do LDA que utiliza o Rayleigh quociente, n√£o busca maximizar a vari√¢ncia entre classes em rela√ß√£o √† vari√¢ncia dentro das classes [^4.2]. A regress√£o linear ajusta modelos para cada classe independentemente, sem considerar as rela√ß√µes entre as classes atrav√©s das suas covari√¢ncias e n√£o busca projetar os dados em subespa√ßos que maximizem a separabilidade das classes [^4.2]. A regress√£o linear, por consequ√™ncia, n√£o utiliza a SVD generalizada ou o conceito de Rayleigh quociente no processo de ajuste e de tomada de decis√£o.

A aus√™ncia do Crit√©rio de Fisher e do uso da SVD generalizada na regress√£o linear implica que o m√©todo n√£o se beneficia da estrutura de dados revelada pela decomposi√ß√£o de covari√¢ncias e proje√ß√£o dos dados nos subespa√ßos que maximizam a vari√¢ncia entre as classes.

Em contraste com o LDA, que utiliza o Crit√©rio de Fisher e a SVD generalizada para encontrar coordenadas discriminantes que separem as classes, a regress√£o linear com matrizes de indicadores busca diretamente uma fun√ß√£o que minimize a soma de quadrados dos erros, sem realizar nenhuma transforma√ß√£o baseada nas covari√¢ncias. A regress√£o linear, portanto,  pode n√£o ser adequada para problemas onde a separabilidade linear n√£o seja evidente e onde uma proje√ß√£o em um subespa√ßo adequado seja necess√°ria [^4.2].

> üí° **Exemplo Num√©rico:**
>
> Considere um problema de classifica√ß√£o com duas classes. Para a regress√£o linear, criamos uma matriz de indicadores $Y$ onde $y_i = 1$ se a observa√ß√£o $x_i$ pertence √† classe 1, e $y_i = 0$ se pertence √† classe 2.
>
> Digamos que temos as seguintes observa√ß√µes e classes:
>
> $x_1 = [1, 1]^T$, classe 1, $y_1 = 1$
>
> $x_2 = [2, 2]^T$, classe 1, $y_2 = 1$
>
> $x_3 = [4, 1]^T$, classe 2, $y_3 = 0$
>
> $x_4 = [5, 2]^T$, classe 2, $y_4 = 0$
>
> Usando regress√£o linear, ajustar√≠amos um modelo $\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2$ para prever $y$. O objetivo √© encontrar $\beta_0, \beta_1, \beta_2$ que minimizem a soma dos erros quadrados $(y_i - \hat{y}_i)^2$.
>
> ```python
> import numpy as np
> from sklearn.linear_model import LinearRegression
>
> X = np.array([[1, 1], [2, 2], [4, 1], [5, 2]])
> y = np.array([1, 1, 0, 0])
>
> model = LinearRegression()
> model.fit(X, y)
> print(f"Coeficientes: {model.coef_}")
> print(f"Intercepto: {model.intercept_}")
> ```
>
> Este c√≥digo ajusta um modelo de regress√£o linear que tenta prever a classe diretamente, sem considerar as covari√¢ncias das classes ou o Rayleigh quociente. Os coeficientes resultantes n√£o correspondem √†s dire√ß√µes que maximizam a separa√ß√£o entre as classes, como no LDA.

**Lemma 2:** *A regress√£o linear com matrizes de indicadores n√£o utiliza a otimiza√ß√£o do Rayleigh quociente para derivar coordenadas discriminantes e n√£o se beneficia da SVD generalizada para simplificar o problema de classifica√ß√£o, o que a distingue dos m√©todos que se baseiam nessas t√©cnicas.* A prova desse lema se encontra na formula√ß√£o da fun√ß√£o de custo da regress√£o linear e na deriva√ß√£o dos par√¢metros do modelo, que n√£o se baseiam no Rayleigh quociente.

**Corol√°rio 2:** *A falta de utiliza√ß√£o do Crit√©rio de Fisher na regress√£o linear com matrizes de indicadores pode limitar o seu desempenho em problemas onde a vari√¢ncia entre classes e intra-classes √© importante para a separa√ß√£o, e que se beneficiariam da escolha de subespa√ßos com melhor separabilidade, como √© feito atrav√©s do Crit√©rio de Fisher e a SVD generalizada.*  Este corol√°rio destaca as limita√ß√µes da regress√£o linear em contraste com abordagens que se baseiam em proje√ß√µes com base na estrutura de covari√¢ncia.

A regress√£o linear com matrizes de indicadores, embora simples de implementar e interpretar, n√£o utiliza o conceito de Rayleigh quociente e a SVD generalizada para a obten√ß√£o de coordenadas discriminantes, o que a torna menos eficiente em problemas onde a separabilidade das classes no espa√ßo original n√£o √© evidente e onde a proje√ß√£o em um subespa√ßo de menor dimens√£o √© fundamental. [^4.2], [^4.3.3]

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
    subgraph "Impact of Feature Selection and Regularization"
        direction TB
        A["Feature Space"]
        B["Feature Selection"] --> C["Reduced Feature Space"]
        D["Model Parameters"]
        E["Regularization"] --> F["Constrained Parameters"]
        A --> C
        D --> F
        C & F --> G["Simplified & Stable Model"]
    end
```

A **sele√ß√£o de vari√°veis** e a **regulariza√ß√£o** desempenham um papel importante na melhoria da estabilidade e interpretabilidade dos modelos lineares de classifica√ß√£o, mesmo quando se utiliza a maximiza√ß√£o do Rayleigh quociente para encontrar os subespa√ßos discriminantes.  A regulariza√ß√£o, ao adicionar um termo de penalidade √† fun√ß√£o de custo, restringe a magnitude dos coeficientes e pode tornar o c√°lculo do Rayleigh quociente mais est√°vel e menos suscet√≠vel ao overfitting [^4.5].

Na **regress√£o log√≠stica**, que modela as probabilidades posteriores atrav√©s de uma fun√ß√£o linear, a regulariza√ß√£o pode ser aplicada da seguinte forma:

$$
\max_{\beta_0, \beta} \left[ \sum_{i=1}^N \left( y_i (\beta_0 + \beta^T x_i) - \log(1 + e^{\beta_0 + \beta^T x_i}) \right) - \lambda P(\beta) \right]
$$

onde $P(\beta)$ √© a penalidade e $\lambda$ √© o par√¢metro de regulariza√ß√£o.  A penalidade **L1** (Lasso) √© dada por $P(\beta) = \sum_{j=1}^p |\beta_j|$, que leva √† esparsidade dos coeficientes, selecionando as vari√°veis mais relevantes para a modelagem do log-odds e da separa√ß√£o das classes [^4.4.4].  A penalidade **L2** (Ridge), dada por $P(\beta) = \sum_{j=1}^p \beta_j^2$, reduz a magnitude dos coeficientes, estabilizando o modelo [^4.5].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um problema de classifica√ß√£o bin√°ria com regress√£o log√≠stica. Ajustamos um modelo com e sem regulariza√ß√£o L1 (Lasso).
>
> Digamos que temos os seguintes dados e classes:
>
> $x_1 = [1, 2, 3, 4]^T$, classe 1, $y_1 = 1$
>
> $x_2 = [2, 3, 4, 5]^T$, classe 1, $y_2 = 1$
>
> $x_3 = [5, 4, 3, 2]^T$, classe 0, $y_3 = 0$
>
> $x_4 = [6, 5, 4, 3]^T$, classe 0, $y_4 = 0$
>
> Sem regulariza√ß√£o:
>
> ```python
> import numpy as np
> from sklearn.linear_model import LogisticRegression
>
> X = np.array([[1, 2, 3, 4], [2, 3, 4, 5], [5, 4, 3, 2], [6, 5, 4, 3]])
> y = np.array([1, 1, 0, 0])
>
> model = LogisticRegression(penalty=None)
> model.fit(X, y)
> print(f"Coeficientes (Sem Reg.): {model.coef_}")
> ```
>
> Com regulariza√ß√£o L1 (Lasso) com $\lambda = 0.1$:
>
> ```python
> model_lasso = LogisticRegression(penalty='l1', C=1/0.1, solver='liblinear')
> model_lasso.fit(X, y)
> print(f"Coeficientes (Lasso): {model_lasso.coef_}")
> ```
>
> Observamos que a regulariza√ß√£o L1 leva a coeficientes esparsos, ou seja, alguns coeficientes s√£o exatamente zero, indicando que algumas vari√°veis foram consideradas menos relevantes para o modelo. A escolha de $\lambda$ (ou $C$ que √© o inverso de $\lambda$ no sklearn) influencia a quantidade de regulariza√ß√£o e a esparsidade dos coeficientes.

A combina√ß√£o da otimiza√ß√£o do Rayleigh quociente com t√©cnicas de regulariza√ß√£o permite a constru√ß√£o de modelos de classifica√ß√£o que s√£o simultaneamente eficientes em termos computacionais e robustos em rela√ß√£o a problemas de *overfitting*, principalmente quando as rela√ß√µes entre classes s√£o mais complexas.

**Lemma 3:** *A regulariza√ß√£o L1, ao induzir a esparsidade dos coeficientes, simplifica a representa√ß√£o das coordenadas discriminantes e melhora a capacidade de generaliza√ß√£o do modelo, resultando em estimativas mais precisas das dire√ß√µes que maximizam a separa√ß√£o entre as classes.*  A demonstra√ß√£o desse lema reside na forma da penalidade L1 e como ela afeta a estrutura do modelo.

**Prova do Lemma 3:** A penalidade L1 for√ßa alguns coeficientes a se tornarem exatamente zero durante a minimiza√ß√£o do custo, o que leva √† sele√ß√£o de vari√°veis e √† redu√ß√£o da dimensionalidade do modelo. Esta propriedade da penalidade L1 √© fundamental para a simplifica√ß√£o e estabilidade do modelo [^4.4.3], [^4.4.4]. $\blacksquare$

**Corol√°rio 3:** *A sele√ß√£o de vari√°veis e a regulariza√ß√£o, ao controlarem a complexidade do modelo, melhoram a estabilidade e a capacidade de generaliza√ß√£o dos modelos de classifica√ß√£o que utilizam proje√ß√µes em subespa√ßos discriminantes, maximizando a raz√£o de Rayleigh, e com isso, tornam a estima√ß√£o dos par√¢metros mais robusta e menos suscet√≠vel a ru√≠do nos dados.* O controle da complexidade impacta na capacidade de generaliza√ß√£o do modelo, e torna os seus resultados mais robustos.

> ‚ö†Ô∏è **Ponto Crucial**: A sele√ß√£o de vari√°veis e a regulariza√ß√£o complementam a otimiza√ß√£o do Rayleigh quociente e a aplica√ß√£o da SVD, e a combina√ß√£o dessas t√©cnicas permite a constru√ß√£o de modelos de classifica√ß√£o mais eficientes e com maior capacidade de generaliza√ß√£o [^4.5].

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
    subgraph "Hyperplane and Subspace"
        direction TB
        A["Original Data Space"] --> B["SVD-based Discriminant Subspace"]
        C["Search for Separating Hyperplane"]
         B --> C
        D["Optimal Hyperplane in Original Space"]
        C --> D
        E["Perceptron Iterative Algorithm"]
        B & C --> E
    end
```

A busca por **hiperplanos separadores** visa encontrar uma fronteira linear que maximize a separa√ß√£o entre as classes, e essa busca pode ser simplificada pela transforma√ß√£o dos dados em um subespa√ßo de menor dimens√£o, onde a separabilidade seja mais evidente. A SVD generalizada e a otimiza√ß√£o do Rayleigh quociente buscam encontrar os subespa√ßos que maximizem a separa√ß√£o entre as classes, utilizando informa√ß√µes sobre as m√©dias e as covari√¢ncias dos dados, e consequentemente auxiliando na busca pelo melhor hiperplano separador [^4.5.2], [^4.3.3].

O algoritmo do **Perceptron** busca um hiperplano separador de forma iterativa, ajustando os par√¢metros do modelo com base nas classifica√ß√µes incorretas [^4.5.1].  A aplica√ß√£o do Perceptron no subespa√ßo projetado atrav√©s das coordenadas discriminantes pode tornar o processo de converg√™ncia mais r√°pido e encontrar um hiperplano que separe as classes de maneira mais eficiente, especialmente quando os dados s√£o de alta dimens√£o.  Apesar da simplicidade do Perceptron, a sua combina√ß√£o com proje√ß√µes via o Crit√©rio de Fisher pode resultar em um modelo com boa performance.

> üí° **Exemplo Num√©rico:**
>
> Considere um problema de classifica√ß√£o com dois atributos e duas classes. Aplicamos o Perceptron nos dados originais e nos dados projetados pelo LDA.
>
> Dados Originais:
>
> $x_1 = [1, 1]^T$, classe 1
>
> $x_2 = [2, 1.5]^T$, classe 1
>
> $x_3 = [4, 3]^T$, classe 2
>
> $x_4 = [5, 3.5]^T$, classe 2
>
> Usando LDA, projetamos os dados em um subespa√ßo de uma dimens√£o (coordenada discriminante). Suponha que a proje√ß√£o resulta em:
>
> $z_1 = 1.2$ (correspondente a $x_1$)
>
> $z_2 = 1.8$ (correspondente a $x_2$)
>
> $z_3 = 4.5$ (correspondente a $x_3$)
>
> $z_4 = 5.2$ (correspondente a $x_4$)
>
> Aplicar o Perceptron nos dados originais envolve ajustar um hiperplano (uma linha neste caso) que separa as classes. Aplicar o Perceptron nos dados projetados envolve encontrar um ponto que separa as classes na dimens√£o projetada. O Perceptron no espa√ßo projetado converge para uma solu√ß√£o mais rapidamente devido √† redu√ß√£o da dimensionalidade e melhor separabilidade das classes.

**Teorema:** *A proje√ß√£o dos dados em um subespa√ßo discriminante, obtido atrav√©s da otimiza√ß√£o do Rayleigh quociente e a SVD generalizada, simplifica a busca por hiperplanos separadores, e a aplica√ß√£o do Perceptron no espa√ßo projetado pode acelerar a converg√™ncia para uma solu√ß√£o separadora, em problemas que s√£o linearmente separ√°veis.*  A combina√ß√£o das t√©cnicas de proje√ß√£o com a busca por hiperplanos pode levar a resultados melhores e mais robustos [^4.5.1].

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

**Resposta:**

A **Regra de Decis√£o Bayesiana** busca classificar uma observa√ß√£o $x$ na classe $k$ que maximize a probabilidade posterior $P(G=k|X=x)$ [^4.3]. Sob a suposi√ß√£o de que as classes seguem distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia $\Sigma$, a probabilidade posterior √© dada por:

$$
P(G=k|X=x) = \frac{ \phi(x;\mu_k,\Sigma)\pi_k}{\sum_{l=1}^K \phi(x;\mu_l,\Sigma)\pi_l}
$$

onde $\phi(x;\mu_k,\Sigma)$ √© a fun√ß√£o densidade gaussiana para a classe $k$, $\mu_k$ √© a m√©dia da classe $k$ e $\pi_k$ √© a probabilidade a priori da classe. O **LDA**, por sua vez, deriva suas fun√ß√µes discriminantes lineares diretamente dessas suposi√ß√µes, buscando maximizar a separa√ß√£o entre as classes, e o Crit√©rio de Fisher e a SVD generalizada podem ser utilizadas para obter as coordenadas discriminantes que simplificam o problema de classifica√ß√£o [^4.3], [^4.3.3].

```mermaid
graph LR
    subgraph "Bayesian Decision and LDA"
        direction TB
        A["Bayesian Decision: Maximize P(G=k|X=x)"]
         B["Gaussian Class-Conditional Densities"]
         C["LDA: Maximize Separability"]
         D["Equal Covariance Assumption"]
         A --> B
         B --> D
        C --> D
    end
```

**Lemma 4:** *Sob a suposi√ß√£o de distribui√ß√µes Gaussianas com a mesma matriz de covari√¢ncia, a regra de decis√£o Bayesiana e o LDA levam √† mesma fronteira de decis√£o linear, e que a otimiza√ß√£o do Rayleigh quociente e a aplica√ß√£o da SVD generalizada n√£o alteram a solu√ß√£o, mas simplificam a sua obten√ß√£o.* Esta equival√™ncia √© obtida mostrando que o log-ratio das probabilidades posteriores leva √† mesma forma da fun√ß√£o discriminante do LDA. [^4.3]

**Corol√°rio 4:** *Ao remover a restri√ß√£o de igualdade de covari√¢ncias na regra de decis√£o Bayesiana, obt√©m-se o QDA, onde a busca por uma solu√ß√£o √≥tima envolve a estima√ß√£o de matrizes de covari√¢ncia distintas para cada classe, e a aplica√ß√£o do Crit√©rio de Fisher n√£o √© diretamente utilizada, j√° que ele pressup√µe uma covari√¢ncia √∫nica.* A complexidade da decis√£o aumenta na aus√™ncia da suposi√ß√£o de covari√¢ncia comum [^4.3.1], [^4.3.3].

> ‚ö†Ô∏è **Ponto Crucial**:  A principal diferen√ßa entre LDA e a regra de decis√£o Bayesiana √© que o LDA imp√µe a restri√ß√£o da igualdade de covari√¢ncias e utiliza o Rayleigh quociente para otimizar a separa√ß√£o das classes, enquanto que sob a mesma restri√ß√£o, a regra Bayesiana se resume ao mesmo modelo [^4.3].

### Conclus√£o

Neste cap√≠tulo, exploramos o conceito de Rayleigh quociente e sua rela√ß√£o com a maximiza√ß√£o da vari√¢ncia e a separa√ß√£o entre as classes. Analisamos como a SVD generalizada pode ser utilizada para resolver o problema de otimiza√ß√£o do Rayleigh quociente, e como essa t√©cnica se conecta com a constru√ß√£o de modelos de classifica√ß√£o linear e a obten√ß√£o de coordenadas discriminantes. Discutimos como a regress√£o linear com matrizes de indicadores n√£o utiliza o conceito de Rayleigh quociente e como a sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o fundamentais para melhorar a estabilidade e a interpretabilidade dos modelos.  Exploramos a busca por hiperplanos separadores no contexto da otimiza√ß√£o do Rayleigh quociente. Ao longo deste cap√≠tulo, procuramos fornecer uma vis√£o clara de como a otimiza√ß√£o do Rayleigh quociente e a utiliza√ß√£o da SVD generalizada podem ser utilizadas para construir modelos de classifica√ß√£o linear mais eficientes e robustos.

### Footnotes

[^4.1]: *In this chapter we revisit the classification problem and focus on linear methods for classification...There are several different ways in which linear decision boundaries can be found.* *(Trecho de Linear Methods for Classification)*

[^4.2]: *In Chapter 2 we fit linear regression models to the class indicator variables, and classify to the largest fit...Linear inequalities in this space are quadratic inequalities in the original space.* *(Trecho de Linear Methods for Classification)*

[^4.3]: *Decision theory for classification (Section 2.4) tells us that we need to know the class posteriors Pr(G|X) for optimal classification. Suppose fk(x) is the class-conditional density of X in class G = k, and let œÄŒ∫ be the prior probability of class k... Linear discriminant analysis (LDA) arises in the special case when we assume that the classes have a common covariance matrix Œ£k = Œ£.* *(Trecho de Linear Methods for Classification)*

[^4.3.1]: *The decision boundary between each pair of classes k and l is described by a quadratic equation {x: Œ¥Œ∫(x) = Œ¥(x)}.* *(Trecho de Linear Methods for Classification)*

[^4.3.2]: *The estimates for QDA are similar to those for LDA, except that separate covariance matrices must be estimated for each class...Their computations are simplified by diagonalizing ‚àë or √âk.* *(Trecho de Linear Methods for Classification)*

[^4.3.3]: *In the special case when we assume that the classes have a common covariance matrix...When the classes are really Gaussian, then LDA is optimal... Finding the sequences of optimal subspaces for LDA involves the following steps:* *(Trecho de Linear Methods for Classification)*

[^4.4]: *The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0,1].* *(Trecho de Linear Methods for Classification)*

[^4.4.1]: *Logistic regression models are usually fit by maximum likelihood... The logistic regression model is more general, in that it makes less assumptions.* *(Trecho de Linear Methods for Classification)*

[^4.4.2]: *It is convenient to code the two-class gi via a 0/1 response Yi, where yi = 1 when gi = 1, and yi = 0 when gi = 2... Typically many models are fit in a search for a parsimonious model involving a subset of the variables.* *(Trecho de Linear Methods for Classification)*

[^4.4.3]: *To maximize the log-likelihood, we set its derivatives to zero. These score equations are...To solve the score equations (4.21), we use the Newton-Raphson algorithm...* *(Trecho de Linear Methods for Classification)*

[^4.4.4]: *The L1 penalty used in the lasso (Section 3.4.2) can be used for variable selection and shrinkage with any linear regression model...As with the lasso, we typically do not penalize the intercept term.* *(Trecho de Linear Methods for Classification)*

[^4.5]: *Here we present an analysis of binary data to illustrate the traditional statistical use of the logistic regression model... With two classes there is a simple correspondence between linear discriminant analysis and classification by linear least squares, as in (4.5).* *(Trecho de Linear Methods for Classification)*

[^4.5.1]: *The perceptron learning algorithm tries to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary.* *(Trecho de Linear Methods for Classification)*

[^4.5.2]: *The optimal separating hyperplane separates the two classes and maximizes the distance to the closest point from either class... In light of (4.40), the constraints define an empty slab or margin around the linear decision boundary...* *(Trecho de Linear Methods for Classification)*
