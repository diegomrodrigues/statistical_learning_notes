## Basis Expansions in a One-Dimensional Setting: A Detailed Examination

```mermaid
graph LR
    subgraph "Basis Expansion in 1D"
        direction LR
        A["Input Variable 'X'"] --> B["Basis Functions 'h_m(X)'"]
        B --> C["Transformed Features 'h_1(X), h_2(X), ...'"]
        C --> D["Linear Model 'f(X) = Œ£ Œ≤_m h_m(X)'"]
        style A fill:#f9f,stroke:#333,stroke-width:2px
        style D fill:#ccf,stroke:#333,stroke-width:2px
    end
    
```

### Introdu√ß√£o

A t√©cnica de *basis expansions*, como discutido anteriormente, √© fundamental para estender a capacidade de modelos lineares, permitindo-lhes capturar rela√ß√µes n√£o lineares nos dados. Este cap√≠tulo se aprofunda no uso de *basis expansions* em um cen√°rio **unidimensional**, examinando como as features de entrada s√£o transformadas por meio de fun√ß√µes de base e como os modelos lineares s√£o aplicados nesse espa√ßo de representa√ß√£o. Ao focar em uma dimens√£o, torna-se mais f√°cil visualizar os efeitos de diferentes fun√ß√µes de base e os *tradeoffs* envolvidos na constru√ß√£o de modelos mais flex√≠veis, sem a complexidade adicionada por m√∫ltiplas dimens√µes.

A explora√ß√£o do cen√°rio unidimensional oferece uma compreens√£o fundamental do funcionamento das *basis expansions*, servindo como base para a aplica√ß√£o dessa t√©cnica em problemas de maior dimens√£o.

###  Fundamentos da Expans√£o de Bases em Uma Dimens√£o

Em um cen√°rio unidimensional, o vetor de entrada $X$ √© composto por uma √∫nica vari√°vel, $X$, que pode ser uma feature quantitativa ou uma vari√°vel categ√≥rica codificada numericamente. O objetivo das *basis expansions* √© transformar essa √∫nica feature em um novo conjunto de features $h_m(X)$ que capturam diferentes aspectos dos dados, permitindo que o modelo linear aprenda rela√ß√µes n√£o lineares com a vari√°vel de resposta.

O modelo linear resultante ap√≥s a expans√£o de bases √© dado por:

$$
f(X) = \sum_{m=1}^{M} \beta_m h_m(X)
$$

onde $h_m(X)$ s√£o as fun√ß√µes de base que transformam $X$, $M$ √© o n√∫mero de fun√ß√µes de base, e $\beta_m$ s√£o os coeficientes do modelo. No contexto unidimensional, as fun√ß√µes de base $h_m(X)$ podem assumir diversas formas:

*   **Fun√ß√µes Polinomiais:** Podem incluir termos como $X, X^2, X^3, \ldots X^d$, onde $d$ √© o grau do polin√¥mio. Esses termos permitem modelar curvas e rela√ß√µes n√£o lineares suaves.
*   **Fun√ß√µes Logar√≠tmicas e Raiz Quadrada:** Podem incluir termos como $\log(X)$ ou $\sqrt{X}$, √∫teis para modelar dados com crescimento ou decaimento n√£o linear.
*   **Fun√ß√µes Indicadoras (Regi√µes):** Dividem o espa√ßo da feature $X$ em regi√µes e atribuem um valor constante para cada regi√£o. Por exemplo, $h_m(X) = I(L_m \leq X < U_m)$, onde $I$ √© uma fun√ß√£o indicadora que retorna 1 se $X$ est√° na regi√£o definida por $L_m$ e $U_m$, e 0 caso contr√°rio.
*   **Splines:** Permitem que o modelo capture varia√ß√µes locais nos dados atrav√©s de fun√ß√µes piecewise polinomiais. Splines s√£o definidos por n√≥s e podem ter diferentes graus de suavidade.
*   **Wavelets:** Permitem decompor o sinal em diferentes frequ√™ncias e escalas, capturando tanto aspectos globais quanto detalhes locais do sinal.

A escolha das fun√ß√µes de base √© crucial, pois determina o tipo de n√£o linearidade que o modelo pode capturar e o grau de complexidade do modelo.

```mermaid
graph LR
    subgraph "Types of Basis Functions"
        direction TB
        A["Polynomial: X, X^2, X^3, ..."]
        B["Logarithmic/Square Root: log(X), sqrt(X)"]
        C["Indicator Functions: I(L_m <= X < U_m)"]
        D["Splines: Piecewise Polynomials"]
        E["Wavelets: Multi-Scale Decomposition"]
    end

```

### Exemplos de Fun√ß√µes de Base Unidimensionais

1. **Modelos Polinomiais:** Utilizar polin√¥mios como base √© uma forma comum de introduzir n√£o linearidade no modelo, e pode ser aplicada a uma √∫nica feature. Por exemplo, um modelo quadr√°tico usa as fun√ß√µes de base $h_1(X) = X$ e $h_2(X) = X^2$, e √© definido como:

$$f(X) = \beta_0 + \beta_1 X + \beta_2 X^2 $$
   Modelos polinomiais de maior grau podem ser utilizados para aproximar fun√ß√µes mais complexas, mas tamb√©m aumentam a complexidade e o risco de *overfitting*.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um conjunto de dados com uma √∫nica vari√°vel de entrada $X$ e uma vari√°vel de resposta $Y$. Suponha que temos os seguintes dados:
>
> | X    | Y    |
> | ---- | ---- |
> | 1    | 2.5  |
> | 2    | 5.8  |
> | 3    | 10.2 |
> | 4    | 16.1 |
> | 5    | 24.0 |
>
> Para modelar esses dados com um modelo quadr√°tico, usaremos as fun√ß√µes de base $h_1(X) = X$ e $h_2(X) = X^2$. A matriz de design $H$ (ap√≥s adicionar um intercepto) e o vetor de resposta $Y$ ser√£o:
>
> $$
> H = \begin{bmatrix}
> 1 & 1 & 1 \\
> 1 & 2 & 4 \\
> 1 & 3 & 9 \\
> 1 & 4 & 16 \\
> 1 & 5 & 25
> \end{bmatrix}, \quad
> Y = \begin{bmatrix}
> 2.5 \\ 5.8 \\ 10.2 \\ 16.1 \\ 24.0
> \end{bmatrix}
> $$
>
> Usando o m√©todo dos m√≠nimos quadrados, podemos estimar os coeficientes $\beta = (H^T H)^{-1} H^T Y$:
>
> ```python
> import numpy as np
>
> X = np.array([1, 2, 3, 4, 5])
> Y = np.array([2.5, 5.8, 10.2, 16.1, 24.0])
>
> # Construindo a matriz de design H
> H = np.column_stack((np.ones(len(X)), X, X**2))
>
> # Calculando os coeficientes beta usando a f√≥rmula dos m√≠nimos quadrados
> beta = np.linalg.inv(H.T @ H) @ H.T @ Y
>
> print("Coeficientes Beta:", beta)
> ```
>
> Isso nos dar√° os coeficientes $\beta_0$, $\beta_1$ e $\beta_2$. Por exemplo, podemos obter $\beta \approx [0.15, 0.05, 0.96]$, que corresponde aproximadamente ao modelo $f(X) = 0.15 + 0.05X + 0.96X^2$. O modelo captura a rela√ß√£o n√£o linear entre X e Y.

```mermaid
graph LR
    subgraph "Quadratic Model"
    direction LR
    A["Input 'X'"] --> B["Basis Functions: h1(X) = X, h2(X) = X^2"]
    B --> C["Design Matrix 'H'"]
    C --> D["Response Vector 'Y'"]
    D --> E["Coefficient Estimation 'Œ≤ = (H^T H)^-1 H^T Y'"]
    E --> F["Quadratic Model: f(X) = Œ≤_0 + Œ≤_1X + Œ≤_2X^2"]
    end
```

2.  **Fun√ß√µes Logar√≠tmicas e de Raiz Quadrada:** Transforma√ß√µes n√£o lineares como $\log(X)$ e $\sqrt{X}$ podem ser usadas quando h√° uma rela√ß√£o n√£o linear particular comumente encontrada em dados do mundo real. Essas transforma√ß√µes podem ajudar a linearizar a rela√ß√£o entre a vari√°vel de resposta e a feature de entrada, permitindo que o modelo capture rela√ß√µes n√£o lineares. Por exemplo, em um modelo com $h_1(X) = \log(X)$, a fun√ß√£o √© definida por:
   $$f(X) = \beta_0 + \beta_1 \log(X)$$

> üí° **Exemplo Num√©rico:**
> Suponha que temos os seguintes dados, onde a rela√ß√£o entre $X$ e $Y$ parece ser logar√≠tmica:
>
> | X    | Y    |
> | ---- | ---- |
> | 1    | 1.0  |
> | 2    | 1.7  |
> | 3    | 2.1  |
> | 4    | 2.4  |
> | 5    | 2.6  |
>
> Para modelar esses dados, usaremos a fun√ß√£o de base $h_1(X) = \log(X)$.  A matriz de design $H$ (ap√≥s adicionar um intercepto) e o vetor de resposta $Y$ ser√£o:
>
> $$
> H = \begin{bmatrix}
> 1 & \log(1) \\
> 1 & \log(2) \\
> 1 & \log(3) \\
> 1 & \log(4) \\
> 1 & \log(5)
> \end{bmatrix}, \quad
> Y = \begin{bmatrix}
> 1.0 \\ 1.7 \\ 2.1 \\ 2.4 \\ 2.6
> \end{bmatrix}
> $$
>
> Calculamos os coeficientes $\beta$ como antes.
>
> ```python
> import numpy as np
>
> X = np.array([1, 2, 3, 4, 5])
> Y = np.array([1.0, 1.7, 2.1, 2.4, 2.6])
>
> # Construindo a matriz de design H com log(X)
> H = np.column_stack((np.ones(len(X)), np.log(X)))
>
> # Calculando os coeficientes beta
> beta = np.linalg.inv(H.T @ H) @ H.T @ Y
>
> print("Coeficientes Beta:", beta)
> ```
>
> O resultado, por exemplo, poderia ser $\beta \approx [0.9, 0.97]$, ent√£o o modelo seria $f(X) = 0.9 + 0.97 \log(X)$.

```mermaid
graph LR
 subgraph "Logarithmic Basis Function"
 direction LR
 A["Input 'X'"] --> B["Basis Function: h1(X) = log(X)"]
 B --> C["Design Matrix 'H'"]
 C --> D["Response Vector 'Y'"]
 D --> E["Coefficient Estimation 'Œ≤ = (H^T H)^-1 H^T Y'"]
 E --> F["Logarithmic Model: f(X) = Œ≤_0 + Œ≤_1 log(X)"]
 end
```

3.  **Fun√ß√µes Indicadoras:** Fun√ß√µes indicadoras de regi√£o permitem dividir a feature $X$ em intervalos, onde cada intervalo √© representado por uma constante. Isso permite modelar varia√ß√µes bruscas na rela√ß√£o entre $X$ e a vari√°vel resposta. Por exemplo, com $h_1(X) = I(X < \xi_1)$, $h_2(X) = I(\xi_1 \leq X < \xi_2)$ e $h_3(X) = I(X \geq \xi_2)$, o modelo se torna:
$$ f(X) = \beta_1 I(X < \xi_1) + \beta_2 I(\xi_1 \leq X < \xi_2) + \beta_3 I(X \geq \xi_2) $$
   onde $\xi_1$ e $\xi_2$ s√£o os pontos de corte que definem as regi√µes.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um conjunto de dados onde a vari√°vel de resposta $Y$ muda em diferentes intervalos de $X$. Suponha que temos:
>
> | X    | Y    |
> | ---- | ---- |
> | 1    | 2    |
> | 2    | 2    |
> | 3    | 5    |
> | 4    | 5    |
> | 5    | 8    |
> | 6    | 8    |
>
> Usaremos fun√ß√µes indicadoras com $\xi_1 = 3$ e $\xi_2 = 5$. As fun√ß√µes de base s√£o:
>  $h_1(X) = I(X < 3)$, $h_2(X) = I(3 \leq X < 5)$, e $h_3(X) = I(X \geq 5)$. A matriz de design $H$ e o vetor de resposta $Y$ ser√£o:
>
> $$
> H = \begin{bmatrix}
> 1 & 0 & 0 \\
> 1 & 0 & 0 \\
> 0 & 1 & 0 \\
> 0 & 1 & 0 \\
> 0 & 0 & 1 \\
> 0 & 0 & 1
> \end{bmatrix}, \quad
> Y = \begin{bmatrix}
> 2 \\ 2 \\ 5 \\ 5 \\ 8 \\ 8
> \end{bmatrix}
> $$
>
> ```python
> import numpy as np
>
> X = np.array([1, 2, 3, 4, 5, 6])
> Y = np.array([2, 2, 5, 5, 8, 8])
>
> # Definindo os pontos de corte
> xi1 = 3
> xi2 = 5
>
> # Criando as fun√ß√µes indicadoras
> h1 = (X < xi1).astype(int)
> h2 = ((X >= xi1) & (X < xi2)).astype(int)
> h3 = (X >= xi2).astype(int)
>
> # Construindo a matriz de design H
> H = np.column_stack((h1, h2, h3))
>
> # Calculando os coeficientes beta
> beta = np.linalg.inv(H.T @ H) @ H.T @ Y
>
> print("Coeficientes Beta:", beta)
> ```
>
> Os coeficientes $\beta$ nos dar√£o os valores m√©dios de $Y$ em cada regi√£o. Por exemplo, $\beta \approx [2, 5, 8]$ corresponde ao modelo $f(X) = 2I(X < 3) + 5I(3 \leq X < 5) + 8I(X \geq 5)$.

```mermaid
graph LR
    subgraph "Indicator Basis Functions"
        direction LR
        A["Input 'X'"] --> B["Indicator Functions: h1, h2, h3"]
        B --> C["Region Definition: Œæ_1, Œæ_2"]
        C --> D["Design Matrix 'H'"]
        D --> E["Response Vector 'Y'"]
        E --> F["Coefficient Estimation 'Œ≤ = (H^T H)^-1 H^T Y'"]
        F --> G["Model: f(X) = Œ≤_1 I(X < Œæ_1) + Œ≤_2 I(Œæ_1 ‚â§ X < Œæ_2) + Œ≤_3 I(X ‚â• Œæ_2)"]
        style B fill:#afa,stroke:#333,stroke-width:2px
        style G fill:#ccf,stroke:#333,stroke-width:2px
    end
```

4. **Splines:** Splines s√£o fun√ß√µes piecewise polinomiais que s√£o suaves em pontos de corte pr√©-definidos (n√≥s). Elas s√£o usadas para modelos onde h√° a necessidade de capturar rela√ß√µes n√£o lineares locais, ao inv√©s de rela√ß√µes polinomiais globais [^5.2]. Por exemplo, um spline c√∫bico com n√≥s em $\xi_1$ e $\xi_2$ pode ser representado por fun√ß√µes como $h_1(X) = 1, h_2(X) = X, h_3(X) = (X-\xi_1)^3_+ , h_4(X) = (X - \xi_2)^3_+$.

   Este conjunto de fun√ß√µes cria um modelo que se torna:

  $$ f(X) = \beta_0 + \beta_1 X + \beta_2 (X-\xi_1)^3_+ + \beta_3 (X - \xi_2)^3_+ $$
onde $(X - \xi_i)^3_+$ √© a fun√ß√£o $(X - \xi_i)^3$ se $X > \xi_i$ ou 0 caso contr√°rio. Splines, portanto, permitem flexibilidade local na modelagem.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um conjunto de dados e usar splines c√∫bicos com n√≥s em $\xi_1 = 3$ e $\xi_2 = 6$.
>
> | X    | Y    |
> | ---- | ---- |
> | 1    | 2.1  |
> | 2    | 3.5  |
> | 3    | 5.2  |
> | 4    | 7.1  |
> | 5    | 8.8  |
> | 6    | 10.2 |
> | 7    | 11.1 |
> | 8    | 11.9 |
>
> As fun√ß√µes de base ser√£o $h_1(X) = 1$, $h_2(X) = X$, $h_3(X) = (X-3)^3_+$ e $h_4(X) = (X-6)^3_+$.
>
> ```python
> import numpy as np
>
> X = np.array([1, 2, 3, 4, 5, 6, 7, 8])
> Y = np.array([2.1, 3.5, 5.2, 7.1, 8.8, 10.2, 11.1, 11.9])
>
> # Definindo os n√≥s
> xi1 = 3
> xi2 = 6
>
> # Criando as fun√ß√µes de base spline
> def cubic_spline_basis(x, knot):
>     return np.maximum(x - knot, 0)**3
>
> h1 = np.ones(len(X))
> h2 = X
> h3 = cubic_spline_basis(X, xi1)
> h4 = cubic_spline_basis(X, xi2)
>
> # Construindo a matriz de design H
> H = np.column_stack((h1, h2, h3, h4))
>
> # Calculando os coeficientes beta
> beta = np.linalg.inv(H.T @ H) @ H.T @ Y
>
> print("Coeficientes Beta:", beta)
> ```
>
> O resultado, por exemplo, poderia ser $\beta \approx [1.2, 1.1, 0.08, 0.03]$, ent√£o o modelo seria $f(X) = 1.2 + 1.1X + 0.08(X-3)^3_+ + 0.03(X-6)^3_+$.

```mermaid
graph LR
    subgraph "Cubic Spline Basis Functions"
    direction LR
    A["Input 'X'"] --> B["Knots: Œæ_1, Œæ_2"]
    B --> C["Basis Functions: h1(X)=1, h2(X)=X, h3(X)=(X-Œæ_1)^3_+, h4(X)=(X-Œæ_2)^3_+"]
    C --> D["Design Matrix 'H'"]
    D --> E["Response Vector 'Y'"]
    E --> F["Coefficient Estimation 'Œ≤ = (H^T H)^-1 H^T Y'"]
    F --> G["Cubic Spline Model: f(X) = Œ≤_0 + Œ≤_1X + Œ≤_2(X-Œæ_1)^3_+ + Œ≤_3(X-Œæ_2)^3_+"]
    style B fill:#afa,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
    end
```

5. **Wavelets:** S√£o transforma√ß√µes que permitem decompor a feature $X$ em diferentes frequ√™ncias e escalas, usando fun√ß√µes de base que s√£o definidas em diferentes escalas e posi√ß√µes. Em uma dimens√£o, elas podem ser vistas como uma cole√ß√£o de fun√ß√µes que capturam detalhes em diferentes n√≠veis de resolu√ß√£o.

    A escolha das fun√ß√µes de base depende das caracter√≠sticas do problema, do tipo de n√£o linearidade esperada nos dados e da complexidade desejada para o modelo.

### Controle da Complexidade e Regulariza√ß√£o em 1D

Em um cen√°rio unidimensional, √© ainda mais evidente a necessidade de controlar a complexidade do modelo para evitar *overfitting*. Isso pode ser feito usando t√©cnicas como:

1.  **Restri√ß√£o do N√∫mero de Fun√ß√µes de Base:** Limitar o n√∫mero de fun√ß√µes de base $h_m(X)$ utilizadas no modelo. Isso pode ser feito atrav√©s da sele√ß√£o de um subconjunto de fun√ß√µes de base mais relevantes, ou da restri√ß√£o do grau de polin√¥mios ou do n√∫mero de n√≥s em splines.

2.  **Regulariza√ß√£o:** Adicionar um termo de penalidade √† fun√ß√£o de custo, o que evita o *overfitting* atrav√©s da redu√ß√£o do tamanho dos coeficientes do modelo. As penalidades $L_1$ e $L_2$, discutidas em cap√≠tulos anteriores, podem ser aplicadas a modelos unidimensionais. A penalidade $L_1$ induz esparsidade, selecionando as fun√ß√µes de base mais relevantes, enquanto a penalidade $L_2$ reduz a magnitude dos coeficientes e, portanto, controla a complexidade.

> üí° **Exemplo Num√©rico:**
> Vamos usar o exemplo do modelo polinomial quadr√°tico anterior com regulariza√ß√£o L2 (Ridge).  Vamos adicionar a penalidade L2 ao nosso modelo com $\lambda = 0.1$.
>
> ```python
> import numpy as np
> from sklearn.linear_model import Ridge
>
> X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
> Y = np.array([2.5, 5.8, 10.2, 16.1, 24.0])
>
> # Construindo a matriz de design H para o modelo quadr√°tico
> H = np.column_stack((X, X**2))
>
> # Criando um modelo Ridge com lambda = 0.1
> ridge_model = Ridge(alpha=0.1)
> ridge_model.fit(H, Y)
>
> # Obtendo os coeficientes beta
> beta_ridge = np.insert(ridge_model.coef_, 0, ridge_model.intercept_)
>
> print("Coeficientes Beta Ridge:", beta_ridge)
>
> # Comparando com o modelo sem regulariza√ß√£o
> H_no_intercept = np.column_stack((np.ones(len(X)), X, X**2))
> beta_no_reg = np.linalg.inv(H_no_intercept.T @ H_no_intercept) @ H_no_intercept.T @ Y
>
> print("Coeficientes Beta Sem Regulariza√ß√£o:", beta_no_reg)
> ```
>
> Observamos que os coeficientes no modelo Ridge s√£o menores em magnitude em compara√ß√£o com o modelo sem regulariza√ß√£o, reduzindo o risco de overfitting.

```mermaid
graph LR
    subgraph "Regularization"
        direction LR
        A["Design Matrix 'H'"] --> B["Response Vector 'Y'"]
        B --> C["Regularization Parameter 'Œª'"]
        C --> D["Ridge Regression: min ||Y - HŒ≤||¬≤ + Œª||Œ≤||¬≤"]
        D --> E["Regularized Coefficients 'Œ≤_ridge'"]
    end
```

3.  **Sele√ß√£o de Vari√°veis:** M√©todos como *stepwise regression* ou m√©todos baseados em valida√ß√£o cruzada, podem ser usados para selecionar as fun√ß√µes de base mais importantes, removendo aquelas que n√£o contribuem significativamente para o desempenho do modelo.

A combina√ß√£o dessas t√©cnicas permite controlar a complexidade do modelo, obtendo um equil√≠brio entre vi√©s e vari√¢ncia que melhora a generaliza√ß√£o para dados n√£o observados. A valida√ß√£o cruzada desempenha um papel essencial na escolha da complexidade do modelo, pois permite avaliar seu desempenho em dados que n√£o foram usados no treinamento, garantindo um modelo com boa performance de generaliza√ß√£o [^5.5.1].

### Tradeoffs em um Cen√°rio Unidimensional

Em um cen√°rio unidimensional, os *tradeoffs* se tornam mais claros:

*   **Simplicidade vs. Flexibilidade:** Usar um modelo linear simples (sem *basis expansions*) pode resultar em um modelo com alto vi√©s, incapaz de capturar rela√ß√µes n√£o lineares. A introdu√ß√£o de *basis expansions* aumenta a flexibilidade do modelo, mas tamb√©m aumenta a complexidade e o risco de *overfitting*.
*   **N√∫mero de Fun√ß√µes de Base:** Usar um n√∫mero pequeno de fun√ß√µes de base ou fun√ß√µes de base muito simples pode levar a um modelo com alto vi√©s, incapaz de capturar as varia√ß√µes nos dados. Usar muitas fun√ß√µes ou fun√ß√µes muito complexas pode levar a um modelo com alta vari√¢ncia, muito sens√≠vel a varia√ß√µes nos dados de treino. A escolha do n√∫mero de fun√ß√µes de base deve equilibrar vi√©s e vari√¢ncia.
*   **Regulariza√ß√£o:** A intensidade da regulariza√ß√£o controla o compromisso entre vi√©s e vari√¢ncia. Valores muito altos de $\lambda$ podem levar a um modelo com alto vi√©s, enquanto valores muito baixos podem levar a um modelo com alta vari√¢ncia. O valor adequado de $\lambda$ depende dos dados e do objetivo da modelagem.
*   **Interpretabilidade:** Modelos lineares com transforma√ß√µes simples s√£o mais f√°ceis de interpretar do que modelos com fun√ß√µes de base complexas (como splines ou wavelets). O *tradeoff* entre interpretabilidade e flexibilidade tamb√©m deve ser considerado.

Ao entender esses *tradeoffs*, √© poss√≠vel construir modelos unidimensionais mais eficazes, que equilibram a capacidade de capturar a complexidade dos dados com a necessidade de evitar *overfitting* e manter a interpretabilidade.

```mermaid
graph LR
    subgraph "Tradeoffs in 1D Basis Expansions"
        direction TB
        A["Simplicity vs Flexibility"]
        B["Number of Basis Functions: Bias-Variance Tradeoff"]
        C["Regularization Strength: Bias-Variance Tradeoff"]
        D["Interpretabilty vs. Model Complexity"]
        A --> B
        B --> C
        C --> D
    end
```

### Conclus√£o

A explora√ß√£o do cen√°rio unidimensional oferece uma base s√≥lida para entender como as *basis expansions* s√£o aplicadas na pr√°tica. A escolha das fun√ß√µes de base, o controle da complexidade atrav√©s da regulariza√ß√£o e sele√ß√£o, e a avalia√ß√£o do *tradeoff* vi√©s-vari√¢ncia s√£o essenciais para a constru√ß√£o de modelos unidimensionais eficazes. As ideias e os conceitos apresentados nesse cap√≠tulo s√£o facilmente estendidos para dados com maior dimensionalidade.

### Footnotes

[^5.2]: "Some simple and widely used examples of the hm are the following: $h_m(X) = X^m$, $m = 1, \ldots, p$ recovers the original linear model. $h_m(X) = X_j^2$ or $h_m(X) = X_jX_k$ allows us to augment the inputs with polynomial terms to achieve higher-order Taylor expansions." *(Trecho de <Basis Expansions and Regularization>)*
[^5.5.1]: "Since $df_x = \text{trace}(S_x)$ is monotone in $\lambda$ for smoothing splines, we can invert the relationship and specify $\lambda$ by fixing $df$. In practice this can be achieved by simple numerical methods. So, for example, in R one can use smooth.spline(x,y,df=6) to specify the amount of smoothing." *(Trecho de <Basis Expansions and Regularization>)*
