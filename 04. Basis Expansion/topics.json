{
  "topics": [
    {
      "topic": "Basis Expansions and Regularization",
      "sub_topics": [
        "Linear models, including linear regression, linear discriminant analysis, and logistic regression, are fundamental in machine learning, but their effectiveness is limited when the true underlying function is nonlinear and non-additive. Basis expansions provide a way to extend these models to capture more complex relationships by transforming the input features. These models serve as fundamental approximations, offering interpretability and computational convenience, especially when dealing with limited data or high dimensionality, where overfitting becomes a concern. Linear Bayes-optimal decision boundaries imply a linear transformation of Pr(Y=1|X).",
        "Basis expansion involves augmenting the original input features with additional variables that are transformations of the original inputs; this creates a new space of derived input features in which linear models can be applied. The input vector X is augmented with transformations hm(X) to create a new space of derived input features, enabling the use of linear models in this expanded space, where the model takes the form f(X) = \u03a3 \u03b2mhm(X), with \u03b2m representing the coefficients. The fitting process remains linear after the basis functions are determined.",
        "Common basis functions include polynomials (e.g., quadratic terms, cross-product terms), logarithms, power functions, and indicator functions for regions of input variables; the choice of basis functions depends on the specific problem and the desired flexibility. These include the original linear model (hm(X) = Xm), polynomial terms (XjXk), and nonlinear transformations of single inputs (log(Xj), \u221aXj), allowing for higher-order Taylor expansions or other types of nonlinear relationships. Polynomial basis functions, such as hm(X) = Xj or hm(X) = XjXk, allow for the approximation of higher-order Taylor expansions, but the number of variables grows exponentially with the polynomial degree, leading to O(pd) terms for a degree-d polynomial in p variables. Nonlinear transformations of single inputs, like hm(X) = log(Xj) or \u221aXj, and functions involving multiple inputs, such as hm(X) = ||X||, introduce nonlinearity while maintaining a manageable number of variables. Indicator functions, such as hm(X) = I(Lm \u2264 Xk < Um), partition the range of Xk into non-overlapping regions, resulting in a piecewise constant contribution for Xk and providing flexibility in representing different functional forms. Radial basis functions are also used.",
        "Piecewise polynomials and splines are useful families of basis functions that allow for local polynomial representations, enabling more flexible modeling than global polynomials; splines are piecewise polynomials that are connected smoothly at the knots. Families of piecewise-polynomials and splines allow for local polynomial representations, addressing the limitations of global polynomials. Piecewise-polynomials and splines are useful families of functions that allow for local polynomial representations. Wavelet bases are also useful for modeling signals and images.",
        "Complexity control methods for basis expansions include restriction methods (limiting the class of functions), selection methods (adaptively scanning the dictionary), and regularization methods (restricting coefficients like Ridge regression or Lasso). When using a dictionary D of basis functions, complexity can be controlled through restriction methods (limiting the class of functions), selection methods (adaptively scanning the dictionary), and regularization methods (restricting the coefficients). Restriction methods limit the class of functions by deciding beforehand to impose constraints such as additivity, where the model has the form f(X) = \u03a3 fj(Xj), which reduces complexity but may limit expressiveness. Selection methods adaptively scan the dictionary and include only those basis functions that contribute significantly to the fit of the model. Regularization methods use the entire dictionary but restrict the coefficients.",
        "Regularization techniques, like ridge regression and the lasso, are used to control the complexity of the model and prevent overfitting, especially when using a large number of basis functions; these methods restrict the coefficients of the basis functions. Regularization methods restrict the coefficients of basis functions to control model complexity, with ridge regression and lasso serving as examples; these methods use the entire dictionary of basis functions but penalize the coefficients to prevent overfitting."
      ]
    },
    {
      "topic": "Piecewise Polynomials and Splines",
      "sub_topics": [
        "Piecewise polynomial functions are constructed by dividing the domain of the input variable into contiguous intervals and representing the function by a separate polynomial in each interval; this allows for more flexible modeling than a single global polynomial, allowing for local approximations and flexibility in capturing different functional forms.",
        "Piecewise constant functions are the simplest form of piecewise polynomials, where the function is constant within each interval; the least squares estimate of the model amounts to the mean of the target variable in each region. These functions can be represented using indicator functions, where the least squares estimate of the model f(X) = \u03a3 \u03b2mhm(X) amounts to \u03b2m = Ym, the mean of Y in the mth region, providing a simple but potentially limited approximation.",
        "Piecewise linear fits can be achieved by adding basis functions of the form hm+3 = hm(X)X, but continuity restrictions at the knots lead to linear constraints on the parameters, reducing the number of free parameters and ensuring a smooth transition between intervals. Continuity restrictions can be imposed on piecewise linear fits, leading to linear constraints on the parameters and a reduction in the number of free parameters. A direct way to proceed is to use a basis that incorporates the constraints directly, such as h1(X) = 1, h2(X) = X, h3(X) = (X \u2212 \u03be1)+, h4(X) = (X \u2212 \u03be2)+.",
        "Splines are piecewise polynomials that are restricted to be continuous at the knots; cubic splines are commonly used and have continuous first and second derivatives at the knots. Smoother functions can be achieved by increasing the order of the local polynomial, such as using piecewise-cubic polynomials. Increasing the order of continuity at the knots leads to smoother functions, with cubic splines having continuous first and second derivatives at the knots. Cubic splines are the lowest-order spline where knot-discontinuity is not visible and are represented by a basis including polynomials up to degree 3 and truncated power functions for knots, with the expression bs(x, df=7) in R generating a basis matrix of cubic-spline functions evaluated at the N observations in x.",
        "A natural cubic spline adds additional constraints, namely that the function is linear beyond the boundary knots; natural cubic splines are often considered reasonable when the function is assumed to be linear near the boundaries. These splines add additional constraints, forcing the function to be linear beyond the boundary knots, which frees up degrees of freedom in the interior region and can improve extrapolation behavior, but may introduce bias near the boundaries. Natural cubic splines add constraints, such as the function being linear beyond the boundary knots, and the truncated power series basis can be used to derive the reduced basis, and the pointwise variance of spline functions fit by least squares can be used to assess erratic behavior near boundaries.",
        "Fixed-knot splines are also known as regression splines; the order of the spline, the number of knots, and their placement need to be selected; one simple approach is to parameterize a family of splines by the number of basis functions or degrees of freedom, and have the observations determine the positions of the knots.",
        "The truncated power basis is conceptually simple but not too attractive numerically; the B-spline basis allows for efficient computations even when the number of knots is large. Since the space of spline functions of a particular order and knot sequence is a vector space, there are many equivalent bases for representing them (just as there are for ordinary polynomials). The B-spline basis, defined recursively in terms of divided differences, provides a numerically stable and efficient way to represent polynomial splines, and is widely used in practice for its computational advantages and flexibility in handling different knot sequences and orders of splines."
      ]
    },
    {
      "topic": "Smoothing Splines",
      "sub_topics": [
        "Smoothing splines address the knot selection problem by using a maximal set of knots and controlling the complexity of the fit through regularization, minimizing the penalized residual sum of squares RSS(f, \u03bb) = \u03a3{yi - f(xi)}\u00b2 + \u03bb \u222b {f''(t)}\u00b2dt. The penalized residual sum of squares (RSS) criterion is minimized, balancing closeness to the data and penalizing curvature in the function.",
        "The smoothing parameter \u03bb establishes a tradeoff between closeness to the data and smoothness of the function, with \u03bb = 0 leading to interpolation and \u03bb = \u221e leading to a simple least squares line fit, and intermediate values indexing an interesting class of functions. The smoothing parameter \u03bb controls the tradeoff between closeness to the data and smoothness of the function; \u03bb = 0 results in a function that interpolates the data, while \u03bb = \u221e results in a simple least squares line fit. The criterion is defined on an infinite-dimensional function space, but it has an explicit, finite-dimensional unique minimizer.",
        "The solution to the smoothing spline problem is a natural cubic spline with knots at the unique values of the xi, i = 1,..., N, and the penalty term translates to a penalty on the spline coefficients, which are shrunk toward the linear fit. A smoothing spline is a natural cubic spline with knots at the unique values of the input variable; the penalty term translates to a penalty on the spline coefficients, which are shrunk some of the way toward the linear fit. The criterion for smoothing splines is defined on an infinite-dimensional function space, but remarkably, it has an explicit, finite-dimensional, unique minimizer which is a natural cubic spline with knots at the unique values of the xi, i = 1,..., N.",
        "A smoothing spline with prechosen \u03bb is a linear smoother, and the finite linear operator S\u03bb is known as the smoother matrix, where the expression M = trace(Hg) gives the dimension of the projection space, and by analogy we define the effective degrees of freedom of a smoothing spline to be df\u03bb = trace(Sx). The smoother matrix S\u03bb is a linear operator that maps the observed values y to the fitted values f, and is symmetric, positive semidefinite, and has a real eigen-decomposition, allowing for a more intuitive parameterization of the smoothing spline. A smoothing spline is a linear smoother, meaning that the estimated parameters are a linear combination of the yi; the smoother matrix Sx relates the fitted values f to the observed values y, and it depends only on the xi and \u03bb. This is an example of a linear smoother, as the estimated parameters are a linear combination of the yi.",
        "The effective degrees of freedom df\u03bb = trace(S\u03bb) provides a measure of the complexity of the smoothing spline, and can be used to specify the amount of smoothing in a consistent fashion, allowing for comparison with other smoothing methods. The effective degrees of freedom of a smoothing spline is defined as the trace of the smoother matrix; this provides a more intuitive way to parameterize the smoothing spline. The effective degrees of freedom df\u03bb of a smoothing spline are defined as trace(Sx), providing a way to parameterize the amount of smoothing; Sx has a real eigen-decomposition, and it can be rewritten in the Reinsch form Sx = (I + \u03bbK)^-1, where K is the penalty matrix.",
        "The Reinsch form S\u03bb = (I + \u03bbK)\u22121 expresses the smoother matrix in terms of the penalty matrix K, and the eigen-decomposition of S\u03bb reveals that the smoothing spline operates by decomposing y w.r.t. the basis {uk} and differentially shrinking the contributions using pk(\u03bb). The smoothing spline operates by decomposing y with respect to a complete basis {uk} and differentially shrinking the contributions using pk(\u03bb); the eigenvectors are not affected by changes in \u03bb, and hence the whole family of smoothing splines indexed by \u03bb have the same eigenvectors.",
        "The pointwise variance of spline functions fit by least squares can be used to assess the behavior of polynomials fit to data, especially near the boundaries where extrapolation can be dangerous, and natural cubic splines address this issue by adding additional constraints that the function is linear beyond the boundary knots."
      ]
    },
    {
      "topic": "Multidimensional Splines",
      "sub_topics": [
        "Multidimensional splines extend the concepts of one-dimensional splines to higher dimensions; tensor product bases are used to represent functions of multiple variables.",
        "A tensor product basis is defined by the product of one-dimensional basis functions for each coordinate; the dimension of the basis grows exponentially with the number of dimensions. The tensor product basis can achieve more flexibility at the decision boundary, but introduces some spurious structure along the way.",
        "Thin-plate splines are a type of multidimensional spline that generalize the one-dimensional roughness penalty to higher dimensions; they share many properties with one-dimensional cubic smoothing splines.",
        "Radial basis functions are used in thin-plate splines; the coefficients are found by plugging the radial basis functions into a finite-dimensional penalized least squares problem.",
        "There are a number of hybrid approaches that are popular in practice, both for computational and conceptual simplicity; unlike one-dimensional smoothing splines, the computational complexity for thin-plate splines is O(N\u00b3). Unlike one-dimensional smoothing splines, the computational complexity for thin-plate splines is O(N\u00b3), since there is not in general any sparse structure that can be exploited. However, as with univariate smoothing splines, we can get away with substantially less than the N knots prescribed by the solution (5.39).",
        "The additive spline models are a restricted class of multidimensional splines; they can be represented in this general formulation as well; that is, there exists a penalty J[f] that guarantees that the solution has the form f(X) = a + f1(X1) + \u00b7\u00b7\u00b7 + fa(Xa) and that each of the functions fj are univariate splines.",
        "The penalty functional for stabilizing a function f in IRd is J[f]. For example, a natural generalization of the one-dimensional roughness penalty (5.9) for functions on IR\u00b2 is J[f] = \u222cIR2 (\u2202\u00b2f/\u2202x1\u00b2)\u00b2 + 2(\u2202\u00b2f/\u2202x1\u2202x2)\u00b2 + (\u2202\u00b2f/\u2202x2\u00b2)\u00b2 dx1dx2."
      ]
    },
    {
      "topic": "Wavelet Smoothing",
      "sub_topics": [
        "Wavelet bases are generated by translations and dilations of a single scaling function \u03c6(x), also known as the father, and the wavelets \u03c8j,k(x) form an orthonormal basis for the orthogonal complement Wj of Vj to Vj+1, written as Vj+1 = Vj \u2295 Wj. These bases are generated by translations and dilations of a single scaling function (father); the Haar basis is particularly easy to understand, especially for anyone with experience in analysis of variance or trees, since it produces a piecewise-constant representation. The Haar basis is particularly easy to understand, since it produces a piecewise-constant representation.",
        "Wavelets typically use a complete orthonormal basis to represent functions, but then shrink and select the coefficients toward a sparse representation; just as a smooth function can be represented by a few spline basis functions, a mostly flat function with a few isolated bumps can be represented with a few (bumpy) basis functions. Wavelets use a complete orthonormal basis to represent functions, and then shrink and select the coefficients toward a sparse representation, achieving both smooth and locally bumpy functions in an efficient way, a phenomenon dubbed time and frequency localization.",
        "Adaptive wavelet filtering involves shrinking the wavelet coefficients toward zero; a popular method is SURE shrinkage (Stein Unbiased Risk Estimation). Adaptive wavelet filtering involves minimizing the criterion ||y \u2013 W\u03b8||\u00b2 + 2\u03bb||\u03b8||1, which is the same as the lasso criterion and leads to the simple solution \u03b8j = sign(y*j)(|y*j| - \u03bb)+, where y* = WTy is the wavelet transform of y.",
        "SURE shrinkage is the same soft-thresholding rule that arises in the lasso procedure for linear regression; the least squares coefficients are translated toward zero, and truncated at zero. The SURE criterion (5.68) on page 179 and the smoothing spline criterion (5.21) on page 156 are similar.",
        "What makes wavelets special is the particular form of basis functions used, which allows for a representation localized in time and in frequency; wavelet coefficients represent characteristics of the signal localized in time and localized in frequency. Wavelets achieve time and frequency localization, unlike traditional Fourier bases which only allow frequency localization; the wavelet transform decomposes a signal into different scales (frequencies) and translations (time positions), allowing for adaptive smoothing. Wavelets are able to represent both smooth and/or locally bumpy functions in an efficient way, a phenomenon dubbed time and frequency localization.",
        "The choice of \u03bb = \u03c3\u221a(2log N), where \u03c3 is an estimate of the standard deviation of the noise, is motivated by the fact that the expected maximum of N white noise variables is approximately \u03c3\u221a(2log N), and hence all coefficients below this value are likely to be noise and are set to zero.",
        "The symmlet-p wavelet \u03c8(x) has p vanishing moments, meaning that \u222b\u03c8(x)xj dx = 0, j = 0,...,p - 1, and any order-p polynomial over the N = 2J times points is reproduced exactly in Vo, and the symmlet-p scaling functions are one of many families of wavelet generators. The symmlet-p wavelet has p vanishing moments, meaning that it can reproduce any order-p polynomial exactly; this is analogous to the null space of the smoothing-spline penalty, and it allows for a more accurate representation of smooth functions.",
        "Adaptive wavelet fitting is known as SURE shrinkage (Stein Unbiased Risk Estimation), and involves the wavelet transform y* = WTy and the inverse wavelet transform f = W\u03b8, with the goal of adaptively fitting the wavelet coefficients to the data. Adaptive wavelet filtering is particularly useful when the data are measured on a uniform lattice, such as a discretized signal, image, or time series; it involves computing the wavelet transform of the data and then thresholding the coefficients to remove noise and retain important features."
      ]
    },
    {
      "topic": "Regularization and Reproducing Kernel Hilbert Spaces",
      "sub_topics": [
        "Regularization problems can be formulated as min \u03a3L(yi, f(xi)) + \u03bbJ(f), where L(y, f(x)) is a loss function, J(f) is a penalty functional, and H is a space of functions, with penalty functionals often taking the form J(f) = \u222b|f(s)|\u00b2/G(s) ds, where G(s) is a positive function that falls off to zero as ||s|| \u2192 \u221e. A general class of regularization problems has the form min f\u2208H \u03a3 Li=1 L(yi, f(xi)) + \u03bbJ(f), where L(y, f(x)) is a loss function, J(f) is a penalty functional, and H is a space of functions on which J(f) is defined.",
        "Reproducing kernel Hilbert spaces (RKHS) are generated by a positive definite kernel K(x, y), with the penalty functional J defined in terms of the kernel, and the solution to the regularization problem has the form f(X) = \u03a3\u03b1k\u03a6k(X) + \u03a3\u03b8iG(X \u2013 Xi), where the \u03a6k span the null space of the penalty functional J, and G is the inverse Fourier transform of G. The solutions can be expressed in terms of a kernel function K, which is a positive definite function that defines the reproducing kernel Hilbert space (RKHS).",
        "The kernel K(x, y) has an eigen-expansion K(x, y) = \u03a3 \u03b3i\u03a6i(x)\u03a6i(y), where \u03b3i \u2265 0 and \u03a3\u03b3i < \u221e, and elements of HK have an expansion in terms of these eigen-functions, f(x) = \u03a3 ci\u03a6i(x), with the constraint that ||f||\u00b2 = \u03a3 ci\u00b2/\u03b3i < \u221e.",
        "The penalty functional in RKHS is defined to be the squared norm J(f) = ||f||\u00b2, and the regularization problem reduces to min \u03a3L(yi, f(xi)) + \u03bb||f||\u00b2, with the solution being finite-dimensional and having the form f(x) = \u03a3\u03b1iK(x, xi), where the \u03b1i are coefficients. The penalty functional J(f) is defined in terms of the kernel, and the solution can be expressed as a linear combination of the kernel evaluated at the training points. This reduces the infinite-dimensional problem to a finite-dimensional optimization problem.",
        "The kernel property refers to the phenomenon whereby the infinite-dimensional problem reduces to a finite-dimensional optimization problem, and has been dubbed the kernel property in the literature on support-vector machines. This occurs because the penalty functional J can be expressed in terms of the kernel matrix K, which has entries K(xi, xj).",
        "Penalized polynomial regression utilizes the kernel K(x, y) = ((x, y) + 1)d, which has M = (p+d) eigen-functions that span the space of polynomials in IRp of total degree d, and the solution involves evaluating the kernel N\u00b2 times and computing the solution in O(N\u00b3) operations. Penalized polynomial regression involves using a kernel that represents an expansion of polynomials, allowing for convenient computation of high-dimensional inner products. Gaussian radial basis functions use a Gaussian kernel, leading to a regression model that is an expansion in Gaussian radial basis functions.",
        "Radial basis functions (RBFs) are functions of the form K(x, y) = \u03c6(||x - y||), where \u03c6 is a univariate function; the Gaussian kernel K(x, y) = e-v||x-y||\u00b2 is a popular choice for RBFs, and it leads to a regression model that is an expansion in Gaussian radial basis functions.",
        "Support vector machines (SVMs) use a kernel to map the input data into a higher-dimensional space, where a linear classifier is constructed; the parameters of the SVM are chosen to minimize a combination of the hinge loss and a regularization term, and the solution is sparse, meaning that only a subset of the data points (support vectors) contribute to the classifier."
      ]
    },
    {
      "topic": "Introduction",
      "sub_topics": [
        "Linear models like linear regression, linear discriminant analysis, and logistic regression are commonly used for regression and classification tasks but may not accurately represent the true underlying function f(X).",
        "Using linear models can be a practical approximation, especially when dealing with limited data (small N) or a high number of features (large p), to prevent overfitting.",
        "Moving beyond linearity involves augmenting or replacing the input vector X with transformations of X, using linear models in this new space of derived input features, which can be achieved through basis expansions.",
        "Basis expansions involve transforming the original input features using functions hm(X) and then modeling the output as a linear combination of these transformed features, represented by f(X) = \u03a3 \u03b2mhm(X)."
      ]
    },
    {
      "topic": "Natural Cubic Splines",
      "sub_topics": [
        "Natural cubic splines add constraints such that the function is linear beyond the boundary knots, freeing up degrees of freedom to be spent more profitably by sprinkling more knots in the interior region.",
        "The truncated power basis set is a general form for the truncated-power basis set. While conceptually simple, it is not too attractive numerically, as powers of large numbers can lead to severe rounding problems.",
        "Since the space of spline functions of a particular order and knot sequence is a vector space, there are many equivalent bases for representing them (just as there are for ordinary polynomials).",
        "The B-spline basis, described in the Appendix to this chapter, allows for efficient computations even when the number of knots K is large."
      ]
    },
    {
      "topic": "Automatic Selection of the Smoothing Parameters",
      "sub_topics": [
        "The effective degrees of freedom df\u03bb = trace(S\u03bb) provide a more intuitive way to parameterize the smoothing spline, allowing for consistent comparison across different smoothers.",
        "The choice of \u03bb affects the eigenvalues and eigenvectors of the smoother matrix, with eigenvectors remaining unchanged and eigenvalues being shrunk differentially based on their magnitude.",
        "The smoothing spline operates by decomposing y with respect to a complete basis and differentially shrinking the contributions using \u03c1k(\u03bb), contrasting with basis-regression methods where components are either left alone or shrunk to zero.",
        "Automatic methods for choosing \u03bb, such as cross-validation, are discussed later in the chapter. In practice, \u03bb can be specified by fixing df, achieved by simple numerical methods."
      ]
    },
    {
      "topic": "Nonparametric Logistic Regression",
      "sub_topics": [
        "The smoothing spline problem can be extended to other domains, such as logistic regression, by transferring the technology to other domains.",
        "The penalized log-likelihood criterion is constructed by adding a penalty term to the log-likelihood based on the binomial distribution.",
        "Arguments similar to those used in Section 5.4 show that the optimal f is a finite-dimensional natural spline with knots at the unique values of x.",
        "The update equation fits a weighted smoothing spline to the working response z. The form of (5.34) is suggestive. It is tempting to replace S\u03bb,w by any nonparametric (weighted) regression operator, and obtain general families of nonparametric logistic regression models."
      ]
    }
  ]
}