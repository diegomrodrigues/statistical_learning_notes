## Goal of Basis Expansions: Extending Linear Models

```mermaid
graph LR
    A["Original Features 'X'"] --> B("Feature Transformation 'h(X)'")
    B --> C("Linear Model Applied 'f(h(X))'")
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1 stroke:#333,stroke-width:2px;
    subgraph "Basis Expansion Process"
    B
    end
    subgraph "Modeling Data"
    C
    end
```

### Introdu√ß√£o

Modelos lineares, como discutido anteriormente, s√£o inerentemente limitados pela sua incapacidade de modelar rela√ß√µes n√£o lineares entre as features de entrada e a vari√°vel de resposta [^4.1], [^5.1]. A t√©cnica de *basis expansions* surge como uma solu√ß√£o para contornar essa limita√ß√£o. A ideia central √© **transformar as features originais** $X$ em um novo espa√ßo de representa√ß√£o, denotado por $h(X)$, onde o modelo linear passa a ser aplicado [^5.1]. Este processo permite que o modelo capture rela√ß√µes n√£o lineares atrav√©s de uma combina√ß√£o linear das novas features, proporcionando maior flexibilidade e capacidade de modelar dados mais complexos.

O objetivo principal do uso de *basis expansions* n√£o √© abandonar a simplicidade e a interpretabilidade dos modelos lineares, mas sim estender o seu poder expressivo, permitindo a modelagem de fun√ß√µes mais complexas sem introduzir modelos totalmente n√£o lineares e de dif√≠cil interpreta√ß√£o. A escolha das fun√ß√µes de base $h_m(X)$ √© crucial, pois determina o tipo de n√£o linearidade que o modelo √© capaz de capturar. Este cap√≠tulo explora diferentes tipos de fun√ß√µes de base e como elas podem ser utilizadas para melhorar a performance dos modelos [^5.1].

###  Transforma√ß√£o de Features e Modelagem N√£o Linear

A t√©cnica de *basis expansions* envolve a transforma√ß√£o do vetor de entrada $X$ atrav√©s de fun√ß√µes $h_m(X)$, criando novas features que s√£o ent√£o usadas em um modelo linear. A modelagem resultante, embora linear no espa√ßo das novas features, √© n√£o linear no espa√ßo original dos dados [^5.1].

A representa√ß√£o geral de um modelo usando *basis expansions* √© dada por:

$$
f(X) = \sum_{m=1}^{M} \beta_m h_m(X),
$$

onde $h_m(X)$ s√£o as fun√ß√µes de base, $M$ √© o n√∫mero total de fun√ß√µes de base, e $\beta_m$ s√£o os coeficientes do modelo. Essa formula√ß√£o permite construir uma variedade de modelos, cada um com caracter√≠sticas espec√≠ficas, dependendo das escolhas das fun√ß√µes de base.

```mermaid
graph LR
    subgraph "Basis Expansion Model"
    direction TB
        A["Model: f(X)"] --> B["Summation: Œ£ Œ≤_m h_m(X)"]
        B --> C["'Œ≤_m' Coefficients"]
        B --> D["'h_m(X)' Basis Functions"]
        style A fill:#ccf,stroke:#333,stroke-width:2px
        style B fill:#f9f,stroke:#333,stroke-width:2px
        style C,D fill:#fff,stroke:#333,stroke-width:1px
    end
```

A chave para uma modelagem eficaz com *basis expansions* √© a escolha apropriada das fun√ß√µes $h_m(X)$. Diferentes tipos de fun√ß√µes de base podem ser utilizados, incluindo:

*   **Fun√ß√µes Polinomiais:** Incluir termos como $X_j^2$, $X_j^3$ ou produtos cruzados $X_jX_k$ permite aproximar fun√ß√µes com maior grau de n√£o linearidade [^5.2]. No entanto, o n√∫mero de termos cresce rapidamente com o grau do polin√¥mio, levando a um problema de dimensionalidade [^5.2].

*   **Transforma√ß√µes Logar√≠tmicas e de Raiz Quadrada:** Fun√ß√µes como $\log(X_j)$ ou $\sqrt{X_j}$ podem ser √∫teis para modelar dados com crescimento ou decaimento n√£o linear [^5.2].

*   **Fun√ß√µes Indicadoras (Regi√µes):** Utilizar fun√ß√µes indicadoras $I(L_m \leq X_k < U_m)$ permite dividir o espa√ßo das features em regi√µes, modelando a fun√ß√£o de forma piecewise constante [^5.2].

A escolha das fun√ß√µes de base tamb√©m √© crucial para controlar a complexidade do modelo e evitar overfitting. As *basis expansions* aumentam a flexibilidade do modelo, mas o n√∫mero de par√¢metros tamb√©m pode aumentar consideravelmente.  Portanto, a sele√ß√£o e a regulariza√ß√£o s√£o t√©cnicas essenciais a serem combinadas com *basis expansions* para garantir um bom desempenho do modelo [^5.2].

### Exemplos de Fun√ß√µes de Base

A seguir, s√£o detalhados alguns exemplos espec√≠ficos de fun√ß√µes de base e como elas se encaixam na estrutura das *basis expansions*:

1.  **Modelo Linear Original:** A forma mais simples de *basis expansion* √© quando $h_m(X) = X_m$, onde $m=1,\ldots,p$ recupera o modelo linear original. Neste caso, a transforma√ß√£o √© uma identidade e o modelo mant√©m sua natureza linear [^5.2].

    > üí° **Exemplo Num√©rico:**
    >
    > Suponha que temos um dataset com uma √∫nica feature $X$ e a vari√°vel resposta $Y$. No modelo linear original, a fun√ß√£o de base seria simplesmente $h_1(X) = X$. O modelo seria ent√£o $f(X) = \beta_0 + \beta_1 X$.  Se, por exemplo, $\beta_0 = 2$ e $\beta_1 = 3$, a previs√£o para $X=4$ seria $f(4) = 2 + 3 * 4 = 14$. Este √© um modelo linear sem nenhuma transforma√ß√£o.

2.  **Expans√£o Polinomial:** Permite que o modelo capture n√£o linearidades atrav√©s da inclus√£o de termos polinomiais das features. Por exemplo, $h_m(X) = X_j^2$ ou $h_m(X) = X_jX_k$. O grau do polin√¥mio determina a capacidade do modelo de capturar rela√ß√µes complexas nos dados, mas tamb√©m aumenta o n√∫mero de vari√°veis do modelo [^5.2]. Um modelo quadr√°tico completo com $p$ vari√°veis requer $O(p^2)$ termos e um polin√¥mio de grau $d$ requer $O(p^d)$ termos.

    > üí° **Exemplo Num√©rico:**
    >
    > Considere novamente o dataset com uma √∫nica feature $X$. Para uma expans√£o polinomial de grau 2, ter√≠amos as fun√ß√µes de base $h_1(X) = X$ e $h_2(X) = X^2$. O modelo seria $f(X) = \beta_0 + \beta_1 X + \beta_2 X^2$. Se $\beta_0 = 1$, $\beta_1 = 2$ e $\beta_2 = -0.5$, a previs√£o para $X=4$ seria $f(4) = 1 + 2 * 4 + (-0.5) * 4^2 = 1 + 8 - 8 = 1$. Aqui, a rela√ß√£o entre $X$ e a vari√°vel resposta √© n√£o linear devido ao termo quadr√°tico.
    >
    > ```python
    > import numpy as np
    > import matplotlib.pyplot as plt
    >
    > # Dados de exemplo
    > X = np.linspace(-5, 5, 100)
    > beta_0 = 1
    > beta_1 = 2
    > beta_2 = -0.5
    > y = beta_0 + beta_1 * X + beta_2 * X**2
    >
    > # Plot
    > plt.figure(figsize=(8, 6))
    > plt.plot(X, y, label='f(X) = 1 + 2X - 0.5X^2')
    > plt.xlabel('X')
    > plt.ylabel('f(X)')
    > plt.title('Modelo com Expans√£o Polinomial (Grau 2)')
    > plt.legend()
    > plt.grid(True)
    > plt.show()
    > ```
    >
    > Este gr√°fico mostra a curvatura introduzida pela expans√£o polinomial.

```mermaid
graph LR
    subgraph "Polynomial Basis Expansion"
        direction TB
        A["Input Feature: X"] --> B["Basis Functions: 'h_1(X) = X', 'h_2(X) = X^2', ..."]
        B --> C["Model: f(X) = Œ≤_0 + Œ≤_1X + Œ≤_2X^2 + ..."]
        style C fill:#ccf,stroke:#333,stroke-width:2px
    end
```

3.  **Transforma√ß√µes N√£o Lineares de Inputs:** Fun√ß√µes como $h_m(X) = \log(X_j)$ ou $h_m(X) = \sqrt{X_j}$ permitem modelar rela√ß√µes n√£o lineares de cada feature individualmente [^5.2]. Essas transforma√ß√µes podem ser aplicadas a uma ou mais features, ou usar combina√ß√µes envolvendo diversas features como $h_m(X) = ||X||$.

    > üí° **Exemplo Num√©rico:**
    >
    > Suponha que temos uma feature $X$ que representa o tamanho de uma cidade e a vari√°vel resposta $Y$ representa o n√∫mero de restaurantes. A rela√ß√£o pode n√£o ser linear, crescendo mais lentamente com o aumento da cidade. Poder√≠amos usar $h_1(X) = \log(X)$. O modelo seria $f(X) = \beta_0 + \beta_1 \log(X)$. Se $\beta_0 = 0$ e $\beta_1 = 5$, para uma cidade de tamanho $X = 100$, a previs√£o seria $f(100) = 0 + 5 * \log(100) \approx 0 + 5 * 4.605 \approx 23.025$. A transforma√ß√£o logar√≠tmica suaviza o efeito de grandes valores de $X$.
    >
    > ```python
    > import numpy as np
    > import matplotlib.pyplot as plt
    >
    > # Dados de exemplo
    > X = np.linspace(1, 100, 100)
    > beta_0 = 0
    > beta_1 = 5
    > y = beta_0 + beta_1 * np.log(X)
    >
    > # Plot
    > plt.figure(figsize=(8, 6))
    > plt.plot(X, y, label='f(X) = 5log(X)')
    > plt.xlabel('X')
    > plt.ylabel('f(X)')
    > plt.title('Modelo com Transforma√ß√£o Logar√≠tmica')
    > plt.legend()
    > plt.grid(True)
    > plt.show()
    > ```
    >
    > O gr√°fico demonstra como a transforma√ß√£o logar√≠tmica comprime a escala do eixo X.

```mermaid
graph LR
    subgraph "Logarithmic Basis Expansion"
        direction TB
        A["Input Feature: X"] --> B["Basis Function: h(X) = log(X)"]
        B --> C["Model: f(X) = Œ≤_0 + Œ≤_1log(X)"]
        style C fill:#ccf,stroke:#333,stroke-width:2px
    end
```

4.  **Fun√ß√µes Indicadoras de Regi√µes:** Permitem que o modelo capture comportamentos piecewise constantes dividindo o espa√ßo das features em regi√µes. Por exemplo, $h_m(X) = I(L_m \leq X_k < U_m)$ √© uma fun√ß√£o indicadora que vale 1 quando a feature $X_k$ est√° entre $L_m$ e $U_m$, e 0 caso contr√°rio [^5.2]. O uso dessas fun√ß√µes cria um modelo que representa a influ√™ncia de cada regi√£o em uma vari√°vel da resposta.

    > üí° **Exemplo Num√©rico:**
    >
    > Considere a feature $X$ representando a idade de uma pessoa e $Y$ representando a probabilidade de comprar um produto. Podemos dividir as idades em faixas: $h_1(X) = I(18 \leq X < 30)$, $h_2(X) = I(30 \leq X < 50)$ e $h_3(X) = I(50 \leq X < 70)$. O modelo seria $f(X) = \beta_0 + \beta_1 h_1(X) + \beta_2 h_2(X) + \beta_3 h_3(X)$. Se $\beta_0 = 0.1$, $\beta_1 = 0.8$, $\beta_2 = 0.5$, e $\beta_3 = 0.2$, uma pessoa com 25 anos teria uma probabilidade de compra de $0.1 + 0.8 = 0.9$, uma pessoa com 40 anos teria $0.1 + 0.5 = 0.6$, e uma pessoa com 60 anos teria $0.1 + 0.2 = 0.3$. O modelo captura diferentes probabilidades em diferentes faixas et√°rias.
    >
    > ```python
    > import numpy as np
    > import matplotlib.pyplot as plt
    >
    > # Dados de exemplo
    > X = np.linspace(10, 80, 100)
    > beta_0 = 0.1
    > beta_1 = 0.8
    > beta_2 = 0.5
    > beta_3 = 0.2
    >
    > y = np.zeros_like(X)
    > y[(X >= 18) & (X < 30)] = beta_0 + beta_1
    > y[(X >= 30) & (X < 50)] = beta_0 + beta_2
    > y[(X >= 50) & (X < 70)] = beta_0 + beta_3
    > y[(X < 18) | (X >= 70)] = beta_0
    >
    > # Plot
    > plt.figure(figsize=(8, 6))
    > plt.plot(X, y, label='f(X) com Fun√ß√µes Indicadoras')
    > plt.xlabel('Idade (X)')
    > plt.ylabel('Probabilidade de Compra (f(X))')
    > plt.title('Modelo com Fun√ß√µes Indicadoras de Regi√µes')
    > plt.legend()
    > plt.grid(True)
    > plt.show()
    > ```
    >
    > Este gr√°fico mostra um modelo piecewise constante com diferentes valores para diferentes faixas de idade.

```mermaid
graph LR
    subgraph "Indicator Basis Expansion"
        direction TB
        A["Input Feature: X"] --> B["Indicator Functions: 'h_m(X) = I(L_m <= X < U_m)'"]
        B --> C["Model: f(X) = Œ≤_0 + Œ£ Œ≤_m h_m(X)"]
        style C fill:#ccf,stroke:#333,stroke-width:2px
    end
```

√â fundamental observar que as fun√ß√µes $h_m$ podem ser usadas para criar modelos aditivos ou n√£o aditivos. Em um modelo aditivo, a fun√ß√£o $f(X)$ √© uma soma de fun√ß√µes de cada feature individualmente:
$$
f(X) = \sum_{j=1}^p f_j(X_j) = \sum_{j=1}^p \sum_{m=1}^{M_j} \beta_{jm} h_{jm}(X_j)
$$
onde $f_j(X_j)$ √© uma fun√ß√£o de cada feature $X_j$. Esta abordagem assume que n√£o h√° intera√ß√µes entre as features, o que pode ser uma limita√ß√£o em algumas aplica√ß√µes [^5.2].

```mermaid
graph LR
    subgraph "Additive Model Structure"
    direction TB
        A["Model: f(X)"] --> B["Summation of Feature Functions: Œ£ f_j(X_j)"]
        B --> C["Each Feature Function: f_j(X_j)"]
        C --> D["Expansion: Œ£ Œ≤_{jm}h_{jm}(X_j)"]
        style A fill:#ccf,stroke:#333,stroke-width:2px
        style B fill:#f9f,stroke:#333,stroke-width:2px
        style C,D fill:#fff,stroke:#333,stroke-width:1px
     end
```

### M√©todos para Controlar a Complexidade do Modelo

As *basis expansions* aumentam a flexibilidade do modelo, mas tamb√©m podem levar a *overfitting* se a complexidade do modelo n√£o for controlada. Para lidar com esse problema, existem tr√™s abordagens principais [^5.2]:

1.  **Restri√ß√£o:** Limitar o tipo ou o n√∫mero de fun√ß√µes de base utilizadas. A escolha de um modelo aditivo √© um exemplo de restri√ß√£o, onde assume-se que n√£o h√° intera√ß√µes entre as features [^5.2].

    > üí° **Exemplo Num√©rico:**
    >
    > Se tivermos 3 features ($X_1$, $X_2$, $X_3$) e decidirmos usar apenas fun√ß√µes de base lineares e quadr√°ticas, mas sem intera√ß√µes, ent√£o ter√≠amos $h_1(X) = X_1$, $h_2(X) = X_1^2$, $h_3(X) = X_2$, $h_4(X) = X_2^2$, $h_5(X) = X_3$, e $h_6(X) = X_3^2$. O modelo seria $f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_2 + \beta_4 X_2^2 + \beta_5 X_3 + \beta_6 X_3^2$. Se permit√≠ssemos intera√ß√µes, ter√≠amos que incluir termos como $X_1X_2$, $X_1X_3$ e $X_2X_3$ aumentando a complexidade. A restri√ß√£o limita a complexidade do modelo.

```mermaid
graph LR
    subgraph "Model Restriction"
        direction TB
        A["Input Features"] --> B["Restricted Basis Functions"]
        B --> C["Simplified Model"]
        style B fill:#f9f,stroke:#333,stroke-width:2px
        style C fill:#ccf,stroke:#333,stroke-width:2px

    end
```

2.  **Sele√ß√£o:** Selecionar adaptativamente as fun√ß√µes de base que contribuem significativamente para o ajuste do modelo. Os m√©todos de sele√ß√£o de vari√°veis discutidos no Cap√≠tulo 3, bem como m√©todos *greedy* como CART, MARS e boosting, s√£o exemplos de abordagens para sele√ß√£o [^5.2]. Esses m√©todos avaliam a import√¢ncia de cada feature ou fun√ß√£o de base e incluem apenas as mais relevantes para a modelagem.

    > üí° **Exemplo Num√©rico:**
    >
    > Ap√≥s uma expans√£o de base, podemos ter muitas features, como $X_1$, $X_1^2$, $X_2$, $X_2^2$, $X_1X_2$, etc. Usando m√©todos de sele√ß√£o, como stepwise regression, podemos come√ßar com um modelo simples e adicionar termos sequencialmente, avaliando a melhoria no ajuste do modelo. Por exemplo, podemos come√ßar com apenas $X_1$ e $X_2$, e depois adicionar $X_1^2$ se sua inclus√£o reduzir significativamente o erro do modelo. Este processo de sele√ß√£o ajuda a evitar o overfitting, focando nas features mais relevantes.

```mermaid
graph LR
    subgraph "Feature Selection"
        direction TB
        A["Expanded Basis Functions"] --> B["Selection Methods (Stepwise, etc.)"]
        B --> C["Selected Basis Functions"]
        C --> D["Reduced Model"]
    end
```

3. **Regulariza√ß√£o:** Utilizar todas as fun√ß√µes de base, mas restringir os coeficientes do modelo atrav√©s de penalidades. A regress√£o de *ridge* e o *lasso* s√£o exemplos de m√©todos de regulariza√ß√£o que podem ser usados para controlar a complexidade do modelo [^5.2]. A regulariza√ß√£o imp√µe uma restri√ß√£o sobre a magnitude dos coeficientes, evitando *overfitting*.

    > üí° **Exemplo Num√©rico:**
    >
    > Suponha que temos um modelo com muitas fun√ß√µes de base $h_1(X), h_2(X), \ldots, h_M(X)$. Em vez de selecionar as features, usamos todas elas, mas aplicamos regulariza√ß√£o Ridge. O modelo seria $f(X) = \sum_{m=1}^M \beta_m h_m(X)$, e o objetivo seria minimizar o erro do modelo mais a penalidade: $\sum_{i=1}^N (y_i - f(x_i))^2 + \lambda \sum_{m=1}^M \beta_m^2$. O par√¢metro $\lambda$ controla a intensidade da regulariza√ß√£o.  Um $\lambda$ maior for√ßa os coeficientes $\beta_m$ a serem menores, evitando o overfitting.
    >
    > ```python
    > import numpy as np
    > from sklearn.linear_model import Ridge
    > from sklearn.preprocessing import PolynomialFeatures
    > from sklearn.pipeline import make_pipeline
    >
    > # Dados de exemplo
    > np.random.seed(0)
    > X = np.sort(5 * np.random.rand(50, 1), axis=0)
    > y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])
    >
    > # Expans√£o Polinomial
    > degree = 10
    > poly = PolynomialFeatures(degree=degree)
    > X_poly = poly.fit_transform(X)
    >
    > # Ridge regression
    > alpha = 0.1  # Regularization strength
    > ridge = Ridge(alpha=alpha)
    > ridge.fit(X_poly, y)
    >
    > # Predi√ß√µes
    > X_test = np.linspace(0, 5, 100).reshape(-1, 1)
    > X_test_poly = poly.transform(X_test)
    > y_ridge = ridge.predict(X_test_poly)
    >
    > # Plot
    > import matplotlib.pyplot as plt
    > plt.figure(figsize=(10, 6))
    > plt.scatter(X, y, color='blue', label='Dados')
    > plt.plot(X_test, y_ridge, color='red', label='Ridge Regression')
    > plt.xlabel('X')
    > plt.ylabel('y')
    > plt.title('Ridge Regression com Expans√£o Polinomial')
    > plt.legend()
    > plt.grid(True)
    > plt.show()
    > ```
    >
    > Este exemplo mostra como a regress√£o Ridge suaviza a curva ajustada, evitando overfitting.

```mermaid
graph LR
    subgraph "Regularization"
        direction TB
        A["Expanded Basis Functions"] --> B["Regularization (Ridge, Lasso)"]
        B --> C["Model with Penalized Coefficients"]
        C --> D["Reduced Model Complexity"]
    end
```

A combina√ß√£o de *basis expansions* com m√©todos de sele√ß√£o e regulariza√ß√£o √© crucial para construir modelos flex√≠veis que n√£o sofram de *overfitting* e que sejam capazes de generalizar bem para dados n√£o observados.

### Objetivo da Expans√£o de Bases

O principal objetivo da expans√£o de bases (basis expansions) √© aumentar a capacidade dos modelos lineares de capturar padr√µes complexos e n√£o lineares nos dados, transformando as features originais em um espa√ßo de representa√ß√£o mais rico. Ao usar fun√ß√µes de base apropriadas, os modelos podem aprender rela√ß√µes complexas que n√£o seriam acess√≠veis com um modelo linear simples. A t√©cnica permite, portanto, um equil√≠brio entre a flexibilidade de modelos n√£o lineares e a interpretabilidade de modelos lineares, tornando-se uma ferramenta √∫til e vers√°til em diversas aplica√ß√µes.

A escolha das fun√ß√µes de base deve ser guiada pelo conhecimento do dom√≠nio do problema, al√©m de uma avalia√ß√£o emp√≠rica do desempenho do modelo. A aplica√ß√£o correta dessa t√©cnica, em combina√ß√£o com m√©todos de sele√ß√£o e regulariza√ß√£o, leva √† constru√ß√£o de modelos que melhoram a acur√°cia das predi√ß√µes e, ao mesmo tempo, s√£o capazes de lidar com dados complexos.

### Footnotes

[^5.1]: "We have already made use of models linear in the input features, both for regression and classification. Linear regression, linear discriminant analysis, logistic regression and separating hyperplanes all rely on a linear model." *(Trecho de <Basis Expansions and Regularization>)*
[^5.2]: "In regression problems, f(X) = E(Y|X) will typically be nonlinear and nonadditive in X, and representing f(X) by a linear model is usually a convenient, and sometimes a necessary, approximation." *(Trecho de <Basis Expansions and Regularization>)*
