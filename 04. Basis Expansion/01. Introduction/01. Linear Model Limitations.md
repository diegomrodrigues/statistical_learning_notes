## Limitations of Linear Models: Basis Expansions and Regularization

```mermaid
graph LR
    subgraph "Linear Model Limitations"
    direction TB
        A["Linear Regression"] --> B["Limitations: Non-linear f(X)"]
        C["LDA"] --> B
        D["Logistic Regression"] --> B
        E["Separating Hyperplanes"] --> B
    end
    B --> F["Basis Expansions"]
    B --> G["Splines"]
    B --> H["Regularization"]

    style B fill:#f9f,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3 stroke:#000,stroke-width:1px
    linkStyle 4,5,6 stroke:#000,stroke-width:2px
```

### Introdu√ß√£o

Modelos lineares, como a regress√£o linear, **Linear Discriminant Analysis (LDA)**, regress√£o log√≠stica e hiperplanos separadores, s√£o ferramentas fundamentais tanto para regress√£o quanto para classifica√ß√£o [^4.1]. No entanto, a suposi√ß√£o de que a rela√ß√£o entre as features de entrada e a vari√°vel de resposta √© linear √© raramente v√°lida no mundo real. Em problemas de regress√£o, a fun√ß√£o verdadeira $f(X) = E(Y|X)$ geralmente exibe um comportamento n√£o linear e n√£o aditivo [^5.1]. Representar $f(X)$ atrav√©s de um modelo linear √© uma aproxima√ß√£o conveniente e, por vezes, necess√°ria. √â conveniente porque modelos lineares s√£o f√°ceis de interpretar e representam a aproxima√ß√£o de primeira ordem de Taylor de $f(X)$. √â necess√°rio em casos onde temos poucos dados ($N$ pequeno) ou muitas features ($p$ grande), onde um modelo linear pode ser a √∫nica op√ß√£o vi√°vel para evitar *overfitting* [^5.1]. De forma similar, em classifica√ß√£o, a fronteira de decis√£o Bayes-√≥tima implica que uma transforma√ß√£o mon√≥tona de $Pr(Y = 1|X)$ √© linear em $X$, uma aproxima√ß√£o que raramente se mant√©m na realidade [^5.1].

Este cap√≠tulo explora m√©todos populares para superar as limita√ß√µes da linearidade, concentrando-se na ideia de aumentar/substituir o vetor de entrada $X$ por vari√°veis adicionais que s√£o transforma√ß√µes de $X$. Modelos lineares s√£o ent√£o aplicados nesse novo espa√ßo de features derivadas [^5.1]. O objetivo √©, portanto, criar modelos mais flex√≠veis que capturam rela√ß√µes n√£o lineares nos dados mantendo a interpretabilidade e a efici√™ncia computacional dos modelos lineares.

### Conceitos Fundamentais

**Conceito 1: O Problema de Classifica√ß√£o e Limita√ß√µes Lineares**

O problema de classifica√ß√£o envolve a atribui√ß√£o de inst√¢ncias de dados a classes pr√©-definidas. M√©todos lineares, como LDA ou regress√£o log√≠stica, buscam uma fronteira de decis√£o linear que separa as classes. No entanto, dados reais frequentemente apresentam rela√ß√µes complexas e n√£o lineares, onde uma simples linha ou hiperplano n√£o √© capaz de distinguir as classes de forma eficaz [^4.1]. O uso de modelos lineares imp√µe um **vi√©s** ao modelo, limitando sua capacidade de capturar a complexidade dos dados e levando, por vezes, a uma alta vari√¢ncia caso o modelo tente se ajustar a *outliers*. Por exemplo, em problemas com fronteiras de decis√£o intrinsecamente n√£o lineares, como um espiral ou um c√≠rculo, os modelos lineares n√£o atingem uma alta acur√°cia na classifica√ß√£o.

> üí° **Exemplo Num√©rico:**
> Considere um conjunto de dados bidimensional com duas classes dispostas em formato de c√≠rculos conc√™ntricos. Um modelo linear tentar√° encontrar uma linha reta que separe as classes, o que ser√° imposs√≠vel, resultando em classifica√ß√µes incorretas. Um modelo n√£o linear, como um classificador baseado em kernel, seria mais adequado para esse cen√°rio.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LogisticRegression
> from sklearn.svm import SVC
>
> # Generate dummy concentric circles data
> np.random.seed(0)
> X_inner = np.random.randn(100, 2) * 2
> X_outer = np.random.randn(100, 2) * 5 + np.array([0,0])
> X = np.vstack((X_inner, X_outer))
> y = np.array([0]*100 + [1]*100)
>
> # Linear Model
> model_linear = LogisticRegression()
> model_linear.fit(X, y)
>
> # Non-linear model
> model_nonlinear = SVC(kernel='rbf', gamma='auto')
> model_nonlinear.fit(X,y)
>
> # Create a mesh to plot decision boundaries
> h = .02
> x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
> y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
> xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
>                      np.arange(y_min, y_max, h))
>
> # Plot decision boundaries
> Z_linear = model_linear.predict(np.c_[xx.ravel(), yy.ravel()])
> Z_linear = Z_linear.reshape(xx.shape)
>
> Z_nonlinear = model_nonlinear.predict(np.c_[xx.ravel(), yy.ravel()])
> Z_nonlinear = Z_nonlinear.reshape(xx.shape)
>
> plt.figure(figsize=(12, 6))
>
> plt.subplot(1, 2, 1)
> plt.contourf(xx, yy, Z_linear, cmap=plt.cm.RdBu, alpha=0.8)
> plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k')
> plt.title('Linear Model (Logistic Regression)')
>
> plt.subplot(1, 2, 2)
> plt.contourf(xx, yy, Z_nonlinear, cmap=plt.cm.RdBu, alpha=0.8)
> plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k')
> plt.title('Non-linear Model (SVM with RBF Kernel)')
>
> plt.show()
> ```
>
> A visualiza√ß√£o mostra que o modelo linear n√£o consegue separar os c√≠rculos, enquanto o modelo n√£o linear o faz com precis√£o.

**Lemma 1:** *A fronteira de decis√£o linear de um modelo linear √© definida pela equa√ß√£o $\beta^T X + \beta_0 = 0$, onde $\beta$ s√£o os coeficientes e $X$ √© o vetor de features.* Este lemma destaca a natureza linear da separa√ß√£o que esses modelos imp√µem. Essa formula√ß√£o linear, embora computacionalmente simples, limita a capacidade do modelo de acomodar distribui√ß√µes de classe mais complexas, que muitas vezes requerem fronteiras n√£o lineares. Esta restri√ß√£o pode levar a uma classifica√ß√£o imprecisa, especialmente em dados do mundo real [^4.3].

```mermaid
graph LR
    subgraph "Linear Decision Boundary"
        direction LR
        A["Decision Boundary Equation"] --> B["Œ≤<sup>T</sup>X + Œ≤<sub>0</sub> = 0"]
        B --> C["Œ≤: Coefficients"]
        B --> D["X: Feature Vector"]
    end
```

**Conceito 2: Linear Discriminant Analysis (LDA)**

LDA √© uma t√©cnica de classifica√ß√£o que assume que as classes seguem uma distribui√ß√£o normal (gaussiana) com a mesma matriz de covari√¢ncia [^4.3]. A fun√ß√£o discriminante linear resultante √© obtida pela proje√ß√£o dos dados em um subespa√ßo que maximiza a separa√ß√£o entre as classes [^4.3.1]. O m√©todo √© dependente da normalidade dos dados e da igualdade das covari√¢ncias entre classes, o que nem sempre √© verdade na pr√°tica [^4.3.2]. Quando essas condi√ß√µes n√£o s√£o satisfeitas, as estimativas de par√¢metros e as decis√µes de classifica√ß√£o podem n√£o ser √≥timas.

> üí° **Exemplo Num√©rico:**
> Considere duas classes com distribui√ß√µes gaussianas, mas com vari√¢ncias diferentes. O LDA, ao assumir vari√¢ncias iguais, n√£o encontrar√° a fronteira de decis√£o √≥tima.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>
> # Generate data with different variances
> np.random.seed(0)
> mean1 = [2, 2]
> cov1 = [[1, 0], [0, 1]]
> X1 = np.random.multivariate_normal(mean1, cov1, 100)
>
> mean2 = [6, 6]
> cov2 = [[3, 0], [0, 3]]
> X2 = np.random.multivariate_normal(mean2, cov2, 100)
>
> X = np.vstack((X1, X2))
> y = np.array([0] * 100 + [1] * 100)
>
> # Fit LDA model
> lda = LinearDiscriminantAnalysis()
> lda.fit(X, y)
>
> # Plot decision boundary
> h = .02
> x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
> y_min, y_max = X[:, 1].min() - 1, Y[:, 1].max() + 1
> xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
>                      np.arange(y_min, y_max, h))
> Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])
> Z = Z.reshape(xx.shape)
>
> plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.8)
> plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k')
> plt.title('LDA with different variances')
> plt.show()
> ```
>
> A visualiza√ß√£o mostra que o LDA tenta separar os dados com uma linha reta, mas devido √† diferen√ßa nas vari√¢ncias, a separa√ß√£o n√£o √© ideal. Um classificador que n√£o assume vari√¢ncias iguais (como o QDA) seria mais adequado nesse caso.

**Corol√°rio 1:** *Sob a suposi√ß√£o de normalidade das classes e igualdade de suas matrizes de covari√¢ncia, a fun√ß√£o discriminante linear do LDA pode ser expressa como uma proje√ß√£o linear dos dados em um subespa√ßo de menor dimens√£o, dado por $w = \Sigma^{-1}(\mu_1 - \mu_2)$, onde $\Sigma$ √© a matriz de covari√¢ncia conjunta e $\mu_1$ e $\mu_2$ s√£o os vetores de m√©dias das classes 1 e 2, respectivamente* [^4.3.1]. Este corol√°rio formaliza a proje√ß√£o linear no contexto da an√°lise discriminante, demonstrando que as decis√µes do LDA s√£o baseadas em uma combina√ß√£o linear das features. Quando as suposi√ß√µes do LDA s√£o violadas, esta proje√ß√£o linear pode n√£o ser eficaz em separar as classes adequadamente [^4.3.2].

```mermaid
graph LR
    subgraph "LDA Discriminant Function"
    direction LR
        A["Discriminant Function"] --> B["w = Œ£<sup>-1</sup>(Œº<sub>1</sub> - Œº<sub>2</sub>)"]
        B --> C["Œ£: Pooled Covariance Matrix"]
        B --> D["Œº<sub>1</sub>: Class 1 Mean Vector"]
        B --> E["Œº<sub>2</sub>: Class 2 Mean Vector"]
     end
```

**Conceito 3: Logistic Regression**

A regress√£o log√≠stica modela a probabilidade de uma inst√¢ncia pertencer a uma classe atrav√©s da fun√ß√£o log√≠stica, transformando o modelo linear em uma probabilidade que varia entre 0 e 1 [^4.4]. Ela utiliza a transforma√ß√£o *logit*, onde $\log(\frac{p(x)}{1-p(x)})$ √© modelado como uma fun√ß√£o linear das features de entrada, onde $p(x)$ representa a probabilidade de pertencer a uma das classes [^4.4.1]. O objetivo √© maximizar a verossimilhan√ßa (likelihood) dos dados, ajustando os par√¢metros do modelo ($\beta$) [^4.4.2]. Apesar da sua utilidade na modelagem de probabilidades, a regress√£o log√≠stica ainda imp√µe uma fronteira de decis√£o linear entre as classes, o que limita sua flexibilidade para modelar dados com rela√ß√µes mais complexas. Ela tamb√©m assume que as classes s√£o linearmente separ√°veis no espa√ßo *logit*, o que nem sempre √© verdadeiro em dados complexos [^4.4.4].

> ‚ö†Ô∏è **Nota Importante**: A regress√£o log√≠stica √© adequada para modelar probabilidades, mas sua limita√ß√£o √© a fronteira de decis√£o linear, que pode ser inadequada quando a separa√ß√£o entre as classes n√£o √© linear [^4.4.1].

> ‚ùó **Ponto de Aten√ß√£o**: Em situa√ß√µes onde as classes s√£o desbalanceadas (uma classe √© muito mais frequente que a outra), pode haver vi√©s no modelo de regress√£o log√≠stica. Ajustes nas probabilidades ou uso de t√©cnicas de balanceamento podem ser necess√°rios nesses casos [^4.4.2].

> ‚úîÔ∏è **Destaque**: As estimativas de par√¢metros nos modelos de LDA e regress√£o log√≠stica s√£o relacionadas, sendo o LDA um caso especial sob algumas suposi√ß√µes. Ambos compartilham a limita√ß√£o de uma fronteira de decis√£o linear [^4.5].

### Regress√£o Linear e M√≠nimos Quadrados para Classifica√ß√£o

```mermaid
graph LR
 subgraph "Linear Regression for Classification"
    direction TB
    A["Encode Classes as Indicators"] --> B["Apply Linear Regression"]
    B --> C["Estimate Coefficients (Œ≤) via Least Squares"]
    C --> D["Apply Decision Rule (e.g., argmax)"]
    D --> E["Classification Result"]
    E --> F["Compare with Probabilistic Methods"]
  end
```

A regress√£o linear pode ser aplicada a problemas de classifica√ß√£o atrav√©s da codifica√ß√£o das classes em uma matriz de indicadores (dummy variables). Por exemplo, para um problema de classifica√ß√£o com $K$ classes, cria-se uma matriz $Y$ de dimens√£o $N \times K$, onde $N$ √© o n√∫mero de inst√¢ncias e $Y_{ik} = 1$ se a inst√¢ncia $i$ pertence √† classe $k$ e 0 caso contr√°rio. O modelo de regress√£o √© ajustado aos dados, usando m√≠nimos quadrados para estimar os coeficientes de cada classe.

A equa√ß√£o do modelo √© dada por:
$$
\hat{Y} = XB
$$
onde $X$ √© a matriz de features, $B$ s√£o os coeficientes estimados para cada classe e $\hat{Y}$ s√£o as predi√ß√µes de cada classe para cada inst√¢ncia. Para classificar, uma nova inst√¢ncia √© alocada √† classe com o maior valor predito em $\hat{Y}$.

Apesar de ser uma abordagem direta, a regress√£o linear para classifica√ß√£o apresenta algumas limita√ß√µes. Uma delas √© a possibilidade de extrapola√ß√µes das predi√ß√µes fora do intervalo $[0, 1]$, o que n√£o √© consistente com probabilidades [^4.2]. Al√©m disso, o m√©todo pode sofrer do problema de *masking*, onde a influ√™ncia de uma classe pode ser suprimida por outra, especialmente quando as classes t√™m distribui√ß√µes com diferentes vari√¢ncias. A regress√£o linear tamb√©m n√£o considera adequadamente as rela√ß√µes entre classes e pode produzir resultados sub√≥timos quando as classes n√£o s√£o linearmente separ√°veis no espa√ßo das features originais.

> üí° **Exemplo Num√©rico:**
> Considere um problema de classifica√ß√£o com tr√™s classes e duas features. Vamos usar dados sint√©ticos para ilustrar como a regress√£o linear tenta classificar:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LinearRegression
>
> # Generate synthetic data for 3 classes
> np.random.seed(0)
> X1 = np.random.randn(50, 2) + np.array([2, 2])
> X2 = np.random.randn(50, 2) + np.array([6, 2])
> X3 = np.random.randn(50, 2) + np.array([4, 6])
> X = np.vstack((X1, X2, X3))
> y = np.array([0] * 50 + [1] * 50 + [2] * 50)
>
> # Create dummy variables for classes
> y_dummy = np.eye(3)[y]
>
> # Fit linear regression model
> model = LinearRegression()
> model.fit(X, y_dummy)
>
> # Predict classes
> y_pred = model.predict(X)
> y_pred_classes = np.argmax(y_pred, axis=1)
>
> # Plot decision regions
> h = .02
> x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
> y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
> xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
>                      np.arange(y_min, y_max, h))
> Z = np.argmax(model.predict(np.c_[xx.ravel(), yy.ravel()]), axis=1)
> Z = Z.reshape(xx.shape)
>
> plt.contourf(xx, yy, Z, cmap=plt.cm.viridis, alpha=0.8)
> plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.viridis, edgecolors='k')
> plt.title('Linear Regression for Classification')
> plt.show()
> ```
>
> A visualiza√ß√£o mostra como a regress√£o linear tenta separar as classes com limites lineares, mas pode n√£o ser a abordagem mais adequada para dados complexos. Al√©m disso, os valores preditos $\hat{Y}$ podem estar fora do intervalo [0, 1], o que n√£o √© ideal para probabilidades.

**Lemma 2:** *Sob certas condi√ß√µes, as proje√ß√µes nos hiperplanos de decis√£o gerados pela regress√£o linear para classifica√ß√£o s√£o equivalentes √†s proje√ß√µes nos hiperplanos gerados por discriminantes lineares.* Este lemma formaliza a conex√£o entre regress√£o de indicadores e LDA, destacando que ambos os m√©todos operam com proje√ß√µes lineares. A equival√™ncia, entretanto, depende das suposi√ß√µes sobre a distribui√ß√£o dos dados e a forma como as classes s√£o codificadas [^4.2].

**Corol√°rio 2:** *Ao codificar as classes como vari√°veis indicadoras, a regress√£o linear busca um hiperplano que separa as classes de forma linear. A proje√ß√£o de novas amostras sobre este hiperplano define a decis√£o de classe.* [^4.3]. Este corol√°rio enfatiza a equival√™ncia entre a proje√ß√£o e o resultado da regress√£o linear, conectando ambos os m√©todos e mostrando que eles s√£o limitados por decis√µes baseadas em proje√ß√µes lineares.

Em compara√ß√£o com a regress√£o log√≠stica, como mencionado em [^4.4], a regress√£o de indicadores pode levar a estimativas de probabilidade menos est√°veis e a extrapola√ß√µes fora do intervalo [0,1]. Entretanto, em certos casos, como apontado em [^4.2], a regress√£o de indicadores √© suficiente e at√© mesmo vantajosa quando o objetivo prim√°rio √© a defini√ß√£o de uma fronteira de decis√£o linear.

### M√©todos de Sele√ß√£o de Vari√°veis e Regulariza√ß√£o em Classifica√ß√£o

```mermaid
graph LR
subgraph "Regularization Techniques"
    direction TB
    A["Classification Model (LDA/Logistic Regression)"] --> B["L1 Regularization (Lasso)"]
    A --> C["L2 Regularization (Ridge)"]
    A --> D["Elastic Net (L1 + L2)"]
    B --> E["Sparsity"]
    C --> F["Coefficient Reduction"]
    D --> G["Combined Sparsity and Reduction"]
    E & F & G --> H["Improved Generalization"]
end
```

A sele√ß√£o de vari√°veis e a regulariza√ß√£o s√£o t√©cnicas cruciais para aprimorar a performance dos modelos de classifica√ß√£o. Essas t√©cnicas visam lidar com a complexidade do modelo, evitando *overfitting* e melhorando a generaliza√ß√£o para dados n√£o observados. Em modelos de classifica√ß√£o linear, como a regress√£o log√≠stica, as penalidades $L_1$ e $L_2$ podem ser adicionadas √† fun√ß√£o de custo para controlar a complexidade do modelo e induzir a esparsidade.

A regulariza√ß√£o $L_1$ adiciona um termo de penalidade proporcional √† soma dos valores absolutos dos coeficientes, que pode ser expresso como:
$$
J(\beta) = \mathcal{L}(\beta) + \lambda \sum_{j=1}^{p} |\beta_j|
$$
onde $\mathcal{L}(\beta)$ √© a fun√ß√£o de custo (verossimilhan√ßa negativa), $\beta_j$ s√£o os coeficientes e $\lambda$ √© o par√¢metro de regulariza√ß√£o. A penalidade $L_1$ induz a esparsidade no modelo, ou seja, alguns coeficientes s√£o for√ßados a zero, resultando na sele√ß√£o de um subconjunto de features mais relevantes [^4.4.4].

A regulariza√ß√£o $L_2$, por outro lado, adiciona um termo de penalidade proporcional ao quadrado da norma dos coeficientes, dado por:
$$
J(\beta) = \mathcal{L}(\beta) + \lambda \sum_{j=1}^{p} \beta_j^2
$$

A penalidade $L_2$ reduz a magnitude dos coeficientes, evitando que o modelo se torne muito sens√≠vel a varia√ß√µes nos dados de treino [^4.5]. Uma combina√ß√£o de $L_1$ e $L_2$, conhecida como Elastic Net, pode ser utilizada para aproveitar os benef√≠cios de ambos os tipos de regulariza√ß√£o [^4.5].

> üí° **Exemplo Num√©rico:**
> Vamos demonstrar o efeito da regulariza√ß√£o $L_1$ (Lasso) em um problema de regress√£o log√≠stica com dados sint√©ticos:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.linear_model import LogisticRegression
> from sklearn.preprocessing import StandardScaler
> from sklearn.pipeline import make_pipeline
>
> # Generate synthetic data with 10 features
> np.random.seed(0)
> X = np.random.randn(100, 10)
> true_coefs = np.array([2, -1, 0.5, 0, 0, 0, 0, 0, 0, 0])
> y = (np.dot(X, true_coefs) > 0).astype(int)
>
> # Model without regularization
> model_no_reg = make_pipeline(StandardScaler(), LogisticRegression(penalty=None))
> model_no_reg.fit(X, y)
> coef_no_reg = model_no_reg.named_steps['logisticregression'].coef_[0]
>
> # Model with L1 regularization
> model_l1 = make_pipeline(StandardScaler(), LogisticRegression(penalty='l1', solver='liblinear', C=0.5))
> model_l1.fit(X, y)
> coef_l1 = model_l1.named_steps['logisticregression'].coef_[0]
>
> # Plot coefficients
> plt.figure(figsize=(10, 6))
> plt.plot(coef_no_reg, marker='o', label='No Regularization')
> plt.plot(coef_l1, marker='x', label='L1 Regularization')
> plt.xlabel('Feature Index')
> plt.ylabel('Coefficient Value')
> plt.title('Effect of L1 Regularization (Lasso)')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> print("Coefficients without regularization:", coef_no_reg)
> print("Coefficients with L1 regularization:", coef_l1)
> ```
>
> A visualiza√ß√£o mostra que a regulariza√ß√£o L1 for√ßa alguns coeficientes a zero, selecionando as features mais relevantes. Os coeficientes impressos confirmam que o modelo L1 apresenta mais coeficientes nulos.

**Lemma 3:** *A penalidade $L_1$ na regress√£o log√≠stica leva a coeficientes esparsos, pois a otimiza√ß√£o da fun√ß√£o de custo penalizada tende a anular os coeficientes menos relevantes.* Este lemma demonstra o efeito pr√°tico da regulariza√ß√£o $L_1$, que √© fundamental para a sele√ß√£o de features. O efeito da esparsidade √© obtido pela natureza n√£o diferenci√°vel da norma $L_1$ na origem, que for√ßa alguns coeficientes a serem exatamente zero [^4.4.4].

```mermaid
graph LR
    subgraph "L1 Regularization and Sparsity"
    direction TB
    A["L1 Penalty: Œª‚àë|Œ≤<sub>j</sub>|"] --> B["Non-differentiable at Œ≤<sub>j</sub> = 0"]
    B --> C["Forces some Œ≤<sub>j</sub> to be exactly 0"]
    C --> D["Induces Sparsity in Coefficients"]
    D --> E["Feature Selection"]
    end
```

**Prova do Lemma 3:** A prova do Lemma 3 envolve demonstrar como a penalidade $L_1$ modifica a fun√ß√£o objetivo da regress√£o log√≠stica. A fun√ß√£o objetivo da regress√£o log√≠stica com penalidade $L_1$ √© dada por:
$$
J(\beta) = - \sum_{i=1}^N [y_i \log p(x_i) + (1-y_i) \log(1-p(x_i))] + \lambda \sum_{j=1}^p |\beta_j|,
$$
onde $p(x_i)$ √© a probabilidade predita para a inst√¢ncia $i$. Para provar o efeito de esparsidade, notamos que a derivada da penalidade $L_1$ em rela√ß√£o a $\beta_j$ √© $\lambda \text{sign}(\beta_j)$, que n√£o √© diferenci√°vel em $\beta_j=0$. Durante a otimiza√ß√£o, se um coeficiente $\beta_j$ est√° pr√≥ximo de zero, o termo de penalidade ir√° empurr√°-lo para exatamente zero, a menos que a contribui√ß√£o da fun√ß√£o de verossimilhan√ßa seja grande o suficiente para compensar a penalidade [^4.4.3]. Este mecanismo induz a sele√ß√£o de features [^4.4.4]. $\blacksquare$

**Corol√°rio 3:** *A esparsidade induzida pela penalidade $L_1$ aumenta a interpretabilidade do modelo, pois apenas um subconjunto das features √© considerado relevante para a classifica√ß√£o.* Este corol√°rio destaca a import√¢ncia da regulariza√ß√£o n√£o apenas para a performance do modelo, mas tamb√©m para a compreens√£o dos fatores que influenciam a classifica√ß√£o [^4.4.5].

> ‚ö†Ô∏è **Ponto Crucial**: A combina√ß√£o das penalidades $L_1$ e $L_2$ (Elastic Net) permite um melhor controle sobre a complexidade do modelo e a indu√ß√£o de esparsidade, aproveitando o melhor de cada t√©cnica de regulariza√ß√£o [^4.5].

### Separating Hyperplanes e Perceptrons

```mermaid
graph LR
    subgraph "Separating Hyperplanes"
        direction TB
        A["Data with Linearly Separable Classes"] --> B["Find Optimal Hyperplane"]
        B --> C["Maximize Margin of Separation"]
        C --> D["Support Vector Points"]
        D --> E["Solution as Linear Combination of Support Vectors"]
    end
```

A ideia de hiperplanos separadores reside na busca por uma superf√≠cie de decis√£o linear que maximize a margem de separa√ß√£o entre as classes. O hiperplano √≥timo √© aquele que est√° o mais distante poss√≠vel das amostras de ambas as classes. Esse conceito leva √† formula√ß√£o de um problema de otimiza√ß√£o que envolve a maximiza√ß√£o da margem, que pode ser resolvida utilizando a dualidade de Wolfe [^4.5.2]. A solu√ß√£o surge como uma combina√ß√£o linear de um subconjunto dos dados, que s√£o os pontos de suporte.

O Perceptron de Rosenblatt √© um algoritmo cl√°ssico para encontrar um hiperplano separador, ajustando os pesos atrav√©s de itera√ß√µes sucessivas baseadas nos erros de classifica√ß√£o [^4.5.1]. Sob certas condi√ß√µes, especialmente se as classes s√£o linearmente separ√°veis, o Perceptron converge para uma solu√ß√£o que separa as classes. No entanto, se os dados n√£o s√£o linearmente separ√°veis, o Perceptron n√£o converge e pode oscilar entre diferentes solu√ß√µes [^4.5.1]. A formula√ß√£o do problema de otimiza√ß√£o dos hiperplanos separadores utiliza conceitos de programa√ß√£o convexa, levando ao uso de algoritmos mais eficientes e com garantias de converg√™ncia, mesmo em casos n√£o linearmente separ√°veis.

### Pergunta Te√≥rica Avan√ßada: Quais as diferen√ßas fundamentais entre a formula√ß√£o de LDA e a Regra de Decis√£o Bayesiana considerando distribui√ß√µes Gaussianas com covari√¢ncias iguais?

**Resposta:**

A An√°lise Discriminante Linear (LDA) e a Regra de Decis√£o Bayesiana s√£o abordagens para classifica√ß√£o que, sob certas condi√ß√µes, se tornam equivalentes quando se assume que as classes seguem distribui√ß√µes Gaussianas com matrizes de covari√¢ncia iguais. O LDA √© um m√©todo que projeta os dados em um subespa√ßo linear de menor dimens√£o, buscando maximizar a separa√ß√£o entre as classes. A Regra de Decis√£o Bayesiana, por sua vez, minimiza o risco de classifica√ß√£o, designando cada amostra √† classe com a maior probabilidade a *posteriori*.

Se assumirmos que as distribui√ß√µes das classes s√£o Gaussianas, com a mesma matriz de covari√¢ncia $\Sigma$ e vetores de m√©dias $\mu_k$, a regra Bayesiana para classificar um ponto $x$ na classe $k$ se traduz em:
$$
\arg \max_k p(x|C_k) P(C_k),
$$
onde $p(x|C_k)$ √© a densidade gaussiana de $x$ sob a classe $C_k$, e $P(C_k)$ √© a probabilidade *a priori* da classe $k$. Em logaritmos, a regra √© equivalente a
$$
\arg \max_k -\frac{1}{2}(x-\mu_k)^T \Sigma^{-1} (x-\mu_k) + \log P(C_k).
$$

Ao expandir a express√£o, vemos que o termo quadr√°tico de $x$ se cancela e ficamos com a fun√ß√£o discriminante linear
$$
\arg \max_k \mu_k^T \Sigma^{-1} x - \frac{1}{2}\mu_k^T\Sigma^{-1} \mu_k + \log P(C_k).
$$
Este resultado mostra que, sob a hip√≥tese de distribui√ß√µes gaussianas e covari√¢ncias iguais, a regra de decis√£o Bayesiana tamb√©m resulta em fronteiras de decis√£o lineares, id√™nticas √†quelas encontradas pelo LDA [^4.3]. Assim, o LDA pode ser visto como uma forma de implementar a regra Bayesiana sob certas suposi√ß√µes [^4.3].

```mermaid
graph LR
    subgraph "Equivalence of LDA and Bayesian Decision"
    direction TB
    A["Bayesian Decision Rule with Gaussian Data"] --> B["Maximizing p(x|C<sub>k</sub>)P(C<sub>k</sub>)"]
    B --> C["Expanding Gaussian Density and Log Likelihood"]
    C --> D["Canceling Quadratic Term"]
    D --> E["Resulting Linear Discriminant Function"]
    E --> F["LDA Decision Boundary"]
    end
```

**Lemma 4:** *Sob a condi√ß√£o de distribui√ß√µes gaussianas com matrizes de covari√¢ncia iguais, as fronteiras de decis√£o obtidas pelo LDA s√£o id√™nticas √†s fronteiras de decis√£o Bayesiana.* Este lemma formaliza a equival√™ncia entre os dois m√©todos em um contexto espec√≠fico. A deriva√ß√£o mostra como as suposi√ß√µes simplificam a regra Bayesiana e a levam a uma fun√ß√£o discriminante linear id√™ntica √† do LDA [^4.3], [^4.3.3].

**Corol√°rio 4:** *Ao remover a suposi√ß√£o de igualdade das covari√¢ncias, o LDA n√£o se aplica mais. A Regra de Decis√£o Bayesiana ainda se aplica, e as fronteiras de decis√£o resultantes se tornam quadr√°ticas (Quadratic Discriminant Analysis - QDA), permitindo maior flexibilidade na separa√ß√£o de classes [^4.3].*

> ‚ö†Ô∏è **Ponto Crucial**: A igualdade ou n√£o das matrizes de covari√¢ncia define se a fronteira de decis√£o √© linear (LDA) ou quadr√°tica (QDA), mostrando como as suposi√ß√µes modelam a separa√ß√£o entre as classes [^4.3.1], [^4.3.3].

As diferen√ßas entre o LDA e a regra Bayesiana surgem quando as suposi√ß√µes sobre a distribui√ß√£o dos dados s√£o violadas. O LDA pode apresentar resultados sub√≥timos caso a condi√ß√£o de covari√¢ncias iguais n√£o seja satisfeita ou quando as distribui√ß√µes n√£o s√£o aproximadamente gaussianas. Em situa√ß√µes com dados complexos e n√£o lineares, outros m√©todos de classifica√ß√£o s√£o necess√°rios.

### Conclus√£o

Modelos lineares, embora simples e computacionalmente eficientes, t√™m limita√ß√µes inerentes na modelagem de rela√ß√µes n√£o lineares e na separa√ß√£o de classes com padr√µes complexos. Este cap√≠tulo explorou m√©todos como *basis expansions*, splines, regulariza√ß√£o e outras t√©cnicas para superar tais limita√ß√µes, oferecendo alternativas flex√≠veis e capazes de capturar a complexidade dos dados do mundo real. As t√©cnicas aqui abordadas permitem, portanto, a constru√ß√£o de modelos com maior poder preditivo e que se adequam a cen√°rios mais desafiadores na √°rea de Aprendizado de M√°quina.

### Footnotes

[^4.1]: "We have already made use of models linear in the input features, both for regression and classification. Linear regression, linear discriminant analysis, logistic regression and separating hyperplanes all rely on a linear model." *(Trecho de <Basis Expansions and Regularization>)*

[^4.2]: "In regression problems, f(X) = E(Y|X) will typically be nonlinear and nonadditive in X, and representing f(X) by a linear model is usually a convenient, and sometimes a necessary, approximation. " *(Trecho de <Basis Expansions and Regularization>)*

[^4.3]: " Likewise in classification, a linear, Bayes-optimal decision boundary implies that some monotone transformation of Pr(Y = 1|X) is linear in X." *(Trecho de <Basis Expansions and Regularization>)*

[^4.3.1]: "Convenient because a linear model is easy to interpret, and is the first-order Taylor approximation to f(X)." *(Trecho de <Basis Expansions and Regularization>)*

[^4.3.2]: "Sometimes necessary, because with N small and/or p large, a linear model might be all we are able to fit to the data without overfitting. " *(Trecho de <Basis Expansions and Regularization>)*

[^4.3.3]: "In this chapter and the next we discuss popular methods for moving beyond linearity." *(Trecho de <Basis Expansions and Regularization>)*

[^4.4]: "The core idea in this chapter is to augment/replace the vector of inputs X with additional variables, which are transformations of X, and then use linear models in this new space of derived input features. " *(Trecho de <Basis Expansions and Regularization>)*

[^4.4.1]: "Denote by hm(X) : IRP ‚Üí IR the mth transformation of X, m = 1, . . ., M. We then model" *(Trecho de <Basis Expansions and Regularization>)*

[^4.4.2]: " f(X) =  \sum_{m=1}^{M}  \beta_m h_m(X)" *(Trecho de <Basis Expansions and Regularization