## 6.9 Considerações Computacionais sobre Métodos de Suavização Kernel

### Introdução
Este capítulo aborda os custos computacionais associados aos métodos de suavização kernel, um aspecto crucial para a aplicação prática dessas técnicas. A suavização kernel e a estimação de densidade são métodos *memory-based* [^26], o que significa que o modelo é essencialmente todo o conjunto de dados de treinamento e o ajuste é realizado no momento da avaliação ou predição. Essa característica pode tornar essa classe de métodos inviável para muitas aplicações em tempo real [^26].

### Conceitos Fundamentais
O custo computacional para ajustar em uma única observação $x_0$ é de $O(N)$ *flops* [^26], onde $N$ é o número de pontos de dados no conjunto de treinamento. Essa complexidade surge da necessidade de calcular os pesos do kernel para cada ponto de dados em relação a $x_0$ e, em seguida, calcular a média ponderada.

No entanto, implementações populares de regressão local usam esquemas de triangulação para reduzir as computações [^26]. Esses esquemas calculam o ajuste exatamente em $M$ locais cuidadosamente escolhidos, o que tem um custo de $O(NM)$ [^26]. Em seguida, eles usam técnicas de *blending* para interpolar o ajuste em outros locais, o que custa $O(M)$ por avaliação [^26]. Tipicamente, $M \sim O(\log N)$ [^26].

Os parâmetros de suavização (smoothing parameters), como $\lambda$, são tipicamente determinados offline usando validação cruzada, incorrendo em um custo de $O(N^2)$ [^26]. Esse custo surge da necessidade de avaliar o modelo em várias partições dos dados para encontrar o valor ideal de $\lambda$.

Em comparação, uma expansão em $M$ funções de base custa $O(M)$ para uma avaliação, e tipicamente $M \sim O(\log N)$ [^26]. Os métodos de função de base têm um custo inicial de pelo menos $O(NM^2 + M^3)$ [^26].

### Conclusão
A escolha entre suavização kernel e métodos de função de base depende das características específicas do problema e das restrições computacionais. Se o tempo de avaliação é crítico e o conjunto de dados é grande, os métodos de função de base podem ser mais adequados, apesar do custo inicial mais alto. Se a interpretabilidade e a flexibilidade são mais importantes, a suavização kernel pode ser preferível, desde que o custo computacional seja gerenciável. As implementações eficientes de regressão local, como aquelas que usam esquemas de triangulação, podem ajudar a reduzir o custo computacional da suavização kernel.

### Referências
[^26]: Página 216, Capítulo 6, "Kernel Smoothing Methods"
<!-- END -->