## Regressão Polinomial Local

### Introdução
Expandindo a regressão linear local, a regressão polinomial local generaliza o conceito para ajustar polinômios locais de qualquer grau $d$ [^1]. Esta abordagem visa reduzir o *bias*, embora à custa de aumentar a variância [^1]. A análise assintótica sugere que polinômios de grau ímpar são preferíveis aos de grau par [^1]. Este capítulo aprofunda-se nos detalhes da regressão polinomial local, explorando suas nuances matemáticas e suas implicações práticas.

### Conceitos Fundamentais

A regressão polinomial local estende a regressão linear local ajustando um polinômio de grau *d* em uma vizinhança local de cada ponto $x_0$. O modelo é definido como:

$$\
\hat{f}(x_0) = \hat{\alpha}(x_0) + \sum_{j=1}^{d} \hat{\beta}_j(x_0) x^j
$$

onde $\hat{\alpha}(x_0)$ e $\hat{\beta}_j(x_0)$ são os coeficientes estimados do polinômio local no ponto $x_0$. Esses coeficientes são obtidos minimizando a soma ponderada dos quadrados dos resíduos:

$$\
\min_{\alpha(x_0), \beta_j(x_0)} \sum_{i=1}^{N} K_{\lambda}(x_0, x_i) [y_i - \alpha(x_0) - \sum_{j=1}^{d} \beta_j(x_0)x_i^j]^2
$$

Aqui, $K_{\lambda}(x_0, x_i)$ é uma função kernel que atribui pesos aos pontos $x_i$ com base em sua proximidade ao ponto de avaliação $x_0$ [^1]. O parâmetro $\lambda$ controla a largura da vizinhança local [^1].

O uso de polinômios de grau superior permite reduzir o *bias* da estimativa, uma vez que o *bias* terá apenas componentes de grau $d+1$ e superiores [^1]. No entanto, essa redução de *bias* tem um custo: o aumento da variância da estimativa [^1].

**Observação:** A escolha do grau do polinômio *d* envolve um *trade-off* entre *bias* e variância. Polinômios de grau mais alto podem reduzir o *bias*, mas também aumentam a variância, tornando a estimativa mais sensível aos dados de treinamento [^1].

A Figura 6.5 [^1] ilustra a regressão quadrática local, mostrando como ela pode corrigir o *bias* em regiões de curvatura da função verdadeira, um fenômeno conhecido como *trimming the hills and filling the valleys* [^1].

**Lemma 1:** *A expansão de Ef(x₀) mostra que o bias terá apenas componentes de grau d+1 e superior.*

*Prova:*

Como visto na equação (6.10) [^1], a expansão de $Ef(x_0)$ é dada por:

$$\
Ef(x_0) = f(x_0) \sum_{i=1}^{N} l_i(x_0) + f'(x_0) \sum_{i=1}^{N} (x_i - x_0)l_i(x_0) + \frac{f''(x_0)}{2} \sum_{i=1}^{N} (x_i - x_0)^2 l_i(x_0) + R
$$

Para a regressão polinomial local de grau *d*, os termos até a derivada de ordem *d* são anulados, deixando o *bias* dependendo apenas dos termos de ordem $d+1$ e superiores. $\blacksquare$

A variância da estimativa é dada por:

$$\
Var(\hat{f}(x_0)) = \sigma^2 ||l(x_0)||^2
$$

onde $l(x_0)$ é o vetor de pesos do kernel equivalente no ponto $x_0$ e $\sigma^2$ é a variância do ruído [^1]. Exercício 6.3 [^1] demonstra que $||l(x_0)||$ aumenta com *d*, indicando um aumento na variância com o aumento do grau do polinômio.

### Conclusão

A regressão polinomial local oferece uma abordagem flexível para estimar funções de regressão, permitindo o ajuste de polinômios locais de qualquer grau [^1]. A escolha do grau do polinômio envolve um *trade-off* entre *bias* e variância, e a análise assintótica sugere que polinômios de grau ímpar podem ser preferíveis aos de grau par [^1]. Embora a regressão polinomial local possa reduzir o *bias*, ela também pode aumentar a variância, tornando a estimativa mais sensível aos dados de treinamento [^1]. A escolha do grau ideal do polinômio depende das características específicas dos dados e dos objetivos da análise.

### Referências
[^1]: Página 191-198 do documento fornecido.
<!-- END -->