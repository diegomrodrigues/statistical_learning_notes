## Kernel Smoothing: Estimando a Função de Regressão por Ajuste Local

### Introdução
Este capítulo explora os **métodos de suavização de kernel** (*kernel smoothing*), uma classe de técnicas de regressão que estimam a função de regressão $f(X)$ ajustando modelos simples localmente em cada ponto de consulta $x_0$ [^1]. Esses métodos oferecem flexibilidade na estimativa da função de regressão sobre o domínio $\mathbb{R}^p$ [^1]. Em contraste com o uso de kernels para calcular produtos internos em espaços de características de alta dimensão para modelagem não linear regularizada, aqui os kernels servem principalmente como um dispositivo para localização [^1].

### Conceitos Fundamentais

A ideia central por trás dos métodos de suavização de kernel é usar apenas as observações próximas ao ponto alvo $x_0$ para ajustar o modelo simples, garantindo que a função estimada resultante $\hat{f}(X)$ seja suave em $\mathbb{R}^p$ [^1]. Isso é alcançado através de uma função de ponderação ou *kernel* $K_\lambda(x_0, x_i)$, que atribui pesos a cada observação $x_i$ com base em sua distância de $x_0$ [^1]. O parâmetro $\lambda$ controla a largura da vizinhança, determinando a influência das observações vizinhas no ajuste local [^1].

**Características Principais:**

*   **Localização:** Os kernels atuam como funções de ponderação, dando mais peso às observações próximas ao ponto de consulta [^1].
*   **Suavização:** Ao ponderar as observações, os métodos de kernel produzem estimativas suaves da função de regressão [^1].
*   **Flexibilidade:** Esses métodos podem se adaptar a diferentes formas de funções de regressão, sem impor fortes suposições paramétricas [^1].
*   **Pouco Treinamento:** Requerem pouco ou nenhum treinamento. A maior parte do trabalho é realizada no tempo de avaliação [^1].

**Kernel $K_\lambda(x_0, x_i)$**: É uma função que quantifica a proximidade entre o ponto de consulta $x_0$ e os pontos de dados $x_i$ [^1]. O parâmetro $\lambda$ controla a largura ou a escala do kernel, definindo o quão longe um ponto de dados precisa estar para influenciar a estimativa no ponto de consulta [^1].

**Estimativa de Kernel:**
A estimativa da função de regressão $f(x_0)$ é dada por uma média ponderada das observações vizinhas, onde os pesos são determinados pelo kernel [^3]:

$$\hat{f}(x_0) = \frac{\sum_{i=1}^{N} K_\lambda(x_0, x_i)y_i}{\sum_{i=1}^{N} K_\lambda(x_0, x_i)}$$

onde $N$ é o número total de observações, $y_i$ são os valores da variável resposta, e $K_\lambda(x_0, x_i)$ são os pesos do kernel [^3].

**Tipos de Kernel:**

*   **Epanechnikov:** Um kernel quadrático com suporte compacto [^3].
    $$\
    K_\lambda(x_0, x) = D\left(\frac{x - x_0}{\lambda}\right), \quad D(t) = \begin{cases} \frac{3}{4}(1 - t^2) & \text{se } |t| \leq 1 \\\\ 0 & \text{caso contrário} \end{cases}
    $$
*   **Tri-cube:** Um kernel compacto com duas derivadas contínuas no limite de seu suporte [^4].
    $$\
    D(t) = \begin{cases} (1 - |t|^3)^3 & \text{se } |t| \leq 1 \\\\ 0 & \text{caso contrário} \end{cases}
    $$
*   **Gaussiano:** Um kernel não compacto com suporte infinito [^4].
    $$\
    D(t) = \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}}
    $$

**Seleção de $\lambda$:**
A escolha do parâmetro de suavização $\lambda$ é crucial. Um $\lambda$ grande resulta em baixa variância, mas alto *bias*, enquanto um $\lambda$ pequeno leva a alta variância e baixo *bias* [^3]. Métodos como validação cruzada (*cross-validation*) são usados para selecionar um valor apropriado para $\lambda$ [^9].

### Suavização de Kernel Unidimensional

Em uma dimensão, a suavização de kernel envolve calcular uma média ponderada dos valores da variável resposta $y_i$ para os pontos de dados $x_i$ que estão próximos ao ponto alvo $x_0$ [^2]. A escolha do kernel e do parâmetro de largura de banda $\lambda$ afeta a suavidade e a precisão da função de regressão estimada [^3].

**Média do Vizinho Mais Próximo:**

Uma abordagem simples é a média do vizinho mais próximo (*nearest-neighbor average*), onde a estimativa em $x_0$ é a média dos $k$ pontos mais próximos [^2]:

$$\hat{f}(x) = \text{Ave}(y_i | x_i \in N_k(x))$$

onde $N_k(x)$ é o conjunto dos $k$ pontos mais próximos de $x$ [^2].

**Kernel de Nadaraya-Watson:**

Uma melhoria em relação à média do vizinho mais próximo é o estimador de kernel de Nadaraya-Watson, que usa uma função kernel para ponderar os pontos de dados com base em sua distância do ponto alvo [^2]:

$$\hat{f}(x_0) = \frac{\sum_{i=1}^{N} K_\lambda(x_0, x_i)y_i}{\sum_{i=1}^{N} K_\lambda(x_0, x_i)}$$

onde $K_\lambda(x_0, x_i)$ é a função kernel e $\lambda$ é o parâmetro de largura de banda [^3].

**Regressão Linear Local:**

A regressão linear local é uma técnica mais avançada que ajusta um modelo linear ponderado localmente aos dados [^5]. Isso pode reduzir o *bias* nas bordas do domínio, onde a média ponderada pode ser tendenciosa [^5]. O modelo é ajustado resolvendo um problema de mínimos quadrados ponderados em cada ponto alvo $x_0$ [^5]:

$$\min_{\alpha(x_0), \beta(x_0)} \sum_{i=1}^{N} K_\lambda(x_0, x_i) [y_i - \alpha(x_0) - \beta(x_0)x_i]^2$$

A estimativa é então dada por $\hat{f}(x_0) = \alpha(x_0) + \beta(x_0)x_0$ [^5].

**Regressão Polinomial Local:**

Expandindo a regressão linear local, a regressão polinomial local ajusta um modelo polinomial de grau $d$ ponderado localmente aos dados [^7]:

$$\min_{\alpha(x_0), \beta_j(x_0)} \sum_{i=1}^{N} K_\lambda(x_0, x_i) \left[y_i - \alpha(x_0) - \sum_{j=1}^{d} \beta_j(x_0)x_i^j\right]^2$$

Isso pode reduzir ainda mais o *bias*, mas aumenta a variância [^7].

### Regressão Local em $\mathbb{R}^p$

Os métodos de suavização de kernel podem ser generalizados para dimensões mais altas, onde o kernel pondera as observações em um espaço $p$-dimensional [^10]. A regressão linear local ajusta um hiperplano localmente em $X$ por mínimos quadrados ponderados, com pesos fornecidos por um kernel $p$-dimensional [^10].

**Kernel Esférico:**

Um kernel comum em dimensões mais altas é o kernel esférico, que é uma função radial da distância entre o ponto alvo $x_0$ e os pontos de dados $x_i$ [^10]:

$$\
K_\lambda(x_0, x) = D\left(\frac{||x - x_0||}{\lambda}\right)
$$

onde $|| \cdot ||$ é a norma Euclidiana [^10].

**Regressão Polinomial Local:**
Em cada $x_0 \in \mathbb{R}^p$, resolve-se:

$$\min_{\beta(x_0)} \sum_{i=1}^{N} K_\lambda(x_0, x_i) (y_i - b(x_i)^T\beta(x_0))^2$$

para produzir o ajuste $\hat{f}(x_0) = b(x_0)^T\beta(x_0)$ [^10]. Aqui, $b(X)$ é um vetor de termos polinomiais em $X$ de grau máximo $d$ [^10].

### Conclusão

Os métodos de suavização de kernel fornecem uma abordagem flexível e poderosa para estimar funções de regressão [^1]. Ao ajustar modelos simples localmente, esses métodos podem se adaptar a uma ampla gama de formas funcionais sem impor fortes suposições paramétricas [^1]. A escolha do kernel, o parâmetro de suavização e o grau do modelo polinomial local afetam o *trade-off bias-variância*, e a seleção cuidadosa desses parâmetros é essencial para obter bons resultados [^3, 7]. Embora esses métodos possam ser computacionalmente intensivos, eles são uma ferramenta valiosa para análise de dados exploratória e modelagem [^26].

### Referências

[^1]: Capítulo 6, Kernel Smoothing Methods, página 191
[^2]: Capítulo 6.1, One-Dimensional Kernel Smoothers, página 192
[^3]: Capítulo 6.1, One-Dimensional Kernel Smoothers, página 193
[^4]: Capítulo 6, Kernel Smoothing Methods, página 194
[^5]: Capítulo 6.1.1, Local Linear Regression, página 195
[^7]: Capítulo 6.1.2, Local Polynomial Regression, página 197
[^9]: Capítulo 6.2, Selecting the Width of the Kernel, página 199
[^10]: Capítulo 6.3, Local Regression in $IR^p$, página 200
[^26]: Capítulo 6.9, Computational Considerations, página 216
<!-- END -->