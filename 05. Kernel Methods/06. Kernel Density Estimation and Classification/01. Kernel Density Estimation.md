## Kernel Density Estimation

### Introdução
Este capítulo aborda os métodos de *Kernel Smoothing*, com foco em *Kernel Density Estimation (KDE)*, uma técnica não paramétrica para estimar a função de densidade de probabilidade (PDF) de uma variável aleatória [^1]. KDE estima a PDF em um ponto $x_0$ contando observações próximas a $x_0$ com pesos que diminuem com a distância, utilizando um parâmetro de suavização para controlar a largura do kernel [^1]. Este capítulo se baseia em conceitos apresentados anteriormente, como a motivação para a média dos *k*-vizinhos mais próximos como uma estimativa da função de regressão $E(Y|X=x)$ [^2]. Em continuidade ao conceito de suavização, o KDE oferece uma alternativa para a média dos *k*-vizinhos mais próximos, que pode resultar em estimativas descontínuas [^2].

### Conceitos Fundamentais

#### Kernel Density Estimation (KDE)
KDE estima a função de densidade de probabilidade (PDF) pela média de funções kernel centradas em cada ponto de dados [^1]. A estimativa de Parzen é uma versão suavizada disso [^1]. Formalmente, a estimativa de densidade kernel em um ponto $x_0$ é dada por [^1]:
$$\
\hat{f}(x_0) = \frac{1}{N} \sum_{i=1}^{N} K_{\lambda}(x_0, x_i),
$$
onde $N$ é o número de pontos de dados, $K_{\lambda}$ é a função kernel com parâmetro de suavização $\lambda$, e $x_i$ são os pontos de dados [^3]. O kernel $K_{\lambda}(x_0, x)$ atribui um peso a cada ponto $x_i$ com base em sua distância de $x_0$ [^1]. O parâmetro $\lambda$ controla a largura do kernel e, portanto, o grau de suavização [^1]. Um $\lambda$ grande implica em menor variância, mas maior bias, enquanto um $\lambda$ pequeno implica em maior variância, mas menor bias [^3].

#### Kernel Parzen
A estimativa de Parzen é uma versão suavizada da KDE [^1]. Ela usa uma função kernel para suavizar a contribuição de cada ponto de dados para a estimativa de densidade [^1]. Uma escolha comum para o kernel é o kernel Gaussiano [^4]:
$$\
K_{\lambda}(x_0, x) = \phi\left(\frac{|x - x_0|}{\lambda}\right),
$$
onde $\phi$ é a função de densidade Gaussiana padrão [^4].

#### Escolha do Kernel
A escolha do kernel afeta a suavidade e a forma da estimativa de densidade [^1]. Alguns kernels comuns incluem [^3, 4]:
*   **Epanechnikov:** $D(t) = \begin{cases} \frac{3}{4}(1 - t^2) & \text{se } |t| \leq 1 \\ 0 & \text{caso contrário} \end{cases}$ [^3].
*   **Tri-cube:** $D(t) = \begin{cases} (1 - |t|^3)^3 & \text{se } |t| \leq 1 \\ 0 & \text{caso contrário} \end{cases}$ [^4].
*   **Gaussiano:** $D(t) = \phi(t)$, onde $\phi(t)$ é a função de densidade Gaussiana padrão [^4].

#### Seleção da Largura do Kernel (Bandwidth)
A seleção da largura do kernel, $\lambda$, é crucial para o desempenho do KDE [^3]. Um $\lambda$ muito pequeno resulta em uma estimativa com alta variância e pouco bias, enquanto um $\lambda$ muito grande resulta em uma estimativa com baixo variância e alto bias [^3]. Métodos comuns para selecionar $\lambda$ incluem [^9]:
*   **Validação Cruzada (Cross-validation):** Escolher $\lambda$ que minimize o erro de previsão [^9].
*   **Regras de bolso (Rules of thumb):** Usar fórmulas baseadas em características dos dados, como desvio padrão [^3].

#### KDE em Dimensões Maiores
KDE pode ser estendido para dimensões maiores usando kernels multivariados [^10]. Um kernel comum é o kernel Gaussiano produto [^19]:
$$\
f_X(x_0) = \frac{1}{N(2\lambda\pi)^{p/2}} \sum_{i=1}^{N} e^{-\frac{||x_i - x_0||^2}{2\lambda^2}},
$$
onde $p$ é a dimensão do espaço [^19].

#### Naive Bayes Classifier
O classificador Naive Bayes é uma técnica popular que utiliza KDE para estimar as densidades de probabilidade das classes [^20]. Ele assume que as características são independentes dado a classe, o que simplifica a estimativa da densidade [^21]. Apesar dessa suposição simplificadora, o Naive Bayes Classifier frequentemente tem um bom desempenho [^21].

### Conclusão
KDE é uma ferramenta poderosa para estimar a função de densidade de probabilidade de dados [^1]. Ele fornece uma estimativa suavizada da densidade, que pode ser usada para visualização de dados, classificação e outras tarefas [^1]. A escolha do kernel e da largura do kernel afeta o desempenho do KDE, e é importante selecionar esses parâmetros cuidadosamente [^3]. KDE pode ser estendido para dimensões maiores e usado em conjunto com outras técnicas, como o Naive Bayes Classifier [^10, 20]. Como foi mencionado anteriormente, o KDE é um método baseado em memória, o que significa que requer armazenar todo o conjunto de dados de treinamento [^16]. Isso pode torná-lo impraticável para conjuntos de dados grandes [^16].

### Referências
[^1]: Página 191
[^2]: Página 192
[^3]: Página 193
[^4]: Página 194
[^9]: Página 199
[^10]: Página 200
[^16]: Página 216
[^19]: Página 209
[^20]: Página 210
[^21]: Página 211
<!-- END -->