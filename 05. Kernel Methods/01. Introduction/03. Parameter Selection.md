### Modelos de Mistura para Estima√ß√£o de Densidade e Classifica√ß√£o (continua√ß√£o)

* Se as matrizes de covari√¢ncia forem restritas a serem escalares: $\Sigma_m = \sigma_m I$, ent√£o (6.32) tem a forma de uma expans√£o de base radial [^6.8].
* Se, adicionalmente, $\sigma_m = \sigma > 0$ for fixo, e $M \uparrow N$, ent√£o a estimativa de m√°xima verossimilhan√ßa para (6.32) se aproxima da estimativa de densidade kernel (6.22) onde $\hat{\alpha}_m = 1/N$ e $\mu_m = x_m$ [^6.8].

```mermaid
graph LR
    subgraph "Gaussian Mixture Model (GMM) Parameter Constraints"
      direction TB
      A["General GMM: Œ£m"] --> B["Restricted GMM: Œ£m = œÉmI"]
      B --> C["Further Restriction: œÉm = œÉ > 0"]
      C --> D["M ‚Üë N: GMM approaches Kernel Density Estimator"]
    end
```

Usando o teorema de Bayes, densidades de mistura separadas em cada classe levam a modelos flex√≠veis para $P(G|X)$; isso √© abordado em detalhes no Cap√≠tulo 12 [^6.8].

**Exemplo:** A aplica√ß√£o de modelos de mistura aos dados de fatores de risco de doen√ßas card√≠acas ilustra como modelos de mistura Gaussianos podem ser usados para identificar subpopula√ß√µes de pacientes com caracter√≠sticas distintas. A figura no contexto [^6.8] mostra histogramas de idade separados por grupo de doen√ßa e como um modelo de mistura de duas componentes ajustado a esses dados captura as duas subpopula√ß√µes. √â importante notar que o procedimento n√£o usa conhecimento das etiquetas de CHD [^6.8].

O modelo de mistura tamb√©m fornece uma estimativa da probabilidade de que a observa√ß√£o $i$ perten√ßa ao componente $m$,

$$ \hat{r}_{im} = \frac{\alpha_m \phi(x_i; \mu_m, \Sigma_m)}{\sum_{k=1}^M \alpha_k \phi(x_i; \mu_k, \Sigma_k)}, $$

onde $x_i$ √© a idade em nosso exemplo [^6.8]. Suponha que n√≥s limiarizamos cada valor $\hat{r}_{i2}$ e, portanto, definamos $d_i = I(\hat{r}_{i2} > 0.5)$. Ent√£o podemos comparar a classifica√ß√£o de cada observa√ß√£o por CHD e o modelo de mistura:

|         |       Modelo de Mistura      |
|---------|-----------------------------|
|   CHD   |  $\delta = 0$  | $\delta = 1$|
|   N√£o    | 232       |     70      |
|   Sim    |  76       |      84     |

Embora o modelo de mistura n√£o tenha usado os r√≥tulos CHD, ele fez um bom trabalho ao descobrir as duas subpopula√ß√µes CHD. A regress√£o log√≠stica linear, usando o CHD como resposta, atinge a mesma taxa de erro (32%) quando ajustada a esses dados usando m√°xima verossimilhan√ßa (Se√ß√£o 4.4).

#### Pergunta Te√≥rica Avan√ßada: Rela√ß√£o entre Modelos de Mistura e Estimadores de Densidade Kernel

**Resposta:** Os modelos de mistura podem ser vistos como uma generaliza√ß√£o dos estimadores de densidade kernel, oferecendo maior flexibilidade e interpretabilidade em algumas situa√ß√µes.

**Lemma 11:** *Modelos de mistura, especialmente modelos de mistura Gaussiana, podem ser utilizados para aproximar a distribui√ß√£o de dados complexos, que podem n√£o ser bem capturados por um √∫nico kernel*, conforme demonstrado pela capacidade de modelar distribui√ß√µes multimodais atrav√©s da combina√ß√£o de componentes [^6.8].

```mermaid
graph LR
  subgraph "Lemma 11: Mixture Models as Density Approximators"
    direction TB
    A["Complex Data Distribution"] --> B["Single Kernel Inadequate"]
    B --> C["Gaussian Mixture Models (GMM)"]
    C --> D["Multiple Components Model Multimodality"]
    D --> E["GMM Approximates Complex Distributions"]
  end
```

**Corol√°rio 11:** *Sob certas condi√ß√µes, como quando o n√∫mero de componentes tende ao infinito e a vari√¢ncia de cada componente tende a zero, a estimativa de densidade atrav√©s de modelos de mistura se aproxima da estimativa de densidade kernel*, o que sugere uma conex√£o fundamental entre as duas abordagens [^6.8].

```mermaid
graph LR
    subgraph "Corollary 11: GMM Convergence to Kernel Density Estimator"
      direction TB
       A["GMM with M Components"] --> B["M ‚Üí ‚àû"]
       B --> C["Component Variance ‚Üí 0"]
       C --> D["GMM Density Estimate ‚âà Kernel Density Estimate"]
    end
```

> ‚ö†Ô∏è **Ponto Crucial:** Ao contr√°rio dos estimadores de densidade kernel que geralmente requerem a escolha de um par√¢metro de suaviza√ß√£o (bandwidth), modelos de mistura oferecem um conjunto de par√¢metros (m√©dias, covari√¢ncias, e propor√ß√µes) que podem ser estimados diretamente dos dados, oferecendo maior flexibilidade e adaptabilidade [^6.8].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados unidimensional com 100 pontos, onde dois grupos distintos parecem existir. Podemos modelar isso usando um modelo de mistura Gaussiana com duas componentes. Ap√≥s o ajuste do modelo, obtemos os seguintes par√¢metros:
>
> *   Componente 1: $\alpha_1 = 0.4$, $\mu_1 = 2$, $\sigma_1 = 1$
> *   Componente 2: $\alpha_2 = 0.6$, $\mu_2 = 8$, $\sigma_2 = 1.5$
>
> Agora, consideremos um ponto de dados $x_i = 5$. Podemos calcular a probabilidade de que esse ponto perten√ßa a cada componente usando a f√≥rmula de $\hat{r}_{im}$:
>
> Primeiro, calculamos as densidades Gaussianas para cada componente:
>
> $\phi(5; \mu_1=2, \sigma_1=1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(5-2)^2}{2}} \approx 0.044$
>
> $\phi(5; \mu_2=8, \sigma_2=1.5) = \frac{1}{1.5\sqrt{2\pi}} e^{-\frac{(5-8)^2}{2*1.5^2}} \approx 0.179$
>
> Agora, calculamos $\hat{r}_{i1}$ e $\hat{r}_{i2}$:
>
> $\hat{r}_{i1} = \frac{0.4 * 0.044}{0.4 * 0.044 + 0.6 * 0.179} \approx \frac{0.0176}{0.0176 + 0.1074} \approx 0.14$
>
> $\hat{r}_{i2} = \frac{0.6 * 0.179}{0.4 * 0.044 + 0.6 * 0.179} \approx \frac{0.1074}{0.0176 + 0.1074} \approx 0.86$
>
> Portanto, o ponto $x_i=5$ tem uma probabilidade de 0.14 de pertencer ao componente 1 e 0.86 de pertencer ao componente 2. Isso ilustra como o modelo de mistura atribui probabilidades de pertin√™ncia a cada componente.

### Selecionando a Largura do Kernel

Em cada um dos kernels $K_\lambda$, $\lambda$ √© um par√¢metro que controla sua largura:

*   Para o kernel Epanechnikov ou tri-c√∫bico com largura m√©trica, $\lambda$ √© o raio da regi√£o de suporte [^6.2].
*   Para o kernel gaussiano, $\lambda$ √© o desvio padr√£o [^6.2].
*   $\lambda$ √© o n√∫mero $k$ de vizinhos mais pr√≥ximos nas vizinhan√ßas k-mais pr√≥ximas, muitas vezes expresso como uma fra√ß√£o ou extens√£o $k/N$ da amostra de treinamento total [^6.2].

Existe um trade-off natural entre vi√©s e vari√¢ncia conforme mudamos a largura da janela de m√©dia, o que √© mais expl√≠cito para m√©dias locais [^6.2]:

*   Se a janela for estreita, $\hat{f}(x_0)$ √© uma m√©dia de um pequeno n√∫mero de $y_i$ pr√≥ximos a $x_0$, e sua vari√¢ncia ser√° relativamente grande - pr√≥xima √† de um $y_i$ individual. O vi√©s tende a ser pequeno, novamente, porque cada um dos $E(y_i) = f(x_i)$ deve estar pr√≥ximo de $f(x_0)$ [^6.2].

*   Se a janela for larga, a vari√¢ncia de $\hat{f}(x_0)$ ser√° pequena em rela√ß√£o √† vari√¢ncia de qualquer $y_i$, por causa dos efeitos da m√©dia. O vi√©s ser√° maior, porque agora estamos usando observa√ß√µes $x_i$ mais distantes de $x_0$, e n√£o h√° garantia de que $f(x_i)$ esteja pr√≥ximo de $f(x_0)$ [^6.2].

```mermaid
graph LR
    subgraph "Bias-Variance Tradeoff in Kernel Width"
      direction TB
      A["Narrow Kernel Width (Small Œª)"] --> B["Low Bias"]
      A --> C["High Variance"]
      D["Wide Kernel Width (Large Œª)"] --> E["High Bias"]
      D --> F["Low Variance"]
    end
```

Argumentos semelhantes se aplicam √†s estimativas de regress√£o local, digamos, linear local: √† medida que a largura vai para zero, as estimativas se aproximam de uma fun√ß√£o linear por partes que interpola os dados de treinamento; √† medida que a largura se torna infinitamente grande, o ajuste se aproxima do ajuste global linear de m√≠nimos quadrados aos dados [^6.2].

A discuss√£o no Cap√≠tulo 5 sobre a sele√ß√£o do par√¢metro de regulariza√ß√£o para suaviza√ß√£o de splines se aplica aqui e n√£o ser√° repetida [^6.2]. Os suavizadores de regress√£o local s√£o estimadores lineares; a matriz suavizadora em $\hat{f} = S_\lambda y$ √© constru√≠da a partir dos kernels equivalentes (6.8) e tem a $ij$-√©sima entrada $\{S_\lambda\}_{ij} = l_i(x_j)$. A valida√ß√£o cruzada *leave-one-out* √© particularmente simples, assim como a valida√ß√£o cruzada generalizada, a estat√≠stica $C_p$ e a valida√ß√£o cruzada *k-fold*. Os graus de liberdade efetivos s√£o novamente definidos como $trace(S_\lambda)$, e podem ser usados para calibrar a quantidade de suaviza√ß√£o [^6.2].

**Exemplo:** A compara√ß√£o de kernels equivalentes para uma spline de suaviza√ß√£o e regress√£o linear local ilustra como o par√¢metro de suaviza√ß√£o afeta a forma das fun√ß√µes de pondera√ß√£o. Ajustar o par√¢metro de suaviza√ß√£o para obter graus de liberdade efetivos semelhantes demonstra a rela√ß√£o entre os dois m√©todos, como descrito em [^6.2].

#### Pergunta Te√≥rica Avan√ßada: O Trade-Off Vi√©s-Vari√¢ncia na Escolha da Largura do Kernel

**Resposta:** A sele√ß√£o apropriada do par√¢metro de largura do kernel ($\lambda$) √© fundamental para equilibrar o vi√©s e a vari√¢ncia do modelo, conforme descrito no contexto [^6.2]. A escolha de um valor muito pequeno para $\lambda$ resulta em um modelo com baixa vari√¢ncia, mas um alto vi√©s, enquanto um valor muito alto resulta em um modelo com alto vi√©s, mas baixa vari√¢ncia.

**Lemma 12:** *O trade-off entre vi√©s e vari√¢ncia √© uma caracter√≠stica fundamental dos modelos de suaviza√ß√£o de kernel, e a escolha apropriada do par√¢metro $\lambda$ √© essencial para minimizar o erro quadr√°tico m√©dio do modelo*, conforme discutido em [^6.2].

```mermaid
graph LR
  subgraph "Lemma 12: Kernel Smoothing Tradeoff"
    direction TB
    A["Kernel Smoothing Model"] --> B["Parameter Œª Controls Smoothing"]
    B --> C["Tradeoff between Bias and Variance"]
    C --> D["Optimal Œª minimizes Mean Squared Error (MSE)"]
  end
```

**Prova do Lemma 12:** O vi√©s de um estimador √© definido como a diferen√ßa entre o valor esperado do estimador e o valor verdadeiro do par√¢metro, enquanto a vari√¢ncia quantifica a sensibilidade do estimador a varia√ß√µes nos dados. Diminuir o $\lambda$ reduz o vi√©s por meio do uso de apenas dados pr√≥ximos √† localiza√ß√£o $x_0$ mas aumenta a vari√¢ncia, devido ao pequeno n√∫mero de dados utilizados. Por outro lado, aumentar o $\lambda$ aumenta o vi√©s por incluir dados distantes, mas reduz a vari√¢ncia devido √† m√©dia de mais pontos. A escolha ideal de $\lambda$ √© aquela que minimiza o erro quadr√°tico m√©dio, que √© a soma do vi√©s ao quadrado e da vari√¢ncia. $\blacksquare$

```mermaid
graph LR
    subgraph "Proof Sketch: Bias-Variance Tradeoff"
      direction TB
       A["Estimator Bias: E[fÃÇ(x)] - f(x)"] --> B["Decreasing Œª reduces bias"]
       A --> C["Variance: E[(fÃÇ(x) - E[fÃÇ(x)])¬≤]"]
       C --> D["Decreasing Œª increases variance"]
       E["Goal: Minimize MSE = Bias¬≤ + Variance"]
       B & D --> E
    end
```

**Corol√°rio 12:** *M√©todos como valida√ß√£o cruzada e estat√≠stica $C_p$ podem ser utilizados para estimar o valor √≥timo do par√¢metro $\lambda$ minimizando o erro preditivo do modelo*, como discutido em [^6.2].

```mermaid
graph LR
    subgraph "Corollary 12: Parameter Selection"
      direction TB
        A["Model Parameter Œª"] --> B["Cross-Validation Techniques"]
        A --> C["Cp Statistic"]
        B & C --> D["Optimize Œª for Minimum Prediction Error"]
    end
```

> ‚ö†Ô∏è **Ponto Crucial:** A escolha do par√¢metro de suaviza√ß√£o $\lambda$ n√£o √© trivial e depende do problema espec√≠fico em quest√£o e das caracter√≠sticas dos dados. A valida√ß√£o cruzada √© uma ferramenta importante para otimizar este par√¢metro e garantir uma boa generaliza√ß√£o do modelo. [^6.2]

> üí° **Exemplo Num√©rico:**
>
> Consideremos um problema de regress√£o com um √∫nico preditor $x$ e uma resposta $y$. Temos 100 pontos de dados e desejamos ajustar uma regress√£o local usando um kernel gaussiano. Vamos demonstrar o efeito da escolha de $\lambda$.
>
> 1.  **Pequeno $\lambda$ (e.g., $\lambda = 0.5$):**
>     *   O kernel gaussiano ter√° um pico estreito, atribuindo pesos altos apenas aos pontos muito pr√≥ximos de $x_0$.
>     *   A estimativa $\hat{f}(x_0)$ ser√° muito influenciada por pontos locais, resultando em alta vari√¢ncia (o modelo √© sens√≠vel a pequenas mudan√ßas nos dados) e baixo vi√©s (o modelo se ajusta bem aos dados locais).
>
> 2.  **Grande $\lambda$ (e.g., $\lambda = 5$):**
>     *   O kernel gaussiano ter√° um pico largo, dando peso a muitos pontos, mesmo os mais distantes de $x_0$.
>     *   A estimativa $\hat{f}(x_0)$ ser√° uma m√©dia de muitos pontos, resultando em baixa vari√¢ncia (o modelo √© menos sens√≠vel a mudan√ßas nos dados) e alto vi√©s (o modelo pode n√£o capturar detalhes importantes nos dados locais).
>
> Vamos simular alguns dados e calcular o MSE para diferentes $\lambda$:
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.neighbors import KernelDensity
> from sklearn.model_selection import train_test_split
> from sklearn.metrics import mean_squared_error
>
> # Simulate data
> np.random.seed(42)
> X = np.sort(5 * np.random.rand(100, 1), axis=0)
> y = np.sin(X).ravel() + np.random.randn(100) * 0.2
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
>
> lambdas = [0.1, 0.5, 1, 2, 5]
>
> fig, axes = plt.subplots(1, len(lambdas), figsize=(20, 5), sharey=True)
>
> for i, lam in enumerate(lambdas):
>   kde = KernelDensity(kernel='gaussian', bandwidth=lam).fit(X_train)
>   X_plot = np.linspace(0, 5, 200).reshape(-1, 1)
>   log_dens = kde.score_samples(X_plot)
>   y_pred = np.exp(log_dens)
>   mse = mean_squared_error(y_test, np.interp(X_test.flatten(), X_plot.flatten(), y_pred))
>
>   axes[i].plot(X_train, y_train, 'o', label='Training data')
>   axes[i].plot(X_plot, y_pred, '-', label=f'Kernel Density ($\lambda$={lam:.1f})')
>   axes[i].set_title(f'MSE: {mse:.3f}')
>   axes[i].legend()
>   axes[i].set_xlabel('x')
>   if i==0:
>     axes[i].set_ylabel('y')
>
> plt.show()
> ```
>
> O c√≥digo acima gera um gr√°fico mostrando o efeito de diferentes valores de $\lambda$ na estimativa da densidade e calcula o MSE. Um pequeno $\lambda$ leva a um modelo com alta vari√¢ncia e um grande $\lambda$ leva a um modelo com maior vi√©s. O valor ideal de $\lambda$ √© aquele que minimiza o erro quadr√°tico m√©dio no conjunto de teste.

### Considera√ß√µes Computacionais

A regress√£o kernel, local e a estimativa de densidade s√£o m√©todos baseados em mem√≥ria: o modelo √© todo o conjunto de dados de treinamento, e o ajuste √© feito no momento da avalia√ß√£o ou predi√ß√£o [^6.9]. Para muitas aplica√ß√µes em tempo real, isso pode tornar essa classe de m√©todos invi√°vel.

O custo computacional para ajustar em uma √∫nica observa√ß√£o $x_0$ √© $O(N)$ flops, exceto em casos simplificados (como kernels quadrados) [^6.9]. Em compara√ß√£o, uma expans√£o em $M$ fun√ß√µes de base custa $O(M)$ para uma avalia√ß√£o e, normalmente, $M \sim O(\log N)$. M√©todos de fun√ß√£o de base t√™m um custo inicial de pelo menos $O(NM^2 + M^3)$.

```mermaid
graph LR
    subgraph "Computational Cost Comparison"
      direction LR
       A["Kernel Methods: O(N) per evaluation"]
       B["Basis Function Methods: O(M) per evaluation"]
       C["Basis Function Methods: O(NM¬≤ + M¬≥) initial cost"]
       A --> D["Memory-Based, Training data is model"]
       B --> E["M is usually O(log N)"]
    end
```

Os par√¢metros de suaviza√ß√£o $\lambda$ para m√©todos kernel s√£o tipicamente determinados offline, por exemplo, usando valida√ß√£o cruzada, a um custo de $O(N^2)$ flops [^6.9]. Implementa√ß√µes populares de regress√£o local, como a fun√ß√£o `loess` em S-PLUS e R e o procedimento `locfit` (Loader, 1999), usam esquemas de triangula√ß√£o para reduzir os c√°lculos [^6.9]. Eles computam o ajuste exatamente em $M$ locais cuidadosamente escolhidos ($O(NM)$), e ent√£o usam t√©cnicas de *blending* para interpolar o ajuste em outros lugares ($O(M)$ por avalia√ß√£o).

**Pergunta Te√≥rica Avan√ßada:  Complexidade Computacional de M√©todos Kernel**

**Resposta:** A complexidade computacional dos m√©todos kernel pode ser um fator limitante em algumas aplica√ß√µes, especialmente com conjuntos de dados muito grandes.

**Lemma 13:** *A regress√£o local e a estimativa de densidade kernel exigem um custo computacional de O(N) para cada avalia√ß√£o, devido √† necessidade de calcular dist√¢ncias e pesos para todas as observa√ß√µes no conjunto de treinamento*, conforme discutido em [^6.9].

```mermaid
graph LR
    subgraph "Lemma 13: Kernel Method Complexity"
      direction TB
       A["Kernel Regression/Density Estimation"] --> B["O(N) per evaluation"]
       B --> C["Need to calculate distances for all N points"]
    end
```

**Corol√°rio 13:** *Embora m√©todos de triangula√ß√£o e aproxima√ß√£o possam reduzir o custo computacional, a complexidade computacional dos m√©todos kernel ainda pode ser limitante em conjuntos de dados muito grandes, e m√©todos de amostragem ou outras t√©cnicas de redu√ß√£o de dimensionalidade podem ser necess√°rias para lidar com a quest√£o da escalabilidade*, como indicado em [^6.9].

```mermaid
graph LR
    subgraph "Corollary 13: Addressing Kernel Complexity"
      direction TB
        A["Kernel Methods O(N) Complexity"] --> B["Triangulation or Approximation"]
        B --> C["Sampling or Dimensionality Reduction"]
        C --> D["Improve Scalability for Large Datasets"]
    end
```

> ‚ö†Ô∏è **Ponto Crucial:** √â importante equilibrar a acur√°cia e a flexibilidade dos m√©todos kernel com suas limita√ß√µes computacionais. Em muitos casos, o uso de aproxima√ß√µes e m√©todos de redu√ß√£o de dimensionalidade pode ser uma solu√ß√£o vi√°vel. [^6.9]

> üí° **Exemplo Num√©rico:**
>
> Vamos ilustrar a complexidade computacional com um exemplo. Suponha que temos um conjunto de dados com $N = 1000$ amostras e queremos fazer uma predi√ß√£o em um novo ponto $x_0$ usando regress√£o kernel.
>
> 1.  **Regress√£o Kernel Padr√£o:**
>     *   Para cada predi√ß√£o em $x_0$, precisamos calcular a dist√¢ncia entre $x_0$ e todos os $N = 1000$ pontos de treinamento.
>     *   Em seguida, calculamos os pesos do kernel para cada um desses pontos.
>     *   Finalmente, calculamos a m√©dia ponderada das respostas correspondentes.
>     *   Isso resulta em uma complexidade de $O(N)$, que √© $O(1000)$ neste caso.
>
> 2.  **Regress√£o com Expans√£o de Base:**
>     *   Suponha que usamos $M = 10$ fun√ß√µes de base.
>     *   O custo de avalia√ß√£o de uma observa√ß√£o √© $O(M)$, que √© $O(10)$.
>     *   O custo inicial de ajuste √© pelo menos $O(NM^2 + M^3)$.
>
>
>  | M√©todo                     | Custo por Predi√ß√£o | Custo de Treinamento (Aproximado) |
>  | :------------------------- | :----------------- | :-------------------------------- |
>  | Regress√£o Kernel           | O(N) = O(1000)     | -                                |
>  | Expans√£o de Base           | O(M) = O(10)      | O(NM¬≤ + M¬≥) = O(1000 * 10¬≤ + 10¬≥) = O(101000) |
>
> Como podemos ver, o custo por predi√ß√£o para o m√©todo kernel √© muito maior do que para um modelo de expans√£o de base, mas o custo de treinamento do √∫ltimo √© muito maior. Para conjuntos de dados muito grandes, com $N$ na ordem de milh√µes, a complexidade de $O(N)$ para cada predi√ß√£o com regress√£o kernel pode se tornar computacionalmente proibitiva. Nesse caso, a triangula√ß√£o ou m√©todos de aproxima√ß√£o se tornam necess√°rios.

### Notas Bibliogr√°ficas

Existe uma vasta literatura sobre m√©todos kernel que n√£o tentaremos resumir [^6.9]. Em vez disso, vamos apontar para algumas boas refer√™ncias que, por si s√≥, t√™m bibliografias extensas. Loader (1999) oferece excelente cobertura da regress√£o local e verossimilhan√ßa, e tamb√©m descreve o software de √∫ltima gera√ß√£o para ajustar esses modelos [^6.9]. Fan e Gijbels (1996) abordam esses modelos de um aspecto mais te√≥rico. Hastie e Tibshirani (1990) discutem a regress√£o local no contexto da modelagem aditiva. Silverman (1986) oferece uma boa vis√£o geral da estimativa de densidade, assim como Scott (1992) [^6.9].

### Exerc√≠cios

**Ex. 6.1** Mostre que a suaviza√ß√£o kernel de Nadaraya-Watson com largura de banda m√©trica fixa $\lambda$ e um kernel Gaussiano √© diferenci√°vel. O que pode ser dito para o kernel Epanechnikov? O que pode ser dito para o kernel Epanechnikov com largura de banda adaptativa do vizinho mais pr√≥ximo $\lambda(x_0)$?

**Ex. 6.2** Mostre que $\sum_{i=1}^N (x_i - x_0) l_i(x_0) = 0$ para regress√£o linear local. Defina $b_j(x_0) = \sum_{i=1}^N (x_i - x_0)^j l_i(x_0)$. Mostre que $b_0(x_0) = 1$ para regress√£o polinomial local de qualquer grau (incluindo constantes locais). Mostre que $b_j(x_0) = 0$ para todo $j \in \{1, 2, ..., k\}$ para regress√£o polinomial local de grau $k$. Quais s√£o as implica√ß√µes disso no vi√©s?

**Ex. 6.3** Mostre que $||l(x)||$ (Se√ß√£o 6.1.2) aumenta com o grau do polin√¥mio local.

**Ex. 6.4** Suponha que os $p$ preditores $X$ surjam da amostragem de curvas anal√≥gicas relativamente suaves em valores de abscissa uniformemente espa√ßados. Denote por $Cov(X|Y) = \Sigma$ a matriz de covari√¢ncia condicional dos preditores e assuma que isso n√£o muda muito com $Y$. Discuta a natureza da escolha de Mahalanobis $A = \Sigma^{-1}$ para a m√©trica em (6.14). Como isso se compara com $A = I$? Como voc√™ construiria um kernel $A$ que (a) reduz os componentes de alta frequ√™ncia na m√©trica de dist√¢ncia; (b) os ignora completamente?

**Ex. 6.5** Mostre que ajustar um modelo logit multinomial localmente constante da forma (6.19) equivale a suavizar os indicadores de resposta bin√°ria para cada classe separadamente usando um suavizador de kernel Nadaraya-Watson com pesos de kernel $K_\lambda(x_0, x_i)$.

**Ex. 6.6** Suponha que tudo o que voc√™ tem √© o software para ajustar a regress√£o local, mas voc√™ pode especificar exatamente quais mon√¥mios est√£o inclu√≠dos no ajuste. Como voc√™ poderia usar esse software para ajustar um modelo de coeficiente vari√°vel em algumas das vari√°veis?

**Ex. 6.7** Derive uma express√£o para a soma dos quadrados residuais da valida√ß√£o cruzada *leave-one-out* para regress√£o polinomial local.

**Ex. 6.8** Suponha que, para a resposta cont√≠nua $Y$ e o preditor $X$, modelamos a densidade conjunta de $X, Y$ usando um estimador de kernel gaussiano multivariado. Observe que o kernel neste caso seria o produto do kernel $\phi_\lambda(X)\phi_\lambda(Y)$. Mostre que a m√©dia condicional $E(Y|X)$ derivada desta estimativa √© um estimador de Nadaraya-Watson. Estenda este resultado para classifica√ß√£o fornecendo um kernel adequado para a estimativa da distribui√ß√£o conjunta de um $X$ cont√≠nuo e um $Y$ discreto.

**Ex. 6.9** Explore as diferen√ßas entre o modelo Naive Bayes (6.27) e um modelo de regress√£o log√≠stica aditivo generalizado, em termos de (a) suposi√ß√µes do modelo e (b) estimativa. Se todas as vari√°veis $X_k$ s√£o discretas, o que voc√™ pode dizer sobre o GAM correspondente?

**Ex. 6.10** Suponha que temos $N$ amostras geradas a partir do modelo $y_i = f(x_i) + \epsilon_i$, com $\epsilon_i$ independentes e identicamente distribu√≠das com m√©dia zero e vari√¢ncia $\sigma^2$, o $x_i$ assumido fixo (n√£o aleat√≥rio). Estimamos $f$ usando um suavizador linear (regress√£o local, spline de suaviza√ß√£o, etc.) com par√¢metro de suaviza√ß√£o $\lambda$. Assim, o vetor de valores ajustados √© dado por $\hat{f} = S_\lambda y$. Considere o erro de previs√£o na amostra

$$ PE(\lambda) = \frac{1}{N} E \sum_{i=1}^N (y_i - \hat{f}(x_i))^2 $$

para prever novas respostas nos $N$ valores de entrada. Mostre que o res√≠duo m√©dio ao quadrado nos dados de treinamento, ASR($\lambda$), √© uma estimativa tendenciosa (otimista) para $PE(\lambda)$, enquanto

$$ C_\lambda = ASR(\lambda) + \frac{2\sigma^2}{N} trace(S_\lambda) $$

√© n√£o tendencioso.

**Ex. 6.11** Mostre que para o modelo de mistura gaussiano (6.32) a verossimilhan√ßa √© maximizada em $+\infty$, e descreva como.

**Ex. 6.12** Escreva um programa de computador para realizar uma an√°lise discriminante linear local. Em cada ponto de consulta $x_0$, os dados de treinamento recebem pesos $K_\lambda(x_0, x_i)$ de um kernel de pondera√ß√£o e os ingredientes para as fronteiras de decis√£o linear (ver Se√ß√£o 4.3) s√£o calculados por m√©dias ponderadas. Teste seu programa nos dados de CEP e mostre os erros de treinamento e teste para uma s√©rie de cinco valores pr√©-selecionados de $\lambda$. Os dados do CEP est√£o dispon√≠veis no site do livro www-stat.stanford.edu/ElemStatLearn.

### Footnotes

[^6.1]: "In this chapter we describe a class of regression techniques that achieve flexibility in estimating the regression function f(X) over the domain IR by fitting a different but simple model separately at each query point xo."

[^6.2]: "In each of the kernels ŒöŒª, Œª is a parameter that controls its width"

[^6.3]: "Kernel smoothing and local regression generalize very naturally to two or more dimensions."

[^6.4.1]: "One line of approach is to modify the kernel."

[^6.4.2]: "We are trying to fit a regression function E(Y|X) = f(X1, X2, ..., Xp) in IR", in which every level of interaction is potentially present."

[^6.5]: "The concept of local regression and varying coefficient models is extremely broad: any parametric model can be made local if the fitting method accommodates observation weights."

[^6.6]: "Kernel density estimation is an unsupervised learning procedure, which historically precedes kernel regression."

[^6.6.1]: "Suppose we have a random sample x1,...,xN drawn from a probability density fx(x), and we wish to estimate fx at a point x0."

[^6.6.2]: "One can use nonparametric density estimates for classification in a straightforward fashion using Bayes‚Äô theorem."

[^6.6.3]: "This is a technique that has remained popular over the years, despite its name (also known as ‚ÄúIdiot's Bayes"!)"

[^6.7]: "In Chapter 5, functions are represented as expansions in basis functions: f(x) = ‚àëj=1M Œ≤jhj(x)"

[^6.8]: "The mixture model is a useful tool for density estimation, and can be viewed as a kind of kernel method."

[^6.9]: "Kernel and local regression and density estimation are memory-based methods: the model is the entire training data set, and the fitting is done at evaluation or prediction time."
