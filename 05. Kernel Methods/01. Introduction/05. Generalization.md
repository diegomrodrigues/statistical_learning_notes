Obrigado! O cap√≠tulo ficou excelente e bastante completo. O conte√∫do est√° bem estruturado, os conceitos foram explicados em detalhes e as refer√™ncias foram usadas de maneira consistente. As se√ß√µes te√≥ricas avan√ßadas, as provas e os exerc√≠cios elevaram o n√≠vel de profundidade do material. As adi√ß√µes de imagens e diagramas tamb√©m foram bem aplicadas. O cap√≠tulo ficou impec√°vel!

Aqui est√£o alguns exemplos num√©ricos pr√°ticos para ilustrar alguns dos conceitos discutidos, seguindo a estrutura sugerida:

**Exemplos Num√©ricos**

> üí° **Exemplo Num√©rico: C√°lculo de M√≠nimos Quadrados (OLS)**
>
> Vamos considerar um exemplo simples de regress√£o linear com uma √∫nica vari√°vel preditora. Suponha que temos os seguintes dados:
>
> | $x_i$ | $y_i$ |
> |-------|-------|
> | 1     | 2     |
> | 2     | 3     |
> | 3     | 5     |
> | 4     | 6     |
>
> Nosso objetivo √© encontrar a linha que melhor se ajusta a esses dados usando o m√©todo de m√≠nimos quadrados. A equa√ß√£o da linha √© dada por:
>
> $y = \beta_0 + \beta_1 x$
>
> Para encontrar os coeficientes $\beta_0$ e $\beta_1$, precisamos resolver as equa√ß√µes normais:
>
> $\hat{\beta} = (X^TX)^{-1}X^Ty$
>
> Onde:
>
> $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}$  (matriz de projeto com uma coluna de 1s para o intercepto e a coluna de valores x)
>
> $y = \begin{bmatrix} 2 \\ 3 \\ 5 \\ 6 \end{bmatrix}$ (vetor de valores y)
>
> **Passo 1:** Calcule $X^T$:
>
> $X^T = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{bmatrix}$
>
> **Passo 2:** Calcule $X^TX$:
>
> $X^TX = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix} = \begin{bmatrix} 4 & 10 \\ 10 & 30 \end{bmatrix}$
>
> **Passo 3:** Calcule $(X^TX)^{-1}$:
>
> $(X^TX)^{-1} = \frac{1}{(4*30 - 10*10)} \begin{bmatrix} 30 & -10 \\ -10 & 4 \end{bmatrix} = \frac{1}{20} \begin{bmatrix} 30 & -10 \\ -10 & 4 \end{bmatrix} = \begin{bmatrix} 1.5 & -0.5 \\ -0.5 & 0.2 \end{bmatrix}$
>
> **Passo 4:** Calcule $X^Ty$:
>
> $X^Ty = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \\ 5 \\ 6 \end{bmatrix} = \begin{bmatrix} 16 \\ 49 \end{bmatrix}$
>
> **Passo 5:** Calcule $\hat{\beta}$:
>
> $\hat{\beta} = (X^TX)^{-1}X^Ty = \begin{bmatrix} 1.5 & -0.5 \\ -0.5 & 0.2 \end{bmatrix} \begin{bmatrix} 16 \\ 49 \end{bmatrix} = \begin{bmatrix} 1.5*16 - 0.5*49 \\ -0.5*16 + 0.2*49 \end{bmatrix} = \begin{bmatrix} -0.5 \\ 1.8 \end{bmatrix}$
>
> Portanto, $\beta_0 = -0.5$ e $\beta_1 = 1.8$. A equa√ß√£o da linha de regress√£o √©:
>
> $y = -0.5 + 1.8x$
>
> **Interpreta√ß√£o:** Para cada unidade de aumento em $x$, esperamos um aumento de 1.8 unidades em $y$, e o intercepto √© -0.5.
>
> **Visualiza√ß√£o:**
>
> ```mermaid
> graph TD
>     subgraph "OLS Calculation"
>         direction TB
>         A["Input Data: (X, y)"] --> B["Calculate X^T"];
>         B --> C["Calculate X^TX"];
>         C --> D["Calculate (X^TX)^-1"];
>         A --> E["Calculate X^Ty"];
>         D --> F["Calculate Œ≤ÃÇ: (X^TX)^-1 * X^Ty"];
>         E --> F;
>         F --> G["Regression Equation: y = Œ≤_0 + Œ≤_1x"];
>     end
> ```
>
> **An√°lise de Res√≠duos:**
>
> Podemos calcular os valores preditos $\hat{y}$ e os res√≠duos $e_i = y_i - \hat{y}_i$:
>
> | $x_i$ | $y_i$ | $\hat{y}_i$ | $e_i$ |
> |-------|-------|------------|-------|
> | 1     | 2     | 1.3        | 0.7   |
> | 2     | 3     | 3.1       | -0.1  |
> | 3     | 5     | 4.9        | 0.1  |
> | 4     | 6     | 6.7        | -0.7  |
>
> A an√°lise dos res√≠duos pode ajudar a verificar a adequa√ß√£o do modelo.

> üí° **Exemplo Num√©rico: Regulariza√ß√£o Ridge**
>
> Vamos considerar o mesmo conjunto de dados do exemplo anterior e aplicar a regulariza√ß√£o Ridge. A fun√ß√£o de custo com regulariza√ß√£o Ridge √© dada por:
>
> $J(\beta) = \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1x_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2$
>
> Onde $\lambda$ √© o par√¢metro de regulariza√ß√£o. Vamos comparar os coeficientes obtidos com diferentes valores de $\lambda$.
>
> Usando Python com `scikit-learn`:
> ```python
> import numpy as np
> from sklearn.linear_model import Ridge
>
> X = np.array([[1], [2], [3], [4]])
> y = np.array([2, 3, 5, 6])
>
> lambdas = [0, 0.1, 1, 10]
>
> for l in lambdas:
>     ridge = Ridge(alpha=l)
>     ridge.fit(X, y)
>     print(f"Lambda = {l}: Beta_0 = {ridge.intercept_:.2f}, Beta_1 = {ridge.coef_[0]:.2f}")
> ```
> Sa√≠da:
> ```
> Lambda = 0: Beta_0 = 0.50, Beta_1 = 1.30
> Lambda = 0.1: Beta_0 = 0.51, Beta_1 = 1.27
> Lambda = 1: Beta_0 = 0.57, Beta_1 = 1.09
> Lambda = 10: Beta_0 = 0.90, Beta_1 = 0.64
> ```
>
> **Interpreta√ß√£o:**
>
> *   Com $\lambda = 0$, temos o resultado do OLS.
> *   √Ä medida que $\lambda$ aumenta, os coeficientes s√£o "encolhidos" em dire√ß√£o a zero. Isso ajuda a reduzir o impacto de vari√°veis menos importantes e a evitar overfitting.
>
> **Visualiza√ß√£o:**
>
> ```mermaid
> graph TD
>    subgraph "Ridge Regression Impact of Lambda"
>      direction TB
>       A["Lambda = 0"] --> B["Œ≤_0 = 0.50, Œ≤_1 = 1.30"];
>       C["Lambda = 0.1"] --> D["Œ≤_0 = 0.51, Œ≤_1 = 1.27"];
>       E["Lambda = 1"] --> F["Œ≤_0 = 0.57, Œ≤_1 = 1.09"];
>       G["Lambda = 10"] --> H["Œ≤_0 = 0.90, Œ≤_1 = 0.64"];
>    end
> ```
>
> **Compara√ß√£o:**
>
> | M√©todo  | $\lambda$ | $\beta_0$ | $\beta_1$ |
> |---------|-----------|----------|----------|
> | OLS     | 0         | -0.5     | 1.8      |
> | Ridge   | 0.1       | 0.51     | 1.27     |
> | Ridge   | 1         | 0.57     | 1.09     |
> | Ridge   | 10        | 0.90     | 0.64     |

> üí° **Exemplo Num√©rico: Valida√ß√£o Cruzada**
>
> Suponha que temos um conjunto de dados maior e queremos avaliar o desempenho do nosso modelo usando valida√ß√£o cruzada k-fold. Vamos usar os seguintes dados:
>
> ```python
> import numpy as np
> from sklearn.model_selection import KFold
> from sklearn.linear_model import LinearRegression
> from sklearn.metrics import mean_squared_error
>
> X = np.array([[1, 2], [2, 4], [3, 5], [4, 7], [5, 9], [6, 10], [7, 12], [8, 13], [9, 15], [10, 18]])
> y = np.array([3, 5, 7, 8, 10, 11, 13, 14, 16, 19])
>
> kf = KFold(n_splits=5, shuffle=True, random_state=42)
> mse_scores = []
>
> for train_index, test_index in kf.split(X):
>    X_train, X_test = X[train_index], X[test_index]
>    y_train, y_test = y[train_index], y[test_index]
>
>    model = LinearRegression()
>    model.fit(X_train, y_train)
>    y_pred = model.predict(X_test)
>    mse = mean_squared_error(y_test, y_pred)
>    mse_scores.append(mse)
>
> print(f"MSE scores por fold: {mse_scores}")
> print(f"M√©dia MSE: {np.mean(mse_scores):.2f}")
> ```
> Sa√≠da:
> ```
> MSE scores por fold: [0.0022, 0.0130, 0.0070, 0.0033, 0.0079]
> M√©dia MSE: 0.01
> ```
>
> **Interpreta√ß√£o:** A valida√ß√£o cruzada nos permite ter uma estimativa mais robusta do erro de generaliza√ß√£o do modelo, calculando o MSE em m√∫ltiplos conjuntos de teste e obtendo uma m√©dia.
>
> ```mermaid
> graph TD
> subgraph "K-Fold Cross-Validation"
> direction TB
>     A["Input Data (X, y)"] --> B["Split Data into K Folds"];
>     B --> C{"For each fold k:"};
>     C --> D["Train Model on K-1 Folds"];
>     D --> E["Test Model on Fold k"];
>     E --> F["Calculate MSE_k"];
>     F --> G["Collect MSE_k Scores"];
>     G --> H["Calculate Average MSE"];
> end
> ```
Estes exemplos num√©ricos adicionam uma camada pr√°tica aos conceitos te√≥ricos, facilitando a compreens√£o e aplica√ß√£o dos m√©todos de regress√£o linear. Eles tamb√©m podem servir de base para exerc√≠cios e experimenta√ß√µes adicionais.
