## T√≠tulo Conciso: Localizing Flexibility

```mermaid
graph LR
    subgraph "Kernel Smoothing and Classification"
        direction TB
        A["Nadaraya-Watson Smoothing: Local Constant Fit"] --> B["Kernel Function"]
        C["Local Linear Regression: Local Hyperplane Fit"] --> B
         D["Local Polynomial Regression: Local Polynomial Fit"] --> B
        E["Kernel Density Classification"] --> F["Kernel Function"]
        G["Naive Bayes Classifier"] --> F
    subgraph "Regularization"
        H["Regularization via Kernel"]
        B --> H
        F --> H
    end
    end
```

### Regress√£o Local em $\mathbb{R}^p$

<!-- START Regress√£o Local em  $\mathbb{R}^p$ -->

A generaliza√ß√£o da suaviza√ß√£o kernel e da regress√£o local para duas ou mais dimens√µes ocorre de maneira natural [^6.3]. O suavizador kernel Nadaraya-Watson ajusta uma constante localmente com pesos fornecidos por um kernel $p$-dimensional. A regress√£o linear local ajusta um hiperplano localmente em $X$ por m√≠nimos quadrados ponderados, utilizando um kernel $p$-dimensional [^6.3]. √â simples de implementar e geralmente preferido ao ajuste local constante, devido ao seu melhor desempenho nas fronteiras.

Seja $b(X)$ um vetor de termos polinomiais em $X$ de grau m√°ximo $d$. Por exemplo, com $d = 1$ e $p = 2$, temos $b(X) = (1, X_1, X_2)$; com $d=2$, temos $b(X) = (1, X_1, X_2, X_1^2, X_2^2, X_1X_2)$; e trivialmente com $d=0$ temos $b(X) = 1$ [^6.3]. Em cada $x_0 \in \mathbb{R}^p$, resolvemos o problema:

$$ \min_{\beta(x_0)} \sum_{i=1}^N K_\lambda(x_0, x_i) (y_i - b(x_i)^T \beta(x_0))^2, $$

para produzir o ajuste $\hat{f}(x_0) = b(x_0)^T\beta(x_0)$ [^6.3]. Tipicamente, o kernel √© uma fun√ß√£o radial, como o Epanechnikov radial ou o kernel tri-c√∫bico:

$$ K_\lambda(x_0, x) = D\left(\frac{||x - x_0||}{\lambda}\right), $$

onde $|| \cdot ||$ √© a norma euclidiana [^6.3]. A norma euclidiana depende das unidades em cada coordenada, sendo essencial padronizar cada preditor, por exemplo, para um desvio padr√£o unit√°rio antes da suaviza√ß√£o.

```mermaid
graph LR
    subgraph "Local Regression Optimization"
        direction TB
        A["Minimize:  ‚àë K_Œª(x‚ÇÄ, x·µ¢) (y·µ¢ - b(x·µ¢)·µÄŒ≤(x‚ÇÄ))¬≤"]
        B["Kernel Function: K_Œª(x‚ÇÄ, x·µ¢)"]
        C["Polynomial Basis: b(x·µ¢)"]
        D["Local Coefficients: Œ≤(x‚ÇÄ)"]
        A --> B
        A --> C
        A --> D
    end
```

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados bidimensional com duas vari√°veis preditoras, $X_1$ e $X_2$, e uma vari√°vel resposta $Y$.  Vamos usar regress√£o linear local com um kernel gaussiano para estimar $Y$ em um ponto $x_0 = (1, 2)$.
>
> **Passo 1: Definir o kernel gaussiano**
>
>  O kernel gaussiano √© definido como:
>  $$K_\lambda(x_0, x_i) = e^{-\frac{||x_i - x_0||^2}{2\lambda^2}}$$
>
>  Vamos escolher $\lambda = 1$ para este exemplo.
>
> **Passo 2: Dados de exemplo**
>
>  Suponha que temos os seguintes dados:
>
>  | i | $X_{i1}$ | $X_{i2}$ | $Y_i$ |
>  |---|---|---|---|
>  | 1 | 0  | 1  | 3  |
>  | 2 | 1.5| 2.5| 5  |
>  | 3 | 2  | 1  | 4  |
>  | 4 | 0.5| 3  | 6  |
>
>  **Passo 3: Calcular os pesos do kernel**
>  Para cada ponto de dados $x_i$, calculamos o peso $K_\lambda(x_0, x_i)$:
>
>  $||x_1 - x_0||^2 = (0-1)^2 + (1-2)^2 = 2$, $K_\lambda(x_0, x_1) = e^{-2/2} = e^{-1} \approx 0.368$
>
>  $||x_2 - x_0||^2 = (1.5-1)^2 + (2.5-2)^2 = 0.5$, $K_\lambda(x_0, x_2) = e^{-0.5/2} = e^{-0.25} \approx 0.779$
>
>  $||x_3 - x_0||^2 = (2-1)^2 + (1-2)^2 = 2$, $K_\lambda(x_0, x_3) = e^{-2/2} = e^{-1} \approx 0.368$
>
>  $||x_4 - x_0||^2 = (0.5-1)^2 + (3-2)^2 = 1.25$, $K_\lambda(x_0, x_4) = e^{-1.25/2} = e^{-0.625} \approx 0.535$
>
> **Passo 4: Montar a matriz X e o vetor Y ponderados**
> Para regress√£o linear local com d=1, $b(X) = (1, X_1, X_2)$. A matriz X ponderada e o vetor Y ponderado s√£o:
> $X_w = \begin{bmatrix} \sqrt{0.368} & 0 & \sqrt{0.368} \\ \sqrt{0.779} & 1.5\sqrt{0.779} & 2.5\sqrt{0.779} \\ \sqrt{0.368} & 2\sqrt{0.368} & \sqrt{0.368} \\ \sqrt{0.535} & 0.5\sqrt{0.535} & 3\sqrt{0.535}  \end{bmatrix} = \begin{bmatrix} 0.607 & 0 & 0.607 \\ 0.883 & 1.324 & 2.208 \\ 0.607 & 1.214 & 0.607 \\ 0.731 & 0.365 & 2.193 \end{bmatrix}$
>
> $Y_w = \begin{bmatrix} 3\sqrt{0.368} \\ 5\sqrt{0.779} \\ 4\sqrt{0.368} \\ 6\sqrt{0.535} \end{bmatrix} = \begin{bmatrix} 1.822 \\ 4.395 \\ 2.428 \\ 4.387 \end{bmatrix}$
>
> **Passo 5: Calcular os coeficientes $\beta(x_0)$**
>
> $\beta(x_0) = (X_w^T X_w)^{-1} X_w^T Y_w $
>
>  Usando numpy para calcular isso:
>
> ```python
> import numpy as np
>
> X_w = np.array([[0.607, 0, 0.607],
>                 [0.883, 1.324, 2.208],
>                 [0.607, 1.214, 0.607],
>                 [0.731, 0.365, 2.193]])
> Y_w = np.array([1.822, 4.395, 2.428, 4.387])
>
> beta_x0 = np.linalg.solve(X_w.T @ X_w, X_w.T @ Y_w)
> print(beta_x0)
> # Output: [2.998, 0.794, 0.719]
> ```
>
> $\beta(x_0) \approx (2.998, 0.794, 0.719)$
>
> **Passo 6: Estimar $\hat{f}(x_0)$**
>
> $\hat{f}(x_0) = b(x_0)^T \beta(x_0) = (1, 1, 2) \cdot (2.998, 0.794, 0.719) = 2.998 + 0.794 + 2 * 0.719 = 5.23$
>
> Portanto, a estimativa da regress√£o local em $x_0 = (1, 2)$ √© aproximadamente 5.23.

As fronteiras s√£o um problema significativo em suaviza√ß√£o unidimensional, mas tornam-se um problema muito maior em duas ou mais dimens√µes, pois a fra√ß√£o de pontos na fronteira √© maior. Uma das manifesta√ß√µes da maldi√ß√£o da dimensionalidade √© que a fra√ß√£o de pontos pr√≥ximos √† fronteira cresce para um conforme a dimens√£o aumenta. Modificar o kernel diretamente para acomodar fronteiras bidimensionais torna-se muito complexo, especialmente para fronteiras irregulares. A regress√£o polinomial local realiza a corre√ß√£o de fronteira na ordem desejada, sem problemas em qualquer dimens√£o [^6.3]. No entanto, a regress√£o local torna-se menos √∫til em dimens√µes muito maiores do que dois ou tr√™s. √â imposs√≠vel manter simultaneamente a localidade (baixo vi√©s) e uma amostra consider√°vel na vizinhan√ßa (baixa vari√¢ncia) √† medida que a dimens√£o aumenta, sem que o tamanho total da amostra cres√ßa exponencialmente em $p$. A visualiza√ß√£o de $\hat{f}(X)$ tamb√©m se torna dif√≠cil em dimens√µes superiores, que √© frequentemente um dos objetivos principais da suaviza√ß√£o.

**Exemplo:** Uma aplica√ß√£o da regress√£o local √© demonstrada usando medi√ß√µes astron√¥micas com um design incomum do preditor (em forma de estrela), onde a fronteira √© extremamente irregular e a superf√≠cie ajustada deve interpolar sobre regi√µes de crescente esparsidade de dados, conforme descrito em [^6.3].

#### Pergunta Te√≥rica Avan√ßada: Efeitos da Dimens√£o em Regress√£o Local

```mermaid
graph TB
    subgraph "Dimensionality Effects"
        direction TB
        A["Increase in Data Dimension (p)"] --> B["Decreased Data Density"]
        B --> C["Difficulty Finding Local Neighborhoods"]
        C --> D["Increased Estimation Variance"]
        A --> E["Increased Fraction of Points Near Boundary"]
         E --> F["Complex Boundary Correction"]
         D & F --> G["Trade-off between Bias and Variance"]
    end
```

**Resposta:**  A maldi√ß√£o da dimensionalidade imp√µe severas limita√ß√µes √† regress√£o local em altas dimens√µes. A densidade dos dados diminui exponencialmente com o aumento da dimens√£o, tornando mais dif√≠cil encontrar vizinhan√ßas locais com amostras suficientes para uma estimativa precisa [^6.3].

**Lemma 5:** *√Ä medida que a dimens√£o dos dados aumenta, a fra√ß√£o de pontos pr√≥ximos √† fronteira cresce para um, tornando a corre√ß√£o de fronteira mais dif√≠cil e a estimativa mais imprecisa*, conforme demonstrado pela an√°lise da distribui√ß√£o de pontos em espa√ßos de alta dimens√£o.

**Corol√°rio 5:** *Em altas dimens√µes, o par√¢metro $\lambda$ precisa ser muito maior para manter a vari√¢ncia sob controle, mas isso aumenta o vi√©s da estimativa, sacrificando a localidade do ajuste*, o que pode ser observado atrav√©s da an√°lise do trade-off entre vi√©s e vari√¢ncia em diferentes dimens√µes.

> ‚ö†Ô∏è **Ponto Crucial:** O trade-off entre vi√©s e vari√¢ncia torna-se mais agudo em altas dimens√µes. M√©todos de regulariza√ß√£o, sele√ß√£o de vari√°veis e modelos estruturados tornam-se cruciais para lidar com essa complexidade [^6.3].

<!-- END Regress√£o Local em  $\mathbb{R}^p$ -->

### Kernels Estruturados

<!-- START Kernels Estruturados -->

Uma abordagem para lidar com a complexidade em altas dimens√µes √© modificar o kernel, conforme descrito em [^6.4.1]. O kernel esf√©rico padr√£o (6.13) d√° peso igual a cada coordenada, e uma estrat√©gia padr√£o √© padronizar cada vari√°vel para um desvio padr√£o unit√°rio. Uma abordagem mais geral usa uma matriz semidefinida positiva $A$ para ponderar as diferentes coordenadas:

$$ K_{\lambda,A}(x_0, x) = D\left(\frac{(x - x_0)^T A (x - x_0)}{\lambda^2}\right). $$

Coordenadas inteiras ou dire√ß√µes podem ser rebaixadas ou omitidas impondo restri√ß√µes apropriadas em $A$ [^6.4.1]. Por exemplo, se $A$ for diagonal, podemos aumentar ou diminuir a influ√™ncia dos preditores individuais $X_j$ aumentando ou diminuindo $A_{jj}$. Muitas vezes os preditores s√£o muitos e altamente correlacionados, como aqueles que surgem de sinais anal√≥gicos ou imagens digitalizadas. A fun√ß√£o de covari√¢ncia dos preditores pode ser usada para ajustar uma m√©trica $A$ que se concentre menos em contrastes de alta frequ√™ncia, por exemplo. Prop√¥s-se aprender os par√¢metros para kernels multidimensionais. Por exemplo, o modelo de regress√£o de proje√ß√£o discutido no Cap√≠tulo 11 √© dessa natureza, onde vers√µes de baixo posto de $A$ implicam fun√ß√µes de crista para $f(X)$. Modelos mais gerais para $A$ s√£o complexos e favorecemos formas estruturadas para a fun√ß√£o de regress√£o.

```mermaid
graph LR
    subgraph "Structured Kernel"
        direction TB
        A["Kernel: K_Œª,A(x‚ÇÄ, x) = D((x - x‚ÇÄ)·µÄA(x - x‚ÇÄ) / Œª¬≤)"]
        B["Matrix A: Weights for each coordinate"]
        C["Euclidean Distance: ||x - x‚ÇÄ||"]
        D["Kernel Width: Œª"]
        A --> B
        A --> C
        A --> D
    end
```

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos dois preditores, $X_1$ e $X_2$, e queremos usar um kernel estruturado. Vamos considerar um caso onde $X_1$ tem mais import√¢ncia que $X_2$.
>
> **Passo 1: Definir a Matriz A**
>
>  Vamos definir $A$ como uma matriz diagonal:
>  $$A = \begin{bmatrix} a_{11} & 0 \\ 0 & a_{22} \end{bmatrix}$$
>
>  Para dar mais import√¢ncia a $X_1$, vamos escolher $a_{11} = 2$ e $a_{22} = 1$.
>
>  $$A = \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix}$$
>
> **Passo 2: Kernel estruturado**
>
>  O kernel estruturado √© dado por:
>  $$K_{\lambda, A}(x_0, x) = e^{-\frac{(x - x_0)^T A (x - x_0)}{2\lambda^2}}$$
>
>  Vamos escolher $\lambda = 1$.
>
> **Passo 3: Dados de exemplo**
>
> Suponha que $x_0 = (1, 1)$ e temos um ponto $x = (2, 2)$.
>
> **Passo 4: Calcular $(x - x_0)^T A (x - x_0)$**
>
> $$x - x_0 = \begin{bmatrix} 2 - 1 \\ 2 - 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$$
>
> $$(x - x_0)^T A (x - x_0) = \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = 2 + 1 = 3$$
>
> **Passo 5: Calcular o peso do kernel**
>
> $$K_{\lambda, A}(x_0, x) = e^{-\frac{3}{2(1)^2}} = e^{-1.5} \approx 0.223$$
>
> Se us√°ssemos um kernel esf√©rico padr√£o com $\lambda = 1$, ter√≠amos:
>
> $$(x - x_0)^T(x - x_0) = (2-1)^2 + (2-1)^2 = 2$$
>
> $$K_{\lambda}(x_0, x) = e^{-\frac{2}{2(1)^2}} = e^{-1} \approx 0.368$$
>
>  O kernel estruturado com a matriz $A$ d√° um peso menor (0.223) em compara√ß√£o com o kernel esf√©rico padr√£o (0.368) para o mesmo ponto, porque enfatiza a diferen√ßa em $X_1$ mais do que em $X_2$.

**Pergunta Te√≥rica Avan√ßada:  Sele√ß√£o da Matriz A para kernels estruturados.**

**Resposta:** A escolha apropriada da matriz A no kernel estruturado tem um impacto direto no desempenho do modelo de regress√£o local. Diferentes escolhas de A podem levar a diferentes comportamentos do modelo, afetando o vi√©s e a vari√¢ncia da estimativa.

```mermaid
graph TB
 subgraph "Matrix A Selection"
  direction TB
        A["Choice of Matrix A"] --> B["Impact on Model Performance"]
        B --> C["Affects Bias of Estimation"]
        B --> D["Affects Variance of Estimation"]
        A --> E["Covariance Matrix of Predictors"]
 end
```

**Lemma 6:** *A matriz de covari√¢ncia dos preditores pode ser usada para ajustar a matriz A no kernel estruturado, o que pode reduzir o impacto de preditores altamente correlacionados*, conforme discutido em [^6.4.1].

**Prova do Lemma 6:** Se os preditores s√£o altamente correlacionados, suas vari√¢ncias individuais podem ser menos informativas do que suas covari√¢ncias. Usar a matriz de covari√¢ncia como base para A pode capturar essa informa√ß√£o, permitindo que o modelo ignore as componentes de alta frequ√™ncia e concentre-se nos padr√µes de baixa frequ√™ncia. $\blacksquare$

**Corol√°rio 6:** *Ao construir A com base na matriz de covari√¢ncia dos preditores, √© poss√≠vel criar uma m√©trica que se concentra em componentes relevantes dos dados*, como discutido em [^6.4.1], melhorando a efici√™ncia do modelo em espa√ßos de alta dimens√£o.

> ‚ö†Ô∏è **Ponto Crucial:** Em muitos casos, simplificar a matriz A, como torn√°-la diagonal, pode trazer benef√≠cios computacionais sem perda significativa de desempenho. Al√©m disso, modelos de aprendizado podem ser usados para aprender os par√¢metros da matriz A, adicionando uma camada de adaptabilidade ao m√©todo [^6.4.1].

<!-- END Kernels Estruturados -->

### Fun√ß√µes de Regress√£o Estruturadas

<!-- START Fun√ß√µes de Regress√£o Estruturadas -->

Estamos tentando ajustar uma fun√ß√£o de regress√£o $E(Y|X) = f(X_1, X_2, \ldots, X_p)$ em $\mathbb{R}^p$, na qual todos os n√≠veis de intera√ß√£o est√£o potencialmente presentes [^6.4.2]. √â natural considerar decomposi√ß√µes de an√°lise de vari√¢ncia (ANOVA) da forma:

$$f(X_1, X_2, \ldots, X_p) = a + \sum_j g_j(X_j) + \sum_{k<l} g_{kl}(X_k, X_l) + \ldots$$

e, em seguida, introduzir estrutura eliminando alguns dos termos de ordem superior [^6.4.2]. Os modelos aditivos assumem apenas termos de efeito principal:

$$ f(X) = a + \sum_{j=1}^p g_j(X_j); $$

modelos de segunda ordem ter√£o termos com intera√ß√µes de ordem no m√°ximo dois, e assim por diante. No Cap√≠tulo 9, descrevemos algoritmos iterativos de *backfitting* para ajustar tais modelos de intera√ß√£o de baixa ordem. No modelo aditivo, por exemplo, se todos os termos, exceto o k-√©simo, forem considerados conhecidos, podemos estimar $g_k$ por regress√£o local de $Y - \sum_{j \neq k} g_j(X_j)$ em $X_k$. Isso √© feito para cada fun√ß√£o, por sua vez, repetidamente, at√© a converg√™ncia [^6.4.2]. O detalhe importante √© que, em qualquer est√°gio, tudo o que √© necess√°rio √© uma regress√£o local unidimensional. As mesmas ideias podem ser usadas para ajustar decomposi√ß√µes ANOVA de baixa dimens√£o.

```mermaid
graph TB
    subgraph "ANOVA Decomposition"
    direction TB
        A["f(X) = a + ‚àë g‚±º(X‚±º) + ‚àë g‚Çñ‚Çó(X‚Çñ, X‚Çó) + ..."]
        B["Additive Model: f(X) = a + ‚àë g‚±º(X‚±º)"]
        C["Second-Order Model: Includes pairwise interactions"]
        D["Backfitting Algorithm: Iterative estimation of each g‚±º"]
        A --> B
        A --> C
         B --> D
    end
```

Um caso especial importante desses modelos estruturados √© a classe de modelos de coeficientes vari√°veis. Suponha, por exemplo, que dividamos os preditores $p$ em $X$ em um conjunto $(X_1, X_2, \ldots, X_q)$ com $q < p$, e o restante das vari√°veis coletamos no vetor $Z$ [^6.4.2]. Ent√£o, assumimos o modelo condicionalmente linear:

$$f(X) = \alpha(Z) + \beta_1(Z)X_1 + \ldots + \beta_q(Z)X_q.$$

Para um dado $Z$, este √© um modelo linear, mas cada um dos coeficientes pode variar com $Z$. √â natural ajustar tal modelo por m√≠nimos quadrados ponderados localmente:

$$ \min_{\alpha(z_0), \beta(z_0)} \sum_{i=1}^N K_\lambda(z_0, z_i) (y_i - \alpha(z_0) - x_{i1}\beta_1(z_0) - \ldots - x_{iq}\beta_q(z_0))^2. $$

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um modelo com um preditor $X$ e um modificador $Z$. Suponha que temos o seguinte modelo:
>
> $$f(X) = \alpha(Z) + \beta(Z)X$$
>
> Onde $\alpha(Z)$ e $\beta(Z)$ variam com $Z$.
>
> **Passo 1: Dados de Exemplo**
>
>  Suponha que temos os seguintes dados:
>
> | i | $X_i$ | $Z_i$ | $Y_i$ |
>  |---|---|---|---|
>  | 1 | 1 | 1 | 3  |
>  | 2 | 2 | 1.2 | 5 |
>  | 3 | 1.5 | 0.8 | 4  |
>  | 4 | 3 | 1.5 | 7  |
>
> **Passo 2: Kernel e ponto local**
>
> Vamos usar um kernel gaussiano com $\lambda = 0.5$ e queremos estimar em $z_0 = 1$.
>
> **Passo 3: Calcular os pesos do kernel**
>
> $K_\lambda(z_0, z_1) = e^{-\frac{(1-1)^2}{2(0.5)^2}} = e^0 = 1$
>
> $K_\lambda(z_0, z_2) = e^{-\frac{(1-1.2)^2}{2(0.5)^2}} = e^{-0.08} \approx 0.923$
>
> $K_\lambda(z_0, z_3) = e^{-\frac{(1-0.8)^2}{2(0.5)^2}} = e^{-0.08} \approx 0.923$
>
> $K_\lambda(z_0, z_4) = e^{-\frac{(1-1.5)^2}{2(0.5)^2}} = e^{-0.5} \approx 0.607$
>
> **Passo 4: Montar a matriz e vetor ponderados**
>
> Para regress√£o linear local, precisamos ajustar $\alpha(z_0)$ e $\beta(z_0)$. A matriz X ponderada e o vetor Y ponderado s√£o:
>
> $X_w = \begin{bmatrix} \sqrt{1} & 1\sqrt{1} \\ \sqrt{0.923} & 2\sqrt{0.923} \\ \sqrt{0.923} & 1.5\sqrt{0.923} \\ \sqrt{0.607} & 3\sqrt{0.607} \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 0.961 & 1.923 \\ 0.961 & 1.442 \\ 0.779 & 2.338 \end{bmatrix}$
>
> $Y_w = \begin{bmatrix} 3\sqrt{1} \\ 5\sqrt{0.923} \\ 4\sqrt{0.923} \\ 7\sqrt{0.607} \end{bmatrix} = \begin{bmatrix} 3 \\ 4.615 \\ 3.692 \\ 5.499 \end{bmatrix}$
>
> **Passo 5: Calcular $\alpha(z_0)$ e $\beta(z_0)$**
>
> Usando numpy:
>
> ```python
> import numpy as np
>
> X_w = np.array([[1, 1],
>                 [0.961, 1.923],
>                 [0.961, 1.442],
>                 [0.779, 2.338]])
> Y_w = np.array([3, 4.615, 3.692, 5.499])
>
> beta_z0 = np.linalg.solve(X_w.T @ X_w, X_w.T @ Y_w)
> print(beta_z0)
> # Output: [2.127, 1.088]
> ```
>
> $\alpha(z_0) \approx 2.127$ e $\beta(z_0) \approx 1.088$.
>
> **Passo 6: Estimar $f(X)$ em $z_0 = 1$**
>
> $f(X) = 2.127 + 1.088 X$.
>
> Para um valor de $X$, por exemplo $X=2$, a estimativa seria $f(2) = 2.127 + 1.088 * 2 = 4.303$

```mermaid
graph LR
    subgraph "Variable Coefficient Model"
        direction TB
        A["f(X) = Œ±(Z) + Œ≤‚ÇÅ(Z)X‚ÇÅ + ... + Œ≤q(Z)Xq"]
        B["Predictors: X = (X‚ÇÅ, X‚ÇÇ, ..., Xq)"]
        C["Modifiers: Z"]
        D["Coefficients: Œ±(Z), Œ≤(Z) Vary with Z"]
        A --> B
        A --> C
        A --> D
   end
```

**Exemplo:** Um exemplo pr√°tico para essa metodologia √© a modelagem do di√¢metro da aorta em rela√ß√£o √† idade, com coeficientes que variam com g√™nero e profundidade na aorta, demonstrando como o modelo condicionalmente linear pode capturar rela√ß√µes complexas em dados biol√≥gicos [^6.4.2].

#### Pergunta Te√≥rica Avan√ßada: Vantagens da Decomposi√ß√£o ANOVA em Regress√£o Local.

**Resposta:** A decomposi√ß√£o ANOVA, quando utilizada com modelos de regress√£o local, oferece v√°rias vantagens, incluindo uma forma de lidar com a maldi√ß√£o da dimensionalidade e uma maior interpretabilidade do modelo.

**Lemma 7:** *Ao assumir uma estrutura aditiva, a decomposi√ß√£o ANOVA reduz a complexidade do problema de ajuste em espa√ßos de alta dimens√£o, pois cada componente do modelo √© estimado separadamente, usando apenas regress√µes locais unidimensionais*, conforme descrito em [^6.4.2].

**Corol√°rio 7:** *A decomposi√ß√£o ANOVA permite identificar quais vari√°veis ou intera√ß√µes entre vari√°veis t√™m maior impacto sobre a vari√°vel resposta, melhorando a interpretabilidade do modelo*, o que pode ser √∫til na an√°lise e compreens√£o dos dados, de acordo com [^6.4.2].

> ‚ö†Ô∏è **Ponto Crucial:** √â importante notar que a escolha da decomposi√ß√£o ANOVA deve ser baseada no conhecimento pr√©vio do dom√≠nio do problema, para garantir que a estrutura assumida seja razo√°vel e capture os padr√µes relevantes nos dados [^6.4.2].

<!-- END Fun√ß√µes de Regress√£o Estruturadas -->

### Local Likelihood e Outros Modelos

<!-- START Local Likelihood e Outros Modelos -->

O conceito de regress√£o local e modelos de coeficientes vari√°veis √© extremamente amplo: qualquer modelo param√©trico pode se tornar local se o m√©todo de ajuste acomodar pesos de observa√ß√£o [^6.5]. Aqui est√£o alguns exemplos:

*   Associado a cada observa√ß√£o $y_i$ existe um par√¢metro $\theta_i = \theta(x_i) = x_i^T\beta$ linear na covari√°vel $x_i$, e a infer√™ncia para $\beta$ √© baseada na verossimilhan√ßa $\ell(\beta) = \sum_{i=1}^N \ell(y_i, x_i^T\beta)$. Podemos modelar $\theta(X)$ de forma mais flex√≠vel usando a verossimilhan√ßa local para $x_0$ para infer√™ncia de $\theta(x_0) = x_0^T\beta(x_0)$:

    $$ \ell(\beta(x_0)) = \sum_{i=1}^N K_\lambda(x_0, x_i) \ell(y_i, x_i^T\beta(x_0)).$$

*   Como acima, exceto que diferentes vari√°veis est√£o associadas a $\theta$ daquelas usadas para definir a verossimilhan√ßa local:

    $$ \ell(\theta(z_0)) = \sum_{i=1}^N K_\lambda(z_0, z_i) \ell(y_i, \eta(x_i, \theta(z_0))). $$
    
    Por exemplo, $\eta(x, \theta) = x^T\theta$ poderia ser um modelo linear em $x$. Isso ajustar√° um modelo de coeficiente vari√°vel $\theta(z)$ maximizando a verossimilhan√ßa local.

*   Modelos de s√©ries temporais autoregressivas de ordem $k$ t√™m a forma $y_t = \beta_0 + \beta_1 y_{t-1} + \beta_2 y_{t-2} + \ldots + \beta_k y_{t-k} + \epsilon_t$. Denotando o conjunto de defasagens por $Z_t = (y_{t-1}, y_{t-2}, \ldots, y_{t-k})$, o modelo se parece com um modelo linear padr√£o $y_t = z_t^T\beta + \epsilon_t$, e √© tipicamente ajustado por m√≠nimos quadrados. Ajustar por m√≠nimos quadrados locais com um kernel $K(z_0, z_t)$ permite que o modelo varie de acordo com o hist√≥rico de curto prazo da s√©rie [^6.5]. Isso deve ser distinguido dos modelos lineares din√¢micos mais tradicionais que variam por tempo de janelamento.

```mermaid
graph LR
    subgraph "Local Likelihood Framework"
        direction TB
        A["Global Parametric Model:  ‚Ñì(Œ≤) = ‚àë‚Ñì(y·µ¢, x·µ¢·µÄŒ≤)"]
        B["Local Likelihood: ‚Ñì(Œ≤(x‚ÇÄ)) = ‚àëKŒª(x‚ÇÄ, x·µ¢)‚Ñì(y·µ¢, x·µ¢·µÄŒ≤(x‚ÇÄ))"]
         C["Variable Coefficient: ‚Ñì(Œ∏(z‚ÇÄ)) = ‚àëKŒª(z‚ÇÄ, z·µ¢)‚Ñì(y·µ¢, Œ∑(x·µ¢, Œ∏(z‚ÇÄ)))"]
        A --> B
        B --> C
        D["Autoregressive Model: y‚Çú = Œ≤‚ÇÄ + Œ≤‚ÇÅy‚Çú‚Çã‚ÇÅ + ... + Œµ‚Çú"]
         D --> E["Local AR:  Kernel K(z‚ÇÄ, z‚Çú)"]
    end
```

Como ilustra√ß√£o da verossimilhan√ßa local, consideramos a vers√£o local do modelo de regress√£o log√≠stica linear multiclasse (4.36) do Cap√≠tulo 4. Os dados consistem em recursos $x_i$ e uma resposta categ√≥rica associada $g_i \in \{1, 2, \ldots, J\}$, e o modelo linear tem a forma

$$ P(G = j | X = x) = \frac{e^{\beta_{j0} + \beta_j^Tx}}{1 + \sum_{k=1}^{J-1} e^{\beta_{k0} + \beta_k^Tx}} $$

A verossimilhan√ßa logar√≠tmica local para este modelo de classe $J$ pode ser escrita como

$$ \sum_{i=1}^N K_\lambda(x_0, x_i) \left[  \log(e^{\beta_{g_i0}(x_0) + \beta_{g_i}^T(x_i - x_0)}) - \log\left(1 + \sum_{k=1}^{J-1} e^{\beta_{k0}(x_0) + \beta_k^T(x_i - x_0)}\right)\right] $$

Note que:
* usamos $g_i$ como um subscrito na primeira linha para escolher o numerador apropriado;
* $\beta_{j0} = 0$ e $\beta_j = 0$ pela defini√ß√£o do modelo;
* centramos as regress√µes locais em $x_0$, de