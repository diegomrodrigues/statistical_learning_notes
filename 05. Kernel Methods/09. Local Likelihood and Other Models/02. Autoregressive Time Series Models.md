## Modelos Autorregressivos de Séries Temporais com Suavização Kernel

### Introdução
Este capítulo se aprofunda no conceito de modelos autorregressivos de séries temporais, expandindo as ideias apresentadas em capítulos anteriores sobre métodos de suavização kernel. Especificamente, exploraremos como a aplicação de técnicas de suavização kernel pode aprimorar a flexibilidade e a precisão dos modelos autorregressivos. Como vimos anteriormente, a suavização kernel permite a estimativa de funções de regressão através da ponderação de observações vizinhas [^1, ^2]. Neste capítulo, demonstraremos como essa abordagem pode ser utilizada para adaptar modelos autorregressivos à história de curto prazo das séries temporais, capturando dinâmicas locais que modelos globais podem negligenciar.

### Conceitos Fundamentais

Um **modelo autorregressivo de ordem k**, denotado por AR(k), assume que o valor atual de uma série temporal, $y_t$, depende linearmente de seus *k* valores passados, $y_{t-1}, y_{t-2}, ..., y_{t-k}$, acrescido de um termo de erro aleatório, $\epsilon_t$ [^16]. A forma geral de um modelo AR(k) é dada por:

$$ y_t = \beta_0 + \beta_1 y_{t-1} + \beta_2 y_{t-2} + \dots + \beta_k y_{t-k} + \epsilon_t $$

onde:
*   $\beta_0$ é o termo constante (intercepto).
*   $\beta_1, \beta_2, ..., \beta_k$ são os coeficientes autorregressivos.
*   $\epsilon_t$ é o termo de erro, geralmente assumido como ruído branco com média zero e variância constante.

A **ideia central** deste capítulo é permitir que os coeficientes $\beta_0, \beta_1, ..., \beta_k$ variem ao longo do tempo ou de acordo com o contexto local da série temporal. Isso é alcançado ajustando o modelo AR(k) localmente usando mínimos quadrados ponderados, com um kernel $K(x_0, x_t)$ que atribui pesos maiores às observações mais próximas do ponto de consulta $x_0$ [^1, ^2].

#### Ajuste Local com Kernel

Para implementar o ajuste local, definimos uma função de ponderação ou **kernel** $K_{\lambda}(x_0, x_i)$ que atribui um peso a cada observação $x_i$ com base em sua distância do ponto de consulta $x_0$ [^1]. O parâmetro $\lambda$ controla a largura do kernel, determinando o tamanho da vizinhança local considerada [^1, ^3]. Kernels comuns incluem o kernel Epanechnikov e o kernel Gaussiano [^3, ^4].

O **kernel Epanechnikov** é dado por:

$$ K_{\lambda}(x_0, x) = D\left(\frac{|x - x_0|}{\lambda}\right) $$

onde

$$ D(t) = \begin{cases} \frac{3}{4}(1 - t^2) & \text{se } |t| \leq 1 \\\\ 0 & \text{caso contrário} \end{cases} $$

O **kernel Gaussiano** é dado por:

$$ K_{\lambda}(x_0, x) = \frac{1}{\sqrt{2\pi\lambda^2}} e^{-\frac{(x - x_0)^2}{2\lambda^2}} $$

Com o kernel definido, o **ajuste local** do modelo AR(k) é realizado minimizando a seguinte função de perda ponderada:

$$ \sum_{t=k+1}^{T} K_{\lambda}(x_0, x_t) (y_t - \beta_0(x_0) - \beta_1(x_0) y_{t-1} - \dots - \beta_k(x_0) y_{t-k})^2 $$

onde $x_t = (y_{t-1}, y_{t-2}, ..., y_{t-k})$ representa o vetor de *k* valores passados da série temporal no instante *t*, e $\beta_i(x_0)$ são os coeficientes autorregressivos estimados localmente no ponto $x_0$.

#### Seleção da Largura do Kernel

A **seleção apropriada da largura do kernel**, $\lambda$, é crucial para o desempenho do modelo [^3, ^6]. Um $\lambda$ muito pequeno pode levar a um ajuste excessivo (alta variância), enquanto um $\lambda$ muito grande pode resultar em um ajuste subestimado (alto bias) [^3, ^6]. Técnicas comuns para selecionar $\lambda$ incluem validação cruzada e métodos baseados em critérios de informação, como AIC ou BIC [^9].

#### Vantagens e Desvantagens

**Vantagens:**

*   **Flexibilidade:** Permite que o modelo AR(k) se adapte a mudanças na dinâmica da série temporal.
*   **Precisão:** Pode fornecer estimativas mais precisas em regiões onde a relação entre o valor atual e os valores passados da série temporal não é constante.
*   **Interpretabilidade:** Os coeficientes $\beta_i(x_0)$ podem ser interpretados como medidas da influência local dos valores passados da série temporal no valor atual.

**Desvantagens:**

*   **Complexidade computacional:** O ajuste local requer o ajuste do modelo AR(k) em cada ponto de consulta, o que pode ser computacionalmente caro.
*   **Seleção da largura do kernel:** A escolha da largura do kernel pode ter um impacto significativo no desempenho do modelo.
*   **Interpretação:** A interpretação dos coeficientes $\beta_i(x_0)$ pode ser mais difícil do que a interpretação dos coeficientes em um modelo AR(k) global.

### Conclusão

A combinação de modelos autorregressivos com técnicas de suavização kernel oferece uma abordagem poderosa e flexível para a modelagem de séries temporais. Ao permitir que os coeficientes do modelo AR(k) variem localmente, podemos capturar dinâmicas complexas e não estacionárias que modelos globais podem negligenciar. Embora a complexidade computacional e a seleção da largura do kernel representem desafios, os benefícios em termos de precisão e interpretabilidade podem justificar o esforço adicional. Em continuidade ao conceito de ajustar modelos localmente, a seção 6.1.1 introduz a regressão linear local, que pode ser vista como uma extensão do conceito explorado neste capítulo [^4, ^5].

### Referências

[^1]: Página 191, "Kernel Smoothing Methods"
[^2]: Página 192, "Kernel Smoothing Methods"
[^3]: Página 193, "Kernel Smoothing Methods"
[^4]: Página 194, "Kernel Smoothing Methods"
[^5]: Página 195, "Kernel Smoothing Methods"
[^6]: Página 194, "Kernel Smoothing Methods"
[^9]: Página 199, "Kernel Smoothing Methods"
[^16]: Página 206, "Kernel Smoothing Methods"
<!-- END -->